<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Wed 25 Oct 23  to  Thu 26 Oct 23, announced Fri, 27 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item347">Cross-lists</a></li>
<li><a href="#item385">Replacements</a></li>
</ul>
<small>[ total of 659 entries:  <b>1-659</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Fri, 27 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16842" title="Abstract">arXiv:2310.16842</a> [<a href="/pdf/2310.16842" title="Download PDF">pdf</a>, <a href="/format/2310.16842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM  Cells for Embedded FPGAs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chao Qian</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+T">Tianheng Ling</a>, 
<a href="/search/cs?searchtype=author&query=Schiele%2C+G">Gregor Schiele</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">To process sensor data in the Internet of Things(IoTs), embedded deep
learning for 1-dimensional data is an important technique. In the past, CNNs
were frequently used because they are simple to optimise for special embedded
hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed
at energy-efficient inference on end devices. Using the traffic speed
prediction as a case study, a vanilla LSTM model with the optimised LSTM cell
achieves 17534 inferences per second while consuming only 3.8 $\mu$J per
inference on the FPGA \textit{XC7S15} from \textit{Spartan-7} family. It
achieves at least 5.4$\times$ faster throughput and 1.37$\times$ more energy
efficient than existing approaches.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16843" title="Abstract">arXiv:2310.16843</a> [<a href="/pdf/2310.16843" title="Download PDF">pdf</a>, <a href="/format/2310.16843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental Demonstration of Non-Stateful In-Memory Logic with 1T1R  OxRAM Valence Change Mechanism Memristors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Padberg%2C+H">Henriette Padberg</a>, 
<a href="/search/cs?searchtype=author&query=Regev%2C+A">Amir Regev</a>, 
<a href="/search/cs?searchtype=author&query=Piccolboni%2C+G">Giuseppe Piccolboni</a>, 
<a href="/search/cs?searchtype=author&query=Bricalli%2C+A">Alessandro Bricalli</a>, 
<a href="/search/cs?searchtype=author&query=Molas%2C+G">Gabriel Molas</a>, 
<a href="/search/cs?searchtype=author&query=Nodin%2C+J+F">Jean Francois Nodin</a>, 
<a href="/search/cs?searchtype=author&query=Kvatinsky%2C+S">Shahar Kvatinsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Circuits and Systems II: Express Briefs
  (2023), pages 1-1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET); Systems and Control (eess.SY)

</div>
<p class="mathjax">Processing-in-memory (PIM) is attractive to overcome the limitations of
modern computing systems. Numerous PIM systems exist, varying by the
technologies and logic techniques used. Successful operation of specific logic
functions is crucial for effective processing-in-memory. Memristive
non-stateful logic techniques are compatible with CMOS logic and can be
integrated into a 1T1R memory array, similar to commercial RRAM products. This
paper analyzes and demonstrates two non-stateful logic techniques: 1T1R logic
and scouting logic. As a first step, the used 1T1R SiO\textsubscript{x} valence
change mechanism memristors are characterized in reference to their feasibility
to perform logic functions. Various logical functions of the two logic
techniques are experimentally demonstrated, showing correct functionality in
all cases. Following the results, the challenges and limitations of the RRAM
characteristics and 1T1R configuration for the application in logical functions
are discussed.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16844" title="Abstract">arXiv:2310.16844</a> [<a href="/pdf/2310.16844" title="Download PDF">pdf</a>, <a href="/format/2310.16844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hardware-Algorithm Co-design Enabling Processing-in-Pixel-in-Memory  (P2M) for Neuromorphic Vision Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaiser%2C+M+A">Md Abdullah-Al Kaiser</a>, 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+A+R">Akhilesh R. Jaiswal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The high volume of data transmission between the edge sensor and the cloud
processor leads to energy and throughput bottlenecks for resource-constrained
edge devices focused on computer vision. Hence, researchers are investigating
different approaches (e.g., near-sensor processing, in-sensor processing,
in-pixel processing) by executing computations closer to the sensor to reduce
the transmission bandwidth. Specifically, in-pixel processing for neuromorphic
vision sensors (e.g., dynamic vision sensors (DVS)) involves incorporating
asynchronous multiply-accumulate (MAC) operations within the pixel array,
resulting in improved energy efficiency. In a CMOS implementation, low overhead
energy-efficient analog MAC accumulates charges on a passive capacitor;
however, the capacitor's limited charge retention time affects the algorithmic
integration time choices, impacting the algorithmic accuracy, bandwidth,
energy, and training efficiency. Consequently, this results in a design
trade-off on the hardware aspect-creating a need for a low-leakage compute unit
while maintaining the area and energy benefits. In this work, we present a
holistic analysis of the hardware-algorithm co-design trade-off based on the
limited integration time posed by the hardware and techniques to improve the
leakage performance of the in-pixel analog MAC operations.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16848" title="Abstract">arXiv:2310.16848</a> [<a href="/pdf/2310.16848" title="Download PDF">pdf</a>, <a href="/format/2310.16848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Changes in Crowdsourced Social Media Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Umair%2C+M">Muhammad Umair</a>, 
<a href="/search/cs?searchtype=author&query=Bouguettaya%2C+A">Athman Bouguettaya</a>, 
<a href="/search/cs?searchtype=author&query=Lakhdari%2C+A">Abdallah Lakhdari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted Paper in ICSOC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">We propose a novel service framework to detect changes in crowdsourced
images. We use a service-oriented approach to model and represent crowdsourced
images as image services. Non-functional attributes of an image service are
leveraged to detect changes in an image. The changes are reported in form of a
version tree. The version tree is constructed in a way that it reflects the
extent of changes introduced in different versions. Afterwards, we find
semantic differences in between different versions to determine the extent of
changes introduced in a specific version. Preliminary experimental results
demonstrate the effectiveness of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16853" title="Abstract">arXiv:2310.16853</a> [<a href="/pdf/2310.16853" title="Download PDF">pdf</a>, <a href="/format/2310.16853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CP-BCS: Binary Code Summarization Guided by Control Flow Graph and  Pseudo Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+T">Tong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lingfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tengfei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yangkai Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peiyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shouling Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automatically generating function summaries for binaries is an extremely
valuable but challenging task, since it involves translating the execution
behavior and semantics of the low-level language (assembly code) into
human-readable natural language. However, most current works on understanding
assembly code are oriented towards generating function names, which involve
numerous abbreviations that make them still confusing. To bridge this gap, we
focus on generating complete summaries for binary functions, especially for
stripped binary (no symbol table and debug information in reality). To fully
exploit the semantics of assembly code, we present a control flow graph and
pseudo code guided binary code summarization framework called CP-BCS. CP-BCS
utilizes a bidirectional instruction-level control flow graph and pseudo code
that incorporates expert knowledge to learn the comprehensive binary function
execution behavior and logic semantics. We evaluate CP-BCS on 3 different
binary optimization levels (O1, O2, and O3) for 3 different computer
architectures (X86, X64, and ARM). The evaluation results demonstrate CP-BCS is
superior and significantly improves the efficiency of reverse engineering.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16856" title="Abstract">arXiv:2310.16856</a> [<a href="/pdf/2310.16856" title="Download PDF">pdf</a>, <a href="/format/2310.16856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraFT: Gradual Fusion Transformer for Multimodal Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Haoli Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiayao Li</a> (Emily), 
<a href="/search/cs?searchtype=author&query=Schiller%2C+E">Eva Schiller</a>, 
<a href="/search/cs?searchtype=author&query=McDermott%2C+L">Luke McDermott</a>, 
<a href="/search/cs?searchtype=author&query=Cummings%2C+D">Daniel Cummings</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 Borderline Reviews at WACV, 8 pages, 5 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Object Re-Identification (ReID) is pivotal in computer vision, witnessing an
escalating demand for adept multimodal representation learning. Current models,
although promising, reveal scalability limitations with increasing modalities
as they rely heavily on late fusion, which postpones the integration of
specific modality insights. Addressing this, we introduce the \textbf{Gradual
Fusion Transformer (GraFT)} for multimodal ReID. At its core, GraFT employs
learnable fusion tokens that guide self-attention across encoders, adeptly
capturing both modality-specific and object-specific features. Further
bolstering its efficacy, we introduce a novel training paradigm combined with
an augmented triplet loss, optimizing the ReID feature embedding space. We
demonstrate these enhancements through extensive ablation studies and show that
GraFT consistently surpasses established multimodal ReID benchmarks.
Additionally, aiming for deployment versatility, we've integrated neural
network pruning into GraFT, offering a balance between model size and
performance.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16858" title="Abstract">arXiv:2310.16858</a> [<a href="/pdf/2310.16858" title="Download PDF">pdf</a>, <a href="/format/2310.16858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance  Fields via 4D Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Dadong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+Z">Zhihui Ke</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaobo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xidong Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://patrickddj.github.io/4D-Editor">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper targets interactive object-level editing(e.g., deletion,
recoloring, transformation, composition) in dynamic scenes. Recently, some
methods aiming for flexible editing static scenes represented by neural
radiance field (NeRF) have shown impressive synthesis quality, while similar
capabilities in time-variant dynamic scenes remain limited. To solve this
problem, we propose 4D-Editor, an interactive semantic-driven editing
framework, allowing editing multiple objects in dynamic NeRF based on user
strokes on a single frame. Our dynamic scene representation is built upon
hybrid semantic feature fields so that the spatial-temporal consistency can be
maintained after editing. In addition, we design recursive selection refinement
that significantly boosts segmentation accuracy in a dynamic NeRF to aid the
editing process. Moreover, we develop multi-view reprojection inpainting to
fill holes caused by incomplete scene capture after editing. Extensive
experiments and editing examples on real-world demonstrate that 4D-Editor
achieves photo-realistic dynamic NeRF editing. Project page:
https://patrickddj.github.io/4D-Editor
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16861" title="Abstract">arXiv:2310.16861</a> [<a href="/pdf/2310.16861" title="Download PDF">pdf</a>, <a href="/format/2310.16861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General Point Model with Autoencoding and Autoregressive
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L+T">Laurence T. Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The pre-training architectures of large language models encompass various
types, including autoencoding models, autoregressive models, and
encoder-decoder models. We posit that any modality can potentially benefit from
a large language model, as long as it undergoes vector quantization to become
discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which
seamlessly integrates autoencoding and autoregressive tasks in point cloud
transformer. This model is versatile, allowing fine-tuning for downstream point
cloud representation tasks, as well as unconditional and conditional generation
tasks. GPM enhances masked prediction in autoencoding through various forms of
mask padding tasks, leading to improved performance in point cloud
understanding. Additionally, GPM demonstrates highly competitive results in
unconditional point cloud generation tasks, even exhibiting the potential for
conditional generation tasks by modifying the input's conditional information.
Compared to models like Point-BERT, MaskPoint and PointMAE, our GPM achieves
superior performance in point cloud understanding tasks. Furthermore, the
integration of autoregressive and autoencoding within the same transformer
underscores its versatility across different downstream tasks.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16862" title="Abstract">arXiv:2310.16862</a> [<a href="/pdf/2310.16862" title="Download PDF">pdf</a>, <a href="/format/2310.16862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing Augmentation with Edge-Utility Filter for Signed GNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Ke-Jia Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Youran Qu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chuhan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Signed graph neural networks (SGNNs) has recently drawn more attention as
many real-world networks are signed networks containing two types of edges:
positive and negative. The existence of negative edges affects the SGNN
robustness on two aspects. One is the semantic imbalance as the negative edges
are usually hard to obtain though they can provide potentially useful
information. The other is the structural unbalance, e.g. unbalanced triangles,
an indication of incompatible relationship among nodes. In this paper, we
propose a balancing augmentation method to address the above two aspects for
SGNNs. Firstly, the utility of each negative edge is measured by calculating
its occurrence in unbalanced structures. Secondly, the original signed graph is
selectively augmented with the use of (1) an edge perturbation regulator to
balance the number of positive and negative edges and to determine the ratio of
perturbed edges to original edges and (2) an edge utility filter to remove the
negative edges with low utility to make the graph structure more balanced.
Finally, a SGNN is trained on the augmented graph which effectively explores
the credible relationships. A detailed theoretical analysis is also conducted
to prove the effectiveness of each module. Experiments on five real-world
datasets in link prediction demonstrate that our method has the advantages of
effectiveness and generalization and can significantly improve the performance
of SGNN backbones.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16866" title="Abstract">arXiv:2310.16866</a> [<a href="/pdf/2310.16866" title="Download PDF">pdf</a>, <a href="/format/2310.16866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Type System for Julia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+B">Benjamin Chung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">The Julia programming language was designed to fill the needs of scientific
computing by combining the benefits of productivity and performance languages.
Julia allows users to write untyped scripts easily without needing to worry
about many implementation details, as do other productivity languages. If one
just wants to get the work done-regardless of how efficient or general the
program might be, such a paradigm is ideal. Simultaneously, Julia also allows
library developers to write efficient generic code that can run as fast as
implementations in performance languages such as C or Fortran. This combination
of user-facing ease and library developer-facing performance has proven quite
attractive, and the language has increasing adoption.
<br />With adoption comes combinatorial challenges to correctness. Multiple
dispatch -- Julia's key mechanism for abstraction -- allows many libraries to
compose "out of the box." However, it creates bugs where one library's
requirements do not match what another provides. Typing could address this at
the cost of Julia's flexibility for scripting.
<br />I developed a "best of both worlds" solution: gradual typing for Julia. My
system forms the core of a gradual type system for Julia, laying the foundation
for improving the correctness of Julia programs while not getting in the way of
script writers. My framework allows methods to be individually typed or
untyped, allowing users to write untyped code that interacts with typed library
code and vice versa. Typed methods then get a soundness guarantee that is
robust in the presence of both dynamically typed code and dynamically generated
definitions. I additionally describe protocols, a mechanism for typing
abstraction over concrete implementation that accommodates one common pattern
in Julia libraries, and describe its implementation into my typed Julia
framework.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16867" title="Abstract">arXiv:2310.16867</a> [<a href="/pdf/2310.16867" title="Download PDF">pdf</a>, <a href="/format/2310.16867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis  Using Generative Data-Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saadatinia%2C+M">Mehrshad Saadatinia</a>, 
<a href="/search/cs?searchtype=author&query=Salimi-Badr%2C+A">Armin Salimi-Badr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In this study, we leverage a deep learning-based method for the automatic
diagnosis of schizophrenia using EEG brain recordings. This approach utilizes
generative data augmentation, a powerful technique that enhances the accuracy
of the diagnosis. To enable the utilization of time-frequency features,
spectrograms were extracted from the raw signals. After exploring several
neural network architectural setups, a proper convolutional neural network
(CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN
with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two
different synthetic datasets were generated in order to augment the initial
dataset and address the over-fitting issue. The augmented dataset using VAE
achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a
lower loss value as well as a faster convergence. Finally, we addressed the
lack of trust in black-box models using the Local Interpretable Model-agnostic
Explanations (LIME) algorithm to determine the most important superpixels
(frequencies) in the diagnosis process.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16870" title="Abstract">arXiv:2310.16870</a> [<a href="/pdf/2310.16870" title="Download PDF">pdf</a>, <a href="/format/2310.16870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MACP: Efficient Model Adaptation for Cooperative Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunsheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Juanwu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Can Cui</a>, 
<a href="/search/cs?searchtype=author&query=ZHao%2C+S">Sicheng ZHao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wenqian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziran Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV 2024, 10 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception
capabilities of connected and automated vehicles (CAVs) by enabling information
sharing to "see through the occlusions", resulting in significant performance
improvements. However, developing and training complex multi-agent perception
models from scratch can be expensive and unnecessary when existing single-agent
models show remarkable generalization capabilities. In this paper, we propose a
new framework termed MACP, which equips a single-agent pre-trained model with
cooperation capabilities. We approach this objective by identifying the key
challenges of shifting from single-agent to cooperative settings, adapting the
model by freezing most of its parameters and adding a few lightweight modules.
We demonstrate in our experiments that the proposed framework can effectively
utilize cooperative observations and outperform other state-of-the-art
approaches in both simulated and real-world cooperative perception benchmarks
while requiring substantially fewer tunable parameters with reduced
communication costs. Our source code is available at
https://github.com/PurdueDigitalTwin/MACP.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16897" title="Abstract">arXiv:2310.16897</a> [<a href="/pdf/2310.16897" title="Download PDF">pdf</a>, <a href="/format/2310.16897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Helland%2C+S">Solveig Helland</a>, 
<a href="/search/cs?searchtype=author&query=Gavagnin%2C+E">Elena Gavagnin</a>, 
<a href="/search/cs?searchtype=author&query=de+Spindler%2C+A">Alexandre de Spindler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the Swiss Text Analytics Conference 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The growing capabilities of transformer models pave the way for solving
increasingly complex NLP tasks. A key to supporting application-specific
requirements is the ability to fine-tune. However, compiling a fine-tuning
dataset tailored to complex tasks is tedious and results in large datasets,
limiting the ability to control transformer output. We present an approach in
which complex tasks are divided into simpler subtasks. Multiple transformer
models are fine-tuned to one subtask each, and lined up to accomplish the
complex task. This simplifies the compilation of fine-tuning datasets and
increases overall controllability. Using the example of reducing gender bias as
a complex task, we demonstrate our approach and show that it performs better
than using a single model.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16898" title="Abstract">arXiv:2310.16898</a> [<a href="/pdf/2310.16898" title="Download PDF">pdf</a>, <a href="/format/2310.16898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited  Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yinan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiuwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yansong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jie%2C+Z">Zhou Jie</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiwen Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Due to the high price and heavy energy consumption of GPUs, deploying deep
models on IoT devices such as microcontrollers makes significant contributions
for ecological AI. Conventional methods successfully enable convolutional
neural network inference of high resolution images on microcontrollers, while
the framework for vision transformers that achieve the state-of-the-art
performance in many vision applications still remains unexplored. In this
paper, we propose a hardware-algorithm co-optimizations method called MCUFormer
to deploy vision transformers on microcontrollers with extremely limited
memory, where we jointly design transformer architecture and construct the
inference operator library to fit the memory resource constraint. More
specifically, we generalize the one-shot network architecture search (NAS) to
discover the optimal architecture with highest task performance given the
memory budget from the microcontrollers, where we enlarge the existing search
space of vision transformers by considering the low-rank decomposition
dimensions and patch resolution for memory reduction. For the construction of
the inference operator library of vision transformers, we schedule the memory
buffer during inference through operator integration, patch embedding
decomposition, and token overwriting, allowing the memory buffer to be fully
utilized to adapt to the forward pass of the vision transformer. Experimental
results demonstrate that our MCUFormer achieves 73.62\% top-1 accuracy on
ImageNet for image classification with 320KB memory on STM32F746
microcontroller. Code is available at https://github.com/liangyn22/MCUFormer.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16906" title="Abstract">arXiv:2310.16906</a> [<a href="/pdf/2310.16906" title="Download PDF">pdf</a>, <a href="/format/2310.16906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensitivity Analysis of the Information Gain in Infinite-Dimensional  Bayesian Linear Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chowdhary%2C+A">Abhijit Chowdhary</a>, 
<a href="/search/math?searchtype=author&query=Tong%2C+S">Shanyin Tong</a>, 
<a href="/search/math?searchtype=author&query=Stadler%2C+G">Georg Stadler</a>, 
<a href="/search/math?searchtype=author&query=Alexanderian%2C+A">Alen Alexanderian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We study the sensitivity of infinite-dimensional Bayesian linear inverse
problems governed by partial differential equations (PDEs) with respect to
modeling uncertainties. In particular, we consider derivative-based sensitivity
analysis of the information gain, as measured by the Kullback-Leibler
divergence from the posterior to the prior distribution. To facilitate this, we
develop a fast and accurate method for computing derivatives of the information
gain with respect to auxiliary model parameters. Our approach combines low-rank
approximations, adjoint-based eigenvalue sensitivity analysis, and post-optimal
sensitivity analysis. The proposed approach also paves way for global
sensitivity analysis by computing derivative-based global sensitivity measures.
We illustrate different aspects of the proposed approach using an inverse
problem governed by a scalar linear elliptic PDE, and an inverse problem
governed by the three-dimensional equations of linear elasticity, which is
motivated by the inversion of the fault-slip field after an earthquake.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16909" title="Abstract">arXiv:2310.16909</a> [<a href="/pdf/2310.16909" title="Download PDF">pdf</a>, <a href="/format/2310.16909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuromorphic weighted sum with magnetic skyrmions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+C%C3%A2mara+Santa+Clara+Gomes%2C+T">Tristan da C&#xe2;mara Santa Clara Gomes</a>, 
<a href="/search/cs?searchtype=author&query=Sassi%2C+Y">Yanis Sassi</a>, 
<a href="/search/cs?searchtype=author&query=Sanz-Hern%C3%A1ndez%2C+D">D&#xe9;dalo Sanz-Hern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Krishnia%2C+S">Sachin Krishnia</a>, 
<a href="/search/cs?searchtype=author&query=Collin%2C+S">Sophie Collin</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+M">Marie-Blandine Martin</a>, 
<a href="/search/cs?searchtype=author&query=Seneor%2C+P">Pierre Seneor</a>, 
<a href="/search/cs?searchtype=author&query=Cros%2C+V">Vincent Cros</a>, 
<a href="/search/cs?searchtype=author&query=Grollier%2C+J">Julie Grollier</a>, 
<a href="/search/cs?searchtype=author&query=Reyren%2C+N">Nicolas Reyren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">Integrating magnetic skyrmion properties into neuromorphic computing promises
advancements in hardware efficiency and computational power. However, a
scalable implementation of the weighted sum of neuron signals, a core operation
in neural networks, has yet to be demonstrated. In this study, we exploit the
non-volatile and particle-like characteristics of magnetic skyrmions, akin to
synaptic vesicles and neurotransmitters, to perform this weighted sum operation
in a compact, biologically-inspired manner. To this aim, skyrmions are
electrically generated in numbers proportional to the input with an efficiency
given by a non-volatile weight. These chiral particles are then directed using
localized current injections to a location where their presence is quantified
through non-perturbative electrical measurements. Our experimental
demonstration, currently with two inputs, can be scaled to accommodate multiple
inputs and outputs using a crossbar array design, potentially nearing the
energy efficiency observed in biological systems.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16917" title="Abstract">arXiv:2310.16917</a> [<a href="/pdf/2310.16917" title="Download PDF">pdf</a>, <a href="/format/2310.16917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MimicTouch: Learning Human&#x27;s Control Strategy with Multi-Modal Tactile  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kelin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yunhai Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Matthew Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Ye Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at CoRL 2023 Deployable Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In robotics and artificial intelligence, the integration of tactile
processing is becoming increasingly pivotal, especially in learning to execute
intricate tasks like alignment and insertion. However, existing works focusing
on tactile methods for insertion tasks predominantly rely on robot
teleoperation data and reinforcement learning, which do not utilize the rich
insights provided by human's control strategy guided by tactile feedback. For
utilizing human sensations, methodologies related to learning from humans
predominantly leverage visual feedback, often overlooking the invaluable
tactile feedback that humans inherently employ to finish complex manipulations.
Addressing this gap, we introduce "MimicTouch", a novel framework that mimics
human's tactile-guided control strategy. In this framework, we initially
collect multi-modal tactile datasets from human demonstrators, incorporating
human tactile-guided control strategies for task completion. The subsequent
step involves instructing robots through imitation learning using multi-modal
sensor data and retargeted human motions. To further mitigate the embodiment
gap between humans and robots, we employ online residual reinforcement learning
on the physical robot. Through comprehensive experiments, we validate the
safety of MimicTouch in transferring a latent policy learned through imitation
learning from human to robot. This ongoing work will pave the way for a broader
spectrum of tactile-guided robotic applications.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16919" title="Abstract">arXiv:2310.16919</a> [<a href="/pdf/2310.16919" title="Download PDF">pdf</a>, <a href="/format/2310.16919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fei%2C+J">Jianwei Fei</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Z">Zhihua Xia</a>, 
<a href="/search/cs?searchtype=author&query=Tondi%2C+B">Benedetta Tondi</a>, 
<a href="/search/cs?searchtype=author&query=Barni%2C+M">Mauro Barni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose a novel multi-bit box-free watermarking method for the protection
of Intellectual Property Rights (IPR) of GANs with improved robustness against
white-box attacks like fine-tuning, pruning, quantization, and surrogate model
attacks. The watermark is embedded by adding an extra watermarking loss term
during GAN training, ensuring that the images generated by the GAN contain an
invisible watermark that can be retrieved by a pre-trained watermark decoder.
In order to improve the robustness against white-box model-level attacks, we
make sure that the model converges to a wide flat minimum of the watermarking
loss term, in such a way that any modification of the model parameters does not
erase the watermark. To do so, we add random noise vectors to the parameters of
the generator and require that the watermarking loss term is as invariant as
possible with respect to the presence of noise. This procedure forces the
generator to converge to a wide flat minimum of the watermarking loss. The
proposed method is architectureand dataset-agnostic, thus being applicable to
many different generation tasks and models, as well as to CNN-based image
processing architectures. We present the results of extensive experiments
showing that the presence of the watermark has a negligible impact on the
quality of the generated images, and proving the superior robustness of the
watermark against model modification and surrogate model attacks.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16924" title="Abstract">arXiv:2310.16924</a> [<a href="/pdf/2310.16924" title="Download PDF">pdf</a>, <a href="/format/2310.16924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physician Detection of Clinical Harm in Machine Translation: Quality  Estimation Aids in Reliance and Backtranslation Identifies Critical Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehandru%2C+N">Nikita Mehandru</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Sweta Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yimin Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Khoong%2C+E+C">Elaine C Khoong</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Ge Gao</a>, 
<a href="/search/cs?searchtype=author&query=Carpuat%2C+M">Marine Carpuat</a>, 
<a href="/search/cs?searchtype=author&query=Salehi%2C+N">Niloufar Salehi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">A major challenge in the practical use of Machine Translation (MT) is that
users lack guidance to make informed decisions about when to rely on outputs.
Progress in quality estimation research provides techniques to automatically
assess MT quality, but these techniques have primarily been evaluated in vitro
by comparison against human judgments outside of a specific context of use.
This paper evaluates quality estimation feedback in vivo with a human study
simulating decision-making in high-stakes medical settings. Using Emergency
Department discharge instructions, we study how interventions based on quality
estimation versus backtranslation assist physicians in deciding whether to show
MT outputs to a patient. We find that quality estimation improves appropriate
reliance on MT, but backtranslation helps physicians detect more clinically
harmful errors that QE alone often misses.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16931" title="Abstract">arXiv:2310.16931</a> [<a href="/pdf/2310.16931" title="Download PDF">pdf</a>, <a href="/format/2310.16931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CL-MASR: A Continual Learning Benchmark for Multilingual ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Della+Libera%2C+L">Luca Della Libera</a>, 
<a href="/search/cs?searchtype=author&query=Mousavi%2C+P">Pooneh Mousavi</a>, 
<a href="/search/cs?searchtype=author&query=Zaiem%2C+S">Salah Zaiem</a>, 
<a href="/search/cs?searchtype=author&query=Subakan%2C+C">Cem Subakan</a>, 
<a href="/search/cs?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Modern multilingual automatic speech recognition (ASR) systems like Whisper
have made it possible to transcribe audio in multiple languages with a single
model. However, current state-of-the-art ASR models are typically evaluated on
individual languages or in a multi-task setting, overlooking the challenge of
continually learning new languages. There is insufficient research on how to
add new languages without losing valuable information from previous data.
Furthermore, existing continual learning benchmarks focus mostly on vision and
language tasks, leaving continual learning for multilingual ASR largely
unexplored. To bridge this gap, we propose CL-MASR, a benchmark designed for
studying multilingual ASR in a continual learning setting. CL-MASR provides a
diverse set of continual learning methods implemented on top of large-scale
pretrained ASR models, along with common metrics to assess the effectiveness of
learning new languages while addressing the issue of catastrophic forgetting.
To the best of our knowledge, CL-MASR is the first continual learning benchmark
for the multilingual ASR task. The code is available at
https://github.com/speechbrain/benchmarks.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16936" title="Abstract">arXiv:2310.16936</a> [<a href="/pdf/2310.16936" title="Download PDF">pdf</a>, <a href="/format/2310.16936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnosing Alzheimer&#x27;s Disease using Early-Late Multimodal Data Fusion  with Jacobian Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+Y">Yasmine Mustafa</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tie Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in Proceedings of 2023 IEEE Healthcom, December 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Alzheimer's disease (AD) is a prevalent and debilitating neurodegenerative
disorder impacting a large aging population. Detecting AD in all its
presymptomatic and symptomatic stages is crucial for early intervention and
treatment. An active research direction is to explore machine learning methods
that harness multimodal data fusion to outperform human inspection of medical
scans. However, existing multimodal fusion models have limitations, including
redundant computation, complex architecture, and simplistic handling of missing
data. Moreover, the preprocessing pipelines of medical scans remain
inadequately detailed and are seldom optimized for individual subjects. In this
paper, we propose an efficient early-late fusion (ELF) approach, which
leverages a convolutional neural network for automated feature extraction and
random forests for their competitive performance on small datasets.
Additionally, we introduce a robust preprocessing pipeline that adapts to the
unique characteristics of individual subjects and makes use of whole brain
images rather than slices or patches. Moreover, to tackle the challenge of
detecting subtle changes in brain volume, we transform images into the Jacobian
domain (JD) to enhance both accuracy and robustness in our classification.
Using MRI and CT images from the OASIS-3 dataset, our experiments demonstrate
the effectiveness of the ELF approach in classifying AD into four stages with
an accuracy of 97.19%.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16937" title="Abstract">arXiv:2310.16937</a> [<a href="/pdf/2310.16937" title="Download PDF">pdf</a>, <a href="/format/2310.16937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Transfers over Several Programming Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baltaji%2C+R">Razan Baltaji</a>, 
<a href="/search/cs?searchtype=author&query=Pujar%2C+S">Saurabh Pujar</a>, 
<a href="/search/cs?searchtype=author&query=Mandel%2C+L">Louis Mandel</a>, 
<a href="/search/cs?searchtype=author&query=Hirzel%2C+M">Martin Hirzel</a>, 
<a href="/search/cs?searchtype=author&query=Buratti%2C+L">Luca Buratti</a>, 
<a href="/search/cs?searchtype=author&query=Varshney%2C+L">Lav Varshney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have recently become remarkably good at
improving developer productivity for high-resource programming languages. These
models use two kinds of data: large amounts of unlabeled code samples for
pretraining and relatively smaller amounts of labeled code samples for
fine-tuning or in-context learning. Unfortunately, many programming languages
are low-resource, lacking labeled samples for most tasks and often even lacking
unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or
new languages) miss out on the benefits of LLMs. Cross-lingual transfer
learning uses data from a source language to improve model performance on a
target language. It has been well-studied for natural languages, but has
received little attention for programming languages. This paper reports
extensive experiments on four tasks using a transformer-based LLM and 11 to 41
programming languages to explore the following questions. First, how well
cross-lingual transfer works for a given task across different language pairs.
Second, given a task and target language, how to best choose a source language.
Third, the characteristics of a language pair that are predictive of transfer
performance, and fourth, how that depends on the given task.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16940" title="Abstract">arXiv:2310.16940</a> [<a href="/pdf/2310.16940" title="Download PDF">pdf</a>, <a href="/ps/2310.16940" title="Download PostScript">ps</a>, <a href="/format/2310.16940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal approximation of infinite-dimensional holomorphic functions II:  recovery from i.i.d. pointwise samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Adcock%2C+B">Ben Adcock</a>, 
<a href="/search/math?searchtype=author&query=Dexter%2C+N">Nick Dexter</a>, 
<a href="/search/math?searchtype=author&query=Moraga%2C+S">Sebastian Moraga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Infinite-dimensional, holomorphic functions have been studied in detail over
the last several decades, due to their relevance to parametric differential
equations and computational uncertainty quantification. The approximation of
such functions from finitely many samples is of particular interest, due to the
practical importance of constructing surrogate models to complex mathematical
models of physical processes. In a previous work, [5] we studied the
approximation of so-called Banach-valued,
$(\boldsymbol{b},\varepsilon)$-holomorphic functions on the
infinite-dimensional hypercube $[-1,1]^{\mathbb{N}}$ from $m$ (potentially
adaptive) samples. In particular, we derived lower bounds for the adaptive
$m$-widths for classes of such functions, which showed that certain algebraic
rates of the form $m^{1/2-1/p}$ are the best possible regardless of the
sampling-recovery pair. In this work, we continue this investigation by
focusing on the practical case where the samples are pointwise evaluations
drawn identically and independently from a probability measure. Specifically,
for Hilbert-valued $(\boldsymbol{b},\varepsilon)$-holomorphic functions, we
show that the same rates can be achieved (up to a small polylogarithmic or
algebraic factor) for essentially arbitrary tensor-product Jacobi
(ultraspherical) measures. Our reconstruction maps are based on least squares
and compressed sensing procedures using the corresponding orthonormal Jacobi
polynomials. In doing so, we strengthen and generalize past work that has
derived weaker nonuniform guarantees for the uniform and Chebyshev measures
(and corresponding polynomials) only. We also extend various best $s$-term
polynomial approximation error bounds to arbitrary Jacobi polynomial
expansions. Overall, we demonstrate that i.i.d.\ pointwise samples are
near-optimal for the recovery of infinite-dimensional, holomorphic functions.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16941" title="Abstract">arXiv:2310.16941</a> [<a href="/pdf/2310.16941" title="Download PDF">pdf</a>, <a href="/format/2310.16941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Behavior Discovery Methods for Heterogeneous Swarms of  Limited-Capability Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mattson%2C+C">Connor Mattson</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+J+C">Jeremy C. Clark</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+D+S">Daniel S. Brown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 9 figures, To be published in Proceedings IEEE International Symposium on Multi-Robot &amp; Multi-Agent Systems (MRS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We study the problem of determining the emergent behaviors that are possible
given a functionally heterogeneous swarm of robots with limited capabilities.
Prior work has considered behavior search for homogeneous swarms and proposed
the use of novelty search over either a hand-specified or learned behavior
space followed by clustering to return a taxonomy of emergent behaviors to the
user. In this paper, we seek to better understand the role of novelty search
and the efficacy of using clustering to discover novel emergent behaviors.
Through a large set of experiments and ablations, we analyze the effect of
representations, evolutionary search, and various clustering methods in the
search for novel behaviors in a heterogeneous swarm. Our results indicate that
prior methods fail to discover many interesting behaviors and that an iterative
human-in-the-loop discovery process discovers more behaviors than random
search, swarm chemistry, and automated behavior discovery. The combined
discoveries of our experiments uncover 23 emergent behaviors, 18 of which are
novel discoveries. To the best of our knowledge, these are the first known
emergent behaviors for heterogeneous swarms of computation-free agents. Videos,
code, and appendix are available at the project website:
https://sites.google.com/view/heterogeneous-bd-methods
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16944" title="Abstract">arXiv:2310.16944</a> [<a href="/pdf/2310.16944" title="Download PDF">pdf</a>, <a href="/format/2310.16944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zephyr: Direct Distillation of LM Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tunstall%2C+L">Lewis Tunstall</a>, 
<a href="/search/cs?searchtype=author&query=Beeching%2C+E">Edward Beeching</a>, 
<a href="/search/cs?searchtype=author&query=Lambert%2C+N">Nathan Lambert</a>, 
<a href="/search/cs?searchtype=author&query=Rajani%2C+N">Nazneen Rajani</a>, 
<a href="/search/cs?searchtype=author&query=Rasul%2C+K">Kashif Rasul</a>, 
<a href="/search/cs?searchtype=author&query=Belkada%2C+Y">Younes Belkada</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shengyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=von+Werra%2C+L">Leandro von Werra</a>, 
<a href="/search/cs?searchtype=author&query=Fourrier%2C+C">Cl&#xe9;mentine Fourrier</a>, 
<a href="/search/cs?searchtype=author&query=Habib%2C+N">Nathan Habib</a>, 
<a href="/search/cs?searchtype=author&query=Sarrazin%2C+N">Nathan Sarrazin</a>, 
<a href="/search/cs?searchtype=author&query=Sanseviero%2C+O">Omar Sanseviero</a>, 
<a href="/search/cs?searchtype=author&query=Rush%2C+A+M">Alexander M. Rush</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+T">Thomas Wolf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We aim to produce a smaller language model that is aligned to user intent.
Previous research has shown that applying distilled supervised fine-tuning
(dSFT) on larger models significantly improves task accuracy; however, these
models are unaligned, i.e. they do not respond well to natural prompts. To
distill this property, we experiment with the use of preference data from AI
Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model,
we apply distilled direct preference optimization (dDPO) to learn a chat model
with significantly improved intent alignment. The approach requires only a few
hours of training without any additional sampling during fine-tuning. The final
result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B
parameter models, and requires no human annotation. In particular, results on
MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access
RLHF-based model. Code, models, data, and tutorials for the system are
available at https://github.com/huggingface/alignment-handbook.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16946" title="Abstract">arXiv:2310.16946</a> [<a href="/pdf/2310.16946" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How does Module Tracking for Agrivoltaics Differ from Standard  Photovoltaics? Performance &amp; Technoeconomic Implications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alam%2C+H">Habeel Alam</a>, 
<a href="/search/eess?searchtype=author&query=Butt%2C+N+Z">Nauman Zafar Butt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript has been formally submitted to the "Applied Energy" journal and is presently in the process of peer review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Spatial-temporal sharing of sunlight between solar modules and crops needs to
be designed optimally in agrivoltaics (AV). For AV with fixed module tilts, the
sunlight balance is governed through the spatial density and elevation of the
modules which cannot be manipulated after the installation. For flexible
food-energy balancing across various seasons and crop rotations, modules with
single or dual axis mobility can be best suitable. AV tracking must be geared
towards ensuring a desired sunlight balance that may depend on many factors
including the crop type, module array density, socio-economic factors, and
local policies. Here, we explore single axis customized tracking (CT) for the
mobile AV using a techno-economic model that incorporates design parameters
including crop's shade sensitivity, module to land area ratio, and module
types, as well as the economic parameters including soft and hardware costs for
modules, feed-in-tariff, and crop income. CT is implemented through standard
tracking that tracks the sun around noon hours and its orthogonal, i.e.,
anti-tracking around sunrise and sunset. We evaluate the optimal CT schemes
that can maximize economic performance while ensuring the desired food-energy
yield thresholds. Economic feasibility for AV is evaluated in terms of the
ratio (ppr) of the price for the module system customizations to the
performance benefit due to the crop income. A case study for Punjab, Pakistan
shows that CT schemes for moderate shade sensitive crops and typically dense AV
module arrays can require 30 to 40 percent increase in the reference FIT to
ensure the food-energy yield threshold of 80 percent relative to standalone
food-energy farms for high and low value crops, respectively. CT schemes for a
lower crop yield threshold of 70 percent require the corresponding increase in
FIT to 10 to 20 percent, respectively.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16951" title="Abstract">arXiv:2310.16951</a> [<a href="/pdf/2310.16951" title="Download PDF">pdf</a>, <a href="/format/2310.16951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Teenager&#x27;s Problem: Efficient Garment Decluttering With Grasp  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adler%2C+A">Aviv Adler</a> (1), 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+A">Ayah Ahmad</a> (1), 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengyin Wang</a> (2), 
<a href="/search/cs?searchtype=author&query=Agboh%2C+W+C">Wisdom C. Agboh</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Llontop%2C+E">Edith Llontop</a> (1), 
<a href="/search/cs?searchtype=author&query=Qiu%2C+T">Tianshuang Qiu</a> (1), 
<a href="/search/cs?searchtype=author&query=Ichnowski%2C+J">Jeffrey Ichnowski</a> (3), 
<a href="/search/cs?searchtype=author&query=Dogar%2C+M">Mehmet Dogar</a> (2), 
<a href="/search/cs?searchtype=author&query=Kollar%2C+T">Thomas Kollar</a> (4), 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Richard Cheng</a> (4), 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+K">Ken Goldberg</a> (1) ((1) AUTOLab at the University of California, Berkeley, (2) University of Leeds, (3) Carnegie Mellon University, (4) Toyota Research Institute)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper addresses the ''Teenager's Problem'': efficiently removing
scattered garments from a planar surface. As grasping and transporting
individual garments is highly inefficient, we propose analytical policies to
select grasp locations for multiple garments using an overhead camera. Two
classes of methods are considered: depth-based, which use overhead depth data
to find efficient grasps, and segment-based, which use segmentation on the RGB
overhead image (without requiring any depth data); grasp efficiency is measured
by Objects per Transport, which denotes the average number of objects removed
per trip to the laundry basket. Experiments suggest that both depth- and
segment-based methods easily reduce Objects per Transport (OpT) by $20\%$;
furthermore, these approaches complement each other, with combined hybrid
methods yielding improvements of $34\%$. Finally, a method employing
consolidation (with segmentation) is considered, which manipulates the garments
on the work surface to increase OpT; this yields an improvement of $67\%$ over
the baseline, though at a cost of additional physical actions.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16955" title="Abstract">arXiv:2310.16955</a> [<a href="/pdf/2310.16955" title="Download PDF">pdf</a>, <a href="/format/2310.16955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Break it, Imitate it, Fix it: Robustness by Generating Human-Like  Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Aradhana Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Balashankar%2C+A">Ananth Balashankar</a>, 
<a href="/search/cs?searchtype=author&query=Beirami%2C+A">Ahmad Beirami</a>, 
<a href="/search/cs?searchtype=author&query=Avrahami%2C+T">Thi Avrahami</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jilin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Beutel%2C+A">Alex Beutel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Real-world natural language processing systems need to be robust to human
adversaries. Collecting examples of human adversaries for training is an
effective but expensive solution. On the other hand, training on synthetic
attacks with small perturbations - such as word-substitution - does not
actually improve robustness to human adversaries. In this paper, we propose an
adversarial training framework that uses limited human adversarial examples to
generate more useful adversarial examples at scale. We demonstrate the
advantages of this system on the ANLI and hate speech detection benchmark
datasets - both collected via an iterative, adversarial
human-and-model-in-the-loop procedure. Compared to training only on observed
human attacks, also training on our synthetic adversarial examples improves
model robustness to future rounds. In ANLI, we see accuracy gains on the
current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of
human generated attacks (32.5%$\,\to\,$43.4%, and 29.4%$\,\to\,$40.2%). In hate
speech detection, we see AUC gains on current attacks (0.76 $\to$ 0.84) and a
future round (0.77 $\to$ 0.79). Attacks from methods that do not learn the
distribution of existing human adversaries, meanwhile, degrade robustness.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16956" title="Abstract">arXiv:2310.16956</a> [<a href="/pdf/2310.16956" title="Download PDF">pdf</a>, <a href="/ps/2310.16956" title="Download PostScript">ps</a>, <a href="/format/2310.16956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Datastore Design for Analysis of Police Broadcast Audio at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+A">Ayah Ahmad</a> (1), 
<a href="/search/cs?searchtype=author&query=Graziul%2C+C">Christopher Graziul</a> (2), 
<a href="/search/cs?searchtype=author&query=Spencer%2C+M+B">Margaret Beale Spencer</a> (2) ((1) University of California, Berkeley, (2) University of Chicago)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Databases (cs.DB)

</div>
<p class="mathjax">With policing coming under greater scrutiny in recent years, researchers have
begun to more thoroughly study the effects of contact between police and
minority communities. Despite data archives of hundreds of thousands of
recorded Broadcast Police Communications (BPC) being openly available to the
public, a closer look at a large-scale analysis of the language of policing has
remained largely unexplored. While this research is critical in understanding a
"pre-reflective" notion of policing, the large quantity of data presents
numerous challenges in its organization and analysis.
<br />In this paper, we describe preliminary work towards enabling Speech Emotion
Recognition (SER) in an analysis of the Chicago Police Department's (CPD) BPC
by demonstrating the pipelined creation of a datastore to enable a multimodal
analysis of composed raw audio files.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16958" title="Abstract">arXiv:2310.16958</a> [<a href="/pdf/2310.16958" title="Download PDF">pdf</a>, <a href="/format/2310.16958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transferring a molecular foundation model for polymer property  predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kearney%2C+L">Logan Kearney</a>, 
<a href="/search/cs?searchtype=author&query=Bhowmik%2C+D">Debsindhu Bhowmik</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+Z">Zachary Fox</a>, 
<a href="/search/cs?searchtype=author&query=Naskar%2C+A+K">Amit K. Naskar</a>, 
<a href="/search/cs?searchtype=author&query=Gounley%2C+J">John Gounley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Transformer-based large language models have remarkable potential to
accelerate design optimization for applications such as drug development and
materials discovery. Self-supervised pretraining of transformer models requires
large-scale datasets, which are often sparsely populated in topical areas such
as polymer science. State-of-the-art approaches for polymers conduct data
augmentation to generate additional samples but unavoidably incurs extra
computational costs. In contrast, large-scale open-source datasets are
available for small molecules and provide a potential solution to data scarcity
through transfer learning. In this work, we show that using transformers
pretrained on small molecules and fine-tuned on polymer properties achieve
comparable accuracy to those trained on augmented polymer datasets for a series
of benchmark prediction tasks.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16959" title="Abstract">arXiv:2310.16959</a> [<a href="/pdf/2310.16959" title="Download PDF">pdf</a>, <a href="/format/2310.16959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Few-shot Generalization of Safety Classifiers via Data  Augmented Parameter-Efficient Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balashankar%2C+A">Ananth Balashankar</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Aradhana Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Beirami%2C+A">Ahmad Beirami</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yao Qin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jilin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Beutel%2C+A">Alex Beutel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As large language models (LLMs) are widely adopted, new safety issues and
policies emerge, to which existing safety classifiers do not generalize well.
If we have only observed a few examples of violations of a new safety rule, how
can we build a classifier to detect violations? In this paper, we study the
novel setting of domain-generalized few-shot learning for LLM-based text safety
classifiers. Unlike prior few-shot work, these new safety issues can be hard to
uncover and we do not get to choose the few examples. We demonstrate that
existing few-shot techniques do not perform well in this setting, and rather we
propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting
training data based on similar examples in prior existing rules. We empirically
show that our approach of similarity-based data-augmentation + prompt-tuning
(DAPT) consistently outperforms baselines that either do not rely on data
augmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral
judgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule
is loosely correlated with existing ones.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16960" title="Abstract">arXiv:2310.16960</a> [<a href="/pdf/2310.16960" title="Download PDF">pdf</a>, <a href="/format/2310.16960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privately Aligning Language Models with Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Inan%2C+H+A">Huseyin A. Inan</a>, 
<a href="/search/cs?searchtype=author&query=Backurs%2C+A">Arturs Backurs</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+V">Varun Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+J">Janardhan Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+R">Robert Sim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Positioned between pre-training and user deployment, aligning large language
models (LLMs) through reinforcement learning (RL) has emerged as a prevailing
strategy for training instruction following-models such as ChatGPT. In this
work, we initiate the study of privacy-preserving alignment of LLMs through
Differential Privacy (DP) in conjunction with RL. Following the influential
work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment
via RL without human in the loop (e.g., positive review generation) and (ii)
alignment via RL from human feedback (RLHF) (e.g., summarization in a
human-preferred way). We give a new DP framework to achieve alignment via RL,
and prove its correctness. Our experimental results validate the effectiveness
of our approach, offering competitive utility while ensuring strong privacy
protections.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16961" title="Abstract">arXiv:2310.16961</a> [<a href="/pdf/2310.16961" title="Download PDF">pdf</a>, <a href="/format/2310.16961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Distributed Compressor Discovers Binning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozyilkan%2C+E">Ezgi Ozyilkan</a>, 
<a href="/search/cs?searchtype=author&query=Ball%C3%A9%2C+J">Johannes Ball&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Erkip%2C+E">Elza Erkip</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> draft of a journal version of our previous ISIT 2023 paper (available at: <a href="/abs/2305.04380">arXiv:2305.04380</a>). arXiv admin note: substantial text overlap with <a href="/abs/2305.04380">arXiv:2305.04380</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We consider lossy compression of an information source when the decoder has
lossless access to a correlated one. This setup, also known as the Wyner-Ziv
problem, is a special case of distributed source coding. To this day, practical
approaches for the Wyner-Ziv problem have neither been fully developed nor
heavily investigated. We propose a data-driven method based on machine learning
that leverages the universal function approximation capability of artificial
neural networks. We find that our neural network-based compression scheme,
based on variational vector quantization, recovers some principles of the
optimum theoretical solution of the Wyner-Ziv setup, such as binning in the
source space as well as optimal combination of the quantization index and side
information, for exemplary sources. These behaviors emerge although no
structure exploiting knowledge of the source distributions was imposed. Binning
is a widely used tool in information theoretic proofs and methods, and to our
knowledge, this is the first time it has been explicitly observed to emerge
from data-driven learning.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16964" title="Abstract">arXiv:2310.16964</a> [<a href="/pdf/2310.16964" title="Download PDF">pdf</a>, <a href="/format/2310.16964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lango%2C+M">Mateusz Lango</a>, 
<a href="/search/cs?searchtype=author&query=Du%C5%A1ek%2C+O">Ond&#x159;ej Du&#x161;ek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Hallucination of text ungrounded in the input is a well-known problem in
neural data-to-text generation. Many methods have been proposed to mitigate it,
but they typically require altering model architecture or collecting additional
data, and thus cannot be easily applied to an existing model. In this paper, we
explore a new way to mitigate hallucinations by combining the probabilistic
output of a generator language model (LM) with the output of a special "text
critic" classifier, which guides the generation by assessing the match between
the input data and the text generated so far. Our method does not need any
changes to the underlying LM's architecture or training procedure and can thus
be combined with any model and decoding operating on word probabilities. The
critic does not need any additional training data, using the base LM's training
data and synthetic negative examples. Our experimental results show that our
method improves over the baseline on the WebNLG and OpenDialKG benchmarks.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16968" title="Abstract">arXiv:2310.16968</a> [<a href="/pdf/2310.16968" title="Download PDF">pdf</a>, <a href="/format/2310.16968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Social Structures from Contemporary Literary Fiction using  Character Interaction Graph -- Half Century Chronology of Influential Bengali  Writers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripto%2C+N+I">Nafis Irtiza Tripto</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+M+E">Mohammed Eunus Ali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 11 figures, 6 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Social structures and real-world incidents often influence contemporary
literary fiction. Existing research in literary fiction analysis explains these
real-world phenomena through the manual critical analysis of stories.
Conventional Natural Language Processing (NLP) methodologies, including
sentiment analysis, narrative summarization, and topic modeling, have
demonstrated substantial efficacy in analyzing and identifying similarities
within fictional works. However, the intricate dynamics of character
interactions within fiction necessitate a more nuanced approach that
incorporates visualization techniques. Character interaction graphs (or
networks) emerge as a highly suitable means for visualization and information
retrieval from the realm of fiction. Therefore, we leverage character
interaction graphs with NLP-derived features to explore a diverse spectrum of
societal inquiries about contemporary culture's impact on the landscape of
literary fiction. Our study involves constructing character interaction graphs
from fiction, extracting relevant graph features, and exploiting these features
to resolve various real-life queries. Experimental evaluation of influential
Bengali fiction over half a century demonstrates that character interaction
graphs can be highly effective in specific assessments and information
retrieval from literary fiction. Our data and codebase are available at
https://cutt.ly/fbMgGEM
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16969" title="Abstract">arXiv:2310.16969</a> [<a href="/pdf/2310.16969" title="Download PDF">pdf</a>, <a href="/format/2310.16969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security Patchworking in Lebanon: Infrastructuring Across Failing  Infrastructures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McClearn%2C+J">Jessica McClearn</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+R+B">Rikke Bjerg Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Talhouk%2C+R">Reem Talhouk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at ACM Conference On Computer-Supported Cooperative Work And Social Computing (CSCW) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In this paper we bring to light the infrastructuring work carried out by
people in Lebanon to establish and maintain everyday security in response to
multiple simultaneously failing infrastructures. We do so through interviews
with 13 participants from 12 digital and human rights organisations and two
weeks of ethnographically informed fieldwork in Beirut, Lebanon, in July 2022.
Through our analysis we develop the notion of security patchworking that makes
visible the infrastructuring work necessitated to secure basic needs such as
electricity provision, identity authentication and financial resources. Such
practices are rooted in differing mechanisms of protection that often result in
new forms of insecurity. We discuss the implications for CSCW and HCI
researchers and point to security patchworking as a lens to be used when
designing technologies to support infrastructuring, while advocating for
collaborative work across CSCW and security research.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16972" title="Abstract">arXiv:2310.16972</a> [<a href="/pdf/2310.16972" title="Download PDF">pdf</a>, <a href="/format/2310.16972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Word2vec Graph Model for Author Attribution and Genre Detection in  Literary Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripto%2C+N+I">Nafis Irtiza Tripto</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+M+E">Mohammed Eunus Ali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Analyzing the writing styles of authors and articles is a key to supporting
various literary analyses such as author attribution and genre detection. Over
the years, rich sets of features that include stylometry, bag-of-words, n-grams
have been widely used to perform such analysis. However, the effectiveness of
these features largely depends on the linguistic aspects of a particular
language and datasets specific characteristics. Consequently, techniques based
on these feature sets cannot give desired results across domains. In this
paper, we propose a novel Word2vec graph based modeling of a document that can
rightly capture both context and style of the document. By using these Word2vec
graph based features, we perform classification to perform author attribution
and genre detection tasks. Our detailed experimental study with a comprehensive
set of literary writings shows the effectiveness of this method over
traditional feature based approaches. Our code and data are publicly available
at https://cutt.ly/svLjSgk
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16976" title="Abstract">arXiv:2310.16976</a> [<a href="/pdf/2310.16976" title="Download PDF">pdf</a>, <a href="/format/2310.16976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Interplay between Social Welfare and Tractability of Equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anagnostides%2C+I">Ioannis Anagnostides</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Computational tractability and social welfare (aka. efficiency) of equilibria
are two fundamental but in general orthogonal considerations in algorithmic
game theory. Nevertheless, we show that when (approximate) full efficiency can
be guaranteed via a smoothness argument \`a la Roughgarden, Nash equilibria are
approachable under a family of no-regret learning algorithms, thereby enabling
fast and decentralized computation. We leverage this connection to obtain new
convergence results in large games -- wherein the number of players $n \gg 1$
-- under the well-documented property of full efficiency via smoothness in the
limit. Surprisingly, our framework unifies equilibrium computation in disparate
classes of problems including games with vanishing strategic sensitivity and
two-player zero-sum games, illuminating en route an immediate but overlooked
equivalence between smoothness and a well-studied condition in the optimization
literature known as the Minty property. Finally, we establish that a family of
no-regret dynamics attains a welfare bound that improves over the smoothness
framework while at the same time guaranteeing convergence to the set of coarse
correlated equilibria. We show this by employing the clairvoyant mirror descent
algortihm recently introduced by Piliouras et al.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16978" title="Abstract">arXiv:2310.16978</a> [<a href="/pdf/2310.16978" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Significance of Machine Learning in Clinical Disease Diagnosis: A  Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+S+M+A">S M Atikur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Ibtisum%2C+S">Sifat Ibtisum</a>, 
<a href="/search/cs?searchtype=author&query=Bazgir%2C+E">Ehsan Bazgir</a>, 
<a href="/search/cs?searchtype=author&query=Barai%2C+T">Tumpa Barai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Computer Applications 185(36):10-17,
  October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The global need for effective disease diagnosis remains substantial, given
the complexities of various disease mechanisms and diverse patient symptoms. To
tackle these challenges, researchers, physicians, and patients are turning to
machine learning (ML), an artificial intelligence (AI) discipline, to develop
solutions. By leveraging sophisticated ML and AI methods, healthcare
stakeholders gain enhanced diagnostic and treatment capabilities. However,
there is a scarcity of research focused on ML algorithms for enhancing the
accuracy and computational efficiency. This research investigates the capacity
of machine learning algorithms to improve the transmission of heart rate data
in time series healthcare metrics, concentrating particularly on optimizing
accuracy and efficiency. By exploring various ML algorithms used in healthcare
applications, the review presents the latest trends and approaches in ML-based
disease diagnosis (MLBDD). The factors under consideration include the
algorithm utilized, the types of diseases targeted, the data types employed,
the applications, and the evaluation metrics. This review aims to shed light on
the prospects of ML in healthcare, particularly in disease diagnosis. By
analyzing the current literature, the study provides insights into
state-of-the-art methodologies and their performance metrics.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16979" title="Abstract">arXiv:2310.16979</a> [<a href="/pdf/2310.16979" title="Download PDF">pdf</a>, <a href="/format/2310.16979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo  Label Self-Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xingchen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mithun%2C+N+C">Niluthpol Chowdhury Mithun</a>, 
<a href="/search/cs?searchtype=author&query=Rajvanshi%2C+A">Abhinav Rajvanshi</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+H">Han-Pang Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Samarasekera%2C+S">Supun Samarasekera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep learning-based solutions for semantic segmentation suffer from
significant performance degradation when tested on data with different
characteristics than what was used during the training. Adapting the models
using annotated data from the new domain is not always practical. Unsupervised
Domain Adaptation (UDA) approaches are crucial in deploying these models in the
actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ
a teacher-student self-training approach, where a teacher model is used to
generate pseudo-labels for the new data which in turn guide the training
process of the student model. Though this approach has seen a lot of success,
it suffers from the issue of noisy pseudo-labels being propagated in the
training process. To address this issue, we propose an auxiliary pseudo-label
refinement network (PRN) for online refining of the pseudo labels and also
localizing the pixels whose predicted labels are likely to be noisy. Being able
to improve the quality of pseudo labels and select highly reliable ones, PRN
helps self-training of segmentation models to be robust against pseudo label
noise propagation during different stages of adaptation. We evaluate our
approach on benchmark datasets with three different domain shifts, and our
approach consistently performs significantly better than the previous
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16981" title="Abstract">arXiv:2310.16981</a> [<a href="/pdf/2310.16981" title="Download PDF">pdf</a>, <a href="/format/2310.16981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A  Comprehensive Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hansen%2C+L">Lasse Hansen</a>, 
<a href="/search/cs?searchtype=author&query=Seedat%2C+N">Nabeel Seedat</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>, 
<a href="/search/cs?searchtype=author&query=Petrovic%2C+A">Andrija Petrovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at NeurIPS 2023 (Datasets &amp; Benchmarks). *Hansen &amp; Seedat contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Synthetic data serves as an alternative in training machine learning models,
particularly when real-world data is limited or inaccessible. However, ensuring
that synthetic data mirrors the complex nuances of real-world data is a
challenging task. This paper addresses this issue by exploring the potential of
integrating data-centric AI techniques which profile the data to guide the
synthetic data generation process. Moreover, we shed light on the often ignored
consequences of neglecting these data profiles during synthetic data generation
-- despite seemingly high statistical fidelity. Subsequently, we propose a
novel framework to evaluate the integration of data profiles to guide the
creation of more representative synthetic data. In an empirical study, we
evaluate the performance of five state-of-the-art models for tabular data
generation on eleven distinct tabular datasets. The findings offer critical
insights into the successes and limitations of current synthetic data
generation techniques. Finally, we provide practical recommendations for
integrating data-centric insights into the synthetic data generation process,
with a specific focus on classification performance, model selection, and
feature selection. This study aims to reevaluate conventional approaches to
synthetic data generation and promote the application of data-centric AI
techniques in improving the quality and effectiveness of synthetic data.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16983" title="Abstract">arXiv:2310.16983</a> [<a href="/pdf/2310.16983" title="Download PDF">pdf</a>, <a href="/format/2310.16983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WaLiN-GUI: a graphical and auditory tool for neuron-based encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller-Cleve%2C+S+F">Simon F. M&#xfc;ller-Cleve</a>, 
<a href="/search/cs?searchtype=author&query=Quintana%2C+F+M">Fernando M. Quintana</a>, 
<a href="/search/cs?searchtype=author&query=Fra%2C+V">Vittorio Fra</a>, 
<a href="/search/cs?searchtype=author&query=Galindo%2C+P+L">Pedro L. Galindo</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Pe%C3%B1a%2C+F">Fernando Perez-Pe&#xf1;a</a>, 
<a href="/search/cs?searchtype=author&query=Urgese%2C+G">Gianvito Urgese</a>, 
<a href="/search/cs?searchtype=author&query=Bartolozzi%2C+C">Chiara Bartolozzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Neuromorphic computing relies on spike-based, energy-efficient communication,
inherently implying the need for conversion between real-valued (sensory) data
and binary, sparse spiking representation. This is usually accomplished using
the real valued data as current input to a spiking neuron model, and tuning the
neuron's parameters to match a desired, often biologically inspired behaviour.
We developed a tool, the WaLiN-GUI, that supports the investigation of neuron
models and parameter combinations to identify suitable configurations for
neuron-based encoding of sample-based data into spike trains. Due to the
generalized LIF model implemented by default, next to the LIF and Izhikevich
neuron models, many spiking behaviors can be investigated out of the box, thus
offering the possibility of tuning biologically plausible responses to the
input data. The GUI is provided open source and with documentation, being easy
to extend with further neuron models and personalize with data analysis
functions.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16984" title="Abstract">arXiv:2310.16984</a> [<a href="/pdf/2310.16984" title="Download PDF">pdf</a>, <a href="/format/2310.16984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patterns of Student Help-Seeking When Using a Large Language  Model-Powered Programming Assistant
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sheese%2C+B">Brad Sheese</a>, 
<a href="/search/cs?searchtype=author&query=Liffiton%2C+M">Mark Liffiton</a>, 
<a href="/search/cs?searchtype=author&query=Savelka%2C+J">Jaromir Savelka</a>, 
<a href="/search/cs?searchtype=author&query=Denny%2C+P">Paul Denny</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Providing personalized assistance at scale is a long-standing challenge for
computing educators, but a new generation of tools powered by large language
models (LLMs) offers immense promise. Such tools can, in theory, provide
on-demand help in large class settings and be configured with appropriate
guardrails to prevent misuse and mitigate common concerns around learner
over-reliance. However, the deployment of LLM-powered tools in authentic
classroom settings is still rare, and very little is currently known about how
students will use them in practice and what type of help they will seek. To
address this, we examine students' use of an innovative LLM-powered tool that
provides on-demand programming assistance without revealing solutions directly.
We deployed the tool for 12 weeks in an introductory computer and data science
course ($n = 52$), collecting more than 2,500 queries submitted by students
throughout the term. We manually categorized all student queries based on the
type of assistance sought, and we automatically analyzed several additional
query characteristics. We found that most queries requested immediate help with
programming assignments, whereas fewer requests asked for help on related
concepts or for deepening conceptual understanding. Furthermore, students often
provided minimal information to the tool, suggesting this is an area in which
targeted instruction would be beneficial. We also found that students who
achieved more success in the course tended to have used the tool more
frequently overall. Lessons from this research can be leveraged by programming
educators and institutions who plan to augment their teaching with emerging
LLM-powered tools.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16985" title="Abstract">arXiv:2310.16985</a> [<a href="/pdf/2310.16985" title="Download PDF">pdf</a>, <a href="/format/2310.16985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TinyMPC: Model-Predictive Control on Resource-Constrained  Microcontrollers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alavilli%2C+A">Anoushka Alavilli</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khai Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Schoedel%2C+S">Sam Schoedel</a>, 
<a href="/search/cs?searchtype=author&query=Plancher%2C+B">Brian Plancher</a>, 
<a href="/search/cs?searchtype=author&query=Manchester%2C+Z">Zachary Manchester</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First three authors contributed equally and are ordered alphabetically
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">Model-predictive control (MPC) is a powerful tool for controlling highly
dynamic robotic systems subject to complex constraints. However, MPC is
computationally demanding, and is often impractical to implement on small,
resource-constrained robotic platforms. We present TinyMPC, a high-speed MPC
solver with a low memory footprint targeting the microcontrollers common on
small robots. Our approach is based on the alternating direction method of
multipliers (ADMM) and leverages the structure of the MPC problem for
efficiency. We demonstrate TinyMPC both by benchmarking against the
state-of-the-art solver OSQP, achieving nearly an order of magnitude speed
increase, as well as through hardware experiments on a 27 g quadrotor,
demonstrating high-speed trajectory tracking and dynamic obstacle avoidance.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16986" title="Abstract">arXiv:2310.16986</a> [<a href="/pdf/2310.16986" title="Download PDF">pdf</a>, <a href="/format/2310.16986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Integral Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gala%2C+G">Gennaro Gala</a>, 
<a href="/search/cs?searchtype=author&query=de+Campos%2C+C">Cassio de Campos</a>, 
<a href="/search/cs?searchtype=author&query=Peharz%2C+R">Robert Peharz</a>, 
<a href="/search/cs?searchtype=author&query=Vergari%2C+A">Antonio Vergari</a>, 
<a href="/search/cs?searchtype=author&query=Quaeghebeur%2C+E">Erik Quaeghebeur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Continuous latent variables (LVs) are a key ingredient of many generative
models, as they allow modelling expressive mixtures with an uncountable number
of components. In contrast, probabilistic circuits (PCs) are hierarchical
discrete mixtures represented as computational graphs composed of input, sum
and product units. Unlike continuous LV models, PCs provide tractable inference
but are limited to discrete LVs with categorical (i.e. unordered) states. We
bridge these model classes by introducing probabilistic integral circuits
(PICs), a new language of computational graphs that extends PCs with integral
units representing continuous LVs. In the first place, PICs are symbolic
computational graphs and are fully tractable in simple cases where analytical
integration is possible. In practice, we parameterise PICs with light-weight
neural nets delivering an intractable hierarchical continuous mixture that can
be approximated arbitrarily well with large PCs using numerical quadrature. On
several distribution estimation benchmarks, we show that such PIC-approximating
PCs systematically outperform PCs commonly learned via expectation-maximization
or SGD.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16990" title="Abstract">arXiv:2310.16990</a> [<a href="/pdf/2310.16990" title="Download PDF">pdf</a>, <a href="/format/2310.16990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STEER: Semantic Turn Extension-Expansion Recognition for Voice  Assistants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L+L">Leon Liyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiarui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Moniz%2C+J+R+A">Joel Ruben Antony Moniz</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Aditya Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Piraviperumal%2C+D">Dhivya Piraviperumal</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T+D">Tien Dung Tran</a>, 
<a href="/search/cs?searchtype=author&query=Tzou%2C+N">Nicholas Tzou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Industry Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the context of a voice assistant system, steering refers to the phenomenon
in which a user issues a follow-up command attempting to direct or clarify a
previous turn. We propose STEER, a steering detection model that predicts
whether a follow-up turn is a user's attempt to steer the previous command.
Constructing a training dataset for steering use cases poses challenges due to
the cold-start problem. To overcome this, we developed heuristic rules to
sample opt-in usage data, approximating positive and negative samples without
any annotation. Our experimental results show promising performance in
identifying steering intent, with over 95% accuracy on our sampled data.
Moreover, STEER, in conjunction with our sampling strategy, aligns effectively
with real-world steering scenarios, as evidenced by its strong zero-shot
performance on a human-graded evaluation set. In addition to relying solely on
user transcripts as input, we introduce STEER+, an enhanced version of the
model. STEER+ utilizes a semantic parse tree to provide more context on
out-of-vocabulary words, such as named entities that often occur at the
sentence boundary. This further improves model performance, reducing error rate
in domains where entities frequently appear, such as messaging. Lastly, we
present a data analysis that highlights the improvement in user experience when
voice assistants support steering use cases.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16991" title="Abstract">arXiv:2310.16991</a> [<a href="/pdf/2310.16991" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Deep Learning-based approach for Recognizing Agricultural  Pests in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rafi%2C+M+H">Mohtasim Hadi Rafi</a>, 
<a href="/search/cs?searchtype=author&query=Mahjabin%2C+M+R">Mohammad Ratul Mahjabin</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+S">Md Sabbir Rahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">One of the biggest challenges that the farmers go through is to fight insect
pests during agricultural product yields. The problem can be solved easily and
avoid economic losses by taking timely preventive measures. This requires
identifying insect pests in an easy and effective manner. Most of the insect
species have similarities between them. Without proper help from the
agriculturist academician it is very challenging for the farmers to identify
the crop pests accurately. To address this issue we have done extensive
experiments considering different methods to find out the best method among
all. This paper presents a detailed overview of the experiments done on mainly
a robust dataset named IP102 including transfer learning with finetuning,
attention mechanism and custom architecture. Some example from another dataset
D0 is also shown to show robustness of our experimented techniques.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16992" title="Abstract">arXiv:2310.16992</a> [<a href="/pdf/2310.16992" title="Download PDF">pdf</a>, <a href="/format/2310.16992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How well can machine-generated texts be identified and can language  models be trained to avoid identification?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneider%2C+S">Sinclair Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Steuber%2C+F">Florian Steuber</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J+A+G">Joao A. G. Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Rodosek%2C+G+D">Gabi Dreo Rodosek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for the upcoming 57th Hawaii International Conference on System Sciences (HICSS-57)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the rise of generative pre-trained transformer models such as GPT-3,
GPT-NeoX, or OPT, distinguishing human-generated texts from machine-generated
ones has become important. We refined five separate language models to generate
synthetic tweets, uncovering that shallow learning classification algorithms,
like Naive Bayes, achieve detection accuracy between 0.6 and 0.8.
<br />Shallow learning classifiers differ from human-based detection, especially
when using higher temperature values during text generation, resulting in a
lower detection rate. Humans prioritize linguistic acceptability, which tends
to be higher at lower temperature values. In contrast, transformer-based
classifiers have an accuracy of 0.9 and above. We found that using a
reinforcement learning approach to refine our generative models can
successfully evade BERT-based classifiers with a detection accuracy of 0.15 or
less.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16995" title="Abstract">arXiv:2310.16995</a> [<a href="/pdf/2310.16995" title="Download PDF">pdf</a>, <a href="/format/2310.16995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality &gt; Quantity: Synthetic Corpora from Foundation Models for  Closed-Domain Extractive Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+S">Saptarshi Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Heaton%2C+C">Connor Heaton</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Shreya Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+P">Prasenjit Mitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Domain adaptation, the process of training a model in one domain and applying
it to another, has been extensively explored in machine learning. While
training a domain-specific foundation model (FM) from scratch is an option,
recent methods have focused on adapting pre-trained FMs for domain-specific
tasks. However, our experiments reveal that either approach does not
consistently achieve state-of-the-art (SOTA) results in the target domain. In
this work, we study extractive question answering within closed domains and
introduce the concept of targeted pre-training. This involves determining and
generating relevant data to further pre-train our models, as opposed to the
conventional philosophy of utilizing domain-specific FMs trained on a wide
range of data. Our proposed framework uses Galactica to generate synthetic,
``targeted'' corpora that align with specific writing styles and topics, such
as research papers and radiology reports. This process can be viewed as a form
of knowledge distillation. We apply our method to two biomedical extractive
question answering datasets, COVID-QA and RadQA, achieving a new benchmark on
the former and demonstrating overall improvements on the latter. Code available
at https://github.com/saptarshi059/CDQA-v1-Targetted-PreTraining/tree/main.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16996" title="Abstract">arXiv:2310.16996</a> [<a href="/pdf/2310.16996" title="Download PDF">pdf</a>, <a href="/format/2310.16996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Continually Learning Application Performance Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinurat%2C+R+A+O">Ray A. O. Sinurat</a>, 
<a href="/search/cs?searchtype=author&query=Daram%2C+A">Anurag Daram</a>, 
<a href="/search/cs?searchtype=author&query=Gunawi%2C+H+S">Haryadi S. Gunawi</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+R+B">Robert B. Ross</a>, 
<a href="/search/cs?searchtype=author&query=Madireddy%2C+S">Sandeep Madireddy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at Workshop on Machine Learning for Systems at 36th Conference on Neural Information Processing Systems (NeurIPS 2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Machine learning-based performance models are increasingly being used to
build critical job scheduling and application optimization decisions.
Traditionally, these models assume that data distribution does not change as
more samples are collected over time. However, owing to the complexity and
heterogeneity of production HPC systems, they are susceptible to hardware
degradation, replacement, and/or software patches, which can lead to drift in
the data distribution that can adversely affect the performance models. To this
end, we develop continually learning performance models that account for the
distribution drift, alleviate catastrophic forgetting, and improve
generalizability. Our best model was able to retain accuracy, regardless of
having to learn the new distribution of data inflicted by system changes, while
demonstrating a 2x improvement in the prediction accuracy of the whole data
sequence in comparison to the naive approach.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16997" title="Abstract">arXiv:2310.16997</a> [<a href="/pdf/2310.16997" title="Download PDF">pdf</a>, <a href="/ps/2310.16997" title="Download PostScript">ps</a>, <a href="/format/2310.16997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using generalized simplex methods to approximate derivatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jarry-Bolduc%2C+G">Gabriel Jarry-Bolduc</a>, 
<a href="/search/math?searchtype=author&query=Planiden%2C+C">Chayne Planiden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2304.03222">arXiv:2304.03222</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper presents two methods for approximating a proper subset of the
entries of a Hessian using only function evaluations. These approximations are
obtained using the techniques called \emph{generalized simplex Hessian} and
\emph{generalized centered simplex Hessian}. We show how to choose the matrices
of directions involved in the computation of these two techniques depending on
the entries of the Hessian of interest. We discuss the number of function
evaluations required in each case and develop a general formula to approximate
all order-$P$ partial derivatives. Since only function evaluations are required
to compute the methods discussed in this paper, they are suitable for use in
derivative-free optimization methods.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16999" title="Abstract">arXiv:2310.16999</a> [<a href="/pdf/2310.16999" title="Download PDF">pdf</a>, <a href="/format/2310.16999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust, but Verify: Robust Image Segmentation using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaman%2C+F+A">Fahim Ahmed Zaman</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaodong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sonka%2C+M">Milan Sonka</a>, 
<a href="/search/cs?searchtype=author&query=Mudumbai%2C+R">Raghuraman Mudumbai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 Pages, 8 Figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">We describe a method for verifying the output of a deep neural network for
medical image segmentation that is robust to several classes of random as well
as worst-case perturbations i.e. adversarial attacks. This method is based on a
general approach recently developed by the authors called ``Trust, but Verify"
wherein an auxiliary verification network produces predictions about certain
masked features in the input image using the segmentation as an input. A
well-designed auxiliary network will produce high-quality predictions when the
input segmentations are accurate, but will produce low-quality predictions when
the segmentations are incorrect. Checking the predictions of such a network
with the original image allows us to detect bad segmentations. However, to
ensure the verification method is truly robust, we need a method for checking
the quality of the predictions that does not itself rely on a black-box neural
network. Indeed, we show that previous methods for segmentation evaluation that
do use deep neural regression networks are vulnerable to false negatives i.e.
can inaccurately label bad segmentations as good. We describe the design of a
verification network that avoids such vulnerability and present results to
demonstrate its robustness compared to previous methods.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17002" title="Abstract">arXiv:2310.17002</a> [<a href="/pdf/2310.17002" title="Download PDF">pdf</a>, <a href="/format/2310.17002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster Recalibration of an Online Predictor via Approachability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okoroafor%2C+P">Princewill Okoroafor</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+R">Robert Kleinberg</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Predictive models in ML need to be trustworthy and reliable, which often at
the very least means outputting calibrated probabilities. This can be
particularly difficult to guarantee in the online prediction setting when the
outcome sequence can be generated adversarially. In this paper we introduce a
technique using Blackwell's approachability theorem for taking an online
predictive model which might not be calibrated and transforming its predictions
to calibrated predictions without much increase to the loss of the original
model. Our proposed algorithm achieves calibration and accuracy at a faster
rate than existing techniques <a href="/abs/1607.03594">arXiv:1607.03594</a> and is the first algorithm to
offer a flexible tradeoff between calibration error and accuracy in the online
setting. We demonstrate this by characterizing the space of jointly achievable
calibration and regret using our technique.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17010" title="Abstract">arXiv:2310.17010</a> [<a href="/pdf/2310.17010" title="Download PDF">pdf</a>, <a href="/format/2310.17010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> This Reads Like That: Deep Learning for Interpretable Natural Language  Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fanconi%2C+C">Claudio Fanconi</a>, 
<a href="/search/cs?searchtype=author&query=Vandenhirtz%2C+M">Moritz Vandenhirtz</a>, 
<a href="/search/cs?searchtype=author&query=Husmann%2C+S">Severin Husmann</a>, 
<a href="/search/cs?searchtype=author&query=Vogt%2C+J+E">Julia E. Vogt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prototype learning, a popular machine learning method designed for inherently
interpretable decisions, leverages similarities to learned prototypes for
classifying new data. While it is mainly applied in computer vision, in this
work, we build upon prior research and further explore the extension of
prototypical networks to natural language processing. We introduce a learned
weighted similarity measure that enhances the similarity computation by
focusing on informative dimensions of pre-trained sentence embeddings.
Additionally, we propose a post-hoc explainability mechanism that extracts
prediction-relevant words from both the prototype and input sentences. Finally,
we empirically demonstrate that our proposed method not only improves
predictive performance on the AG News and RT Polarity datasets over a previous
prototype-based approach, but also improves the faithfulness of explanations
compared to rationale-based recurrent convolutions.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17011" title="Abstract">arXiv:2310.17011</a> [<a href="/pdf/2310.17011" title="Download PDF">pdf</a>, <a href="/format/2310.17011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Speech-driven Expressive 3D Facial Animation Synthesis with  Style Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bozkurt%2C+E">Elif Bozkurt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Different people have different facial expressions while speaking
emotionally. A realistic facial animation system should consider such
identity-specific speaking styles and facial idiosyncrasies to achieve
high-degree of naturalness and plausibility. Existing approaches to
personalized speech-driven 3D facial animation either use one-hot identity
labels or rely-on person specific models which limit their scalability. We
present a personalized speech-driven expressive 3D facial animation synthesis
framework that models identity specific facial motion as latent representations
(called as styles), and synthesizes novel animations given a speech input with
the target style for various emotion categories. Our framework is trained in an
end-to-end fashion and has a non-autoregressive encoder-decoder architecture
with three main components: expression encoder, speech encoder and expression
decoder. Since, expressive facial motion includes both identity-specific style
and speech-related content information; expression encoder first disentangles
facial motion sequences into style and content representations, respectively.
Then, both of the speech encoder and the expression decoders input the
extracted style information to update transformer layer weights during training
phase. Our speech encoder also extracts speech phoneme label and duration
information to achieve better synchrony within the non-autoregressive synthesis
mechanism more effectively. Through detailed experiments, we demonstrate that
our approach produces temporally coherent facial expressions from input speech
while preserving the speaking styles of the target identities.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17012" title="Abstract">arXiv:2310.17012</a> [<a href="/pdf/2310.17012" title="Download PDF">pdf</a>, <a href="/format/2310.17012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Packed to the Brim: Investigating the Impact of Highly Responsive  Prefixes on Internet-wide Measurement Campaigns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sattler%2C+P">Patrick Sattler</a>, 
<a href="/search/cs?searchtype=author&query=Zirngibl%2C+J">Johannes Zirngibl</a>, 
<a href="/search/cs?searchtype=author&query=Jonker%2C+M">Mattijs Jonker</a>, 
<a href="/search/cs?searchtype=author&query=Gasser%2C+O">Oliver Gasser</a>, 
<a href="/search/cs?searchtype=author&query=Carle%2C+G">Georg Carle</a>, 
<a href="/search/cs?searchtype=author&query=Holz%2C+R">Ralph Holz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Internet-wide scans are an important tool to evaluate the deployment of
services. To enable large-scale application layer scans, a fast, stateless port
scan (e.g., using ZMap) is often performed ahead of time to collect responsive
targets. It is a common expectation that port scans on the entire IPv4 address
space provide a relatively unbiased view as they cover the complete address
space. Previous work, however, has found prefixes where all addresses share
particular properties. In IPv6, aliased prefixes and fully responsive prefixes,
i.e., prefixes where all addresses are responsive, are a well-known phenomenon.
However, there is no such in-depth analysis for prefixes with these
responsiveness patterns in IPv4. This paper delves into the underlying factors
of this phenomenon in the context of IPv4 and evaluates port scans on a total
of 161 ports (142 TCP &amp; 19 UDP ports) from three different vantage points. To
account for packet loss and other scanning artifacts, we propose the notion of
a new category of prefixes, which we call highly responsive prefixes (HRPs).
Our findings show that the share of HRPs can make up 70 % of responsive
addresses on selected ports. Regarding specific ports, we observe that CDNs
contribute to the largest fraction of HRPs on TCP/80 and TCP/443, while TCP
proxies emerge as the primary cause of HRPs on other ports. Our analysis also
reveals that application layer handshakes to targets outside HRPs are,
depending on the chosen service, up to three times more likely to be successful
compared to handshakes with targets located in HRPs. To improve future scanning
campaigns conducted by the research community, we make our study's data
publicly available and provide a tool for detecting HRPs. Furthermore, we
propose an approach for a more efficient, ethical, and sustainable application
layer target selection.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17013" title="Abstract">arXiv:2310.17013</a> [<a href="/pdf/2310.17013" title="Download PDF">pdf</a>, <a href="/format/2310.17013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whitepaper on Reusable Hybrid and Multi-Cloud Analytics Service  Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+Laszewski%2C+G">Gregor von Laszewski</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+W">Wo Chang</a>, 
<a href="/search/cs?searchtype=author&query=Reinsch%2C+R">Russell Reinsch</a>, 
<a href="/search/cs?searchtype=author&query=Kotevska%2C+O">Olivera Kotevska</a>, 
<a href="/search/cs?searchtype=author&query=Karimi%2C+A">Ali Karimi</a>, 
<a href="/search/cs?searchtype=author&query=Sattar%2C+A+R">Abdul Rahman Sattar</a>, 
<a href="/search/cs?searchtype=author&query=Mazzaferro%2C+G">Garry Mazzaferro</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+G+C">Geoffrey C. Fox</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Over the last several years, the computation landscape for conducting data
analytics has completely changed. While in the past, a lot of the activities
have been undertaken in isolation by companies, and research institutions,
today's infrastructure constitutes a wealth of services offered by a variety of
providers that offer opportunities for reuse, and interactions while leveraging
service collaboration, and service cooperation.
<br />This document focuses on expanding analytics services to develop a framework
for reusable hybrid multi-service data analytics. It includes (a) a short
technology review that explicitly targets the intersection of hybrid
multi-provider analytics services, (b) a small motivation based on use cases we
looked at, (c) enhancing the concepts of services to showcase how hybrid, as
well as multi-provider services can be integrated and reused via the proposed
framework, (d) address analytics service composition, and (e) integrate
container technologies to achieve state-of-the-art analytics service deployment
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17015" title="Abstract">arXiv:2310.17015</a> [<a href="/pdf/2310.17015" title="Download PDF">pdf</a>, <a href="/format/2310.17015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation for Emotion Detection in Small Imbalanced Text Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koufakou%2C+A">Anna Koufakou</a>, 
<a href="/search/cs?searchtype=author&query=Grisales%2C+D">Diego Grisales</a>, 
<a href="/search/cs?searchtype=author&query=de+jesus%2C+R+C">Ragy Costa de jesus</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+O">Oscar Fox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted paper at IEEE ICMLA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Emotion recognition in text, the task of identifying emotions such as joy or
anger, is a challenging problem in NLP with many applications. One of the
challenges is the shortage of available datasets that have been annotated with
emotions. Certain existing datasets are small, follow different emotion
taxonomies and display imbalance in their emotion distribution. In this work,
we studied the impact of data augmentation techniques precisely when applied to
small imbalanced datasets, for which current state-of-the-art models (such as
RoBERTa) under-perform. Specifically, we utilized four data augmentation
methods (Easy Data Augmentation EDA, static and contextual Embedding-based, and
ProtAugment) on three datasets that come from different sources and vary in
size, emotion categories and distributions. Our experimental results show that
using the augmented data when training the classifier model leads to
significant improvements. Finally, we conducted two case studies: a) directly
using the popular chat-GPT API to paraphrase text using different prompts, and
b) using external data to augment the training set. Results show the promising
potential of these methods.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17017" title="Abstract">arXiv:2310.17017</a> [<a href="/pdf/2310.17017" title="Download PDF">pdf</a>, <a href="/format/2310.17017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Integrative Survey on Mental Health Conversational Agents to Bridge  Computer Science and Medical Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y+M">Young Min Cho</a>, 
<a href="/search/cs?searchtype=author&query=Rai%2C+S">Sunny Rai</a>, 
<a href="/search/cs?searchtype=author&query=Ungar%2C+L">Lyle Ungar</a>, 
<a href="/search/cs?searchtype=author&query=Sedoc%2C+J">Jo&#xe3;o Sedoc</a>, 
<a href="/search/cs?searchtype=author&query=Guntuku%2C+S+C">Sharath Chandra Guntuku</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in EMNLP 2023 Main Conference, camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mental health conversational agents (a.k.a. chatbots) are widely studied for
their potential to offer accessible support to those experiencing mental health
challenges. Previous surveys on the topic primarily consider papers published
in either computer science or medicine, leading to a divide in understanding
and hindering the sharing of beneficial knowledge between both domains. To
bridge this gap, we conduct a comprehensive literature review using the PRISMA
framework, reviewing 534 papers published in both computer science and
medicine. Our systematic review reveals 136 key papers on building mental
health-related conversational agents with diverse characteristics of modeling
and experimental design techniques. We find that computer science papers focus
on LLM techniques and evaluating response quality using automated metrics with
little attention to the application while medical papers use rule-based
conversational agents and outcome metrics to measure the health outcomes of
participants. Based on our findings on transparency, ethics, and cultural
heterogeneity in this review, we provide a few recommendations to help bridge
the disciplinary divide and enable the cross-disciplinary development of mental
health conversational agents.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17019" title="Abstract">arXiv:2310.17019</a> [<a href="/pdf/2310.17019" title="Download PDF">pdf</a>, <a href="/format/2310.17019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditionally Combining Robot Skills using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zentner%2C+K+R">K.R. Zentner</a>, 
<a href="/search/cs?searchtype=author&query=Julian%2C+R">Ryan Julian</a>, 
<a href="/search/cs?searchtype=author&query=Ichter%2C+B">Brian Ichter</a>, 
<a href="/search/cs?searchtype=author&query=Sukhatme%2C+G+S">Gaurav S. Sukhatme</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Robotics (cs.RO)

</div>
<p class="mathjax">This paper combines two contributions. First, we introduce an extension of
the Meta-World benchmark, which we call "Language-World," which allows a large
language model to operate in a simulated robotic environment using
semi-structured natural language queries and scripted skills described using
natural language. By using the same set of tasks as Meta-World, Language-World
results can be easily compared to Meta-World results, allowing for a point of
comparison between recent methods using Large Language Models (LLMs) and those
using Deep Reinforcement Learning. Second, we introduce a method we call Plan
Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of
high-level plans using end-to-end demonstrations. Using Language-World, we show
that PCBC is able to achieve strong performance in a variety of few-shot
regimes, often achieving task generalization with as little as a single
demonstration. We have made Language-World available as open-source software at
https://github.com/krzentner/language-world/.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17021" title="Abstract">arXiv:2310.17021</a> [<a href="/pdf/2310.17021" title="Download PDF">pdf</a>, <a href="/format/2310.17021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming Factor Trajectory Learning for Temporal Tensor Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Shikai Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shibo Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kirby%2C+R">Robert Kirby</a>, 
<a href="/search/cs?searchtype=author&query=Zhe%2C+S">Shandian Zhe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Practical tensor data is often along with time information. Most existing
temporal decomposition approaches estimate a set of fixed factors for the
objects in each tensor mode, and hence cannot capture the temporal evolution of
the objects' representation. More important, we lack an effective approach to
capture such evolution from streaming data, which is common in real-world
applications. To address these issues, we propose Streaming Factor Trajectory
Learning (SFTL) for temporal tensor decomposition. We use Gaussian processes
(GPs) to model the trajectory of factors so as to flexibly estimate their
temporal evolution. To address the computational challenges in handling
streaming data, we convert the GPs into a state-space prior by constructing an
equivalent stochastic differential equation (SDE). We develop an efficient
online filtering algorithm to estimate a decoupled running posterior of the
involved factor states upon receiving new data. The decoupled estimation
enables us to conduct standard Rauch-Tung-Striebel smoothing to compute the
full posterior of all the trajectories in parallel, without the need for
revisiting any previous data. We have shown the advantage of SFTL in both
synthetic tasks and real-world applications.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17022" title="Abstract">arXiv:2310.17022</a> [<a href="/pdf/2310.17022" title="Download PDF">pdf</a>, <a href="/format/2310.17022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlled Decoding from Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mudgal%2C+S">Sidharth Mudgal</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ganapathy%2C+H">Harish Ganapathy</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">YaGuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yanping Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Heng-Tze Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+M">Michael Collins</a>, 
<a href="/search/cs?searchtype=author&query=Strohman%2C+T">Trevor Strohman</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jilin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Beutel%2C+A">Alex Beutel</a>, 
<a href="/search/cs?searchtype=author&query=Beirami%2C+A">Ahmad Beirami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We propose controlled decoding (CD), a novel off-policy reinforcement
learning method to control the autoregressive generation from language models
towards high reward outcomes. CD solves an off-policy reinforcement learning
problem through a value function for the reward, which we call a prefix scorer.
The prefix scorer is used at inference time to steer the generation towards
higher reward outcomes. We show that the prefix scorer may be trained on
(possibly) off-policy data to predict the expected reward when decoding is
continued from a partially decoded response. We empirically demonstrate that CD
is effective as a control mechanism on Reddit conversations corpus. We also
show that the modularity of the design of CD makes it possible to control for
multiple rewards, effectively solving a multi-objective reinforcement learning
problem with no additional complexity. Finally, we show that CD can be applied
in a novel blockwise fashion at inference-time, again without the need for any
training-time changes, essentially bridging the gap between the popular
best-of-$K$ strategy and token-level reinforcement learning. This makes CD a
promising approach for alignment of language models.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17025" title="Abstract">arXiv:2310.17025</a> [<a href="/pdf/2310.17025" title="Download PDF">pdf</a>, <a href="/format/2310.17025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> netFound: Foundation Model for Network Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guthula%2C+S">Satyandra Guthula</a>, 
<a href="/search/cs?searchtype=author&query=Battula%2C+N">Navya Battula</a>, 
<a href="/search/cs?searchtype=author&query=Beltiukov%2C+R">Roman Beltiukov</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wenbo Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Arpit Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In ML for network security, traditional workflows rely on high-quality
labeled data and manual feature engineering, but limited datasets and human
expertise hinder feature selection, leading to models struggling to capture
crucial relationships and generalize effectively. Inspired by recent
advancements in ML application domains like GPT-4 and Vision Transformers, we
have developed netFound, a foundational model for network security. This model
undergoes pre-training using self-supervised algorithms applied to readily
available unlabeled network packet traces. netFound's design incorporates
hierarchical and multi-modal attributes of network traffic, effectively
capturing hidden networking contexts, including application logic,
communication protocols, and network conditions.
<br />With this pre-trained foundation in place, we can fine-tune netFound for a
wide array of downstream tasks, even when dealing with low-quality, limited,
and noisy labeled data. Our experiments demonstrate netFound's superiority over
existing state-of-the-art ML-based solutions across three distinct network
downstream tasks: traffic classification, network intrusion detection, and APT
detection. Furthermore, we emphasize netFound's robustness against noisy and
missing labels, as well as its ability to generalize across temporal variations
and diverse network environments. Finally, through a series of ablation
studies, we provide comprehensive insights into how our design choices enable
netFound to more effectively capture hidden networking contexts, further
solidifying its performance and utility in network security applications.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17029" title="Abstract">arXiv:2310.17029</a> [<a href="/pdf/2310.17029" title="Download PDF">pdf</a>, <a href="/format/2310.17029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward the use of proxies for efficient learning manipulation and  locomotion strategies on soft robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%A9nager%2C+E">Etienne M&#xe9;nager</a>, 
<a href="/search/cs?searchtype=author&query=Peyron%2C+Q">Quentin Peyron</a>, 
<a href="/search/cs?searchtype=author&query=Duriez%2C+C">Christian Duriez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE Robotics and Automation Letters (RAL) in October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Soft robots are naturally designed to perform safe interactions with their
environment, like locomotion and manipulation. In the literature, there are now
many concepts, often bio-inspired, to propose new modes of locomotion or
grasping. However, a methodology for implementing motion planning of these
tasks, as exists for rigid robots, is still lacking. One of the difficulties
comes from the modeling of these robots, which is very different, as it is
based on the mechanics of deformable bodies. These models, whose dimension is
often very large, make learning and optimization methods very costly. In this
paper, we propose a proxy approach, as exists for humanoid robotics. This proxy
is a simplified model of the robot that enables frugal learning of a motion
strategy. This strategy is then transferred to the complete model to obtain the
corresponding actuation inputs. Our methodology is illustrated and analyzed on
two classical designs of soft robots doing manipulation and locomotion tasks.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17034" title="Abstract">arXiv:2310.17034</a> [<a href="/pdf/2310.17034" title="Download PDF">pdf</a>, <a href="/format/2310.17034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Follow-on Question Suggestion via Voice Hints for Voice Assistants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fetahu%2C+B">Besnik Fetahu</a>, 
<a href="/search/cs?searchtype=author&query=Faustini%2C+P">Pedro Faustini</a>, 
<a href="/search/cs?searchtype=author&query=Castellucci%2C+G">Giuseppe Castellucci</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+A">Anjie Fang</a>, 
<a href="/search/cs?searchtype=author&query=Rokhlenko%2C+O">Oleg Rokhlenko</a>, 
<a href="/search/cs?searchtype=author&query=Malmasi%2C+S">Shervin Malmasi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Long Paper at EMNLP'23 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The adoption of voice assistants like Alexa or Siri has grown rapidly,
allowing users to instantly access information via voice search. Query
suggestion is a standard feature of screen-based search experiences, allowing
users to explore additional topics. However, this is not trivial to implement
in voice-based settings. To enable this, we tackle the novel task of suggesting
questions with compact and natural voice hints to allow users to ask follow-up
questions.
<br />We define the task, ground it in syntactic theory and outline linguistic
desiderata for spoken hints. We propose baselines and an approach using
sequence-to-sequence Transformers to generate spoken hints from a list of
questions. Using a new dataset of 6681 input questions and human written hints,
we evaluated the models with automatic metrics and human evaluation. Results
show that a naive approach of concatenating suggested questions creates poor
voice hints. Our approach, which applies a linguistically-motivated pretraining
task was strongly preferred by humans for producing the most natural hints.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17039" title="Abstract">arXiv:2310.17039</a> [<a href="/pdf/2310.17039" title="Download PDF">pdf</a>, <a href="/format/2310.17039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The limiting distribution of a random variable transformed by Chebyshev  Polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Vazquez%2C+J+C">Javier Chico Vazquez</a>, 
<a href="/search/math?searchtype=author&query=Horning%2C+A+J">Andrew J. Horning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Probability (math.PR)

</div>
<p class="mathjax">In this paper we present the result of successively applying a Chebyshev
polynomial to a continuous random variable. In particular we show that under
mild assumptions the limiting distribution will be the same as the weight with
respect to which Chebyshev polynomials are orthogonal.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17041" title="Abstract">arXiv:2310.17041</a> [<a href="/pdf/2310.17041" title="Download PDF">pdf</a>, <a href="/format/2310.17041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Surgical Fine-tuning for Language Encoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lodha%2C+A">Abhilasha Lodha</a>, 
<a href="/search/cs?searchtype=author&query=Belapurkar%2C+G">Gayatri Belapurkar</a>, 
<a href="/search/cs?searchtype=author&query=Chalkapurkar%2C+S">Saloni Chalkapurkar</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yuanming Tao</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+R">Reshmi Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+S">Samyadeep Basu</a>, 
<a href="/search/cs?searchtype=author&query=Petrov%2C+D">Dmitrii Petrov</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+S">Soundararajan Srinivasan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Fine-tuning all the layers of a pre-trained neural language encoder (either
using all the parameters or using parameter-efficient methods) is often the
de-facto way of adapting it to a new task. We show evidence that for different
downstream language tasks, fine-tuning only a subset of layers is sufficient to
obtain performance that is close to and often better than fine-tuning all the
layers in the language encoder. We propose an efficient metric based on the
diagonal of the Fisher information matrix (FIM score), to select the candidate
layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE
tasks and across distinct language encoders, that this metric can effectively
select layers leading to a strong downstream performance. Our work highlights
that task-specific information corresponding to a given downstream task is
often localized within a few layers, and tuning only those is sufficient for
strong performance. Additionally, we demonstrate the robustness of the FIM
score to rank layers in a manner that remains constant during the optimization
process.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17042" title="Abstract">arXiv:2310.17042</a> [<a href="/pdf/2310.17042" title="Download PDF">pdf</a>, <a href="/format/2310.17042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StochGradAdam: Accelerating Neural Networks Training with Stochastic  Gradient Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yun%2C+J">Juyoung Yun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In the rapidly advancing domain of deep learning optimization, this paper
unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded
Adam algorithm. Central to StochGradAdam is its gradient sampling technique.
This method not only ensures stable convergence but also leverages the
advantages of selective gradient consideration, fostering robust training by
potentially mitigating the effects of noisy or outlier data and enhancing the
exploration of the loss landscape for more dependable convergence. In both
image classification and segmentation tasks, StochGradAdam has demonstrated
superior performance compared to the traditional Adam optimizer. By judiciously
sampling a subset of gradients at each iteration, the optimizer is optimized
for managing intricate models. The paper provides a comprehensive exploration
of StochGradAdam's methodology, from its mathematical foundations to bias
correction strategies, heralding a promising advancement in deep learning
training techniques.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17044" title="Abstract">arXiv:2310.17044</a> [<a href="/pdf/2310.17044" title="Download PDF">pdf</a>, <a href="/format/2310.17044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Rank for Active Learning via Multi-Task Bilevel Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zixin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Si Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Ruoxi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Active learning is a promising paradigm to reduce the labeling cost by
strategically requesting labels to improve model performance. However, existing
active learning methods often rely on expensive acquisition function to
compute, extensive modeling retraining and multiple rounds of interaction with
annotators. To address these limitations, we propose a novel approach for
active learning, which aims to select batches of unlabeled instances through a
learned surrogate model for data acquisition. A key challenge in this approach
is developing an acquisition function that generalizes well, as the history of
data, which forms part of the utility function's input, grows over time. Our
novel algorithmic contribution is a bilevel multi-task bilevel optimization
framework that predicts the relative utility -- measured by the validation
accuracy -- of different training sets, and ensures the learned acquisition
function generalizes effectively. For cases where validation accuracy is
expensive to evaluate, we introduce efficient interpolation-based surrogate
models to estimate the utility function, reducing the evaluation cost. We
demonstrate the performance of our approach through extensive experiments on
standard active classification benchmarks. By employing our learned utility
function, we show significant improvements over traditional techniques, paving
the way for more efficient and effective utility maximization in active
learning applications.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17046" title="Abstract">arXiv:2310.17046</a> [<a href="/pdf/2310.17046" title="Download PDF">pdf</a>, <a href="/format/2310.17046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proving the Absence of Microarchitectural Timing Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buckley%2C+S">Scott Buckley</a> (1), 
<a href="/search/cs?searchtype=author&query=Sison%2C+R">Robert Sison</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Wistoff%2C+N">Nils Wistoff</a> (3), 
<a href="/search/cs?searchtype=author&query=Millar%2C+C">Curtis Millar</a> (1), 
<a href="/search/cs?searchtype=author&query=Murray%2C+T">Toby Murray</a> (2), 
<a href="/search/cs?searchtype=author&query=Klein%2C+G">Gerwin Klein</a> (4 and 1), 
<a href="/search/cs?searchtype=author&query=Heiser%2C+G">Gernot Heiser</a> (1) ((1) UNSW Sydney, (2) University of Melbourne, (3) ETH Z&#xfc;rich, (4) Proofcraft)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Scott Buckley and Robert Sison were joint lead authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>; Cryptography and Security (cs.CR); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Microarchitectural timing channels are a major threat to computer security. A
set of OS mechanisms called time protection was recently proposed as a
principled way of preventing information leakage through such channels and
prototyped in the seL4 microkernel. We formalise time protection and the
underlying hardware mechanisms in a way that allows linking them to the
information-flow proofs that showed the absence of storage channels in seL4.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17049" title="Abstract">arXiv:2310.17049</a> [<a href="/pdf/2310.17049" title="Download PDF">pdf</a>, <a href="/format/2310.17049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Repeatable Speech Embeddings Using An Intra-class Correlation  Regularizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jayasuriya%2C+S">Suren Jayasuriya</a>, 
<a href="/search/cs?searchtype=author&query=Berisha%2C+V">Visar Berisha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">A good supervised embedding for a specific machine learning task is only
sensitive to changes in the label of interest and is invariant to other
confounding factors. We leverage the concept of repeatability from measurement
theory to describe this property and propose to use the intra-class correlation
coefficient (ICC) to evaluate the repeatability of embeddings. We then propose
a novel regularizer, the ICC regularizer, as a complementary component for
contrastive losses to guide deep neural networks to produce embeddings with
higher repeatability. We use simulated data to explain why the ICC regularizer
works better on minimizing the intra-class variance than the contrastive loss
alone. We implement the ICC regularizer and apply it to three speech tasks:
speaker verification, voice style conversion, and a clinical application for
detecting dysphonic voice. The experimental results demonstrate that adding an
ICC regularizer can improve the repeatability of learned embeddings compared to
only using the contrastive loss; further, these embeddings lead to improved
performance in these downstream tasks.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17050" title="Abstract">arXiv:2310.17050</a> [<a href="/pdf/2310.17050" title="Download PDF">pdf</a>, <a href="/format/2310.17050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Question Decomposition for Zero-Shot VQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+Z">Zaid Khan</a>, 
<a href="/search/cs?searchtype=author&query=BG%2C+V+K">Vijay Kumar BG</a>, 
<a href="/search/cs?searchtype=author&query=Schulter%2C+S">Samuel Schulter</a>, 
<a href="/search/cs?searchtype=author&query=Chandraker%2C+M">Manmohan Chandraker</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yun Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual question answering (VQA) has traditionally been treated as a
single-step task where each question receives the same amount of effort, unlike
natural human question-answering strategies. We explore a question
decomposition strategy for VQA to overcome this limitation. We probe the
ability of recently developed large vision-language models to use human-written
decompositions and produce their own decompositions of visual questions,
finding they are capable of learning both tasks from demonstrations alone.
However, we show that naive application of model-written decompositions can
hurt performance. We introduce a model-driven selective decomposition approach
for second-guessing predictions and correcting errors, and validate its
effectiveness on eight VQA tasks across three domains, showing consistent
improvements in accuracy, including improvements of &gt;20% on medical VQA
datasets and boosting the zero-shot performance of BLIP-2 above chance on a VQA
reformulation of the challenging Winoground task. Project Site:
https://zaidkhan.me/decomposition-0shot-vqa/
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17052" title="Abstract">arXiv:2310.17052</a> [<a href="/pdf/2310.17052" title="Download PDF">pdf</a>, <a href="/format/2310.17052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Performance of OPC UA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirdan%2C+E">Erkin Kirdan</a>, 
<a href="/search/cs?searchtype=author&query=Rezabek%2C+F">Filip Rezabek</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BClbauer%2C+N">Nikolas M&#xfc;lbauer</a>, 
<a href="/search/cs?searchtype=author&query=Carle%2C+G">Georg Carle</a>, 
<a href="/search/cs?searchtype=author&query=Pahl%2C+M">Marc-Oliver Pahl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">OPC UA is a prevalent communication protocol used in the Industrial Internet
of Things and machine-to-machine communication, utilizing time-sensitive
networking to guarantee real-time requirements. Linux implements time-sensitive
networking through various queueing disciplines (qdiscs), including Time Aware
Priority, Multiqueue Priority, Earliest TxTime First, and Credit-Based Shaper.
Prior studies on these qdiscs have been limited to a few despite their
significance. They have often been confined to point-to-point network
configurations using proprietary software or specialized hardware. This study
builds upon existing research by evaluating all qdiscs in point-to-point and
bridged network configurations using open-source software on commercial
off-the-shelf hardware. The study identifies the optimal configuration for each
qdisc and compares their jitter, latency, and reliability performance. Our
results provide a foundation for future research and practical deployments in
real-world industrial applications and show that open-source OPC UA on
commercial off-the-shelf hardware can effectively support the real-time demands
of many industrial applications.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17054" title="Abstract">arXiv:2310.17054</a> [<a href="/pdf/2310.17054" title="Download PDF">pdf</a>, <a href="/format/2310.17054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs&#x27;  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yufei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Felix Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) such as GPT-3 have demonstrated a strong
capability to generate coherent and contextually relevant text. However, amidst
their successes, a crucial issue persists: their generated outputs still lack
commonsense at times. Moreover, fine-tuning the entire LLM towards more
commonsensical outputs is computationally expensive if not infeasible. In this
paper, we present a computation-efficient framework that steers a frozen
Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e.,
producing a plausible output that incorporates a list of concepts in a
meaningful way). Specifically, we first construct a reference-free evaluator
that assigns a sentence with a commonsensical score by grounding the sentence
to a dynamic commonsense knowledge base from four different relational aspects.
We then use the scorer as the oracle for commonsense knowledge, and extend the
controllable generation method called NADO to train an auxiliary head that
guides a fixed PTLM to better satisfy the oracle. We test our framework on a
series of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two
constrained concept-to-sentence benchmarks. Human evaluation results
demonstrate that our method consistently leads to the most commonsensical
outputs.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17056" title="Abstract">arXiv:2310.17056</a> [<a href="/pdf/2310.17056" title="Download PDF">pdf</a>, <a href="/format/2310.17056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategizing EV Charging and Renewable Integration in Texas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mohammadi%2C+M">Mohammad Mohammadi</a>, 
<a href="/search/eess?searchtype=author&query=Thornburg%2C+J">Jesse Thornburg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Exploring the convergence of electric vehicles (EVs), renewable energy, and
smart grid technologies in the context of Texas, this study addresses
challenges hindering the widespread adoption of EVs. Acknowledging their
environmental benefits, the research focuses on grid stability concerns,
uncoordinated charging patterns, and the complicated relationship between EVs
and renewable energy sources. Dynamic time warping (DTW) clustering and k-means
clustering methodologies categorize days based on total load and net load,
offering nuanced insights into daily electricity consumption and renewable
energy generation patterns. By establishing optimal charging and
vehicle-to-grid (V2G) windows tailored to specific load characteristics, the
study provides a sophisticated methodology for strategic decision-making in
energy consumption and renewable integration. The findings contribute to the
ongoing discourse on achieving a sustainable and resilient energy future
through the seamless integration of EVs into smart grids.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17058" title="Abstract">arXiv:2310.17058</a> [<a href="/pdf/2310.17058" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diseno y Desarrollo de Prototipos Roboticos para Competencias de Futbol  utilizando Motores Dynamixel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moraes%2C+P">Pablo Moraes</a>, 
<a href="/search/cs?searchtype=author&query=Sodre%2C+H">Hiago Sodre</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+M">Monica Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Kelbouscas%2C+A">Andre Kelbouscas</a>, 
<a href="/search/cs?searchtype=author&query=Schuster%2C+J">Jean Schuster</a>, 
<a href="/search/cs?searchtype=author&query=Schuster%2C+C">Cristiano Schuster</a>, 
<a href="/search/cs?searchtype=author&query=Grando%2C+R">Ricardo Grando</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Febitec 2023, in Spanish language
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This article describes the design and development of robotic prototypes for
robotic soccer competitions using Dynamixel motors. Although the prototypes are
not aimed at world-class competitions, they represent a significant step in the
development of sports robots. Model XL430-W250 Dynamixel motors were chosen and
electronic circuits were implemented using control boards such as OpenCR and
Raspberry Pi 3. A crucial component was introduced: a step-up board that
charges a capacitor to create a powerful kick to the ball via anelectromagnet
controlled by Arduino Nano. The programming and coordination of the prototypes
was carried out using the ROS environment (Robot Operating System), which
allows effective integration of movements and communication. Although the
prototypes were not optimized for global competition, they underwent extensive
testing, evaluating their speed and maneuverability, as well as soccer tactics
in the GRSim simulator. These prototypes contribute to the further development
of sports robotics and illustrate the research potential in this exciting area.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17060" title="Abstract">arXiv:2310.17060</a> [<a href="/pdf/2310.17060" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aplicacion de Robots Humanoides como Guias Interactivos en Museos: Una  Simulacion con el Robot NAO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sodre%2C+H">Hiago Sodre</a>, 
<a href="/search/cs?searchtype=author&query=Moraes%2C+P">Pablo Moraes</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+M">Monica Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Castelli%2C+V">Victor Castelli</a>, 
<a href="/search/cs?searchtype=author&query=Barboza%2C+P">Pamela Barboza</a>, 
<a href="/search/cs?searchtype=author&query=Mattos%2C+M">Martin Mattos</a>, 
<a href="/search/cs?searchtype=author&query=Vivas%2C+G">Guillermo Vivas</a>, 
<a href="/search/cs?searchtype=author&query=de+Vargas%2C+B">Bruna de Vargas</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%B6rnbach%2C+T">Tobias D&#xf6;rnbach</a>, 
<a href="/search/cs?searchtype=author&query=Grando%2C+R">Ricardo Grando</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Febitec 2023, in Spanish language
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This article presents an application that evaluates the feasibility of
humanoid robots as interactive guides in art museums. The application entailes
programming a NAO robot and a chatbot to provide information about art pieces
in a simulated museum environment. In this controlled scenario, the learning
employees interact with the robot and the chatbot. The result is a skilled
participation in the interactions, along with the effectiveness of the robot
and chatbot that communicates the basic details of the art objects. You see
natural and fluid interactions between the students and the robot. This
suggests that the addition of humanoid robots to museums may provide a better
experience for visitors, but also the need to continue to do more to optimize
the quality of interaction. This study contributes to understanding the
possibilities and requirements of applying humanoid technologies in a cultural
context.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17062" title="Abstract">arXiv:2310.17062</a> [<a href="/pdf/2310.17062" title="Download PDF">pdf</a>, <a href="/format/2310.17062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Open, Programmable, Multi-vendor 5G O-RAN Testbed with NVIDIA ARC and  OpenAirInterface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Villa%2C+D">Davide Villa</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+I">Imran Khan</a>, 
<a href="/search/cs?searchtype=author&query=Kaltenberger%2C+F">Florian Kaltenberger</a>, 
<a href="/search/cs?searchtype=author&query=Hedberg%2C+N">Nicholas Hedberg</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+R+S">Ruben Soares da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Kelkar%2C+A">Anupa Kelkar</a>, 
<a href="/search/cs?searchtype=author&query=Dick%2C+C">Chris Dick</a>, 
<a href="/search/cs?searchtype=author&query=Basagni%2C+S">Stefano Basagni</a>, 
<a href="/search/cs?searchtype=author&query=Jornet%2C+J+M">Josep M. Jornet</a>, 
<a href="/search/cs?searchtype=author&query=Melodia%2C+T">Tommaso Melodia</a>, 
<a href="/search/cs?searchtype=author&query=Polese%2C+M">Michele Polese</a>, 
<a href="/search/cs?searchtype=author&query=Koutsonikolas%2C+D">Dimitrios Koutsonikolas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 9 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The transition of fifth generation (5G) cellular systems to softwarized,
programmable, and intelligent networks depends on successfully enabling public
and private 5G deployments that are (i) fully software-driven and (ii) with a
performance at par with that of traditional monolithic systems. This requires
hardware acceleration to scale the Physical (PHY) layer performance, end-to-end
integration and testing, and careful planning of the Radio Frequency (RF)
environment. In this paper, we describe how the X5G testbed at Northeastern
University has addressed these challenges through the first 8-node network
deployment of the NVIDIA Aerial Research Cloud (ARC), with the Aerial SDK for
the PHY layer, accelerated on Graphics Processing Unit (GPU), and through its
integration with higher layers from the OpenAirInterface (OAI) open-source
project through the Small Cell Forum Functional Application Platform Interface
(FAPI). We discuss software integration, the network infrastructure, and a
digital twin framework for RF planning. We then profile the performance with up
to 4 Commercial Off-the-Shelf (COTS) smartphones for each base station with
iPerf and video streaming applications, measuring a cell rate higher than 500
Mbps in downlink and 45 Mbps in uplink.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17064" title="Abstract">arXiv:2310.17064</a> [<a href="/pdf/2310.17064" title="Download PDF">pdf</a>, <a href="/format/2310.17064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> math-PVS: A Large Language Model Framework to Map Scientific  Publications to PVS Theories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saidi%2C+H">Hassen Saidi</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Susmit Jha</a>, 
<a href="/search/cs?searchtype=author&query=Sahai%2C+T">Tuhin Sahai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">As artificial intelligence (AI) gains greater adoption in a wide variety of
applications, it has immense potential to contribute to mathematical discovery,
by guiding conjecture generation, constructing counterexamples, assisting in
formalizing mathematics, and discovering connections between different
mathematical areas, to name a few.
<br />While prior work has leveraged computers for exhaustive mathematical proof
search, recent efforts based on large language models (LLMs) aspire to position
computing platforms as co-contributors in the mathematical research process.
Despite their current limitations in logic and mathematical tasks, there is
growing interest in melding theorem proving systems with foundation models.
This work investigates the applicability of LLMs in formalizing advanced
mathematical concepts and proposes a framework that can critically review and
check mathematical reasoning in research papers. Given the noted reasoning
shortcomings of LLMs, our approach synergizes the capabilities of proof
assistants, specifically PVS, with LLMs, enabling a bridge between textual
descriptions in academic papers and formal specifications in PVS. By harnessing
the PVS environment, coupled with data ingestion and conversion mechanisms, we
envision an automated process, called \emph{math-PVS}, to extract and formalize
mathematical theorems from research papers, offering an innovative tool for
academic review and discovery.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17069" title="Abstract">arXiv:2310.17069</a> [<a href="/pdf/2310.17069" title="Download PDF">pdf</a>, <a href="/format/2310.17069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electric Vehicle Aggregation Review: Benefits and Vulnerabilities of  Managing a Growing EV Fleet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nelsona%2C+K">Kelsey Nelsona</a>, 
<a href="/search/eess?searchtype=author&query=Mohammadia%2C+J">Javad Mohammadia</a>, 
<a href="/search/eess?searchtype=author&query=Chenb%2C+Y">Yu Chenb</a>, 
<a href="/search/eess?searchtype=author&query=Blaschc%2C+E">Erik Blaschc</a>, 
<a href="/search/eess?searchtype=author&query=Avedc%2C+A">Alex Avedc</a>, 
<a href="/search/eess?searchtype=author&query=Ferrisc%2C+D">David Ferrisc</a>, 
<a href="/search/eess?searchtype=author&query=Cruzc%2C+E+A">Erika Ardiles Cruzc</a>, 
<a href="/search/eess?searchtype=author&query=Morronec%2C+P">Philip Morronec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Electric vehicles (EVs) are becoming more popular within the United States,
making up an increasingly large portion of the US's electricity consumption.
Hence, there is much attention has been directed on how to manage EVs within
the power sector. A well-investigated strategy for managing the increase in
electricity demand from EV charging is aggregation, which allows for an
intermediary to manage electricity flow between EV owners and their utilities.
When implemented effectively, EV aggregation provides key benefits to power
grids by relieving electrical loads.. These benefits are aggregation's ability
to shift EV loads to peak shave, which often leads to lower emissions,
electricity generation prices, and consumer costs depending on the penetration
levels of non-dispatchable electricity sources. This review seeks to
appropriately highlight the broad vulnerabilities of EV aggregation alongside
its benefits, namely those regarding battery degradation, rebound peaks, and
cybersecurity. The holistic overview of EV aggregation provides comparisons
that balance expectations with realistic performance.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17072" title="Abstract">arXiv:2310.17072</a> [<a href="/pdf/2310.17072" title="Download PDF">pdf</a>, <a href="/format/2310.17072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Isometric Motion Manifold Primitives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yonghyeon Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 13 figures. This work has been submitted to the IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">The Motion Manifold Primitive (MMP) produces, for a given task, a continuous
manifold of trajectories each of which can successfully complete the task. It
consists of the decoder function that parametrizes the manifold and the
probability density in the latent coordinate space. In this paper, we first
show that the MMP performance can significantly degrade due to the geometric
distortion in the latent space -- by distortion, we mean that similar motions
are not located nearby in the latent space. We then propose {\it Isometric
Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the
geometry of the manifold. For this purpose, we formulate and use a Riemannian
metric for the motion space (i.e., parametric curve space), which we call a
{\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding
motions and pushing manipulation tasks show that IMMP significantly outperforms
existing MMP methods. Code is available at
https://github.com/Gabe-YHLee/IMMP-public.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17074" title="Abstract">arXiv:2310.17074</a> [<a href="/pdf/2310.17074" title="Download PDF">pdf</a>, <a href="/format/2310.17074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benign Oscillation of Stochastic Gradient Descent with Large Learning  Rates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+M">Miao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Beining Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+D">Difan Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 63 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this work, we theoretically investigate the generalization properties of
neural networks (NN) trained by stochastic gradient descent (SGD) algorithm
with large learning rates. Under such a training regime, our finding is that,
the oscillation of the NN weights caused by the large learning rate SGD
training turns out to be beneficial to the generalization of the NN, which
potentially improves over the same NN trained by SGD with small learning rates
that converges more smoothly. In view of this finding, we call such a
phenomenon "benign oscillation". Our theory towards demystifying such a
phenomenon builds upon the feature learning perspective of deep learning.
Specifically, we consider a feature-noise data generation model that consists
of (i) weak features which have a small $\ell_2$-norm and appear in each data
point; (ii) strong features which have a larger $\ell_2$-norm but only appear
in a certain fraction of all data points; and (iii) noise. We prove that NNs
trained by oscillating SGD with a large learning rate can effectively learn the
weak features in the presence of those strong features. In contrast, NNs
trained by SGD with a small learning rate can only learn the strong features
but makes little progress in learning the weak features. Consequently, when it
comes to the new testing data which consist of only weak features, the NN
trained by oscillating SGD with a large learning rate could still make correct
predictions consistently, while the NN trained by small learning rate SGD
fails. Our theory sheds light on how large learning rate training benefits the
generalization of NNs. Experimental results demonstrate our finding on "benign
oscillation".
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17075" title="Abstract">arXiv:2310.17075</a> [<a href="/pdf/2310.17075" title="Download PDF">pdf</a>, <a href="/format/2310.17075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperFields: Towards Zero-Shot Generation of NeRFs from Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Babu%2C+S">Sudarshan Babu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Richard Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Avery Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Maire%2C+M">Michael Maire</a>, 
<a href="/search/cs?searchtype=author&query=Shakhnarovich%2C+G">Greg Shakhnarovich</a>, 
<a href="/search/cs?searchtype=author&query=Hanocka%2C+R">Rana Hanocka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://threedle.github.io/hyperfields/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce HyperFields, a method for generating text-conditioned Neural
Radiance Fields (NeRFs) with a single forward pass and (optionally) some
fine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learns
a smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRF
distillation training, which distills scenes encoded in individual NeRFs into
one dynamic hypernetwork. These techniques enable a single network to fit over
a hundred unique scenes. We further demonstrate that HyperFields learns a more
general map between text and NeRFs, and consequently is capable of predicting
novel in-distribution and out-of-distribution scenes -- either zero-shot or
with a few finetuning steps. Finetuning HyperFields benefits from accelerated
convergence thanks to the learned general map, and is capable of synthesizing
novel scenes 5 to 10 times faster than existing neural optimization-based
methods. Our ablation experiments show that both the dynamic architecture and
NeRF distillation are critical to the expressivity of HyperFields.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17078" title="Abstract">arXiv:2310.17078</a> [<a href="/pdf/2310.17078" title="Download PDF">pdf</a>, <a href="/format/2310.17078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HCT: Hybrid Convnet-Transformer for Parkinson&#x27;s disease detection and  severity prediction from gait
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naimi%2C+S">Safwen Naimi</a>, 
<a href="/search/cs?searchtype=author&query=Bouachir%2C+W">Wassim Bouachir</a>, 
<a href="/search/cs?searchtype=author&query=Bilodeau%2C+G">Guillaume-Alexandre Bilodeau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, 3 tables, Accepted for publication in IEEE International Conference on Machine Learning and Applications (ICMLA), copyright IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we propose a novel deep learning method based on a new Hybrid
ConvNet-Transformer architecture to detect and stage Parkinson's disease (PD)
from gait data. We adopt a two-step approach by dividing the problem into two
sub-problems. Our Hybrid ConvNet-Transformer model first distinguishes healthy
versus parkinsonian patients. If the patient is parkinsonian, a multi-class
Hybrid ConvNet-Transformer model determines the Hoehn and Yahr (H&amp;Y) score to
assess the PD severity stage. Our hybrid architecture exploits the strengths of
both Convolutional Neural Networks (ConvNets) and Transformers to accurately
detect PD and determine the severity stage. In particular, we take advantage of
ConvNets to capture local patterns and correlations in the data, while we
exploit Transformers for handling long-term dependencies in the input signal.
We show that our hybrid method achieves superior performance when compared to
other state-of-the-art methods, with a PD detection accuracy of 97% and a
severity staging accuracy of 87%. Our source code is available at:
https://github.com/SafwenNaimi
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17080" title="Abstract">arXiv:2310.17080</a> [<a href="/pdf/2310.17080" title="Download PDF">pdf</a>, <a href="/format/2310.17080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automating lichen monitoring in ecological studies using instance  segmentation of time-lapse images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naimi%2C+S">Safwen Naimi</a>, 
<a href="/search/cs?searchtype=author&query=Koubaa%2C+O">Olfa Koubaa</a>, 
<a href="/search/cs?searchtype=author&query=Bouachir%2C+W">Wassim Bouachir</a>, 
<a href="/search/cs?searchtype=author&query=Bilodeau%2C+G">Guillaume-Alexandre Bilodeau</a>, 
<a href="/search/cs?searchtype=author&query=Jeddore%2C+G">Gregory Jeddore</a>, 
<a href="/search/cs?searchtype=author&query=Baines%2C+P">Patricia Baines</a>, 
<a href="/search/cs?searchtype=author&query=Correia%2C+D">David Correia</a>, 
<a href="/search/cs?searchtype=author&query=Arsenault%2C+A">Andre Arsenault</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 Figures, 8 Tables, Accepted for publication in IEEE International Conference on Machine Learning and Applications (ICMLA), copyright IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Lichens are symbiotic organisms composed of fungi, algae, and/or
cyanobacteria that thrive in a variety of environments. They play important
roles in carbon and nitrogen cycling, and contribute directly and indirectly to
biodiversity. Ecologists typically monitor lichens by using them as indicators
to assess air quality and habitat conditions. In particular, epiphytic lichens,
which live on trees, are key markers of air quality and environmental health. A
new method of monitoring epiphytic lichens involves using time-lapse cameras to
gather images of lichen populations. These cameras are used by ecologists in
Newfoundland and Labrador to subsequently analyze and manually segment the
images to determine lichen thalli condition and change. These methods are
time-consuming and susceptible to observer bias. In this work, we aim to
automate the monitoring of lichens over extended periods and to estimate their
biomass and condition to facilitate the task of ecologists. To accomplish this,
our proposed framework uses semantic segmentation with an effective training
approach to automate monitoring and biomass estimation of epiphytic lichens on
time-lapse images. We show that our method has the potential to significantly
improve the accuracy and efficiency of lichen population monitoring, making it
a valuable tool for forest ecologists and environmental scientists to evaluate
the impact of climate change on Canada's forests. To the best of our knowledge,
this is the first time that such an approach has been used to assist ecologists
in monitoring and analyzing epiphytic lichens.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17086" title="Abstract">arXiv:2310.17086</a> [<a href="/pdf/2310.17086" title="Download PDF">pdf</a>, <a href="/format/2310.17086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers Learn Higher-Order Optimization Methods for In-Context  Learning: A Study with Linear Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+D">Deqing Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tian-Qi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Robin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Sharan%2C+V">Vatsal Sharan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Transformers are remarkably good at in-context learning (ICL) -- learning
from demonstrations without parameter updates -- but how they perform ICL
remains a mystery. Recent work suggests that Transformers may learn in-context
by internally running Gradient Descent, a first-order optimization method. In
this paper, we instead demonstrate that Transformers learn to implement
higher-order optimization methods to perform ICL. Focusing on in-context linear
regression, we show that Transformers learn to implement an algorithm very
similar to Iterative Newton's Method, a higher-order optimization method,
rather than Gradient Descent. Empirically, we show that predictions from
successive Transformer layers closely match different iterations of Newton's
Method linearly, with each middle layer roughly computing 3 iterations. In
contrast, exponentially more Gradient Descent steps are needed to match an
additional Transformers layer; this suggests that Transformers have an
comparable rate of convergence with high-order methods such as Iterative
Newton, which are exponentially faster than Gradient Descent. We also show that
Transformers can learn in-context on ill-conditioned data, a setting where
Gradient Descent struggles but Iterative Newton succeeds. Finally, we show
theoretical results which support our empirical findings and have a close
correspondence with them: we prove that Transformers can implement $k$
iterations of Newton's method with $\mathcal{O}(k)$ layers.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17087" title="Abstract">arXiv:2310.17087</a> [<a href="/pdf/2310.17087" title="Download PDF">pdf</a>, <a href="/format/2310.17087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Good regularity creates large learning rate implicit biases: edge of  stability, balancing, and catapult
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenghao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Molei Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Large learning rates, when applied to gradient descent for nonconvex
optimization, yield various implicit biases including the edge of stability
(Cohen et al., 2021), balancing (Wang et al., 2022), and catapult (Lewkowycz et
al., 2020). These phenomena cannot be well explained by classical optimization
theory. Though significant theoretical progress has been made in understanding
these implicit biases, it remains unclear for which objective functions would
they occur. This paper provides an initial step in answering this question,
namely that these implicit biases are in fact various tips of the same iceberg.
They occur when the objective function of optimization has some good
regularity, which, in combination with a provable preference of large learning
rate gradient descent for moving toward flatter regions, results in these
nontrivial dynamical phenomena. To establish this result, we develop a new
global convergence theory under large learning rates, for a family of nonconvex
functions without globally Lipschitz continuous gradient, which was typically
assumed in existing convergence analysis. A byproduct is the first
non-asymptotic convergence rate bound for large-learning-rate gradient descent
optimization of nonconvex functions. We also validate our theory with
experiments on neural networks, where different losses, activation functions,
and batch normalization all can significantly affect regularity and lead to
very different training dynamics.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17089" title="Abstract">arXiv:2310.17089</a> [<a href="/pdf/2310.17089" title="Download PDF">pdf</a>, <a href="/format/2310.17089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pac-Sim: Simulation of Multi-threaded Workloads using Intelligent, Live  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sabu%2C+A">Alen Sabu</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+A">Akanksha Chaudhari</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Q">Qingxuan Kang</a>, 
<a href="/search/cs?searchtype=author&query=Carlson%2C+T+E">Trevor E. Carlson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">High-performance, multi-core processors are the key to accelerating workloads
in several application domains. To continue to scale performance at the limit
of Moore's Law and Dennard scaling, software and hardware designers have turned
to dynamic solutions that adapt to the needs of applications in a transparent,
automatic way. For example, modern hardware improves its performance and power
efficiency by changing the hardware configuration, like the frequency and
voltage of cores, according to a number of parameters such as the technology
used, the workload running, etc. With this level of dynamism, it is essential
to simulate next-generation multi-core processors in a way that can both
respond to system changes and accurately determine system performance metrics.
Currently, no sampled simulation platform can achieve these goals of dynamic,
fast, and accurate simulation of multi-threaded workloads.
<br />In this work, we propose a solution that allows for fast, accurate simulation
in the presence of both hardware and software dynamism. To accomplish this
goal, we present Pac-Sim, a novel sampled simulation methodology for fast,
accurate sampled simulation that requires no upfront analysis of the workload.
With our proposed methodology, it is now possible to simulate long-running
dynamically scheduled multi-threaded programs with significant simulation
speedups even in the presence of dynamic hardware events. We evaluate Pac-Sim
using the multi-threaded SPEC CPU2017, NPB, and PARSEC benchmarks with both
static and dynamic thread scheduling. The experimental results show that
Pac-Sim achieves a very low sampling error of 1.63% and 3.81% on average for
statically and dynamically scheduled benchmarks, respectively. Pac-Sim also
demonstrates significant simulation speedups as high as 523.5$\times$
(210.3$\times$ on average) for the train input set of SPEC CPU2017.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17091" title="Abstract">arXiv:2310.17091</a> [<a href="/pdf/2310.17091" title="Download PDF">pdf</a>, <a href="/format/2310.17091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting stealthy cyberattacks on adaptive cruise control vehicles: A  machine learning approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+M">Mingfeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Stern%2C+R">Raphael Stern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the advent of vehicles equipped with advanced driver-assistance systems,
such as adaptive cruise control (ACC) and other automated driving features, the
potential for cyberattacks on these automated vehicles (AVs) has emerged. While
overt attacks that force vehicles to collide may be easily identified, more
insidious attacks, which only slightly alter driving behavior, can result in
network-wide increases in congestion, fuel consumption, and even crash risk
without being easily detected. To address the detection of such attacks, we
first present a traffic model framework for three types of potential
cyberattacks: malicious manipulation of vehicle control commands, false data
injection attacks on sensor measurements, and denial-of-service (DoS) attacks.
We then investigate the impacts of these attacks at both the individual vehicle
(micro) and traffic flow (macro) levels. A novel generative adversarial network
(GAN)-based anomaly detection model is proposed for real-time identification of
such attacks using vehicle trajectory data. We provide numerical evidence {to
demonstrate} the efficacy of our machine learning approach in detecting
cyberattacks on ACC-equipped vehicles. The proposed method is compared against
some recently proposed neural network models and observed to have higher
accuracy in identifying anomalous driving behaviors of ACC vehicles.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17097" title="Abstract">arXiv:2310.17097</a> [<a href="/pdf/2310.17097" title="Download PDF">pdf</a>, <a href="/format/2310.17097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating Data Heterogeneity in Federated Learning: A Semi-Supervised  Approach for Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+E">Eric Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+C">Christian Lau</a>, 
<a href="/search/cs?searchtype=author&query=Mugunthan%2C+V">Vaikkunth Mugunthan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) has emerged as a potent framework for training models
across distributed data sources while maintaining data privacy. Nevertheless,
it faces challenges with limited high-quality labels and non-IID client data,
particularly in applications like autonomous driving. To address these hurdles,
we navigate the uncharted waters of Semi-Supervised Federated Object Detection
(SSFOD). We present a pioneering SSFOD framework, designed for scenarios where
labeled data reside only at the server while clients possess unlabeled data.
Notably, our method represents the inaugural implementation of SSFOD for
clients with 0% labeled non-IID data, a stark contrast to previous studies that
maintain some subset of labels at each client. We propose FedSTO, a two-stage
strategy encompassing Selective Training followed by Orthogonally enhanced
full-parameter training, to effectively address data shift (e.g. weather
conditions) between server and clients. Our contributions include selectively
refining the backbone of the detector to avert overfitting, orthogonality
regularization to boost representation divergence, and local EMA-driven pseudo
label assignment to yield high-quality pseudo labels. Extensive validation on
prominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M)
attests to the efficacy of our approach, demonstrating state-of-the-art
results. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as
well as fully-supervised centralized training methods.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17100" title="Abstract">arXiv:2310.17100</a> [<a href="/pdf/2310.17100" title="Download PDF">pdf</a>, <a href="/ps/2310.17100" title="Download PostScript">ps</a>, <a href="/format/2310.17100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Design through Graph Neural Networks: Identifying Challenges and  Improving Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loveland%2C+D">Donald Loveland</a>, 
<a href="/search/cs?searchtype=author&query=Caceres%2C+R">Rajmonda Caceres</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Network (GNN) research has produced strategies to modify a
graph's edges using gradients from a trained GNN, with the goal of network
design. However, the factors which govern gradient-based editing are
understudied, obscuring why edges are chosen and if edits are grounded in an
edge's importance. Thus, we begin by analyzing the gradient computation in
previous works, elucidating the factors that influence edits and highlighting
the potential over-reliance on structural properties. Specifically, we find
that edges can achieve high gradients due to structural biases, rather than
importance, leading to erroneous edits when the factors are unrelated to the
design task. To improve editing, we propose ORE, an iterative editing method
that (a) edits the highest scoring edges and (b) re-embeds the edited graph to
refresh gradients, leading to less biased edge choices. We empirically study
ORE through a set of proposed design tasks, each with an external validation
method, demonstrating that ORE improves upon previous methods by up to 50%.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17109" title="Abstract">arXiv:2310.17109</a> [<a href="/pdf/2310.17109" title="Download PDF">pdf</a>, <a href="/format/2310.17109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LP-OVOD: Open-Vocabulary Object Detection by Linear Probing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+C">Chau Pham</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Truong Vu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khoi Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper addresses the challenging problem of open-vocabulary object
detection (OVOD) where an object detector must identify both seen and unseen
classes in test images without labeled examples of the unseen classes in
training. A typical approach for OVOD is to use joint text-image embeddings of
CLIP to assign box proposals to their closest text label. However, this method
has a critical issue: many low-quality boxes, such as over- and
under-covered-object boxes, have the same similarity score as high-quality
boxes since CLIP is not trained on exact object location information. To
address this issue, we propose a novel method, LP-OVOD, that discards
low-quality boxes by training a sigmoid linear classifier on pseudo labels
retrieved from the top relevant region proposals to the novel text.
Experimental results on COCO affirm the superior performance of our approach
over the state of the art, achieving $\textbf{40.5}$ in $\text{AP}_{novel}$
using ResNet50 as the backbone and without external datasets or knowing novel
classes during training. Our code will be available at
https://github.com/VinAIResearch/LP-OVOD.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17110" title="Abstract">arXiv:2310.17110</a> [<a href="/pdf/2310.17110" title="Download PDF">pdf</a>, <a href="/format/2310.17110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yijian Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Simin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenwu Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In an era marked by the increasing adoption of Large Language Models (LLMs)
for various tasks, there is a growing focus on exploring LLMs' capabilities in
handling web data, particularly graph data. Dynamic graphs, which capture
temporal network evolution patterns, are ubiquitous in real-world web data.
Evaluating LLMs' competence in understanding spatial-temporal information on
dynamic graphs is essential for their adoption in web applications, which
remains unexplored in the literature. In this paper, we bridge the gap via
proposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic
graphs, to the best of our knowledge, for the first time. Specifically, we
propose the LLM4DyG benchmark, which includes nine specially designed tasks
considering the capability evaluation of LLMs from both temporal and spatial
dimensions. Then, we conduct extensive experiments to analyze the impacts of
different data generators, data statistics, prompting techniques, and LLMs on
the model performance. Finally, we propose Disentangled Spatial-Temporal
Thoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporal
understanding abilities. Our main observations are: 1) LLMs have preliminary
spatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph
tasks show increasing difficulties for LLMs as the graph size and density
increase, while not sensitive to the time span and data generation mechanism,
3) the proposed DST2 prompting method can help to improve LLMs'
spatial-temporal understanding abilities on dynamic graphs for most tasks. The
data and codes will be open-sourced at publication time.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17115" title="Abstract">arXiv:2310.17115</a> [<a href="/pdf/2310.17115" title="Download PDF">pdf</a>, <a href="/format/2310.17115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Robotic Assembly Sequence Planning: A Sequential Decision-Making  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nagpal%2C+K">Kartik Nagpal</a>, 
<a href="/search/cs?searchtype=author&query=Mehr%2C+N">Negar Mehr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 conference page paper, 3 page appendix, 23 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The optimal robot assembly planning problem is challenging due to the
necessity of finding the optimal solution amongst an exponentially vast number
of possible plans, all while satisfying a selection of constraints.
Traditionally, robotic assembly planning problems have been solved using
heuristics, but these methods are specific to a given objective structure or
set of problem parameters. In this paper, we propose a novel approach to
robotic assembly planning that poses assembly sequencing as a sequential
decision making problem, enabling us to harness methods that far outperform the
state-of-the-art. We formulate the problem as a Markov Decision Process (MDP)
and utilize Dynamic Programming (DP) to find optimal assembly policies for
moderately sized strictures. We further expand our framework to exploit the
deterministic nature of assembly planning and introduce a class of optimal
Graph Exploration Assembly Planners (GEAPs). For larger structures, we show how
Reinforcement Learning (RL) enables us to learn policies that generate high
reward assembly sequences. We evaluate our approach on a variety of robotic
assembly problems, such as the assembly of the Hubble Space Telescope, the
International Space Station, and the James Webb Space Telescope. We further
showcase how our DP, GEAP, and RL implementations are capable of finding
optimal solutions under a variety of different objective functions and how our
formulation allows us to translate precedence constraints to branch pruning and
thus further improve performance. We have published our code at
https://github.com/labicon/ORASP-Code.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17117" title="Abstract">arXiv:2310.17117</a> [<a href="/pdf/2310.17117" title="Download PDF">pdf</a>, <a href="/ps/2310.17117" title="Download PostScript">ps</a>, <a href="/format/2310.17117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Cumulative Distribution Function Based Method for Random Drift Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Duan%2C+C">Chenghua Duan</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+C">Chun Liu</a>, 
<a href="/search/math?searchtype=author&query=Yue%2C+X">Xingye Yue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we propose a numerical method to uniformly handle the random
genetic drift model for pure drift with or without natural selection and
mutation.
<br />For pure drift and natural selection case, the Dirac $\delta$ singularity
will develop at two boundary ends and the mass lumped at the two ends stands
for the fixation probability. For the one-way mutation case, known as Muller's
ratchet, the accumulation of deleterious mutations leads to the loss of the
fittest gene, the Dirac $\delta$ singularity will spike only at one boundary
end, which stands for the fixation of the deleterious gene and loss of the
fittest one. For two-way mutation case, the singularity with negative power law
may emerge near boundary points. We first rewrite the original model on the
probability density function (PDF) to one with respect to the cumulative
distribution function (CDF). Dirac $\delta$ singularity of the PDF becomes the
discontinuity of the CDF. Then we establish a upwind scheme, which keeps the
total probability, is positivity preserving and unconditionally stable. For
pure drift, the scheme also keeps the conservation of expectation. It can catch
the discontinuous jump of the CDF, then predicts accurately the fixation
probability for pure drift with or without natural selection and one-way
mutation. For two-way mutation case, it can catch the power law of the
singularity. %Moreover, some artificial algorithms or additional boundary
criteria is not needed in the numerical simulation. The numerical results show
the effectiveness of the scheme.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17119" title="Abstract">arXiv:2310.17119</a> [<a href="/pdf/2310.17119" title="Download PDF">pdf</a>, <a href="/format/2310.17119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLEEK: Factual Error Detection and Correction with Evidence Retrieved  from External Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bayat%2C+F+F">Farima Fatahi Bayat</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kun Qian</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Benjamin Han</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+Y">Yisi Sang</a>, 
<a href="/search/cs?searchtype=author&query=Belyi%2C+A">Anton Belyi</a>, 
<a href="/search/cs?searchtype=author&query=Khorshidi%2C+S">Samira Khorshidi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ilyas%2C+I+F">Ihab F. Ilyas</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunyao Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Demonstration Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Detecting factual errors in textual information, whether generated by large
language models (LLM) or curated by humans, is crucial for making informed
decisions. LLMs' inability to attribute their claims to external knowledge and
their tendency to hallucinate makes it difficult to rely on their responses.
Humans, too, are prone to factual errors in their writing. Since manual
detection and correction of factual errors is labor-intensive, developing an
automatic approach can greatly reduce human effort. We present FLEEK, a
prototype tool that automatically extracts factual claims from text, gathers
evidence from external knowledge sources, evaluates the factuality of each
claim, and suggests revisions for identified errors using the collected
evidence. Initial empirical evaluation on fact error detection (77-85\% F1)
shows the potential of FLEEK. A video demo of FLEEK can be found at
https://youtu.be/NapJFUlkPdQ.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17120" title="Abstract">arXiv:2310.17120</a> [<a href="/pdf/2310.17120" title="Download PDF">pdf</a>, <a href="/format/2310.17120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topic Segmentation of Semi-Structured and Unstructured Conversational  Datasets using Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+R">Reshmi Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Kajal%2C+H+S">Harjeet Singh Kajal</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+S">Sharanya Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+D">Dhuri Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+S">Samyadeep Basu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H">Hansi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+S">Soundararajan Srinivasan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IntelliSys 2023. arXiv admin note: substantial text overlap with <a href="/abs/2211.14954">arXiv:2211.14954</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Breaking down a document or a conversation into multiple contiguous segments
based on its semantic structure is an important and challenging problem in NLP,
which can assist many downstream tasks. However, current works on topic
segmentation often focus on segmentation of structured texts. In this paper, we
comprehensively analyze the generalization capabilities of state-of-the-art
topic segmentation models on unstructured texts. We find that: (a) Current
strategies of pre-training on a large corpus of structured text such as
Wiki-727K do not help in transferability to unstructured conversational data.
(b) Training from scratch with only a relatively small-sized dataset of the
target unstructured domain improves the segmentation results by a significant
margin. We stress-test our proposed Topic Segmentation approach by
experimenting with multiple loss functions, in order to mitigate effects of
imbalance in unstructured conversational datasets. Our empirical evaluation
indicates that Focal Loss function is a robust alternative to Cross-Entropy and
re-weighted Cross-Entropy loss function when segmenting unstructured and
semi-structured chats.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17121" title="Abstract">arXiv:2310.17121</a> [<a href="/pdf/2310.17121" title="Download PDF">pdf</a>, <a href="/format/2310.17121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-time Augmentation for Factual Probing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamoda%2C+G">Go Kamoda</a>, 
<a href="/search/cs?searchtype=author&query=Heinzerling%2C+B">Benjamin Heinzerling</a>, 
<a href="/search/cs?searchtype=author&query=Sakaguchi%2C+K">Keisuke Sakaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Inui%2C+K">Kentaro Inui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures, accepted to EMNLP 2023 Findings (short paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Factual probing is a method that uses prompts to test if a language model
"knows" certain world knowledge facts. A problem in factual probing is that
small changes to the prompt can lead to large changes in model output. Previous
work aimed to alleviate this problem by optimizing prompts via text mining or
fine-tuning. However, such approaches are relation-specific and do not
generalize to unseen relation types. Here, we propose to use test-time
augmentation (TTA) as a relation-agnostic method for reducing sensitivity to
prompt variations by automatically augmenting and ensembling prompts at test
time. Experiments show improved model calibration, i.e., with TTA, model
confidence better reflects prediction accuracy. Improvements in prediction
accuracy are observed for some models, but for other models, TTA leads to
degradation. Error analysis identifies the difficulty of producing high-quality
prompt variations as the main challenge for TTA.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17126" title="Abstract">arXiv:2310.17126</a> [<a href="/pdf/2310.17126" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning on SAR Imagery: Transfer Learning Versus Randomly  Initialized Weights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karimzadeh%2C+M">Morteza Karimzadeh</a>, 
<a href="/search/cs?searchtype=author&query=de+Lima%2C+R+P">Rafael Pires de Lima</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IGARSS 2023 - 2023 IEEE International Geoscience and Remote
  Sensing Symposium, Pasadena, CA, USA, 2023, pp. 1983-1986
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Deploying deep learning on Synthetic Aperture Radar (SAR) data is becoming
more common for mapping purposes. One such case is sea ice, which is highly
dynamic and rapidly changes as a result of the combined effect of wind,
temperature, and ocean currents. Therefore, frequent mapping of sea ice is
necessary to ensure safe marine navigation. However, there is a general
shortage of expert-labeled data to train deep learning algorithms. Fine-tuning
a pre-trained model on SAR imagery is a potential solution. In this paper, we
compare the performance of deep learning models trained from scratch using
randomly initialized weights against pre-trained models that we fine-tune for
this purpose. Our results show that pre-trained models lead to better results,
especially on test samples from the melt season.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17127" title="Abstract">arXiv:2310.17127</a> [<a href="/pdf/2310.17127" title="Download PDF">pdf</a>, <a href="/format/2310.17127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Method for Network Intrusion Detection Using Flow Sequence and BERT  Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+L+G">Loc Gia Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Watabe%2C+K">Kohei Watabe</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICC 2023 - IEEE International Conference on Communications, Rome,
  Italy, 2023, pp. 3006-3011
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">A Network Intrusion Detection System (NIDS) is a tool that identifies
potential threats to a network. Recently, different flow-based NIDS designs
utilizing Machine Learning (ML) algorithms have been proposed as solutions to
detect intrusions efficiently. However, conventional ML-based classifiers have
not seen widespread adoption in the real world due to their poor domain
adaptation capability. In this research, our goal is to explore the possibility
of using sequences of flows to improve the domain adaptation capability of
network intrusion detection systems. Our proposal employs natural language
processing techniques and Bidirectional Encoder Representations from
Transformers framework, which is an effective technique for modeling data with
respect to its context. Early empirical results show that our approach has
improved domain adaptation capability compared to previous approaches. The
proposed approach provides a new research method for building a robust
intrusion detection system.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17128" title="Abstract">arXiv:2310.17128</a> [<a href="/pdf/2310.17128" title="Download PDF">pdf</a>, <a href="/format/2310.17128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-driven Prompt Evolution for Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sathish%2C+R">Rachana Sathish</a>, 
<a href="/search/cs?searchtype=author&query=Venkataramani%2C+R">Rahul Venkataramani</a>, 
<a href="/search/cs?searchtype=author&query=Shriram%2C+K+S">K S Shriram</a>, 
<a href="/search/cs?searchtype=author&query=Sudhakar%2C+P">Prasad Sudhakar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Promptable foundation models, particularly Segment Anything Model (SAM), have
emerged as a promising alternative to the traditional task-specific supervised
learning for image segmentation. However, many evaluation studies have found
that their performance on medical imaging modalities to be underwhelming
compared to conventional deep learning methods. In the world of large
pre-trained language and vision-language models, learning prompt from
downstream tasks has achieved considerable success in improving performance. In
this work, we propose a plug-and-play Prompt Optimization Technique for
foundation models like SAM (SAMPOT) that utilizes the downstream segmentation
task to optimize the human-provided prompt to obtain improved performance. We
demonstrate the utility of SAMPOT on lung segmentation in chest X-ray images
and obtain an improvement on a significant number of cases ($\sim75\%$) over
human-provided initial prompts. We hope this work will lead to further
investigations in the nascent field of automatic visual prompt-tuning.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17129" title="Abstract">arXiv:2310.17129</a> [<a href="/pdf/2310.17129" title="Download PDF">pdf</a>, <a href="/format/2310.17129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECN based Congestion Control for a Software Defined Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moorthy%2C+M+P+S">Mohana Prasad Sathya Moorthy</a>, 
<a href="/search/cs?searchtype=author&query=Sure%2C+M+K">Manoj Kumar Sure</a>, 
<a href="/search/cs?searchtype=author&query=Sivalingam%2C+K+M">Krishna M. Sivalingam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">This paper deals with congestion control in a software defined network (SDN)
setting. Presently, explicit router schemes, such as Explicit Congestion
Notification (ECN), work in conjunction with the TCP protocol to handle
congestion in a distributed manner. With the emergence of SDN and centralized
control, it is possible to leverage the global view of the network state to
make better congestion control decisions. In this work, we explore the
advantages of bringing in global information into distributed congestion
control. We propose a framework where the controller with its global view of
the network actively participates in the congestion control decisions of the
end TCP hosts, by setting the ECN bits of IP packets appropriately. Our
framework can be deployed very easily without any change to the end node TCPs
or the SDN switches. We also show 30x improvement over the TCP Cubic variant
and 1.7x improvement over TCP/RED in terms of flow completion times for one
implementation of this framework, using the Mininet emulator.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17130" title="Abstract">arXiv:2310.17130</a> [<a href="/pdf/2310.17130" title="Download PDF">pdf</a>, <a href="/format/2310.17130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M2C: Towards Automatic Multimodal Manga Complement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hongcheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jiaqi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhoujun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023. arXiv admin note: text overlap with <a href="/abs/2210.15461">arXiv:2210.15461</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multimodal manga analysis focuses on enhancing manga understanding with
visual and textual features, which has attracted considerable attention from
both natural language processing and computer vision communities. Currently,
most comics are hand-drawn and prone to problems such as missing pages, text
contamination, and aging, resulting in missing comic text content and seriously
hindering human comprehension. In other words, the Multimodal Manga Complement
(M2C) task has not been investigated, which aims to handle the aforementioned
issues by providing a shared semantic space for vision and language
understanding. To this end, we first propose the Multimodal Manga Complement
task by establishing a new M2C benchmark dataset covering two languages. First,
we design a manga argumentation method called MCoT to mine event knowledge in
comics with large language models. Then, an effective baseline FVP-M$^{2}$
using fine-grained visual prompts is proposed to support manga complement.
Extensive experimental results show the effectiveness of FVP-M$^{2}$ method for
Multimodal Mange Complement.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17131" title="Abstract">arXiv:2310.17131</a> [<a href="/pdf/2310.17131" title="Download PDF">pdf</a>, <a href="/format/2310.17131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Virtual Accessory Try-On via Keypoint Hallucination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gou%2C+J">Junhong Gou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Li Niu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianfu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+J">Jianlou Si</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liqing Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The virtual try-on task refers to fitting the clothes from one image onto
another portrait image. In this paper, we focus on virtual accessory try-on,
which fits accessory (e.g., glasses, ties) onto a face or portrait image.
Unlike clothing try-on, which relies on human silhouette as guidance, accessory
try-on warps the accessory into an appropriate location and shape to generate a
plausible composite image. In contrast to previous try-on methods that treat
foreground (i.e., accessories) and background (i.e., human faces or bodies)
equally, we propose a background-oriented network to utilize the prior
knowledge of human bodies and accessories. Specifically, our approach learns
the human body priors and hallucinates the target locations of specified
foreground keypoints in the background. Then our approach will inject
foreground information with accessory priors into the background UNet. Based on
the hallucinated target locations, the warping parameters are calculated to
warp the foreground. Moreover, this background-oriented network can also easily
incorporate auxiliary human face/body semantic segmentation supervision to
further boost performance. Experiments conducted on STRAT dataset validate the
effectiveness of our proposed method.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17132" title="Abstract">arXiv:2310.17132</a> [<a href="/pdf/2310.17132" title="Download PDF">pdf</a>, <a href="/format/2310.17132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the potential of GNNs via Bi-directional Knowledge Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shuai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhizhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhenfeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yao Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Based on the message-passing paradigm, there has been an amount of research
proposing diverse and impressive feature propagation mechanisms to improve the
performance of GNNs. However, less focus has been put on feature
transformation, another major operation of the message-passing framework. In
this paper, we first empirically investigate the performance of the feature
transformation operation in several typical GNNs. Unexpectedly, we notice that
GNNs do not completely free up the power of the inherent feature transformation
operation. By this observation, we propose the Bi-directional Knowledge
Transfer (BiKT), a plug-and-play approach to unleash the potential of the
feature transformation operations without modifying the original architecture.
Taking the feature transformation operation as a derived representation
learning model that shares parameters with the original GNN, the direct
prediction by this model provides a topological-agnostic knowledge feedback
that can further instruct the learning of GNN and the feature transformations
therein. On this basis, BiKT not only allows us to acquire knowledge from both
the GNN and its derived model but promotes each other by injecting the
knowledge into the other. In addition, a theoretical analysis is further
provided to demonstrate that BiKT improves the generalization bound of the GNNs
from the perspective of domain adaption. An extensive group of experiments on
up to 7 datasets with 5 typical GNNs demonstrates that BiKT brings up to 0.5% -
4% performance gain over the original GNN, which means a boosted GNN is
obtained. Meanwhile, the derived model also shows a powerful performance to
compete with or even surpass the original GNN, enabling us to flexibly apply it
independently to some other specific downstream tasks.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17133" title="Abstract">arXiv:2310.17133</a> [<a href="/pdf/2310.17133" title="Download PDF">pdf</a>, <a href="/format/2310.17133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating Probing Signals into Multimodal Machine Translation via  Visual Question-Answering Pairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zuo%2C+Y">Yuxin Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bei Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Chuanhao Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jingbo Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents an in-depth study of multimodal machine translation
(MMT), examining the prevailing understanding that MMT systems exhibit
decreased sensitivity to visual information when text inputs are complete.
Instead, we attribute this phenomenon to insufficient cross-modal interaction,
rather than image information redundancy. A novel approach is proposed to
generate parallel Visual Question-Answering (VQA) style pairs from the source
text, fostering more robust cross-modal interaction. Using Large Language
Models (LLMs), we explicitly model the probing signal in MMT to convert it into
VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask
learning framework is introduced to incorporate explicit probing signals from
the dataset into the MMT training process. Experimental results on two
widely-used benchmarks demonstrate the effectiveness of this novel approach.
Our code and data would be available at:
\url{https://github.com/libeineu/MMT-VQA}.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17135" title="Abstract">arXiv:2310.17135</a> [<a href="/pdf/2310.17135" title="Download PDF">pdf</a>, <a href="/format/2310.17135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of Cross-Entropy, Dice, and Focal Loss for Sea Ice Type  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Lima%2C+R+P">Rafael Pires de Lima</a>, 
<a href="/search/cs?searchtype=author&query=Vahedi%2C+B">Behzad Vahedi</a>, 
<a href="/search/cs?searchtype=author&query=Karimzadeh%2C+M">Morteza Karimzadeh</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IGARSS 2023 - 2023 IEEE International Geoscience and Remote
  Sensing Symposium, 2023, pp. 145-148
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Up-to-date sea ice charts are crucial for safer navigation in ice-infested
waters. Recently, Convolutional Neural Network (CNN) models show the potential
to accelerate the generation of ice maps for large regions. However, results
from CNN models still need to undergo scrutiny as higher metrics performance
not always translate to adequate outputs. Sea ice type classes are imbalanced,
requiring special treatment during training. We evaluate how three different
loss functions, some developed for imbalanced class problems, affect the
performance of CNN models trained to predict the dominant ice type in
Sentinel-1 images. Despite the fact that Dice and Focal loss produce higher
metrics, results from cross-entropy seem generally more physically consistent.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17136" title="Abstract">arXiv:2310.17136</a> [<a href="/pdf/2310.17136" title="Download PDF">pdf</a>, <a href="/format/2310.17136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Core Challenge 2023: Solver and Graph Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soh%2C+T">Takehide Soh</a>, 
<a href="/search/cs?searchtype=author&query=Okamoto%2C+Y">Yoshio Okamoto</a>, 
<a href="/search/cs?searchtype=author&query=Ito%2C+T">Takehiro Ito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2208.02495">arXiv:2208.02495</a>, <a href="/abs/2207.13959">arXiv:2207.13959</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">This paper collects all descriptions of solvers and ISR instances submitted
to CoRe Challenge 2023.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17137" title="Abstract">arXiv:2310.17137</a> [<a href="/pdf/2310.17137" title="Download PDF">pdf</a>, <a href="/ps/2310.17137" title="Download PostScript">ps</a>, <a href="/format/2310.17137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Scale Gaussian Processes via Alternating Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kaiwen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wenger%2C+J">Jonathan Wenger</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+H">Haydn Jones</a>, 
<a href="/search/cs?searchtype=author&query=Pleiss%2C+G">Geoff Pleiss</a>, 
<a href="/search/cs?searchtype=author&query=Gardner%2C+J+R">Jacob R. Gardner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Gaussian process (GP) hyperparameter optimization requires repeatedly solving
linear systems with $n \times n$ kernel matrices. To address the prohibitive
$\mathcal{O}(n^3)$ time complexity, recent work has employed fast iterative
numerical methods, like conjugate gradients (CG). However, as datasets increase
in magnitude, the corresponding kernel matrices become increasingly
ill-conditioned and still require $\mathcal{O}(n^2)$ space without
partitioning. Thus, while CG increases the size of datasets GPs can be trained
on, modern datasets reach scales beyond its applicability. In this work, we
propose an iterative method which only accesses subblocks of the kernel matrix,
effectively enabling \emph{mini-batching}. Our algorithm, based on alternating
projection, has $\mathcal{O}(n)$ per-iteration time and space complexity,
solving many of the practical challenges of scaling GPs to very large datasets.
Theoretically, we prove our method enjoys linear convergence and empirically we
demonstrate its robustness to ill-conditioning. On large-scale benchmark
datasets up to four million datapoints our approach accelerates training by a
factor of 2$\times$ to 27$\times$ compared to CG.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17138" title="Abstract">arXiv:2310.17138</a> [<a href="/pdf/2310.17138" title="Download PDF">pdf</a>, <a href="/format/2310.17138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Classifier Using Global Character Level and Local Sub-unit Level  Features for Hindi Online Handwritten Character Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Anand Sharma</a> (MIET, Meerut), 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+A+G">A. G. Ramakrishnan</a> (IISc, Bengaluru)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 8 jpg figures. arXiv admin note: text overlap with <a href="/abs/2310.08222">arXiv:2310.08222</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A classifier is developed that defines a joint distribution of global
character features, number of sub-units and local sub-unit features to model
Hindi online handwritten characters. The classifier uses latent variables to
model the structure of sub-units. The classifier uses histograms of points,
orientations, and dynamics of orientations (HPOD) features to represent
characters at global character level and local sub-unit level and is
independent of character stroke order and stroke direction variations. The
parameters of the classifier is estimated using maximum likelihood method.
Different classifiers and features used in other studies are considered in this
study for classification performance comparison with the developed classifier.
The classifiers considered are Second Order Statistics (SOS), Sub-space (SS),
Fisher Discriminant (FD), Feedforward Neural Network (FFN) and Support Vector
Machines (SVM) and the features considered are Spatio Temporal (ST), Discrete
Fourier Transform (DFT), Discrete Cosine Transform (SCT), Discrete Wavelet
Transform (DWT), Spatial (SP) and Histograms of Oriented Gradients (HOG). Hindi
character datasets used for training and testing the developed classifier
consist of samples of handwritten characters from 96 different character
classes. There are 12832 samples with an average of 133 samples per character
class in the training set and 2821 samples with an average of 29 samples per
character class in the testing set. The developed classifier has the highest
accuracy of 93.5\% on the testing set compared to that of the classifiers
trained on different features extracted from the same training set and
evaluated on the same testing set considered in this study.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17139" title="Abstract">arXiv:2310.17139</a> [<a href="/pdf/2310.17139" title="Download PDF">pdf</a>, <a href="/format/2310.17139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Addressing the Pitfalls of Bisimulation-based  Representations in Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zang%2C+H">Hongyu Zang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Leiji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Baigui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+R">Riashat Islam</a>, 
<a href="/search/cs?searchtype=author&query=Combes%2C+R+T+d">Remi Tachet des Combes</a>, 
<a href="/search/cs?searchtype=author&query=Laroche%2C+R">Romain Laroche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While bisimulation-based approaches hold promise for learning robust state
representations for Reinforcement Learning (RL) tasks, their efficacy in
offline RL tasks has not been up to par. In some instances, their performance
has even significantly underperformed alternative methods. We aim to understand
why bisimulation methods succeed in online settings, but falter in offline
tasks. Our analysis reveals that missing transitions in the dataset are
particularly harmful to the bisimulation principle, leading to ineffective
estimation. We also shed light on the critical role of reward scaling in
bounding the scale of bisimulation measurements and of the value error they
induce. Based on these findings, we propose to apply the expectile operator for
representation learning to our offline RL setting, which helps to prevent
overfitting to incomplete data. Meanwhile, by introducing an appropriate reward
scaling strategy, we avoid the risk of feature collapse in representation
space. We implement these recommendations on two state-of-the-art
bisimulation-based algorithms, MICo and SimSR, and demonstrate performance
gains on two benchmark suites: D4RL and Visual D4RL. Codes are provided at
\url{https://github.com/zanghyu/Offline_Bisimulation}.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17140" title="Abstract">arXiv:2310.17140</a> [<a href="/pdf/2310.17140" title="Download PDF">pdf</a>, <a href="/format/2310.17140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbolic Planning and Code Generation for Grounded Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiu%2C+J+T">Justin T. Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenting Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Derek Chen</a>, 
<a href="/search/cs?searchtype=author&query=Vaduguru%2C+S">Saujas Vaduguru</a>, 
<a href="/search/cs?searchtype=author&query=Rush%2C+A+M">Alexander M. Rush</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+D">Daniel Fried</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) excel at processing and generating both text and
code. However, LLMs have had limited applicability in grounded task-oriented
dialogue as they are difficult to steer toward task objectives and fail to
handle novel grounding. We present a modular and interpretable grounded
dialogue system that addresses these shortcomings by composing LLMs with a
symbolic planner and grounded code execution. Our system consists of a reader
and planner: the reader leverages an LLM to convert partner utterances into
executable code, calling functions that perform grounding. The translated
code's output is stored to track dialogue state, while a symbolic planner
determines the next appropriate response. We evaluate our system's performance
on the demanding OneCommon dialogue task, involving collaborative reference
resolution on abstract images of scattered dots. Our system substantially
outperforms the previous state-of-the-art, including improving task success in
human evaluations from 56% to 69% in the most challenging setting.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17143" title="Abstract">arXiv:2310.17143</a> [<a href="/pdf/2310.17143" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supercharging academic writing with generative AI: framework,  techniques, and caveats
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhicheng Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 2 figures, 1 table, 1 box
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Academic writing is an indispensable yet laborious part of the research
enterprise. This Perspective maps out principles and methods for using
generative artificial intelligence (AI), specifically large language models
(LLMs), to elevate the quality and efficiency of academic writing. We introduce
a human-AI collaborative framework that delineates the rationale (why), process
(how), and nature (what) of AI engagement in writing. The framework pinpoints
both short-term and long-term reasons for engagement and their underlying
mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals
the role of AI throughout the writing process, conceptualized through a
two-stage model for human-AI collaborative writing, and the nature of AI
assistance in writing, represented through a model of writing-assistance types
and levels. Building on this framework, we describe effective prompting
techniques for incorporating AI into the writing routine (outlining, drafting,
and editing) as well as strategies for maintaining rigorous scholarship,
adhering to varied journal policies, and avoiding overreliance on AI.
Ultimately, the prudent integration of AI into academic writing can ease the
communication burden, empower authors, accelerate discovery, and promote
diversity in science.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17146" title="Abstract">arXiv:2310.17146</a> [<a href="/pdf/2310.17146" title="Download PDF">pdf</a>, <a href="/format/2310.17146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual-Augmented Importance Sampling for Semi-Offline Policy  Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shengpu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wiens%2C+J">Jenna Wiens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 12 figures, 5 tables. NeurIPS 2023. Code available at <a href="https://github.com/MLD3/CounterfactualAnnot-SemiOPE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In applying reinforcement learning (RL) to high-stakes domains, quantitative
and qualitative evaluation using observational data can help practitioners
understand the generalization performance of new policies. However, this type
of off-policy evaluation (OPE) is inherently limited since offline data may not
reflect the distribution shifts resulting from the application of new policies.
On the other hand, online evaluation by collecting rollouts according to the
new policy is often infeasible, as deploying new policies in these domains can
be unsafe. In this work, we propose a semi-offline evaluation framework as an
intermediate step between offline and online evaluation, where human users
provide annotations of unobserved counterfactual trajectories. While tempting
to simply augment existing data with such annotations, we show that this naive
approach can lead to biased results. Instead, we design a new family of OPE
estimators based on importance sampling (IS) and a novel weighting scheme that
incorporate counterfactual annotations without introducing additional bias. We
analyze the theoretical properties of our approach, showing its potential to
reduce both bias and variance compared to standard IS estimators. Our analyses
reveal important practical considerations for handling biased, noisy, or
missing annotations. In a series of proof-of-concept experiments involving
bandits and a healthcare-inspired simulator, we demonstrate that our approach
outperforms purely offline IS estimators and is robust to imperfect
annotations. Our framework, combined with principled human-centered design of
annotation solicitation, can enable the application of RL in high-stakes
domains.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17147" title="Abstract">arXiv:2310.17147</a> [<a href="/pdf/2310.17147" title="Download PDF">pdf</a>, <a href="/format/2310.17147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Baselines for Projection-based Full-reference and No-reference  Point Cloud Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+X">Xiongkuo Min</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Point clouds are widely used in 3D content representation and have various
applications in multimedia. However, compression and simplification processes
inevitably result in the loss of quality-aware information under storage and
bandwidth constraints. Therefore, there is an increasing need for effective
methods to quantify the degree of distortion in point clouds. In this paper, we
propose simple baselines for projection-based point cloud quality assessment
(PCQA) to tackle this challenge. We use multi-projections obtained via a common
cube-like projection process from the point clouds for both full-reference (FR)
and no-reference (NR) PCQA tasks. Quality-aware features are extracted with
popular vision backbones. The FR quality representation is computed as the
similarity between the feature maps of reference and distorted projections
while the NR quality representation is obtained by simply squeezing the feature
maps of distorted projections with average pooling The corresponding quality
representations are regressed into visual quality scores by fully-connected
layers. Taking part in the ICIP 2023 PCVQA Challenge, we succeeded in achieving
the top spot in four out of the five competition tracks.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17149" title="Abstract">arXiv:2310.17149</a> [<a href="/pdf/2310.17149" title="Download PDF">pdf</a>, <a href="/format/2310.17149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Spatio-Temporal Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiabin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32nd ACM International Conference on Information and Knowledge Management (CIKM' 23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Spatio-temporal graph neural networks (STGNNs) have gained popularity as a
powerful tool for effectively modeling spatio-temporal dependencies in diverse
real-world urban applications, including intelligent transportation and public
safety. However, the black-box nature of STGNNs limits their interpretability,
hindering their application in scenarios related to urban resource allocation
and policy formulation. To bridge this gap, we propose an Explainable
Spatio-Temporal Graph Neural Networks (STExplainer) framework that enhances
STGNNs with inherent explainability, enabling them to provide accurate
predictions and faithful explanations simultaneously. Our framework integrates
a unified spatio-temporal graph attention network with a positional information
fusion layer as the STG encoder and decoder, respectively. Furthermore, we
propose a structure distillation approach based on the Graph Information
Bottleneck (GIB) principle with an explainable objective, which is instantiated
by the STG encoder and decoder. Through extensive experiments, we demonstrate
that our STExplainer outperforms state-of-the-art baselines in terms of
predictive accuracy and explainability metrics (i.e., sparsity and fidelity) on
traffic and crime prediction tasks. Furthermore, our model exhibits superior
representation ability in alleviating data missing and sparsity issues. The
implementation code is available at: https://github.com/HKUDS/STExplainer.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17152" title="Abstract">arXiv:2310.17152</a> [<a href="/pdf/2310.17152" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technical Note: Feasibility of translating 3.0T-trained Deep-Learning  Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy  Controls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+R">Rupsa Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Akkaya%2C+Z">Zehra Akkaya</a>, 
<a href="/search/cs?searchtype=author&query=Luitjens%2C+J">Johanna Luitjens</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+P">Pan Su</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pedoia%2C+V">Valentina Pedoia</a>, 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+S">Sharmila Majumdar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 Pages, 3 Figures, 2 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">In the current study, our purpose is to evaluate the feasibility of applying
deep learning (DL) enabled algorithms to quantify bilateral knee biomarkers in
healthy controls scanned at 0.55T and compared with 3.0T. The current study
assesses the performance of standard in-practice bone, and cartilage
segmentation algorithms at 0.55T, both qualitatively and quantitatively, in
terms of comparing segmentation performance, areas of improvement, and
compartment-wise cartilage thickness values between 0.55T vs. 3.0T. Initial
results demonstrate a usable to good technical feasibility of translating
existing quantitative deep-learning-based image segmentation techniques,
trained on 3.0T, out of 0.55T for knee MRI, in a multi-vendor acquisition
environment. Especially in terms of segmenting cartilage compartments, the
models perform almost equivalent to 3.0T in terms of Likert ranking. The 0.55T
low-field sustainable and easy-to-install MRI, as demonstrated, thus, can be
utilized for evaluating knee cartilage thickness and bone segmentations aided
by established DL algorithms trained at higher-field strengths out-of-the-box
initially. This could be utilized at the far-spread point-of-care locations
with a lack of radiologists available to manually segment low-field images, at
least till a decent base of low-field data pool is collated. With further
fine-tuning with manual labeling of low-field data or utilizing synthesized
higher SNR images from low-field images, OA biomarker quantification
performance is potentially guaranteed to be further improved.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17153" title="Abstract">arXiv:2310.17153</a> [<a href="/pdf/2310.17153" title="Download PDF">pdf</a>, <a href="/format/2310.17153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Semi-Implicit Variational Inference with Application to  Diffusion Model Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Longlin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tianyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 13 figures, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">Semi-implicit variational inference (SIVI) has been introduced to expand the
analytical variational families by defining expressive semi-implicit
distributions in a hierarchical manner. However, the single-layer architecture
commonly used in current SIVI methods can be insufficient when the target
posterior has complicated structures. In this paper, we propose hierarchical
semi-implicit variational inference, called HSIVI, which generalizes SIVI to
allow more expressive multi-layer construction of semi-implicit distributions.
By introducing auxiliary distributions that interpolate between a simple base
distribution and the target distribution, the conditional layers can be trained
by progressively matching these auxiliary distributions one layer after
another. Moreover, given pre-trained score networks, HSIVI can be used to
accelerate the sampling process of diffusion models with the score matching
objective. We show that HSIVI significantly enhances the expressiveness of SIVI
on several Bayesian inference problems with complicated target distributions.
When used for diffusion model acceleration, we show that HSIVI can produce high
quality samples comparable to or better than the existing fast diffusion model
based samplers with a small number of function evaluations on various datasets.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17154" title="Abstract">arXiv:2310.17154</a> [<a href="/pdf/2310.17154" title="Download PDF">pdf</a>, <a href="/format/2310.17154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Imbalanced Regression via Hierarchical Classification Adjustment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Haipeng Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+A">Angela Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Regression tasks in computer vision, such as age estimation or counting, are
often formulated into classification by quantizing the target space into
classes. Yet real-world data is often imbalanced -- the majority of training
samples lie in a head range of target values, while a minority of samples span
a usually larger tail range. By selecting the class quantization, one can
adjust imbalanced regression targets into balanced classification outputs,
though there are trade-offs in balancing classification accuracy and
quantization error. To improve regression performance over the entire range of
data, we propose to construct hierarchical classifiers for solving imbalanced
regression tasks. The fine-grained classifiers limit the quantization error
while being modulated by the coarse predictions to ensure high accuracy.
Standard hierarchical classification approaches, however, when applied to the
regression problem, fail to ensure that predicted ranges remain consistent
across the hierarchy. As such, we propose a range-preserving distillation
process that can effectively learn a single classifier from the set of
hierarchical classifiers. Our novel hierarchical classification adjustment
(HCA) for imbalanced regression shows superior results on three diverse tasks:
age estimation, crowd counting and depth estimation. We will release the source
code upon acceptance.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17155" title="Abstract">arXiv:2310.17155</a> [<a href="/pdf/2310.17155" title="Download PDF">pdf</a>, <a href="/ps/2310.17155" title="Download PostScript">ps</a>, <a href="/format/2310.17155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Max-min Rate Optimization of Low-Complexity Hybrid Multi-User  Beamforming Maintaining Rate-Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">W. Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+H+D">H. D. Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Dutkiewicz%2C+E">E. Dutkiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. V. Poor</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">L. Hanzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">A wireless network serving multiple users in the millimeter-wave or the
sub-terahertz band by a base station is considered. High-throughput multi-user
hybrid-transmit beamforming is conceived by maximizing the minimum rate of the
users. For the sake of energy-efficient signal transmission, the
array-of-subarrays structure is used for analog beamforming relying on
low-resolution phase shifters. We develop a convexsolver based algorithm, which
iteratively invokes a convex problem of the same beamformer size for its
solution. We then introduce the soft max-min rate objective function and
develop a scalable algorithm for its optimization. Our simulation results
demonstrate the striking fact that soft max-min rate optimization not only
approaches the minimum user rate obtained by max-min rate optimization but it
also achieves a sum rate similar to that of sum-rate maximization. Thus, the
soft max-min rate optimization based beamforming design conceived offers a new
technique of simultaneously achieving a high individual quality-of-service for
all users and a high total network throughput.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17156" title="Abstract">arXiv:2310.17156</a> [<a href="/pdf/2310.17156" title="Download PDF">pdf</a>, <a href="/format/2310.17156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning depth from monocular video sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhenwei Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Learning single image depth estimation model from monocular video sequence is
a very challenging problem. In this paper, we propose a novel training loss
which enables us to include more images for supervision during the training
process. We propose a simple yet effective model to account the frame to frame
pixel motion. We also design a novel network architecture for single image
estimation. When combined, our method produces state of the art results for
monocular depth estimation on the KITTI dataset in the self-supervised setting.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17157" title="Abstract">arXiv:2310.17157</a> [<a href="/pdf/2310.17157" title="Download PDF">pdf</a>, <a href="/format/2310.17157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zichang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dao%2C+T">Tri Dao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+B">Binhang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+A">Anshumali Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Ce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Re%2C+C">Christopher Re</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 40th International Conference on Machine
  Learning, 2023, 919
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large language models (LLMs) with hundreds of billions of parameters have
sparked a new wave of exciting AI applications. However, they are
computationally expensive at inference time. Sparsity is a natural approach to
reduce this cost, but existing methods either require costly retraining, have
to forgo LLM's in-context learning ability, or do not yield wall-clock time
speedup on modern hardware. We hypothesize that contextual sparsity, which are
small, input-dependent sets of attention heads and MLP parameters that yield
approximately the same output as the dense model for a given input, can address
these issues. We show that contextual sparsity exists, that it can be
accurately predicted, and that we can exploit it to speed up LLM inference in
wall-clock time without compromising LLM's quality or in-context learning
ability. Based on these insights, we propose DejaVu, a system that uses a
low-cost algorithm to predict contextual sparsity on the fly given inputs to
each layer, along with an asynchronous and hardware-aware implementation that
speeds up LLM inference. We validate that DejaVu can reduce the inference
latency of OPT-175B by over 2X compared to the state-of-the-art
FasterTransformer, and over 6X compared to the widely used Hugging Face
implementation, without compromising model quality. The code is available at
https://github.com/FMInference/DejaVu.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17159" title="Abstract">arXiv:2310.17159</a> [<a href="/pdf/2310.17159" title="Download PDF">pdf</a>, <a href="/format/2310.17159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MaxEnt Loss: Constrained Maximum Entropy for Calibration under  Out-of-Distribution Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neo%2C+D">Dexter Neo</a>, 
<a href="/search/cs?searchtype=author&query=Winkler%2C+S">Stefan Winkler</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tsuhan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present a new loss function that addresses the out-of-distribution (OOD)
calibration problem. While many objective functions have been proposed to
effectively calibrate models in-distribution, our findings show that they do
not always fare well OOD. Based on the Principle of Maximum Entropy, we
incorporate helpful statistical constraints observed during training,
delivering better model calibration without sacrificing accuracy. We provide
theoretical analysis and show empirically that our method works well in
practice, achieving state-of-the-art calibration on both synthetic and
real-world benchmarks.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17162" title="Abstract">arXiv:2310.17162</a> [<a href="/pdf/2310.17162" title="Download PDF">pdf</a>, <a href="/format/2310.17162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Content-based Controls For Music Large Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Liwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gus Xia</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junyan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixiao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recent years have witnessed a rapid growth of large-scale language models in
the domain of music audio. Such models enable end-to-end generation of
higher-quality music, and some allow conditioned generation using text
descriptions. However, the control power of text controls on music is
intrinsically limited, as they can only describe music indirectly through
meta-data (such as singers and instruments) or high-level representations (such
as genre and emotion). We aim to further equip the models with direct and
content-based controls on innate music languages such as pitch, chords and drum
track. To this end, we contribute Coco-Mulla, a content-based control method
for music large language modeling. It uses a parameter-efficient fine-tuning
(PEFT) method tailored for Transformer-based audio models. Experiments show
that our approach achieved high-quality music generation with low-resource
semi-supervised learning, tuning with less than 4% parameters compared to the
original model and training on a small dataset with fewer than 300 songs.
Moreover, our approach enables effective content-based controls, and we
illustrate the control power via chords and rhythms, two of the most salient
features of music audio. Furthermore, we show that by combining content-based
controls and text descriptions, our system achieves flexible music variation
generation and style transfer. Our source codes and demos are available online.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17163" title="Abstract">arXiv:2310.17163</a> [<a href="/pdf/2310.17163" title="Download PDF">pdf</a>, <a href="/format/2310.17163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Dimensional Gradient Helps Out-of-Distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yingwen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xinwen Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaolin Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Detecting out-of-distribution (OOD) samples is essential for ensuring the
reliability of deep neural networks (DNNs) in real-world scenarios. While
previous research has predominantly investigated the disparity between
in-distribution (ID) and OOD data through forward information analysis, the
discrepancy in parameter gradients during the backward process of DNNs has
received insufficient attention. Existing studies on gradient disparities
mainly focus on the utilization of gradient norms, neglecting the wealth of
information embedded in gradient directions. To bridge this gap, in this paper,
we conduct a comprehensive investigation into leveraging the entirety of
gradient information for OOD detection. The primary challenge arises from the
high dimensionality of gradients due to the large number of network parameters.
To solve this problem, we propose performing linear dimension reduction on the
gradient using a designated subspace that comprises principal components. This
innovative technique enables us to obtain a low-dimensional representation of
the gradient with minimal information loss. Subsequently, by integrating the
reduced gradient with various existing detection score functions, our approach
demonstrates superior performance across a wide range of detection tasks. For
instance, on the ImageNet benchmark, our method achieves an average reduction
of 11.15% in the false positive rate at 95% recall (FPR95) compared to the
current state-of-the-art approach. The code would be released.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17164" title="Abstract">arXiv:2310.17164</a> [<a href="/pdf/2310.17164" title="Download PDF">pdf</a>, <a href="/format/2310.17164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Phylogeny and Taxonomy with Protein-protein Interaction  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long-Huei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Moorthy%2C+M+P+S">Mohana Prasad Sathya Moorthy</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+P">Pratyaksh Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The protein-protein interaction (PPI) network provides an overview of the
complex biological reactions vital to an organism's metabolism and survival.
Even though in the past PPI network were compared across organisms in detail,
there has not been large-scale research on how individual PPI networks reflect
on the species relationships. In this study we aim to increase our
understanding of the tree of life and taxonomy by gleaming information from the
PPI networks. We successful created (1) a predictor of network statistics based
on known traits of existing species in the phylogeny, and (2) a taxonomic
classifier of organism using the known protein network statistics, whether
experimentally determined or predicted de novo. With the knowledge of protein
interactions at its core, our two models effectively connects two field with
widely diverging methodologies - the phylogeny and taxonomy of species.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17166" title="Abstract">arXiv:2310.17166</a> [<a href="/pdf/2310.17166" title="Download PDF">pdf</a>, <a href="/format/2310.17166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yun%2C+T">Taejun Yun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinhyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Deokyeong Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+S+H">Seong Hoon Lim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jihoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taeuk Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Cross-lingual transfer (XLT) is an emergent ability of multilingual language
models that preserves their performance on a task to a significant extent when
evaluated in languages that were not included in the fine-tuning process. While
English, due to its widespread usage, is typically regarded as the primary
language for model adaption in various tasks, recent studies have revealed that
the efficacy of XLT can be amplified by selecting the most appropriate source
languages based on specific conditions. In this work, we propose the
utilization of sub-network similarity between two languages as a proxy for
predicting the compatibility of the languages in the context of XLT. Our
approach is model-oriented, better reflecting the inner workings of foundation
models. In addition, it requires only a moderate amount of raw text from
candidate languages, distinguishing it from the majority of previous methods
that rely on external resources. In experiments, we demonstrate that our method
is more effective than baselines across diverse tasks. Specifically, it shows
proficiency in ranking candidates for zero-shot XLT, achieving an improvement
of 4.6% on average in terms of NDCG@3. We also provide extensive analyses that
confirm the utility of sub-networks for XLT prediction.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17167" title="Abstract">arXiv:2310.17167</a> [<a href="/pdf/2310.17167" title="Download PDF">pdf</a>, <a href="/format/2310.17167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Denoising Diffusion Models via Simultaneous Estimation of  Image and Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenkai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ehinger%2C+K+A">Krista A. Ehinger</a>, 
<a href="/search/cs?searchtype=author&query=Drummond%2C+T">Tom Drummond</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper introduces two key contributions aimed at improving the speed and
quality of images generated through inverse diffusion processes. The first
contribution involves reparameterizing the diffusion process in terms of the
angle on a quarter-circular arc between the image and noise, specifically
setting the conventional $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$. This
reparameterization eliminates two singularities and allows for the expression
of diffusion evolution as a well-behaved ordinary differential equation (ODE).
In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be
used effectively. The second contribution is to directly estimate both the
image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which
enables more stable calculations of the update step in the inverse diffusion
steps, as accurate estimation of both the image and noise are crucial at
different stages of the process. Together with these changes, our model
achieves faster generation, with the ability to converge on high-quality images
more quickly, and higher quality of the generated images, as measured by
metrics such as Frechet Inception Distance (FID), spatial Frechet Inception
Distance (sFID), precision, and recall.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17168" title="Abstract">arXiv:2310.17168</a> [<a href="/pdf/2310.17168" title="Download PDF">pdf</a>, <a href="/format/2310.17168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning an Inventory Control Policy with General Inventory Arrival  Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andaz%2C+S">Sohrab Andaz</a>, 
<a href="/search/cs?searchtype=author&query=Eisenach%2C+C">Carson Eisenach</a>, 
<a href="/search/cs?searchtype=author&query=Madeka%2C+D">Dhruv Madeka</a>, 
<a href="/search/cs?searchtype=author&query=Torkkola%2C+K">Kari Torkkola</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Randy Jia</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+D">Dean Foster</a>, 
<a href="/search/cs?searchtype=author&query=Kakade%2C+S">Sham Kakade</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper we address the problem of learning and backtesting inventory
control policies in the presence of general arrival dynamics -- which we term
as a quantity-over-time arrivals model (QOT). We also allow for order
quantities to be modified as a post-processing step to meet vendor constraints
such as order minimum and batch size constraints -- a common practice in real
supply chains. To the best of our knowledge this is the first work to handle
either arbitrary arrival dynamics or an arbitrary downstream post-processing of
order quantities. Building upon recent work (Madeka et al., 2022) we similarly
formulate the periodic review inventory control problem as an exogenous
decision process, where most of the state is outside the control of the agent.
Madeka et al. (2022) show how to construct a simulator that replays historic
data to solve this class of problem. In our case, we incorporate a deep
generative model for the arrivals process as part of the history replay. By
formulating the problem as an exogenous decision process, we can apply results
from Madeka et al. (2022) to obtain a reduction to supervised learning.
Finally, we show via simulation studies that this approach yields statistically
significant improvements in profitability over production baselines. Using data
from an ongoing real-world A/B test, we show that Gen-QOT generalizes well to
off-policy data.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17169" title="Abstract">arXiv:2310.17169</a> [<a href="/pdf/2310.17169" title="Download PDF">pdf</a>, <a href="/format/2310.17169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bivariate Spline based Collocation Method for Numerical Solution to  Optimal Transport Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lai%2C+M">Ming-Jun Lai</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+J">Jinsil Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">In this paper, we study a spline collocation method for a numerical solution
to the optimal transport problem
<br />We mainly solve the \MAE with the second boundary condition numerically by
proposing a center matching algorithm. We prove a pointwise convergence of our
iterative algorithm under the assumption the boundedness of spline iterates. We
use the \MAE with Dirichlet boundary condition and some known solutions to the
\MAE with second boundary condition to demonstrate the effectiveness of our
algorithm. Then we use our method to solve some real-life problems. One
application problem is to use the optimal transportation for the conversion of
fisheye view images into standard rectangular images.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17170" title="Abstract">arXiv:2310.17170</a> [<a href="/pdf/2310.17170" title="Download PDF">pdf</a>, <a href="/format/2310.17170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MO-YOLO: End-to-End Multiple-Object Tracking Method with YOLO and MOTR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liao Pan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+W">Wu Di</a>, 
<a href="/search/cs?searchtype=author&query=Bo%2C+L">Liu Bo</a>, 
<a href="/search/cs?searchtype=author&query=Xingle%2C+Z">Zhang Xingle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper aims to address critical issues in the field of Multi-Object
Tracking (MOT) by proposing an efficient and computationally resource-efficient
end-to-end multi-object tracking model, named MO-YOLO. Traditional MOT methods
typically involve two separate steps: object detection and object tracking,
leading to computational complexity and error propagation issues. Recent
research has demonstrated outstanding performance in end-to-end MOT models
based on Transformer architectures, but they require substantial hardware
support. MO-YOLO combines the strengths of YOLO and RT-DETR models to construct
a high-efficiency, lightweight, and resource-efficient end-to-end multi-object
tracking network, offering new opportunities in the multi-object tracking
domain. On the MOT17 dataset, MOTR\cite{zeng2022motr} requires training with 8
GeForce 2080 Ti GPUs for 4 days to achieve satisfactory results, while MO-YOLO
only requires 1 GeForce 2080 Ti GPU and 12 hours of training to achieve
comparable performance.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17171" title="Abstract">arXiv:2310.17171</a> [<a href="/pdf/2310.17171" title="Download PDF">pdf</a>, <a href="/format/2310.17171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating True Beliefs from Declared Opinions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tang%2C+J">Jennifer Tang</a>, 
<a href="/search/eess?searchtype=author&query=Adler%2C+A">Aviv Adler</a>, 
<a href="/search/eess?searchtype=author&query=Ajorlou%2C+A">Amir Ajorlou</a>, 
<a href="/search/eess?searchtype=author&query=Jadbabaie%2C+A">Ali Jadbabaie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Social and Information Networks (cs.SI); Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
<p class="mathjax">Social networks often exert social pressure, causing individuals to adapt
their expressed opinions to conform to their peers. An agent in such systems
can be modeled as having a (true and unchanging) inherent belief but broadcasts
a declared opinion at each time step based on her inherent belief and the past
declared opinions of her neighbors. An important question in this setting is
parameter estimation: how to disentangle the effects of social pressure to
estimate inherent beliefs from declared opinions. To address this, Jadbabaie et
al. formulated the interacting P\'olya urn model of opinion dynamics under
social pressure and studied it on complete-graph social networks using an
aggregate estimator, and found that their estimator converges to the inherent
beliefs unless majority pressure pushes the network to consensus. In this work,
we study this model on arbitrary networks, providing an estimator which
converges to the inherent beliefs even in consensus situations. Finally, we
bound the convergence rate of our estimator in both consensus and non-consensus
scenarios; to get the bound for consensus scenarios (which converge slower than
non-consensus) we additionally found how quickly the system converges to
consensus.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17173" title="Abstract">arXiv:2310.17173</a> [<a href="/pdf/2310.17173" title="Download PDF">pdf</a>, <a href="/format/2310.17173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor  Critic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neo%2C+D">Dexter Neo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tsuhan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present a novel extension to the family of Soft Actor-Critic (SAC)
algorithms. We argue that based on the Maximum Entropy Principle, discrete SAC
can be further improved via additional statistical constraints derived from a
surrogate critic policy. Furthermore, our findings suggests that these
constraints provide an added robustness against potential domain shifts, which
are essential for safe deployment of reinforcement learning agents in the
real-world. We provide theoretical analysis and show empirical results on low
data regimes for both in-distribution and out-of-distribution variants of Atari
2600 games.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17176" title="Abstract">arXiv:2310.17176</a> [<a href="/pdf/2310.17176" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Learning Approach to Teeth Segmentation and Orientation from  Panoramic X-rays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhar%2C+M+K">Mrinal Kanti Dhar</a>, 
<a href="/search/cs?searchtype=author&query=Deb%2C+M">Mou Deb</a>, 
<a href="/search/cs?searchtype=author&query=Madhab%2C+D">D. Madhab</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zeyun Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurate teeth segmentation and orientation are fundamental in modern oral
healthcare, enabling precise diagnosis, treatment planning, and dental implant
design. In this study, we present a comprehensive approach to teeth
segmentation and orientation from panoramic X-ray images, leveraging deep
learning techniques. We build our model based on FUSegNet, a popular model
originally developed for wound segmentation, and introduce modifications by
incorporating grid-based attention gates into the skip connections. We
introduce oriented bounding box (OBB) generation through principal component
analysis (PCA) for precise tooth orientation estimation. Evaluating our
approach on the publicly available DNS dataset, comprising 543 panoramic X-ray
images, we achieve the highest Intersection-over-Union (IoU) score of 82.43%
and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in
teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU)
score of 82.82%. We also conduct detailed analyses of individual tooth labels
and categorical performance, shedding light on strengths and weaknesses. The
proposed model's accuracy and versatility offer promising prospects for
improving dental diagnoses, treatment planning, and personalized healthcare in
the oral domain. Our generated OBB coordinates and codes are available at
https://github.com/mrinal054/Instance_teeth_segmentation.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17177" title="Abstract">arXiv:2310.17177</a> [<a href="/pdf/2310.17177" title="Download PDF">pdf</a>, <a href="/format/2310.17177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging The Gaps Between Token Pruning and Full Pre-training via Masked  Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+F">Fengyuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Limin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to TIP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite the success of transformers on various computer vision tasks, they
suffer from excessive memory and computational cost. Some works present dynamic
vision transformers to accelerate inference by pruning redundant tokens. A key
to improving token pruning is using well-trained models as initialization for
faster convergence and better performance. However, current base models usually
adopt full image training, i.e., using full images as inputs and keeping the
whole feature maps through the forward process, which causes inconsistencies
with dynamic models that gradually reduce tokens, including calculation
pattern, information amount and token selection strategy inconsistencies.
Inspired by MAE which performs masking and reconstruction self-supervised task,
we devise masked fine-tuning to bridge the gaps between pre-trained base models
used for initialization and token pruning based dynamic vision transformers, by
masking image patches and predicting the image class label based on left
unmasked patches. Extensive experiments on ImageNet demonstrate that base
models via masked fine-tuning gain strong occlusion robustness and ability
against information loss. With this better initialization, Dynamic ViT achieves
higher accuracies, especially under large token pruning ratios (e.g., 81.9% vs.
81.3%, and 62.3% vs. 58.9% for DeiT based Dynamic ViT/0.8 and Dynamic ViT/0.3).
Moreover, we apply our method into different token pruning based dynamic vision
transformers, different pre-trained models and randomly initialized models to
demonstrate the generalization ability.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17178" title="Abstract">arXiv:2310.17178</a> [<a href="/pdf/2310.17178" title="Download PDF">pdf</a>, <a href="/format/2310.17178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graphical Object-Centric Actor-Critic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ugadiarov%2C+L">Leonid Ugadiarov</a>, 
<a href="/search/cs?searchtype=author&query=Panov%2C+A+I">Aleksandr I. Panov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">There have recently been significant advances in the problem of unsupervised
object-centric representation learning and its application to downstream tasks.
The latest works support the argument that employing disentangled object
representations in image-based object-centric reinforcement learning tasks
facilitates policy learning. We propose a novel object-centric reinforcement
learning algorithm combining actor-critic and model-based approaches to utilize
these representations effectively. In our approach, we use a transformer
encoder to extract object representations and graph neural networks to
approximate the dynamics of an environment. The proposed method fills a
research gap in developing efficient object-centric world models for
reinforcement learning settings that can be used for environments with discrete
or continuous action spaces. Our algorithm performs better in a visually
complex 3D robotic environment and a 2D environment with compositional
structure than the state-of-the-art model-free actor-critic algorithm built
upon transformer architecture and the state-of-the-art monolithic model-based
algorithm.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17179" title="Abstract">arXiv:2310.17179</a> [<a href="/pdf/2310.17179" title="Download PDF">pdf</a>, <a href="/format/2310.17179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linking intra- and extra-cellular metabolic domains via neural-network  surrogates for dynamic metabolic control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Espinel-R%C3%ADos%2C+S">Sebasti&#xe1;n Espinel-R&#xed;os</a>, 
<a href="/search/eess?searchtype=author&query=Avalos%2C+J+L">Jos&#xe9; L. Avalos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference paper submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this study, we aim to optimize biotechnological production by manipulating
intracellular metabolic fluxes in microbial cell factories. Model-based dynamic
optimization is proposed to determine the optimal dynamic trajectories of the
manipulatable intracellular fluxes. A challenge emerges as existing models are
often oversimplified, lacking insights into intracellular metabolism, or are
excessively complex, leading to numerical and implementation challenges in
optimal control (e.g., related to bilevel optimizations). We propose a solution
involving a machine-learning surrogate derived from steady-state
constraint-based metabolic modeling. This surrogate bridges the gap between
manipulatable intracellular fluxes and process exchange rates. By integrating
the surrogate model with simple macro-kinetic dynamic models, we can develop
hybrid machine-learning-supported dynamic models. Conveniently, the
manipulatable intracellular fluxes in these augmented models can be exploited
as dynamic optimization degrees of freedom. We apply this modeling and
optimization strategy to a representative metabolic network that showcases
common challenges in dynamic metabolic control. We also present an example of
cybernetic control to counteract system uncertainties. Our approach facilitates
the in silico evaluation of dynamic metabolic interventions and can aid in the
selection of suitable control and actuation strategies.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17180" title="Abstract">arXiv:2310.17180</a> [<a href="/pdf/2310.17180" title="Download PDF">pdf</a>, <a href="/format/2310.17180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Forward Reachability Perspective on Robust Control Invariance and  Discount Factors in Reachability Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Choi%2C+J+J">Jason J. Choi</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+D">Donggun Lee</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+B">Boyang Li</a>, 
<a href="/search/eess?searchtype=author&query=How%2C+J+P">Jonathan P. How</a>, 
<a href="/search/eess?searchtype=author&query=Sreenath%2C+K">Koushil Sreenath</a>, 
<a href="/search/eess?searchtype=author&query=Herbert%2C+S+L">Sylvia L. Herbert</a>, 
<a href="/search/eess?searchtype=author&query=Tomlin%2C+C+J">Claire J. Tomlin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Control invariant sets are crucial for various methods that aim to design
safe control policies for systems whose state constraints must be satisfied
over an indefinite time horizon. In this article, we explore the connections
among reachability, control invariance, and Control Barrier Functions (CBFs) by
examining the forward reachability problem associated with control invariant
sets. We present the notion of an "inevitable Forward Reachable Tube" (FRT) as
a tool for analyzing control invariant sets. Our findings show that the
inevitable FRT of a robust control invariant set with a differentiable boundary
is the set itself. We highlight the role of the differentiability of the
boundary in shaping the FRTs of the sets through numerical examples. We also
formulate a zero-sum differential game between the control and disturbance,
where the inevitable FRT is characterized by the zero-superlevel set of the
value function. By incorporating a discount factor in the cost function of the
game, the barrier constraint of the CBF naturally arises as the constraint that
is imposed on the optimal control policy. As a result, the value function of
our FRT formulation serves as a CBF-like function, which has not been
previously realized in reachability studies. Conversely, any valid CBF is also
a forward reachability value function inside the control invariant set, thereby
revealing the inverse optimality of the CBF. As such, our work establishes a
strong link between reachability, control invariance, and CBFs, filling a gap
that prior formulations based on backward reachability were unable to bridge.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17183" title="Abstract">arXiv:2310.17183</a> [<a href="/pdf/2310.17183" title="Download PDF">pdf</a>, <a href="/format/2310.17183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Effects of Projectors in Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yudong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiajun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=de+Hoog%2C+F">Frank de Hoog</a>, 
<a href="/search/cs?searchtype=author&query=Kusy%2C+B">Brano Kusy</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zi Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2210.15274">arXiv:2210.15274</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Conventionally, during the knowledge distillation process (e.g. feature
distillation), an additional projector is often required to perform feature
transformation due to the dimension mismatch between the teacher and the
student networks. Interestingly, we discovered that even if the student and the
teacher have the same feature dimensions, adding a projector still helps to
improve the distillation performance. In addition, projectors even improve
logit distillation if we add them to the architecture too. Inspired by these
surprising findings and the general lack of understanding of the projectors in
the knowledge distillation process from existing literature, this paper
investigates the implicit role that projectors play but so far have been
overlooked. Our empirical study shows that the student with a projector (1)
obtains a better trade-off between the training accuracy and the testing
accuracy compared to the student without a projector when it has the same
feature dimensions as the teacher, (2) better preserves its similarity to the
teacher beyond shallow and numeric resemblance, from the view of Centered
Kernel Alignment (CKA), and (3) avoids being over-confident as the teacher does
at the testing phase. Motivated by the positive effects of projectors, we
propose a projector ensemble-based feature distillation method to further
improve distillation performance. Despite the simplicity of the proposed
strategy, empirical results from the evaluation of classification tasks on
benchmark datasets demonstrate the superior classification performance of our
method on a broad range of teacher-student pairs and verify from the aspects of
CKA and model calibration that the student's features are of improved quality
with the projector ensemble design.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17185" title="Abstract">arXiv:2310.17185</a> [<a href="/pdf/2310.17185" title="Download PDF">pdf</a>, <a href="/format/2310.17185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive important sampling for Deep Ritz
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaoliang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuancheng Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce an adaptive sampling method for the Deep Ritz method aimed at
solving partial differential equations (PDEs). Two deep neural networks are
used. One network is employed to approximate the solution of PDEs, while the
other one is a deep generative model used to generate new collocation points to
refine the training set. The adaptive sampling procedure consists of two main
steps. The first step is solving the PDEs using the Deep Ritz method by
minimizing an associated variational loss discretized by the collocation points
in the training set. The second step involves generating a new training set,
which is then used in subsequent computations to further improve the accuracy
of the current approximate solution. We treat the integrand in the variational
loss as an unnormalized probability density function (PDF) and approximate it
using a deep generative model called bounded KRnet. The new samples and their
associated PDF values are obtained from the bounded KRnet. With these new
samples and their associated PDF values, the variational loss can be
approximated more accurately by importance sampling. Compared to the original
Deep Ritz method, the proposed adaptive method improves accuracy, especially
for problems characterized by low regularity and high dimensionality. We
demonstrate the effectiveness of our new method through a series of numerical
experiments.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17186" title="Abstract">arXiv:2310.17186</a> [<a href="/pdf/2310.17186" title="Download PDF">pdf</a>, <a href="/format/2310.17186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Compiler Unstable Feature Usage and Impacts in the Rust  Ecosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenghao Li</a> (1), 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yifei Wu</a> (1), 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wenbo Shen</a> (1), 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zichen Zhao</a> (1), 
<a href="/search/cs?searchtype=author&query=Chang%2C+R">Rui Chang</a> (1), 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengwei Liu</a> (2), 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a> (2), 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">Kui Ren</a> (1) ((1) Zhejiang University, Hangzhou, China, (2) Nanyang Technological University, Singapore, Singapore)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ICSE'2024 Conference: <a href="https://conf.researchr.org/details/icse-2024/icse-2024-research-track/6/Demystifying-Compiler-Unstable-Feature-Usage-and-Impacts-in-the-Rust-Ecosystem.">this https URL</a> Project webiste: <a href="https://sites.google.com/view/ruf-study/home.">this https URL</a> Released Source Code Zonodo: <a href="https://zenodo.org/records/8289375">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Rust programming language is gaining popularity rapidly in building reliable
and secure systems due to its security guarantees and outstanding performance.
To provide extra functionalities, the Rust compiler introduces Rust unstable
features (RUF) to extend compiler functionality, syntax, and standard library
support. However, these features are unstable and may get removed, introducing
compilation failures to dependent packages. Even worse, their impacts propagate
through transitive dependencies, causing large-scale failures in the whole
ecosystem. Although RUF is widely used in Rust, previous research has primarily
concentrated on Rust code safety, with the usage and impacts of RUF from the
Rust compiler remaining unexplored. Therefore, we aim to bridge this gap by
systematically analyzing the RUF usage and impacts in the Rust ecosystem. We
propose novel techniques for extracting RUF precisely, and to assess its impact
on the entire ecosystem quantitatively, we accurately resolve package
dependencies. We have analyzed the whole Rust ecosystem with 590K package
versions and 140M transitive dependencies. Our study shows that the Rust
ecosystem uses 1000 different RUF, and at most 44% of package versions are
affected by RUF, causing compiling failures for at most 12%. To mitigate wide
RUF impacts, we further design and implement a RUF-compilation-failure recovery
tool that can recover up to 90% of the failure. We believe our techniques,
findings, and tools can help to stabilize the Rust compiler, ultimately
enhancing the security and reliability of the Rust ecosystem.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17188" title="Abstract">arXiv:2310.17188</a> [<a href="/pdf/2310.17188" title="Download PDF">pdf</a>, <a href="/format/2310.17188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blind Image Super-resolution with Rich Texture-Aware Codebooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+R">Rui Qin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Ming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fangyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xing Wen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Blind super-resolution (BSR) methods based on high-resolution (HR)
reconstruction codebooks have achieved promising results in recent years.
However, we find that a codebook based on HR reconstruction may not effectively
capture the complex correlations between low-resolution (LR) and HR images. In
detail, multiple HR images may produce similar LR versions due to complex blind
degradations, causing the HR-dependent only codebooks having limited texture
diversity when faced with confusing LR inputs. To alleviate this problem, we
propose the Rich Texture-aware Codebook-based Network (RTCNet), which consists
of the Degradation-robust Texture Prior Module (DTPM) and the Patch-aware
Texture Prior Module (PTPM). DTPM effectively mines the cross-resolution
correlation of textures between LR and HR images by exploiting the
cross-resolution correspondence of textures. PTPM uses patch-wise semantic
pre-training to correct the misperception of texture similarity in the
high-level semantic regularization. By taking advantage of this, RTCNet
effectively gets rid of the misalignment of confusing textures between HR and
LR in the BSR scenarios. Experiments show that RTCNet outperforms
state-of-the-art methods on various benchmarks by up to 0.16 ~ 0.46dB.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17189" title="Abstract">arXiv:2310.17189</a> [<a href="/pdf/2310.17189" title="Download PDF">pdf</a>, <a href="/format/2310.17189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Iterative Refinement with Diffusion Models for Video Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Tao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yaoyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+T">Te Tao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shao-Lun Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video grounding aims to localize the target moment in an untrimmed video
corresponding to a given sentence query. Existing methods typically select the
best prediction from a set of predefined proposals or directly regress the
target span in a single-shot manner, resulting in the absence of a systematical
prediction refinement process. In this paper, we propose DiffusionVG, a novel
framework with diffusion models that formulates video grounding as a
conditional generation task, where the target span is generated from Gaussian
noise inputs and interatively refined in the reverse diffusion process. During
training, DiffusionVG progressively adds noise to the target span with a fixed
forward diffusion process and learns to recover the target span in the reverse
diffusion process. In inference, DiffusionVG can generate the target span from
Gaussian noise inputs by the learned reverse diffusion process conditioned on
the video-sentence representations. Our DiffusionVG follows the encoder-decoder
architecture, which firstly encodes the video-sentence features and iteratively
denoises the predicted spans in its specialized span refining decoder. Without
bells and whistles, our DiffusionVG demonstrates competitive or even superior
performance compared to existing well-crafted models on mainstream Charades-STA
and ActivityNet Captions benchmarks.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17190" title="Abstract">arXiv:2310.17190</a> [<a href="/pdf/2310.17190" title="Download PDF">pdf</a>, <a href="/format/2310.17190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction  Network for Tone Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Feng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+M">Ming Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qingbo Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Changxin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+N">Nong Sang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures, accepted by NeurlPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Tone mapping aims to convert high dynamic range (HDR) images to low dynamic
range (LDR) representations, a critical task in the camera imaging pipeline. In
recent years, 3-Dimensional LookUp Table (3D LUT) based methods have gained
attention due to their ability to strike a favorable balance between
enhancement performance and computational efficiency. However, these methods
often fail to deliver satisfactory results in local areas since the look-up
table is a global operator for tone mapping, which works based on pixel values
and fails to incorporate crucial local information. To this end, this paper
aims to address this issue by exploring a novel strategy that integrates global
and local operators by utilizing closed-form Laplacian pyramid decomposition
and reconstruction. Specifically, we employ image-adaptive 3D LUTs to
manipulate the tone in the low-frequency image by leveraging the specific
characteristics of the frequency information. Furthermore, we utilize local
Laplacian filters to refine the edge details in the high-frequency components
in an adaptive manner. Local Laplacian filters are widely used to preserve edge
details in photographs, but their conventional usage involves manual tuning and
fixed implementation within camera imaging pipelines or photo editing tools. We
propose to learn parameter value maps progressively for local Laplacian filters
from annotated data using a lightweight network. Our model achieves
simultaneous global tone manipulation and local edge detail preservation in an
end-to-end manner. Extensive experimental results on two benchmark datasets
demonstrate that the proposed method performs favorably against
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17191" title="Abstract">arXiv:2310.17191</a> [<a href="/pdf/2310.17191" title="Download PDF">pdf</a>, <a href="/format/2310.17191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do Language Models Bind Entities in Context?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiahai Feng</a>, 
<a href="/search/cs?searchtype=author&query=Steinhardt%2C+J">Jacob Steinhardt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">To correctly use in-context information, language models (LMs) must bind
entities to their attributes. For example, given a context describing a "green
square" and a "blue circle", LMs must bind the shapes to their respective
colors. We analyze LM representations and identify the binding ID mechanism: a
general mechanism for solving the binding problem, which we observe in every
sufficiently large model from the Pythia and LLaMA families. Using causal
interventions, we show that LMs' internal activations represent binding
information by attaching binding ID vectors to corresponding entities and
attributes. We further show that binding ID vectors form a continuous subspace,
in which distances between binding ID vectors reflect their discernability.
Overall, our results uncover interpretable strategies in LMs for representing
symbolic knowledge in-context, providing a step towards understanding general
in-context reasoning in large-scale LMs.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17192" title="Abstract">arXiv:2310.17192</a> [<a href="/pdf/2310.17192" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-Motor Robotic Gripper With Three Functional Modes for Grasping in  Confined Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nishimura%2C+T">Toshihiro Nishimura</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+T">Tetsuyou Watanabe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This study proposes a novel robotic gripper driven by a single motor. The
main task is to pick up objects in confined spaces. For this purpose, the
developed gripper has three operating modes: grasping, finger-bending, and
pull-in modes. Using these three modes, the developed gripper can rotate and
translate a grasped object, i.e., can perform in-hand manipulation. This
in-hand manipulation is effective for grasping in extremely confined spaces,
such as the inside of a box in a shelf, to avoid interference between the
grasped object and obstacles. To achieve the three modes using a single motor,
the developed gripper is equipped with two novel self-motion switching
mechanisms. These mechanisms switch their motions automatically when the motion
being generated is prevented. An analysis of the mechanism and control
methodology used to achieve the desired behavior are presented. Furthermore,
the validity of the analysis and methodology are experimentally demonstrated.
The gripper performance is also evaluated through the grasping tests.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17193" title="Abstract">arXiv:2310.17193</a> [<a href="/pdf/2310.17193" title="Download PDF">pdf</a>, <a href="/format/2310.17193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Edge Error Judgment in Figure Skating Using 3D Pose Estimation  from a Monocular Camera and IMUs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+R">Ryota Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">Tomohiro Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+K">Kazuya Takeda</a>, 
<a href="/search/cs?searchtype=author&query=Fujii%2C+K">Keisuke Fujii</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">Automatic evaluating systems are fundamental issues in sports technologies.
In many sports, such as figure skating, automated evaluating methods based on
pose estimation have been proposed. However, previous studies have evaluated
skaters' skills in 2D analysis. In this paper, we propose an automatic edge
error judgment system with a monocular smartphone camera and inertial sensors,
which enable us to analyze 3D motions. Edge error is one of the most
significant scoring items and is challenging to automatically judge due to its
3D motion. The results show that the model using 3D joint position coordinates
estimated from the monocular camera as the input feature had the highest
accuracy at 83% for unknown skaters' data. We also analyzed the detailed motion
analysis for edge error judgment. These results indicate that the monocular
camera can be used to judge edge errors automatically. We will provide the
figure skating single Lutz jump dataset, including pre-processed videos and
labels, at https://github.com/ryota-takedalab/JudgeAI-LutzEdge.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17197" title="Abstract">arXiv:2310.17197</a> [<a href="/pdf/2310.17197" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight High-Speed and High-Force Gripper for Assembly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nishimura%2C+T">Toshihiro Nishimura</a>, 
<a href="/search/cs?searchtype=author&query=Takaki%2C+T">Takeshi Takaki</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+Y">Yosuke Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Tsuji%2C+T">Tokuo Tsuji</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+T">Tetsuyou Watanabe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents a novel industrial robotic gripper with a high grasping
speed (maximum: 1396 mm/s), high tip force (maximum: 80 N) for grasping, large
motion range, and lightweight design (0.3 kg). To realize these features, the
high-speed section of the quick-return mechanism and load-sensitive
continuously variable transmission mechanism are installed in the gripper. The
gripper is also equipped with a self-centering function. The high grasping
speed and self-centering function improve the cycle time in robotic operations.
In addition, the high tip force is advantageous for stably grasping and
assembling heavy objects. Moreover, the design of the gripper reduce the
gripper's proportion of the manipulator's payload, thus increasing the weight
of the object that can be grasped. The gripper performance was validated
through kinematic and static analyses as well as experimental evaluations. This
paper also presents the analysis of the self-centering function of the
developed gripper.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17200" title="Abstract">arXiv:2310.17200</a> [<a href="/pdf/2310.17200" title="Download PDF">pdf</a>, <a href="/format/2310.17200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming Gradient Variance in Federated Learning with Networked Control  Variates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaling Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Huaming Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yu Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning, a decentralized approach to machine learning, faces
significant challenges such as extensive communication overheads, slow
convergence, and unstable improvements. These challenges primarily stem from
the gradient variance due to heterogeneous client data distributions. To
address this, we introduce a novel Networked Control Variates (FedNCV)
framework for Federated Learning. We adopt the REINFORCE Leave-One-Out (RLOO)
as a fundamental control variate unit in the FedNCV framework, implemented at
both client and server levels. At the client level, the RLOO control variate is
employed to optimize local gradient updates, mitigating the variance introduced
by data samples. Once relayed to the server, the RLOO-based estimator further
provides an unbiased and low-variance aggregated gradient, leading to robust
global updates. This dual-side application is formalized as a linear
combination of composite control variates. We provide a mathematical expression
capturing this integration of double control variates within FedNCV and present
three theoretical results with corresponding proofs. This unique dual structure
equips FedNCV to address data heterogeneity and scalability issues, thus
potentially paving the way for large-scale applications. Moreover, we tested
FedNCV on six diverse datasets under a Dirichlet distribution with {\alpha} =
0.1, and benchmarked its performance against six SOTA methods, demonstrating
its superiority.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17202" title="Abstract">arXiv:2310.17202</a> [<a href="/pdf/2310.17202" title="Download PDF">pdf</a>, <a href="/format/2310.17202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> miditok: A Python package for MIDI file tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fradet%2C+N">Nathan Fradet</a>, 
<a href="/search/cs?searchtype=author&query=Briot%2C+J">Jean-Pierre Briot</a>, 
<a href="/search/cs?searchtype=author&query=Chhel%2C+F">Fabien Chhel</a>, 
<a href="/search/cs?searchtype=author&query=Seghrouchni%2C+A+E+F">Amal El Fallah Seghrouchni</a>, 
<a href="/search/cs?searchtype=author&query=Gutowski%2C+N">Nicolas Gutowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated and comprehensive report. Original ISMIR 2021 document at <a href="https://archives.ismir.net/ismir2021/latebreaking/000005.pdf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent progress in natural language processing has been adapted to the
symbolic music modality. Language models, such as Transformers, have been used
with symbolic music for a variety of tasks among which music generation,
modeling or transcription, with state-of-the-art performances. These models are
beginning to be used in production products. To encode and decode music for the
backbone model, they need to rely on tokenizers, whose role is to serialize
music into sequences of distinct elements called tokens. MidiTok is an
open-source library allowing to tokenize symbolic music with great flexibility
and extended features. It features the most popular music tokenizations, under
a unified API. It is made to be easily used and extensible for everyone.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17207" title="Abstract">arXiv:2310.17207</a> [<a href="/pdf/2310.17207" title="Download PDF">pdf</a>, <a href="/format/2310.17207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Data Fusion using the Tsetlin Machine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+R">Rupsa Saha</a>, 
<a href="/search/cs?searchtype=author&query=Zadorozhny%2C+V+I">Vladimir I. Zadorozhny</a>, 
<a href="/search/cs?searchtype=author&query=Granmo%2C+O">Ole-Christoffer Granmo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We propose a novel way of assessing and fusing noisy dynamic data using a
Tsetlin Machine. Our approach consists in monitoring how explanations in form
of logical clauses that a TM learns changes with possible noise in dynamic
data. This way TM can recognize the noise by lowering weights of previously
learned clauses, or reflect it in the form of new clauses. We also perform a
comprehensive experimental study using notably different datasets that
demonstrated high performance of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17209" title="Abstract">arXiv:2310.17209</a> [<a href="/pdf/2310.17209" title="Download PDF">pdf</a>, <a href="/format/2310.17209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly-Supervised Surgical Phase Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hirsch%2C+R">Roy Hirsch</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+R">Regev Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Caron%2C+M">Mathilde Caron</a>, 
<a href="/search/cs?searchtype=author&query=Golany%2C+T">Tomer Golany</a>, 
<a href="/search/cs?searchtype=author&query=Freedman%2C+D">Daniel Freedman</a>, 
<a href="/search/cs?searchtype=author&query=Rivlin%2C+E">Ehud Rivlin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A key element of computer-assisted surgery systems is phase recognition of
surgical videos. Existing phase recognition algorithms require frame-wise
annotation of a large number of videos, which is time and money consuming. In
this work we join concepts of graph segmentation with self-supervised learning
to derive a random-walk solution for per-frame phase prediction. Furthermore,
we utilize within our method two forms of weak supervision: sparse timestamps
or few-shot learning. The proposed algorithm enjoys low complexity and can
operate in lowdata regimes. We validate our method by running experiments with
the public Cholec80 dataset of laparoscopic cholecystectomy videos,
demonstrating promising performance in multiple setups.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17212" title="Abstract">arXiv:2310.17212</a> [<a href="/pdf/2310.17212" title="Download PDF">pdf</a>, <a href="/format/2310.17212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emotion Recognition by Video: A review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Junxiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuecheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Liangyu Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Video emotion recognition is an important branch of affective computing, and
its solutions can be applied in different fields such as human-computer
interaction (HCI) and intelligent medical treatment. Although the number of
papers published in the field of emotion recognition is increasing, there are
few comprehensive literature reviews covering related research on video emotion
recognition. Therefore, this paper selects articles published from 2015 to 2023
to systematize the existing trends in video emotion recognition in related
studies. In this paper, we first talk about two typical emotion models, then we
talk about databases that are frequently utilized for video emotion
recognition, including unimodal databases and multimodal databases. Next, we
look at and classify the specific structure and performance of modern unimodal
and multimodal video emotion recognition methods, talk about the benefits and
drawbacks of each, and then we compare them in detail in the tables. Further,
we sum up the primary difficulties right now looked by video emotion
recognition undertakings and point out probably the most encouraging future
headings, such as establishing an open benchmark database and better multimodal
fusion strategys. The essential objective of this paper is to assist scholarly
and modern scientists with keeping up to date with the most recent advances and
new improvements in this speedy, high-influence field of video emotion
recognition.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17217" title="Abstract">arXiv:2310.17217</a> [<a href="/pdf/2310.17217" title="Download PDF">pdf</a>, <a href="/format/2310.17217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond MLE: Convex Learning for Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+C">Chenze Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhengrui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Maximum likelihood estimation (MLE) is a statistical method used to estimate
the parameters of a probability distribution that best explain the observed
data. In the context of text generation, MLE is often used to train generative
language models, which can then be used to generate new text. However, we argue
that MLE is not always necessary and optimal, especially for closed-ended text
generation tasks like machine translation. In these tasks, the goal of model is
to generate the most appropriate response, which does not necessarily require
it to estimate the entire data distribution with MLE. To this end, we propose a
novel class of training objectives based on convex functions, which enables
text generation models to focus on highly probable outputs without having to
estimate the entire data distribution. We investigate the theoretical
properties of the optimal predicted distribution when applying convex functions
to the loss, demonstrating that convex functions can sharpen the optimal
distribution, thereby enabling the model to better capture outputs with high
probabilities. Experiments on various text generation tasks and models show the
effectiveness of our approach. It enables autoregressive models to bridge the
gap between greedy and beam search, and facilitates the learning of
non-autoregressive models with a maximum improvement of 9+ BLEU points.
Moreover, our approach also exhibits significant impact on large language
models (LLMs), substantially enhancing their generative capability on various
tasks. Source code is available at
\url{https://github.com/ictnlp/Convex-Learning}.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17218" title="Abstract">arXiv:2310.17218</a> [<a href="/pdf/2310.17218" title="Download PDF">pdf</a>, <a href="/format/2310.17218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prototypical Contrastive Learning-based CLIP Fine-tuning for Object  Re-identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiachen Li</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xiaojin Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This work aims to adapt large-scale pre-trained vision-language models, such
as contrastive language-image pretraining (CLIP), to enhance the performance of
object reidentification (Re-ID) across various supervision settings. Although
prompt learning has enabled a recent work named CLIP-ReID to achieve promising
performance, the underlying mechanisms and the necessity of prompt learning
remain unclear due to the absence of semantic labels in ReID tasks. In this
work, we first analyze the role prompt learning in CLIP-ReID and identify its
limitations. Based on our investigations, we propose a simple yet effective
approach to adapt CLIP for supervised object Re-ID. Our approach directly
fine-tunes the image encoder of CLIP using a prototypical contrastive learning
(PCL) loss, eliminating the need for prompt learning. Experimental results on
both person and vehicle Re-ID datasets demonstrate the competitiveness of our
method compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP
fine-tuning approach to unsupervised scenarios, where we achieve state-of-the
art performance.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17219" title="Abstract">arXiv:2310.17219</a> [<a href="/pdf/2310.17219" title="Download PDF">pdf</a>, <a href="/format/2310.17219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Verification of Strategy Logic through Three-valued Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belardinelli%2C+F">Francesco Belardinelli</a>, 
<a href="/search/cs?searchtype=author&query=Ferrando%2C+A">Angelo Ferrando</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Malvone%2C+V">Vadim Malvone</a>, 
<a href="/search/cs?searchtype=author&query=Murano%2C+A">Aniello Murano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">The model checking problem for multi-agent systems against Strategy Logic
specifications is known to be non-elementary. On this logic several fragments
have been defined to tackle this issue but at the expense of expressiveness. In
this paper, we propose a three-valued semantics for Strategy Logic upon which
we define an abstraction method. We show that the latter semantics is an
approximation of the classic two-valued one for Strategy Logic. Furthermore, we
extend MCMAS, an open-source model checker for multi-agent specifications, to
incorporate our abstraction method and present some promising experimental
results.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17220" title="Abstract">arXiv:2310.17220</a> [<a href="/pdf/2310.17220" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Validating Digital Traces with Survey Data: The Use Case of Religiosity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K%C4%B1na%2C+M+F">M.Fuat K&#x131;na</a>, 
<a href="/search/cs?searchtype=author&query=Y%C3%B6r%C3%BCk%2C+E">Erdem Y&#xf6;r&#xfc;k</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCrriyeto%C4%9Flu%2C+A">Ali H&#xfc;rriyeto&#x11f;lu</a>, 
<a href="/search/cs?searchtype=author&query=Yard%C4%B1%2C+M+C">Melih Can Yard&#x131;</a>, 
<a href="/search/cs?searchtype=author&query=Ats%C4%B1zelti%2C+%C5%9E">&#x15e;&#xfc;kr&#xfc; Ats&#x131;zelti</a>, 
<a href="/search/cs?searchtype=author&query=Duru%C5%9Fan%2C+F">F&#x131;rat Duru&#x15f;an</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCrerk%2C+O">O&#x11f;uz G&#xfc;rerk</a>, 
<a href="/search/cs?searchtype=author&query=Etg%C3%BC%2C+T">Tolga Etg&#xfc;</a>, 
<a href="/search/cs?searchtype=author&query=Ni%C5%9Fanc%C4%B1%2C+Z">Z&#xfc;beyir Ni&#x15f;anc&#x131;</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Osman Mutlu</a>, 
<a href="/search/cs?searchtype=author&query=Turbic%2C+G+B">Gizem Bacaks&#x131;zlar Turbic</a>, 
<a href="/search/cs?searchtype=author&query=Akbulut%2C+Y">Yusuf Akbulut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">This paper tests the validity of a digital trace database (Politus) obtained
from Twitter, with a recently conducted representative social survey, focusing
on the use case of religiosity in Turkey. Religiosity scores in the research
are extracted using supervised machine learning under the Politus project. The
validation analysis depends on two steps. First, we compare the performances of
two alternative tweet-to-user transformation strategies, and second, test for
the impact of resampling via the MRP technique. Estimates of the Politus are
examined at both aggregate and region-level. The results are intriguing for
future research on measuring public opinion via social media data.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17223" title="Abstract">arXiv:2310.17223</a> [<a href="/pdf/2310.17223" title="Download PDF">pdf</a>, <a href="/format/2310.17223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quickest Change Detection with Controlled Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veeravalli%2C+V+V">Venugopal V. Veeravalli</a>, 
<a href="/search/cs?searchtype=author&query=Fellouris%2C+G">Georgios Fellouris</a>, 
<a href="/search/cs?searchtype=author&query=Moustakides%2C+G+V">George V. Moustakides</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">In the problem of quickest change detection, a change occurs at some unknown
time in the distribution of a sequence of random vectors that are monitored in
real time, and the goal is to detect this change as quickly as possible subject
to a certain false alarm constraint. In this work we consider this problem in
the presence of parametric uncertainty in the post-change regime and controlled
sensing. That is, the post-change distribution contains an unknown parameter,
and the distribution of each observation, before and after the change, is
affected by a control action. In this context, in addition to a stopping rule
that determines the time at which it is declared that the change has occurred,
one also needs to determine a sequential control policy, which chooses the
control action at each time based on the already collected observations. We
formulate this problem mathematically using Lorden's minimax criterion, and
assuming that there are finitely many possible actions and post-change
parameter values. We then propose a specific procedure for this problem that
employs an adaptive CuSum statistic in which (i) the estimate of the parameter
is based on a fixed number of the more recent observations, and (ii) each
action is selected to maximize the Kullback-Leibler divergence of the next
observation based on the current parameter estimate, apart from a small number
of exploration times. We show that this procedure, which we call the Windowed
Chernoff-CuSum (WCC), is first-order asymptotically optimal under Lorden's
minimax criterion, for every possible possible value of the unknown post-change
parameter, as the mean time to false alarm goes to infinity. We also provide
simulation results to illustrate the performance of the WCC procedure.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17224" title="Abstract">arXiv:2310.17224</a> [<a href="/pdf/2310.17224" title="Download PDF">pdf</a>, <a href="/format/2310.17224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the decentralized coordination of multiple self-adaptive systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dragan%2C+P">Paul-Andrei Dragan</a>, 
<a href="/search/cs?searchtype=author&query=Metzger%2C+A">Andreas Metzger</a>, 
<a href="/search/cs?searchtype=author&query=Pohl%2C+K">Klaus Pohl</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 4th IEEE Intl. Conference on Autonomic Computing and
  Self-Organizing Systems (ACSOS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">When multiple self-adaptive systems share the same environment and have
common goals, they may coordinate their adaptations at runtime to avoid
conflicts and to satisfy their goals. There are two approaches to coordination.
(1) Logically centralized, where a supervisor has complete control over the
individual self-adaptive systems. Such approach is infeasible when the systems
have different owners or administrative domains. (2) Logically decentralized,
where coordination is achieved through direct interactions. Because the
individual systems have control over the information they share, decentralized
coordination accommodates multiple administrative domains. However, existing
techniques do not account simultaneously for both local concerns, e.g.,
preferences, and shared concerns, e.g., conflicts, which may lead to goals not
being achieved as expected. Our idea to address this shortcoming is to express
both types of concerns within the same constraint optimization problem. We
propose CoADAPT, a decentralized coordination technique introducing two types
of constraints: preference constraints, expressing local concerns, and
consistency constraints, expressing shared concerns. At runtime, the problem is
solved in a decentralized way using distributed constraint optimization
algorithms implemented by each self-adaptive system. As a first step in
realizing CoADAPT, we focus in this work on the coordination of adaptation
planning strategies, traditionally addressed only with centralized techniques.
We show the feasibility of CoADAPT in an exemplar from cloud computing and
analyze experimentally its scalability.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17228" title="Abstract">arXiv:2310.17228</a> [<a href="/pdf/2310.17228" title="Download PDF">pdf</a>, <a href="/format/2310.17228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khatry%2C+A">Anirudh Khatry</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Priyanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Vu Le</a>, 
<a href="/search/cs?searchtype=author&query=Singha%2C+A">Ananya Singha</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mukul Singh</a>, 
<a href="/search/cs?searchtype=author&query=Verbruggen%2C+G">Gust Verbruggen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for EMNLP-Findings, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Software Engineering (cs.SE)

</div>
<p class="mathjax">Target similarity tuning (TST) is a method of selecting relevant examples in
natural language (NL) to code generation through large language models (LLMs)
to improve performance. Its goal is to adapt a sentence embedding model to have
the similarity between two NL inputs match the similarity between their
associated code outputs. In this paper, we propose different methods to apply
and improve TST in the real world. First, we replace the sentence transformer
with embeddings from a larger model, which reduces sensitivity to the language
distribution and thus provides more flexibility in synthetic generation of
examples, and we train a tiny model that transforms these embeddings to a space
where embedding similarity matches code similarity, which allows the model to
remain a black box and only requires a few matrix multiplications at inference
time. Second, we how to efficiently select a smaller number of training
examples to train the TST model. Third, we introduce a ranking-based evaluation
for TST that does not require end-to-end code generation experiments, which can
be expensive to perform.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17230" title="Abstract">arXiv:2310.17230</a> [<a href="/pdf/2310.17230" title="Download PDF">pdf</a>, <a href="/format/2310.17230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Codebook Features: Sparse and Discrete Interpretability for Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tamkin%2C+A">Alex Tamkin</a>, 
<a href="/search/cs?searchtype=author&query=Taufeeque%2C+M">Mohammad Taufeeque</a>, 
<a href="/search/cs?searchtype=author&query=Goodman%2C+N+D">Noah D. Goodman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Understanding neural networks is challenging in part because of the dense,
continuous nature of their hidden states. We explore whether we can train
neural networks to have hidden states that are sparse, discrete, and more
interpretable by quantizing their continuous features into what we call
codebook features. Codebook features are produced by finetuning neural networks
with vector quantization bottlenecks at each layer, producing a network whose
hidden features are the sum of a small number of discrete vector codes chosen
from a larger codebook. Surprisingly, we find that neural networks can operate
under this extreme bottleneck with only modest degradation in performance. This
sparse, discrete bottleneck also provides an intuitive way of controlling
neural network behavior: first, find codes that activate when the desired
behavior is present, then activate those same codes during generation to elicit
that behavior. We validate our approach by training codebook Transformers on
several different datasets. First, we explore a finite state machine dataset
with far more hidden states than neurons. In this setting, our approach
overcomes the superposition problem by assigning states to distinct codes, and
we find that we can make the neural network behave as if it is in a different
state by activating the code for that state. Second, we train Transformer
language models with up to 410M parameters on two natural language datasets. We
identify codes in these models representing diverse, disentangled concepts
(ranging from negative emotions to months of the year) and find that we can
guide the model to generate different topics by activating the appropriate
codes during inference. Overall, codebook features appear to be a promising
unit of analysis and control for neural networks and interpretability. Our
codebase and models are open-sourced at
https://github.com/taufeeque9/codebook-features.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17233" title="Abstract">arXiv:2310.17233</a> [<a href="/pdf/2310.17233" title="Download PDF">pdf</a>, <a href="/format/2310.17233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Ping Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiangpeng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yue Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Baosong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dayiheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jun Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Expressing universal semantics common to all languages is helpful in
understanding the meanings of complex and culture-specific sentences. The
research theme underlying this scenario focuses on learning universal
representations across languages with the usage of massive parallel corpora.
However, due to the sparsity and scarcity of parallel data, there is still a
big challenge in learning authentic ``universals'' for any two languages. In
this paper, we propose EMMA-X: an EM-like Multilingual pre-training Algorithm,
to learn (X)Cross-lingual universals with the aid of excessive multilingual
non-parallel data. EMMA-X unifies the cross-lingual representation learning
task and an extra semantic relation prediction task within an EM framework.
Both the extra semantic classifier and the cross-lingual sentence encoder
approximate the semantic relation of two sentences, and supervise each other
until convergence. To evaluate EMMA-X, we conduct experiments on XRETE, a newly
introduced benchmark containing 12 widely studied cross-lingual tasks that
fully depend on sentence-level representations. Results reveal that EMMA-X
achieves state-of-the-art performance. Further geometric analysis of the built
representation space with three requirements demonstrates the superiority of
EMMA-X over advanced models.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17234" title="Abstract">arXiv:2310.17234</a> [<a href="/pdf/2310.17234" title="Download PDF">pdf</a>, <a href="/format/2310.17234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computationally Feasible Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dima%2C+C">Catalin Dima</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Real-life agents seldom have unlimited reasoning power. In this paper, we
propose and study a new formal notion of computationally bounded strategic
ability in multi-agent systems. The notion characterizes the ability of a set
of agents to synthesize an executable strategy in the form of a Turing machine
within a given complexity class, that ensures the satisfaction of a temporal
objective in a parameterized game arena. We show that the new concept induces a
proper hierarchy of strategic abilities -- in particular, polynomial-time
abilities are strictly weaker than the exponential-time ones. We also propose
an ``adaptive'' variant of computational ability which allows for different
strategies for each parameter value, and show that the two notions do not
coincide. Finally, we define and study the model-checking problem for
computational strategies. We show that the problem is undecidable even for
severely restricted inputs, and present our first steps towards decidable
fragments.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17238" title="Abstract">arXiv:2310.17238</a> [<a href="/pdf/2310.17238" title="Download PDF">pdf</a>, <a href="/format/2310.17238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Entity and Relation Extraction with Span Pruning and Hypergraph  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhaohui Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Songlin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+K">Kewei Tu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Proceedings of EMNLP, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Entity and Relation Extraction (ERE) is an important task in information
extraction. Recent marker-based pipeline models achieve state-of-the-art
performance, but still suffer from the error propagation issue. Also, most of
current ERE models do not take into account higher-order interactions between
multiple entities and relations, while higher-order modeling could be
beneficial.In this work, we propose HyperGraph neural network for ERE
($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based
pipleline model). To alleviate error propagation,we use a high-recall pruner
mechanism to transfer the burden of entity identification and labeling from the
NER module to the joint module of our model. For higher-order modeling, we
build a hypergraph, where nodes are entities (provided by the span pruner) and
relations thereof, and hyperedges encode interactions between two different
relations or between a relation and its associated subject and object entities.
We then run a hypergraph neural network for higher-order inference by applying
message passing over the built hypergraph. Experiments on three widely used
benchmarks (\acef{}, \ace{} and \scierc{}) for ERE task show significant
improvements over the previous state-of-the-art PL-marker.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17240" title="Abstract">arXiv:2310.17240</a> [<a href="/pdf/2310.17240" title="Download PDF">pdf</a>, <a href="/ps/2310.17240" title="Download PostScript">ps</a>, <a href="/format/2310.17240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Abilities of Forgetful Agents in Stochastic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belardinelli%2C+F">Francesco Belardinelli</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Mittelmann%2C+M">Munyque Mittelmann</a>, 
<a href="/search/cs?searchtype=author&query=Murano%2C+A">Aniello Murano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">In this paper, we investigate the probabilistic variants of the strategy
logics ATL and ATL* under imperfect information. Specifically, we present novel
decidability and complexity results when the model transitions are stochastic
and agents play uniform strategies. That is, the semantics of the logics are
based on multi-agent, stochastic transition systems with imperfect information,
which combine two sources of uncertainty, namely, the partial observability
agents have on the environment, and the likelihood of transitions to occur from
a system state. Since the model checking problem is undecidable in general in
this setting, we restrict our attention to agents with memoryless (positional)
strategies. The resulting setting captures the situation in which agents have
qualitative uncertainty of the local state and quantitative uncertainty about
the occurrence of future events. We illustrate the usefulness of this setting
with meaningful examples.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17245" title="Abstract">arXiv:2310.17245</a> [<a href="/pdf/2310.17245" title="Download PDF">pdf</a>, <a href="/format/2310.17245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CROP: Conservative Reward for Model-based Offline Policy Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiao-Hu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiao-Liang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shi-Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhen-Qiu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao-Yin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+M">Mei-Jiang Gui</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tian-Yu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">De-Xing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bo-Xian Yao</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Z">Zeng-Guang Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline reinforcement learning (RL) aims to optimize policy using collected
data without online interactions. Model-based approaches are particularly
appealing for addressing offline RL challenges due to their capability to
mitigate the limitations of offline data through data generation using models.
Prior research has demonstrated that introducing conservatism into the model or
Q-function during policy optimization can effectively alleviate the prevalent
distribution drift problem in offline RL. However, the investigation into the
impacts of conservatism in reward estimation is still lacking. This paper
proposes a novel model-based offline RL algorithm, Conservative Reward for
model-based Offline Policy optimization (CROP), which conservatively estimates
the reward in model training. To achieve a conservative reward estimation, CROP
simultaneously minimizes the estimation error and the reward of random actions.
Theoretical analysis shows that this conservative reward mechanism leads to a
conservative policy evaluation and helps mitigate distribution drift.
Experiments on D4RL benchmarks showcase that the performance of CROP is
comparable to the state-of-the-art baselines. Notably, CROP establishes an
innovative connection between offline and online RL, highlighting that offline
RL problems can be tackled by adopting online RL techniques to the empirical
Markov decision process trained with a conservative reward. The source code is
available with https://github.com/G0K0URURI/CROP.git.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17247" title="Abstract">arXiv:2310.17247</a> [<a href="/pdf/2310.17247" title="Download PDF">pdf</a>, <a href="/format/2310.17247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grokking Beyond Neural Networks: An Empirical Exploration with Model  Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miller%2C+J">Jack Miller</a>, 
<a href="/search/cs?searchtype=author&query=O%27Neill%2C+C">Charles O&#x27;Neill</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+T">Thang Bui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In some settings neural networks exhibit a phenomenon known as grokking,
where they achieve perfect or near-perfect accuracy on the validation set long
after the same performance has been achieved on the training set. In this
paper, we discover that grokking is not limited to neural networks but occurs
in other settings such as Gaussian process (GP) classification, GP regression
and linear regression. We also uncover a mechanism by which to induce grokking
on algorithmic datasets via the addition of dimensions containing spurious
information. The presence of the phenomenon in non-neural architectures
provides evidence that grokking is not specific to SGD or weight norm
regularisation. Instead, grokking may be possible in any setting where solution
search is guided by complexity and error. Based on this insight and further
trends we see in the training trajectories of a Bayesian neural network (BNN)
and GP regression model, we make progress towards a more general theory of
grokking. Specifically, we hypothesise that the phenomenon is governed by the
accessibility of certain regions in the error and complexity landscapes.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17250" title="Abstract">arXiv:2310.17250</a> [<a href="/pdf/2310.17250" title="Download PDF">pdf</a>, <a href="/format/2310.17250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IDENAS: Internal Dependency Exploration for Neural Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoang%2C+A+T">Anh T. Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Viharos%2C+Z+J">Zsolt J. Viharos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 57 pages, 19 figures + appendix, the related software code can be found under the link: <a href="https://github.com/viharoszsolt/IDENAS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Machine learning is a powerful tool for extracting valuable information and
making various predictions from diverse datasets. Traditional algorithms rely
on well-defined input and output variables however, there are scenarios where
the distinction between the input and output variables and the underlying,
associated (input and output) layers of the model, are unknown. Neural
Architecture Search (NAS) and Feature Selection have emerged as promising
solutions in such scenarios. This research proposes IDENAS, an Internal
Dependency-based Exploration for Neural Architecture Search, integrating NAS
with feature selection. The methodology explores internal dependencies in the
complete parameter space for classification involving 1D sensor and 2D image
data as well. IDENAS employs a modified encoder-decoder model and the
Sequential Forward Search (SFS) algorithm, combining input-output configuration
search with embedded feature selection. Experimental results demonstrate
IDENASs superior performance in comparison to other algorithms, showcasing its
effectiveness in model development pipelines and automated machine learning. On
average, IDENAS achieved significant modelling improvements, underscoring its
significant contribution to advancing the state-of-the-art in neural
architecture search and feature selection integration.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17255" title="Abstract">arXiv:2310.17255</a> [<a href="/pdf/2310.17255" title="Download PDF">pdf</a>, <a href="/format/2310.17255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing to Unseen Domains in Diabetic Retinopathy Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galappaththige%2C+C+J">Chamuditha Jayanga Galappaththige</a>, 
<a href="/search/cs?searchtype=author&query=Kuruppu%2C+G">Gayal Kuruppu</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+H">Muhammad Haris Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diabetic retinopathy (DR). is caused by long-standing diabetes and is among
the fifth leading cause for visual impairments. The process of early diagnosis
and treatments could be helpful in curing the disease, however, the detection
procedure is rather challenging and mostly tedious. Therefore, automated
diabetic retinopathy classification using deep learning techniques has gained
interest in the medical imaging community. Akin to several other real-world
applications of deep learning, the typical assumption of i.i.d data is also
violated in DR classification that relies on deep learning. Therefore,
developing DR classification methods robust to unseen distributions is of great
value. In this paper, we study the problem of generalizing a model to unseen
distributions or domains (a.k.a domain generalization) in DR classification. To
this end, we propose a simple and effective domain generalization (DG) approach
that achieves self-distillation in vision transformers (ViT) via a novel
prediction softening mechanism. This prediction softening is an adaptive convex
combination one-hot labels with the model's own knowledge. We perform extensive
experiments on challenging open-source DR classification datasets under both
multi-source and single-source DG settings with three different ViT backbones
to establish the efficacy and applicability of our approach against competing
methods. For the first time, we report the performance of several
state-of-the-art DG methods on open-source DR classification datasets after
conducting thorough experiments. Finally, our method is also capable of
delivering improved calibration performance than other methods, showing its
suitability for safety-critical applications, including healthcare. We hope
that our contributions would investigate more DG research across the medical
imaging community.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17256" title="Abstract">arXiv:2310.17256</a> [<a href="/pdf/2310.17256" title="Download PDF">pdf</a>, <a href="/format/2310.17256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> fairret: a Framework for Differentiable Fairness Regularization Terms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buyl%2C+M">Maarten Buyl</a>, 
<a href="/search/cs?searchtype=author&query=Defrance%2C+M">MaryBeth Defrance</a>, 
<a href="/search/cs?searchtype=author&query=De+Bie%2C+T">Tijl De Bie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Current tools for machine learning fairness only admit a limited range of
fairness definitions and have seen little integration with automatic
differentiation libraries, despite the central role these libraries play in
modern machine learning pipelines.
<br />We introduce a framework of fairness regularization terms (fairrets) which
quantify bias as modular objectives that are easily integrated in automatic
differentiation pipelines. By employing a general definition of fairness in
terms of linear-fractional statistics, a wide class of fairrets can be computed
efficiently. Experiments show the behavior of their gradients and their utility
in enforcing fairness with minimal loss of predictive power compared to
baselines. Our contribution includes a PyTorch implementation of the fairret
framework.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17260" title="Abstract">arXiv:2310.17260</a> [<a href="/pdf/2310.17260" title="Download PDF">pdf</a>, <a href="/format/2310.17260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Socially Beneficial Metaverse: Framework, Technologies, Applications,  and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaolong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuanhong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bilal%2C+M">Muhammad Bilal</a>, 
<a href="/search/cs?searchtype=author&query=Zeadally%2C+S">Sherali Zeadally</a>, 
<a href="/search/cs?searchtype=author&query=Crowcroft%2C+J">Jon Crowcroft</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+L">Lianyong Qi</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Shengjun Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 6 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In recent years, the maturation of emerging technologies such as Virtual
Reality, Digital twins, and Blockchain has accelerated the realization of the
metaverse. As a virtual world independent of the real world, the metaverse will
provide users with a variety of virtual activities that bring great convenience
to society. In addition, the metaverse can facilitate digital twins, which
offers transformative possibilities for the industry. Thus, the metaverse has
attracted the attention of the industry, and a huge amount of capital is about
to be invested. However, the development of the metaverse is still in its
infancy and little research has been undertaken so far. We describe the
development of the metaverse. Next, we introduce the architecture of the
socially beneficial metaverse (SB-Metaverse) and we focus on the technologies
that support the operation of SB-Metaverse. In addition, we also present the
applications of SB-Metaverse. Finally, we discuss several challenges faced by
SB-Metaverse which must be addressed in the future.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17261" title="Abstract">arXiv:2310.17261</a> [<a href="/pdf/2310.17261" title="Download PDF">pdf</a>, <a href="/format/2310.17261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attribute Based Interpretable Evaluation Metrics for Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dongkyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+M">Mingi Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Uh%2C+Y">Youngjung Uh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">When the training dataset comprises a 1:1 proportion of dogs to cats, a
generative model that produces 1:1 dogs and cats better resembles the training
species distribution than another model with 3:1 dogs and cats. Can we capture
this phenomenon using existing metrics? Unfortunately, we cannot, because these
metrics do not provide any interpretability beyond "diversity". In this
context, we propose a new evaluation protocol that measures the divergence of a
set of generated images from the training set regarding the distribution of
attribute strengths as follows. Single-attribute Divergence (SaD) measures the
divergence regarding PDFs of a single attribute. Paired-attribute Divergence
(PaD) measures the divergence regarding joint PDFs of a pair of attributes.
They provide which attributes the models struggle. For measuring the attribute
strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures
the cosine similarity between image and text vectors with heterogeneous initial
points. With SaD and PaD, we reveal the following about existing generative
models. ProjectedGAN generates implausible attribute relationships such as a
baby with a beard even though it has competitive scores of existing metrics.
Diffusion models struggle to capture diverse colors in the datasets. The larger
sampling timesteps of latent diffusion model generate the more minor objects
including earrings and necklaces. Stable Diffusion v1.5 better captures the
attributes than v2.1. Our metrics lay a foundation for explainable evaluations
of generative models.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17264" title="Abstract">arXiv:2310.17264</a> [<a href="/pdf/2310.17264" title="Download PDF">pdf</a>, <a href="/format/2310.17264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance of ML-based software fault predictors: are we really improving  fault prediction?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahini%2C+X">Xhulja Shahini</a>, 
<a href="/search/cs?searchtype=author&query=Bubel%2C+D">Domenic Bubel</a>, 
<a href="/search/cs?searchtype=author&query=Metzger%2C+A">Andreas Metzger</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 49th Euromicro Conference on Software Engineering and Advanced
  Applications (SEAA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Software quality assurance activities become increasingly difficult as
software systems become more and more complex and continuously grow in size.
Moreover, testing becomes even more expensive when dealing with large-scale
systems. Thus, to effectively allocate quality assurance resources, researchers
have proposed fault prediction (FP) which utilizes machine learning (ML) to
predict fault-prone code areas. However, ML algorithms typically make use of
stochastic elements to increase the prediction models' generalizability and
efficiency of the training process. These stochastic elements, also known as
nondeterminism-introducing (NI) factors, lead to variance in the training
process and as a result, lead to variance in prediction accuracy and training
time. This variance poses a challenge for reproducibility in research. More
importantly, while fault prediction models may have shown good performance in
the lab (e.g., often-times involving multiple runs and averaging outcomes),
high variance of results can pose the risk that these models show low
performance when applied in practice. In this work, we experimentally analyze
the variance of a state-of-the-art fault prediction approach. Our experimental
results indicate that NI factors can indeed cause considerable variance in the
fault prediction models' accuracy. We observed a maximum variance of 10.10% in
terms of the per-class accuracy metric. We thus, also discuss how to deal with
such variance.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17271" title="Abstract">arXiv:2310.17271</a> [<a href="/pdf/2310.17271" title="Download PDF">pdf</a>, <a href="/format/2310.17271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Role of Input Token Characters in Language Models: How  Does Information Loss Affect Performance?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alajrami%2C+A">Ahmed Alajrami</a>, 
<a href="/search/cs?searchtype=author&query=Margatina%2C+K">Katerina Margatina</a>, 
<a href="/search/cs?searchtype=author&query=Aletras%2C+N">Nikolaos Aletras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Understanding how and what pre-trained language models (PLMs) learn about
language is an open challenge in natural language processing. Previous work has
focused on identifying whether they capture semantic and syntactic information,
and how the data or the pre-training objective affects their performance.
However, to the best of our knowledge, no previous work has specifically
examined how information loss in input token characters affects the performance
of PLMs. In this study, we address this gap by pre-training language models
using small subsets of characters from individual tokens. Surprisingly, we find
that pre-training even under extreme settings, i.e. using only one character of
each token, the performance retention in standard NLU benchmarks and probing
tasks compared to full-token models is high. For instance, a model pre-trained
only on single first characters from tokens achieves performance retention of
approximately $90$\% and $77$\% of the full-token model in SuperGLUE and GLUE
tasks, respectively.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17273" title="Abstract">arXiv:2310.17273</a> [<a href="/pdf/2310.17273" title="Download PDF">pdf</a>, <a href="/format/2310.17273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Looping in the Human: Collaborative and Explainable Bayesian  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adachi%2C+M">Masaki Adachi</a>, 
<a href="/search/cs?searchtype=author&query=Planden%2C+B">Brady Planden</a>, 
<a href="/search/cs?searchtype=author&query=Howey%2C+D+A">David A. Howey</a>, 
<a href="/search/cs?searchtype=author&query=Maundet%2C+K">Krikamol Maundet</a>, 
<a href="/search/cs?searchtype=author&query=Osborne%2C+M+A">Michael A. Osborne</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+S+L">Siu Lun Chau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Like many optimizers, Bayesian optimization often falls short of gaining user
trust due to opacity. While attempts have been made to develop human-centric
optimizers, they typically assume user knowledge is well-specified and
error-free, employing users mainly as supervisors of the optimization process.
We relax these assumptions and propose a more balanced human-AI partnership
with our Collaborative and Explainable Bayesian Optimization (CoExBO)
framework. Instead of explicitly requiring a user to provide a knowledge model,
CoExBO employs preference learning to seamlessly integrate human insights into
the optimization, resulting in algorithmic suggestions that resonate with user
preference. CoExBO explains its candidate selection every iteration to foster
trust, empowering users with a clearer grasp of the optimization. Furthermore,
CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with
extreme adversarial interventions, the algorithm converges asymptotically to a
vanilla Bayesian optimization. We validate CoExBO's efficacy through human-AI
teaming experiments in lithium-ion battery design, highlighting substantial
improvements over conventional methods.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17274" title="Abstract">arXiv:2310.17274</a> [<a href="/pdf/2310.17274" title="Download PDF">pdf</a>, <a href="/format/2310.17274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CuRobo: Parallelized Collision-Free Minimum-Jerk Robot Motion Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sundaralingam%2C+B">Balakumar Sundaralingam</a>, 
<a href="/search/cs?searchtype=author&query=Hari%2C+S+K+S">Siva Kumar Sastry Hari</a>, 
<a href="/search/cs?searchtype=author&query=Fishman%2C+A">Adam Fishman</a>, 
<a href="/search/cs?searchtype=author&query=Garrett%2C+C">Caelan Garrett</a>, 
<a href="/search/cs?searchtype=author&query=Van+Wyk%2C+K">Karl Van Wyk</a>, 
<a href="/search/cs?searchtype=author&query=Blukis%2C+V">Valts Blukis</a>, 
<a href="/search/cs?searchtype=author&query=Millane%2C+A">Alexander Millane</a>, 
<a href="/search/cs?searchtype=author&query=Oleynikova%2C+H">Helen Oleynikova</a>, 
<a href="/search/cs?searchtype=author&query=Handa%2C+A">Ankur Handa</a>, 
<a href="/search/cs?searchtype=author&query=Ramos%2C+F">Fabio Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Ratliff%2C+N">Nathan Ratliff</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+D">Dieter Fox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report, 61 pages, Website: <a href="https://curobo.org">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Hardware Architecture (cs.AR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">This paper explores the problem of collision-free motion generation for
manipulators by formulating it as a global motion optimization problem. We
develop a parallel optimization technique to solve this problem and demonstrate
its effectiveness on massively parallel GPUs. We show that combining simple
optimization techniques with many parallel seeds leads to solving difficult
motion generation problems within 50ms on average, 60x faster than
state-of-the-art (SOTA) trajectory optimization methods. We achieve SOTA
performance by combining L-BFGS step direction estimation with a novel parallel
noisy line search scheme and a particle-based optimization solver. To further
aid trajectory optimization, we develop a parallel geometric planner that plans
within 20ms and also introduce a collision-free IK solver that can solve over
7000 queries/s. We package our contributions into a state of the art GPU
accelerated motion generation library, CuRobo and release it to enrich the
robotics community. Additional details are available at https://curobo.org
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17279" title="Abstract">arXiv:2310.17279</a> [<a href="/pdf/2310.17279" title="Download PDF">pdf</a>, <a href="/format/2310.17279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Logical Forms improve fidelity in Table-to-Text generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alonso%2C+I">I&#xf1;igo Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Agirre%2C+E">Eneko Agirre</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Expert Systems with Applications, Volume 238, Part D, 15 March
  2024, 121869
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Table-to-text systems generate natural language statements from structured
data like tables. While end-to-end techniques suffer from low factual
correctness (fidelity), a previous study reported gains when using manual
logical forms (LF) that represent the selected content and the semantics of the
target text. Given the manual step, it was not clear whether automatic LFs
would be effective, or whether the improvement came from content selection
alone. We present TlT which, given a table and a selection of the content,
first produces LFs and then the textual statement. We show for the first time
that automatic LFs improve quality, with an increase in fidelity of 30 points
over a comparable system not using LFs. Our experiments allow to quantify the
remaining challenges for high factual correctness, with automatic selection of
content coming first, followed by better Logic-to-Text generation and, to a
lesser extent, better Table-to-Logic parsing.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17281" title="Abstract">arXiv:2310.17281</a> [<a href="/pdf/2310.17281" title="Download PDF">pdf</a>, <a href="/format/2310.17281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point  Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sautier%2C+C">Corentin Sautier</a>, 
<a href="/search/cs?searchtype=author&query=Puy%2C+G">Gilles Puy</a>, 
<a href="/search/cs?searchtype=author&query=Boulch%2C+A">Alexandre Boulch</a>, 
<a href="/search/cs?searchtype=author&query=Marlet%2C+R">Renaud Marlet</a>, 
<a href="/search/cs?searchtype=author&query=Lepetit%2C+V">Vincent Lepetit</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 3DV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a surprisingly simple and efficient method for self-supervision of
3D backbone on automotive Lidar point clouds. We design a contrastive loss
between features of Lidar scans captured in the same scene. Several such
approaches have been proposed in the literature from PointConstrast, which uses
a contrast at the level of points, to the state-of-the-art TARL, which uses a
contrast at the level of segments, roughly corresponding to objects. While the
former enjoys a great simplicity of implementation, it is surpassed by the
latter, which however requires a costly pre-processing. In BEVContrast, we
define our contrast at the level of 2D cells in the Bird's Eye View plane.
Resulting cell-level representations offer a good trade-off between the
point-level representations exploited in PointContrast and segment-level
representations exploited in TARL: we retain the simplicity of PointContrast
(cell representations are cheap to compute) while surpassing the performance of
TARL in downstream semantic segmentation.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17284" title="Abstract">arXiv:2310.17284</a> [<a href="/pdf/2310.17284" title="Download PDF">pdf</a>, <a href="/format/2310.17284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Abstract with Nonparametric Variational Information  Bottleneck
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behjati%2C+M">Melika Behjati</a>, 
<a href="/search/cs?searchtype=author&query=Fehr%2C+F">Fabio Fehr</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+J">James Henderson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Learned representations at the level of characters, sub-words, words and
sentences, have each contributed to advances in understanding different NLP
tasks and linguistic phenomena. However, learning textual embeddings is costly
as they are tokenization specific and require different models to be trained
for each level of abstraction. We introduce a novel language representation
model which can learn to compress to different levels of abstraction at
different layers of the same model. We apply Nonparametric Variational
Information Bottleneck (NVIB) to stacked Transformer self-attention layers in
the encoder, which encourages an information-theoretic compression of the
representations through the model. We find that the layers within the model
correspond to increasing levels of abstraction and that their representations
are more linguistically informed. Finally, we show that NVIB compression
results in a model which is more robust to adversarial perturbations.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17286" title="Abstract">arXiv:2310.17286</a> [<a href="/pdf/2310.17286" title="Download PDF">pdf</a>, <a href="/ps/2310.17286" title="Download PostScript">ps</a>, <a href="/format/2310.17286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical properties and numerical approximation of pseudo-parabolic  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Abreu%2C+E">E. Abreu</a>, 
<a href="/search/math?searchtype=author&query=Cuesta%2C+E">E. Cuesta</a>, 
<a href="/search/math?searchtype=author&query=Duran%2C+A">A. Duran</a>, 
<a href="/search/math?searchtype=author&query=Lambert%2C+W">W. Lambert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">The paper is concerned with the mathematical theory and numerical
approximation of systems of partial differential equations (pde) of hyperbolic,
pseudo-parabolic type. Some mathematical properties of the
initial-boundary-value problem (ibvp) with Dirichlet boundary conditions are
first studied. They include the weak formulation, well-posedness and existence
of traveling wave solutions connecting two states, when the equations are
considered as a variant of a conservation law. Then, the numerical
approximation consists of a spectral approximation in space based on Legendre
polynomials along with a temporal discretization with strong stability
preserving (SSP) property. The convergence of the semidiscrete approximation is
proved under suitable regularity conditions on the data. The choice of the
temporal discretization is justified in order to guarantee the stability of the
full discretization when dealing with nonsmooth initial conditions. A
computational study explores the performance of the fully discrete scheme with
regular and nonregular data.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17290" title="Abstract">arXiv:2310.17290</a> [<a href="/pdf/2310.17290" title="Download PDF">pdf</a>, <a href="/format/2310.17290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIO: A Benchmark for Reasoning Intention-Oriented Objects in Open  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+M">Mengxue Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jingkuan Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yunchao Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 D&amp;B accepted. See our project page for more details: <a href="https://reasonio.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Intention-oriented object detection aims to detect desired objects based on
specific intentions or requirements. For instance, when we desire to "lie down
and rest", we instinctively seek out a suitable option such as a "bed" or a
"sofa" that can fulfill our needs. Previous work in this area is limited either
by the number of intention descriptions or by the affordance vocabulary
available for intention objects. These limitations make it challenging to
handle intentions in open environments effectively. To facilitate this
research, we construct a comprehensive dataset called Reasoning
Intention-Oriented Objects (RIO). In particular, RIO is specifically designed
to incorporate diverse real-world scenarios and a wide range of object
categories. It offers the following key features: 1) intention descriptions in
RIO are represented as natural sentences rather than a mere word or verb
phrase, making them more practical and meaningful; 2) the intention
descriptions are contextually relevant to the scene, enabling a broader range
of potential functionalities associated with the objects; 3) the dataset
comprises a total of 40,214 images and 130,585 intention-object pairs. With the
proposed RIO, we evaluate the ability of some existing models to reason
intention-oriented objects in open environments.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17292" title="Abstract">arXiv:2310.17292</a> [<a href="/pdf/2310.17292" title="Download PDF">pdf</a>, <a href="/format/2310.17292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boolean Abstractions for Realizability Modulo Theories (Extended  version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+A">Andoni Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez%2C+C">Cesar Sanchez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">In this paper, we address the problem of the (reactive) realizability of
specifications of theories richer than Booleans, including arithmetic theories.
Our approach transforms theory specifications into purely Boolean
specifications by (1) substituting theory literals by Boolean variables, and
(2) computing an additional Boolean requirement that captures the dependencies
between the new variables imposed by the literals. The resulting specification
can be passed to existing Boolean off-the-shelf realizability tools, and is
realizable if and only if the original specification is realizable. The first
contribution is a brute-force version of our method, which requires a number of
SMT queries that is doubly exponential in the number of input literals. Then,
we present a faster method that exploits a nested encoding of the search for
the extra requirement and uses SAT solving for faster traversing the search
space and uses SMT queries internally. Another contribution is a prototype in
Z3-Python. Finally, we report an empirical evaluation using specifications
inspired in real industrial cases. To the best of our knowledge, this is the
first method that succeeds in non-Boolean LTL realizability.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17294" title="Abstract">arXiv:2310.17294</a> [<a href="/pdf/2310.17294" title="Download PDF">pdf</a>, <a href="/format/2310.17294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scale-Adaptive Feature Aggregation for Efficient Space-Time Video  Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhewei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+A">Ailin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaotao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shuchang Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV2024, 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The Space-Time Video Super-Resolution (STVSR) task aims to enhance the visual
quality of videos, by simultaneously performing video frame interpolation (VFI)
and video super-resolution (VSR). However, facing the challenge of the
additional temporal dimension and scale inconsistency, most existing STVSR
methods are complex and inflexible in dynamically modeling different motion
amplitudes. In this work, we find that choosing an appropriate processing scale
achieves remarkable benefits in flow-based feature propagation. We propose a
novel Scale-Adaptive Feature Aggregation (SAFA) network that adaptively selects
sub-networks with different processing scales for individual samples.
Experiments on four public STVSR benchmarks demonstrate that SAFA achieves
state-of-the-art performance. Our SAFA network outperforms recent
state-of-the-art methods such as TMNet and VideoINR by an average improvement
of over 0.5dB on PSNR, while requiring less than half the number of parameters
and only 1/3 computational costs.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17295" title="Abstract">arXiv:2310.17295</a> [<a href="/pdf/2310.17295" title="Download PDF">pdf</a>, <a href="/ps/2310.17295" title="Download PostScript">ps</a>, <a href="/format/2310.17295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normal Forms for Elements of the ${}^*$-Continuous Kleene Algebras  $K\mathop{\otimes_{\cal R}} C_2&#x27;$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hopkins%2C+M">Mark Hopkins</a>, 
<a href="/search/cs?searchtype=author&query=Lei%C3%9F%2C+H">Hans Lei&#xdf;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">The tensor product $K \mathop{\otimes_{\cal R}} C_2'$ of the
${}^*$-continuous Kleene algebra $K$ with the polycyclic ${}^*$-continuous
Kleene algebra $C_2'$ over two bracket pairs contains a copy of the fixed-point
closure of $K$: the centralizer of $C_2'$ in $K \mathop{\otimes_{\cal R}}
C_2'$. We prove a representation of elements of $K\mathop{\otimes_{\cal R}}
C_2'$ by automata \`a la Kleene and refine it by normal form theorems that
restrict the occurrences of brackets on paths through the automata. This is a
foundation for a calculus of context-free expressions. We also show that $C_2'$
validates a relativized form of the ``completeness property'' that
distinguishes the bra-ket ${}^*$-continuous Kleene algebra $C_2$ from the
polycyclic one.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17297" title="Abstract">arXiv:2310.17297</a> [<a href="/pdf/2310.17297" title="Download PDF">pdf</a>, <a href="/format/2310.17297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redactable and Sanitizable Signature Schemes: Applications and  Limitations for use in Decentralized Digital Identity Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumara%2C+B">Bryan Kumara</a>, 
<a href="/search/cs?searchtype=author&query=Hooper%2C+M">Mark Hooper</a>, 
<a href="/search/cs?searchtype=author&query=Maple%2C+C">Carsten Maple</a>, 
<a href="/search/cs?searchtype=author&query=Hobson%2C+T">Timothy Hobson</a>, 
<a href="/search/cs?searchtype=author&query=Crowcroft%2C+J">Jon Crowcroft</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract, 3 Pages, 1 Figure, International Conference on AI and the Digital Economy 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Redactable signature schemes and sanitizable signature schemes are methods
that permit modification of a given digital message and retain a valid
signature. This can be applied to decentralized identity systems for delegating
identity issuance and redacting sensitive information for privacy-preserving
verification of identity. We propose implementing these protocols on a digital
credential and compare them against other privacy-enhancing techniques to
assess their suitability
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17300" title="Abstract">arXiv:2310.17300</a> [<a href="/pdf/2310.17300" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Photorealistic and Animated Embodied Conversational Agents in  Serious Games: An Empirical Study on User Experience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korre%2C+D">Danai Korre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 14 figures, preprint to be published in HCI INTERNATIONAL 2023 25TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multimedia (cs.MM)

</div>
<p class="mathjax">Embodied conversational agents (ECAs) are paradigms of conversational user
interfaces in the form of embodied characters. While ECAs offer various
manipulable features, this paper focuses on a study conducted to explore two
distinct levels of presentation realism. The two agent versions are
photorealistic and animated. The study aims to provide insights and design
suggestions for speech-enabled ECAs within serious game environments. A
within-subjects, two-by-two factorial design was employed for this research
with a cohort of 36 participants balanced for gender. The results showed that
both the photorealistic and the animated versions were perceived as highly
usable, with overall mean scores of 5.76 and 5.71, respectively. However, 69.4
per cent of the participants stated they preferred the photorealistic version,
25 per cent stated they preferred the animated version and 5.6 per cent had no
stated preference. The photorealistic agents were perceived as more realistic
and human-like, while the animated characters made the task feel more like a
game. Even though the agents' realism had no significant effect on usability,
it positively influenced participants' perceptions of the agent. This research
aims to lay the groundwork for future studies on ECA realism's impact in
serious games across diverse contexts.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17304" title="Abstract">arXiv:2310.17304</a> [<a href="/pdf/2310.17304" title="Download PDF">pdf</a>, <a href="/format/2310.17304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Static Semantics Reconstruction for Enhancing JavaScript-WebAssembly  Multilingual Malware Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yifan Xia</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Ping He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peiyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shouling Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ESORICS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">The emergence of WebAssembly allows attackers to hide the malicious
functionalities of JavaScript malware in cross-language interoperations, termed
JavaScript-WebAssembly multilingual malware (JWMM). However, existing
anti-virus solutions based on static program analysis are still limited to
monolingual code. As a result, their detection effectiveness decreases
significantly against JWMM. The detection of JWMM is challenging due to the
complex interoperations and semantic diversity between JavaScript and
WebAssembly. To bridge this gap, we present JWBinder, the first technique aimed
at enhancing the static detection of JWMM. JWBinder performs a
language-specific data-flow analysis to capture the cross-language
interoperations and then characterizes the functionalities of JWMM through a
unified high-level structure called Inter-language Program Dependency Graph.
The extensive evaluation on one of the most representative real-world
anti-virus platforms, VirusTotal, shows that \system effectively enhances
anti-virus systems from various vendors and increases the overall successful
detection rate against JWMM from 49.1\% to 86.2\%. Additionally, we assess the
side effects and runtime overhead of JWBinder, corroborating its practical
viability in real-world applications.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17306" title="Abstract">arXiv:2310.17306</a> [<a href="/pdf/2310.17306" title="Download PDF">pdf</a>, <a href="/format/2310.17306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FormaT5: Abstention and Examples for Conditional Table Formatting with  Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mukul Singh</a>, 
<a href="/search/cs?searchtype=author&query=Cambronero%2C+J">Jos&#xe9; Cambronero</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Vu Le</a>, 
<a href="/search/cs?searchtype=author&query=Negreanu%2C+C">Carina Negreanu</a>, 
<a href="/search/cs?searchtype=author&query=Nouri%2C+E">Elnaz Nouri</a>, 
<a href="/search/cs?searchtype=author&query=Raza%2C+M">Mohammad Raza</a>, 
<a href="/search/cs?searchtype=author&query=Verbruggen%2C+G">Gust Verbruggen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> VLDB 2024, 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Databases (cs.DB); Programming Languages (cs.PL)

</div>
<p class="mathjax">Formatting is an important property in tables for visualization,
presentation, and analysis. Spreadsheet software allows users to automatically
format their tables by writing data-dependent conditional formatting (CF)
rules. Writing such rules is often challenging for users as it requires them to
understand and implement the underlying logic. We present FormaT5, a
transformer-based model that can generate a CF rule given the target table and
a natural language description of the desired formatting logic. We find that
user descriptions for these tasks are often under-specified or ambiguous,
making it harder for code generation systems to accurately learn the desired
rule in a single step. To tackle this problem of under-specification and
minimise argument errors, FormaT5 learns to predict placeholders though an
abstention objective. These placeholders can then be filled by a second model
or, when examples of rows that should be formatted are available, by a
programming-by-example system. To evaluate FormaT5 on diverse and real
scenarios, we create an extensive benchmark of 1053 CF tasks, containing
real-world descriptions collected from four different sources. We release our
benchmarks to encourage research in this area. Abstention and filling allow
FormaT5 to outperform 8 different neural approaches on our benchmarks, both
with and without examples. Our results illustrate the value of building
domain-specific learning systems.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17312" title="Abstract">arXiv:2310.17312</a> [<a href="/pdf/2310.17312" title="Download PDF">pdf</a>, <a href="/format/2310.17312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Ensemble Method Based on the Combination of Transformers with  Convolutional Neural Networks to Detect Artificially Generated Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liyanage%2C+V">Vijini Liyanage</a>, 
<a href="/search/cs?searchtype=author&query=Buscaldi%2C+D">Davide Buscaldi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 21st Annual Workshop of the Australasian Language Technology Association (ALTA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Thanks to the state-of-the-art Large Language Models (LLMs), language
generation has reached outstanding levels. These models are capable of
generating high quality content, thus making it a challenging task to detect
generated text from human-written content. Despite the advantages provided by
Natural Language Generation, the inability to distinguish automatically
generated text can raise ethical concerns in terms of authenticity.
Consequently, it is important to design and develop methodologies to detect
artificial content. In our work, we present some classification models
constructed by ensembling transformer models such as Sci-BERT, DeBERTa and
XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate
that the considered ensemble architectures surpass the performance of the
individual transformer models for classification. Furthermore, the proposed
SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared
task 2023 data.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17315" title="Abstract">arXiv:2310.17315</a> [<a href="/pdf/2310.17315" title="Download PDF">pdf</a>, <a href="/format/2310.17315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nabra: Syrian Arabic Dialects with Morphological Annotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nayouf%2C+A">Amal Nayouf</a>, 
<a href="/search/cs?searchtype=author&query=Hammouda%2C+T">Tymaa Hammouda</a>, 
<a href="/search/cs?searchtype=author&query=Jarrar%2C+M">Mustafa Jarrar</a>, 
<a href="/search/cs?searchtype=author&query=Zaraket%2C+F">Fadi Zaraket</a>, 
<a href="/search/cs?searchtype=author&query=Kurdy%2C+M">Mohamad-Bassam Kurdy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper presents Nabra, a corpora of Syrian Arabic dialects with
morphological annotations. A team of Syrian natives collected more than 6K
sentences containing about 60K words from several sources including social
media posts, scripts of movies and series, lyrics of songs and local proverbs
to build Nabra. Nabra covers several local Syrian dialects including those of
Aleppo, Damascus, Deir-ezzur, Hama, Homs, Huran, Latakia, Mardin, Raqqah, and
Suwayda. A team of nine annotators annotated the 60K tokens with full
morphological annotations across sentence contexts. We trained the annotators
to follow methodological annotation guidelines to ensure unique morpheme
annotations, and normalized the annotations. F1 and kappa agreement scores
ranged between 74% and 98% across features, showing the excellent quality of
Nabra annotations. Our corpora are open-source and publicly available as part
of the Currasat portal https://sina.birzeit.edu/currasat.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17316" title="Abstract">arXiv:2310.17316</a> [<a href="/pdf/2310.17316" title="Download PDF">pdf</a>, <a href="/format/2310.17316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with  Rich Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhifei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pengguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+X">Xi Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingcong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Defect inspection is paramount within the closed-loop manufacturing system.
However, existing datasets for defect inspection often lack precision and
semantic granularity required for practical applications. In this paper, we
introduce the Defect Spectrum, a comprehensive benchmark that offers precise,
semantic-abundant, and large-scale annotations for a wide range of industrial
defects. Building on four key industrial benchmarks, our dataset refines
existing annotations and introduces rich semantic details, distinguishing
multiple defect types within a single image. Furthermore, we introduce
Defect-Gen, a two-stage diffusion-based generator designed to create
high-quality and diverse defective images, even when working with limited
datasets. The synthetic images generated by Defect-Gen significantly enhance
the efficacy of defect inspection models. Overall, The Defect Spectrum dataset
demonstrates its potential in defect inspection research, offering a solid
platform for testing and refining advanced models.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17317" title="Abstract">arXiv:2310.17317</a> [<a href="/pdf/2310.17317" title="Download PDF">pdf</a>, <a href="/format/2310.17317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAN Functional Split Options for Integrated Terrestrial and  Non-Terrestrial 6G Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rihan%2C+M">Mohamed Rihan</a>, 
<a href="/search/cs?searchtype=author&query=Due%2C+T">Tim Due</a>, 
<a href="/search/cs?searchtype=author&query=Vakilifard%2C+M">MohammadAmin Vakilifard</a>, 
<a href="/search/cs?searchtype=author&query=Wubben%2C+D">Dirk Wubben</a>, 
<a href="/search/cs?searchtype=author&query=Dekorsy%2C+A">Armin Dekorsy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Leveraging non-terrestrial platforms in 6G networks holds immense
significance as it opens up opportunities to expand network coverage, enhance
connectivity, and support a wide range of innovative applications, including
global-scale Internet of Things and ultra-high-definition content delivery. To
accomplish the seamless integration between terrestrial and non-terrestrial
networks, substantial changes in radio access network (RAN) architecture are
required. These changes involve the development of new RAN solutions that can
efficiently manage the diverse characteristics of both terrestrial and
non-terrestrial components, ensuring smooth handovers, resource allocation, and
quality of service across the integrated network ecosystem. Additionally, the
establishment of robust interconnection and communication protocols between
terrestrial and non-terrestrial elements will be pivotal to utilize the full
potential of 6G technology. Additionally, innovative approaches have been
introduced to split the functionalities within the RAN into centralized and
distributed domains. These novel paradigms are designed to enhance RAN's
flexibility while simultaneously lowering the costs associated with
infrastructure deployment, all while ensuring that the quality of service for
end-users remains unaffected. In this work, we provide an extensive examination
of various Non-Terrestrial Networks (NTN) architectures and the necessary
adaptations required on the existing 5G RAN architecture to align with the
distinct attributes of NTN. Of particular significance, we emphasize the
crucial RAN functional split choices essential for the seamless integration of
terrestrial and non-terrestrial components within advanced 6G networks.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17318" title="Abstract">arXiv:2310.17318</a> [<a href="/pdf/2310.17318" title="Download PDF">pdf</a>, <a href="/format/2310.17318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Behaviours of RESTful APIs in an Industrial Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karlsson%2C+S">Stefan Karlsson</a>, 
<a href="/search/cs?searchtype=author&query=Jongeling%2C+R">Robbert Jongeling</a>, 
<a href="/search/cs?searchtype=author&query=Causevic%2C+A">Adnan Causevic</a>, 
<a href="/search/cs?searchtype=author&query=Sundmark%2C+D">Daniel Sundmark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">A common way of exposing functionality in contemporary systems is by
providing a Web-API based on the REST API architectural guidelines. To describe
REST APIs, the industry standard is currently OpenAPI-specifications. Test
generation and fuzzing methods targeting OpenAPI-described REST APIs have been
a very active research area in recent years. An open research challenge is to
aid users in better understanding their API, in addition to finding faults and
to cover all the code. In this paper, we address this challenge by proposing a
set of behavioural properties, common to REST APIs, which are used to generate
examples of behaviours that these APIs exhibit. These examples can be used both
(i) to further the understanding of the API and (ii) as a source of automatic
test cases. Our evaluation shows that our approach can generate examples deemed
relevant for understanding the system and for a source of test generation by
practitioners. In addition, we show that basing test generation on behavioural
properties provides tests that are less dependent on the state of the system,
while at the same time yielding a similar code coverage as state-of-the-art
methods in REST API fuzzing in a given time limit.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17320" title="Abstract">arXiv:2310.17320</a> [<a href="/pdf/2310.17320" title="Download PDF">pdf</a>, <a href="/format/2310.17320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mode Selection for Component Mode Synthesis with Guaranteed Assembly  Accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Janssen%2C+L+A+L">Lars A.L. Janssen</a>, 
<a href="/search/eess?searchtype=author&query=Fey%2C+R+H+B">Rob H.B. Fey</a>, 
<a href="/search/eess?searchtype=author&query=Besselink%2C+B">Bart Besselink</a>, 
<a href="/search/eess?searchtype=author&query=van+de+Wouw%2C+N">Nathan van de Wouw</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this work, a modular approach is introduced to select the most important
eigenmodes for each component of a composed structural dynamics system to
obtain the required accuracy of the reduced-order assembly model. To enable the
use of models of complex (structural) dynamical systems in engineering
practice, e.g., in a design, optimization and/or control context, the
complexity of the models needs to be reduced. When the model consist of an
assembly of multiple interconnected structural components, component mode
synthesis is often the preferred model reduction method. The standard approach
to component mode synthesis for such system is to select the eigenmodes of a
component that are most important to accurately model the dynamic behavior of
this component in a certain frequency range of interest. However, often, a more
relevant goal is to obtain, in this frequency range, an accurate model of the
assembly. In the proposed approach, accuracy requirements on the level of the
assembly are translated to accuracy requirements on component level, by
employing techniques from the field of systems and control. With these
component-level requirements, the eigenmodes that are most important to
accurately model the dynamic behavior of the assembly can be selected in a
modular fashion. We demonstrate with two structural dynamics benchmark systems
that this method based on assembly accuracy allows for a computationally
efficient selection of eigenmodes that 1) guarantees satisfaction of the
assembly accuracy requirements and 2) results in most cases in reduced-order
models of significantly lower order with respect to the industrial standard
approach in which component eigenmodes are selected using a frequency
criterion.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17323" title="Abstract">arXiv:2310.17323</a> [<a href="/pdf/2310.17323" title="Download PDF">pdf</a>, <a href="/format/2310.17323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IndustReal: A Dataset for Procedure Step Recognition Handling Execution  Errors in Egocentric Videos in an Industrial-Like Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schoonbeek%2C+T+J">Tim J. Schoonbeek</a>, 
<a href="/search/cs?searchtype=author&query=Houben%2C+T">Tim Houben</a>, 
<a href="/search/cs?searchtype=author&query=Onvlee%2C+H">Hans Onvlee</a>, 
<a href="/search/cs?searchtype=author&query=de+With%2C+P+H+N">Peter H.N. de With</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Sommen%2C+F">Fons van der Sommen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for WACV 2024. 15 pages, 9 figures, including supplementary materials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Although action recognition for procedural tasks has received notable
attention, it has a fundamental flaw in that no measure of success for actions
is provided. This limits the applicability of such systems especially within
the industrial domain, since the outcome of procedural actions is often
significantly more important than the mere execution. To address this
limitation, we define the novel task of procedure step recognition (PSR),
focusing on recognizing the correct completion and order of procedural steps.
Alongside the new task, we also present the multi-modal IndustReal dataset.
Unlike currently available datasets, IndustReal contains procedural errors
(such as omissions) as well as execution errors. A significant part of these
errors are exclusively present in the validation and test sets, making
IndustReal suitable to evaluate robustness of algorithms to new, unseen
mistakes. Additionally, to encourage reproducibility and allow for scalable
approaches trained on synthetic data, the 3D models of all parts are publicly
available. Annotations and benchmark performance are provided for action
recognition and assembly state detection, as well as the new PSR task.
IndustReal, along with the code and model weights, is available at:
https://github.com/TimSchoonbeek/IndustReal .
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17325" title="Abstract">arXiv:2310.17325</a> [<a href="/pdf/2310.17325" title="Download PDF">pdf</a>, <a href="/format/2310.17325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> C-Disentanglement: Discovering Causally-Independent Generative Factors  under an Inductive Bias of Confounder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiaxin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuancheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Representation learning assumes that real-world data is generated by a few
semantically meaningful generative factors (i.e., sources of variation) and
aims to discover them in the latent space. These factors are expected to be
causally disentangled, meaning that distinct factors are encoded into separate
latent variables, and changes in one factor will not affect the values of the
others. Compared to statistical independence, causal disentanglement allows
more controllable data generation, improved robustness, and better
generalization. However, most existing work assumes unconfoundedness in the
discovery process, that there are no common causes to the generative factors
and thus obtain only statistical independence. In this paper, we recognize the
importance of modeling confounders in discovering causal generative factors.
Unfortunately, such factors are not identifiable without proper inductive bias.
We fill the gap by introducing a framework entitled Confounded-Disentanglement
(C-Disentanglement), the first framework that explicitly introduces the
inductive bias of confounder via labels from domain expertise. In addition, we
accordingly propose an approach to sufficiently identify the causally
disentangled factors under any inductive bias of the confounder. We conduct
extensive experiments on both synthetic and real-world datasets. Our method
demonstrates competitive results compared to various SOTA baselines in
obtaining causally disentangled features and downstream tasks under domain
shifts.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17327" title="Abstract">arXiv:2310.17327</a> [<a href="/pdf/2310.17327" title="Download PDF">pdf</a>, <a href="/format/2310.17327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Field Positioning and Attitude Sensing Based on Electromagnetic  Propagation Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Ang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+N">Nan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Changsheng You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures. Submitted to JSAC - Special Issue on Positioning and Sensing Over Wireless Networks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Positioning and sensing over wireless networks are imperative for many
emerging applications. However, traditional wireless channel models cannot be
used for sensing the attitude of the user equipment (UE), since they
over-simplify the UE as a point target. In this paper, a comprehensive
electromagnetic propagation modeling (EPM) based on electromagnetic theory is
developed to precisely model the near-field channel. For the noise-free case,
the EPM model establishes the non-linear functional dependence of observed
signals on both the position and attitude of the UE. To address the difficulty
in the non-linear coupling, we first propose to divide the distance domain into
three regions, separated by the defined Phase ambiguity distance and Spacing
constraint distance. Then, for each region, we obtain the closed-form solutions
for joint position and attitude estimation with low complexity. Next, to
investigate the impact of random noise on the joint estimation performance, the
Ziv-Zakai bound (ZZB) is derived to yield useful insights. The expected
Cram\'er-Rao bound (ECRB) is further provided to obtain the simplified
closed-form expressions for the performance lower bounds. Our numerical results
demonstrate that the derived ZZB can provide accurate predictions of the
performance of estimators in all signal-to-noise ratio (SNR) regimes. More
importantly, we achieve the millimeter-level accuracy in position estimation
and attain the 0.1-level accuracy in attitude estimation.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17330" title="Abstract">arXiv:2310.17330</a> [<a href="/pdf/2310.17330" title="Download PDF">pdf</a>, <a href="/format/2310.17330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CQM: Curriculum Reinforcement Learning with a Quantized World Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seungjae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+D">Daesol Cho</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jonghae Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">H. Jin Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent curriculum Reinforcement Learning (RL) has shown notable progress in
solving complex tasks by proposing sequences of surrogate tasks. However, the
previous approaches often face challenges when they generate curriculum goals
in a high-dimensional space. Thus, they usually rely on manually specified goal
spaces. To alleviate this limitation and improve the scalability of the
curriculum, we propose a novel curriculum method that automatically defines the
semantic goal space which contains vital information for the curriculum
process, and suggests curriculum goals over it. To define the semantic goal
space, our method discretizes continuous observations via vector
quantized-variational autoencoders (VQ-VAE) and restores the temporal relations
between the discretized observations by a graph. Concurrently, ours suggests
uncertainty and temporal distance-aware curriculum goals that converges to the
final goals over the automatically composed goal space. We demonstrate that the
proposed method allows efficient explorations in an uninformed environment with
raw goal examples only. Also, ours outperforms the state-of-the-art curriculum
RL methods on data efficiency and performance, in various goal-reaching tasks
even with ego-centric visual inputs.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17331" title="Abstract">arXiv:2310.17331</a> [<a href="/pdf/2310.17331" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel solution for seepage problems using physics-informed neural  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tianfu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yelin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingfu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zongliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Mingjiao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zaihong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Dawei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">A Physics-Informed Neural Network (PINN) provides a distinct advantage by
synergizing neural networks' capabilities with the problem's governing physical
laws. In this study, we introduce an innovative approach for solving seepage
problems by utilizing the PINN, harnessing the capabilities of Deep Neural
Networks (DNNs) to approximate hydraulic head distributions in seepage
analysis. To effectively train the PINN model, we introduce a comprehensive
loss function comprising three components: one for evaluating differential
operators, another for assessing boundary conditions, and a third for
appraising initial conditions. The validation of the PINN involves solving four
benchmark seepage problems. The results unequivocally demonstrate the
exceptional accuracy of the PINN in solving seepage problems, surpassing the
accuracy of FEM in addressing both steady-state and free-surface seepage
problems. Hence, the presented approach highlights the robustness of the PINN
and underscores its precision in effectively addressing a spectrum of seepage
challenges. This amalgamation enables the derivation of accurate solutions,
overcoming limitations inherent in conventional methods such as mesh generation
and adaptability to complex geometries.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17332" title="Abstract">arXiv:2310.17332</a> [<a href="/pdf/2310.17332" title="Download PDF">pdf</a>, <a href="/format/2310.17332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Forecast Stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Godahewa%2C+R">Rakshitha Godahewa</a>, 
<a href="/search/cs?searchtype=author&query=Bergmeir%2C+C">Christoph Bergmeir</a>, 
<a href="/search/cs?searchtype=author&query=Baz%2C+Z+E">Zeynep Erkin Baz</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chengjun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhangdi Song</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa%2C+S">Salvador Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+D">Dario Benavides</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Forecasts are typically not produced in a vacuum but in a business context,
where forecasts are generated on a regular basis and interact with each other.
For decisions, it may be important that forecasts do not change arbitrarily,
and are stable in some sense. However, this area has received only limited
attention in the forecasting literature. In this paper, we explore two types of
forecast stability that we call vertical stability and horizontal stability.
The existing works in the literature are only applicable to certain base models
and extending these frameworks to be compatible with any base model is not
straightforward. Furthermore, these frameworks can only stabilise the forecasts
vertically. To fill this gap, we propose a simple linear-interpolation-based
approach that is applicable to stabilise the forecasts provided by any base
model vertically and horizontally. The approach can produce both accurate and
stable forecasts. Using N-BEATS, Pooled Regression and LightGBM as the base
models, in our evaluation on four publicly available datasets, the proposed
framework is able to achieve significantly higher stability and/or accuracy
compared to a set of benchmarks including a state-of-the-art forecast
stabilisation method across three error metrics and six stability metrics.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17333" title="Abstract">arXiv:2310.17333</a> [<a href="/pdf/2310.17333" title="Download PDF">pdf</a>, <a href="/format/2310.17333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arabic Fine-Grained Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liqreina%2C+H">Haneen Liqreina</a>, 
<a href="/search/cs?searchtype=author&query=Jarrar%2C+M">Mustafa Jarrar</a>, 
<a href="/search/cs?searchtype=author&query=Khalilia%2C+M">Mohammed Khalilia</a>, 
<a href="/search/cs?searchtype=author&query=El-Shangiti%2C+A+O">Ahmed Oumar El-Shangiti</a>, 
<a href="/search/cs?searchtype=author&query=AbdulMageed%2C+M">Muhammad AbdulMageed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Traditional NER systems are typically trained to recognize coarse-grained
entities, and less attention is given to classifying entities into a hierarchy
of fine-grained lower-level subtypes. This article aims to advance Arabic NER
with fine-grained entities. We chose to extend Wojood (an open-source Nested
Arabic Named Entity Corpus) with subtypes. In particular, four main entity
types in Wojood, geopolitical entity (GPE), location (LOC), organization (ORG),
and facility (FAC), are extended with 31 subtypes. To do this, we first revised
Wojood's annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC's
ACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC,
ORG, and FAC (~44K) in Wojood are manually annotated with the LDC's ACE
sub-types. We refer to this extended version of Wojood as WojoodF ine. To
evaluate our annotations, we measured the inter-annotator agreement (IAA) using
both Cohen's Kappa and F1 score, resulting in 0.9861 and 0.9889, respectively.
To compute the baselines of WojoodF ine, we fine-tune three pre-trained Arabic
BERT encoders in three settings: flat NER, nested NER and nested NER with
subtypes and achieved F1 score of 0.920, 0.866, and 0.885, respectively. Our
corpus and models are open-source and available at
https://sina.birzeit.edu/wojood/.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17335" title="Abstract">arXiv:2310.17335</a> [<a href="/pdf/2310.17335" title="Download PDF">pdf</a>, <a href="/format/2310.17335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A multi-artifact EEG denoising by frequency-based deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gabardi%2C+M">Matteo Gabardi</a>, 
<a href="/search/cs?searchtype=author&query=Saibene%2C+A">Aurora Saibene</a>, 
<a href="/search/cs?searchtype=author&query=Gasparini%2C+F">Francesca Gasparini</a>, 
<a href="/search/cs?searchtype=author&query=Rizzo%2C+D">Daniele Rizzo</a>, 
<a href="/search/cs?searchtype=author&query=Stella%2C+F+A">Fabio Antonio Stella</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Italian Workshop on Artificial Intelligence for Human-Machine Interaction (AIxHMI 2023), November 06, 2023, Rome, Italy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Electroencephalographic (EEG) signals are fundamental to neuroscience
research and clinical applications such as brain-computer interfaces and
neurological disorder diagnosis. These signals are typically a combination of
neurological activity and noise, originating from various sources, including
physiological artifacts like ocular and muscular movements. Under this setting,
we tackle the challenge of distinguishing neurological activity from
noise-related sources. We develop a novel EEG denoising model that operates in
the frequency domain, leveraging prior knowledge about noise spectral features
to adaptively compute optimal convolutional filters for noise separation. The
model is trained to learn an empirical relationship connecting the spectral
characteristics of noise and noisy signal to a non-linear transformation which
allows signal denoising. Performance evaluation on the EEGdenoiseNet dataset
shows that the proposed model achieves optimal results according to both
temporal and spectral metrics. The model is found to remove physiological
artifacts from input EEG data, thus achieving effective EEG denoising. Indeed,
the model performance either matches or outperforms that achieved by benchmark
models, proving to effectively remove both muscle and ocular artifacts without
the need to perform any training on the particular type of artifact.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17338" title="Abstract">arXiv:2310.17338</a> [<a href="/pdf/2310.17338" title="Download PDF">pdf</a>, <a href="/format/2310.17338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acceleration and restart for the randomized Bregman-Kaczmarz method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tondji%2C+L">Lionel Tondji</a>, 
<a href="/search/math?searchtype=author&query=Necoara%2C+I">Ion Necoara</a>, 
<a href="/search/math?searchtype=author&query=Lorenz%2C+D+A">Dirk A. Lorenz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Optimizing strongly convex functions subject to linear constraints is a
fundamental problem with numerous applications. In this work, we propose a
block (accelerated) randomized Bregman-Kaczmarz method that only uses a block
of constraints in each iteration to tackle this problem. We consider a dual
formulation of this problem in order to deal in an efficient way with the
linear constraints. Using convex tools, we show that the corresponding dual
function satisfies the Polyak-Lojasiewicz (PL) property, provided that the
primal objective function is strongly convex and verifies additionally some
other mild assumptions. However, adapting the existing theory on coordinate
descent methods to our dual formulation can only give us sublinear convergence
results in the dual space. In order to obtain convergence results in some
criterion corresponding to the primal (original) problem, we transfer our
algorithm to the primal space, which combined with the PL property allows us to
get linear convergence rates. More specifically, we provide a theoretical
analysis of the convergence of our proposed method under different assumptions
on the objective and demonstrate in the numerical experiments its superior
efficiency and speed up compared to existing methods for the same problem.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17341" title="Abstract">arXiv:2310.17341</a> [<a href="/pdf/2310.17341" title="Download PDF">pdf</a>, <a href="/format/2310.17341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> De-novo Chemical Reaction Generation by Means of Temporarily  Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buin%2C+A">Andrei Buin</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+H+Y">Hung Yi Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Gadsden%2C+S+A">S. Andrew Gadsden</a>, 
<a href="/search/cs?searchtype=author&query=Alderson%2C+F+A">Faraz A. Alderson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present here a combination of two networks, Recurrent Neural Networks
(RNN) and Temporarily Convolutional Neural Networks (TCN) in de novo reaction
generation using the novel Reaction Smiles-like representation of reactions
(CGRSmiles) with atom mapping directly incorporated. Recurrent Neural Networks
are known for their autoregressive properties and are frequently used in
language modelling with direct application to SMILES generation. The relatively
novel TCNs possess similar properties with wide receptive field while obeying
the causality required for natural language processing (NLP). The combination
of both latent representations expressed through TCN and RNN results in an
overall better performance compared to RNN alone. Additionally, it is shown
that different fine-tuning protocols have a profound impact on generative scope
of the model when applied on a dataset of interest via transfer learning.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17342" title="Abstract">arXiv:2310.17342</a> [<a href="/pdf/2310.17342" title="Download PDF">pdf</a>, <a href="/format/2310.17342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACT-SQL: In-Context Learning for Text-to-SQL with  Automatically-Generated Chain-of-Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanchong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+R">Ruisheng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongshen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently Large Language Models (LLMs) have been proven to have strong
abilities in various domains and tasks. We study the problem of prompt
designing in the text-to-SQL task and attempt to improve the LLMs' reasoning
ability when generating SQL queries. Besides the trivial few-shot in-context
learning setting, we design our chain-of-thought (CoT) prompt with a similar
method to schema linking. We provide a method named ACT-SQL to automatically
generate auto-CoT exemplars and thus the whole process doesn't need manual
labeling. Our approach is cost-saving since we only use the LLMs' API call once
when generating one SQL query. Furthermore, we extend our in-context learning
method to the multi-turn text-to-SQL task. The experiment results show that the
LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves
SOTA performance on the Spider dev set among existing in-context learning
approaches.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17344" title="Abstract">arXiv:2310.17344</a> [<a href="/pdf/2310.17344" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examination of Cybersickness in Virtual Reality: The Role of Individual  Differences, Effects on Cognitive Functions &amp; Motor Skills, and Intensity  Differences During and After Immersion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kourtesis%2C+P">Panagiotis Kourtesis</a>, 
<a href="/search/cs?searchtype=author&query=Papadopoulou%2C+A">Agapi Papadopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Roussos%2C+P">Petros Roussos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 Pages, 4 figures, 14 Tables. The article has been submitted to Virtual Worlds Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Background: Given that VR is applied in multiple domains, understanding the
effects of cyber-sickness on human cognition and motor skills and the factors
contributing to cybersickness gains urgency. This study aimed to explore the
predictors of cybersickness and its interplay with cognitive and motor skills.
Methods: 30 participants, 20-45 years old, completed the MSSQ and the CSQ-VR,
and were immersed in VR. During immersion, they were exposed to a roller
coaster ride. Before and after the ride, participants responded to CSQ-VR and
performed VR-based cognitive and psychomotor tasks. Post-VR session,
participants completed the CSQ-VR again. Results: Motion sickness
susceptibility, during adulthood, was the most prominent predictor of
cybersickness. Pupil dilation emerged as a significant predictor of
cybersickness. Experience in videogaming was a significant predictor of both
cybersickness and cognitive/motor functions. Cybersickness negatively affected
visuospatial working memory and psychomotor skills. Overall cybersickness',
nausea and vestibular symptoms' intensities significantly decreased after
removing the VR headset. Conclusions: In order of importance, motion sickness
susceptibility and gaming experience are significant predictors of
cybersickness. Pupil dilation appears as a cybersickness' biomarker.
Cybersickness negatively affects visuospatial working memory and psychomotor
skills. Cybersickness and its effects on performance should be examined during
and not after immersion.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17347" title="Abstract">arXiv:2310.17347</a> [<a href="/pdf/2310.17347" title="Download PDF">pdf</a>, <a href="/format/2310.17347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CADS: Unleashing the Diversity of Diffusion Models through  Condition-Annealed Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadat%2C+S">Seyedmorteza Sadat</a>, 
<a href="/search/cs?searchtype=author&query=Buhmann%2C+J">Jakob Buhmann</a>, 
<a href="/search/cs?searchtype=author&query=Bradely%2C+D">Derek Bradely</a>, 
<a href="/search/cs?searchtype=author&query=Hilliges%2C+O">Otmar Hilliges</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+R+M">Romann M. Weber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While conditional diffusion models are known to have good coverage of the
data distribution, they still face limitations in output diversity,
particularly when sampled with a high classifier-free guidance scale for
optimal image quality or when trained on small datasets. We attribute this
problem to the role of the conditioning signal in inference and offer an
improved sampling strategy for diffusion models that can increase generation
diversity, especially at high guidance scales, with minimal loss of sample
quality. Our sampling strategy anneals the conditioning signal by adding
scheduled, monotonically decreasing Gaussian noise to the conditioning vector
during inference to balance diversity and condition alignment. Our
Condition-Annealed Diffusion Sampler (CADS) can be used with any pretrained
model and sampling algorithm, and we show that it boosts the diversity of
diffusion models in various conditional generation tasks. Further, using an
existing pretrained diffusion model, CADS achieves a new state-of-the-art FID
of 1.70 and 2.31 for class-conditional ImageNet generation at 256$\times$256
and 512$\times$512 respectively.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17348" title="Abstract">arXiv:2310.17348</a> [<a href="/pdf/2310.17348" title="Download PDF">pdf</a>, <a href="/format/2310.17348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Intrusion Detection with Edge-Directed Graph Multi-Head  Attention Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yali Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Cangqi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">A network intrusion usually involves a number of network locations. Data flow
(including the data generated by intrusion behaviors) among these locations
(usually represented by IP addresses) naturally forms a graph. Thus, graph
neural networks (GNNs) have been used in the construction of intrusion
detection models in recent years since they have an excellent ability to
capture graph topological features of intrusion data flow. However, existing
GNN models treat node mean aggregation equally in node information aggregation.
In reality, the correlations of nodes and their neighbors as well as the linked
edges are different. Assigning higher weights to nodes and edges with high
similarity can highlight the correlation among them, which will enhance the
accuracy and expressiveness of the model. To this end, this paper proposes
novel Edge-Directed Graph Multi-Head Attention Networks (EDGMAT) for network
intrusion detection. The proposed EDGMAT model introduces a multi-head
attention mechanism into the intrusion detection model. Additional weight
learning is realized through the combination of a multi-head attention
mechanism and edge features. Weighted aggregation makes better use of the
relationship between different network traffic data. Experimental results on
four recent NIDS benchmark datasets show that the performance of EDGMAT in
terms of weighted F1-Score is significantly better than that of four
state-of-the-art models in multi-class detection tasks.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17350" title="Abstract">arXiv:2310.17350</a> [<a href="/pdf/2310.17350" title="Download PDF">pdf</a>, <a href="/format/2310.17350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A mixed FEM for a time-fractional Fokker-Planck model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Karaa%2C+S">Samir Karaa</a>, 
<a href="/search/math?searchtype=author&query=Mustapha%2C+K">Kassem Mustapha</a>, 
<a href="/search/math?searchtype=author&query=Ahmed%2C+N">Naveed Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">We propose and analyze a mixed finite element method for the spatial
approximation of a time-fractional Fokker--Planck equation in a convex
polyhedral domain, where the given driving force is a function of space. Taking
into account the limited smoothing properties of the model, and considering an
appropriate splitting of the errors, we employed a sequence of clever energy
arguments to show optimal convergence rates with respect to both approximation
properties and regularity results. In particular, error bounds for both primary
and secondary variables are derived in $L^2$-norm for cases with smooth and
nonsmooth initial data. We further investigate a fully implicit time-stepping
scheme based on a convolution quadrature in time generated by the backward
Euler method. Our main result provides pointwise-in-time optimal $L^2$-error
estimates for the primary variable. Numerical examples are then presented to
illustrate the theoretical contributions.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17353" title="Abstract">arXiv:2310.17353</a> [<a href="/pdf/2310.17353" title="Download PDF">pdf</a>, <a href="/format/2310.17353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cultural Adaptation of Recipes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kementchedjhieva%2C+Y">Yova Kementchedjhieva</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+R">Ruixiang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Karamolegkou%2C+A">Antonia Karamolegkou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Li Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dare%2C+M">Megan Dare</a>, 
<a href="/search/cs?searchtype=author&query=Donatelli%2C+L">Lucia Donatelli</a>, 
<a href="/search/cs?searchtype=author&query=Hershcovich%2C+D">Daniel Hershcovich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to TACL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Building upon the considerable advances in Large Language Models (LLMs), we
are now equipped to address more sophisticated tasks demanding a nuanced
understanding of cross-cultural contexts. A key example is recipe adaptation,
which goes beyond simple translation to include a grasp of ingredients,
culinary techniques, and dietary preferences specific to a given culture. We
introduce a new task involving the translation and cultural adaptation of
recipes between Chinese and English-speaking cuisines. To support this
investigation, we present CulturalRecipes, a unique dataset comprised of
automatically paired recipes written in Mandarin Chinese and English. This
dataset is further enriched with a human-written and curated test set. In this
intricate task of cross-cultural recipe adaptation, we evaluate the performance
of various methods, including GPT-4 and other LLMs, traditional machine
translation, and information retrieval techniques. Our comprehensive analysis
includes both automatic and human evaluation metrics. While GPT-4 exhibits
impressive abilities in adapting Chinese recipes into English, it still lags
behind human expertise when translating English recipes into Chinese. This
underscores the multifaceted nature of cultural adaptations. We anticipate that
these insights will significantly contribute to future research on
culturally-aware language models and their practical application in culturally
diverse contexts.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17355" title="Abstract">arXiv:2310.17355</a> [<a href="/pdf/2310.17355" title="Download PDF">pdf</a>, <a href="/format/2310.17355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Trie of Rules: a fast data structure for the  representation of association rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kudriavtsev%2C+M">Mikhail Kudriavtsev</a>, 
<a href="/search/cs?searchtype=author&query=Bezbradica%2C+D+M">Dr Marija Bezbradica</a>, 
<a href="/search/cs?searchtype=author&query=McCarren%2C+D+A">Dr Andrew McCarren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 13 figures, preprint of journal article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Association rule mining techniques can generate a large volume of sequential
data when implemented on transactional databases. Extracting insights from a
large set of association rules has been found to be a challenging process. When
examining a ruleset, the fundamental question is how to summarise and represent
meaningful mined knowledge efficiently. Many algorithms and strategies have
been developed to address issue of knowledge extraction; however, the
effectiveness of this process can be limited by the data structures. A better
data structure can sufficiently affect the speed of the knowledge extraction
process. This paper proposes a novel data structure, called the Trie of rules,
for storing a ruleset that is generated by association rule mining. The
resulting data structure is a prefix-tree graph structure made of pre-mined
rules. This graph stores the rules as paths within the prefix-tree in a way
that similar rules overlay each other. Each node in the tree represents a rule
where a consequent is this node, and an antecedent is a path from this node to
the root of the tree. The evaluation showed that the proposed representation
technique is promising. It compresses a ruleset with almost no data loss and
benefits in terms of time for basic operations such as searching for a specific
rule and sorting, which is the base for many knowledge discovery methods.
Moreover, our method demonstrated a significant improvement in traversing time,
achieving an 8-fold increase compared to traditional data structures.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17356" title="Abstract">arXiv:2310.17356</a> [<a href="/pdf/2310.17356" title="Download PDF">pdf</a>, <a href="/format/2310.17356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sky Imager-Based Forecast of Solar Irradiance Using Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-lahham%2C+A">Anas Al-lahham</a>, 
<a href="/search/cs?searchtype=author&query=Theeb%2C+O">Obaidah Theeb</a>, 
<a href="/search/cs?searchtype=author&query=Elalem%2C+K">Khaled Elalem</a>, 
<a href="/search/cs?searchtype=author&query=Alshawi%2C+T+A">Tariq A. Alshawi</a>, 
<a href="/search/cs?searchtype=author&query=Alshebeili%2C+S+A">Saleh A. Alshebeili</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in MDPI Electronics Journal
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Electronics 2020, 9, 1700
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Ahead-of-time forecasting of the output power of power plants is essential
for the stability of the electricity grid and ensuring uninterrupted service.
However, forecasting renewable energy sources is difficult due to the chaotic
behavior of natural energy sources. This paper presents a new approach to
estimate short-term solar irradiance from sky images. The~proposed algorithm
extracts features from sky images and use learning-based techniques to estimate
the solar irradiance. The~performance of proposed machine learning (ML)
algorithm is evaluated using two publicly available datasets of sky images.
The~datasets contain over 350,000 images for an interval of 16 years, from 2004
to 2020, with the corresponding global horizontal irradiance (GHI) of each
image as the ground truth. Compared to the state-of-the-art computationally
heavy algorithms proposed in the literature, our approach achieves competitive
results with much less computational complexity for both nowcasting and
forecasting up to 4 h ahead of time.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17357" title="Abstract">arXiv:2310.17357</a> [<a href="/pdf/2310.17357" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Automated Vehicles on Large Lane-free Roundabouts (Extended  Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Naderi%2C+M">Mehdi Naderi</a>, 
<a href="/search/eess?searchtype=author&query=Papageorgiou%2C+M">Markos Papageorgiou</a>, 
<a href="/search/eess?searchtype=author&query=Troullinos%2C+D">Dimitrios Troullinos</a>, 
<a href="/search/eess?searchtype=author&query=Karafyllis%2C+I">Iasson Karafyllis</a>, 
<a href="/search/eess?searchtype=author&query=Papamichail%2C+I">Ioannis Papamichail</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 21 figures, 1 table, and 47 equations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Controlling automated vehicles on large lane-free roundabouts is challenging
because of the geometrical complexity and frequent conflicts among entering,
rotating, and exiting vehicles. This paper proposes a comprehensive methodology
to control the vehicles within the roundabout and the connected road branches.
The developed real-time vehicle movement strategy relies on offline-computed
wide overlapping movement corridors, one for each Origin-Destination (OD)
movement, which delineate the admissible movement zones of corresponding OD
vehicles. Also, space-dependent desired orientations are determined by
destination, so as to mitigate potential vehicle conflicts and reduce trip
distance. A distributed (per vehicle) movement control strategy, using two
nonlinear feedback controllers (NLFC), for circular and straight movements,
respectively, is employed to navigate each vehicle within the respective OD
corridor toward its destination, accounting for the desired orientation and
avoiding collisions with other vehicles; while boundary controllers guarantee
that the corridor boundaries will not be violated, and the exit will not be
missed. As an overly complicated case study, we consider the famous roundabout
of Place Charles de Gaulle in Paris, featuring a width of 38 m and comprising a
dozen of bidirectional radial streets, hence a total of 144 ODs. The pertinence
and effectiveness of the presented method is verified via microscopic
simulation and evaluation of macroscopic data.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17359" title="Abstract">arXiv:2310.17359</a> [<a href="/pdf/2310.17359" title="Download PDF">pdf</a>, <a href="/format/2310.17359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SE(3) Diffusion Model-based Point Cloud Registration for Robust 6D  Object Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haobo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Salzmann%2C+M">Mathieu Salzmann</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+Z">Zheng Dang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS-2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we introduce an SE(3) diffusion model-based point cloud
registration framework for 6D object pose estimation in real-world scenarios.
Our approach formulates the 3D registration task as a denoising diffusion
process, which progressively refines the pose of the source point cloud to
obtain a precise alignment with the model point cloud. Training our framework
involves two operations: An SE(3) diffusion process and an SE(3) reverse
process. The SE(3) diffusion process gradually perturbs the optimal rigid
transformation of a pair of point clouds by continuously injecting noise
(perturbation transformation). By contrast, the SE(3) reverse process focuses
on learning a denoising network that refines the noisy transformation
step-by-step, bringing it closer to the optimal transformation for accurate
pose estimation. Unlike standard diffusion models used in linear Euclidean
spaces, our diffusion model operates on the SE(3) manifold. This requires
exploiting the linear Lie algebra $\mathfrak{se}(3)$ associated with SE(3) to
constrain the transformation transitions during the diffusion and reverse
processes. Additionally, to effectively train our denoising network, we derive
a registration-specific variational lower bound as the optimization objective
for model learning. Furthermore, we show that our denoising network can be
constructed with a surrogate registration model, making our approach applicable
to different deep registration networks. Extensive experiments demonstrate that
our diffusion registration framework presents outstanding pose estimation
performance on the real-world TUD-L, LINEMOD, and Occluded-LINEMOD datasets.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17360" title="Abstract">arXiv:2310.17360</a> [<a href="/pdf/2310.17360" title="Download PDF">pdf</a>, <a href="/format/2310.17360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Unifying Diffusion Models for Probabilistic Spatio-Temporal  Graph Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junfeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhencheng Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+R">Roger Zimmermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spatio-temporal graph learning is a fundamental problem in the Web of Things
era, which enables a plethora of Web applications such as smart cities, human
mobility and climate analysis. Existing approaches tackle different learning
tasks independently, tailoring their models to unique task characteristics.
These methods, however, fall short of modeling intrinsic uncertainties in the
spatio-temporal data. Meanwhile, their specialized designs limit their
universality as general spatio-temporal learning solutions. In this paper, we
propose to model the learning tasks in a unified perspective, viewing them as
predictions based on conditional information with shared spatio-temporal
patterns. Based on this proposal, we introduce Unified Spatio-Temporal
Diffusion Models (USTD) to address the tasks uniformly within the
uncertainty-aware diffusion framework. USTD is holistically designed,
comprising a shared spatio-temporal encoder and attention-based denoising
networks that are task-specific. The shared encoder, optimized by a
pre-training strategy, effectively captures conditional spatio-temporal
patterns. The denoising networks, utilizing both cross- and self-attention,
integrate conditional dependencies and generate predictions. Opting for
forecasting and kriging as downstream tasks, we design Gated Attention (SGA)
and Temporal Gated Attention (TGA) for each task, with different emphases on
the spatial and temporal dimensions, respectively. By combining the advantages
of deterministic encoders and probabilistic diffusion models, USTD achieves
state-of-the-art performances compared to deterministic and probabilistic
baselines in both tasks, while also providing valuable uncertainty estimates.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17363" title="Abstract">arXiv:2310.17363</a> [<a href="/pdf/2310.17363" title="Download PDF">pdf</a>, <a href="/ps/2310.17363" title="Download PostScript">ps</a>, <a href="/format/2310.17363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controllability of networked multiagent systems based on linearized  Turing&#x27;s model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+T">Tianhao Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+R">Ruichang Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhixin Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zou%2C+Z">Zhuo Zou</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+X">Xiaoming Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures, submitted to automatica
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Turing's model has been widely used to explain how simple, uniform structures
can give rise to complex, patterned structures during the development of
organisms. However, it is very hard to establish rigorous theoretical results
for the dynamic evolution behavior of Turing's model since it is described by
nonlinear partial differential equations. We focus on controllability of
Turing's model by linearization and spatial discretization. This linearized
model is a networked system whose agents are second order linear systems and
these agents interact with each other by Laplacian dynamics on a graph. A
control signal can be added to agents of choice. Under mild conditions on the
parameters of the linearized Turing's model, we prove the equivalence between
controllability of the linearized Turing's model and controllability of a
Laplace dynamic system with agents of first order dynamics. When the graph is a
grid graph or a cylinder grid graph, we then give precisely the minimal number
of control nodes and a corresponding control node set such that the Laplace
dynamic systems on these graphs with agents of first order dynamics are
controllable.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17364" title="Abstract">arXiv:2310.17364</a> [<a href="/pdf/2310.17364" title="Download PDF">pdf</a>, <a href="/ps/2310.17364" title="Download PostScript">ps</a>, <a href="/format/2310.17364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Adaptive Control for Uncertain Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Renganathan%2C+V">Venkatraman Renganathan</a>, 
<a href="/search/eess?searchtype=author&query=Rantzer%2C+A">Anders Rantzer</a>, 
<a href="/search/eess?searchtype=author&query=Kjellqvist%2C+O">Olle Kjellqvist</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2210.00081">arXiv:2210.00081</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Control of network systems with uncertain local dynamics has remained an open
problem for a long time. In this paper, a distributed minimax adaptive control
algorithm is proposed for such networks whose local dynamics has an uncertain
parameter possibly taking finite number of values. To hedge against this
uncertainty, each node in the network collects the historical data of its
neighboring nodes to decide its control action along its edges by finding the
parameter that best describes the observed disturbance trajectory. Our proposed
distributed adaptive controller is scalable and we give both lower and upper
bounds for its $\ell_{2}$ gain. Numerical simulations demonstrate that once
each node has sufficiently estimated its local uncertainty, the distributed
minimax adaptive controller behaves like the optimal distributed $H_{\infty}$
controller in hindsight.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17369" title="Abstract">arXiv:2310.17369</a> [<a href="/pdf/2310.17369" title="Download PDF">pdf</a>, <a href="/format/2310.17369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language and Mental Health: Measures of Emotion Dynamics from Text as  Linguistic Biosocial Markers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teodorescu%2C+D">Daniela Teodorescu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T">Tiffany Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Fyshe%2C+A">Alona Fyshe</a>, 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+S+M">Saif M. Mohammad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Research in psychopathology has shown that, at an aggregate level, the
patterns of emotional change over time -- emotion dynamics -- are indicators of
one's mental health. One's patterns of emotion change have traditionally been
determined through self-reports of emotions; however, there are known issues
with accuracy, bias, and convenience. Recent approaches to determining emotion
dynamics from one's everyday utterances, addresses many of these concerns, but
it is not yet known whether these measures of utterance emotion dynamics (UED)
correlate with mental health diagnoses. Here, for the first time, we study the
relationship between tweet emotion dynamics and mental health disorders. We
find that each of the UED metrics studied varied by the user's self-disclosed
diagnosis. For example: average valence was significantly higher (i.e., more
positive text) in the control group compared to users with ADHD, MDD, and PTSD.
Valence variability was significantly lower in the control group compared to
ADHD, depression, bipolar disorder, MDD, PTSD, and OCD but not PPD. Rise and
recovery rates of valence also exhibited significant differences from the
control. This work provides important early evidence for how linguistic cues
pertaining to emotion dynamics can play a crucial role as biosocial markers for
mental illnesses and aid in the understanding, diagnosis, and management of
mental health disorders.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17370" title="Abstract">arXiv:2310.17370</a> [<a href="/pdf/2310.17370" title="Download PDF">pdf</a>, <a href="/format/2310.17370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Potential of Generative AI for the World Wide Web
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AlDahoul%2C+N">Nouar AlDahoul</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Joseph Hong</a>, 
<a href="/search/cs?searchtype=author&query=Varvello%2C+M">Matteo Varvello</a>, 
<a href="/search/cs?searchtype=author&query=Zaki%2C+Y">Yasir Zaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Generative Artificial Intelligence (AI) is a cutting-edge technology capable
of producing text, images, and various media content leveraging generative
models and user prompts. Between 2022 and 2023, generative AI surged in
popularity with a plethora of applications spanning from AI-powered movies to
chatbots. In this paper, we delve into the potential of generative AI within
the realm of the World Wide Web, specifically focusing on image generation. Web
developers already harness generative AI to help crafting text and images,
while Web browsers might use it in the future to locally generate images for
tasks like repairing broken webpages, conserving bandwidth, and enhancing
privacy. To explore this research area, we have developed WebDiffusion, a tool
that allows to simulate a Web powered by stable diffusion, a popular
text-to-image model, from both a client and server perspective. WebDiffusion
further supports crowdsourcing of user opinions, which we use to evaluate the
quality and accuracy of 409 AI-generated images sourced from 60 webpages. Our
findings suggest that generative AI is already capable of producing pertinent
and high-quality Web images, even without requiring Web designers to manually
input prompts, just by leveraging contextual information available within the
webpages. However, we acknowledge that direct in-browser image generation
remains a challenge, as only highly powerful GPUs, such as the A40 and A100,
can (partially) compete with classic image downloads. Nevertheless, this
approach could be valuable for a subset of the images, for example when fixing
broken webpages or handling highly private content.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17372" title="Abstract">arXiv:2310.17372</a> [<a href="/pdf/2310.17372" title="Download PDF">pdf</a>, <a href="/format/2310.17372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dialogue-based generation of self-driving simulation scenarios using  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miceli-Barone%2C+A+V">Antonio Valerio Miceli-Barone</a>, 
<a href="/search/cs?searchtype=author&query=Lascarides%2C+A">Alex Lascarides</a>, 
<a href="/search/cs?searchtype=author&query=Innes%2C+C">Craig Innes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures, SpLU-RoboNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Robotics (cs.RO)

</div>
<p class="mathjax">Simulation is an invaluable tool for developing and evaluating controllers
for self-driving cars. Current simulation frameworks are driven by
highly-specialist domain specific languages, and so a natural language
interface would greatly enhance usability. But there is often a gap, consisting
of tacit assumptions the user is making, between a concise English utterance
and the executable code that captures the user's intent. In this paper we
describe a system that addresses this issue by supporting an extended
multimodal interaction: the user can follow up prior instructions with
refinements or revisions, in reaction to the simulations that have been
generated from their utterances so far. We use Large Language Models (LLMs) to
map the user's English utterances in this interaction into domain-specific
code, and so we explore the extent to which LLMs capture the context
sensitivity that's necessary for computing the speaker's intended message in
discourse.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17373" title="Abstract">arXiv:2310.17373</a> [<a href="/pdf/2310.17373" title="Download PDF">pdf</a>, <a href="/format/2310.17373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FMMRec: Fairness-aware Multimodal Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Y">Yongxin Ni</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuhan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+F">Fajie Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recently, multimodal recommendations have gained increasing attention for
effectively addressing the data sparsity problem by incorporating
modality-based representations. Although multimodal recommendations excel in
accuracy, the introduction of different modalities (e.g., images, text, and
audio) may expose more users' sensitive information (e.g., gender and age) to
recommender systems, resulting in potentially more serious unfairness issues.
Despite many efforts on fairness, existing fairness-aware methods are either
incompatible with multimodal scenarios, or lead to suboptimal fairness
performance due to neglecting sensitive information of multimodal content. To
achieve counterfactual fairness in multimodal recommendations, we propose a
novel fairness-aware multimodal recommendation approach (dubbed as FMMRec) to
disentangle the sensitive and non-sensitive information from modal
representations and leverage the disentangled modal representations to guide
fairer representation learning. Specifically, we first disentangle biased and
filtered modal representations by maximizing and minimizing their sensitive
attribute prediction ability respectively. With the disentangled modal
representations, we mine the modality-based unfair and fair (corresponding to
biased and filtered) user-user structures for enhancing explicit user
representation with the biased and filtered neighbors from the corresponding
structures, followed by adversarially filtering out sensitive information.
Experiments on two real-world public datasets demonstrate the superiority of
our FMMRec relative to the state-of-the-art baselines. Our source code is
available at https://anonymous.4open.science/r/FMMRec.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17378" title="Abstract">arXiv:2310.17378</a> [<a href="/pdf/2310.17378" title="Download PDF">pdf</a>, <a href="/format/2310.17378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization dependent generalization bound for ReLU networks based on  sensitivity in the tangent bundle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R%C3%A1cz%2C+D">D&#xe1;niel R&#xe1;cz</a>, 
<a href="/search/cs?searchtype=author&query=Petreczky%2C+M">Mih&#xe1;ly Petreczky</a>, 
<a href="/search/cs?searchtype=author&query=Csert%C3%A1n%2C+A">Andr&#xe1;s Csert&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Dar%C3%B3czy%2C+B">B&#xe1;lint Dar&#xf3;czy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures, OPT2023: 15th Annual Workshop on Optimization for Machine Learning at the 37th NeurIPS 2023, New Orleans, LA, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advances in deep learning have given us some very promising results on
the generalization ability of deep neural networks, however literature still
lacks a comprehensive theory explaining why heavily over-parametrized models
are able to generalize well while fitting the training data. In this paper we
propose a PAC type bound on the generalization error of feedforward ReLU
networks via estimating the Rademacher complexity of the set of networks
available from an initial parameter vector via gradient descent. The key idea
is to bound the sensitivity of the network's gradient to perturbation of the
input data along the optimization trajectory. The obtained bound does not
explicitly depend on the depth of the network. Our results are experimentally
verified on the MNIST and CIFAR-10 datasets.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17379" title="Abstract">arXiv:2310.17379</a> [<a href="/pdf/2310.17379" title="Download PDF">pdf</a>, <a href="/format/2310.17379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLO-BEV: Generating Bird&#x27;s-Eye View in the Same Way as 2D Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Liguo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yanliang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vehicle perception systems strive to achieve comprehensive and rapid visual
interpretation of their surroundings for improved safety and navigation. We
introduce YOLO-BEV, an efficient framework that harnesses a unique surrounding
cameras setup to generate a 2D bird's-eye view of the vehicular environment. By
strategically positioning eight cameras, each at a 45-degree interval, our
system captures and integrates imagery into a coherent 3x3 grid format, leaving
the center blank, providing an enriched spatial representation that facilitates
efficient processing. In our approach, we employ YOLO's detection mechanism,
favoring its inherent advantages of swift response and compact model structure.
Instead of leveraging the conventional YOLO detection head, we augment it with
a custom-designed detection head, translating the panoramically captured data
into a unified bird's-eye view map of ego car. Preliminary results validate the
feasibility of YOLO-BEV in real-time vehicular perception tasks. With its
streamlined architecture and potential for rapid deployment due to minimized
parameters, YOLO-BEV poses as a promising tool that may reshape future
perspectives in autonomous driving systems.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17381" title="Abstract">arXiv:2310.17381</a> [<a href="/pdf/2310.17381" title="Download PDF">pdf</a>, <a href="/format/2310.17381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proactive Emergency Collision Avoidance for Automated Driving in Highway  Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gharavi%2C+L">Leila Gharavi</a>, 
<a href="/search/eess?searchtype=author&query=Dabiri%2C+A">Azita Dabiri</a>, 
<a href="/search/eess?searchtype=author&query=Verkuijlen%2C+J">Jelske Verkuijlen</a>, 
<a href="/search/eess?searchtype=author&query=De+Schutter%2C+B">Bart De Schutter</a>, 
<a href="/search/eess?searchtype=author&query=Baldi%2C+S">Simone Baldi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures, submitted to IEEE Transactions on Control Systems Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Uncertainty in the behavior of other traffic participants is a crucial factor
in collision avoidance for automated driving; here, stochastic metrics should
often be considered to avoid overly conservative decisions. This paper
introduces a Stochastic Model Predictive Control (SMPC) planner for emergency
collision avoidance in highway scenarios to proactively minimize collision risk
while ensuring safety through chance constraints. To address the challenge of
guaranteeing the feasibility for the emergency trajectory, we incorporate
nonlinear tire dynamics in the prediction model of the ego vehicle. Further, we
exploit Max-Min-Plus-Scaling (MMPS) approximations of the nonlinearities to
avoid conservatism, enforce proactive collision avoidance, and improve
computational efficiency in terms of performance and speed. Consequently, our
contributions include integrating a dynamic ego vehicle model into the SMPC
planner, introducing the MMPS approximation for real-time implementation in
emergency scenarios, and integrating SMPC with hybridized chance constraints
and risk minimization. We evaluate our SMPC formulation in terms of proactivity
and efficiency in various hazardous scenarios. Moreover, we demonstrate the
effectiveness of our proposed approach by comparing it with a state-of-the-art
SMPC planner and validate the feasibility of generated trajectories using a
high-fidelity vehicle model in IPG CarMaker.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17383" title="Abstract">arXiv:2310.17383</a> [<a href="/pdf/2310.17383" title="Download PDF">pdf</a>, <a href="/format/2310.17383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the recognition of the game type based on physiological signals and  eye tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Czekaj%2C+%C5%81">&#x141;ukasz Czekaj</a>, 
<a href="/search/cs?searchtype=author&query=Radzinski%2C+%C5%81">&#x141;ukasz Radzinski</a>, 
<a href="/search/cs?searchtype=author&query=Kolimaga%2C+M">Mateusz Kolimaga</a>, 
<a href="/search/cs?searchtype=author&query=Domaszewicz%2C+J">Jakub Domaszewicz</a>, 
<a href="/search/cs?searchtype=author&query=Kit%C5%82owski%2C+R">Robert Kit&#x142;owski</a>, 
<a href="/search/cs?searchtype=author&query=Szwoch%2C+M">Mariusz Szwoch</a>, 
<a href="/search/cs?searchtype=author&query=Duch%2C+W">W&#x142;odzis&#x142;aw Duch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, extended version of ESM paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In: Modelling and Simulation 2023. The European Simulation and
  Modelling Conference 2023 / Rob Vingerhoeds, Pierre de Saqui-Sannes, Geril
  Philippe (eds.), 2023, EUROSIS-ETI, pp.235-238, ISBN 978-9-492-859-28-0
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Automated interpretation of signals yields many impressive applications from
the area of affective computing and human activity recognition (HAR). In this
paper we ask the question about possibility of cognitive activity recognition
on the base of particular set of signals. We use recognition of the game played
by the participant as a playground for exploration of the problem. We build
classifier of three different games (Space Invaders, Tetris, Tower Defence) and
inter-game pause. We validate classifier in the player-independent and
player-dependent scenario. We discuss the improvement in the player-dependent
scenario in the context of biometric person recognition. On the base of the
results obtained in game classification, we consider potential applications in
smart surveillance and quantified self.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17385" title="Abstract">arXiv:2310.17385</a> [<a href="/pdf/2310.17385" title="Download PDF">pdf</a>, <a href="/format/2310.17385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multitask Online Learning: Listen to the Neighborhood Buzz
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Achddou%2C+J">Juliette Achddou</a>, 
<a href="/search/cs?searchtype=author&query=Cesa-Bianchi%2C+N">Nicol&#xf2; Cesa-Bianchi</a>, 
<a href="/search/cs?searchtype=author&query=Laforgue%2C+P">Pierre Laforgue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study multitask online learning in a setting where agents can only
exchange information with their neighbors on an arbitrary communication
network. We introduce $\texttt{MT-CO}_2\texttt{OL}$, a decentralized algorithm
for this setting whose regret depends on the interplay between the task
similarities and the network structure. Our analysis shows that the regret of
$\texttt{MT-CO}_2\texttt{OL}$ is never worse (up to constants) than the bound
obtained when agents do not share information. On the other hand, our bounds
significantly improve when neighboring agents operate on similar tasks. In
addition, we prove that our algorithm can be made differentially private with a
negligible impact on the regret when the losses are linear. Finally, we provide
experimental support for our theory.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17388" title="Abstract">arXiv:2310.17388</a> [<a href="/pdf/2310.17388" title="Download PDF">pdf</a>, <a href="/format/2310.17388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Architecture Design of a Networked Music Performance Platform for a  Chamber Choir
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cychnerski%2C+J">Jan Cychnerski</a>, 
<a href="/search/cs?searchtype=author&query=Mr%C3%B3z%2C+B">Bart&#x142;omiej Mr&#xf3;z</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In: Chiusano, S., et al. New Trends in Database and Information
  Systems. ADBIS 2022. CCIS, vol 1652. Springer, Cham (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">This paper describes an architecture design process for Networked Music
Performance (NMP) platform for medium-sized conducted music ensembles, based on
remote rehearsals of Academic Choir of Gdansk University of Technology. The
issues of real-time remote communication, in-person music performance, and NMP
are described. Three iterative steps defining and extending the architecture of
the NMP platform with additional features to enhance its utility in remote
rehearsals are presented. The first iteration uses a regular video conferencing
platform, the second iteration uses dedicated NMP devices and tools, and the
third iteration adds video transmission and utilizes professional low-latency
audio and video workstations. For each iteration, the platform architecture is
defined and deployed with simultaneous usability tests. Its strengths and
weaknesses are identified through qualitative and quantitative measurements -
statistical analysis shows a significant improvement in rehearsal quality after
each iteration. The final optimal architecture is described and concluded with
guidelines for creating NMP systems for said music ensembles.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17389" title="Abstract">arXiv:2310.17389</a> [<a href="/pdf/2310.17389" title="Download PDF">pdf</a>, <a href="/format/2310.17389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in  Real-World User-AI Conversation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+Y">Yongqi Tong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yangkun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuxin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yujia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite remarkable advances that large language models have achieved in
chatbots, maintaining a non-toxic user-AI interactive environment has become
increasingly critical nowadays. However, previous efforts in toxicity detection
have been mostly based on benchmarks derived from social media content, leaving
the unique challenges inherent to real-world user-AI interactions
insufficiently explored. In this work, we introduce ToxicChat, a novel
benchmark based on real user queries from an open-source chatbot. This
benchmark contains the rich, nuanced phenomena that can be tricky for current
toxicity detection models to identify, revealing a significant domain
difference compared to social media content. Our systematic evaluation of
models trained on existing toxicity datasets has shown their shortcomings when
applied to this unique domain of ToxicChat. Our work illuminates the
potentially overlooked challenges of toxicity detection in real-world user-AI
conversations. In the future, ToxicChat can be a valuable resource to drive
further advancements toward building a safe and healthy environment for user-AI
interactions.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17394" title="Abstract">arXiv:2310.17394</a> [<a href="/pdf/2310.17394" title="Download PDF">pdf</a>, <a href="/format/2310.17394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Graph Neural Networks with Structure-Based Prompt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+Q">Qingqing Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zeyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiding Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+A">Anfeng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) are powerful in learning semantics of graph
data. Recently, a new paradigm "pre-train, prompt" has shown promising results
in adapting GNNs to various tasks with less supervised data. The success of
such paradigm can be attributed to the more consistent objectives of
pre-training and task-oriented prompt tuning, where the pre-trained knowledge
can be effectively transferred to downstream tasks. However, an overlooked
issue of existing studies is that the structure information of graph is usually
exploited during pre-training for learning node representations, while
neglected in the prompt tuning stage for learning task-specific parameters. To
bridge this gap, we propose a novel structure-based prompting method for GNNs,
namely SAP, which consistently exploits structure information in both
pre-training and prompt tuning stages. In particular, SAP 1) employs a
dual-view contrastive learning to align the latent semantic spaces of node
attributes and graph structure, and 2) incorporates structure information in
prompted graph to elicit more pre-trained knowledge in prompt tuning. We
conduct extensive experiments on node classification and graph classification
tasks to show the effectiveness of SAP. Moreover, we show that SAP can lead to
better performance in more challenging few-shot scenarios on both homophilous
and heterophilous graphs.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17395" title="Abstract">arXiv:2310.17395</a> [<a href="/pdf/2310.17395" title="Download PDF">pdf</a>, <a href="/format/2310.17395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Temporal Sentence Grounding From Narrated EgoVideos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Flanagan%2C+K">Kevin Flanagan</a>, 
<a href="/search/cs?searchtype=author&query=Damen%2C+D">Dima Damen</a>, 
<a href="/search/cs?searchtype=author&query=Wray%2C+M">Michael Wray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in BMVC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The onset of long-form egocentric datasets such as Ego4D and EPIC-Kitchens
presents a new challenge for the task of Temporal Sentence Grounding (TSG).
Compared to traditional benchmarks on which this task is evaluated, these
datasets offer finer-grained sentences to ground in notably longer videos. In
this paper, we develop an approach for learning to ground sentences in these
datasets using only narrations and their corresponding rough narration
timestamps. We propose to artificially merge clips to train for temporal
grounding in a contrastive manner using text-conditioning attention. This Clip
Merging (CliMer) approach is shown to be effective when compared with a high
performing TSG method -- e.g. mean R@1 improves from 3.9 to 5.7 on Ego4D and
from 10.7 to 13.0 on EPIC-Kitchens. Code and data splits available from:
https://github.com/keflanagan/CliMer
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17401" title="Abstract">arXiv:2310.17401</a> [<a href="/pdf/2310.17401" title="Download PDF">pdf</a>, <a href="/format/2310.17401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Efficient Robust Beamforming for Vehicular ISAC with Imperfect  Channel Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haijian Sun</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianyi He</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+W">Weiming Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R+Q">Rose Qingyang Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE for future publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper investigates robust beamforming for system-centric energy
efficiency (EE) optimization in the vehicular integrated sensing and
communication (ISAC) system, where the mobility of vehicles poses significant
challenges to channel estimation. To obtain the optimal beamforming under
channel uncertainty, we first formulate an optimization problem for maximizing
the system EE under bounded channel estimation errors. Next, fractional
programming and semidefinite relaxation (SDR) are utilized to relax the rank-1
constraints. We further use Schur complement and S-Procedure to transform
Cramer-Rao bound (CRB) and channel estimation error constraints into convex
forms, respectively. Based on the Lagrangian dual function and
Karush-Kuhn-Tucker (KKT) conditions, it is proved that the optimal beamforming
solution is rank-1. Finally, we present comprehensive simulation results to
demonstrate two key findings: 1) the proposed algorithm exhibits a favorable
convergence rate, and 2) the approach effectively mitigates the impact of
channel estimation errors.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17403" title="Abstract">arXiv:2310.17403</a> [<a href="/pdf/2310.17403" title="Download PDF">pdf</a>, <a href="/format/2310.17403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection Defenses: An Empty Promise against Adversarial Patch Attacks  on Optical Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scheurer%2C+E">Erik Scheurer</a>, 
<a href="/search/cs?searchtype=author&query=Schmalfuss%2C+J">Jenny Schmalfuss</a>, 
<a href="/search/cs?searchtype=author&query=Lis%2C+A">Alexander Lis</a>, 
<a href="/search/cs?searchtype=author&query=Bruhn%2C+A">Andr&#xe9;s Bruhn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Adversarial patches undermine the reliability of optical flow predictions
when placed in arbitrary scene locations. Therefore, they pose a realistic
threat to real-world motion detection and its downstream applications.
Potential remedies are defense strategies that detect and remove adversarial
patches, but their influence on the underlying motion prediction has not been
investigated. In this paper, we thoroughly examine the currently available
detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art
optical flow methods, and illuminate their side effects on the quality and
robustness of the final flow predictions. In particular, we implement
defense-aware attacks to investigate whether current defenses are able to
withstand attacks that take the defense mechanism into account. Our experiments
yield two surprising results: Detect-and-remove defenses do not only lower the
optical flow quality on benign scenes, in doing so, they also harm the
robustness under patch attacks for all tested optical flow methods except
FlowNetC. As currently employed detect-and-remove defenses fail to deliver the
promised adversarial robustness for optical flow, they evoke a false sense of
security. The code is available at
https://github.com/cv-stuttgart/DetectionDefenses.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17404" title="Abstract">arXiv:2310.17404</a> [<a href="/pdf/2310.17404" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariance Measures for Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quiroga%2C+F+M">Facundo Manuel Quiroga</a>, 
<a href="/search/cs?searchtype=author&query=Torrents-Barrena%2C+J">Jordina Torrents-Barrena</a>, 
<a href="/search/cs?searchtype=author&query=Lanzarini%2C+L+C">Laura Cristina Lanzarini</a>, 
<a href="/search/cs?searchtype=author&query=Puig-Valls%2C+D">Domenec Puig-Valls</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Soft Computing, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Invariances in neural networks are useful and necessary for many tasks.
However, the representation of the invariance of most neural network models has
not been characterized. We propose measures to quantify the invariance of
neural networks in terms of their internal representation. The measures are
efficient and interpretable, and can be applied to any neural network model.
They are also more sensitive to invariance than previously defined measures. We
validate the measures and their properties in the domain of affine
transformations and the CIFAR10 and MNIST datasets, including their stability
and interpretability. Using the measures, we perform a first analysis of CNN
models and show that their internal invariance is remarkably stable to random
weight initializations, but not to changes in dataset or transformation. We
believe the measures will enable new avenues of research in invariance
representation.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17405" title="Abstract">arXiv:2310.17405</a> [<a href="/pdf/2310.17405" title="Download PDF">pdf</a>, <a href="/format/2310.17405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Modeling with Stationary Diffusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorch%2C+L">Lars Lorch</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We develop a novel approach towards causal inference. Rather than structural
equations over a causal graph, we learn stochastic differential equations
(SDEs) whose stationary densities model a system's behavior under
interventions. These stationary diffusion models do not require the formalism
of causal graphs, let alone the common assumption of acyclicity. We show that
in several cases, they generalize to unseen interventions on their variables,
often better than classical approaches. Our inference method is based on a new
theoretical result that expresses a stationarity condition on the diffusion's
generator in a reproducing kernel Hilbert space. The resulting kernel deviation
from stationarity (KDS) is an objective function of independent interest.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17407" title="Abstract">arXiv:2310.17407</a> [<a href="/pdf/2310.17407" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meaning and understanding in large language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Havl%C3%ADk%2C+V">Vladim&#xed;r Havl&#xed;k</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Can a machine understand the meanings of natural language? Recent
developments in the generative large language models (LLMs) of artificial
intelligence have led to the belief that traditional philosophical assumptions
about machine understanding of language need to be revised. This article
critically evaluates the prevailing tendency to regard machine language
performance as mere syntactic manipulation and the simulation of understanding,
which is only partial and very shallow, without sufficient referential
grounding in the world. The aim is to highlight the conditions crucial to
attributing natural language understanding to state-of-the-art LLMs, where it
can be legitimately argued that LLMs not only use syntax but also semantics,
their understanding not being simulated but duplicated; and determine how they
ground the meanings of linguistic expressions.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17408" title="Abstract">arXiv:2310.17408</a> [<a href="/pdf/2310.17408" title="Download PDF">pdf</a>, <a href="/format/2310.17408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling the Matrix Multiplication Micro-kernel Generation with Exo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castell%C3%B3%2C+A">Adri&#xe1;n Castell&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Bellavita%2C+J">Julian Bellavita</a>, 
<a href="/search/cs?searchtype=author&query=Dinh%2C+G">Grace Dinh</a>, 
<a href="/search/cs?searchtype=author&query=Ikarashi%2C+Y">Yuka Ikarashi</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+H">H&#xe9;ctor Mart&#xed;nez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 18 figures. Presented at CGO 2024. It includes a software artifact step-by-step execution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Computation and Language (cs.CL); Performance (cs.PF)

</div>
<p class="mathjax">The optimization of the matrix multiplication (or GEMM) has been a need
during the last decades. This operation is considered the flagship of current
linear algebra libraries such as BLIS, OpenBLAS, or Intel OneAPI because of its
widespread use in a large variety of scientific applications. The GEMM is
usually implemented following the GotoBLAS philosophy, which tiles the GEMM
operands and uses a series of nested loops for performance improvement. These
approaches extract the maximum computational power of the architectures through
small pieces of hardware-oriented, high-performance code called micro-kernel.
However, this approach forces developers to generate, with a non-negligible
effort, a dedicated micro-kernel for each new hardware.
<br />In this work, we present a step-by-step procedure for generating
micro-kernels with the Exo compiler that performs close to (or even better
than) manually developed microkernels written with intrinsic functions or
assembly language. Our solution also improves the portability of the generated
code, since a hardware target is fully specified by a concise library-based
description of its instructions.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17410" title="Abstract">arXiv:2310.17410</a> [<a href="/pdf/2310.17410" title="Download PDF">pdf</a>, <a href="/ps/2310.17410" title="Download PostScript">ps</a>, <a href="/format/2310.17410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Efficiently Monitorable Formulas in Metric Temporal Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raha%2C+R">Ritam Raha</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+R">Rajarshi Roy</a>, 
<a href="/search/cs?searchtype=author&query=Fijalkow%2C+N">Nathanael Fijalkow</a>, 
<a href="/search/cs?searchtype=author&query=Neider%2C+D">Daniel Neider</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+G+A">Guillermo A. Perez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">In runtime verification, manually formalizing a specification for monitoring
system executions is a tedious and error-prone process. To address this issue,
we consider the problem of automatically synthesizing formal specifications
from system executions. To demonstrate our approach, we consider the popular
specification language Metric Temporal Logic (MTL), which is particularly
tailored towards specifying temporal properties for cyber-physical systems
(CPS). Most of the classical approaches for synthesizing temporal logic
formulas aim at minimizing the size of the formula. However, for efficiency in
monitoring, along with the size, the amount of "lookahead" required for the
specification becomes relevant, especially for safety-critical applications. We
formalize this notion and devise a learning algorithm that synthesizes concise
formulas having bounded lookahead. To do so, our algorithm reduces the
synthesis task to a series of satisfiability problems in Linear Real Arithmetic
(LRA) and generates MTL formulas from their satisfying assignments. The
reduction uses a novel encoding of a popular MTL monitoring procedure using
LRA. Finally, we implement our algorithm in a tool called TEAL and demonstrate
its ability to synthesize efficiently monitorable MTL formulas in a CPS
application.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17413" title="Abstract">arXiv:2310.17413</a> [<a href="/pdf/2310.17413" title="Download PDF">pdf</a>, <a href="/format/2310.17413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belfathi%2C+A">Anas Belfathi</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+N">Nicolas Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Monceaux%2C+L">Laura Monceaux</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> JURIX 2023 - The 36th International Conference on Legal Knowledge
  and Information System, Maastricht, the Netherlands
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose a comprehensive study of one-stage elicitation techniques for
querying a large pre-trained generative transformer (GPT-3.5-turbo) in the
rhetorical role prediction task of legal cases. This task is known as requiring
textual context to be addressed. Our study explores strategies such as zero-few
shots, task specification with definitions and clarification of annotation
ambiguities, textual context and reasoning with general prompts and specific
questions. We show that the number of examples, the definition of labels, the
presentation of the (labelled) textual context and specific questions about
this context have a positive influence on the performance of the model. Given
non-equivalent test set configurations, we observed that prompting with a few
labelled examples from direct context can lead the model to a better
performance than a supervised fined-tuned multi-class classifier based on the
BERT encoder (weighted F1 score of = 72%). But there is still a gap to reach
the performance of the best systems = 86%) in the LegalEval 2023 task which, on
the other hand, require dedicated resources, architectures and training.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17414" title="Abstract">arXiv:2310.17414</a> [<a href="/pdf/2310.17414" title="Download PDF">pdf</a>, <a href="/format/2310.17414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEI2JSON: Schema-based Validation and Conversion of Livestock Event  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Habib%2C+M">Mahir Habib</a>, 
<a href="/search/eess?searchtype=author&query=Kabir%2C+M+A">Muhammad Ashad Kabir</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+L">Lihong Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Livestock producers often need help in standardising (i.e., converting and
validating) their livestock event data. This article introduces a novel
solution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on
for Google Sheets, adhering to the livestock event information (LEI) schema.
The core objective of LEI2JSON is to provide livestock producers with an
efficient mechanism to standardise their data, leading to substantial savings
in time and resources. This is achieved by building the spreadsheet template
with the appropriate column headers, notes, and validation rules, converting
the spreadsheet data into JSON format, and validating the output against the
schema. LEI2JSON facilitates the seamless storage of livestock event
information locally or on Google Drive in JSON. Additionally, we have conducted
an extensive experimental evaluation to assess the effectiveness of the tool.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17415" title="Abstract">arXiv:2310.17415</a> [<a href="/pdf/2310.17415" title="Download PDF">pdf</a>, <a href="/format/2310.17415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word  Tokenization on Downstream Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Y">Yang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+P">Pan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Ziyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Huiqun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+G">Guisheng Fan</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Liang Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 4figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Large protein language models are adept at capturing the underlying
evolutionary information in primary structures, offering significant practical
value for protein engineering. Compared to natural language models, protein
amino acid sequences have a smaller data volume and a limited combinatorial
space. Choosing an appropriate vocabulary size to optimize the pre-trained
model is a pivotal issue. Moreover, despite the wealth of benchmarks and
studies in the natural language community, there remains a lack of a
comprehensive benchmark for systematically evaluating protein language model
quality. Given these challenges, PETA trained language models with 14 different
vocabulary sizes under three tokenization methods. It conducted thousands of
tests on 33 diverse downstream datasets to assess the models' transfer learning
capabilities, incorporating two classification heads and three random seeds to
mitigate potential biases. Extensive experiments indicate that vocabulary sizes
between 50 and 200 optimize the model, whereas sizes exceeding 800
detrimentally affect the model's representational performance. Our code, model
weights and datasets are available at
https://github.com/ginnm/ProteinPretraining.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17416" title="Abstract">arXiv:2310.17416</a> [<a href="/pdf/2310.17416" title="Download PDF">pdf</a>, <a href="/format/2310.17416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Goals are Enough: Inducing AdHoc cooperation among unseen Multi-Agent  systems in IMFs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dey%2C+K">Kaushik Dey</a>, 
<a href="/search/cs?searchtype=author&query=Perepu%2C+S+K">Satheesh K. Perepu</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Abir Das</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE CCNC 2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Intent-based management will play a critical role in achieving customers'
expectations in the next-generation mobile networks. Traditional methods cannot
perform efficient resource management since they tend to handle each
expectation independently. Existing approaches, e.g., based on multi-agent
reinforcement learning (MARL) allocate resources in an efficient fashion when
there are conflicting expectations on the network slice. However, in reality,
systems are often far more complex to be addressed by a standalone MARL
formulation. Often there exists a hierarchical structure of intent fulfilment
where multiple pre-trained, self-interested agents may need to be further
orchestrated by a supervisor or controller agent. Such agents may arrive in the
system adhoc, which then needs to be orchestrated along with other available
agents. Retraining the whole system every time is often infeasible given the
associated time and cost. Given the challenges, such adhoc coordination of
pre-trained systems could be achieved through an intelligent supervisor agent
which incentivizes pre-trained RL/MARL agents through sets of dynamic contracts
(goals or bonuses) and encourages them to act as a cohesive unit towards
fulfilling a global expectation. Some approaches use a rule-based supervisor
agent and deploy the hierarchical constituent agents sequentially, based on
human-coded rules.
<br />In the current work, we propose a framework whereby pre-trained agents can be
orchestrated in parallel leveraging an AI-based supervisor agent. For this, we
propose to use Adhoc-Teaming approaches which assign optimal goals to the MARL
agents and incentivize them to exhibit certain desired behaviours. Results on
the network emulator show that the proposed approach results in faster and
improved fulfilment of expectations when compared to rule-based approaches and
even generalizes to changes in environments.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17417" title="Abstract">arXiv:2310.17417</a> [<a href="/pdf/2310.17417" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training for Open-Ended Drilling through a Virtual Reality Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lie%2C+H">Hing Lie</a>, 
<a href="/search/cs?searchtype=author&query=Studer%2C+K">Kachina Studer</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Thomson%2C+B">Ben Thomson</a>, 
<a href="/search/cs?searchtype=author&query=Turakhia%2C+D+G">Dishita G Turakhia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">John Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Virtual Reality (VR) can support effective and scalable training of
psychomotor skills in manufacturing. However, many industry training modules
offer experiences that are close-ended and do not allow for human error. We aim
to address this gap in VR training tools for psychomotor skills training by
exploring an open-ended approach to the system design. We designed a VR
training simulation prototype to perform open-ended practice of drilling using
a 3-axis milling machine. The simulation employs near "end-to-end" instruction
through a safety module, a setup and drilling tutorial, open-ended practice
complete with warnings of mistakes and failures, and a function to assess the
geometries and locations of drilled holes against an engineering drawing. We
developed and conducted a user study within an undergraduate-level introductory
fabrication course to investigate the impact of open-ended VR practice on
learning outcomes. Study results reveal positive trends, with the VR group
successfully completing the machining task of drilling at a higher rate (75% vs
64%), with fewer mistakes (1.75 vs 2.14 score), and in less time (17.67 mins vs
21.57 mins) compared to the control group. We discuss our findings and
limitations and implications for the design of open-ended VR training systems
for learning psychomotor skills.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17418" title="Abstract">arXiv:2310.17418</a> [<a href="/pdf/2310.17418" title="Download PDF">pdf</a>, <a href="/format/2310.17418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Circuit as Set of Points
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jialv Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinggang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiahao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chang Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">As the size of circuit designs continues to grow rapidly, artificial
intelligence technologies are being extensively used in Electronic Design
Automation (EDA) to assist with circuit design. Placement and routing are the
most time-consuming parts of the physical design process, and how to quickly
evaluate the placement has become a hot research topic. Prior works either
transformed circuit designs into images using hand-crafted methods and then
used Convolutional Neural Networks (CNN) to extract features, which are limited
by the quality of the hand-crafted methods and could not achieve end-to-end
training, or treated the circuit design as a graph structure and used Graph
Neural Networks (GNN) to extract features, which require time-consuming
preprocessing. In our work, we propose a novel perspective for circuit design
by treating circuit components as point clouds and using Transformer-based
point cloud perception methods to extract features from the circuit. This
approach enables direct feature extraction from raw data without any
preprocessing, allows for end-to-end training, and results in high performance.
Experimental results show that our method achieves state-of-the-art performance
in congestion prediction tasks on both the CircuitNet and ISPD2015 datasets, as
well as in design rule check (DRC) violation prediction tasks on the CircuitNet
dataset. Our method establishes a bridge between the relatively mature point
cloud perception methods and the fast-developing EDA algorithms, enabling us to
leverage more collective intelligence to solve this task. To facilitate the
research of open EDA design, source codes and pre-trained models are released
at https://github.com/hustvl/circuitformer.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17419" title="Abstract">arXiv:2310.17419</a> [<a href="/pdf/2310.17419" title="Download PDF">pdf</a>, <a href="/format/2310.17419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image  Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">You-Ming Chang</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C">Chen Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+W">Wei-Chen Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Ning Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep generative models can create remarkably photorealistic fake images while
raising concerns about misinformation and copyright infringement, known as
deepfake threats. Deepfake detection technique is developed to distinguish
between real and fake images, where the existing methods typically learn
classifiers in the image domain or various feature domains. However, the
generalizability of deepfake detection against emerging and more advanced
generative models remains challenging. In this paper, being inspired by the
zero-shot advantages of Vision-Language Models (VLMs), we propose a novel
approach using VLMs (e.g. InstructBLIP) and prompt tuning techniques to improve
the deepfake detection accuracy over unseen data. We formulate deepfake
detection as a visual question answering problem, and tune soft prompts for
InstructBLIP to answer the real/fake information of a query image. We conduct
full-spectrum experiments on datasets from 3 held-in and 13 held-out generative
models, covering modern text-to-image generation, image editing and image
attacks. Results demonstrate that (1) the deepfake detection accuracy can be
significantly and consistently improved (from 58.8% to 91.31%, in average
accuracy over unseen data) using pretrained vision-language models with prompt
tuning; (2) our superior performance is at less cost of trainable parameters,
resulting in an effective and efficient solution for deepfake detection. Code
and models can be found at https://github.com/nctu-eva-lab/AntifakePrompt.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17420" title="Abstract">arXiv:2310.17420</a> [<a href="/pdf/2310.17420" title="Download PDF">pdf</a>, <a href="/format/2310.17420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Dynamic $k$-Clustering in $\tilde O(k)$ Update Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Sayan Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+M">Mart&#xed;n Costa</a>, 
<a href="/search/cs?searchtype=author&query=Lattanzi%2C+S">Silvio Lattanzi</a>, 
<a href="/search/cs?searchtype=author&query=Parotsidis%2C+N">Nikos Parotsidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We present a $O(1)$-approximate fully dynamic algorithm for the $k$-median
and $k$-means problems on metric spaces with amortized update time $\tilde
O(k)$ and worst-case query time $\tilde O(k^2)$. We complement our theoretical
analysis with the first in-depth experimental study for the dynamic $k$-median
problem on general metrics, focusing on comparing our dynamic algorithm to the
current state-of-the-art by Henzinger and Kale [ESA'20]. Finally, we also
provide a lower bound for dynamic $k$-median which shows that any
$O(1)$-approximate algorithm with $\tilde O(\text{poly}(k))$ query time must
have $\tilde \Omega(k)$ amortized update time, even in the incremental setting.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17421" title="Abstract">arXiv:2310.17421</a> [<a href="/pdf/2310.17421" title="Download PDF">pdf</a>, <a href="/format/2310.17421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution of Action Movements (DAM): A Descriptor for Human Action  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quiroga%2C+F+M">Facundo Manuel Quiroga</a>, 
<a href="/search/cs?searchtype=author&query=Ronchetti%2C+F">Franco Ronchetti</a>, 
<a href="/search/cs?searchtype=author&query=Lanzarini%2C+L">Laura Lanzarini</a>, 
<a href="/search/cs?searchtype=author&query=Eestrebou%2C+C">Cesar Eestrebou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Human action recognition from skeletal data is an important and active area
of research in which the state of the art has not yet achieved near-perfect
accuracy on many well-known datasets. In this paper, we introduce the
Distribution of Action Movements Descriptor, a novel action descriptor based on
the distribution of the directions of the motions of the joints between frames,
over the set of all possible motions in the dataset. The descriptor is computed
as a normalized histogram over a set of representative directions of the
joints, which are in turn obtained via clustering. While the descriptor is
global in the sense that it represents the overall distribution of movement
directions of an action, it is able to partially retain its temporal structure
by applying a windowing scheme.
<br />The descriptor, together with a standard classifier, outperforms several
state-of-the-art techniques on many well-known datasets.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17423" title="Abstract">arXiv:2310.17423</a> [<a href="/pdf/2310.17423" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Newtonian Mechanics of Demand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mendel%2C+M">Max Mendel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">Economic engineering is a new field wherein economic systems are modelled in
the same manner as traditional mechanical and electrical engineering systems.
In this paper, we use Newton's theory of motion as the basis for the theory of
demand; thereby establishing a theoretical foundation for economic engineering.
We follow Newton's original development, as set forth in the Principia, to
determine economic analogs to his three laws of motion. The pivotal result is
an operational definition for an economic force, i.e. a want or a desire, in
terms of a price adjustment. With this, we model the price effects of scarcity
and trade friction in analogy with the models for the spring and damping force.
In turn, we define economic benefits and surplus as analogous to the
definitions of mechanical work and energy. These are then used to interpret the
various types of economic equilibrium considered by economists from a
mechanical perspective. The effectiveness of the analogy is illustrated by
applying it to modelling the price and inventory dynamics of various economic
agents -- including consumers, dealers, holders, spot and futures traders --
using linear-time invariant systems theory.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17427" title="Abstract">arXiv:2310.17427</a> [<a href="/pdf/2310.17427" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Handshape recognition for Argentinian Sign Language using ProbSom
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ronchetti%2C+F">Franco Ronchetti</a>, 
<a href="/search/cs?searchtype=author&query=Quiroga%2C+F+M">Facundo Manuel Quiroga</a>, 
<a href="/search/cs?searchtype=author&query=Estrebou%2C+C">C&#xe9;sar Estrebou</a>, 
<a href="/search/cs?searchtype=author&query=Lanzarini%2C+L">Laura Lanzarini</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Computer Science and Technology, 2016
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Automatic sign language recognition is an important topic within the areas of
human-computer interaction and machine learning. On the one hand, it poses a
complex challenge that requires the intervention of various knowledge areas,
such as video processing, image processing, intelligent systems and
linguistics. On the other hand, robust recognition of sign language could
assist in the translation process and the integration of hearing-impaired
people.
<br />This paper offers two main contributions: first, the creation of a database
of handshapes for the Argentinian Sign Language (LSA), which is a topic that
has barely been discussed so far. Secondly, a technique for image processing,
descriptor extraction and subsequent handshape classification using a
supervised adaptation of self-organizing maps that is called ProbSom. This
technique is compared to others in the state of the art, such as Support Vector
Machines (SVM), Random Forests, and Neural Networks.
<br />The database that was built contains 800 images with 16 LSA handshapes, and
is a first step towards building a comprehensive database of Argentinian signs.
The ProbSom-based neural classifier, using the proposed descriptor, achieved an
accuracy rate above 90%.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17428" title="Abstract">arXiv:2310.17428</a> [<a href="/pdf/2310.17428" title="Download PDF">pdf</a>, <a href="/format/2310.17428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &#x27;&#x27;Fifty Shades of Bias&#x27;&#x27;: Normative Ratings of Gender Bias in GPT  Generated English Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hada%2C+R">Rishav Hada</a>, 
<a href="/search/cs?searchtype=author&query=Seth%2C+A">Agrima Seth</a>, 
<a href="/search/cs?searchtype=author&query=Diddee%2C+H">Harshita Diddee</a>, 
<a href="/search/cs?searchtype=author&query=Bali%2C+K">Kalika Bali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready version in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language serves as a powerful tool for the manifestation of societal belief
systems. In doing so, it also perpetuates the prevalent biases in our society.
Gender bias is one of the most pervasive biases in our society and is seen in
online and offline discourses. With LLMs increasingly gaining human-like
fluency in text generation, gaining a nuanced understanding of the biases these
systems can generate is imperative. Prior work often treats gender bias as a
binary classification task. However, acknowledging that bias must be perceived
at a relative scale; we investigate the generation and consequent receptivity
of manual annotators to bias of varying degrees. Specifically, we create the
first dataset of GPT-generated English text with normative ratings of gender
bias. Ratings were obtained using Best--Worst Scaling -- an efficient
comparative annotation framework. Next, we systematically analyze the variation
of themes of gender biases in the observed ranking and show that
identity-attack is most closely related to gender bias. Finally, we show the
performance of existing automated models trained on related concepts on our
dataset.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17429" title="Abstract">arXiv:2310.17429</a> [<a href="/pdf/2310.17429" title="Download PDF">pdf</a>, <a href="/format/2310.17429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LSA64: An Argentinian Sign Language Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ronchetti%2C+F">Franco Ronchetti</a>, 
<a href="/search/cs?searchtype=author&query=Quiroga%2C+F+M">Facundo Manuel Quiroga</a>, 
<a href="/search/cs?searchtype=author&query=Estrebou%2C+C">C&#xe9;sar Estrebou</a>, 
<a href="/search/cs?searchtype=author&query=Lanzarini%2C+L">Laura Lanzarini</a>, 
<a href="/search/cs?searchtype=author&query=Rosete%2C+A">Alejandro Rosete</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in CACIC XXII
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automatic sign language recognition is a research area that encompasses
human-computer interaction, computer vision and machine learning. Robust
automatic recognition of sign language could assist in the translation process
and the integration of hearing-impaired people, as well as the teaching of sign
language to the hearing population. Sign languages differ significantly in
different countries and even regions, and their syntax and semantics are
different as well from those of written languages. While the techniques for
automatic sign language recognition are mostly the same for different
languages, training a recognition system for a new language requires having an
entire dataset for that language. This paper presents a dataset of 64 signs
from the Argentinian Sign Language (LSA). The dataset, called LSA64, contains
3200 videos of 64 different LSA signs recorded by 10 subjects, and is a first
step towards building a comprehensive research-level dataset of Argentinian
signs, specifically tailored to sign language recognition or other machine
learning tasks. The subjects that performed the signs wore colored gloves to
ease the hand tracking and segmentation steps, allowing experiments on the
dataset to focus specifically on the recognition of signs. We also present a
pre-processed version of the dataset, from which we computed statistics of
movement, position and handshape of the signs.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17430" title="Abstract">arXiv:2310.17430</a> [<a href="/pdf/2310.17430" title="Download PDF">pdf</a>, <a href="/format/2310.17430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A near-autonomous and incremental intrusion detection system through  active learning of known and unknown attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boukela%2C+L">Lynda Boukela</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gongxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yacoub%2C+M">Meziane Yacoub</a>, 
<a href="/search/cs?searchtype=author&query=Bouzefrane%2C+S">Samia Bouzefrane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, 32 references, conference
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Pattern Analysis, and Cybernetics, 2021, pp. 374-379
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Intrusion detection is a traditional practice of security experts, however,
there are several issues which still need to be tackled. Therefore, in this
paper, after highlighting these issues, we present an architecture for a hybrid
Intrusion Detection System (IDS) for an adaptive and incremental detection of
both known and unknown attacks. The IDS is composed of supervised and
unsupervised modules, namely, a Deep Neural Network (DNN) and the K-Nearest
Neighbors (KNN) algorithm, respectively. The proposed system is near-autonomous
since the intervention of the expert is minimized through the active learning
(AL) approach. A query strategy for the labeling process is presented, it aims
at teaching the supervised module to detect unknown attacks and improve the
detection of the already-known attacks. This teaching is achieved through
sliding windows (SW) in an incremental fashion where the DNN is retrained when
the data is available over time, thus rendering the IDS adaptive to cope with
the evolutionary aspect of the network traffic. A set of experiments was
conducted on the CICIDS2017 dataset in order to evaluate the performance of the
IDS, promising results were obtained.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17431" title="Abstract">arXiv:2310.17431</a> [<a href="/pdf/2310.17431" title="Download PDF">pdf</a>, <a href="/format/2310.17431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient safe learning for controller tuning with experimental  validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zagorowska%2C+M">Marta Zagorowska</a>, 
<a href="/search/eess?searchtype=author&query=K%C3%B6nig%2C+C">Christopher K&#xf6;nig</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+H">Hanlin Yu</a>, 
<a href="/search/eess?searchtype=author&query=Balta%2C+E+C">Efe C. Balta</a>, 
<a href="/search/eess?searchtype=author&query=Rupenyan%2C+A">Alisa Rupenyan</a>, 
<a href="/search/eess?searchtype=author&query=Lygeros%2C+J">John Lygeros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Optimization-based controller tuning is challenging because it requires
formulating optimization problems explicitly as functions of controller
parameters. Safe learning algorithms overcome the challenge by creating
surrogate models from measured data. To ensure safety, such data-driven
algorithms often rely on exhaustive grid search, which is computationally
inefficient. In this paper, we propose a novel approach to safe learning by
formulating a series of optimization problems instead of a grid search. We also
develop a method for initializing the optimization problems to guarantee
feasibility while using numerical solvers. The performance of the new method is
first validated in a simulated precision motion system, demonstrating improved
computational efficiency, and illustrating the role of exploiting numerical
solvers to reach the desired precision. Experimental validation on an
industrial-grade precision motion system confirms that the proposed algorithm
achieves 30% better tracking at sub-micrometer precision as a state-of-the-art
safe learning algorithm, improves the default auto-tuning solution, and reduces
the computational cost seven times compared to learning algorithms based on
exhaustive search.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17432" title="Abstract">arXiv:2310.17432</a> [<a href="/pdf/2310.17432" title="Download PDF">pdf</a>, <a href="/format/2310.17432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Likelihood-based Out-of-Distribution Detection with Denoising Diffusion  Probabilistic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goodier%2C+J">Joseph Goodier</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+N+D+F">Neill D.F. Campbell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages (main paper), 3 pages (acknowledgements &amp; references), 3 figures, 2 tables, 1 algorithm, work accepted for BMVC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Out-of-Distribution detection between dataset pairs has been extensively
explored with generative models. We show that likelihood-based
Out-of-Distribution detection can be extended to diffusion models by leveraging
the fact that they, like other likelihood-based generative models, are
dramatically affected by the input sample complexity. Currently, all
Out-of-Distribution detection methods with Diffusion Models are
reconstruction-based. We propose a new likelihood ratio for Out-of-Distribution
detection with Deep Denoising Diffusion Models, which we call the Complexity
Corrected Likelihood Ratio. Our likelihood ratio is constructed using Evidence
Lower-Bound evaluations from an individual model at various noising levels. We
present results that are comparable to state-of-the-art Out-of-Distribution
detection methods with generative models.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17436" title="Abstract">arXiv:2310.17436</a> [<a href="/pdf/2310.17436" title="Download PDF">pdf</a>, <a href="/format/2310.17436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-weighted Loss Functions for Improved Adversarial Attacks on  Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maag%2C+K">Kira Maag</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+A">Asja Fischer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">State-of-the-art deep neural networks have been shown to be extremely
powerful in a variety of perceptual tasks like semantic segmentation. However,
these networks are vulnerable to adversarial perturbations of the input which
are imperceptible for humans but lead to incorrect predictions. Treating image
segmentation as a sum of pixel-wise classifications, adversarial attacks
developed for classification models were shown to be applicable to segmentation
models as well. In this work, we present simple uncertainty-based weighting
schemes for the loss functions of such attacks that (i) put higher weights on
pixel classifications which can more easily perturbed and (ii) zero-out the
pixel-wise losses corresponding to those pixels that are already confidently
misclassified. The weighting schemes can be easily integrated into the loss
function of a range of well-known adversarial attackers with minimal additional
computational overhead, but lead to significant improved perturbation
performance, as we demonstrate in our empirical analysis on several datasets
and models.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17437" title="Abstract">arXiv:2310.17437</a> [<a href="/pdf/2310.17437" title="Download PDF">pdf</a>, <a href="/format/2310.17437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sign Languague Recognition without frame-sequencing constraints: A proof  of concept on the Argentinian Sign Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ronchetti%2C+F">Franco Ronchetti</a>, 
<a href="/search/cs?searchtype=author&query=Quiroga%2C+F+M">Facundo Manuel Quiroga</a>, 
<a href="/search/cs?searchtype=author&query=Estrebou%2C+C">C&#xe9;sar Estrebou</a>, 
<a href="/search/cs?searchtype=author&query=Lanzarini%2C+L">Laura Lanzarini</a>, 
<a href="/search/cs?searchtype=author&query=Rosete%2C+A">Alejandro Rosete</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IBERAMIA 2016
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Automatic sign language recognition (SLR) is an important topic within the
areas of human-computer interaction and machine learning. On the one hand, it
poses a complex challenge that requires the intervention of various knowledge
areas, such as video processing, image processing, intelligent systems and
linguistics. On the other hand, robust recognition of sign language could
assist in the translation process and the integration of hearing-impaired
people, as well as the teaching of sign language for the hearing population.
<br />SLR systems usually employ Hidden Markov Models, Dynamic Time Warping or
similar models to recognize signs. Such techniques exploit the sequential
ordering of frames to reduce the number of hypothesis. This paper presents a
general probabilistic model for sign classification that combines
sub-classifiers based on different types of features such as position, movement
and handshape. The model employs a bag-of-words approach in all classification
steps, to explore the hypothesis that ordering is not essential for
recognition. The proposed model achieved an accuracy rate of 97% on an
Argentinian Sign Language dataset containing 64 classes of signs and 3200
samples, providing some evidence that indeed recognition without ordering is
possible.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17448" title="Abstract">arXiv:2310.17448</a> [<a href="/pdf/2310.17448" title="Download PDF">pdf</a>, <a href="/format/2310.17448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech  Systems for the MADASR 2023 Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alum%C3%A4e%2C+T">Tanel Alum&#xe4;e</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+J">Jiaming Kong</a>, 
<a href="/search/cs?searchtype=author&query=Robnikov%2C+D">Daniil Robnikov</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE Automatic Speech Recognition and Understanding Workshop
  (ASRU)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper describes Tallinn University of Technology (TalTech) systems
developed for the ASRU MADASR 2023 Challenge. The challenge focuses on
automatic speech recognition of dialect-rich Indian languages with limited
training audio and text data. TalTech participated in two tracks of the
challenge: Track 1 that allowed using only the provided training data and Track
3 which allowed using additional audio data. In both tracks, we relied on
wav2vec2.0 models. Our methodology diverges from the traditional procedure of
finetuning pretrained wav2vec2.0 models in two key points: firstly, through the
implementation of the aligned data augmentation technique to enhance the
linguistic diversity of the training data, and secondly, via the application of
deep prefix tuning for dialect adaptation of wav2vec2.0 models. In both tracks,
our approach yielded significant improvements over the provided baselines,
achieving the lowest word error rates across all participating teams.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17451" title="Abstract">arXiv:2310.17451</a> [<a href="/pdf/2310.17451" title="Download PDF">pdf</a>, <a href="/format/2310.17451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating by Understanding: Neural Visual Generation with Logical  Symbol Groundings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yifei Peng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhexu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yao-Xiang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+W">Wang-Zhou Dai</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kun Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)

</div>
<p class="mathjax">Despite the great success of neural visual generative models in recent years,
integrating them with strong symbolic knowledge reasoning systems remains a
challenging task. The main challenges are two-fold: one is symbol assignment,
i.e. bonding latent factors of neural visual generators with meaningful symbols
from knowledge reasoning systems. Another is rule learning, i.e. learning new
rules, which govern the generative process of the data, to augment the
knowledge reasoning systems. To deal with these symbol grounding problems, we
propose a neural-symbolic learning approach, Abductive Visual Generation
(AbdGen), for integrating logic programming systems with neural visual
generative models based on the abductive learning framework. To achieve
reliable and efficient symbol assignment, the quantized abduction method is
introduced for generating abduction proposals by the nearest-neighbor lookups
within semantic codebooks. To achieve precise rule learning, the contrastive
meta-abduction method is proposed to eliminate wrong rules with positive cases
and avoid less-informative rules with negative cases simultaneously.
Experimental results on various benchmark datasets show that compared to the
baselines, AbdGen requires significantly fewer instance-level labeling
information for symbol assignment. Furthermore, our approach can effectively
learn underlying logical generative rules from data, which is out of the
capability of existing approaches.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17455" title="Abstract">arXiv:2310.17455</a> [<a href="/pdf/2310.17455" title="Download PDF">pdf</a>, <a href="/format/2310.17455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OTMatch: Improving Semi-Supervised Learning with Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhiquan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kaipeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semi-supervised learning has made remarkable strides by effectively utilizing
a limited amount of labeled data while capitalizing on the abundant information
present in unlabeled data. However, current algorithms often prioritize
aligning image predictions with specific classes generated through
self-training techniques, thereby neglecting the inherent relationships that
exist within these classes. In this paper, we present a new approach called
OTMatch, which leverages semantic relationships among classes by employing an
optimal transport loss function. By utilizing optimal transport, our proposed
method consistently outperforms established state-of-the-art methods. Notably,
we observed a substantial improvement of a certain percentage in accuracy
compared to the current state-of-the-art method, FreeMatch. OTMatch achieves
3.18%, 3.46%, and 1.28% error rate reduction over FreeMatch on CIFAR-10 with 1
label per class, STL-10 with 4 labels per class, and ImageNet with 100 labels
per class, respectively. This demonstrates the effectiveness and superiority of
our approach in harnessing semantic relationships to enhance learning
performance in a semi-supervised setting.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17458" title="Abstract">arXiv:2310.17458</a> [<a href="/pdf/2310.17458" title="Download PDF">pdf</a>, <a href="/format/2310.17458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coalitional Bargaining via Reinforcement Learning: An Application to  Collaborative Vehicle Routing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mak%2C+S">Stephen Mak</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pearce%2C+T">Tim Pearce</a>, 
<a href="/search/cs?searchtype=author&query=Ostroumov%2C+M">Michael Ostroumov</a>, 
<a href="/search/cs?searchtype=author&query=Brintrup%2C+A">Alexandra Brintrup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2021 Workshop on Cooperative AI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Collaborative Vehicle Routing is where delivery companies cooperate by
sharing their delivery information and performing delivery requests on behalf
of each other. This achieves economies of scale and thus reduces cost,
greenhouse gas emissions, and road congestion. But which company should partner
with whom, and how much should each company be compensated? Traditional game
theoretic solution concepts, such as the Shapley value or nucleolus, are
difficult to calculate for the real-world problem of Collaborative Vehicle
Routing due to the characteristic function scaling exponentially with the
number of agents. This would require solving the Vehicle Routing Problem (an
NP-Hard problem) an exponential number of times. We therefore propose to model
this problem as a coalitional bargaining game where - crucially - agents are
not given access to the characteristic function. Instead, we implicitly reason
about the characteristic function, and thus eliminate the need to evaluate the
VRP an exponential number of times - we only need to evaluate it once. Our
contribution is that our decentralised approach is both scalable and considers
the self-interested nature of companies. The agents learn using a modified
Independent Proximal Policy Optimisation. Our RL agents outperform a strong
heuristic bot. The agents correctly identify the optimal coalitions 79% of the
time with an average optimality gap of 4.2% and reduction in run-time of 62%.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17462" title="Abstract">arXiv:2310.17462</a> [<a href="/pdf/2310.17462" title="Download PDF">pdf</a>, <a href="/format/2310.17462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Learning Monocular 3D Object Localization From 2D Labels using  the Physical Laws of Motion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kienzle%2C+D">Daniel Kienzle</a>, 
<a href="/search/cs?searchtype=author&query=Lorenz%2C+J">Julian Lorenz</a>, 
<a href="/search/cs?searchtype=author&query=Ludwig%2C+K">Katja Ludwig</a>, 
<a href="/search/cs?searchtype=author&query=Lienhart%2C+R">Rainer Lienhart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a novel method for precise 3D object localization in single images
from a single calibrated camera using only 2D labels. No expensive 3D labels
are needed. Thus, instead of using 3D labels, our model is trained with
easy-to-annotate 2D labels along with the physical knowledge of the object's
motion. Given this information, the model can infer the latent third dimension,
even though it has never seen this information during training. Our method is
evaluated on both synthetic and real-world datasets, and we are able to achieve
a mean distance error of just 6 cm in our experiments on real data. The results
indicate the method's potential as a step towards learning 3D object location
estimation, where collecting 3D data for training is not feasible.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17463" title="Abstract">arXiv:2310.17463</a> [<a href="/pdf/2310.17463" title="Download PDF">pdf</a>, <a href="/format/2310.17463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Neural Controlled Differential Equations for Treatment Effect  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hess%2C+K">Konstantin Hess</a>, 
<a href="/search/cs?searchtype=author&query=Melnychuk%2C+V">Valentyn Melnychuk</a>, 
<a href="/search/cs?searchtype=author&query=Frauen%2C+D">Dennis Frauen</a>, 
<a href="/search/cs?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Treatment effect estimation in continuous time is crucial for personalized
medicine. However, existing methods for this task are limited to point
estimates of the potential outcomes, whereas uncertainty estimates have been
ignored. Needless to say, uncertainty quantification is crucial for reliable
decision-making in medical applications. To fill this gap, we propose a novel
Bayesian neural controlled differential equation (BNCDE) for treatment effect
estimation in continuous time. In our BNCDE, the time dimension is modeled
through a coupled system of neural controlled differential equations and neural
stochastic differential equations, where the neural stochastic differential
equations allow for tractable variational Bayesian inference. Thereby, for an
assigned sequence of treatments, our BNCDE provides meaningful posterior
predictive distributions of the potential outcomes. To the best of our
knowledge, ours is the first tailored neural method to provide uncertainty
estimates of treatment effects in continuous time. As such, our method is of
direct practical value for promoting reliable decision-making in medicine.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17468" title="Abstract">arXiv:2310.17468</a> [<a href="/pdf/2310.17468" title="Download PDF">pdf</a>, <a href="/format/2310.17468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-modal Active Complementary Learning with Self-refining  Correspondence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Dezhong Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+P">Peng Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, image-text matching has attracted more and more attention from
academia and industry, which is fundamental to understanding the latent
correspondence across visual and textual modalities. However, most existing
methods implicitly assume the training pairs are well-aligned while ignoring
the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby
inevitably leading to a performance drop. Although some methods attempt to
address such noise, they still face two challenging problems: excessive
memorizing/overfitting and unreliable correction for NC, especially under high
noise. To address the two problems, we propose a generalized Cross-modal Robust
Complementary Learning framework (CRCL), which benefits from a novel Active
Complementary Loss (ACL) and an efficient Self-refining Correspondence
Correction (SCC) to improve the robustness of existing methods. Specifically,
ACL exploits active and complementary learning losses to reduce the risk of
providing erroneous supervision, leading to theoretically and experimentally
demonstrated robustness against NC. SCC utilizes multiple self-refining
processes with momentum correction to enlarge the receptive field for
correcting correspondences, thereby alleviating error accumulation and
achieving accurate and stable corrections. We carry out extensive experiments
on three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify
the superior robustness of our CRCL against synthetic and real-world noisy
correspondences.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17471" title="Abstract">arXiv:2310.17471</a> [<a href="/pdf/2310.17471" title="Download PDF">pdf</a>, <a href="/format/2310.17471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Model Based Native AI Framework in 6G with Cloud-Edge-End  Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhiheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H+H">Howard H. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chenyuan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Junshen Su</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sihui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q. S. Quek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Future wireless communication networks are in a position to move beyond
data-centric, device-oriented connectivity and offer intelligent, immersive
experiences based on task-oriented connections, especially in the context of
the thriving development of pre-trained foundation models (PFM) and the
evolving vision of 6G native artificial intelligence (AI). Therefore,
redefining modes of collaboration between devices and servers and constructing
native intelligence libraries become critically important in 6G. In this paper,
we analyze the challenges of achieving 6G native AI from the perspectives of
data, intelligence, and networks. Then, we propose a 6G native AI framework
based on foundation models, provide a customization approach for intent-aware
PFM, present a construction of a task-oriented AI toolkit, and outline a novel
cloud-edge-end collaboration paradigm. As a practical use case, we apply this
framework for orchestration, achieving the maximum sum rate within a wireless
communication system, and presenting preliminary evaluation results. Finally,
we outline research directions for achieving native AI in 6G.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17475" title="Abstract">arXiv:2310.17475</a> [<a href="/pdf/2310.17475" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical model for large-scale design of sidewalk delivery robot  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuchen Du</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T+V">Tho V. Le</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+J+Y+J">Joseph Y. J. Chow</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">With the rise in demand for local deliveries and e-commerce, robotic
deliveries are being considered as efficient and sustainable solutions.
However, the deployment of such systems can be highly complex due to numerous
factors involving stochastic demand, stochastic charging and maintenance needs,
complex routing, etc. We propose a model that uses continuous approximation
methods for evaluating service trade-offs that consider the unique
characteristics of large-scale sidewalk delivery robot systems used to serve
online food deliveries. The model captures both the initial cost and the
operation cost of the delivery system and evaluates the impact of constraints
and operation strategies on the deployment. By minimizing the system cost,
variables related to the system design can be determined. First, the
minimization problem is formulated based on a homogeneous area, and the optimal
system cost can be derived as a closed-form expression. By evaluating the
expression, relationships between variables and the system cost can be directly
obtained. We then apply the model in neighborhoods in New York City to evaluate
the cost of deploying the sidewalk delivery robot system in a real-world
scenario. The results shed light on the potential of deploying such a system in
the future.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17477" title="Abstract">arXiv:2310.17477</a> [<a href="/pdf/2310.17477" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure short-term load forecasting for smart grids with  transformer-based federated learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sievers%2C+J">Jonas Sievers</a>, 
<a href="/search/cs?searchtype=author&query=Blank%2C+T">Thomas Blank</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Electricity load forecasting is an essential task within smart grids to
assist demand and supply balance. While advanced deep learning models require
large amounts of high-resolution data for accurate short-term load predictions,
fine-grained load profiles can expose users' electricity consumption behaviors,
which raises privacy and security concerns. One solution to improve data
privacy is federated learning, where models are trained locally on private
data, and only the trained model parameters are merged and updated on a global
server. Therefore, this paper presents a novel transformer-based deep learning
approach with federated learning for short-term electricity load prediction. To
evaluate our results, we benchmark our federated learning architecture against
central and local learning and compare the performance of our model to long
short-term memory models and convolutional neural networks. Our simulations are
based on a dataset from a German university campus and show that
transformer-based forecasting is a promising alternative to state-of-the-art
models within federated learning.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17485" title="Abstract">arXiv:2310.17485</a> [<a href="/pdf/2310.17485" title="Download PDF">pdf</a>, <a href="/format/2310.17485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair collaborative vehicle routing: A deep multi-agent reinforcement  learning approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mak%2C+S">Stephen Mak</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pearce%2C+T">Tim Pearce</a>, 
<a href="/search/cs?searchtype=author&query=Ostroumov%2C+M">Michael Ostroumov</a>, 
<a href="/search/cs?searchtype=author&query=Brintrup%2C+A">Alexandra Brintrup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final, published version can be found here: <a href="https://www.sciencedirect.com/science/article/pii/S0968090X23003662">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Volume 157, December 2023, 104376
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Collaborative vehicle routing occurs when carriers collaborate through
sharing their transportation requests and performing transportation requests on
behalf of each other. This achieves economies of scale, thus reducing cost,
greenhouse gas emissions and road congestion. But which carrier should partner
with whom, and how much should each carrier be compensated? Traditional game
theoretic solution concepts are expensive to calculate as the characteristic
function scales exponentially with the number of agents. This would require
solving the vehicle routing problem (NP-hard) an exponential number of times.
We therefore propose to model this problem as a coalitional bargaining game
solved using deep multi-agent reinforcement learning, where - crucially -
agents are not given access to the characteristic function. Instead, we
implicitly reason about the characteristic function; thus, when deployed in
production, we only need to evaluate the expensive post-collaboration vehicle
routing problem once. Our contribution is that we are the first to consider
both the route allocation problem and gain sharing problem simultaneously -
without access to the expensive characteristic function. Through decentralised
machine learning, our agents bargain with each other and agree to outcomes that
correlate well with the Shapley value - a fair profit allocation mechanism.
Importantly, we are able to achieve a reduction in run-time of 88%.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17488" title="Abstract">arXiv:2310.17488</a> [<a href="/pdf/2310.17488" title="Download PDF">pdf</a>, <a href="/format/2310.17488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightLM: A Lightweight Deep and Narrow Language Model for Generative  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+K">Kai Mei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">This paper presents LightLM, a lightweight Transformer-based language model
for generative recommendation. While Transformer-based generative modeling has
gained importance in various AI sub-fields such as NLP and vision, generative
recommendation is still in its infancy due to its unique demand on personalized
generative modeling. Existing works on generative recommendation often use
NLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are
heavy-weight and are not specifically designed for recommendation tasks.
LightLM tackles the issue by introducing a light-weight deep and narrow
Transformer architecture, which is specifically tailored for direct generation
of recommendation items. This structure is especially apt for straightforward
generative recommendation and stems from the observation that language model
does not have to be too wide for this task, as the input predominantly consists
of short tokens that are well-suited for the model's capacity. We also show
that our devised user and item ID indexing methods, i.e., Spectral
Collaborative Indexing (SCI) and Graph Collaborative Indexing (GCI), enables
the deep and narrow Transformer architecture to outperform large-scale language
models for recommendation. Besides, to address the hallucination problem of
generating items as output, we propose the constrained generation process for
generative recommenders. Experiments on real-world datasets show that LightLM
outperforms various competitive baselines in terms of both recommendation
accuracy and efficiency. The code can be found at
https://github.com/dongyuanjushi/LightLM.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17489" title="Abstract">arXiv:2310.17489</a> [<a href="/pdf/2310.17489" title="Download PDF">pdf</a>, <a href="/format/2310.17489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bias in Evaluation Processes: An Optimization-Based Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Celis%2C+L+E">L. Elisa Celis</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Amit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Mehrotra%2C+A">Anay Mehrotra</a>, 
<a href="/search/cs?searchtype=author&query=Vishnoi%2C+N+K">Nisheeth K. Vishnoi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The conference version of this paper appears in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Biases with respect to socially-salient attributes of individuals have been
well documented in evaluation processes used in settings such as admissions and
hiring. We view such an evaluation process as a transformation of a
distribution of the true utility of an individual for a task to an observed
distribution and model it as a solution to a loss minimization problem subject
to an information constraint. Our model has two parameters that have been
identified as factors leading to biases: the resource-information trade-off
parameter in the information constraint and the risk-averseness parameter in
the loss function. We characterize the distributions that arise from our model
and study the effect of the parameters on the observed distribution. The
outputs of our model enrich the class of distributions that can be used to
capture variation across groups in the observed evaluations. We empirically
validate our model by fitting real-world datasets and use it to study the
effect of interventions in a downstream selection task. These results
contribute to an understanding of the emergence of bias in evaluation processes
and provide tools to guide the deployment of interventions to mitigate biases.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17490" title="Abstract">arXiv:2310.17490</a> [<a href="/pdf/2310.17490" title="Download PDF">pdf</a>, <a href="/format/2310.17490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Zero-shot Reader by Reducing Distractions from Irrelevant  Documents in Open-Domain Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Sukmin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J+y">Jeong yeon Seo</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+S">Soyeong Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J+C">Jong C. Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) enable zero-shot approaches in open-domain
question answering (ODQA), yet with limited advancements as the reader is
compared to the retriever. This study aims at the feasibility of a zero-shot
reader that addresses the challenges of computational cost and the need for
labeled data. We find that LLMs are distracted due to irrelevant documents in
the retrieved set and the overconfidence of the generated answers when they are
exploited as zero-shot readers. To tackle these problems, we mitigate the
impact of such documents via Distraction-aware Answer Selection (DAS) with a
negation-based instruction and score adjustment for proper answer selection.
Experimental results show that our approach successfully handles distraction
across diverse scenarios, enhancing the performance of zero-shot readers.
Furthermore, unlike supervised readers struggling with unseen data, zero-shot
readers demonstrate outstanding transferability without any training.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17491" title="Abstract">arXiv:2310.17491</a> [<a href="/pdf/2310.17491" title="Download PDF">pdf</a>, <a href="/format/2310.17491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine  Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation  Models with Mobile Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chua%2C+T+J">Terence Jie Chua</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+K">Kwok-Yan Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The emergence of foundation models, including language and vision models, has
reshaped AI's landscape, offering capabilities across various applications.
Deploying and fine-tuning these large models, like GPT-3 and BERT, presents
challenges, especially in the current foundation model era. We introduce
Emulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning
(PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, we
expand this into federated learning as Federated PEAT (FedPEAT). FedPEAT uses
adapters, emulators, and PEFT for federated model tuning, enhancing model
privacy and memory efficiency. Adapters adjust pre-trained models, while
emulators give a compact representation of original models, addressing both
privacy and efficiency. Adaptable to various neural networks, our approach also
uses deep reinforcement learning for hyper-parameter optimization. We tested
FedPEAT in a unique scenario with a server participating in collaborative
federated tuning, showcasing its potential in tackling foundation model
challenges.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17492" title="Abstract">arXiv:2310.17492</a> [<a href="/pdf/2310.17492" title="Download PDF">pdf</a>, <a href="/ps/2310.17492" title="Download PostScript">ps</a>, <a href="/format/2310.17492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation  Models: A Multi-Agent Deep Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T+J">Terence Jie Chua</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The efficient deployment and fine-tuning of foundation models are pivotal in
contemporary artificial intelligence. In this study, we present a
groundbreaking paradigm integrating Mobile Edge Computing (MEC) with foundation
models, specifically designed to enhance local task performance on user
equipment (UE). Central to our approach is the innovative Emulator-Adapter
architecture, segmenting the foundation model into two cohesive modules. This
design not only conserves computational resources but also ensures adaptability
and fine-tuning efficiency for downstream tasks. Additionally, we introduce an
advanced resource allocation mechanism that is fine-tuned to the needs of the
Emulator-Adapter structure in decentralized settings. To address the challenges
presented by this system, we employ a hybrid multi-agent Deep Reinforcement
Learning (DRL) strategy, adept at handling mixed discrete-continuous action
spaces, ensuring dynamic and optimal resource allocations. Our comprehensive
simulations and validations underscore the practical viability of our approach,
demonstrating its robustness, efficiency, and scalability. Collectively, this
work offers a fresh perspective on deploying foundation models and balancing
computational efficiency with task proficiency.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17493" title="Abstract">arXiv:2310.17493</a> [<a href="/pdf/2310.17493" title="Download PDF">pdf</a>, <a href="/format/2310.17493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Graph Network for Complex Activity Detection in Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Teeti%2C+I">Izzeddin Teeti</a>, 
<a href="/search/cs?searchtype=author&query=Bradley%2C+A">Andrew Bradley</a>, 
<a href="/search/cs?searchtype=author&query=Elhoseiny%2C+M">Mohamed Elhoseiny</a>, 
<a href="/search/cs?searchtype=author&query=Cuzzolin%2C+F">Fabio Cuzzolin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Interpretation and understanding of video presents a challenging computer
vision task in numerous fields - e.g. autonomous driving and sports analytics.
Existing approaches to interpreting the actions taking place within a video
clip are based upon Temporal Action Localisation (TAL), which typically
identifies short-term actions. The emerging field of Complex Activity Detection
(CompAD) extends this analysis to long-term activities, with a deeper
understanding obtained by modelling the internal structure of a complex
activity taking place within the video. We address the CompAD problem using a
hybrid graph neural network which combines attention applied to a graph
encoding the local (short-term) dynamic scene with a temporal graph modelling
the overall long-duration activity. Our approach is as follows: i) Firstly, we
propose a novel feature extraction technique which, for each video snippet,
generates spatiotemporal `tubes' for the active elements (`agents') in the
(local) scene by detecting individual objects, tracking them and then
extracting 3D features from all the agent tubes as well as the overall scene.
ii) Next, we construct a local scene graph where each node (representing either
an agent tube or the scene) is connected to all other nodes. Attention is then
applied to this graph to obtain an overall representation of the local dynamic
scene. iii) Finally, all local scene graph representations are interconnected
via a temporal graph, to estimate the complex activity class together with its
start and end time. The proposed framework outperforms all previous
state-of-the-art methods on all three datasets including ActivityNet-1.3,
Thumos-14, and ROAD.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17498" title="Abstract">arXiv:2310.17498</a> [<a href="/pdf/2310.17498" title="Download PDF">pdf</a>, <a href="/format/2310.17498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CBD: A Certified Backdoor Detector Based on Local Dominant Probability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Z">Zhen Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zidi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Backdoor attack is a common threat to deep neural networks. During testing,
samples embedded with a backdoor trigger will be misclassified as an
adversarial target by a backdoored model, while samples without the backdoor
trigger will be correctly classified. In this paper, we present the first
certified backdoor detector (CBD), which is based on a novel, adjustable
conformal prediction scheme based on our proposed statistic local dominant
probability. For any classifier under inspection, CBD provides 1) a detection
inference, 2) the condition under which the attacks are guaranteed to be
detectable for the same classification domain, and 3) a probabilistic upper
bound for the false positive rate. Our theoretical results show that attacks
with triggers that are more resilient to test-time noise and have smaller
perturbation magnitudes are more likely to be detected with guarantees.
Moreover, we conduct extensive experiments on four benchmark datasets
considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves
comparable or even higher detection accuracy than state-of-the-art detectors,
and it in addition provides detection certification. Notably, for backdoor
attacks with random perturbation triggers bounded by $\ell_2\leq0.75$ which
achieves more than 90\% attack success rate, CBD achieves 100\% (98\%), 100\%
(84\%), 98\% (98\%), and 72\% (40\%) empirical (certified) detection true
positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and
TinyImageNet, respectively, with low false positive rates.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17499" title="Abstract">arXiv:2310.17499</a> [<a href="/pdf/2310.17499" title="Download PDF">pdf</a>, <a href="/format/2310.17499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The IMS Toucan System for the Blizzard Challenge 2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lux%2C+F">Florian Lux</a>, 
<a href="/search/cs?searchtype=author&query=Koch%2C+J">Julia Koch</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+S">Sarina Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Bott%2C+T">Thomas Bott</a>, 
<a href="/search/cs?searchtype=author&query=Schauffler%2C+N">Nadja Schauffler</a>, 
<a href="/search/cs?searchtype=author&query=Denisov%2C+P">Pavel Denisov</a>, 
<a href="/search/cs?searchtype=author&query=Schweitzer%2C+A">Antje Schweitzer</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+N+T">Ngoc Thang Vu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at the Blizzard Challenge Workshop 2023, colocated with the Speech Synthesis Workshop 2023, a sattelite event of the Interspeech 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">For our contribution to the Blizzard Challenge 2023, we improved on the
system we submitted to the Blizzard Challenge 2021. Our approach entails a
rule-based text-to-phoneme processing system that includes rule-based
disambiguation of homographs in the French language. It then transforms the
phonemes to spectrograms as intermediate representations using a fast and
efficient non-autoregressive synthesis architecture based on Conformer and
Glow. A GAN based neural vocoder that combines recent state-of-the-art
approaches converts the spectrogram to the final wave. We carefully designed
the data processing, training, and inference procedures for the challenge data.
Our system identifier is G. Open source code and demo are available.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17501" title="Abstract">arXiv:2310.17501</a> [<a href="/pdf/2310.17501" title="Download PDF">pdf</a>, <a href="/format/2310.17501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lightweight, Compiler-Assisted Register File Cache for GPGPU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shoushtary%2C+M+A">Mojtaba Abaie Shoushtary</a>, 
<a href="/search/cs?searchtype=author&query=Arnau%2C+J+M">Jose Maria Arnau</a>, 
<a href="/search/cs?searchtype=author&query=Murgadas%2C+J+T">Jordi Tubella Murgadas</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+A">Antonio Gonzalez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Modern GPUs require an enormous register file (RF) to store the context of
thousands of active threads. It consumes considerable energy and contains
multiple large banks to provide enough throughput. Thus, a RF caching mechanism
can significantly improve the performance and energy consumption of the GPUs by
avoiding reads from the large banks that consume significant energy and may
cause port conflicts.
<br />This paper introduces an energy-efficient RF caching mechanism called Malekeh
that repurposes an existing component in GPUs' RF to operate as a cache in
addition to its original functionality. In this way, Malekeh minimizes the
overhead of adding a RF cache to GPUs. Besides, Malekeh leverages an issue
scheduling policy that utilizes the reuse distance of the values in the RF
cache and is controlled by a dynamic algorithm. The goal is to adapt the issue
policy to the runtime program characteristics to maximize the GPU's performance
and the hit ratio of the RF cache. The reuse distance is approximated by the
compiler using profiling and is used at run time by the proposed caching
scheme. We show that Malekeh reduces the number of reads to the RF banks by
46.4% and the dynamic energy of the RF by 28.3%. Besides, it improves
performance by 6.1% while adding only 2KB of extra storage per core to the
baseline RF of 256KB, which represents a negligible overhead of 0.78%.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17502" title="Abstract">arXiv:2310.17502</a> [<a href="/pdf/2310.17502" title="Download PDF">pdf</a>, <a href="/format/2310.17502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controllable Generation of Artificial Speaker Embeddings through  Discovery of Principal Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lux%2C+F">Florian Lux</a>, 
<a href="/search/cs?searchtype=author&query=Tilli%2C+P">Pascal Tilli</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+S">Sarina Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+N+T">Ngoc Thang Vu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ISCA Interspeech 2023 <a href="https://www.isca-speech.org/archive/interspeech_2023/lux23_interspeech.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Customizing voice and speaking style in a speech synthesis system with
intuitive and fine-grained controls is challenging, given that little data with
appropriate labels is available. Furthermore, editing an existing human's voice
also comes with ethical concerns. In this paper, we propose a method to
generate artificial speaker embeddings that cannot be linked to a real human
while offering intuitive and fine-grained control over the voice and speaking
style of the embeddings, without requiring any labels for speaker or style. The
artificial and controllable embeddings can be fed to a speech synthesis system,
conditioned on embeddings of real humans during training, without sacrificing
privacy during inference.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17504" title="Abstract">arXiv:2310.17504</a> [<a href="/pdf/2310.17504" title="Download PDF">pdf</a>, <a href="/format/2310.17504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Distillation of Image Representations into Point Clouds  for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Puy%2C+G">Gilles Puy</a>, 
<a href="/search/cs?searchtype=author&query=Gidaris%2C+S">Spyros Gidaris</a>, 
<a href="/search/cs?searchtype=author&query=Boulch%2C+A">Alexandre Boulch</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%A9oni%2C+O">Oriane Sim&#xe9;oni</a>, 
<a href="/search/cs?searchtype=author&query=Sautier%2C+C">Corentin Sautier</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+P">Patrick P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Bursuc%2C+A">Andrei Bursuc</a>, 
<a href="/search/cs?searchtype=author&query=Marlet%2C+R">Renaud Marlet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised image networks can be used to address complex 2D tasks (e.g.,
semantic segmentation, object discovery) very efficiently and with little or no
downstream supervision. However, self-supervised 3D networks on lidar data do
not perform as well for now. A few methods therefore propose to distill
high-quality self-supervised 2D features into 3D networks. The most recent ones
doing so on autonomous driving data show promising results. Yet, a performance
gap persists between these distilled features and fully-supervised ones. In
this work, we revisit 2D-to-3D distillation. First, we propose, for semantic
segmentation, a simple approach that leads to a significant improvement
compared to prior 3D distillation methods. Second, we show that distillation in
high capacity 3D networks is key to reach high quality 3D features. This
actually allows us to significantly close the gap between unsupervised
distilled 3D features and fully-supervised ones. Last, we show that our
high-quality distilled representations can also be used for open-vocabulary
segmentation and background/foreground discovery.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17505" title="Abstract">arXiv:2310.17505</a> [<a href="/pdf/2310.17505" title="Download PDF">pdf</a>, <a href="/format/2310.17505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Free Space Optical Communication for Inter-Satellite Link: Architecture,  Potentials and Trends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+G">Guanhua Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+F">Fang Yang</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+J">Jian Song</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+Z">Zhu Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The sixth-generation (6G) network is expected to achieve global coverage
based on the space-air-ground integrated network, and the latest satellite
network will play an important role in it. The introduction of inter-satellite
links (ISLs) can significantly improve the throughput of the satellite network,
and recently gets lots of attention from both academia and industry. In this
paper, we illustrate the advantages of using the laser for ISLs due to its
longer communication distance, higher data speed, and stronger security.
Specifically, space-borne laser terminals with the acquisition, pointing and
tracking mechanism which realize long-distance communication are illustrated,
advanced modulation and multiplexing modes that make high communication rates
possible are introduced, and the security of ISLs ensured by the
characteristics of both laser and the optical channel is also analyzed.
Moreover, some open issues such as advanced optical beam steering, routing and
scheduling algorithm, and integrated sensing and communication are discussed to
direct future research.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17512" title="Abstract">arXiv:2310.17512</a> [<a href="/pdf/2310.17512" title="Download PDF">pdf</a>, <a href="/format/2310.17512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CompeteAI: Understanding the Competition Behaviors in Large Language  Model-based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qinlin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yiqiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kaijie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report; 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Large language models (LLMs) have been widely used as agents to complete
different tasks, such as personal assistance or event planning. While most work
has focused on cooperation and collaboration between agents, little work
explores competition, another important mechanism that fosters the development
of society and economy. In this paper, we seek to examine the competition
behaviors in LLM-based agents. We first propose a general framework to study
the competition between agents. Then, we implement a practical competitive
environment using GPT-4 to simulate a virtual town with two types of agents,
including restaurant agents and customer agents. Specifically, restaurant
agents compete with each other to attract more customers, where the competition
fosters them to transform, such as cultivating new operating strategies. The
results of our experiments reveal several interesting findings ranging from
social learning to Matthew Effect, which aligns well with existing sociological
and economic theories. We believe that competition between agents deserves
further investigation to help us understand society better. The code will be
released soon.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17513" title="Abstract">arXiv:2310.17513</a> [<a href="/pdf/2310.17513" title="Download PDF">pdf</a>, <a href="/format/2310.17513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Expressive Power of Low-Rank Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yuchen Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kangwook Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages,5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that
leverages low-rank adaptation of weight matrices, has emerged as a prevalent
technique for fine-tuning pre-trained models such as large language models and
diffusion models. Despite its huge success in practice, the theoretical
underpinnings of LoRA have largely remained unexplored. This paper takes the
first step to bridge this gap by theoretically analyzing the expressive power
of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any
model $f$ to accurately represent any smaller target model $\overline{f}$ if
LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of
}\overline{f}}{\text{depth of }f}$. We also quantify the approximation error
when LoRA-rank is lower than the threshold. For Transformer networks, we show
any model can be adapted to a target model of the same size with
rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17514" title="Abstract">arXiv:2310.17514</a> [<a href="/pdf/2310.17514" title="Download PDF">pdf</a>, <a href="/format/2310.17514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Validity of Evaluation Results: Assessing Concurrence Across  Compositionality Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Kaiser Sun</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+A">Adina Williams</a>, 
<a href="/search/cs?searchtype=author&query=Hupkes%2C+D">Dieuwke Hupkes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoNLL2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">NLP models have progressed drastically in recent years, according to numerous
datasets proposed to evaluate performance. Questions remain, however, about how
particular dataset design choices may impact the conclusions we draw about
model capabilities. In this work, we investigate this question in the domain of
compositional generalization. We examine the performance of six modeling
approaches across 4 datasets, split according to 8 compositional splitting
strategies, ranking models by 18 compositional generalization splits in total.
Our results show that: i) the datasets, although all designed to evaluate
compositional generalization, rank modeling approaches differently; ii)
datasets generated by humans align better with each other than they with
synthetic datasets, or than synthetic datasets among themselves; iii)
generally, whether datasets are sampled from the same source is more predictive
of the resulting model ranking than whether they maintain the same
interpretation of compositionality; and iv) which lexical items are used in the
data can strongly impact conclusions. Overall, our results demonstrate that
much work remains to be done when it comes to assessing whether popular
evaluation datasets measure what they intend to measure, and suggest that
elucidating more rigorous standards for establishing the validity of evaluation
sets could benefit the field.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17519" title="Abstract">arXiv:2310.17519</a> [<a href="/pdf/2310.17519" title="Download PDF">pdf</a>, <a href="/format/2310.17519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLARE: Fast Learning of Animatable and Relightable Mesh Avatars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bharadwaj%2C+S">Shrisha Bharadwaj</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yufeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hilliges%2C+O">Otmar Hilliges</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+M+J">Michael J. Black</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Abrevaya%2C+V">Victoria Fernandez-Abrevaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, Accepted: ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia), 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Volume 42, article number 204, year 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Our goal is to efficiently learn personalized animatable 3D head avatars from
videos that are geometrically accurate, realistic, relightable, and compatible
with current rendering systems. While 3D meshes enable efficient processing and
are highly portable, they lack realism in terms of shape and appearance. Neural
representations, on the other hand, are realistic but lack compatibility and
are slow to train and render. Our key insight is that it is possible to
efficiently learn high-fidelity 3D mesh representations via differentiable
rendering by exploiting highly-optimized methods from traditional computer
graphics and approximating some of the components with neural networks. To that
end, we introduce \moniker, a technique that enables the creation of animatable
and relightable mesh avatars from a single monocular video. First, we learn a
canonical geometry using a mesh representation, enabling efficient
differentiable rasterization and straightforward animation via learned
blendshapes and linear blend skinning weights. Second, we follow
physically-based rendering and factor observed colors into intrinsic albedo,
roughness, and a neural representation of the illumination, allowing the
learned avatars to be relit in novel scenes. Since our input videos are
captured on a single device with a narrow field of view, modeling the
surrounding environment light is non-trivial. Based on the split-sum
approximation for modeling specular reflections, we address this by
approximating the pre-filtered environment map with a multi-layer perceptron
(MLP) modulated by the surface roughness, eliminating the need to explicitly
model the light. We demonstrate that our mesh-based avatar formulation,
combined with learned deformation, material, and lighting MLPs, produces
avatars with high-quality geometry and appearance, while also being efficient
to train and render compared to existing approaches.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17522" title="Abstract">arXiv:2310.17522</a> [<a href="/pdf/2310.17522" title="Download PDF">pdf</a>, <a href="/format/2310.17522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proposal on Model Based Current Overshoot Suppression of Receiver Side  Coil in Drone Wireless Power Transfer System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fujimoto%2C+K">Kota Fujimoto</a>, 
<a href="/search/eess?searchtype=author&query=Hamada%2C+T">Takumi Hamada</a>, 
<a href="/search/eess?searchtype=author&query=Fujimoto%2C+H">Hiroshi Fujimoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was presented at IEEE 2022 Wireless Power Week (WPW)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper proposes a model-based control method in the wireless power
transfer (WPT) system by operating a semi-bridgeless active rectifier (SBAR) to
suppress the secondary coil current overshoot. By damping the current
overshoot, it is possible to reduce the rectifier's rated current and decrease
the rectifier's size, which is beneficial for the lightweight-oriented system
such as drones. In the control method, an inverse of the plant model is used to
calculate the reference input to the system. The current overshoot is reduced
by operating the SBAR under the duty ratio calculated from the model. To
confirm the performance of the proposed method, the simulation and the
experiment using the WPT prototype are conducted. The experimental results show
that the proposed method can suppress the secondary coil current overshoot. The
results suggest it is possible to realize the lighter secondary system by
applying the proposed method.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17523" title="Abstract">arXiv:2310.17523</a> [<a href="/pdf/2310.17523" title="Download PDF">pdf</a>, <a href="/ps/2310.17523" title="Download PostScript">ps</a>, <a href="/format/2310.17523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Resource Management for Edge Network Slicing using Incremental  Multi-Agent Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haiyuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yuelin Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+X">Xueqing Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Vasilakos%2C+X">Xenofon Vasilakos</a>, 
<a href="/search/eess?searchtype=author&query=Nejabati%2C+R">Reza Nejabati</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+S">Shuangyi Yan</a>, 
<a href="/search/eess?searchtype=author&query=Simenidou%2C+D">Dimitra Simenidou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Multi-access edge computing provides local resources in mobile networks as
the essential means for meeting the demands of emerging ultra-reliable
low-latency communications. At the edge, dynamic computing requests require
advanced resource management for adaptive network slicing, including resource
allocations, function scaling and load balancing to utilize only the necessary
resources in resource-constraint networks. Recent solutions are designed for a
static number of slices. Therefore, the painful process of optimization is
required again with any update on the number of slices. In addition, these
solutions intend to maximize instant rewards, neglecting long-term resource
scheduling. Unlike these efforts, we propose an algorithmic approach based on
multi-agent deep deterministic policy gradient (MADDPG) for optimizing resource
management for edge network slicing. Our objective is two-fold: (i) maximizing
long-term network slicing benefits in terms of delay and energy consumption,
and (ii) adapting to slice number changes. Through simulations, we demonstrate
that MADDPG outperforms benchmark solutions including a static slicing-based
one from the literature, achieving stable and high long-term performance.
Additionally, we leverage incremental learning to facilitate a dynamic number
of edge slices, with enhanced performance compared to pre-trained base models.
Remarkably, this approach yields superior reward performance while saving
approximately 90% of training time costs.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17526" title="Abstract">arXiv:2310.17526</a> [<a href="/pdf/2310.17526" title="Download PDF">pdf</a>, <a href="/format/2310.17526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can large language models replace humans in the systematic review  process? Evaluating GPT-4&#x27;s efficacy in screening and extracting data from  peer-reviewed and grey literature in multiple languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khraisha%2C+Q">Qusai Khraisha</a>, 
<a href="/search/cs?searchtype=author&query=Put%2C+S">Sophie Put</a>, 
<a href="/search/cs?searchtype=author&query=Kappenberg%2C+J">Johanna Kappenberg</a>, 
<a href="/search/cs?searchtype=author&query=Warraitch%2C+A">Azza Warraitch</a>, 
<a href="/search/cs?searchtype=author&query=Hadfield%2C+K">Kristin Hadfield</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Systematic reviews are vital for guiding practice, research, and policy, yet
they are often slow and labour-intensive. Large language models (LLMs) could
offer a way to speed up and automate systematic reviews, but their performance
in such tasks has not been comprehensively evaluated against humans, and no
study has tested GPT-4, the biggest LLM so far. This pre-registered study
evaluates GPT-4's capability in title/abstract screening, full-text review, and
data extraction across various literature types and languages using a
'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human
performance in most tasks, results were skewed by chance agreement and dataset
imbalance. After adjusting for these, there was a moderate level of performance
for data extraction, and - barring studies that used highly reliable prompts -
screening performance levelled at none to moderate for different stages and
languages. When screening full-text literature using highly reliable prompts,
GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key
studies using highly reliable prompts improved its performance even more. Our
findings indicate that, currently, substantial caution should be used if LLMs
are being used to conduct systematic reviews, but suggest that, for certain
systematic review tasks delivered under reliable prompts, LLMs can rival human
performance.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17527" title="Abstract">arXiv:2310.17527</a> [<a href="/pdf/2310.17527" title="Download PDF">pdf</a>, <a href="/format/2310.17527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Space-Time Hash Encoding for Efficient Dynamic Scene  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zilong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guokang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yafei Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huaping Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose the Masked Space-Time Hash encoding (MSTH), a novel
method for efficiently reconstructing dynamic 3D scenes from multi-view or
monocular videos. Based on the observation that dynamic scenes often contain
substantial static areas that result in redundancy in storage and computations,
MSTH represents a dynamic scene as a weighted combination of a 3D hash encoding
and a 4D hash encoding. The weights for the two components are represented by a
learnable mask which is guided by an uncertainty-based objective to reflect the
spatial and temporal importance of each 3D position. With this design, our
method can reduce the hash collision rate by avoiding redundant queries and
modifications on static areas, making it feasible to represent a large number
of space-time voxels by hash tables with small size.Besides, without the
requirements to fit the large numbers of temporally redundant features
independently, our method is easier to optimize and converge rapidly with only
twenty minutes of training for a 300-frame dynamic scene.As a result, MSTH
obtains consistently better results than previous methods with only 20 minutes
of training time and 130 MB of memory storage. Code is available at
https://github.com/masked-spacetime-hashing/msth
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17530" title="Abstract">arXiv:2310.17530</a> [<a href="/pdf/2310.17530" title="Download PDF">pdf</a>, <a href="/format/2310.17530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Bias and Fairness in Gender-Neutral Pretrained  Vision-and-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabello%2C+L">Laura Cabello</a>, 
<a href="/search/cs?searchtype=author&query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
<a href="/search/cs?searchtype=author&query=Brandl%2C+S">Stephanie Brandl</a>, 
<a href="/search/cs?searchtype=author&query=Elliott%2C+D">Desmond Elliott</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EMNLP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Pretrained machine learning models are known to perpetuate and even amplify
existing biases in data, which can result in unfair outcomes that ultimately
impact user experience. Therefore, it is crucial to understand the mechanisms
behind those prejudicial biases to ensure that model performance does not
result in discriminatory behaviour toward certain groups or populations. In
this work, we define gender bias as our case study. We quantify bias
amplification in pretraining and after fine-tuning on three families of
vision-and-language models. We investigate the connection, if any, between the
two learning stages, and evaluate how bias amplification reflects on model
performance. Overall, we find that bias amplification in pretraining and after
fine-tuning are independent. We then examine the effect of continued
pretraining on gender-neutral data, finding that this reduces group
disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without
significantly compromising task performance.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17531" title="Abstract">arXiv:2310.17531</a> [<a href="/pdf/2310.17531" title="Download PDF">pdf</a>, <a href="/ps/2310.17531" title="Download PostScript">ps</a>, <a href="/format/2310.17531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Regularized Graphon Mean-Field Games with Unknown Graphons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fengzhuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+V+Y+F">Vincent Y. F. Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We design and analyze reinforcement learning algorithms for Graphon
Mean-Field Games (GMFGs). In contrast to previous works that require the
precise values of the graphons, we aim to learn the Nash Equilibrium (NE) of
the regularized GMFGs when the graphons are unknown. Our contributions are
threefold. First, we propose the Proximal Policy Optimization for GMFG
(GMFG-PPO) algorithm and show that it converges at a rate of $O(T^{-1/3})$
after $T$ iterations with an estimation oracle, improving on a previous work by
Xie et al. (ICML, 2021). Second, using kernel embedding of distributions, we
design efficient algorithms to estimate the transition kernels, reward
functions, and graphons from sampled agents. Convergence rates are then derived
when the positions of the agents are either known or unknown. Results for the
combination of the optimization algorithm GMFG-PPO and the estimation algorithm
are then provided. These algorithms are the first specifically designed for
learning graphons from sampled agents. Finally, the efficacy of the proposed
algorithms are corroborated through simulations. These simulations demonstrate
that learning the unknown graphons reduces the exploitability effectively.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17532" title="Abstract">arXiv:2310.17532</a> [<a href="/pdf/2310.17532" title="Download PDF">pdf</a>, <a href="/format/2310.17532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Consensus in Content Centric Networking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mosko%2C+M">Marc Mosko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">We describe a method to achieve distributed consensus in a Content Centric
Network using the PAXOS algorithm. Consensus is necessary, for example, if
multiple writers wish to agree on the current version number of a CCNx name or
if multiple distributed systems wish to elect a leader for fast transaction
processing. We describe two forms of protocols, one using standard CCNx
Interest request and Content Object response, and the second using a CCNx Push
request and response. We further divide the protocols in to those using the
CCNx 0.x protocol where Content Object name may continue Interest names and the
CCNx 1.0 protocol where Content Object names exactly match Interest names.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17533" title="Abstract">arXiv:2310.17533</a> [<a href="/pdf/2310.17533" title="Download PDF">pdf</a>, <a href="/ps/2310.17533" title="Download PostScript">ps</a>, <a href="/format/2310.17533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding The Digital Fuk&#xfa;: Deciphering Colonial Legacies to Critically  Assess ChatGPT in Dominican Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ovalle%2C+A">Anaelia Ovalle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Educational disparities within the Dominican Republic (DR) have long-standing
origins rooted in economic, political, and social inequity. Addressing these
challenges has necessarily called for capacity building with respect to
educational materials, high-quality instruction, and structural resourcing.
Generative AI tools like ChatGPT have begun to pique the interest of Dominican
educators due to their perceived potential to bridge these educational gaps.
However, a substantial body of AI fairness literature has documented ways AI
disproportionately reinforces power dynamics reflective of jurisdictions
driving AI development and deployment policies, collectively termed the AI
Global North. As such, indiscriminate adoption of this technology for DR
education, even in part, risks perpetuating forms of digital coloniality.
Therefore, this paper centers embracing AI-facilitated educational reform by
critically examining how AI-driven tools like ChatGPT in DR education may
replicate facets of digital colonialism. We provide a concise overview of
20th-century Dominican education reforms following the 1916 US occupation.
Then, we employ identified neocolonial aspects historically shaping Dominican
education to interrogate the perceived advantages of ChatGPT for contemporary
Dominican education, as outlined by a Dominican scholar. This work invites AI
Global North &amp; South developers, stakeholders, and Dominican leaders alike to
exercise a relational contextualization of data-centric epistemologies like
ChatGPT to reap its transformative benefits while remaining vigilant of
safeguarding Dominican digital sovereignty.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17534" title="Abstract">arXiv:2310.17534</a> [<a href="/pdf/2310.17534" title="Download PDF">pdf</a>, <a href="/format/2310.17534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoK: Pitfalls in Evaluating Black-Box Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suya%2C+F">Fnu Suya</a>, 
<a href="/search/cs?searchtype=author&query=Suri%2C+A">Anshuman Suri</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tingwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Jingtao Hong</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+D">David Evans</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Numerous works study black-box attacks on image classifiers. However, these
works make different assumptions on the adversary's knowledge and current
literature lacks a cohesive organization centered around the threat model. To
systematize knowledge in this area, we propose a taxonomy over the threat space
spanning the axes of feedback granularity, the access of interactive queries,
and the quality and quantity of the auxiliary data available to the attacker.
Our new taxonomy provides three key insights. 1) Despite extensive literature,
numerous under-explored threat spaces exist, which cannot be trivially solved
by adapting techniques from well-explored settings. We demonstrate this by
establishing a new state-of-the-art in the less-studied setting of access to
top-k confidence scores by adapting techniques from well-explored settings of
accessing the complete confidence vector, but show how it still falls short of
the more restrictive setting that only obtains the prediction label,
highlighting the need for more research. 2) Identification the threat model of
different attacks uncovers stronger baselines that challenge prior
state-of-the-art claims. We demonstrate this by enhancing an initially weaker
baseline (under interactive query access) via surrogate models, effectively
overturning claims in the respective paper. 3) Our taxonomy reveals
interactions between attacker knowledge that connect well to related areas,
such as model inversion and extraction attacks. We discuss how advances in
other areas can enable potentially stronger black-box attacks. Finally, we
emphasize the need for a more realistic assessment of attack success by
factoring in local attack runtime. This approach reveals the potential for
certain attacks to achieve notably higher success rates and the need to
evaluate attacks in diverse and harder settings, highlighting the need for
better selection criteria.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17537" title="Abstract">arXiv:2310.17537</a> [<a href="/pdf/2310.17537" title="Download PDF">pdf</a>, <a href="/format/2310.17537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic  Forgetting in Curiosity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J">Jaedong Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Z">Zhang-Wei Hong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Eric Chen</a>, 
<a href="/search/cs?searchtype=author&query=Boopathy%2C+A">Akhilan Boopathy</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pulkit Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Fiete%2C+I">Ila Fiete</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Workshop - Intrinsically Motivated Open-ended Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep reinforcement learning methods exhibit impressive performance on a range
of tasks but still struggle on hard exploration tasks in large environments
with sparse rewards. To address this, intrinsic rewards can be generated using
forward model prediction errors that decrease as the environment becomes known,
and incentivize an agent to explore novel states. While prediction-based
intrinsic rewards can help agents solve hard exploration tasks, they can suffer
from catastrophic forgetting and actually increase at visited states. We first
examine the conditions and causes of catastrophic forgetting in grid world
environments. We then propose a new method FARCuriosity, inspired by how humans
and animals learn. The method depends on fragmentation and recall: an agent
fragments an environment based on surprisal, and uses different local curiosity
modules (prediction-based intrinsic reward functions) for each fragment so that
modules are not trained on the entire environment. At each fragmentation event,
the agent stores the current module in long-term memory (LTM) and either
initializes a new module or recalls a previously stored module based on its
match with the current state. With fragmentation and recall, FARCuriosity
achieves less forgetting and better overall performance in games with varied
and heterogeneous environments in the Atari benchmark suite of tasks. Thus,
this work highlights the problem of catastrophic forgetting in prediction-based
curiosity methods and proposes a solution.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17538" title="Abstract">arXiv:2310.17538</a> [<a href="/pdf/2310.17538" title="Download PDF">pdf</a>, <a href="/format/2310.17538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Little Exploration is All You Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H+H+H">Henry H.H. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiaming Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The prevailing principle of "Optimism in the Face of Uncertainty" advocates
for the incorporation of an exploration bonus, generally assumed to be
proportional to the inverse square root of the visit count ($1/\sqrt{n}$),
where $n$ is the number of visits to a particular state-action pair. This
approach, however, exclusively focuses on "uncertainty," neglecting the
inherent "difficulty" of different options. To address this gap, we introduce a
novel modification of standard UCB algorithm in the multi-armed bandit problem,
proposing an adjusted bonus term of $1/n^\tau$, where $\tau &gt; 1/2$, that
accounts for task difficulty. Our proposed algorithm, denoted as UCB$^\tau$, is
substantiated through comprehensive regret and risk analyses, confirming its
theoretical robustness. Comparative evaluations with standard UCB and Thompson
Sampling algorithms on synthetic datasets demonstrate that UCB$^\tau$ not only
outperforms in efficacy but also exhibits lower risk across various
environmental conditions and hyperparameter settings.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17540" title="Abstract">arXiv:2310.17540</a> [<a href="/pdf/2310.17540" title="Download PDF">pdf</a>, <a href="/format/2310.17540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EqDrive: Efficient Equivariant Motion Forecasting with Multi-Modality  for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jier Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Forecasting vehicular motions in autonomous driving requires a deep
understanding of agent interactions and the preservation of motion equivariance
under Euclidean geometric transformations. Traditional models often lack the
sophistication needed to handle the intricate dynamics inherent to autonomous
vehicles and the interaction relationships among agents in the scene. As a
result, these models have a lower model capacity, which then leads to higher
prediction errors and lower training efficiency. In our research, we employ
EqMotion, a leading equivariant particle, and human prediction model that also
accounts for invariant agent interactions, for the task of multi-agent vehicle
motion forecasting. In addition, we use a multi-modal prediction mechanism to
account for multiple possible future paths in a probabilistic manner. By
leveraging EqMotion, our model achieves state-of-the-art (SOTA) performance
with fewer parameters (1.2 million) and a significantly reduced training time
(less than 2 hours).
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17541" title="Abstract">arXiv:2310.17541</a> [<a href="/pdf/2310.17541" title="Download PDF">pdf</a>, <a href="/format/2310.17541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test Bench Study on Attitude Estimation in Ground Effect Region Based on  Motor Current for In-Flight Inductive Power Transfer of Drones
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+K">Kota Fujimoto</a>, 
<a href="/search/cs?searchtype=author&query=Nagai%2C+S">Sakahisa Nagai</a>, 
<a href="/search/cs?searchtype=author&query=Minh%2C+N+B">Nguyen Binh Minh</a>, 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+H">Hiroshi Fujimoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was presented at The 49th Annual Conference of the IEEE Industrial Electronics Society (IECON2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">To overcome the short flight duration of drones, research on in-flight
inductive power transfer has been recognized as an essential solution. Thus, it
is important to accurately estimate and control the attitude of the drones
which operate close to the charging surface. To this end, this paper proposes
an attitude estimation method based solely on the motor current for precision
flight control in the ground effect region. The model for the estimation is
derived based on the motor equation when it rotates at a constant rotational
speed. The proposed method is verified on the simulations and experiments. It
allows simultaneous estimation of altitude and pitch angle with the accuracy of
0.30$\hspace{0.5mm}$m and 0.04 rad, respectively. The minimum transmission
efficiency of the in-flight power transfer system based on the proposed
estimation is calculated as 95.3 %, which is sufficient for the efficient
system.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17544" title="Abstract">arXiv:2310.17544</a> [<a href="/pdf/2310.17544" title="Download PDF">pdf</a>, <a href="/ps/2310.17544" title="Download PostScript">ps</a>, <a href="/format/2310.17544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Ensemble-Based Feature Selection for Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tumay%2C+A">Aysin Tumay</a>, 
<a href="/search/cs?searchtype=author&query=Aydin%2C+M+E">Mustafa E. Aydin</a>, 
<a href="/search/cs?searchtype=author&query=Kozat%2C+S+S">Suleyman S. Kozat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study a novel ensemble approach for feature selection based on
hierarchical stacking in cases of non-stationarity and limited number of
samples with large number of features. Our approach exploits the co-dependency
between features using a hierarchical structure. Initially, a machine learning
model is trained using a subset of features, and then the model's output is
updated using another algorithm with the remaining features to minimize the
target loss. This hierarchical structure allows for flexible depth and feature
selection. By exploiting feature co-dependency hierarchically, our proposed
approach overcomes the limitations of traditional feature selection methods and
feature importance scores. The effectiveness of the approach is demonstrated on
synthetic and real-life datasets, indicating improved performance with
scalability and stability compared to the traditional methods and
state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17545" title="Abstract">arXiv:2310.17545</a> [<a href="/pdf/2310.17545" title="Download PDF">pdf</a>, <a href="/format/2310.17545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Buckingham&#x27;s $&#x3c0;$ Theorem for Multi-System Learning Transfer: a  Case-study with 3 Vehicles Sharing a Database
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Therrien%2C+W">William Therrien</a>, 
<a href="/search/cs?searchtype=author&query=Lecompte%2C+O">Olivier Lecompte</a>, 
<a href="/search/cs?searchtype=author&query=Girard%2C+A">Alexandre Girard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Learning schemes for planning and control are limited by the difficulty of
collecting large amounts of experimental data or having to rely on
high-fidelity simulations. This paper explores the potential of a proposed
learning scheme that leverages dimensionless numbers based on Buckingham's
$\pi$ theorem to improve data efficiency and facilitate knowledge sharing
between similar systems. A case study using car-like robots compares
traditional and dimensionless learning models on simulated and experimental
data to validate the benefits of the new dimensionless learning approach.
Preliminary results show that this new dimensionless approach could accelerate
the learning rate and improve the accuracy of the model and should be
investigated further.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17550" title="Abstract">arXiv:2310.17550</a> [<a href="/pdf/2310.17550" title="Download PDF">pdf</a>, <a href="/format/2310.17550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Guided Complexity-Controlled Abstractions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+A">Andi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tucker%2C+M">Mycal Tucker</a>, 
<a href="/search/cs?searchtype=author&query=Kenny%2C+E">Eoin Kenny</a>, 
<a href="/search/cs?searchtype=author&query=Zaslavsky%2C+N">Noga Zaslavsky</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pulkit Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+J">Julie Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Neural networks often learn task-specific latent representations that fail to
generalize to novel settings or tasks. Conversely, humans learn discrete
representations (i.e., concepts or words) at a variety of abstraction levels
(e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based
on task. Inspired by this, we train neural models to generate a spectrum of
discrete representations, and control the complexity of the representations
(roughly, how many bits are allocated for encoding inputs) by tuning the
entropy of the distribution over representations. In finetuning experiments,
using only a small number of labeled examples for a new task, we show that (1)
tuning the representation to a task-appropriate complexity level supports the
highest finetuning performance, and (2) in a human-participant study, users
were able to identify the appropriate complexity level for a downstream task
using visualizations of discrete representations. Our results indicate a
promising direction for rapid model finetuning by leveraging human insight.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17551" title="Abstract">arXiv:2310.17551</a> [<a href="/pdf/2310.17551" title="Download PDF">pdf</a>, <a href="/format/2310.17551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unpacking the Ethical Value Alignment in Big Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+X">Xiaoyuan Yi</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jing Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Big models have greatly advanced AI's ability to understand, generate, and
manipulate information and content, enabling numerous applications. However, as
these models become increasingly integrated into everyday life, their inherent
ethical values and potential biases pose unforeseen risks to society. This
paper provides an overview of the risks and challenges associated with big
models, surveys existing AI ethics guidelines, and examines the ethical
implications arising from the limitations of these models. Taking a normative
ethics perspective, we propose a reassessment of recent normative guidelines,
highlighting the importance of collaborative efforts in academia to establish a
unified and universal AI ethics framework. Furthermore, we investigate the
moral inclinations of current mainstream LLMs using the Moral Foundation
theory, analyze existing alignment algorithms, and outline the unique
challenges encountered in aligning ethical values within them. To address these
challenges, we introduce a novel conceptual paradigm for aligning the ethical
values of big models and discuss promising research directions for alignment
criteria, evaluation, and method, representing an initial step towards the
interdisciplinary construction of the ethically aligned AI
<br />This paper is a modified English version of our Chinese paper
https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended
to help non-Chinese native speakers better understand our work.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17552" title="Abstract">arXiv:2310.17552</a> [<a href="/pdf/2310.17552" title="Download PDF">pdf</a>, <a href="/format/2310.17552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Based Runtime Monitoring with Interactive Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dass%2C+S">Shivin Dass</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn-Mart%C3%ADn%2C+R">Roberto Mart&#xed;n-Mart&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Robot learning methods have recently made great strides, but generalization
and robustness challenges still hinder their widespread deployment. Failing to
detect and address potential failures renders state-of-the-art learning systems
not combat-ready for high-stakes tasks. Recent advances in interactive
imitation learning have presented a promising framework for human-robot
teaming, enabling the robots to operate safely and continually improve their
performances over long-term deployments. Nonetheless, existing methods
typically require constant human supervision and preemptive feedback, limiting
their practicality in realistic domains. This work aims to endow a robot with
the ability to monitor and detect errors during task execution. We introduce a
model-based runtime monitoring algorithm that learns from deployment data to
detect system anomalies and anticipate failures. Unlike prior work that cannot
foresee future failures or requires failure experiences for training, our
method learns a latent-space dynamics model and a failure classifier, enabling
our method to simulate future action outcomes and detect out-of-distribution
and high-risk states preemptively. We train our method within an interactive
imitation learning framework, where it continually updates the model from the
experiences of the human-robot team collected using trustworthy deployments.
Consequently, our method reduces the human workload needed over time while
ensuring reliable task execution. Our method outperforms the baselines across
system-level and unit-test metrics, with 23% and 40% higher success rates in
simulation and on physical hardware, respectively. More information at
https://ut-austin-rpl.github.io/sirius-runtime-monitor/
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17555" title="Abstract">arXiv:2310.17555</a> [<a href="/pdf/2310.17555" title="Download PDF">pdf</a>, <a href="/format/2310.17555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Robot Learning from Verbal Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Alice Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Swaminathan%2C+A">Adith Swaminathan</a>, 
<a href="/search/cs?searchtype=author&query=Kolobov%2C+A">Andrey Kolobov</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Ching-An Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The ability to learn and refine behavior after deployment has become ever
more important for robots as we design them to operate in unstructured
environments like households. In this work, we design a new learning system
based on large language model (LLM), OLAF, that allows everyday users to teach
a robot using verbal corrections when the robot makes mistakes, e.g., by saying
"Stop what you're doing. You should move closer to the cup." A key feature of
OLAF is its ability to update the robot's visuomotor neural policy based on the
verbal feedback to avoid repeating mistakes in the future. This is in contrast
to existing LLM-based robotic systems, which only follow verbal commands or
corrections but not learn from them. We demonstrate the efficacy of our design
in experiments where a user teaches a robot to perform long-horizon
manipulation tasks both in simulation and on physical hardware, achieving on
average 20.0% improvement in policy success rate. Videos and more results are
at https://ut-austin-rpl.github.io/olaf/
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17556" title="Abstract">arXiv:2310.17556</a> [<a href="/pdf/2310.17556" title="Download PDF">pdf</a>, <a href="/format/2310.17556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Numerical Algorithm for Large-Scale Damped Natural Gradient  Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">We propose a new algorithm for efficiently solving the damped Fisher matrix
in large-scale scenarios where the number of parameters significantly exceeds
the number of available samples. This problem is fundamental for natural
gradient descent and stochastic reconfiguration. Our algorithm is based on
Cholesky decomposition and is generally applicable. Benchmark results show that
the algorithm is significantly faster than existing methods.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17558" title="Abstract">arXiv:2310.17558</a> [<a href="/pdf/2310.17558" title="Download PDF">pdf</a>, <a href="/format/2310.17558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Matching Phones and Speech Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Gene-Ping Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Learning phone types from phone instances has been a long-standing problem,
while still being open. In this work, we revisit this problem in the context of
self-supervised learning, and pose it as the problem of matching cluster
centroids to phone embeddings. We study two key properties that enable
matching, namely, whether cluster centroids of self-supervised representations
reduce the variability of phone instances and respect the relationship among
phones. We then use the matching result to produce pseudo-labels and introduce
a new loss function for improving self-supervised representations. Our
experiments show that the matching result captures the relationship among
phones. Training the new loss function jointly with the regular self-supervised
losses, such as APC and CPC, significantly improves the downstream phone
classification.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17559" title="Abstract">arXiv:2310.17559</a> [<a href="/pdf/2310.17559" title="Download PDF">pdf</a>, <a href="/format/2310.17559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instability of computer vision models is a necessary result of the task  itself
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Turnbull%2C+O">Oliver Turnbull</a>, 
<a href="/search/cs?searchtype=author&query=Cevora%2C+G">George Cevora</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Adversarial examples resulting from instability of current computer vision
models are an extremely important topic due to their potential to compromise
any application. In this paper we demonstrate that instability is inevitable
due to a) symmetries (translational invariance) of the data, b) the categorical
nature of the classification task, and c) the fundamental discrepancy of
classifying images as objects themselves. The issue is further exacerbated by
non-exhaustive labelling of the training data. Therefore we conclude that
instability is a necessary result of how the problem of computer vision is
currently formulated. While the problem cannot be eliminated, through the
analysis of the causes, we have arrived at ways how it can be partially
alleviated. These include i) increasing the resolution of images, ii) providing
contextual information for the image, iii) exhaustive labelling of training
data, and iv) preventing attackers from frequent access to the computer vision
system.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17561" title="Abstract">arXiv:2310.17561</a> [<a href="/pdf/2310.17561" title="Download PDF">pdf</a>, <a href="/format/2310.17561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bifurcations and loss jumps in RNN training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eisenmann%2C+L">Lukas Eisenmann</a>, 
<a href="/search/cs?searchtype=author&query=Monfared%2C+Z">Zahra Monfared</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B6ring%2C+N+A">Niclas Alexander G&#xf6;ring</a>, 
<a href="/search/cs?searchtype=author&query=Durstewitz%2C+D">Daniel Durstewitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Dynamical Systems (math.DS)

</div>
<p class="mathjax">Recurrent neural networks (RNNs) are popular machine learning tools for
modeling and forecasting sequential data and for inferring dynamical systems
(DS) from observed time series. Concepts from DS theory (DST) have variously
been used to further our understanding of both, how trained RNNs solve complex
tasks, and the training process itself. Bifurcations are particularly important
phenomena in DS, including RNNs, that refer to topological (qualitative)
changes in a system's dynamical behavior as one or more of its parameters are
varied. Knowing the bifurcation structure of an RNN will thus allow to deduce
many of its computational and dynamical properties, like its sensitivity to
parameter variations or its behavior during training. In particular,
bifurcations may account for sudden loss jumps observed in RNN training that
could severely impede the training process. Here we first mathematically prove
for a particular class of ReLU-based RNNs that certain bifurcations are indeed
associated with loss gradients tending toward infinity or zero. We then
introduce a novel heuristic algorithm for detecting all fixed points and
k-cycles in ReLU-based RNNs and their existence and stability regions, hence
bifurcation manifolds in parameter space. In contrast to previous numerical
algorithms for finding fixed points and common continuation methods, our
algorithm provides exact results and returns fixed points and cycles up to high
orders with surprisingly good scaling behavior. We exemplify the algorithm on
the analysis of the training process of RNNs, and find that the recently
introduced technique of generalized teacher forcing completely avoids certain
types of bifurcations in training. Thus, besides facilitating the DST analysis
of trained RNNs, our algorithm provides a powerful instrument for analyzing the
training process itself.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17565" title="Abstract">arXiv:2310.17565</a> [<a href="/pdf/2310.17565" title="Download PDF">pdf</a>, <a href="/format/2310.17565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fabric-based Pneumatic Actuator for the Infant Elbow: Design and  Comparative Kinematic Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahin%2C+I">Ipsita Sahin</a>, 
<a href="/search/cs?searchtype=author&query=Ayazi%2C+M">Mehrnoosh Ayazi</a>, 
<a href="/search/cs?searchtype=author&query=Mucchiani%2C+C">Caio Mucchiani</a>, 
<a href="/search/cs?searchtype=author&query=Dube%2C+J">Jared Dube</a>, 
<a href="/search/cs?searchtype=author&query=Karydis%2C+K">Konstantinos Karydis</a>, 
<a href="/search/cs?searchtype=author&query=Kokkoni%2C+E">Elena Kokkoni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper focuses on the design and systematic evaluation of fabric-based,
bellow-type soft pneumatic actuators to assist with flexion and extension of
the elbow, intended for use in infant wearable devices. Initially, the
performance of a range of actuator variants was explored via simulation. The
actuator variants were parameterized based on the shape, number, and size of
the cells present. Subsequently, viable actuator variants identified from the
simulations were fabricated and underwent further testing on a physical model
based on an infant's body anthropometrics. The performance of these variants
was evaluated based on kinematic analyses using metrics including movement
smoothness, path length, and elbow joint angle. Internal pressure of the
actuators was also attained. Taken together, results reported herein provide
valuable insights about the suitability of several actuator designs to serve as
components for pediatric wearable assistive devices.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17567" title="Abstract">arXiv:2310.17567</a> [<a href="/pdf/2310.17567" title="Download PDF">pdf</a>, <a href="/format/2310.17567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dingli Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kaur%2C+S">Simran Kaur</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Arushi Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Brown-Cohen%2C+J">Jonah Brown-Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+A">Anirudh Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Sanjeev Arora</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">With LLMs shifting their role from statistical modeling of language to
serving as general-purpose AI agents, how should LLM evaluations change?
Arguably, a key ability of an AI agent is to flexibly combine, as needed, the
basic skills it has learned. The capability to combine skills plays an
important role in (human) pedagogy and also in a paper on emergence phenomena
(Arora &amp; Goyal, 2023).
<br />This work introduces Skill-Mix, a new evaluation to measure ability to
combine skills. Using a list of $N$ skills the evaluator repeatedly picks
random subsets of $k$ skills and asks the LLM to produce text combining that
subset of skills. Since the number of subsets grows like $N^k$, for even modest
$k$ this evaluation will, with high probability, require the LLM to produce
text significantly different from any text in the training set. The paper
develops a methodology for (a) designing and administering such an evaluation,
and (b) automatic grading (plus spot-checking by humans) of the results using
GPT-4 as well as the open LLaMA-2 70B model.
<br />Administering a version of to popular chatbots gave results that, while
generally in line with prior expectations, contained surprises. Sizeable
differences exist among model capabilities that are not captured by their
ranking on popular LLM leaderboards ("cramming for the leaderboard").
Furthermore, simple probability calculations indicate that GPT-4's reasonable
performance on $k=5$ is suggestive of going beyond "stochastic parrot" behavior
(Bender et al., 2021), i.e., it combines skills in ways that it had not seen
during training.
<br />We sketch how the methodology can lead to a Skill-Mix based eco-system of
open evaluations for AI capabilities of future models.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17568" title="Abstract">arXiv:2310.17568</a> [<a href="/pdf/2310.17568" title="Download PDF">pdf</a>, <a href="/format/2310.17568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating to Success in Multi-Modal Human-Robot Collaboration: Analysis  and Corpus Release
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lukin%2C+S+M">Stephanie M. Lukin</a>, 
<a href="/search/cs?searchtype=author&query=Pollard%2C+K+A">Kimberly A. Pollard</a>, 
<a href="/search/cs?searchtype=author&query=Bonial%2C+C">Claire Bonial</a>, 
<a href="/search/cs?searchtype=author&query=Hudson%2C+T">Taylor Hudson</a>, 
<a href="/search/cs?searchtype=author&query=Arstein%2C+R">Ron Arstein</a>, 
<a href="/search/cs?searchtype=author&query=Voss%2C+C">Clare Voss</a>, 
<a href="/search/cs?searchtype=author&query=Traum%2C+D">David Traum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2023 IEEE Robot and Human Interactive
  Communication Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL); Robotics (cs.RO)

</div>
<p class="mathjax">Human-guided robotic exploration is a useful approach to gathering
information at remote locations, especially those that might be too risky,
inhospitable, or inaccessible for humans. Maintaining common ground between the
remotely-located partners is a challenge, one that can be facilitated by
multi-modal communication. In this paper, we explore how participants utilized
multiple modalities to investigate a remote location with the help of a robotic
partner. Participants issued spoken natural language instructions and received
from the robot: text-based feedback, continuous 2D LIDAR mapping, and
upon-request static photographs. We noticed that different strategies were
adopted in terms of use of the modalities, and hypothesize that these
differences may be correlated with success at several exploration sub-tasks. We
found that requesting photos may have improved the identification and counting
of some key entities (doorways in particular) and that this strategy did not
hinder the amount of overall area exploration. Future work with larger samples
may reveal the effects of more nuanced photo and dialogue strategies, which can
inform the training of robotic agents. Additionally, we announce the release of
our unique multi-modal corpus of human-robot communication in an exploration
context: SCOUT, the Situated Corpus on Understanding Transactions.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17569" title="Abstract">arXiv:2310.17569</a> [<a href="/pdf/2310.17569" title="Download PDF">pdf</a>, <a href="/format/2310.17569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SD4Match: Learning to Prompt Stable Diffusion Model for Semantic  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jingyi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Prisacariu%2C+V">Victor Prisacariu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we address the challenge of matching semantically similar
keypoints across image pairs. Existing research indicates that the intermediate
output of the UNet within the Stable Diffusion (SD) can serve as robust image
feature maps for such a matching task. We demonstrate that by employing a basic
prompt tuning technique, the inherent potential of Stable Diffusion can be
harnessed, resulting in a significant enhancement in accuracy over previous
approaches. We further introduce a novel conditional prompting module that
conditions the prompt on the local details of the input image pairs, leading to
a further improvement in performance. We designate our approach as SD4Match,
short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of
SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets
new benchmarks in accuracy across all these datasets. Particularly, SD4Match
outperforms the previous state-of-the-art by a margin of 12 percentage points
on the challenging SPair-71k dataset.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17570" title="Abstract">arXiv:2310.17570</a> [<a href="/pdf/2310.17570" title="Download PDF">pdf</a>, <a href="/format/2310.17570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct  Speech-to-Speech Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yongxin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhujin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zhongyi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Linli Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in EMNLP2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While Diffusion Generative Models have achieved great success on image
generation tasks, how to efficiently and effectively incorporate them into
speech generation especially translation tasks remains a non-trivial problem.
Specifically, due to the low information density of speech data, the
transformed discrete speech unit sequence is much longer than the corresponding
text transcription, posing significant challenges to existing auto-regressive
models. Furthermore, it is not optimal to brutally apply discrete diffusion on
the speech unit sequence while disregarding the continuous space structure,
which will degrade the generation performance significantly. In this paper, we
propose a novel diffusion model by applying the diffusion forward process in
the \textit{continuous} speech representation space, while employing the
diffusion backward process in the \textit{discrete} speech unit space. In this
way, we preserve the semantic structure of the continuous speech representation
space in the diffusion process and integrate the continuous and discrete
diffusion models. We conduct extensive experiments on the textless direct
speech-to-speech translation task, where the proposed method achieves
comparable results to the computationally intensive auto-regressive baselines
(500 steps on average) with significantly fewer decoding steps (50 steps).
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17576" title="Abstract">arXiv:2310.17576</a> [<a href="/pdf/2310.17576" title="Download PDF">pdf</a>, <a href="/format/2310.17576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 1D-Touch: NLP-Assisted Coarse Text Selection via a Semi-Direct Gesture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Peiling Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Li Feng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fuling Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+P">Parakrant Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haijun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Can Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Existing text selection techniques on touchscreen focus on improving the
control for moving the carets. Coarse-grained text selection on word and phrase
levels has not received much support beyond word-snapping and entity
recognition. We introduce 1D-Touch, a novel text selection method that
complements the carets-based sub-word selection by facilitating the selection
of semantic units of words and above. This method employs a simple vertical
slide gesture to expand and contract a selection area from a word. The
expansion can be by words or by semantic chunks ranging from sub-phrases to
sentences. This technique shifts the concept of text selection, from defining a
range by locating the first and last words, towards a dynamic process of
expanding and contracting a textual semantic entity. To understand the effects
of our approach, we prototyped and tested two variants: WordTouch, which offers
a straightforward word-by-word expansion, and ChunkTouch, which leverages NLP
to chunk text into syntactic units, allowing the selection to grow by
semantically meaningful units in response to the sliding gesture. Our
evaluation, focused on the coarse-grained selection tasks handled by 1D-Touch,
shows a 20% improvement over the default word-snapping selection method on
Android.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17577" title="Abstract">arXiv:2310.17577</a> [<a href="/pdf/2310.17577" title="Download PDF">pdf</a>, <a href="/format/2310.17577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Structure-Aware Diffusion Process for Low-Light Image Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jinhui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhiyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Junhui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H">Huanqiang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hui Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper studies a diffusion-based framework to address the low-light image
enhancement problem. To harness the capabilities of diffusion models, we delve
into this intricate process and advocate for the regularization of its inherent
ODE-trajectory. To be specific, inspired by the recent research that low
curvature ODE-trajectory results in a stable and effective diffusion process,
we formulate a curvature regularization term anchored in the intrinsic
non-local structures of image data, i.e., global structure-aware
regularization, which gradually facilitates the preservation of complicated
details and the augmentation of contrast during the diffusion process. This
incorporation mitigates the adverse effects of noise and artifacts resulting
from the diffusion process, leading to a more precise and flexible enhancement.
To additionally promote learning in challenging regions, we introduce an
uncertainty-guided regularization technique, which wisely relaxes constraints
on the most extreme regions of the image. Experimental evaluations reveal that
the proposed diffusion-based framework, complemented by rank-informed
regularization, attains distinguished performance in low-light enhancement. The
outcomes indicate substantial advancements in image quality, noise suppression,
and contrast amplification in comparison with state-of-the-art methods. We
believe this innovative approach will stimulate further exploration and
advancement in low-light image processing, with potential implications for
other applications of diffusion models. The code is publicly available at
https://github.com/jinnh/GSAD.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17579" title="Abstract">arXiv:2310.17579</a> [<a href="/pdf/2310.17579" title="Download PDF">pdf</a>, <a href="/format/2310.17579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BLIS-Net: Classifying and Analyzing Signals on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Charles Xu</a>, 
<a href="/search/cs?searchtype=author&query=Goldman%2C+L">Laney Goldman</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+V">Valentina Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hollander-Bodie%2C+B">Benjamin Hollander-Bodie</a>, 
<a href="/search/cs?searchtype=author&query=Trank-Greene%2C+M">Maedee Trank-Greene</a>, 
<a href="/search/cs?searchtype=author&query=Adelstein%2C+I">Ian Adelstein</a>, 
<a href="/search/cs?searchtype=author&query=De+Brouwer%2C+E">Edward De Brouwer</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+R">Rex Ying</a>, 
<a href="/search/cs?searchtype=author&query=Krishnaswamy%2C+S">Smita Krishnaswamy</a>, 
<a href="/search/cs?searchtype=author&query=Perlmutter%2C+M">Michael Perlmutter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Graph neural networks (GNNs) have emerged as a powerful tool for tasks such
as node classification and graph classification. However, much less work has
been done on signal classification, where the data consists of many functions
(referred to as signals) defined on the vertices of a single graph. These tasks
require networks designed differently from those designed for traditional GNN
tasks. Indeed, traditional GNNs rely on localized low-pass filters, and signals
of interest may have intricate multi-frequency behavior and exhibit long range
interactions. This motivates us to introduce the BLIS-Net (Bi-Lipschitz
Scattering Net), a novel GNN that builds on the previously introduced geometric
scattering transform. Our network is able to capture both local and global
signal structure and is able to capture both low-frequency and high-frequency
information. We make several crucial changes to the original geometric
scattering architecture which we prove increase the ability of our network to
capture information about the input signal and show that BLIS-Net achieves
superior performance on both synthetic and real-world data sets based on
traffic flow and fMRI data.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17586" title="Abstract">arXiv:2310.17586</a> [<a href="/pdf/2310.17586" title="Download PDF">pdf</a>, <a href="/format/2310.17586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Voices, Local Biases: Socio-Cultural Prejudices across Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Anjishnu Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+C">Chahat Raj</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Ziwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Anastasopoulos%2C+A">Antonios Anastasopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Human biases are ubiquitous but not uniform: disparities exist across
linguistic, cultural, and societal borders. As large amounts of recent
literature suggest, language models (LMs) trained on human data can reflect and
often amplify the effects of these social biases. However, the vast majority of
existing studies on bias are heavily skewed towards Western and European
languages. In this work, we scale the Word Embedding Association Test (WEAT) to
24 languages, enabling broader studies and yielding interesting findings about
LM bias. We additionally enhance this data with culturally relevant information
for each language, capturing local contexts on a global scale. Further, to
encompass more widely prevalent societal biases, we examine new bias dimensions
across toxicity, ableism, and more. Moreover, we delve deeper into the Indian
linguistic landscape, conducting a comprehensive regional bias analysis across
six prevalent Indian languages. Finally, we highlight the significance of these
social biases and the new dimensions through an extensive comparison of
embedding methods, reinforcing the need to address them in pursuit of more
equitable language models. All code, data and results are available here:
https://github.com/iamshnoo/weathub.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17588" title="Abstract">arXiv:2310.17588</a> [<a href="/pdf/2310.17588" title="Download PDF">pdf</a>, <a href="/format/2310.17588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven  Perturbed Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guangliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhiyu Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xitong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+K+M">Kristen Marie Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rongrong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP23 main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Fine-tuning pretrained language models (PLMs) for downstream tasks is a
large-scale optimization problem, in which the choice of the training algorithm
critically determines how well the trained model can generalize to unseen test
data, especially in the context of few-shot learning. To achieve good
generalization performance and avoid overfitting, techniques such as data
augmentation and pruning are often applied. However, adding these
regularizations necessitates heavy tuning of the hyperparameters of
optimization algorithms, such as the popular Adam optimizer. In this paper, we
propose a two-stage fine-tuning method, PAC-tuning, to address this
optimization challenge. First, based on PAC-Bayes training, PAC-tuning directly
minimizes the PAC-Bayes generalization bound to learn proper parameter
distribution. Second, PAC-tuning modifies the gradient by injecting noise with
the variance learned in the first stage into the model parameters during
training, resulting in a variant of perturbed gradient descent (PGD). In the
past, the few-shot scenario posed difficulties for PAC-Bayes training because
the PAC-Bayes bound, when applied to large models with limited training data,
might not be stringent. Our experimental results across 5 GLUE benchmark tasks
demonstrate that PAC-tuning successfully handles the challenges of fine-tuning
tasks and outperforms strong baseline methods by a visible margin, further
confirming the potential to apply PAC training for any other settings where the
Adam optimizer is currently used for training.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17589" title="Abstract">arXiv:2310.17589</a> [<a href="/pdf/2310.17589" title="Download PDF">pdf</a>, <a href="/format/2310.17589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Open Source Data Contamination Report for Llama Series Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yucheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Data contamination in language model evaluation is increasingly prevalent as
the popularity of large language models. It allows models to "cheat" via
memorisation instead of displaying true capabilities. Therefore, contamination
analysis has became an crucial part of reliable model evaluation to validate
results. However, existing contamination analysis is usually conducted
internally by LLM developers and often lacks transparency and completeness.
This paper present an open source data contamination reports for the Llama
series models. We analyse six popular multi-choice QA benchmarks and quantify
their overlapping with the training set of Llama. Various levels of
contamination ranging from 1\% to 8.7\% are found across benchmarks. Our
comparison also reveals that Llama models can gain over 5\% higher accuracy on
contaminated subsets versus clean subsets. Data and code are available at:
https://github.com/liyucheng09/Contamination_Detector.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17590" title="Abstract">arXiv:2310.17590</a> [<a href="/pdf/2310.17590" title="Download PDF">pdf</a>, <a href="/format/2310.17590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noise-Free Score Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katzir%2C+O">Oren Katzir</a>, 
<a href="/search/cs?searchtype=author&query=Patashnik%2C+O">Or Patashnik</a>, 
<a href="/search/cs?searchtype=author&query=Cohen-Or%2C+D">Daniel Cohen-Or</a>, 
<a href="/search/cs?searchtype=author&query=Lischinski%2C+D">Dani Lischinski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page at <a href="https://orenkatzir.github.io/nfsd/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Score Distillation Sampling (SDS) has emerged as the de facto approach for
text-to-content generation in non-image domains. In this paper, we reexamine
the SDS process and introduce a straightforward interpretation that demystifies
the necessity for large Classifier-Free Guidance (CFG) scales, rooted in the
distillation of an undesired noise term. Building upon our interpretation, we
propose a novel Noise-Free Score Distillation (NFSD) process, which requires
minimal modifications to the original SDS framework. Through this streamlined
design, we achieve more effective distillation of pre-trained text-to-image
diffusion models while using a nominal CFG scale. This strategic choice allows
us to prevent the over-smoothing of results, ensuring that the generated data
is both realistic and complies with the desired prompt. To demonstrate the
efficacy of NFSD, we provide qualitative examples that compare NFSD and SDS, as
well as several other methods.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17591" title="Abstract">arXiv:2310.17591</a> [<a href="/pdf/2310.17591" title="Download PDF">pdf</a>, <a href="/format/2310.17591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lil-Bevo: Explorations of Strategies for Training Language Models in  More Humanlike Ways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Govindarajan%2C+V+S">Venkata S Govindarajan</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+J+D">Juan Diego Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Bostrom%2C+K">Kaj Bostrom</a>, 
<a href="/search/cs?searchtype=author&query=Mahowald%2C+K">Kyle Mahowald</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the BabyLM Challenge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present Lil-Bevo, our submission to the BabyLM Challenge. We pretrained
our masked language models with three ingredients: an initial pretraining with
music data, training on shorter sequences before training on longer ones, and
masking specific tokens to target some of the BLiMP subtasks. Overall, our
baseline models performed above chance, but far below the performance levels of
larger LLMs trained on more data. We found that training on short sequences
performed better than training on longer sequences.Pretraining on music may
help performance marginally, but, if so, the effect seems small. Our targeted
Masked Language Modeling augmentation did not seem to improve model performance
in general, but did seem to help on some of the specific BLiMP tasks that we
were targeting (e.g., Negative Polarity Items). Training performant LLMs on
small amounts of data is a difficult but potentially informative task. While
some of our techniques showed some promise, more work is needed to explore
whether they can improve performance more than the modest gains here. Our code
is available at https://github.com/venkatasg/Lil-Bevo and out models at
https://huggingface.co/collections/venkatasg/babylm-653591cdb66f4bf68922873a
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17594" title="Abstract">arXiv:2310.17594</a> [<a href="/pdf/2310.17594" title="Download PDF">pdf</a>, <a href="/format/2310.17594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPA: A Graph Spectral Alignment Perspective for Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhiqing Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haobo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Ying Jin</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Gang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junbo Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Unsupervised domain adaptation (UDA) is a pivotal form in machine learning to
extend the in-domain model to the distinctive target domains where the data
distributions differ. Most prior works focus on capturing the inter-domain
transferability but largely overlook rich intra-domain structures, which
empirically results in even worse discriminability. In this work, we introduce
a novel graph SPectral Alignment (SPA) framework to tackle the tradeoff. The
core of our method is briefly condensed as follows: (i)-by casting the DA
problem to graph primitives, SPA composes a coarse graph alignment mechanism
with a novel spectral regularizer towards aligning the domain graphs in
eigenspaces; (ii)-we further develop a fine-grained message propagation module
-- upon a novel neighbor-aware self-training mechanism -- in order for enhanced
discriminability in the target domain. On standardized benchmarks, the
extensive experiments of SPA demonstrate that its performance has surpassed the
existing cutting-edge DA methods. Coupled with dense model analysis, we
conclude that our approach indeed possesses superior efficacy, robustness,
discriminability, and transferability. Code and data are available at:
https://github.com/CrownX/SPA.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17596" title="Abstract">arXiv:2310.17596</a> [<a href="/pdf/2310.17596" title="Download PDF">pdf</a>, <a href="/format/2310.17596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MimicGen: A Data Generation System for Scalable Robot Learning using  Human Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandlekar%2C+A">Ajay Mandlekar</a>, 
<a href="/search/cs?searchtype=author&query=Nasiriany%2C+S">Soroush Nasiriany</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+B">Bowen Wen</a>, 
<a href="/search/cs?searchtype=author&query=Akinola%2C+I">Iretiayo Akinola</a>, 
<a href="/search/cs?searchtype=author&query=Narang%2C+Y">Yashraj Narang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Linxi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+D">Dieter Fox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Robot Learning (CoRL) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Imitation learning from a large set of human demonstrations has proved to be
an effective paradigm for building capable robot agents. However, the
demonstrations can be extremely costly and time-consuming to collect. We
introduce MimicGen, a system for automatically synthesizing large-scale, rich
datasets from only a small number of human demonstrations by adapting them to
new contexts. We use MimicGen to generate over 50K demonstrations across 18
tasks with diverse scene configurations, object instances, and robot arms from
just ~200 human demonstrations. We show that robot agents can be effectively
trained on this generated dataset by imitation learning to achieve strong
performance in long-horizon and high-precision tasks, such as multi-part
assembly and coffee preparation, across broad initial state distributions. We
further demonstrate that the effectiveness and utility of MimicGen data compare
favorably to collecting additional human demonstrations, making it a powerful
and economical approach towards scaling up robot learning. Datasets, simulation
environments, videos, and more at https://mimicgen.github.io .
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17599" title="Abstract">arXiv:2310.17599</a> [<a href="/pdf/2310.17599" title="Download PDF">pdf</a>, <a href="/ps/2310.17599" title="Download PostScript">ps</a>, <a href="/format/2310.17599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-dependent electromagnetic scattering from dispersive materials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nick%2C+J">J&#xf6;rg Nick</a>, 
<a href="/search/math?searchtype=author&query=Burkhard%2C+S">Selina Burkhard</a>, 
<a href="/search/math?searchtype=author&query=Lubich%2C+C">Christian Lubich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2103.08930">arXiv:2103.08930</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper studies time-dependent electromagnetic scattering from
metamaterials that are described by dispersive material laws. We consider the
numerical treatment of a scattering problem in which a dispersive material law,
for a causal and passive homogeneous material, determines the wave-material
interaction in the scatterer. The resulting problem is nonlocal in time inside
the scatterer and is posed on an unbounded domain. Well-posedness of the
scattering problem is shown using a formulation that is fully given on the
surface of the scatterer via a time-dependent boundary integral equation.
Discretizing this equation by convolution quadrature in time and boundary
elements in space yields a provably stable and convergent method that is fully
parallel in time and space. Under regularity assumptions on the exact solution
we derive error bounds with explicit convergence rates in time and space.
Numerical experiments illustrate the theoretical results and show the
effectiveness of the method.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17603" title="Abstract">arXiv:2310.17603</a> [<a href="/pdf/2310.17603" title="Download PDF">pdf</a>, <a href="/format/2310.17603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient frequency-independent numerical method for computing the  far-field pattern induced by polygonal obstacles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gibbs%2C+A">A. Gibbs</a>, 
<a href="/search/math?searchtype=author&query=Langdon%2C+S">S. Langdon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">For problems of time-harmonic scattering by rational polygonal obstacles,
embedding formulae express the far-field pattern induced by any incident plane
wave in terms of the far-field patterns for a relatively small
(frequency-independent) set of canonical incident angles. Although these
remarkable formulae are exact in theory, here we demonstrate that: (i) they are
highly sensitive to numerical errors in practice, and; (ii) direct calculation
of the coefficients in these formulae may be impossible for particular sets of
canonical incident angles, even in exact arithmetic. Only by overcoming these
practical issues can embedding formulae provide a highly efficient approach to
computing the far-field pattern induced by a large number of incident angles.
<br />Here we propose solutions for problems (i) and (ii), backed up by theory and
numerical experiments. Problem (i) is solved using techniques from
computational complex analysis: we reformulate the embedding formula as a
complex contour integral and prove that this is much less sensitive to
numerical errors. In practice, this contour integral can be efficiently
evaluated by residue calculus. Problem (ii) is addressed using techniques from
numerical linear algebra: we oversample, considering more canonical incident
angles than are necessary, thus expanding the space of valid coefficients
vectors. The coefficients vectors can then be selected using either a least
squares approach or column subset selection.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17606" title="Abstract">arXiv:2310.17606</a> [<a href="/pdf/2310.17606" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using State-of-the-Art Speech Models to Evaluate Oral Reading Fluency in  Ghana
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henkel%2C+O">Owen Henkel</a>, 
<a href="/search/cs?searchtype=author&query=Horne-Robinson%2C+H">Hannah Horne-Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Hills%2C+L">Libby Hills</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+B">Bill Roberts</a>, 
<a href="/search/cs?searchtype=author&query=McGrane%2C+J">Joshua McGrane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper reports on a set of three recent experiments utilizing large-scale
speech models to evaluate the oral reading fluency (ORF) of students in Ghana.
While ORF is a well-established measure of foundational literacy, assessing it
typically requires one-on-one sessions between a student and a trained
evaluator, a process that is time-consuming and costly. Automating the
evaluation of ORF could support better literacy instruction, particularly in
education contexts where formative assessment is uncommon due to large class
sizes and limited resources. To our knowledge, this research is among the first
to examine the use of the most recent versions of large-scale speech models
(Whisper V2 wav2vec2.0) for ORF assessment in the Global South.
<br />We find that Whisper V2 produces transcriptions of Ghanaian students reading
aloud with a Word Error Rate of 13.5. This is close to the model's average WER
on adult speech (12.8) and would have been considered state-of-the-art for
children's speech transcription only a few years ago. We also find that when
these transcriptions are used to produce fully automated ORF scores, they
closely align with scores generated by expert human graders, with a correlation
coefficient of 0.96. Importantly, these results were achieved on a
representative dataset (i.e., students with regional accents, recordings taken
in actual classrooms), using a free and publicly available speech model out of
the box (i.e., no fine-tuning). This suggests that using large-scale speech
models to assess ORF may be feasible to implement and scale in lower-resource,
linguistically diverse educational contexts.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17609" title="Abstract">arXiv:2310.17609</a> [<a href="/pdf/2310.17609" title="Download PDF">pdf</a>, <a href="/format/2310.17609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haitao Li</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yunqiu Shao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yueyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+Q">Qingyao Ai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yixiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">As an important component of intelligent legal systems, legal case retrieval
plays a critical role in ensuring judicial justice and fairness. However, the
development of legal case retrieval technologies in the Chinese legal system is
restricted by three problems in existing datasets: limited data size, narrow
definitions of legal relevance, and naive candidate pooling strategies used in
data sampling. To alleviate these issues, we introduce LeCaRDv2, a large-scale
Legal Case Retrieval Dataset (version 2). It consists of 800 queries and 55,192
candidates extracted from 4.3 million criminal case documents. To the best of
our knowledge, LeCaRDv2 is one of the largest Chinese legal case retrieval
datasets, providing extensive coverage of criminal charges. Additionally, we
enrich the existing relevance criteria by considering three key aspects:
characterization, penalty, procedure. This comprehensive criteria enriches the
dataset and may provides a more holistic perspective. Furthermore, we propose a
two-level candidate set pooling strategy that effectively identify potential
candidates for each query case. It's important to note that all cases in the
dataset have been annotated by multiple legal experts specializing in criminal
law. Their expertise ensures the accuracy and reliability of the annotations.
We evaluate several state-of-the-art retrieval models at LeCaRDv2,
demonstrating that there is still significant room for improvement in legal
case retrieval. The details of LeCaRDv2 can be found at the anonymous website
https://github.com/anonymous1113243/LeCaRDv2.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17611" title="Abstract">arXiv:2310.17611</a> [<a href="/pdf/2310.17611" title="Download PDF">pdf</a>, <a href="/format/2310.17611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering Meanings of Embeddings via Partial Orthogonality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yibo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Aragam%2C+B">Bryon Aragam</a>, 
<a href="/search/cs?searchtype=author&query=Veitch%2C+V">Victor Veitch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">Machine learning tools often rely on embedding text as vectors of real
numbers. In this paper, we study how the semantic structure of language is
encoded in the algebraic structure of such embeddings. Specifically, we look at
a notion of ``semantic independence'' capturing the idea that, e.g.,
``eggplant'' and ``tomato'' are independent given ``vegetable''. Although such
examples are intuitive, it is difficult to formalize such a notion of semantic
independence. The key observation here is that any sensible formalization
should obey a set of so-called independence axioms, and thus any algebraic
encoding of this structure should also obey these axioms. This leads us
naturally to use partial orthogonality as the relevant algebraic structure. We
develop theory and methods that allow us to demonstrate that partial
orthogonality does indeed capture semantic independence. Complementary to this,
we also introduce the concept of independence preserving embeddings where
embeddings preserve the conditional independence structures of a distribution,
and we prove the existence of such embeddings and approximations to them.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17616" title="Abstract">arXiv:2310.17616</a> [<a href="/pdf/2310.17616" title="Download PDF">pdf</a>, <a href="/format/2310.17616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verifying Programs with Logic and Extended Proof Rules: Deep Embedding  v.s. Shallow Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qinxiang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yichen Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Many foundational program verification tools have been developed to build
machine-checked program correctness proofs, a majority of which are based on
Hoare logic. Their program logics, their assertion languages, and their
underlying programming languages can be formalized by either a shallow
embedding or a deep embedding. Tools like Iris and early versions of Verified
Software Toolchain (VST) choose different shallow embeddings to formalize their
program logics. But the pros and cons of these different embeddings were not
yet well studied. Therefore, we want to study the impact of the program logic's
embedding on logic's proof rules in this paper. This paper considers a set of
useful extended proof rules, and four different logic embeddings: one deep
embedding and three common shallow embeddings. We prove the validity of these
extended rules under these embeddings and discuss their main challenges.
Furthermore, we propose a method to lift existing shallowly embedded logics to
deeply embedded ones to greatly simplify proofs of extended rules in specific
proof systems. We evaluate our results on two existing verification tools. We
lift the originally shallowly embedded VST to our deeply embedded VST to
support extended rules, and we implement Iris-CF and deeply embedded Iris-Imp
based on the Iris framework to evaluate our theory in real verification
projects.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17620" title="Abstract">arXiv:2310.17620</a> [<a href="/pdf/2310.17620" title="Download PDF">pdf</a>, <a href="/format/2310.17620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Radar-Only Off-Road Local Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Overbye%2C+T">Timothy Overbye</a>, 
<a href="/search/cs?searchtype=author&query=Saripalli%2C+S">Srikanth Saripalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 17 figures, ITSC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Off-road robotics have traditionally utilized lidar for local navigation due
to its accuracy and high resolution. However, the limitations of lidar, such as
reduced performance in harsh environmental conditions and limited range, have
prompted the exploration of alternative sensing technologies. This paper
investigates the potential of radar for off-road local navigation, as it offers
the advantages of a longer range and the ability to penetrate dust and light
vegetation. We adapt existing lidar-based methods for radar and evaluate the
performance in comparison to lidar under various off-road conditions. We show
that radar can provide a significant range advantage over lidar while
maintaining accuracy for both ground plane estimation and obstacle detection.
And finally, we demonstrate successful autonomous navigation at a speed of 2.5
m/s over a path length of 350 m using only radar for ground plane estimation
and obstacle detection.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17621" title="Abstract">arXiv:2310.17621</a> [<a href="/pdf/2310.17621" title="Download PDF">pdf</a>, <a href="/ps/2310.17621" title="Download PostScript">ps</a>, <a href="/format/2310.17621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A spectral element solution of the Poisson equation with shifted  boundary polynomial corrections: influence of the surrogate to true boundary  mapping and an asymptotically preserving Robin formulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Visbech%2C+J">Jens Visbech</a>, 
<a href="/search/math?searchtype=author&query=Engsig-Karup%2C+A+P">Allan Peter Engsig-Karup</a>, 
<a href="/search/math?searchtype=author&query=Ricchiuto%2C+M">Mario Ricchiuto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 table, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a new high-order accurate spectral element solution to the
two-dimensional scalar Poisson equation subject to a general Robin boundary
condition. The solution is based on a simplified version of the shifted
boundary method employing a continuous arbitrary order $hp$-Galerkin spectral
element method as the numerical discretization procedure. The simplification
relies on a polynomial correction to avoid explicitly evaluating high-order
partial derivatives from the Taylor series expansion, which traditionally have
been used within the shifted boundary method. In this setting, we apply an
extrapolation and novel interpolation approach to project the basis functions
from the true domain onto the approximate surrogate domain. The resulting
solution provides a method that naturally incorporates curved geometrical
features of the domain, overcomes complex and cumbersome mesh generation, and
avoids problems with small-cut-cells. Dirichlet, Neumann, and general Robin
boundary conditions are enforced weakly through: i) a generalized Nitsche's
method and ii) a generalized Aubin's method. For this, a consistent asymptotic
preserving formulation of the embedded Robin formulations is presented.
<br />We present several numerical experiments and analysis of the algorithmic
properties of the different weak formulations. With this, we include
convergence studies under polynomial, $p$, increase of the basis functions,
mesh, $h$, refinement, and matrix conditioning to highlight the spectral and
algebraic convergence features, respectively. This is done to assess the
influence of errors across variational formulations, polynomial order, mesh
size, and mappings between the true and surrogate boundaries.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17622" title="Abstract">arXiv:2310.17622</a> [<a href="/pdf/2310.17622" title="Download PDF">pdf</a>, <a href="/format/2310.17622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combating Representation Learning Disparity with Geometric Harmonization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhihan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiangchao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+F">Feng Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Self-supervised learning (SSL) as an effective paradigm of representation
learning has achieved tremendous success on various curated datasets in diverse
scenarios. Nevertheless, when facing the long-tailed distribution in real-world
applications, it is still hard for existing methods to capture transferable and
robust representation. Conventional SSL methods, pursuing sample-level
uniformity, easily leads to representation learning disparity where head
classes dominate the feature regime but tail classes passively collapse. To
address this problem, we propose a novel Geometric Harmonization (GH) method to
encourage category-level uniformity in representation learning, which is more
benign to the minority and almost does not hurt the majority under long-tailed
distribution. Specially, GH measures the population statistics of the embedding
space on top of self-supervised learning, and then infer an fine-grained
instance-wise calibration to constrain the space expansion of head classes and
avoid the passive collapse of tail classes. Our proposal does not alter the
setting of SSL and can be easily integrated into existing methods in a low-cost
manner. Extensive results on a range of benchmark datasets show the
effectiveness of GH with high tolerance to the distribution skewness. Our code
is available at https://github.com/MediaBrain-SJTU/Geometric-Harmonization.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17623" title="Abstract">arXiv:2310.17623</a> [<a href="/pdf/2310.17623" title="Download PDF">pdf</a>, <a href="/format/2310.17623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proving Test Set Contamination in Black Box Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oren%2C+Y">Yonatan Oren</a>, 
<a href="/search/cs?searchtype=author&query=Meister%2C+N">Nicole Meister</a>, 
<a href="/search/cs?searchtype=author&query=Chatterji%2C+N">Niladri Chatterji</a>, 
<a href="/search/cs?searchtype=author&query=Ladhak%2C+F">Faisal Ladhak</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T+B">Tatsunori B. Hashimoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models are trained on vast amounts of internet data, prompting
concerns and speculation that they have memorized public benchmarks. Going from
speculation to proof of contamination is challenging, as the pretraining data
used by proprietary models are often not publicly accessible. We show that it
is possible to provide provable guarantees of test set contamination in
language models without access to pretraining data or model weights. Our
approach leverages the fact that when there is no data contamination, all
orderings of an exchangeable benchmark should be equally likely. In contrast,
the tendency for language models to memorize example order means that a
contaminated language model will find certain canonical orderings to be much
more likely than others. Our test flags potential contamination whenever the
likelihood of a canonically ordered benchmark dataset is significantly higher
than the likelihood after shuffling the examples. We demonstrate that our
procedure is sensitive enough to reliably prove test set contamination in
challenging situations, including models as small as 1.4 billion parameters, on
small test sets of only 1000 examples, and datasets that appear only a few
times in the pretraining corpus. Using our test, we audit five popular publicly
accessible language models for test set contamination and find little evidence
for pervasive contamination.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17626" title="Abstract">arXiv:2310.17626</a> [<a href="/pdf/2310.17626" title="Download PDF">pdf</a>, <a href="/ps/2310.17626" title="Download PostScript">ps</a>, <a href="/format/2310.17626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Transferability of Adversarial Examples across Deep Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jindong Gu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaojun Jia</a>, 
<a href="/search/cs?searchtype=author&query=de+Jorge%2C+P">Pau de Jorge</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenqain Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+A">Avery Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xun%2C+Y">Yuan Xun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+A">Anjun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Khakzar%2C+A">Ashkan Khakzar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhijiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The emergence of Deep Neural Networks (DNNs) has revolutionized various
domains, enabling the resolution of complex tasks spanning image recognition,
natural language processing, and scientific problem-solving. However, this
progress has also exposed a concerning vulnerability: adversarial examples.
These crafted inputs, imperceptible to humans, can manipulate machine learning
models into making erroneous predictions, raising concerns for safety-critical
applications. An intriguing property of this phenomenon is the transferability
of adversarial examples, where perturbations crafted for one model can deceive
another, often with a different architecture. This intriguing property enables
"black-box" attacks, circumventing the need for detailed knowledge of the
target model. This survey explores the landscape of the adversarial
transferability of adversarial examples. We categorize existing methodologies
to enhance adversarial transferability and discuss the fundamental principles
guiding each approach. While the predominant body of research primarily
concentrates on image classification, we also extend our discussion to
encompass other vision tasks and beyond. Challenges and future prospects are
discussed, highlighting the importance of fortifying DNNs against adversarial
vulnerabilities in an evolving landscape.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17630" title="Abstract">arXiv:2310.17630</a> [<a href="/pdf/2310.17630" title="Download PDF">pdf</a>, <a href="/format/2310.17630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstOptima: Evolutionary Multi-objective Instruction Optimization via  Large Language Model-based Instruction Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Heng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Instruction-based language modeling has received significant attention in
pretrained language models. However, the efficiency of instruction engineering
remains low and hinders the development of instruction studies. Recent studies
have focused on automating instruction generation, but they primarily aim to
improve performance without considering other crucial objectives that impact
instruction quality, such as instruction length and perplexity. Therefore, we
propose a novel approach (i.e., InstOptima) that treats instruction generation
as an evolutionary multi-objective optimization problem. In contrast to text
edition-based methods, our approach utilizes a large language model (LLM) to
simulate instruction operators, including mutation and crossover. Furthermore,
we introduce an objective-guided mechanism for these operators, allowing the
LLM to comprehend the objectives and enhance the quality of the generated
instructions. Experimental results demonstrate improved fine-tuning performance
and the generation of a diverse set of high-quality instructions.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17631" title="Abstract">arXiv:2310.17631</a> [<a href="/pdf/2310.17631" title="Download PDF">pdf</a>, <a href="/format/2310.17631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JudgeLM: Fine-tuned Large Language Models are Scalable Judges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lianghui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinggang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinlong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 23 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Evaluating Large Language Models (LLMs) in open-ended scenarios is
challenging because existing benchmarks and metrics can not measure them
comprehensively. To address this problem, we propose to fine-tune LLMs as
scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in
open-ended benchmarks. We first propose a comprehensive, large-scale,
high-quality dataset containing task seeds, LLMs-generated answers, and
GPT-4-generated judgments for fine-tuning high-performance judges, as well as a
new benchmark for evaluating the judges. We train JudgeLM at different scales
from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its
capabilities and behaviors. We then analyze the key biases in fine-tuning LLM
as a judge and consider them as position bias, knowledge bias, and format bias.
To address these issues, JudgeLM introduces a bag of techniques including swap
augmentation, reference support, and reference drop, which clearly enhance the
judge's performance. JudgeLM obtains the state-of-the-art judge performance on
both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM
is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8
A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an
agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM
also demonstrates extended capabilities in being judges of the single answer,
multimodal models, multiple answers, and multi-turn chat.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17632" title="Abstract">arXiv:2310.17632</a> [<a href="/pdf/2310.17632" title="Download PDF">pdf</a>, <a href="/format/2310.17632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepShaRM: Multi-View Shape and Reflectance Map Recovery Under Unknown  Lighting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamashita%2C+K">Kohei Yamashita</a>, 
<a href="/search/cs?searchtype=author&query=Nobuhara%2C+S">Shohei Nobuhara</a>, 
<a href="/search/cs?searchtype=author&query=Nishino%2C+K">Ko Nishino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3DV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Geometry reconstruction of textureless, non-Lambertian objects under unknown
natural illumination (i.e., in the wild) remains challenging as correspondences
cannot be established and the reflectance cannot be expressed in simple
analytical forms. We derive a novel multi-view method, DeepShaRM, that achieves
state-of-the-art accuracy on this challenging task. Unlike past methods that
formulate this as inverse-rendering, i.e., estimation of reflectance,
illumination, and geometry from images, our key idea is to realize that
reflectance and illumination need not be disentangled and instead estimated as
a compound reflectance map. We introduce a novel deep reflectance map
estimation network that recovers the camera-view reflectance maps from the
surface normals of the current geometry estimate and the input multi-view
images. The network also explicitly estimates per-pixel confidence scores to
handle global light transport effects. A deep shape-from-shading network then
updates the geometry estimate expressed with a signed distance function using
the recovered reflectance maps. By alternating between these two, and, most
important, by bypassing the ill-posed problem of reflectance and illumination
decomposition, the method accurately recovers object geometry in these
challenging settings. Extensive experiments on both synthetic and real-world
data clearly demonstrate its state-of-the-art accuracy.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17634" title="Abstract">arXiv:2310.17634</a> [<a href="/pdf/2310.17634" title="Download PDF">pdf</a>, <a href="/format/2310.17634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grow Your Limits: Continuous Improvement with Real-World RL for Robotic  Locomotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+L">Laura Smith</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yunhao Cao</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally. Project website: <a href="https://sites.google.com/berkeley.edu/aprl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep reinforcement learning (RL) can enable robots to autonomously acquire
complex behaviors, such as legged locomotion. However, RL in the real world is
complicated by constraints on efficiency, safety, and overall training
stability, which limits its practical applicability. We present APRL, a policy
regularization framework that modulates the robot's exploration over the course
of training, striking a balance between flexible improvement potential and
focused, efficient exploration. APRL enables a quadrupedal robot to efficiently
learn to walk entirely in the real world within minutes and continue to improve
with more training where prior work saturates in performance. We demonstrate
that continued training with APRL results in a policy that is substantially
more capable of navigating challenging situations and is able to adapt to
changes in dynamics with continued training.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17637" title="Abstract">arXiv:2310.17637</a> [<a href="/pdf/2310.17637" title="Download PDF">pdf</a>, <a href="/format/2310.17637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Positivity-preserving and entropy-bounded discontinuous Galerkin method  for the chemically reacting, compressible Navier-Stokes equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ching%2C+E+J">Eric J. Ching</a>, 
<a href="/search/math?searchtype=author&query=Johnson%2C+R+F">Ryan F. Johnson</a>, 
<a href="/search/math?searchtype=author&query=Burrows%2C+S">Sarah Burrows</a>, 
<a href="/search/math?searchtype=author&query=Higgs%2C+J">Jacklyn Higgs</a>, 
<a href="/search/math?searchtype=author&query=Kercher%2C+A+D">Andrew D. Kercher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">This article concerns the development of a fully conservative,
positivity-preserving, and entropy-bounded discontinuous Galerkin scheme for
simulating the multicomponent, chemically reacting, compressible Navier-Stokes
equations with complex thermodynamics. In particular, we extend to viscous
flows the fully conservative, positivity-preserving, and entropy-bounded
discontinuous Galerkin method for the chemically reacting Euler equations that
we previously introduced. An important component of the formulation is the
positivity-preserving Lax-Friedrichs-type viscous flux function devised by
Zhang [J. Comput. Phys., 328 (2017), pp. 301-343], which was adapted to
multicomponent flows by Du and Yang [J. Comput. Phys., 469 (2022), pp. 111548]
in a manner that treats the inviscid and viscous fluxes as a single flux. Here,
we similarly extend the aforementioned flux function to multicomponent flows
but separate the inviscid and viscous fluxes. This separation of the fluxes
allows for use of other inviscid flux functions, as well as enforcement of
entropy boundedness on only the convective contribution to the evolved state,
as motivated by physical and mathematical principles. We also discuss in detail
how to account for boundary conditions and incorporate previously developed
pressure-equilibrium-preserving techniques into the positivity-preserving
framework. Comparisons between the Lax-Friedrichs-type viscous flux function
and more conventional flux functions are provided, the results of which
motivate an adaptive solution procedure that employs the former only when the
element-local solution average has negative species concentrations, nonpositive
density, or nonpositive pressure. A variety of multicomponent, viscous flows is
computed, ranging from a one-dimensional shock tube problem to multidimensional
detonation waves and shock/mixing-layer interaction.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17638" title="Abstract">arXiv:2310.17638</a> [<a href="/pdf/2310.17638" title="Download PDF">pdf</a>, <a href="/format/2310.17638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Fractional Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nobis%2C+G">Gabriel Nobis</a>, 
<a href="/search/cs?searchtype=author&query=Aversa%2C+M">Marco Aversa</a>, 
<a href="/search/cs?searchtype=author&query=Springenberg%2C+M">Maximilian Springenberg</a>, 
<a href="/search/cs?searchtype=author&query=Detzel%2C+M">Michael Detzel</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>, 
<a href="/search/cs?searchtype=author&query=Nakajima%2C+S">Shinichi Nakajima</a>, 
<a href="/search/cs?searchtype=author&query=Murray-Smith%2C+R">Roderick Murray-Smith</a>, 
<a href="/search/cs?searchtype=author&query=Lapuschkin%2C+S">Sebastian Lapuschkin</a>, 
<a href="/search/cs?searchtype=author&query=Knochenhauer%2C+C">Christoph Knochenhauer</a>, 
<a href="/search/cs?searchtype=author&query=Oala%2C+L">Luis Oala</a>, 
<a href="/search/cs?searchtype=author&query=Samek%2C+W">Wojciech Samek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We generalize the continuous time framework for score-based generative models
from an underlying Brownian motion (BM) to an approximation of fractional
Brownian motion (FBM). We derive a continuous reparameterization trick and the
reverse time model by representing FBM as a stochastic integral over a family
of Ornstein-Uhlenbeck processes to define generative fractional diffusion
models (GFDM) with driving noise converging to a non-Markovian process of
infinite quadratic variation. The Hurst index $H\in(0,1)$ of FBM enables
control of the roughness of the distribution transforming path. To the best of
our knowledge, this is the first attempt to build a generative model upon a
stochastic process with infinite quadratic variation.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17639" title="Abstract">arXiv:2310.17639</a> [<a href="/pdf/2310.17639" title="Download PDF">pdf</a>, <a href="/format/2310.17639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning Dynamics with Random Binary Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bigelow%2C+E+J">Eric J. Bigelow</a>, 
<a href="/search/cs?searchtype=author&query=Lubana%2C+E+S">Ekdeep Singh Lubana</a>, 
<a href="/search/cs?searchtype=author&query=Dick%2C+R+P">Robert P. Dick</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+H">Hidenori Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Ullman%2C+T+D">Tomer D. Ullman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) trained on huge corpora of text datasets
demonstrate complex, emergent capabilities, achieving state-of-the-art
performance on tasks they were not explicitly trained for. The precise nature
of LLM capabilities is often mysterious, and different prompts can elicit
different capabilities through in-context learning. We propose a Cognitive
Interpretability framework that enables us to analyze in-context learning
dynamics to understand latent concepts in LLMs underlying behavioral patterns.
This provides a more nuanced understanding than success-or-failure evaluation
benchmarks, but does not require observing internal activations as a
mechanistic interpretation of circuits would. Inspired by the cognitive science
of human randomness perception, we use random binary sequences as context and
study dynamics of in-context learning by manipulating properties of context
data, such as sequence length. In the latest GPT-3.5+ models, we find emergent
abilities to generate pseudo-random numbers and learn basic formal languages,
with striking in-context learning dynamics where model outputs transition
sharply from pseudo-random behaviors to deterministic repetition.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17642" title="Abstract">arXiv:2310.17642</a> [<a href="/pdf/2310.17642" title="Download PDF">pdf</a>, <a href="/format/2310.17642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Drive Anywhere: Generalizable End-to-end Autonomous Driving with  Multi-modal Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tsun-Hsuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Maalouf%2C+A">Alaa Maalouf</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+W">Wei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ban%2C+Y">Yutong Ban</a>, 
<a href="/search/cs?searchtype=author&query=Amini%2C+A">Alexander Amini</a>, 
<a href="/search/cs?searchtype=author&query=Rosman%2C+G">Guy Rosman</a>, 
<a href="/search/cs?searchtype=author&query=Karaman%2C+S">Sertac Karaman</a>, 
<a href="/search/cs?searchtype=author&query=Rus%2C+D">Daniela Rus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project webpage: <a href="https://drive-anywhere.github.io">this https URL</a> Explainer video: <a href="https://www.youtube.com/watch?v=4n-DJf8vXxo">this https URL</a>&amp;feature=youtu.be
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">As autonomous driving technology matures, end-to-end methodologies have
emerged as a leading strategy, promising seamless integration from perception
to control via deep learning. However, existing systems grapple with challenges
such as unexpected open set environments and the complexity of black-box
models. At the same time, the evolution of deep learning introduces larger,
multimodal foundational models, offering multi-modal visual and textual
understanding. In this paper, we harness these multimodal foundation models to
enhance the robustness and adaptability of autonomous driving systems, enabling
out-of-distribution, end-to-end, multimodal, and more explainable autonomy.
Specifically, we present an approach to apply end-to-end open-set (any
environment/scene) autonomous driving that is capable of providing driving
decisions from representations queryable by image and text. To do so, we
introduce a method to extract nuanced spatial (pixel/patch-aligned) features
from transformers to enable the encapsulation of both spatial and semantic
features. Our approach (i) demonstrates unparalleled results in diverse tests
while achieving significantly greater robustness in out-of-distribution
situations, and (ii) allows the incorporation of latent space simulation (via
text) for improved training (data augmentation via text) and policy debugging.
We encourage the reader to check our explainer video at
https://www.youtube.com/watch?v=4n-DJf8vXxo&amp;feature=youtu.be and to view the
code and demos on our project webpage at https://drive-anywhere.github.io/.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17643" title="Abstract">arXiv:2310.17643</a> [<a href="/pdf/2310.17643" title="Download PDF">pdf</a>, <a href="/format/2310.17643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where you go is who you are -- A study on machine learning based  semantic privacy attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wiedemann%2C+N">Nina Wiedemann</a>, 
<a href="/search/cs?searchtype=author&query=Kounadi%2C+O">Ourania Kounadi</a>, 
<a href="/search/cs?searchtype=author&query=Raubal%2C+M">Martin Raubal</a>, 
<a href="/search/cs?searchtype=author&query=Janowicz%2C+K">Krzysztof Janowicz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">Concerns about data privacy are omnipresent, given the increasing usage of
digital applications and their underlying business model that includes selling
user data. Location data is particularly sensitive since they allow us to infer
activity patterns and interests of users, e.g., by categorizing visited
locations based on nearby points of interest (POI). On top of that, machine
learning methods provide new powerful tools to interpret big data. In light of
these considerations, we raise the following question: What is the actual risk
that realistic, machine learning based privacy attacks can obtain meaningful
semantic information from raw location data, subject to inaccuracies in the
data? In response, we present a systematic analysis of two attack scenarios,
namely location categorization and user profiling. Experiments on the
Foursquare dataset and tracking data demonstrate the potential for abuse of
high-quality spatial information, leading to a significant privacy loss even
with location inaccuracy of up to 200m. With location obfuscation of more than
1 km, spatial information hardly adds any value, but a high privacy risk solely
from temporal information remains. The availability of public context data such
as POIs plays a key role in inference based on spatial information. Our
findings point out the risks of ever-growing databases of tracking data and
spatial context data, which policymakers should consider for privacy
regulations, and which could guide individuals in their personal location
protection measures.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17644" title="Abstract">arXiv:2310.17644</a> [<a href="/pdf/2310.17644" title="Download PDF">pdf</a>, <a href="/format/2310.17644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free  Deep Learning Studies: A Case Study on NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matsubara%2C+Y">Yoshitomo Matsubara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS) at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Reproducibility in scientific work has been becoming increasingly important
in research communities such as machine learning, natural language processing,
and computer vision communities due to the rapid development of the research
domains supported by recent advances in deep learning. In this work, we present
a significantly upgraded version of torchdistill, a modular-driven coding-free
deep learning framework significantly upgraded from the initial release, which
supports only image classification and object detection tasks for reproducible
knowledge distillation experiments. To demonstrate that the upgraded framework
can support more tasks with third-party libraries, we reproduce the GLUE
benchmark results of BERT models using a script based on the upgraded
torchdistill, harmonizing with various Hugging Face libraries. All the 27
fine-tuned BERT models and configurations to reproduce the results are
published at Hugging Face, and the model weights have already been widely used
in research communities. We also reimplement popular small-sized models and new
knowledge distillation methods and perform additional experiments for computer
vision tasks.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17645" title="Abstract">arXiv:2310.17645</a> [<a href="/pdf/2310.17645" title="Download PDF">pdf</a>, <a href="/format/2310.17645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Against Transfer Attacks From Public Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sitawarin%2C+C">Chawin Sitawarin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Jaewon Chang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">David Huang</a>, 
<a href="/search/cs?searchtype=author&query=Altoyan%2C+W">Wesson Altoyan</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+D">David Wagner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission. Code available at <a href="https://github.com/wagner-group/pubdef">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Adversarial attacks have been a looming and unaddressed threat in the
industry. However, through a decade-long history of the robustness evaluation
literature, we have learned that mounting a strong or optimal attack is
challenging. It requires both machine learning and domain expertise. In other
words, the white-box threat model, religiously assumed by a large majority of
the past literature, is unrealistic. In this paper, we propose a new practical
threat model where the adversary relies on transfer attacks through publicly
available surrogate models. We argue that this setting will become the most
prevalent for security-sensitive applications in the future. We evaluate the
transfer attacks in this setting and propose a specialized defense method based
on a game-theoretic perspective. The defenses are evaluated under 24 public
models and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and
ImageNet). Under this threat model, our defense, PubDef, outperforms the
state-of-the-art white-box adversarial training by a large margin with almost
no loss in the normal accuracy. For instance, on ImageNet, our defense achieves
62% accuracy under the strongest transfer attack vs only 36% of the best
adversarially trained model. Its accuracy when not under attack is only 2%
lower than that of an undefended model (78% vs 80%). We release our code at
https://github.com/wagner-group/pubdef.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17649" title="Abstract">arXiv:2310.17649</a> [<a href="/pdf/2310.17649" title="Download PDF">pdf</a>, <a href="/format/2310.17649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 6-DoF Stability Field via Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoneda%2C+T">Takuma Yoneda</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+T">Tianchong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shakhnarovich%2C+G">Gregory Shakhnarovich</a>, 
<a href="/search/cs?searchtype=author&query=Walter%2C+M+R">Matthew R. Walter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">A core capability for robot manipulation is reasoning over where and how to
stably place objects in cluttered environments. Traditionally, robots have
relied on object-specific, hand-crafted heuristics in order to perform such
reasoning, with limited generalizability beyond a small number of object
instances and object interaction patterns. Recent approaches instead learn
notions of physical interaction, namely motion prediction, but require
supervision in the form of labeled object information or come at the cost of
high sample complexity, and do not directly reason over stability or object
placement. We present 6-DoFusion, a generative model capable of generating 3D
poses of an object that produces a stable configuration of a given scene.
Underlying 6-DoFusion is a diffusion model that incrementally refines a
randomly initialized SE(3) pose to generate a sample from a learned,
context-dependent distribution over stable poses. We evaluate our model on
different object placement and stacking tasks, demonstrating its ability to
construct stable scenes that involve novel object classes as well as to improve
the accuracy of state-of-the-art 3D pose estimation methods.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17650" title="Abstract">arXiv:2310.17650</a> [<a href="/pdf/2310.17650" title="Download PDF">pdf</a>, <a href="/format/2310.17650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised  Video Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-lahham%2C+A">Anas Al-lahham</a>, 
<a href="/search/cs?searchtype=author&query=Tastan%2C+N">Nurbek Tastan</a>, 
<a href="/search/cs?searchtype=author&query=Zaheer%2C+Z">Zaigham Zaheer</a>, 
<a href="/search/cs?searchtype=author&query=Nandakumar%2C+K">Karthik Nandakumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Detection of anomalous events in videos is an important problem in
applications such as surveillance. Video anomaly detection (VAD) is
well-studied in the one-class classification (OCC) and weakly supervised (WS)
settings. However, fully unsupervised (US) video anomaly detection methods,
which learn a complete system without any annotation or human supervision, have
not been explored in depth. This is because the lack of any ground truth
annotations significantly increases the magnitude of the VAD challenge. To
address this challenge, we propose a simple-but-effective two-stage
pseudo-label generation framework that produces segment-level (normal/anomaly)
pseudo-labels, which can be further used to train a segment-level anomaly
detector in a supervised manner. The proposed coarse-to-fine pseudo-label
(C2FPL) generator employs carefully-designed hierarchical divisive clustering
and statistical hypothesis testing to identify anomalous video segments from a
set of completely unlabeled videos. The trained anomaly detector can be
directly applied on segments of an unseen test video to obtain segment-level,
and subsequently, frame-level anomaly predictions. Extensive studies on two
large-scale public-domain datasets, UCF-Crime and XD-Violence, demonstrate that
the proposed unsupervised approach achieves superior performance compared to
all existing OCC and US methods , while yielding comparable performance to the
state-of-the-art WS methods.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17651" title="Abstract">arXiv:2310.17651</a> [<a href="/pdf/2310.17651" title="Download PDF">pdf</a>, <a href="/format/2310.17651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Dimensional Prediction for Sequential Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noarov%2C+G">Georgy Noarov</a>, 
<a href="/search/cs?searchtype=author&query=Ramalingam%2C+R">Ramya Ramalingam</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+A">Aaron Roth</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Stephan Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the problem of making predictions of an adversarially chosen
high-dimensional state that are unbiased subject to an arbitrary collection of
conditioning events, with the goal of tailoring these events to downstream
decision makers. We give efficient algorithms for solving this problem, as well
as a number of applications that stem from choosing an appropriate set of
conditioning events.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17653" title="Abstract">arXiv:2310.17653</a> [<a href="/pdf/2310.17653" title="Download PDF">pdf</a>, <a href="/format/2310.17653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fantastic Gains and Where to Find Them: On the Existence and Prospect of  General Knowledge Transfer between Any Pretrained Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roth%2C+K">Karsten Roth</a>, 
<a href="/search/cs?searchtype=author&query=Thede%2C+L">Lukas Thede</a>, 
<a href="/search/cs?searchtype=author&query=Koepke%2C+A+S">Almut Sophia Koepke</a>, 
<a href="/search/cs?searchtype=author&query=Vinyals%2C+O">Oriol Vinyals</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A9naff%2C+O">Olivier H&#xe9;naff</a>, 
<a href="/search/cs?searchtype=author&query=Akata%2C+Z">Zeynep Akata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Training deep networks requires various design decisions regarding for
instance their architecture, data augmentation, or optimization. In this work,
we find these training variations to result in networks learning unique feature
sets from the data. Using public model libraries comprising thousands of models
trained on canonical datasets like ImageNet, we observe that for arbitrary
pairings of pretrained models, one model extracts significant data context
unavailable in the other -- independent of overall performance. Given any
arbitrary pairing of pretrained models and no external rankings (such as
separate test sets, e.g. due to data privacy), we investigate if it is possible
to transfer such "complementary" knowledge from one model to another without
performance degradation -- a task made particularly difficult as additional
knowledge can be contained in stronger, equiperformant or weaker models. Yet
facilitating robust transfer in scenarios agnostic to pretrained model pairings
would unlock auxiliary gains and knowledge fusion from any model repository
without restrictions on model and problem specifics - including from weaker,
lower-performance models. This work therefore provides an initial, in-depth
exploration on the viability of such general-purpose knowledge transfer. Across
large-scale experiments, we first reveal the shortcomings of standard knowledge
distillation techniques, and then propose a much more general extension through
data partitioning for successful transfer between nearly all pretrained models,
which we show can also be done unsupervised. Finally, we assess both the
scalability and impact of fundamental model properties on successful
model-agnostic knowledge transfer.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Fri, 27 Oct 23</h3>
<dl>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16851" title="Abstract">arXiv:2310.16851</a> (cross-list from eess.IV) [<a href="/pdf/2310.16851" title="Download PDF">pdf</a>, <a href="/format/2310.16851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Models for Classification of COVID-19 Cases by Medical  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ali%2C+A">Amir Ali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Master's thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In recent times, the use of chest Computed Tomography (CT) images for
detecting coronavirus infections has gained significant attention, owing to
their ability to reveal bilateral changes in affected individuals. However,
classifying patients from medical images presents a formidable challenge,
particularly in identifying such bilateral changes. To tackle this challenge,
our study harnesses the power of deep learning models for the precise
classification of infected patients. Our research involves a comparative
analysis of deep transfer learning-based classification models, including
DenseNet201, GoogleNet, and AlexNet, against carefully chosen supervised
learning models. Additionally, our work encompasses Covid-19 classification,
which involves the identification and differentiation of medical images, such
as X-rays and electrocardiograms, that exhibit telltale signs of Covid-19
infection. This comprehensive approach ensures that our models can handle a
wide range of medical image types and effectively identify characteristic
patterns indicative of Covid-19. By conducting meticulous research and
employing advanced deep learning techniques, we have made significant strides
in enhancing the accuracy and speed of Covid-19 diagnosis. Our results
demonstrate the effectiveness of these models and their potential to make
substantial contributions to the global effort to combat COVID-19.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16857" title="Abstract">arXiv:2310.16857</a> (cross-list from eess.IV) [<a href="/pdf/2310.16857" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improvement in Alzheimer&#x27;s Disease MRI Images Analysis by Convolutional  Neural Networks Via Topological Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tan%2C+P">Peiwen Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This research underscores the efficacy of Fourier topological optimization in
refining MRI imagery, thereby bolstering the classification precision of
Alzheimer's Disease through convolutional neural networks. Recognizing that MRI
scans are indispensable for neurological assessments, but frequently grapple
with issues like blurriness and contrast irregularities, the deployment of
Fourier topological optimization offered enhanced delineation of brain
structures, ameliorated noise, and superior contrast. The applied techniques
prioritized boundary enhancement, contrast and brightness adjustments, and
overall image lucidity. Employing CNN architectures VGG16, ResNet50,
InceptionV3, and Xception, the post-optimization analysis revealed a marked
elevation in performance. Conclusively, the amalgamation of Fourier topological
optimization with CNNs delineates a promising trajectory for the nuanced
classification of Alzheimer's Disease, portending a transformative impact on
its diagnostic paradigms.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16863" title="Abstract">arXiv:2310.16863</a> (cross-list from eess.IV) [<a href="/pdf/2310.16863" title="Download PDF">pdf</a>, <a href="/ps/2310.16863" title="Download PostScript">ps</a>, <a href="/format/2310.16863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based multimodal multi-lesion DLBCL treatment response prediction  from PET images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Thiery%2C+O">Oriane Thiery</a> (LS2N, LS2N - &#xe9;quipe SIMS, CFE, Nantes Univ - ECN, Nantes Univ), 
<a href="/search/eess?searchtype=author&query=Rizkallah%2C+M">Mira Rizkallah</a> (LS2N, LS2N - &#xe9;quipe SIMS, CFE, Nantes Univ - ECN, Nantes Univ), 
<a href="/search/eess?searchtype=author&query=Bailly%2C+C">Cl&#xe9;ment Bailly</a> (CFE, IT, CRCI2NA, Nantes Univ), 
<a href="/search/eess?searchtype=author&query=Bodet-Milin%2C+C">Caroline Bodet-Milin</a> (CFE, IT, CRCI2NA, Nantes Univ), 
<a href="/search/eess?searchtype=author&query=Itti%2C+E">Emmanuel Itti</a>, 
<a href="/search/eess?searchtype=author&query=Casasnovas%2C+R">Ren&#xe9;-Olivier Casasnovas</a>, 
<a href="/search/eess?searchtype=author&query=Gouill%2C+S+L">Steven Le Gouill</a> (CFE, IT, CRCI2NA, Nantes Univ), 
<a href="/search/eess?searchtype=author&query=Carlier%2C+T">Thomas Carlier</a> (CFE, IT, CRCI2NA, Nantes Univ), 
<a href="/search/eess?searchtype=author&query=Mateus%2C+D">Diana Mateus</a> (LS2N - &#xe9;quipe SIMS, LS2N, CFE, Nantes Univ - ECN, Nantes Univ)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AI4Treat, Oct 2023, Vancouver (Canada), Canada
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Diffuse Large B-cell Lymphoma (DLBCL) is a lymphatic cancer involving one or
more lymph nodes and extranodal sites. Its diagnostic and follow-up rely on
Positron Emission Tomography (PET) and Computed Tomography (CT). After
diagnosis, the number of nonresponding patients to standard front-line therapy
remains significant (30-40%). This work aims to develop a computer-aided
approach to identify high-risk patients requiring adapted treatment by
efficiently exploiting all the information available for each patient,
including both clinical and image data. We propose a method based on recent
graph neural networks that combine imaging information from multiple lesions,
and a cross-attention module to integrate different data modalities
efficiently. The model is trained and evaluated on a private prospective
multicentric dataset of 583 patients. Experimental results show that our
proposed method outperforms classical supervised methods based on either
clinical, imaging or both clinical and imaging data for the 2-year
progression-free survival (PFS) classification accuracy.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16872" title="Abstract">arXiv:2310.16872</a> (cross-list from eess.IV) [<a href="/pdf/2310.16872" title="Download PDF">pdf</a>, <a href="/format/2310.16872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SonoSAM -- Segment Anything on Ultrasound Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ravishankar%2C+H">Hariharan Ravishankar</a>, 
<a href="/search/eess?searchtype=author&query=Patil%2C+R">Rohan Patil</a>, 
<a href="/search/eess?searchtype=author&query=Melapudi%2C+V">Vikram Melapudi</a>, 
<a href="/search/eess?searchtype=author&query=Bhatia%2C+P">Parminder Bhatia</a>, 
<a href="/search/eess?searchtype=author&query=Taha%2C+K">Kass-Hout Taha</a>, 
<a href="/search/eess?searchtype=author&query=Annangi%2C+P">Pavan Annangi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we present SonoSAM - a promptable foundational model for
segmenting objects of interest on ultrasound images. Fine-tuned exclusively on
a rich, diverse set of objects from roughly 200k ultrasound image-mask pairs,
SonoSAM demonstrates state-of-the-art performance on 8 unseen ultrasound
data-sets, outperforming competing methods by a significant margin on all
metrics of interest. SonoSAM achieves average dice similarity score of more
than 90% on almost all test datasets within 2-6 clicks on an average, making it
a valuable tool for annotating ultrasound images. We also extend SonoSAM to 3-D
(2-D +t) applications and demonstrate superior performance making it a valuable
tool for generating dense annotations from ultrasound cine-loops. Further, to
increase practical utility of SonoSAM, we propose a two-step process of
fine-tuning followed by knowledge distillation to a smaller footprint model
without comprising the performance. We present detailed qualitative and
quantitative comparisons of SonoSAM with state-of-the art methods showcasing
efficacy of SonoSAM as one of the first reliable, generic foundational model
for ultrasound.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16908" title="Abstract">arXiv:2310.16908</a> (cross-list from q-bio.GN) [<a href="/pdf/2310.16908" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SequenceLab: A Comprehensive Benchmark of Computational Methods for  Comparing Genomic Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Rumpf%2C+M">Maximilian-David Rumpf</a>, 
<a href="/search/q-bio?searchtype=author&query=Alser%2C+M">Mohammed Alser</a>, 
<a href="/search/q-bio?searchtype=author&query=Gollwitzer%2C+A+E">Arvid E. Gollwitzer</a>, 
<a href="/search/q-bio?searchtype=author&query=Lindegger%2C+J">Joel Lindegger</a>, 
<a href="/search/q-bio?searchtype=author&query=Almadhoun%2C+N">Nour Almadhoun</a>, 
<a href="/search/q-bio?searchtype=author&query=Firtina%2C+C">Can Firtina</a>, 
<a href="/search/q-bio?searchtype=author&query=Mangul%2C+S">Serghei Mangul</a>, 
<a href="/search/q-bio?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Hardware Architecture (cs.AR); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Computational complexity is a key limitation of genomic analyses. Thus, over
the last 30 years, researchers have proposed numerous fast heuristic methods
that provide computational relief. Comparing genomic sequences is one of the
most fundamental computational steps in most genomic analyses. Due to its high
computational complexity, there are still new, more optimized exact and
heuristic algorithms being developed. We find that these methods are highly
sensitive to the underlying data, its quality, and various hyperparameters.
Despite their wide use, no in-depth analysis has been performed, potentially
falsely discarding genetic sequences from further analysis and unnecessarily
inflating computational costs. We provide the first analysis and benchmark of
this heterogeneity. We deliver an actionable overview of 11 most widely used
state-of-the-art methods for comparing genomic sequences and inform readers
about their pros and cons using thorough experimental evaluation and different
real datasets from all major manufacturers (i.e., Illumina, ONT, and PacBio).
SequenceLab is publicly available on: https://github.com/CMU-SAFARI/SequenceLab
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16912" title="Abstract">arXiv:2310.16912</a> (cross-list from physics.ao-ph) [<a href="/pdf/2310.16912" title="Download PDF">pdf</a>, <a href="/format/2310.16912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-based Atmospheric Density Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Briden%2C+J">Julia Briden</a>, 
<a href="/search/physics?searchtype=author&query=Siew%2C+P+M">Peng Mun Siew</a>, 
<a href="/search/physics?searchtype=author&query=Rodriguez-Fernandez%2C+V">Victor Rodriguez-Fernandez</a>, 
<a href="/search/physics?searchtype=author&query=Linares%2C+R">Richard Linares</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference: 24th Advanced Maui Optical and Space Surveillance Technologies At: Maui, Hawaii, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG); Space Physics (physics.space-ph)

</div>
<p class="mathjax">As the peak of the solar cycle approaches in 2025 and the ability of a single
geomagnetic storm to significantly alter the orbit of Resident Space Objects
(RSOs), techniques for atmospheric density forecasting are vital for space
situational awareness. While linear data-driven methods, such as dynamic mode
decomposition with control (DMDc), have been used previously for forecasting
atmospheric density, deep learning-based forecasting has the ability to capture
nonlinearities in data. By learning multiple layer weights from historical
atmospheric density data, long-term dependencies in the dataset are captured in
the mapping between the current atmospheric density state and control input to
the atmospheric density state at the next timestep. This work improves upon
previous linear propagation methods for atmospheric density forecasting, by
developing a nonlinear transformer-based architecture for atmospheric density
forecasting. Empirical NRLMSISE-00 and JB2008, as well as physics-based TIEGCM
atmospheric density models are compared for forecasting with DMDc and with the
transformer-based propagator.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16920" title="Abstract">arXiv:2310.16920</a> (cross-list from math.OC) [<a href="/pdf/2310.16920" title="Download PDF">pdf</a>, <a href="/format/2310.16920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smoothed Gradient Clipping and Error Feedback for Distributed  Optimization under Heavy-Tailed Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yu%2C+S">Shuhua Yu</a>, 
<a href="/search/math?searchtype=author&query=Jakovetic%2C+D">Dusan Jakovetic</a>, 
<a href="/search/math?searchtype=author&query=Kar%2C+S">Soummya Kar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Motivated by understanding and analysis of large-scale machine learning under
heavy-tailed gradient noise, we study distributed optimization with smoothed
gradient clipping, i.e., in which certain smoothed clipping operators are
applied to the gradients or gradient estimates computed from local clients
prior to further processing. While vanilla gradient clipping has proven
effective in mitigating the impact of heavy-tailed gradient noises in
non-distributed setups, it incurs bias that causes convergence issues in
heterogeneous distributed settings. To address the inherent bias introduced by
gradient clipping, we develop a smoothed clipping operator, and propose a
distributed gradient method equipped with an error feedback mechanism, i.e.,
the clipping operator is applied on the difference between some local gradient
estimator and local stochastic gradient. We establish that, for the first time
in the strongly convex setting with heavy-tailed gradient noises that may not
have finite moments of order greater than one, the proposed distributed
gradient method's mean square error (MSE) converges to zero at a rate
$O(1/t^\iota)$, $\iota \in (0, 0.4)$, where the exponent $\iota$ stays bounded
away from zero as a function of the problem condition number and the first
absolute moment of the noise. Numerical experiments validate our theoretical
findings.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16921" title="Abstract">arXiv:2310.16921</a> (cross-list from quant-ph) [<a href="/pdf/2310.16921" title="Download PDF">pdf</a>, <a href="/format/2310.16921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the connection between least squares, regularization, and classical  shadows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zhu%2C+Z">Zhihui Zhu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lukens%2C+J+M">Joseph M. Lukens</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kirby%2C+B+T">Brian T. Kirby</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Classical shadows (CS) offer a resource-efficient means to estimate quantum
observables, circumventing the need for exhaustive state tomography. Here, we
clarify and explore the connection between CS techniques and least squares (LS)
and regularized least squares (RLS) methods commonly used in machine learning
and data analysis. By formal identification of LS and RLS ``shadows''
completely analogous to those in CS -- namely, point estimators calculated from
the empirical frequencies of single measurements -- we show that both RLS and
CS can be viewed as regularizers for the underdetermined regime, replacing the
pseudoinverse with invertible alternatives. Through numerical simulations, we
evaluate RLS and CS from three distinct angles: the tradeoff in bias and
variance, mismatch between the expected and actual measurement distributions,
and the interplay between the number of measurements and number of shots per
measurement. Compared to CS, RLS attains lower variance at the expense of bias,
is robust to distribution mismatch, and is more sensitive to the number of
shots for a fixed number of state copies -- differences that can be understood
from the distinct approaches taken to regularization. Conceptually, our
integration of LS, RLS, and CS under a unifying ``shadow'' umbrella aids in
advancing the overall picture of CS techniques, while practically our results
highlight the tradeoffs intrinsic to these measurement approaches, illuminating
the circumstances under which either RLS or CS would be preferred, such as
unverified randomness for the former or unbiased estimation for the latter.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16945" title="Abstract">arXiv:2310.16945</a> (cross-list from stat.ML) [<a href="/pdf/2310.16945" title="Download PDF">pdf</a>, <a href="/format/2310.16945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Q-Aggregation for CATE Model Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lan%2C+H">Hui Lan</a>, 
<a href="/search/stat?searchtype=author&query=Syrgkanis%2C+V">Vasilis Syrgkanis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The main text is 10 pages, and we include the Appendix at the end (totaling 49 pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurate estimation of conditional average treatment effects (CATE) is at the
core of personalized decision making. While there is a plethora of models for
CATE estimation, model selection is a nontrivial task, due to the fundamental
problem of causal inference. Recent empirical work provides evidence in favor
of proxy loss metrics with double robust properties and in favor of model
ensembling. However, theoretical understanding is lacking. Direct application
of prior theoretical work leads to suboptimal oracle model selection rates due
to the non-convexity of the model selection problem. We provide regret rates
for the major existing CATE ensembling approaches and propose a new CATE model
ensembling approach based on Q-aggregation using the doubly robust loss. Our
main result shows that causal Q-aggregation achieves statistically optimal
oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and
$n$ samples), with the addition of higher-order estimation error terms related
to products of errors in the nuisance functions. Crucially, our regret rate
does not require that any of the candidate CATE models be close to the truth.
We validate our new method on many semi-synthetic datasets and also provide
extensions of our work to CATE model selection with instrumental variables and
unobserved confounding.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16954" title="Abstract">arXiv:2310.16954</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.16954" title="Download PDF">pdf</a>, <a href="/format/2310.16954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Performance in Colorectal Cancer Histology Decomposition using  Deep and Ensemble Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Prezja%2C+F">Fabi Prezja</a>, 
<a href="/search/q-bio?searchtype=author&query=Annala%2C+L">Leevi Annala</a>, 
<a href="/search/q-bio?searchtype=author&query=Kiiskinen%2C+S">Sampsa Kiiskinen</a>, 
<a href="/search/q-bio?searchtype=author&query=Lahtinen%2C+S">Suvi Lahtinen</a>, 
<a href="/search/q-bio?searchtype=author&query=Ojala%2C+T">Timo Ojala</a>, 
<a href="/search/q-bio?searchtype=author&query=Ruusuvuori%2C+P">Pekka Ruusuvuori</a>, 
<a href="/search/q-bio?searchtype=author&query=Kuopio%2C+T">Teijo Kuopio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In routine colorectal cancer management, histologic samples stained with
hematoxylin and eosin are commonly used. Nonetheless, their potential for
defining objective biomarkers for patient stratification and treatment
selection is still being explored. The current gold standard relies on
expensive and time-consuming genetic tests. However, recent research highlights
the potential of convolutional neural networks (CNNs) in facilitating the
extraction of clinically relevant biomarkers from these readily available
images. These CNN-based biomarkers can predict patient outcomes comparably to
golden standards, with the added advantages of speed, automation, and minimal
cost. The predictive potential of CNN-based biomarkers fundamentally relies on
the ability of convolutional neural networks (CNNs) to classify diverse tissue
types from whole slide microscope images accurately. Consequently, enhancing
the accuracy of tissue class decomposition is critical to amplifying the
prognostic potential of imaging-based biomarkers. This study introduces a
hybrid Deep and ensemble machine learning model that surpassed all preceding
solutions for this classification task. Our model achieved 96.74% accuracy on
the external test set and 99.89% on the internal test set. Recognizing the
potential of these models in advancing the task, we have made them publicly
available for further research and development.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16975" title="Abstract">arXiv:2310.16975</a> (cross-list from stat.ML) [<a href="/pdf/2310.16975" title="Download PDF">pdf</a>, <a href="/format/2310.16975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Neural Network Approaches for Conditional Optimal Transport  with Applications in Bayesian Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+Z+O">Zheyu Oliver Wang</a>, 
<a href="/search/stat?searchtype=author&query=Baptista%2C+R">Ricardo Baptista</a>, 
<a href="/search/stat?searchtype=author&query=Marzouk%2C+Y">Youssef Marzouk</a>, 
<a href="/search/stat?searchtype=author&query=Ruthotto%2C+L">Lars Ruthotto</a>, 
<a href="/search/stat?searchtype=author&query=Verma%2C+D">Deepanshu Verma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 7 tables, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present two neural network approaches that approximate the solutions of
static and dynamic conditional optimal transport (COT) problems, respectively.
Both approaches enable sampling and density estimation of conditional
probability distributions, which are core tasks in Bayesian inference. Our
methods represent the target conditional distributions as transformations of a
tractable reference distribution and, therefore, fall into the framework of
measure transport. COT maps are a canonical choice within this framework, with
desirable properties such as uniqueness and monotonicity. However, the
associated COT problems are computationally challenging, even in moderate
dimensions. To improve the scalability, our numerical algorithms leverage
neural networks to parameterize COT maps. Our methods exploit the structure of
the static and dynamic formulations of the COT problem. PCP-Map models
conditional transport maps as the gradient of a partially input convex neural
network (PICNN) and uses a novel numerical implementation to increase
computational efficiency compared to state-of-the-art alternatives. COT-Flow
models conditional transports via the flow of a regularized neural ODE; it is
slower to train but offers faster sampling. We demonstrate their effectiveness
and efficiency by comparing them with state-of-the-art approaches using
benchmark datasets and Bayesian inverse problems.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16982" title="Abstract">arXiv:2310.16982</a> (cross-list from quant-ph) [<a href="/pdf/2310.16982" title="Download PDF">pdf</a>, <a href="/format/2310.16982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Clifford and parallelizable fault-tolerant logical gates on constant  and almost-constant rate homological quantum LDPC codes via higher symmetries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zhu%2C+G">Guanyu Zhu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sikander%2C+S">Shehryar Sikander</a>, 
<a href="/search/quant-ph?searchtype=author&query=Portnoy%2C+E">Elia Portnoy</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cross%2C+A+W">Andrew W. Cross</a>, 
<a href="/search/quant-ph?searchtype=author&query=Brown%2C+B+J">Benjamin J. Brown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 31 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Strongly Correlated Electrons (cond-mat.str-el); Information Theory (cs.IT); High Energy Physics - Theory (hep-th); Geometric Topology (math.GT)

</div>
<p class="mathjax">We study parallel fault-tolerant quantum computing for families of
homological quantum low-density parity-check (LDPC) codes defined on
3-manifolds with constant or almost-constant encoding rate. We derive generic
formula for a transversal $T$ gate of color codes on general 3-manifolds, which
acts as collective non-Clifford logical CCZ gates on any triplet of logical
qubits with their logical-$X$ membranes having a $\mathbb{Z}_2$ triple
intersection at a single point. The triple intersection number is a topological
invariant, which also arises in the path integral of the emergent higher
symmetry operator in a topological quantum field theory: the $\mathbb{Z}_2^3$
gauge theory. Moreover, the transversal $S$ gate of the color code corresponds
to a higher-form symmetry supported on a codimension-1 submanifold, giving rise
to exponentially many addressable and parallelizable logical CZ gates. We have
developed a generic formalism to compute the triple intersection invariants for
3-manifolds and also study the scaling of the Betti number and systoles with
volume for various 3-manifolds, which translates to the encoding rate and
distance. We further develop three types of LDPC codes supporting such logical
gates: (1) A quasi-hyperbolic code from the product of 2D hyperbolic surface
and a circle, with almost-constant rate $k/n=O(1/\log(n))$ and $O(\log(n))$
distance; (2) A homological fibre bundle code with $O(1/\log^{\frac{1}{2}}(n))$
rate and $O(\log^{\frac{1}{2}}(n))$ distance; (3) A specific family of 3D
hyperbolic codes: the Torelli mapping torus code, constructed from mapping tori
of a pseudo-Anosov element in the Torelli subgroup, which has constant rate
while the distance scaling is currently unknown. We then show a generic
constant-overhead scheme for applying a parallelizable universal gate set with
the aid of logical-$X$ measurements.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17004" title="Abstract">arXiv:2310.17004</a> (cross-list from eess.AS) [<a href="/pdf/2310.17004" title="Download PDF">pdf</a>, <a href="/format/2310.17004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Panning on Non-Equidistant Loudspeakers with Direct Sound Level  Compensation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hanschke%2C+J">Jan-Hendrik Hanschke</a>, 
<a href="/search/eess?searchtype=author&query=Arteaga%2C+D">Daniel Arteaga</a>, 
<a href="/search/eess?searchtype=author&query=Cengarle%2C+G">Giulio Cengarle</a>, 
<a href="/search/eess?searchtype=author&query=Lando%2C+J">Joshua Lando</a>, 
<a href="/search/eess?searchtype=author&query=Thomas%2C+M+R+P">Mark R. P. Thomas</a>, 
<a href="/search/eess?searchtype=author&query=Seefeldt%2C+A">Alan Seefeldt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages. Accepted for presentation in AES Convention 155 (2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the Audio Engineering Society Convention 155, New
  York, paper 10669 (October 2023).
  https://www.aes.org/e-lib/inst/browse.cfm?elib=22250
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Loudspeaker rendering techniques that create phantom sound sources often
assume an equidistant loudspeaker layout. Typical home setups might not fulfill
this condition as loudspeakers deviate from canonical positions, thus requiring
a corresponding calibration. The standard approach is to compensate for delays
and to match the loudness of each loudspeaker at the listener's location.It was
found that a shift of the phantom image occurs when this calibration procedure
is applied and one of a pair of loudspeakers is significantly closer to the
listener than the other. In this paper, a novel approach to panning on
non-equidistant loudspeaker layouts is presented whereby the panning position
is governed by the direct sound and the perceived loudness is governed by the
full impulse response. Subjective listening tests are presented that validate
the approach and quantify the perceived effect of the compensation. In a setup
where the standard calibration leads to an average error of 10 degrees, the
proposed direct sound compensation largely returns the phantom source to its
intended position.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17009" title="Abstract">arXiv:2310.17009</a> (cross-list from stat.ME) [<a href="/pdf/2310.17009" title="Download PDF">pdf</a>, <a href="/format/2310.17009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation based stacking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yao%2C+Y">Yuling Yao</a>, 
<a href="/search/stat?searchtype=author&query=Blancard%2C+B+R">Bruno R&#xe9;galdo-Saint Blancard</a>, 
<a href="/search/stat?searchtype=author&query=Domke%2C+J">Justin Domke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">Simulation-based inference has been popular for amortized Bayesian
computation. It is typical to have more than one posterior approximation, from
different inference algorithms, different architectures, or simply the
randomness of initialization and stochastic gradients. With a provable
asymptotic guarantee, we present a general stacking framework to make use of
all available posterior approximations. Our stacking method is able to combine
densities, simulation draws, confidence intervals, and moments, and address the
overall precision, calibration, coverage, and bias at the same time. We
illustrate our method on several benchmark simulations and a challenging
cosmological inference task.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17023" title="Abstract">arXiv:2310.17023</a> (cross-list from stat.ML) [<a href="/pdf/2310.17023" title="Download PDF">pdf</a>, <a href="/format/2310.17023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Identifiability and Interpretability of Gaussian Process Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+J">Jiawen Chen</a>, 
<a href="/search/stat?searchtype=author&query=Mu%2C+W">Wancen Mu</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+Y">Yun Li</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+D">Didong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we critically examine the prevalent practice of using additive
mixtures of Mat\'ern kernels in single-output Gaussian process (GP) models and
explore the properties of multiplicative mixtures of Mat\'ern kernels for
multi-output GP models. For the single-output case, we derive a series of
theoretical results showing that the smoothness of a mixture of Mat\'ern
kernels is determined by the least smooth component and that a GP with such a
kernel is effectively equivalent to the least smooth kernel component.
Furthermore, we demonstrate that none of the mixing weights or parameters
within individual kernel components are identifiable. We then turn our
attention to multi-output GP models and analyze the identifiability of the
covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where
$K_0$ is a standard single output kernel such as Mat\'ern. We show that $A$ is
identifiable up to a multiplicative constant, suggesting that multiplicative
mixtures are well suited for multi-output tasks. Our findings are supported by
extensive simulations and real applications for both single- and multi-output
settings. This work provides insight into kernel selection and interpretation
for GP models, emphasizing the importance of choosing appropriate kernel
structures for different tasks.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17032" title="Abstract">arXiv:2310.17032</a> (cross-list from quant-ph) [<a href="/pdf/2310.17032" title="Download PDF">pdf</a>, <a href="/format/2310.17032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series  Forecasting: A Comparative Study in Solar Power Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Khan%2C+S+Z">Saad Zafar Khan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Muzammil%2C+N">Nazeefa Muzammil</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zaidi%2C+S+M+H">Syed Mohammad Hassan Zaidi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Aljohani%2C+A+J">Abdulah Jeza Aljohani</a>, 
<a href="/search/quant-ph?searchtype=author&query=Khan%2C+H">Haibat Khan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ghafoor%2C+S">Salman Ghafoor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurately forecasting solar power generation is crucial in the global
progression towards sustainable energy systems. In this study, we conduct a
meticulous comparison between Quantum Long Short-Term Memory (QLSTM) and
classical Long Short-Term Memory (LSTM) models for solar power production
forecasting. Our controlled experiments reveal promising advantages of QLSTMs,
including accelerated training convergence and substantially reduced test loss
within the initial epoch compared to classical LSTMs. These empirical findings
demonstrate QLSTM's potential to swiftly assimilate complex time series
relationships, enabled by quantum phenomena like superposition. However,
realizing QLSTM's full capabilities necessitates further research into model
validation across diverse conditions, systematic hyperparameter optimization,
hardware noise resilience, and applications to correlated renewable forecasting
problems. With continued progress, quantum machine learning can offer a
paradigm shift in renewable energy time series prediction. This pioneering work
provides initial evidence substantiating quantum advantages over classical
LSTM, while acknowledging present limitations. Through rigorous benchmarking
grounded in real-world data, our study elucidates a promising trajectory for
quantum learning in renewable forecasting. Additional research and development
can further actualize this potential to achieve unprecedented accuracy and
reliability in predicting solar power generation worldwide.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17101" title="Abstract">arXiv:2310.17101</a> (cross-list from eess.AS) [<a href="/pdf/2310.17101" title="Download PDF">pdf</a>, <a href="/format/2310.17101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Speaker Expressive Speech Synthesis via Semi-supervised  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhu%2C+X">Xinfa Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yuke Li</a>, 
<a href="/search/eess?searchtype=author&query=Lei%2C+Y">Yi Lei</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+N">Ning Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+G">Guoqing Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">This paper aims to build an expressive TTS system for multi-speakers,
synthesizing a target speaker's speech with multiple styles and emotions. To
this end, we propose a novel contrastive learning-based TTS approach to
transfer style and emotion across speakers. Specifically, we construct
positive-negative sample pairs at both utterance and category (such as
emotion-happy or style-poet or speaker A) levels and leverage contrastive
learning to better extract disentangled style, emotion, and speaker
representations from speech. Furthermore, we introduce a semi-supervised
training strategy to the proposed approach to effectively leverage multi-domain
data, including style-labeled data, emotion-labeled data, and unlabeled data.
We integrate the learned representations into an improved VITS model, enabling
it to synthesize expressive speech with diverse styles and emotions for a
target speaker. Experiments on multi-domain data demonstrate the good design of
our model.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17114" title="Abstract">arXiv:2310.17114</a> (cross-list from stat.ML) [<a href="/pdf/2310.17114" title="Download PDF">pdf</a>, <a href="/format/2310.17114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Convergence of CART under Sufficient Impurity Decrease Condition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Mazumder%2C+R">Rahul Mazumder</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+H">Haoyue Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The decision tree is a flexible machine learning model that finds its success
in numerous applications. It is usually fitted in a recursively greedy manner
using CART. In this paper, we investigate the convergence rate of CART under a
regression setting. First, we establish an upper bound on the prediction error
of CART under a sufficient impurity decrease (SID) condition
\cite{chi2022asymptotic} -- our result improves upon the known result by
\cite{chi2022asymptotic} under a similar assumption. Furthermore, we provide
examples that demonstrate the error bound cannot be further improved by more
than a constant or a logarithmic factor. Second, we introduce a set of easily
verifiable sufficient conditions for the SID condition. Specifically, we
demonstrate that the SID condition can be satisfied in the case of an additive
model, provided that the component functions adhere to a ``locally reverse
Poincar{\'e} inequality". We discuss several well-known function classes in
non-parametric estimation to illustrate the practical utility of this concept.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17116" title="Abstract">arXiv:2310.17116</a> (cross-list from eess.AS) [<a href="/pdf/2310.17116" title="Download PDF">pdf</a>, <a href="/format/2310.17116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Neonatal Chest Sound Separation using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Poh%2C+Y+Y">Yang Yi Poh</a>, 
<a href="/search/eess?searchtype=author&query=Grooby%2C+E">Ethan Grooby</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+K">Kenneth Tan</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+L">Lindsay Zhou</a>, 
<a href="/search/eess?searchtype=author&query=King%2C+A">Arrabella King</a>, 
<a href="/search/eess?searchtype=author&query=Ramanathan%2C+A">Ashwin Ramanathan</a>, 
<a href="/search/eess?searchtype=author&query=Malhotra%2C+A">Atul Malhotra</a>, 
<a href="/search/eess?searchtype=author&query=Harandi%2C+M">Mehrtash Harandi</a>, 
<a href="/search/eess?searchtype=author&query=Marzbanrad%2C+F">Faezeh Marzbanrad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Auscultation for neonates is a simple and non-invasive method of providing
diagnosis for cardiovascular and respiratory disease. Such diagnosis often
requires high-quality heart and lung sounds to be captured during auscultation.
However, in most cases, obtaining such high-quality sounds is non-trivial due
to the chest sounds containing a mixture of heart, lung, and noise sounds. As
such, additional preprocessing is needed to separate the chest sounds into
heart and lung sounds. This paper proposes a novel deep-learning approach to
separate such chest sounds into heart and lung sounds. Inspired by the
Conv-TasNet model, the proposed model has an encoder, decoder, and mask
generator. The encoder consists of a 1D convolution model and the decoder
consists of a transposed 1D convolution. The mask generator is constructed
using stacked 1D convolutions and transformers. The proposed model outperforms
previous methods in terms of objective distortion measures by 2.01 dB to 5.06
dB in the artificial dataset, as well as computation time, with at least a
17-time improvement. Therefore, our proposed model could be a suitable
preprocessing step for any phonocardiogram-based health monitoring system.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17122" title="Abstract">arXiv:2310.17122</a> (cross-list from eess.IV) [<a href="/pdf/2310.17122" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing sea ice segmentation in Sentinel-1 images with atrous  convolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=de+Lima%2C+R+P">Rafael Pires de Lima</a>, 
<a href="/search/eess?searchtype=author&query=Vahedi%2C+B">Behzad Vahedi</a>, 
<a href="/search/eess?searchtype=author&query=Hughes%2C+N">Nick Hughes</a>, 
<a href="/search/eess?searchtype=author&query=Barrett%2C+A+P">Andrew P. Barrett</a>, 
<a href="/search/eess?searchtype=author&query=Meier%2C+W">Walter Meier</a>, 
<a href="/search/eess?searchtype=author&query=Karimzadeh%2C+M">Morteza Karimzadeh</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Remote Sensing 4:17, 5344-5374 (20230
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Due to the growing volume of remote sensing data and the low latency required
for safe marine navigation, machine learning (ML) algorithms are being
developed to accelerate sea ice chart generation, currently a manual
interpretation task. However, the low signal-to-noise ratio of the freely
available Sentinel-1 Synthetic Aperture Radar (SAR) imagery, the ambiguity of
backscatter signals for ice types, and the scarcity of open-source
high-resolution labelled data makes automating sea ice mapping challenging. We
use Extreme Earth version 2, a high-resolution benchmark dataset generated for
ML training and evaluation, to investigate the effectiveness of ML for
automated sea ice mapping. Our customized pipeline combines ResNets and Atrous
Spatial Pyramid Pooling for SAR image segmentation. We investigate the
performance of our model for: i) binary classification of sea ice and open
water in a segmentation framework; and ii) a multiclass segmentation of five
sea ice types. For binary ice-water classification, models trained with our
largest training set have weighted F1 scores all greater than 0.95 for January
and July test scenes. Specifically, the median weighted F1 score was 0.98,
indicating high performance for both months. By comparison, a competitive
baseline U-Net has a weighted average F1 score of ranging from 0.92 to 0.94
(median 0.93) for July, and 0.97 to 0.98 (median 0.97) for January. Multiclass
ice type classification is more challenging, and even though our models achieve
2% improvement in weighted F1 average compared to the baseline U-Net, test
weighted F1 is generally between 0.6 and 0.80. Our approach can efficiently
segment full SAR scenes in one run, is faster than the baseline U-Net, retains
spatial resolution and dimension, and is more robust against noise compared to
approaches that rely on patch classification.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17142" title="Abstract">arXiv:2310.17142</a> (cross-list from eess.AS) [<a href="/pdf/2310.17142" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single channel speech enhancement by colored spectrograms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gul%2C+S">Sania Gul</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+M+S">Muhammad Salman Khan</a>, 
<a href="/search/eess?searchtype=author&query=Fazeel%2C+M">Muhammad Fazeel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Speech enhancement concerns the processes required to remove unwanted
background sounds from the target speech to improve its quality and
intelligibility. In this paper, a novel approach for single-channel speech
enhancement is presented, using colored spectrograms. We propose the use of a
deep neural network (DNN) architecture adapted from the pix2pix generative
adversarial network (GAN) and train it over colored spectrograms of speech to
denoise them. After denoising, the colors of spectrograms are translated to
magnitudes of short-time Fourier transform (STFT) using a shallow regression
neural network. These estimated STFT magnitudes are later combined with the
noisy phases to obtain an enhanced speech. The results show an improvement of
almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1%
in the short-term objective intelligibility (STOI) over the unprocessed noisy
data. The gain in quality and intelligibility over the unprocessed signal is
almost equal to the gain achieved by the baseline methods used for comparison
with the proposed model, but at a much reduced computational cost. The proposed
solution offers a comparative PESQ score at almost 10 times reduced
computational cost than a similar baseline model that has generated the highest
PESQ score trained on grayscaled spectrograms, while it provides only a 1%
deficit in STOI at 28 times reduced computational cost when compared to another
baseline system based on convolutional neural network-GAN (CNN-GAN) that
produces the most intelligible speech.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17158" title="Abstract">arXiv:2310.17158</a> (cross-list from astro-ph.EP) [<a href="/pdf/2310.17158" title="Download PDF">pdf</a>, <a href="/format/2310.17158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CosmosDSR -- a methodology for automated detection and tracking of  orbital debris using the Unscented Kalman Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Roll%2C+D+S">Daniel S. Roll</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kurt%2C+Z">Zeyneb Kurt</a>, 
<a href="/search/astro-ph?searchtype=author&query=Woo%2C+W+L">Wai Lok Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 figures, 15 pages inc refs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The Kessler syndrome refers to the escalating space debris from frequent
space activities, threatening future space exploration. Addressing this issue
is vital. Several AI models, including Convolutional Neural Networks (CNN),
Kernel Principal Component Analysis (KPCA), and Model-Agnostic Meta-Learning
(MAML), have been assessed with various data types. Earlier studies highlighted
the combination of the YOLO object detector and a linear Kalman filter for
object detection and tracking. Building on this, our project introduces
CosmosDSR, a novel methodology combining YOLOv3 with an Unscented Kalman Filter
for tracking satellites in sequential images, compared to a linear Kalman
filter. Using the SPARK dataset from the University of Luxembourg for training
and testing, the YOLOv3 precisely detected and classified all satellite
categories (mAP=97.18%, F1=0.95) with few errors (TP=4163, FP=209, FN=237).
Both CosmosDSR and the LKF tracked satellites accurately (UKF:
MSE=2.83/RMSE=1.66, LKF: MSE=2.84/RMSE=1.66). Despite concerns of class
imbalance and the absence of real images, the model shows promise. Future work
should address these limitations, increase tracking sample size, and improve
metrics. This research suggests the algorithm's potential in detecting and
tracking satellites, paving the way for solutions to the Kessler syndrome.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17216" title="Abstract">arXiv:2310.17216</a> (cross-list from eess.IV) [<a href="/pdf/2310.17216" title="Download PDF">pdf</a>, <a href="/format/2310.17216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three-dimensional Bone Image Synthesis with Generative Adversarial  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Angermann%2C+C">Christoph Angermann</a>, 
<a href="/search/eess?searchtype=author&query=Bereiter-Payr%2C+J">Johannes Bereiter-Payr</a>, 
<a href="/search/eess?searchtype=author&query=Stock%2C+K">Kerstin Stock</a>, 
<a href="/search/eess?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>, 
<a href="/search/eess?searchtype=author&query=Degenhart%2C+G">Gerald Degenhart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the journal Artificial Intelligence in Medicine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Medical image processing has been highlighted as an area where deep
learning-based models have the greatest potential. However, in the medical
field in particular, problems of data availability and privacy are hampering
research progress and thus rapid implementation in clinical routine. The
generation of synthetic data not only ensures privacy, but also allows to
\textit{draw} new patients with specific characteristics, enabling the
development of data-driven models on a much larger scale. This work
demonstrates that three-dimensional generative adversarial networks (GANs) can
be efficiently trained to generate high-resolution medical volumes with finely
detailed voxel-based architectures. In addition, GAN inversion is successfully
implemented for the three-dimensional setting and used for extensive research
on model interpretability and applications such as image morphing, attribute
editing and style mixing. The results are comprehensively validated on a
database of three-dimensional HR-pQCT instances representing the bone
micro-architecture of the distal radius.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17232" title="Abstract">arXiv:2310.17232</a> (cross-list from quant-ph) [<a href="/pdf/2310.17232" title="Download PDF">pdf</a>, <a href="/ps/2310.17232" title="Download PostScript">ps</a>, <a href="/format/2310.17232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoherence time in quantum harmonic oscillators as quantum memory  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Vladimirov%2C+I+G">Igor G. Vladimirov</a>, 
<a href="/search/quant-ph?searchtype=author&query=Petersen%2C+I+R">Ian R. Petersen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper is concerned with open quantum harmonic oscillators (OQHOs)
described by linear quantum stochastic differential equations. This framework
includes isolated oscillators with zero Hamiltonian, whose system variables
remain unchanged (in the Heisenberg picture of quantum dynamics) over the
course of time, making such systems potentially applicable as quantum memory
devices. In a more realistic case of system-environment coupling, we define a
memory decoherence horizon as a typical time for a mean-square deviation of the
system variables from their initial values to become relatively significant as
specified by a weighting matrix and a fidelity parameter. We consider the
maximization of the decoherence time over the energy and coupling matrices of
the OQHO as a memory system in its storage phase and obtain a condition under
which the zero Hamiltonian delivers a suboptimal solution. This optimization
problem is also discussed for an interconnection of OQHOs.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17303" title="Abstract">arXiv:2310.17303</a> (cross-list from stat.ML) [<a href="/pdf/2310.17303" title="Download PDF">pdf</a>, <a href="/ps/2310.17303" title="Download PostScript">ps</a>, <a href="/format/2310.17303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demonstration-Regularized RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tiapkin%2C+D">Daniil Tiapkin</a>, 
<a href="/search/stat?searchtype=author&query=Belomestny%2C+D">Denis Belomestny</a>, 
<a href="/search/stat?searchtype=author&query=Calandriello%2C+D">Daniele Calandriello</a>, 
<a href="/search/stat?searchtype=author&query=Moulines%2C+E">Eric Moulines</a>, 
<a href="/search/stat?searchtype=author&query=Naumov%2C+A">Alexey Naumov</a>, 
<a href="/search/stat?searchtype=author&query=Perrault%2C+P">Pierre Perrault</a>, 
<a href="/search/stat?searchtype=author&query=Valko%2C+M">Michal Valko</a>, 
<a href="/search/stat?searchtype=author&query=Menard%2C+P">Pierre Menard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Incorporating expert demonstrations has empirically helped to improve the
sample efficiency of reinforcement learning (RL). This paper quantifies
theoretically to what extent this extra information reduces RL's sample
complexity. In particular, we study the demonstration-regularized reinforcement
learning that leverages the expert demonstrations by KL-regularization for a
policy learned by behavior cloning. Our findings reveal that using
$N^{\mathrm{E}}$ expert demonstrations enables the identification of an optimal
policy at a sample complexity of order
$\widetilde{\mathcal{O}}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$
in finite and $\widetilde{\mathcal{O}}(\mathrm{Poly}(d,H)/(\varepsilon^2
N^{\mathrm{E}}))$ in linear Markov decision processes, where $\varepsilon$ is
the target precision, $H$ the horizon, $A$ the number of action, $S$ the number
of states in the finite case and $d$ the dimension of the feature space in the
linear case. As a by-product, we provide tight convergence guarantees for the
behaviour cloning procedure under general assumptions on the policy classes.
Additionally, we establish that demonstration-regularized methods are provably
efficient for reinforcement learning from human feedback (RLHF). In this
respect, we provide theoretical evidence showing the benefits of
KL-regularization for RLHF in tabular and linear MDPs. Interestingly, we avoid
pessimism injection by employing computationally feasible regularization to
handle reward estimation uncertainty, thus setting our approach apart from the
prior works.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17329" title="Abstract">arXiv:2310.17329</a> (cross-list from quant-ph) [<a href="/pdf/2310.17329" title="Download PDF">pdf</a>, <a href="/format/2310.17329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tightening continuity bounds on entropies and bounds on quantum  capacities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Jabbour%2C+M+G">Michael G. Jabbour</a>, 
<a href="/search/quant-ph?searchtype=author&query=Datta%2C+N">Nilanjana Datta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Uniform continuity bounds on entropies are generally expressed in terms of a
single distance measure between a pair of probability distributions or quantum
states, typically, the total variation distance or trace distance. However, if
an additional distance measure between the probability distributions or states
is known, then the continuity bounds can be significantly strengthened. Here,
we prove a tight uniform continuity bound for the Shannon entropy in terms of
both the local- and total variation distances, sharpening an inequality proven
in [I. Sason, IEEE Trans. Inf. Th., 59, 7118 (2013)]. We also obtain a uniform
continuity bound for the von Neumann entropy in terms of both the operator
norm- and trace distances. The bound is tight when the quotient of the trace
distance by the operator norm distance is an integer. We then apply our results
to compute upper bounds on the quantum- and private classical capacities of
channels. We begin by refining the concept of approximate degradable channels,
namely, $\varepsilon$-degradable channels, which are, by definition,
$\varepsilon$-close in diamond norm to their complementary channel when
composed with a degrading channel. To this end, we introduce the notion of
$(\varepsilon,\nu)$-degradable channels; these are $\varepsilon$-degradable
channels that are, in addition, $\nu$-close in completely bounded spectral norm
to their complementary channel, when composed with the same degrading channel.
This allows us to derive improved upper bounds to the quantum- and private
classical capacities of such channels. Moreover, these bounds can be further
improved by considering certain unstabilized versions of the above norms. We
show that upper bounds on the latter can be efficiently expressed as
semidefinite programs. We illustrate our results by obtaining a new upper bound
on the quantum capacity of the qubit depolarizing channel.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17386" title="Abstract">arXiv:2310.17386</a> (cross-list from stat.ML) [<a href="/pdf/2310.17386" title="Download PDF">pdf</a>, <a href="/format/2310.17386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Challenge in Reweighting Data with Bilevel Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ivanova%2C+A">Anastasia Ivanova</a>, 
<a href="/search/stat?searchtype=author&query=Ablin%2C+P">Pierre Ablin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In many scenarios, one uses a large training set to train a model with the
goal of performing well on a smaller testing set with a different distribution.
Learning a weight for each data point of the training set is an appealing
solution, as it ideally allows one to automatically learn the importance of
each training point for generalization on the testing set. This task is usually
formalized as a bilevel optimization problem. Classical bilevel solvers are
based on a warm-start strategy where both the parameters of the models and the
data weights are learned at the same time. We show that this joint dynamic may
lead to sub-optimal solutions, for which the final data weights are very
sparse. This finding illustrates the difficulty of data reweighting and offers
a clue as to why this method is rarely used in practice.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17425" title="Abstract">arXiv:2310.17425</a> (cross-list from eess.SP) [<a href="/pdf/2310.17425" title="Download PDF">pdf</a>, <a href="/ps/2310.17425" title="Download PostScript">ps</a>, <a href="/format/2310.17425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Abrupt Change of Channel Covariance Matrix in IRS-Assisted  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+R">Runnan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+L">Liang Liu</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yin Xu</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+D">Dazhi He</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+W">Wenjun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+C+W">Chang Wen Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by IEEE Wireless Communications Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The knowledge of channel covariance matrices is crucial to the design of
intelligent reflecting surface (IRS) assisted communication. However, channel
covariance matrices may change suddenly in practice. This letter focuses on the
detection of the above change in IRS-assisted communication. Specifically, we
consider the uplink communication system consisting of a single-antenna user
(UE), an IRS, and a multi-antenna base station (BS). We first categorize two
types of channel covariance matrix changes based on their impact on system
design: Type I change, which denotes the change in the BS receive covariance
matrix, and Type II change, which denotes the change in the IRS
transmit/receive covariance matrix. Secondly, a powerful method is proposed to
detect whether a Type I change occurs, a Type II change occurs, or no change
occurs. The effectiveness of our proposed scheme is verified by numerical
results.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17467" title="Abstract">arXiv:2310.17467</a> (cross-list from stat.ML) [<a href="/pdf/2310.17467" title="Download PDF">pdf</a>, <a href="/format/2310.17467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The statistical thermodynamics of generative diffusion models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ambrogioni%2C+L">Luca Ambrogioni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative diffusion models have achieved spectacular performance in many
areas of generative modeling. While the fundamental ideas behind these models
come from non-equilibrium physics, in this paper we show that many aspects of
these models can be understood using the tools of equilibrium statistical
mechanics. Using this reformulation, we show that generative diffusion models
undergo second-order phase transitions corresponding to symmetry breaking
phenomena. We argue that this lead to a form of instability that lies at the
heart of their generative capabilities and that can be described by a set of
mean field critical exponents. We conclude by analyzing recent work connecting
diffusion models and associative memory networks in view of the thermodynamic
formulations.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17482" title="Abstract">arXiv:2310.17482</a> (cross-list from math.CO) [<a href="/pdf/2310.17482" title="Download PDF">pdf</a>, <a href="/ps/2310.17482" title="Download PostScript">ps</a>, <a href="/format/2310.17482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orthonormal representations, vector chromatic number, and extension  complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Balla%2C+I">Igor Balla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We construct a bipartite generalization of Alon and Szegedy's nearly
orthogonal vectors, thereby obtaining strong bounds for several extremal
problems involving the Lov\'asz theta function, vector chromatic number,
minimum semidefinite rank, nonnegative rank, and extension complexity of
polytopes. In particular, we derive some general lower bounds for the vector
chromatic number which may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17496" title="Abstract">arXiv:2310.17496</a> (cross-list from stat.ME) [<a href="/pdf/2310.17496" title="Download PDF">pdf</a>, <a href="/format/2310.17496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Interference Induced by Data Training Loops in A/B Tests: A  Weighted Training Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Si%2C+N">Nian Si</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Econometrics (econ.EM)

</div>
<p class="mathjax">In modern recommendation systems, the standard pipeline involves training
machine learning models on historical data to predict user behaviors and
improve recommendations continuously. However, these data training loops can
introduce interference in A/B tests, where data generated by control and
treatment algorithms, potentially with different distributions, are combined.
To address these challenges, we introduce a novel approach called weighted
training. This approach entails training a model to predict the probability of
each data point appearing in either the treatment or control data and
subsequently applying weighted losses during model training. We demonstrate
that this approach achieves the least variance among all estimators without
causing shifts in the training distributions. Through simulation studies, we
demonstrate the lower bias and variance of our approach compared to other
methods.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17548" title="Abstract">arXiv:2310.17548</a> (cross-list from quant-ph) [<a href="/pdf/2310.17548" title="Download PDF">pdf</a>, <a href="/format/2310.17548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information reconciliation for discretely-modulated continuous-variable  quantum key distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Leverrier%2C+A">Anthony Leverrier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The goal of this note is to explain the reconciliation problem for
continuous-variable quantum key distribution protocols with a discrete
modulation. Such modulation formats are attractive since they significantly
simplify experimental implementations compared to protocols with a Gaussian
modulation. Previous security proofs that relied crucially on the Gaussian
distribution of the input states are rendered inapplicable, and new proofs
based on the entropy accumulation theorem have emerged. Unfortunately, these
proofs are not compatible with existing reconciliation procedures, and
necessitate a reevaluation of the reconciliation problem. We argue that this
problem is nontrivial and deserves further attention. In particular, assuming
it can be solved with optimal efficiency leads to overly optimistic predictions
for the performance of the key distribution protocol, in particular for long
distances.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17574" title="Abstract">arXiv:2310.17574</a> (cross-list from quant-ph) [<a href="/pdf/2310.17574" title="Download PDF">pdf</a>, <a href="/format/2310.17574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Prime Factorization via Quantum Annealing by Modular  Locally-structured Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ding%2C+J">Jingwen Ding</a>, 
<a href="/search/quant-ph?searchtype=author&query=Spallitta%2C+G">Giuseppe Spallitta</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sebastiani%2C+R">Roberto Sebastiani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">This paper investigates novel techniques to solve prime factorization by
quantum annealing (QA). Our contribution is twofold. First, we present a novel
and very compact modular encoding of a binary multiplier circuit into the
Pegasus architecture of current D-Wave QA devices. The key contribution is a
compact encoding of a controlled full-adder into an 8-qubit module in the
Pegasus topology, which we synthesized offline by means of Optimization Modulo
Theories. This allows us to encode up to a 21*12-bit multiplier (and a 22*8-bit
one) into the Pegasus 5760-qubit topology of current annealers. To the best of
our knowledge, these are the largest factorization problems ever encoded into a
quantum annealer. Second, we have investigated the problem of actually solving
encoded PF problems by running an extensive experimental evaluation on a D-Wave
Advantage 4.1 quantum annealer. In order to help the annealer in reaching the
global minimum, in the experiments we introduced different approaches to
initialize the multiplier qubits and adopted several performance enhancement
techniques. Overall, exploiting all the encoding and solving techniques
described in this paper, 8, 219, 999 = 32, 749 * 251 was the highest prime
product we were able to factorize within the limits of our QPU resources. To
the best of our knowledge, this is the largest number which was ever factorized
by means of a quantum annealer, and, more generally, by a quantum device.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17582" title="Abstract">arXiv:2310.17582</a> (cross-list from stat.ML) [<a href="/pdf/2310.17582" title="Download PDF">pdf</a>, <a href="/format/2310.17582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of flow-based generative models via proximal gradient  descent in Wasserstein space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Lu%2C+J">Jianfeng Lu</a>, 
<a href="/search/stat?searchtype=author&query=Tan%2C+Y">Yixin Tan</a>, 
<a href="/search/stat?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Statistics Theory (math.ST)

</div>
<p class="mathjax">Flow-based generative models enjoy certain advantages in computing the data
generation and the likelihood, and have recently shown competitive empirical
performance. Compared to the accumulating theoretical studies on related
score-based diffusion models, analysis of flow-based models, which are
deterministic in both forward (data-to-noise) and reverse (noise-to-data)
directions, remain sparse. In this paper, we provide a theoretical guarantee of
generating data distribution by a progressive flow model, the so-called JKO
flow model, which implements the Jordan-Kinderleherer-Otto (JKO) scheme in a
normalizing flow network. Leveraging the exponential convergence of the
proximal gradient descent (GD) in Wasserstein space, we prove the
Kullback-Leibler (KL) guarantee of data generation by a JKO flow model to be
$O(\varepsilon^2)$ when using $N \lesssim \log (1/\varepsilon)$ many JKO steps
($N$ Residual Blocks in the flow) where $\varepsilon $ is the error in the
per-step first-order condition. The assumption on data density is merely a
finite second moment, and the theory extends to data distributions without
density and when there are inversion errors in the reverse process where we
obtain KL-$W_2$ mixed error guarantees. The non-asymptotic convergence rate of
the JKO-type $W_2$-proximal GD is proved for a general class of convex
objective functionals that includes the KL divergence as a special case, which
can be of independent interest.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17584" title="Abstract">arXiv:2310.17584</a> (cross-list from math.OC) [<a href="/pdf/2310.17584" title="Download PDF">pdf</a>, <a href="/format/2310.17584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A minimax optimal control approach for robust neural ODEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cipriani%2C+C">Cristina Cipriani</a>, 
<a href="/search/math?searchtype=author&query=Scagliotti%2C+A">Alessandro Scagliotti</a>, 
<a href="/search/math?searchtype=author&query=W%C3%B6hrer%2C+T">Tobias W&#xf6;hrer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, we address the adversarial training of neural ODEs from a
robust control perspective. This is an alternative to the classical training
via empirical risk minimization, and it is widely used to enforce reliable
outcomes for input perturbations. Neural ODEs allow the interpretation of deep
neural networks as discretizations of control systems, unlocking powerful tools
from control theory for the development and the understanding of machine
learning. In this specific case, we formulate the adversarial training with
perturbed data as a minimax optimal control problem, for which we derive first
order optimality conditions in the form of Pontryagin's Maximum Principle. We
provide a novel interpretation of robust training leading to an alternative
weighted technique, which we test on a low-dimensional classification task.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17610" title="Abstract">arXiv:2310.17610</a> (cross-list from math.OC) [<a href="/pdf/2310.17610" title="Download PDF">pdf</a>, <a href="/format/2310.17610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A qualitative difference between gradient flows of convex functions in  finite- and infinite-dimensional Hilbert spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Siegel%2C+J+W">Jonathan W. Siegel</a>, 
<a href="/search/math?searchtype=author&query=Wojtowytsch%2C+S">Stephan Wojtowytsch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Classical Analysis and ODEs (math.CA); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider gradient flow/gradient descent and heavy ball/accelerated
gradient descent optimization for convex objective functions. In the gradient
flow case, we prove the following:
<br />1. If $f$ does not have a minimizer, the convergence $f(x_t)\to \inf f$ can
be arbitrarily slow.
<br />2. If $f$ does have a minimizer, the excess energy $f(x_t) - \inf f$ is
integrable/summable in time. In particular, $f(x_t) - \inf f = o(1/t)$ as
$t\to\infty$.
<br />3. In Hilbert spaces, this is optimal: $f(x_t) - \inf f$ can decay to $0$ as
slowly as any given function which is monotone decreasing and integrable at
$\infty$, even for a fixed quadratic objective.
<br />4. In finite dimension (or more generally, for all gradient flow curves of
finite length), this is not optimal: We prove that there are convex monotone
decreasing integrable functions $g(t)$ which decrease to zero slower than
$f(x_t)-\inf f$ for the gradient flow of any convex function on $\mathbb R^d$.
For instance, we show that any gradient flow $x_t$ of a convex function $f$ in
finite dimension satisfies $\liminf_{t\to\infty} \big(t\cdot \log^2(t)\cdot
\big\{f(x_t) -\inf f\big\}\big)=0$.
<br />This improves on the commonly reported $O(1/t)$ rate and provides a sharp
characterization of the energy decay law. We also note that it is impossible to
establish a rate $O(1/(t\phi(t))$ for any function $\phi$ which satisfies
$\lim_{t\to\infty}\phi(t) = \infty$, even asymptotically.
<br />Similar results are obtained in related settings for (1) discrete time
gradient descent, (2) stochastic gradient descent with multiplicative noise and
(3) the heavy ball ODE. In the case of stochastic gradient descent, the
summability of $\mathbb E[f(x_n) - \inf f]$ is used to prove that $f(x_n)\to
\inf f$ almost surely - an improvement on the convergence almost surely up to a
subsequence which follows from the $O(1/n)$ decay estimate.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17646" title="Abstract">arXiv:2310.17646</a> (cross-list from physics.plasm-ph) [<a href="/pdf/2310.17646" title="Download PDF">pdf</a>, <a href="/format/2310.17646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Graph Neural Networks Dream of Landau Damping? Insights from Kinetic  Simulations of a Plasma Sheet Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Carvalho%2C+D+D">Diogo D Carvalho</a>, 
<a href="/search/physics?searchtype=author&query=Ferreira%2C+D+R">Diogo R Ferreira</a>, 
<a href="/search/physics?searchtype=author&query=Silva%2C+L+O">Luis O Silva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Plasma Physics (physics.plasm-ph)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">We explore the possibility of fully replacing a plasma physics kinetic
simulator with a graph neural network-based simulator. We focus on this class
of surrogate models given the similarity between their message-passing update
mechanism and the traditional physics solver update, and the possibility of
enforcing known physical priors into the graph construction and update. We show
that our model learns the kinetic plasma dynamics of the one-dimensional plasma
model, a predecessor of contemporary kinetic plasma simulation codes, and
recovers a wide range of well-known kinetic plasma processes, including plasma
thermalization, electrostatic fluctuations about thermal equilibrium, and the
drag on a fast sheet and Landau damping. We compare the performance against the
original plasma model in terms of run-time, conservation laws, and temporal
evolution of key physical quantities. The limitations of the model are
presented and possible directions for higher-dimensional surrogate models for
kinetic plasmas are discussed.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17648" title="Abstract">arXiv:2310.17648</a> (cross-list from physics.optics) [<a href="/pdf/2310.17648" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chip-to-chip ODDM network with optically enabled equalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zazzi%2C+A">Andrea Zazzi</a>, 
<a href="/search/physics?searchtype=author&query=Witzens%2C+J">Jeremy Witzens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">We propose and model an optical communication scheme for short distance
datacom links based on the distribution of information across a wide comb
spectrum. This modulation format, orthogonal delay division multiplexing,
allows the multiplexing of data streams from multiple modulators, as well as
the deserialization and equalization of the data in the optical domain. A
concrete communication system, that allows the transport of 400 Gb/s across a
single CWDM channel with a single 80 GHz cutoff lithium niobate on insulator
modulator, is modeled under consideration of all noise sources present in the
system and its sensitivity to group velocity dispersion is analyzed. Data is
deserialized and equalized at the receiver with a 5-tap optical equalizer. This
communication architecture may provide a path forward to implement
high-baud-rate signaling in short-reach optical links without requiring
high-speed ADCs and electronic deserializers at the receiver, thus maintaining
the in-package power consumption at manageable levels.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Fri, 27 Oct 23</h3>
<dl>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1901.05066" title="Abstract">arXiv:1901.05066</a> (replaced) [<a href="/pdf/1901.05066" title="Download PDF">pdf</a>, <a href="/format/1901.05066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Antigram Behaviour using Distributional Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+S">Saptarshi Sengupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1908.10907" title="Abstract">arXiv:1908.10907</a> (replaced) [<a href="/pdf/1908.10907" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DFPENet-geology: A Deep Learning Framework for High Precision  Recognition and Segmentation of Co-seismic Landslides
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qingsong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+C">Chaojun Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+T">Tianhai Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xuanmei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Duoxiang Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1909.00097" title="Abstract">arXiv:1909.00097</a> (replaced) [<a href="/pdf/1909.00097" title="Download PDF">pdf</a>, <a href="/format/1909.00097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VST-A: A Foundationally Sound Annotation Verifier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Litao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jianxing Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qinshi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Appel%2C+A+W">Andrew W. Appel</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qinxiang Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1910.08517" title="Abstract">arXiv:1910.08517</a> (replaced) [<a href="/pdf/1910.08517" title="Download PDF">pdf</a>, <a href="/format/1910.08517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cluster Editing parameterized above modification-disjoint $P_3$-packings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shaohua Li</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Marcin Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Sorge%2C+M">Manuel Sorge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.12759" title="Abstract">arXiv:1912.12759</a> (replaced) [<a href="/pdf/1912.12759" title="Download PDF">pdf</a>, <a href="/format/1912.12759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Faithful Discretization of the Augmented Persistent Homology Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fasy%2C+B+T">Brittany Terese Fasy</a>, 
<a href="/search/cs?searchtype=author&query=Micka%2C+S">Samuel Micka</a>, 
<a href="/search/cs?searchtype=author&query=Millman%2C+D+L">David L. Millman</a>, 
<a href="/search/cs?searchtype=author&query=Schenfisch%2C+A">Anna Schenfisch</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+L">Lucia Williams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.08417" title="Abstract">arXiv:2008.08417</a> (replaced) [<a href="/pdf/2008.08417" title="Download PDF">pdf</a>, <a href="/format/2008.08417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modular Subset Sum, Dynamic Strings, and Zero-Sum Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cardinal%2C+J">Jean Cardinal</a>, 
<a href="/search/cs?searchtype=author&query=Iacono%2C+J">John Iacono</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised version of the original which appeared at the 2021 SIAM Symposium on Simplicity in Algorithms (SOSA21)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.01742" title="Abstract">arXiv:2009.01742</a> (replaced) [<a href="/pdf/2009.01742" title="Download PDF">pdf</a>, <a href="/format/2009.01742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Estimation and Community Detection of Network Point Processes for  Event Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+G">Guanhua Fang</a>, 
<a href="/search/cs?searchtype=author&query=Ward%2C+O+G">Owen G. Ward</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tian Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.05435" title="Abstract">arXiv:2012.05435</a> (replaced) [<a href="/pdf/2012.05435" title="Download PDF">pdf</a>, <a href="/format/2012.05435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization-Inspired Learning with Architecture Augmentations and  Control Mechanisms for Low-Level Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Risheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+P">Pan Mu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhongxuan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages. The codes are available at <a href="https://github.com/LiuZhu-CV/GDC-OptimizationLearning">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.09241" title="Abstract">arXiv:2101.09241</a> (replaced) [<a href="/pdf/2101.09241" title="Download PDF">pdf</a>, <a href="/ps/2101.09241" title="Download PostScript">ps</a>, <a href="/format/2101.09241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Requirements for COVID-19 Mitigation Strategies. Part II:  Elicitation of Requirements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.02931" title="Abstract">arXiv:2102.02931</a> (replaced) [<a href="/pdf/2102.02931" title="Download PDF">pdf</a>, <a href="/ps/2102.02931" title="Download PostScript">ps</a>, <a href="/format/2102.02931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cutoff stability under distributional constraints with an application to  summer internship matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aziz%2C+H">Haris Aziz</a>, 
<a href="/search/cs?searchtype=author&query=Baychkov%2C+A">Anton Baychkov</a>, 
<a href="/search/cs?searchtype=author&query=Biro%2C+P">Peter Biro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of our AAMAS 2020 paper "Summer Internship Matching with Funding Constraints"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02397" title="Abstract">arXiv:2106.02397</a> (replaced) [<a href="/pdf/2106.02397" title="Download PDF">pdf</a>, <a href="/format/2106.02397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Classifying Continuous Constraint Satisfaction Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miltzow%2C+T">Tillmann Miltzow</a>, 
<a href="/search/cs?searchtype=author&query=Schmiermann%2C+R+F">Reinier F. Schmiermann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Computational Geometry (cs.CG); Computation and Language (cs.CL); Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07368" title="Abstract">arXiv:2106.07368</a> (replaced) [<a href="/pdf/2106.07368" title="Download PDF">pdf</a>, <a href="/format/2106.07368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality-Aware Network for Face Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Q">Qing Song</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+X">Xueshi Xin</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+W">Wenhe Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2nd place in Short-video Face Parsing Track of The 3rd Person in Context (PIC) Workshop and Challenge at CVPR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.09462" title="Abstract">arXiv:2106.09462</a> (replaced) [<a href="/pdf/2106.09462" title="Download PDF">pdf</a>, <a href="/format/2106.09462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+J+M">Juan Manuel P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Rajngewerc%2C+M">Mariela Rajngewerc</a>, 
<a href="/search/cs?searchtype=author&query=Giudici%2C+J+C">Juan Carlos Giudici</a>, 
<a href="/search/cs?searchtype=author&query=Furman%2C+D+A">Dami&#xe1;n A. Furman</a>, 
<a href="/search/cs?searchtype=author&query=Luque%2C+F">Franco Luque</a>, 
<a href="/search/cs?searchtype=author&query=Alemany%2C+L+A">Laura Alonso Alemany</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+M+V">Mar&#xed;a Vanina Mart&#xed;nez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.11959" title="Abstract">arXiv:2106.11959</a> (replaced) [<a href="/pdf/2106.11959" title="Download PDF">pdf</a>, <a href="/format/2106.11959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Deep Learning Models for Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorishniy%2C+Y">Yury Gorishniy</a>, 
<a href="/search/cs?searchtype=author&query=Rubachev%2C+I">Ivan Rubachev</a>, 
<a href="/search/cs?searchtype=author&query=Khrulkov%2C+V">Valentin Khrulkov</a>, 
<a href="/search/cs?searchtype=author&query=Babenko%2C+A">Artem Babenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2021 camera-ready. Code: <a href="https://github.com/yandex-research/tabular-dl-revisiting-models">this https URL</a> (v3-v5: minor changes)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.07420" title="Abstract">arXiv:2107.07420</a> (replaced) [<a href="/pdf/2107.07420" title="Download PDF">pdf</a>, <a href="/format/2107.07420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Scoring Rule Design under Partial Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fang-Yi Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.11419" title="Abstract">arXiv:2107.11419</a> (replaced) [<a href="/pdf/2107.11419" title="Download PDF">pdf</a>, <a href="/format/2107.11419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Komiyama%2C+J">Junpei Komiyama</a>, 
<a href="/search/stat?searchtype=author&query=Fouch%C3%A9%2C+E">Edouard Fouch&#xe9;</a>, 
<a href="/search/stat?searchtype=author&query=Honda%2C+J">Junya Honda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revision: Regret bound for ADR-Bandit + TS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.14251" title="Abstract">arXiv:2109.14251</a> (replaced) [<a href="/pdf/2109.14251" title="Download PDF">pdf</a>, <a href="/format/2109.14251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Road Network Guided Fine-Grained Urban Traffic Flow Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengmeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Ziyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Junfan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Liang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted to TNNLS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.07398" title="Abstract">arXiv:2111.07398</a> (replaced) [<a href="/pdf/2111.07398" title="Download PDF">pdf</a>, <a href="/format/2111.07398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single- versus Multi-Carrier Terahertz-Band Communications: A  Comparative Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tarboush%2C+S">Simon Tarboush</a>, 
<a href="/search/cs?searchtype=author&query=Sarieddeen%2C+H">Hadi Sarieddeen</a>, 
<a href="/search/cs?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>, 
<a href="/search/cs?searchtype=author&query=Al-Naffouri%2C+T+Y">Tareq Y. Al-Naffouri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 12 figures, journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.01646" title="Abstract">arXiv:2112.01646</a> (replaced) [<a href="/pdf/2112.01646" title="Download PDF">pdf</a>, <a href="/format/2112.01646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the usefulness of Quantum Blur
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wootton%2C+J+R">James R. Wootton</a>, 
<a href="/search/cs?searchtype=author&query=Pfaffhauser%2C+M">Marcel Pfaffhauser</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. ISQCMC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.12458" title="Abstract">arXiv:2112.12458</a> (replaced) [<a href="/pdf/2112.12458" title="Download PDF">pdf</a>, <a href="/format/2112.12458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Advantage Networks for Cooperative Multi-Agent Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Avalos%2C+R">Rapha&#xeb;l Avalos</a>, 
<a href="/search/cs?searchtype=author&query=Reymond%2C+M">Mathieu Reymond</a>, 
<a href="/search/cs?searchtype=author&query=Now%C3%A9%2C+A">Ann Now&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Roijers%2C+D+M">Diederik M. Roijers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://openreview.net/forum?id=adpKzWQunW">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research - October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.01074" title="Abstract">arXiv:2201.01074</a> (replaced) [<a href="/pdf/2201.01074" title="Download PDF">pdf</a>, <a href="/format/2201.01074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaussian Process Regression in the Flat Limit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Barthelm%C3%A9%2C+S">Simon Barthelm&#xe9;</a>, 
<a href="/search/math?searchtype=author&query=Amblard%2C+P">Pierre-Olivier Amblard</a>, 
<a href="/search/math?searchtype=author&query=Tremblay%2C+N">Nicolas Tremblay</a>, 
<a href="/search/math?searchtype=author&query=Usevich%2C+K">Konstantin Usevich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.11157" title="Abstract">arXiv:2201.11157</a> (replaced) [<a href="/pdf/2201.11157" title="Download PDF">pdf</a>, <a href="/format/2201.11157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Optimization over Submanifolds for Linearly Constrained Feedback  Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Talebi%2C+S">Shahriar Talebi</a>, 
<a href="/search/math?searchtype=author&query=Mesbahi%2C+M">Mehran Mesbahi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Differential Geometry (math.DG)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.05556" title="Abstract">arXiv:2203.05556</a> (replaced) [<a href="/pdf/2203.05556" title="Download PDF">pdf</a>, <a href="/format/2203.05556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Embeddings for Numerical Features in Tabular Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorishniy%2C+Y">Yury Gorishniy</a>, 
<a href="/search/cs?searchtype=author&query=Rubachev%2C+I">Ivan Rubachev</a>, 
<a href="/search/cs?searchtype=author&query=Babenko%2C+A">Artem Babenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2022 camera-ready. Code: <a href="https://github.com/yandex-research/tabular-dl-num-embeddings">this https URL</a> (v3-v4: minor changes)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.09249" title="Abstract">arXiv:2203.09249</a> (replaced) [<a href="/pdf/2203.09249" title="Download PDF">pdf</a>, <a href="/format/2203.09249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning Global Model via Data-Free Knowledge Distillation for  Non-IID Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+L">Ling-Yu Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by CVPR2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.03018" title="Abstract">arXiv:2205.03018</a> (replaced) [<a href="/pdf/2205.03018" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aksharantar: Open Indic-language Transliteration datasets and models for  the Next Billion Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madhani%2C+Y">Yash Madhani</a>, 
<a href="/search/cs?searchtype=author&query=Parthan%2C+S">Sushane Parthan</a>, 
<a href="/search/cs?searchtype=author&query=Bedekar%2C+P">Priyanka Bedekar</a>, 
<a href="/search/cs?searchtype=author&query=NC%2C+G">Gokul NC</a>, 
<a href="/search/cs?searchtype=author&query=Khapra%2C+R">Ruchi Khapra</a>, 
<a href="/search/cs?searchtype=author&query=Kunchukuttan%2C+A">Anoop Kunchukuttan</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Pratyush Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Khapra%2C+M+M">Mitesh M. Khapra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript is an extended version of the paper accepted to EMNLP Findings 2023. You can find the EMNLP Findings version at <a href="https://anoopkunchukuttan.gitlab.io/publications/emnlp_findings_2023_aksharantar.pdf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.03752" title="Abstract">arXiv:2205.03752</a> (replaced) [<a href="/pdf/2205.03752" title="Download PDF">pdf</a>, <a href="/format/2205.03752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Representation of Large-Alphabet Probability Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adler%2C+A">Aviv Adler</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jennifer Tang</a>, 
<a href="/search/cs?searchtype=author&query=Polyanskiy%2C+Y">Yury Polyanskiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> corrected typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.15403" title="Abstract">arXiv:2205.15403</a> (replaced) [<a href="/pdf/2205.15403" title="Download PDF">pdf</a>, <a href="/format/2205.15403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Optimal Transport with General Cost Functionals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asadulaev%2C+A">Arip Asadulaev</a>, 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>, 
<a href="/search/cs?searchtype=author&query=Egiazarian%2C+V">Vage Egiazarian</a>, 
<a href="/search/cs?searchtype=author&query=Mokrov%2C+P">Petr Mokrov</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.01878" title="Abstract">arXiv:2206.01878</a> (replaced) [<a href="/pdf/2206.01878" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Remote Collaboration Fuses Fewer Breakthrough Ideas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yiling Lin</a>, 
<a href="/search/cs?searchtype=author&query=Frey%2C+C+B">Carl Benedikt Frey</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lingfei Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.05869" title="Abstract">arXiv:2206.05869</a> (replaced) [<a href="/pdf/2206.05869" title="Download PDF">pdf</a>, <a href="/format/2206.05869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Convergence to a Global Solution of Shuffling-Type Gradient  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+L+M">Lam M. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T+H">Trang H. Tran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.04306" title="Abstract">arXiv:2207.04306</a> (replaced) [<a href="/pdf/2207.04306" title="Download PDF">pdf</a>, <a href="/format/2207.04306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Out-of-Distribution Detection in Time-Series Domain: A Novel Seasonal  Ratio Scoring Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belkhouja%2C+T">Taha Belkhouja</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Doppa%2C+J+R">Janardhan Rao Doppa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at ACM Transactions on Intelligent Systems and Technology (TIST)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.11699" title="Abstract">arXiv:2207.11699</a> (replaced) [<a href="/pdf/2207.11699" title="Download PDF">pdf</a>, <a href="/format/2207.11699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised Deep Multi-view Stereo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongbin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weitao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhipeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Haihong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Baigui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xuansong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wenxiong Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted in ACMMM-2023. The code is released at: <a href="https://github.com/ToughStoneX/Semi-MVS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.12334" title="Abstract">arXiv:2207.12334</a> (replaced) [<a href="/pdf/2207.12334" title="Download PDF">pdf</a>, <a href="/format/2207.12334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Newton-Anderson at Singular Points
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dallas%2C+M">Matt Dallas</a>, 
<a href="/search/math?searchtype=author&query=Pollock%2C+S">Sara Pollock</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 8 figures; fixed typos, added journal reference
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Int. J. Numer. Anal. Mod., 20(5), p. 667-692, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.14469" title="Abstract">arXiv:2207.14469</a> (replaced) [<a href="/pdf/2207.14469" title="Download PDF">pdf</a>, <a href="/ps/2207.14469" title="Download PostScript">ps</a>, <a href="/format/2207.14469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharp Thresholds in Adaptive Random Graph Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=MacRury%2C+C">Calum MacRury</a>, 
<a href="/search/math?searchtype=author&query=Surya%2C+E">Erlang Surya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Random Structures and Algorithms (RSA). Minor corrections made to Section 3, and the exposition of Section 4 was improved from the previous arXiv version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.01254" title="Abstract">arXiv:2208.01254</a> (replaced) [<a href="/pdf/2208.01254" title="Download PDF">pdf</a>, <a href="/format/2208.01254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Robust Morphological Approach for Semantic Segmentation of Very High  Resolution Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saravanan%2C+S">Siddharth Saravanan</a>, 
<a href="/search/cs?searchtype=author&query=Challa%2C+A">Aditya Challa</a>, 
<a href="/search/cs?searchtype=author&query=Danda%2C+S">Sravan Danda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at Computer Vision and Image Understanding
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12117" title="Abstract">arXiv:2208.12117</a> (replaced) [<a href="/pdf/2208.12117" title="Download PDF">pdf</a>, <a href="/format/2208.12117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coarser Equivalences for Causal Concurrency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farzan%2C+A">Azadeh Farzan</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+U">Umang Mathur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12616" title="Abstract">arXiv:2208.12616</a> (replaced) [<a href="/pdf/2208.12616" title="Download PDF">pdf</a>, <a href="/format/2208.12616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normative Ethics Principles for Responsible AI Systems: Taxonomy and  Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woodgate%2C+J">Jessica Woodgate</a>, 
<a href="/search/cs?searchtype=author&query=Ajmeri%2C+N">Nirav Ajmeri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.06589" title="Abstract">arXiv:2209.06589</a> (replaced) [<a href="/pdf/2209.06589" title="Download PDF">pdf</a>, <a href="/format/2209.06589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Better Generalization with Flexible Representation of  Multi-Module Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyungeun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+K">Kijung Yoon</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research (TMLR) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09493" title="Abstract">arXiv:2209.09493</a> (replaced) [<a href="/pdf/2209.09493" title="Download PDF">pdf</a>, <a href="/format/2209.09493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A framework for benchmarking clustering algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gagolewski%2C+M">Marek Gagolewski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This preprint includes some minor corrections
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SoftwareX 20 (2022) 101270
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.13959" title="Abstract">arXiv:2209.13959</a> (replaced) [<a href="/pdf/2209.13959" title="Download PDF">pdf</a>, <a href="/format/2209.13959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual  Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+F">Fengyuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruopeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weilin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Limin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) in October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.15543" title="Abstract">arXiv:2209.15543</a> (replaced) [<a href="/pdf/2209.15543" title="Download PDF">pdf</a>, <a href="/format/2209.15543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Neural Networks for Geothermal Resource Assessment: Prediction  with Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Brown%2C+S">Stephen Brown</a>, 
<a href="/search/physics?searchtype=author&query=Rodi%2C+W+L">William L. Rodi</a>, 
<a href="/search/physics?searchtype=author&query=Seracini%2C+M">Marco Seracini</a>, 
<a href="/search/physics?searchtype=author&query=Gu%2C+C">Chen Gu</a>, 
<a href="/search/physics?searchtype=author&query=Fehler%2C+M">Michael Fehler</a>, 
<a href="/search/physics?searchtype=author&query=Faulds%2C+J">James Faulds</a>, 
<a href="/search/physics?searchtype=author&query=Smith%2C+C+M">Connor M. Smith</a>, 
<a href="/search/physics?searchtype=author&query=Treitel%2C+S">Sven Treitel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09933" title="Abstract">arXiv:2210.09933</a> (replaced) [<a href="/pdf/2210.09933" title="Download PDF">pdf</a>, <a href="/format/2210.09933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explanations Based on Item Response Theory (eXirt): A Model-Specific  Method to Explain Tree-Ensemble Model in Trust Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+J">Jos&#xe9; Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Cardoso%2C+L">Lucas Cardoso</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+R">Ra&#xed;ssa Silva</a>, 
<a href="/search/cs?searchtype=author&query=Cirilo%2C+V">Vitor Cirilo</a>, 
<a href="/search/cs?searchtype=author&query=Carneiro%2C+N">N&#xed;kolas Carneiro</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+R">Ronnie Alves</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 15 Figures, 3 Equations, 7 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.10482" title="Abstract">arXiv:2210.10482</a> (replaced) [<a href="/pdf/2210.10482" title="Download PDF">pdf</a>, <a href="/format/2210.10482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Targeted Attacks for Adversarial Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minseon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+H">Hyeonjeong Ha</a>, 
<a href="/search/cs?searchtype=author&query=Son%2C+S">Sooel Son</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.10485" title="Abstract">arXiv:2210.10485</a> (replaced) [<a href="/pdf/2210.10485" title="Download PDF">pdf</a>, <a href="/format/2210.10485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Transferable Adversarial Robust Representations via Multi-view  Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minseon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+H">Hyeonjeong Ha</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D+B">Dong Bok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> *Equal contribution (Author ordering determined by coin flip). NeurIPS SafetyML workshop 2022, Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15748" title="Abstract">arXiv:2210.15748</a> (replaced) [<a href="/pdf/2210.15748" title="Download PDF">pdf</a>, <a href="/format/2210.15748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DESSERT: An Efficient Algorithm for Vector Set Search with Vector Set  Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Engels%2C+J">Joshua Engels</a>, 
<a href="/search/cs?searchtype=author&query=Coleman%2C+B">Benjamin Coleman</a>, 
<a href="/search/cs?searchtype=author&query=Lakshman%2C+V">Vihan Lakshman</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+A">Anshumali Shrivastava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available, <a href="https://github.com/ThirdAIResearch/Dessert">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.17218" title="Abstract">arXiv:2210.17218</a> (replaced) [<a href="/pdf/2210.17218" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial intelligence in government: Concepts, standards, and a  unified framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Straub%2C+V+J">Vincent J. Straub</a>, 
<a href="/search/cs?searchtype=author&query=Morgan%2C+D">Deborah Morgan</a>, 
<a href="/search/cs?searchtype=author&query=Bright%2C+J">Jonathan Bright</a>, 
<a href="/search/cs?searchtype=author&query=Margetts%2C+H">Helen Margetts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages with references and appendix, 3 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.00990" title="Abstract">arXiv:2211.00990</a> (replaced) [<a href="/pdf/2211.00990" title="Download PDF">pdf</a>, <a href="/ps/2211.00990" title="Download PostScript">ps</a>, <a href="/format/2211.00990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A weighted-variance variational autoencoder model for speech enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golmakani%2C+A">Ali Golmakani</a> (MULTISPEECH), 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+M">Mostafa Sadeghi</a> (MULTISPEECH), 
<a href="/search/cs?searchtype=author&query=Alameda-Pineda%2C+X">Xavier Alameda-Pineda</a> (ROBOTLEARN), 
<a href="/search/cs?searchtype=author&query=Serizel%2C+R">Romain Serizel</a> (MULTISPEECH)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.05321" title="Abstract">arXiv:2211.05321</a> (replaced) [<a href="/pdf/2211.05321" title="Download PDF">pdf</a>, <a href="/format/2211.05321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness and bias correction in machine learning for depression  prediction: results from four study populations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dang%2C+V+N">Vien Ngoc Dang</a>, 
<a href="/search/cs?searchtype=author&query=Cascarano%2C+A">Anna Cascarano</a>, 
<a href="/search/cs?searchtype=author&query=Mulder%2C+R+H">Rosa H. Mulder</a>, 
<a href="/search/cs?searchtype=author&query=Cecil%2C+C">Charlotte Cecil</a>, 
<a href="/search/cs?searchtype=author&query=Zuluaga%2C+M+A">Maria A. Zuluaga</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Gonz%C3%A1lez%2C+J">Jer&#xf3;nimo Hern&#xe1;ndez-Gonz&#xe1;lez</a>, 
<a href="/search/cs?searchtype=author&query=Lekadir%2C+K">Karim Lekadir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15287" title="Abstract">arXiv:2211.15287</a> (replaced) [<a href="/pdf/2211.15287" title="Download PDF">pdf</a>, <a href="/format/2211.15287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YA-DA: YAng-Based DAta Model for Fine-Grained IIoT Air Quality  Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yigit%2C+Y">Yagmur Yigit</a> (1), 
<a href="/search/cs?searchtype=author&query=Huseynov%2C+K">Khayal Huseynov</a> (1) (2), 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+H">Hamed Ahmadi</a> (3), 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a> (4) (5) ((1) Department of Computer Engineering, Istanbul Technical University, Turkey, (2) BTS Group, Istanbul, Turkey, (3) Department of Electronic Engineering, University of York, United Kingdom, (4) School of Computing, Engineering and The Build Environment, Edinburgh Napier University, United Kingdom, (5) Department of Artificial Intelligence and Data Engineering, Istanbul Technical University, Turkey)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted at the 4th Workshop on Future of Wireless Access and Sensing for Industrial IoT (FUTUREIIOT) in IEEE Global Communications Conference (IEEE GLOBECOM) 2022
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE Globecom Workshops (GC Wkshps), Rio de Janeiro, Brazil,
  2022, pp. 438-443
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.01488" title="Abstract">arXiv:2212.01488</a> (replaced) [<a href="/pdf/2212.01488" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event knowledge in large language models: the gap between the impossible  and the unlikely
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kauf%2C+C">Carina Kauf</a>, 
<a href="/search/cs?searchtype=author&query=Ivanova%2C+A+A">Anna A. Ivanova</a>, 
<a href="/search/cs?searchtype=author&query=Rambelli%2C+G">Giulia Rambelli</a>, 
<a href="/search/cs?searchtype=author&query=Chersoni%2C+E">Emmanuele Chersoni</a>, 
<a href="/search/cs?searchtype=author&query=She%2C+J+S">Jingyuan Selena She</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+Z">Zawad Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Fedorenko%2C+E">Evelina Fedorenko</a>, 
<a href="/search/cs?searchtype=author&query=Lenci%2C+A">Alessandro Lenci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The two lead authors have contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03654" title="Abstract">arXiv:2212.03654</a> (replaced) [<a href="/pdf/2212.03654" title="Download PDF">pdf</a>, <a href="/format/2212.03654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Node-oriented Spectral Filtering for Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shuai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhenfeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhizhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Youru Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yao Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures, accepted by TPAMI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.14489" title="Abstract">arXiv:2212.14489</a> (replaced) [<a href="/pdf/2212.14489" title="Download PDF">pdf</a>, <a href="/format/2212.14489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inference of interaction kernels in mean-field models of opinion  dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+W">Weiqi Chu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qin Li</a>, 
<a href="/search/cs?searchtype=author&query=Porter%2C+M+A">Mason A. Porter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Optimization and Control (math.OC); Statistics Theory (math.ST); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00157" title="Abstract">arXiv:2301.00157</a> (replaced) [<a href="/pdf/2301.00157" title="Download PDF">pdf</a>, <a href="/format/2301.00157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ponder: Point Cloud Pre-training via Neural Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Di Huang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Sida Peng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tong He</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Honghui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaowei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://dihuang.me/ponder/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03965" title="Abstract">arXiv:2301.03965</a> (replaced) [<a href="/pdf/2301.03965" title="Download PDF">pdf</a>, <a href="/format/2301.03965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiCurNet: Pre-Movement EEG based Neural Decoder for Biceps Curl  Trajectory Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Saini%2C+M">Manali Saini</a>, 
<a href="/search/eess?searchtype=author&query=Jain%2C+A">Anant Jain</a>, 
<a href="/search/eess?searchtype=author&query=Kumar%2C+L">Lalan Kumar</a>, 
<a href="/search/eess?searchtype=author&query=Muthukrishnan%2C+S+P">Suriya Prakash Muthukrishnan</a>, 
<a href="/search/eess?searchtype=author&query=Bhasin%2C+S">Shubhendu Bhasin</a>, 
<a href="/search/eess?searchtype=author&query=Roy%2C+S">Sitikantha Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.06874" title="Abstract">arXiv:2301.06874</a> (replaced) [<a href="/pdf/2301.06874" title="Download PDF">pdf</a>, <a href="/ps/2301.06874" title="Download PostScript">ps</a>, <a href="/format/2301.06874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Methods of Multi-label Prediction Classifiers for Hyperspectral  Remote Sensing Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haidar%2C+S">Salma Haidar</a>, 
<a href="/search/cs?searchtype=author&query=Oramas%2C+J">Jos&#xe9; Oramas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1- Added references. 2- updated methodology figure and added new figures to visualise the different training schemes and 3- Correcting typos 4- Revised introduction, no change in results or discussion
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08951" title="Abstract">arXiv:2301.08951</a> (replaced) [<a href="/pdf/2301.08951" title="Download PDF">pdf</a>, <a href="/format/2301.08951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Conditioned Generative Modeling of Object-Centric Representations  for Video Decomposition and Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chengmin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bin Li</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 39th Conference on Uncertainty in Artificial
  Intelligence (UAI-23), pp.613-623, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09289" title="Abstract">arXiv:2301.09289</a> (replaced) [<a href="/pdf/2301.09289" title="Download PDF">pdf</a>, <a href="/format/2301.09289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Limits of Spectral Clustering in Stochastic Block Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+A+Y">Anderson Ye Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Social and Information Networks (cs.SI); Spectral Theory (math.SP)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10813" title="Abstract">arXiv:2301.10813</a> (replaced) [<a href="/pdf/2301.10813" title="Download PDF">pdf</a>, <a href="/format/2301.10813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increasing Fairness via Combination with Learning Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yijun Bian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+A">Anqi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nanguang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 13 figures, and 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11113" title="Abstract">arXiv:2301.11113</a> (replaced) [<a href="/pdf/2301.11113" title="Download PDF">pdf</a>, <a href="/format/2301.11113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Regions of Counterfactual Explanations via Robust Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maragno%2C+D">Donato Maragno</a>, 
<a href="/search/cs?searchtype=author&query=Kurtz%2C+J">Jannis Kurtz</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6ber%2C+T+E">Tabea E. R&#xf6;ber</a>, 
<a href="/search/cs?searchtype=author&query=Goedhart%2C+R">Rob Goedhart</a>, 
<a href="/search/cs?searchtype=author&query=Birbil%2C+%C5%9E+I">&#x15e;. Ilker Birbil</a>, 
<a href="/search/cs?searchtype=author&query=Hertog%2C+D+d">Dick den Hertog</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11588" title="Abstract">arXiv:2301.11588</a> (replaced) [<a href="/pdf/2301.11588" title="Download PDF">pdf</a>, <a href="/format/2301.11588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounding Box-based Multi-objective Bayesian Optimization of Risk  Measures under Input Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Inatsu%2C+Y">Yu Inatsu</a>, 
<a href="/search/stat?searchtype=author&query=Takeno%2C+S">Shion Takeno</a>, 
<a href="/search/stat?searchtype=author&query=Hanada%2C+H">Hiroyuki Hanada</a>, 
<a href="/search/stat?searchtype=author&query=Iwata%2C+K">Kazuki Iwata</a>, 
<a href="/search/stat?searchtype=author&query=Takeuchi%2C+I">Ichiro Takeuchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12593" title="Abstract">arXiv:2301.12593</a> (replaced) [<a href="/pdf/2301.12593" title="Download PDF">pdf</a>, <a href="/format/2301.12593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk-Averse Model Uncertainty for Distributionally Robust Safe  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Queeney%2C+J">James Queeney</a>, 
<a href="/search/cs?searchtype=author&query=Benosman%2C+M">Mouhacine Benosman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12906" title="Abstract">arXiv:2301.12906</a> (replaced) [<a href="/pdf/2301.12906" title="Download PDF">pdf</a>, <a href="/format/2301.12906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curvature Filtrations for Graph Generative Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Southern%2C+J">Joshua Southern</a>, 
<a href="/search/cs?searchtype=author&query=Wayland%2C+J">Jeremy Wayland</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Rieck%2C+B">Bastian Rieck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 37th Conference on Neural Information Processing Systems (NeurIPS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Algebraic Topology (math.AT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12930" title="Abstract">arXiv:2301.12930</a> (replaced) [<a href="/pdf/2301.12930" title="Download PDF">pdf</a>, <a href="/format/2301.12930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cause-Effect Inference in Location-Scale Noise Models: Maximum  Likelihood vs. Independence Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiangyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Schulte%2C+O">Oliver Schulte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13349" title="Abstract">arXiv:2301.13349</a> (replaced) [<a href="/pdf/2301.13349" title="Download PDF">pdf</a>, <a href="/format/2301.13349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unconstrained Dynamic Regret via Sparse Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cutkosky%2C+A">Ashok Cutkosky</a>, 
<a href="/search/cs?searchtype=author&query=Paschalidis%2C+I+C">Ioannis Ch. Paschalidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01681" title="Abstract">arXiv:2302.01681</a> (replaced) [<a href="/pdf/2302.01681" title="Download PDF">pdf</a>, <a href="/format/2302.01681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the Timing Resolution of Positron Emission Tomography  Detectors Using Boosted Learning -- A Residual Physics Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naunheim%2C+S">Stephan Naunheim</a>, 
<a href="/search/cs?searchtype=author&query=Kuhl%2C+Y">Yannick Kuhl</a>, 
<a href="/search/cs?searchtype=author&query=Schug%2C+D">David Schug</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+V">Volkmar Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+F">Florian Mueller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Instrumentation and Detectors (physics.ins-det); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01757" title="Abstract">arXiv:2302.01757</a> (replaced) [<a href="/pdf/2302.01757" title="Download PDF">pdf</a>, <a href="/format/2302.01757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers  via Randomized Deletion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhuoqun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Marchant%2C+N+G">Neil G. Marchant</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+K">Keane Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+L">Lujo Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Ohrimenko%2C+O">Olga Ohrimenko</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+B+I+P">Benjamin I. P. Rubinstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in NeurIPS 2023. 36 pages, 7 figures, 12 tables. Includes 20 pages of appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02209" title="Abstract">arXiv:2302.02209</a> (replaced) [<a href="/pdf/2302.02209" title="Download PDF">pdf</a>, <a href="/ps/2302.02209" title="Download PostScript">ps</a>, <a href="/format/2302.02209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xingyue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Orth%2C+M+R">Miguel Romero Orth</a>, 
<a href="/search/cs?searchtype=author&query=Ceylan%2C+%C4%B0+%C4%B0">&#x130;smail &#x130;lkan Ceylan</a>, 
<a href="/search/cs?searchtype=author&query=Barcel%C3%B3%2C+P">Pablo Barcel&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the Thirty-Seventh Annual Conference on Advances in Neural Information Processing Systems (NeurIPS 2023). Code available at: <a href="https://github.com/HxyScotthuang/CMPNN">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03857" title="Abstract">arXiv:2302.03857</a> (replaced) [<a href="/pdf/2302.03857" title="Download PDF">pdf</a>, <a href="/format/2302.03857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xilie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>, 
<a href="/search/cs?searchtype=author&query=Kankanhalli%2C+M">Mohan Kankanhalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04089" title="Abstract">arXiv:2302.04089</a> (replaced) [<a href="/pdf/2302.04089" title="Download PDF">pdf</a>, <a href="/format/2302.04089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZipLM: Inference-Aware Structured Pruning of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurtic%2C+E">Eldar Kurtic</a>, 
<a href="/search/cs?searchtype=author&query=Frantar%2C+E">Elias Frantar</a>, 
<a href="/search/cs?searchtype=author&query=Alistarh%2C+D">Dan Alistarh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04449" title="Abstract">arXiv:2302.04449</a> (replaced) [<a href="/pdf/2302.04449" title="Download PDF">pdf</a>, <a href="/format/2302.04449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Read and Reap the Rewards: Learning to Play Atari with the Help of  Instruction Manuals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yewen Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P+P">Paul Pu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Azaria%2C+A">Amos Azaria</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+T+M">Tom M. Mitchell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04824" title="Abstract">arXiv:2302.04824</a> (replaced) [<a href="/pdf/2302.04824" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lithium Metal Battery Quality Control via Transformer-CNN Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quenum%2C+J">Jerome Quenum</a>, 
<a href="/search/cs?searchtype=author&query=Zenyuk%2C+I">Iryna Zenyuk</a>, 
<a href="/search/cs?searchtype=author&query=Ushizima%2C+D">Daniela Ushizima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 12 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Imaging - Special Issue Computer Vision and Deep
  Learning: Trends and Applications 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06961" title="Abstract">arXiv:2302.06961</a> (replaced) [<a href="/pdf/2302.06961" title="Download PDF">pdf</a>, <a href="/format/2302.06961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical  Awareness for Robust Fovea Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Sifan Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shaopeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jionglong Su</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xiaowei Ding</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+K">Kang Dang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is prepared for IEEE Transactions on Biomedical Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08014" title="Abstract">arXiv:2302.08014</a> (replaced) [<a href="/pdf/2302.08014" title="Download PDF">pdf</a>, <a href="/format/2302.08014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy conserving/stable schemes for a vector-kinetic model of  hyperbolic systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Anandan%2C+M">Megala Anandan</a>, 
<a href="/search/math?searchtype=author&query=Rao%2C+S+V+R">S. V. Raghurama Rao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13534" title="Abstract">arXiv:2302.13534</a> (replaced) [<a href="/pdf/2302.13534" title="Download PDF">pdf</a>, <a href="/ps/2302.13534" title="Download PostScript">ps</a>, <a href="/format/2302.13534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL  with General Regularizers and Multiple Optimal Arms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+T">Tiancheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junyan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haipeng Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update the camera-ready version for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00198" title="Abstract">arXiv:2303.00198</a> (replaced) [<a href="/pdf/2303.00198" title="Download PDF">pdf</a>, <a href="/format/2303.00198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutional Visual Prompt for Robust Visual Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y">Yun-Yun Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+C">Chengzhi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junfeng Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00905" title="Abstract">arXiv:2303.00905</a> (replaced) [<a href="/pdf/2303.00905" title="Download PDF">pdf</a>, <a href="/format/2303.00905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-World Object Manipulation using Pre-trained Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stone%2C+A">Austin Stone</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Ted Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+K">Keerthana Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kuang-Huei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Vuong%2C+Q">Quan Vuong</a>, 
<a href="/search/cs?searchtype=author&query=Wohlhart%2C+P">Paul Wohlhart</a>, 
<a href="/search/cs?searchtype=author&query=Kirmani%2C+S">Sean Kirmani</a>, 
<a href="/search/cs?searchtype=author&query=Zitkovich%2C+B">Brianna Zitkovich</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Hausman%2C+K">Karol Hausman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 7th Conference on Robot Learning (CoRL 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02269" title="Abstract">arXiv:2303.02269</a> (replaced) [<a href="/pdf/2303.02269" title="Download PDF">pdf</a>, <a href="/ps/2303.02269" title="Download PostScript">ps</a>, <a href="/format/2303.02269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Information-Theoretic Characterization of MIMO-FAS: Optimization,  Diversity-Multiplexing Tradeoff and $q$-Outage Capacity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=New%2C+W+K">Wee Kiat New</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kai-Kit Wong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+K">Kin-Fai Tong</a>, 
<a href="/search/cs?searchtype=author&query=Chae%2C+C">Chan-Byoung Chae</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 12 figures, 2 tables, 1 algorithm. Accepted by IEEE Transactions on Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02893" title="Abstract">arXiv:2303.02893</a> (replaced) [<a href="/pdf/2303.02893" title="Download PDF">pdf</a>, <a href="/format/2303.02893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-shot Adaptation for Manipulating Granular Materials Under Domain  Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Thangeda%2C+P">Pranay Thangeda</a>, 
<a href="/search/cs?searchtype=author&query=Ornik%2C+M">Melkior Ornik</a>, 
<a href="/search/cs?searchtype=author&query=Hauser%2C+K">Kris Hauser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03284" title="Abstract">arXiv:2303.03284</a> (replaced) [<a href="/pdf/2303.03284" title="Download PDF">pdf</a>, <a href="/format/2303.03284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Wasserstein Believer: Learning Belief Updates for Partially  Observable Environments through Reliable Latent Space Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Avalos%2C+R">Raphael Avalos</a>, 
<a href="/search/cs?searchtype=author&query=Delgrange%2C+F">Florent Delgrange</a>, 
<a href="/search/cs?searchtype=author&query=Now%C3%A9%2C+A">Ann Now&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+G+A">Guillermo A. P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Roijers%2C+D+M">Diederik M. Roijers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04053" title="Abstract">arXiv:2303.04053</a> (replaced) [<a href="/pdf/2303.04053" title="Download PDF">pdf</a>, <a href="/format/2303.04053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Describe me an Aucklet: Generating Grounded Perceptual Category  Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noble%2C+B">Bill Noble</a>, 
<a href="/search/cs?searchtype=author&query=Ilinykh%2C+N">Nikolai Ilinykh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP, Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04658" title="Abstract">arXiv:2303.04658</a> (replaced) [<a href="/pdf/2303.04658" title="Download PDF">pdf</a>, <a href="/format/2303.04658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Localization in Unstructured Environments using Semantic Object  Maps Built from Various Viewpoints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ankenbauer%2C+J">Jacqueline Ankenbauer</a>, 
<a href="/search/cs?searchtype=author&query=Lusk%2C+P+C">Parker C. Lusk</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+A">Annika Thomas</a>, 
<a href="/search/cs?searchtype=author&query=How%2C+J+P">Jonathan P. How</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, presented at IROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04909" title="Abstract">arXiv:2303.04909</a> (replaced) [<a href="/pdf/2303.04909" title="Download PDF">pdf</a>, <a href="/format/2303.04909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robotic Fabric Flattening with Wrinkle Direction Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yulei Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jihong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Della+Santina%2C+C">Cosimo Della Santina</a>, 
<a href="/search/cs?searchtype=author&query=Gienger%2C+M">Michael Gienger</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 18th International Symposium on Experimental Robotics (ISER 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05683" title="Abstract">arXiv:2303.05683</a> (replaced) [<a href="/pdf/2303.05683" title="Download PDF">pdf</a>, <a href="/format/2303.05683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical clustering with OWA-based linkages, the Lance-Williams  formula, and dendrogram inversions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gagolewski%2C+M">Marek Gagolewski</a>, 
<a href="/search/stat?searchtype=author&query=Cena%2C+A">Anna Cena</a>, 
<a href="/search/stat?searchtype=author&query=James%2C+S">Simon James</a>, 
<a href="/search/stat?searchtype=author&query=Beliakov%2C+G">Gleb Beliakov</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Fuzzy Sets and Systems 473, 108740, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07914" title="Abstract">arXiv:2303.07914</a> (replaced) [<a href="/pdf/2303.07914" title="Download PDF">pdf</a>, <a href="/format/2303.07914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting Offline Speech Translation Models for Streaming with  Future-Aware Distillation and Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+B">Biao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+M">Minpeng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+K">Kai Fan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhongqiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yidong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaodong Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accept to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07963" title="Abstract">arXiv:2303.07963</a> (replaced) [<a href="/pdf/2303.07963" title="Download PDF">pdf</a>, <a href="/format/2303.07963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoCNet: 3D Robust Registration of Point-Clouds using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Slimani%2C+K">Karim Slimani</a>, 
<a href="/search/cs?searchtype=author&query=Tamadazte%2C+B">Brahim Tamadazte</a>, 
<a href="/search/cs?searchtype=author&query=Achard%2C+C">Catherine Achard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09412" title="Abstract">arXiv:2303.09412</a> (replaced) [<a href="/pdf/2303.09412" title="Download PDF">pdf</a>, <a href="/format/2303.09412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing  Diverse Intrinsic and Extrinsic Camera Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schieber%2C+H">Hannah Schieber</a>, 
<a href="/search/cs?searchtype=author&query=Deuser%2C+F">Fabian Deuser</a>, 
<a href="/search/cs?searchtype=author&query=Egger%2C+B">Bernhard Egger</a>, 
<a href="/search/cs?searchtype=author&query=Oswald%2C+N">Norbert Oswald</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Daniel Roth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10500" title="Abstract">arXiv:2303.10500</a> (replaced) [<a href="/pdf/2303.10500" title="Download PDF">pdf</a>, <a href="/format/2303.10500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blockchain-Based, Confidentiality-Preserving Orchestration of  Collaborative Workflows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toldi%2C+B+%C3%81">Bal&#xe1;zs &#xc1;d&#xe1;m Toldi</a>, 
<a href="/search/cs?searchtype=author&query=Kocsis%2C+I">Imre Kocsis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11207" title="Abstract">arXiv:2303.11207</a> (replaced) [<a href="/pdf/2303.11207" title="Download PDF">pdf</a>, <a href="/format/2303.11207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Topological Order using Recurrent Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Hibat-Allah%2C+M">Mohamed Hibat-Allah</a>, 
<a href="/search/cond-mat?searchtype=author&query=Melko%2C+R+G">Roger G. Melko</a>, 
<a href="/search/cond-mat?searchtype=author&query=Carrasquilla%2C+J">Juan Carrasquilla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 6 figures, 2 tables. Published version in Physical Review B
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. B 108, 075152, August 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Strongly Correlated Electrons (cond-mat.str-el)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Computational Physics (physics.comp-ph); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11249" title="Abstract">arXiv:2303.11249</a> (replaced) [<a href="/pdf/2303.11249" title="Download PDF">pdf</a>, <a href="/format/2303.11249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Makes Data Suitable for a Locally Connected Neural Network? A  Necessary and Sufficient Condition Based on Quantum Entanglement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alexander%2C+Y">Yotam Alexander</a>, 
<a href="/search/cs?searchtype=author&query=De+La+Vega%2C+N">Nimrod De La Vega</a>, 
<a href="/search/cs?searchtype=author&query=Razin%2C+N">Noam Razin</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+N">Nadav Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11888" title="Abstract">arXiv:2303.11888</a> (replaced) [<a href="/pdf/2303.11888" title="Download PDF">pdf</a>, <a href="/format/2303.11888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Penalty-Based Imitation Learning With Cross Semantics Generation Sensor  Fusion for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongkuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+A">Aifen Sui</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Letian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinxian Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13001" title="Abstract">arXiv:2303.13001</a> (replaced) [<a href="/pdf/2303.13001" title="Download PDF">pdf</a>, <a href="/format/2303.13001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT A Good Keyphrase Generator? A Preliminary Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haiyun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Songfang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shilong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huafeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+L">Liping Jing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report, 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13497" title="Abstract">arXiv:2303.13497</a> (replaced) [<a href="/pdf/2303.13497" title="Download PDF">pdf</a>, <a href="/format/2303.13497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TriPlaneNet: An Encoder for EG3D Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+A+R">Ananta R. Bhattarai</a>, 
<a href="/search/cs?searchtype=author&query=Nie%C3%9Fner%2C+M">Matthias Nie&#xdf;ner</a>, 
<a href="/search/cs?searchtype=author&query=Sevastopolsky%2C+A">Artem Sevastopolsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://anantarb.github.io/triplanenet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14188" title="Abstract">arXiv:2303.14188</a> (replaced) [<a href="/pdf/2303.14188" title="Download PDF">pdf</a>, <a href="/format/2303.14188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Few Demonstrations with Frame-Weighted Motion Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jianyong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>, 
<a href="/search/cs?searchtype=author&query=Gienger%2C+M">Michael Gienger</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jihong Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ISER. For the experiment video, see <a href="https://youtu.be/JpGjk4eKC3o">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15027" title="Abstract">arXiv:2303.15027</a> (replaced) [<a href="/pdf/2303.15027" title="Download PDF">pdf</a>, <a href="/format/2303.15027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Causal Discovery Methods for I.I.D. and Time Series Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+U">Uzma Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Hossain%2C+E">Emam Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Gani%2C+M+O">Md Osman Gani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15972" title="Abstract">arXiv:2303.15972</a> (replaced) [<a href="/pdf/2303.15972" title="Download PDF">pdf</a>, <a href="/format/2303.15972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coordinated Multi-Robot Shared Autonomy Based on Scheduling and  Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hagenow%2C+M">Michael Hagenow</a>, 
<a href="/search/cs?searchtype=author&query=Senft%2C+E">Emmanuel Senft</a>, 
<a href="/search/cs?searchtype=author&query=Orr%2C+N">Nitzan Orr</a>, 
<a href="/search/cs?searchtype=author&query=Radwin%2C+R">Robert Radwin</a>, 
<a href="/search/cs?searchtype=author&query=Gleicher%2C+M">Michael Gleicher</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+B">Bilge Mutlu</a>, 
<a href="/search/cs?searchtype=author&query=Losey%2C+D+P">Dylan P. Losey</a>, 
<a href="/search/cs?searchtype=author&query=Zinn%2C+M">Michael Zinn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Robotics and Automation Letters (RA-L)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00223" title="Abstract">arXiv:2304.00223</a> (replaced) [<a href="/e-print/2304.00223" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Limits of Holographic MIMO Channels: Tackling Non-Separable  Transceiver Correlation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shenghui Song</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Further revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01091" title="Abstract">arXiv:2304.01091</a> (replaced) [<a href="/pdf/2304.01091" title="Download PDF">pdf</a>, <a href="/format/2304.01091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Changes to Captions: An Attentive Network for Remote Sensing Change  Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shizhen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Ghamisi%2C+P">Pedram Ghamisi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03659" title="Abstract">arXiv:2304.03659</a> (replaced) [<a href="/pdf/2304.03659" title="Download PDF">pdf</a>, <a href="/format/2304.03659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing Conceptual Understanding of Large Visual-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schiappa%2C+M+C">Madeline Chantry Schiappa</a>, 
<a href="/search/cs?searchtype=author&query=Cogswell%2C+M">Michael Cogswell</a>, 
<a href="/search/cs?searchtype=author&query=Divakaran%2C+A">Ajay Divakaran</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+Y+S">Yogesh Singh Rawat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> All code and dataset is available at: <a href="https://tinyurl.com/vlm-robustness">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03965" title="Abstract">arXiv:2304.03965</a> (replaced) [<a href="/pdf/2304.03965" title="Download PDF">pdf</a>, <a href="/format/2304.03965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The n-vehicle exploration problem is NP-complete
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jinchuan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoya Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06700" title="Abstract">arXiv:2304.06700</a> (replaced) [<a href="/pdf/2304.06700" title="Download PDF">pdf</a>, <a href="/format/2304.06700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Control3Diff: Learning Controllable 3D Diffusion Models from Single-view  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiatao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Q">Qingzhe Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+S">Shuangfei Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baoquan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+J">Josh Susskind</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 3DV24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08965" title="Abstract">arXiv:2304.08965</a> (replaced) [<a href="/pdf/2304.08965" title="Download PDF">pdf</a>, <a href="/format/2304.08965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PointDC:Unsupervised Semantic Segmentation of 3D Point Clouds via  Cross-modal Distillation and Super-Voxel Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zisheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongbin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weitao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhipeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Haihong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Baigui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xuansong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wenxiong Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by International Conference on Computer Vision (ICCV) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11411" title="Abstract">arXiv:2304.11411</a> (replaced) [<a href="/pdf/2304.11411" title="Download PDF">pdf</a>, <a href="/format/2304.11411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Spoilers in Movie Reviews with External Movie Knowledge and  User Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Heng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yuyang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qinghua Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Minnan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12406" title="Abstract">arXiv:2304.12406</a> (replaced) [<a href="/pdf/2304.12406" title="Download PDF">pdf</a>, <a href="/format/2304.12406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoFocusFormer: Image Segmentation off the Grid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziwen%2C+C">Chen Ziwen</a>, 
<a href="/search/cs?searchtype=author&query=Patnaik%2C+K">Kaushik Patnaik</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+S">Shuangfei Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+A">Alvin Wan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhile Ren</a>, 
<a href="/search/cs?searchtype=author&query=Schwing%2C+A">Alex Schwing</a>, 
<a href="/search/cs?searchtype=author&query=Colburn%2C+A">Alex Colburn</a>, 
<a href="/search/cs?searchtype=author&query=Fuxin%2C+L">Li Fuxin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12470" title="Abstract">arXiv:2304.12470</a> (replaced) [<a href="/pdf/2304.12470" title="Download PDF">pdf</a>, <a href="/format/2304.12470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent Transformer Encoders for Vision-based Estimation of Fatigue  and Engagement in Cognitive Training Sessions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yunlong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F+V">Feng Vankee Lin</a>, 
<a href="/search/cs?searchtype=author&query=Adeli%2C+E">Ehsan Adeli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12760" title="Abstract">arXiv:2304.12760</a> (replaced) [<a href="/pdf/2304.12760" title="Download PDF">pdf</a>, <a href="/format/2304.12760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel Spiking Neurons with High Efficiency and Ability to Learn  Long-term Dependencies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+W">Wei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhaofei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhaokun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Ding Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhengyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Masquelier%2C+T">Timoth&#xe9;e Masquelier</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonghong Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00633" title="Abstract">arXiv:2305.00633</a> (replaced) [<a href="/pdf/2305.00633" title="Download PDF">pdf</a>, <a href="/format/2305.00633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Evaluation Guided Beam Search for Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuxi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiran Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+M">Min-Yen Kan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junxian He</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qizhe Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. 10 pages, 7 figures, 4 tables (33 pages, 14 figures, 15 tables including references and appendices)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02324" title="Abstract">arXiv:2305.02324</a> (replaced) [<a href="/pdf/2305.02324" title="Download PDF">pdf</a>, <a href="/format/2305.02324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Stream Contrastive Learning for Self-Supervised Skeleton-Based  Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Ding Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yongqiang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wensheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02531" title="Abstract">arXiv:2305.02531</a> (replaced) [<a href="/pdf/2305.02531" title="Download PDF">pdf</a>, <a href="/ps/2305.02531" title="Download PostScript">ps</a>, <a href="/format/2305.02531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Capture Intertemporal Preferences?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goli%2C+A">Ali Goli</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Amandeep Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03598" title="Abstract">arXiv:2305.03598</a> (replaced) [<a href="/pdf/2305.03598" title="Download PDF">pdf</a>, <a href="/format/2305.03598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial  Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jullien%2C+M">Ma&#xeb;l Jullien</a>, 
<a href="/search/cs?searchtype=author&query=Valentino%2C+M">Marco Valentino</a>, 
<a href="/search/cs?searchtype=author&query=Frost%2C+H">Hannah Frost</a>, 
<a href="/search/cs?searchtype=author&query=O%27Regan%2C+P">Paul O&#x27;Regan</a>, 
<a href="/search/cs?searchtype=author&query=Landers%2C+D">Donal Landers</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+A">Andr&#xe9; Freitas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04247" title="Abstract">arXiv:2305.04247</a> (replaced) [<a href="/pdf/2305.04247" title="Download PDF">pdf</a>, <a href="/format/2305.04247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimation of control area in badminton doubles with pose information  from top and back view drone videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+N">Ning Ding</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+K">Kazuya Takeda</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wenhui Jin</a>, 
<a href="/search/cs?searchtype=author&query=Bei%2C+Y">Yingjiu Bei</a>, 
<a href="/search/cs?searchtype=author&query=Fujii%2C+K">Keisuke Fujii</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures, to appear in Multimedia Tools and Applications
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Multimedia Tools and Applications (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04798" title="Abstract">arXiv:2305.04798</a> (replaced) [<a href="/pdf/2305.04798" title="Download PDF">pdf</a>, <a href="/format/2305.04798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-grained Hypergraph Interest Modeling for Conversational  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shang%2C+C">Chenzhan Shang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yupeng Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07224" title="Abstract">arXiv:2305.07224</a> (replaced) [<a href="/pdf/2305.07224" title="Download PDF">pdf</a>, <a href="/format/2305.07224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymmetric feature interaction for interpreting model predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xiaolei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianghong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haode Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of the Association for Computational Linguistics: ACL 2023 (long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08175" title="Abstract">arXiv:2305.08175</a> (replaced) [<a href="/pdf/2305.08175" title="Download PDF">pdf</a>, <a href="/ps/2305.08175" title="Download PostScript">ps</a>, <a href="/format/2305.08175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optimal and Scalable Matrix Mechanism for Noisy Marginals under  Convex Loss Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yingtai Xiao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guanlin He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Danfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kifer%2C+D">Daniel Kifer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09758" title="Abstract">arXiv:2305.09758</a> (replaced) [<a href="/pdf/2305.09758" title="Download PDF">pdf</a>, <a href="/format/2305.09758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In  Zero Shot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Aanisha Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+Y+K">Yaman K Singla</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+B">Balaji Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+R+R">Rajiv Ratn Shah</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changyou Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP-23 TL;DR: Video understanding lags far behind NLP; LLMs excel in zero-shot. Our approach utilizes LLMs to verbalize videos, creating stories for zero-shot video understanding. This yields state-of-the-art results across five datasets, covering fifteen tasks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10355" title="Abstract">arXiv:2305.10355</a> (replaced) [<a href="/pdf/2305.10355" title="Download PDF">pdf</a>, <a href="/format/2305.10355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Object Hallucination in Large Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifan Li</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yifan Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11531" title="Abstract">arXiv:2305.11531</a> (replaced) [<a href="/pdf/2305.11531" title="Download PDF">pdf</a>, <a href="/format/2305.11531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing to new geometries with Geometry-Aware Autoregressive Models  (GAAMs) for fast calorimeter simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Liu%2C+J">Junze Liu</a>, 
<a href="/search/physics?searchtype=author&query=Ghosh%2C+A">Aishik Ghosh</a>, 
<a href="/search/physics?searchtype=author&query=Smith%2C+D">Dylan Smith</a>, 
<a href="/search/physics?searchtype=author&query=Baldi%2C+P">Pierre Baldi</a>, 
<a href="/search/physics?searchtype=author&query=Whiteson%2C+D">Daniel Whiteson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Detectors (physics.ins-det)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); High Energy Physics - Phenomenology (hep-ph); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11685" title="Abstract">arXiv:2305.11685</a> (replaced) [<a href="/pdf/2305.11685" title="Download PDF">pdf</a>, <a href="/format/2305.11685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recycle-and-Distill: Universal Compression Strategy for  Transformer-based Speech SSL Models with Attention Map Reusing and Masking  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jang%2C+K">Kangwook Jang</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+S">Sungnyun Kim</a>, 
<a href="/search/eess?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+H">Hoirin Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of Interspeech 2023. Code URL: <a href="https://github.com/sungnyun/ARMHuBERT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11982" title="Abstract">arXiv:2305.11982</a> (replaced) [<a href="/pdf/2305.11982" title="Download PDF">pdf</a>, <a href="/format/2305.11982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Memory with Temporal Predictive Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Tang%2C+M">Mufeng Tang</a>, 
<a href="/search/q-bio?searchtype=author&query=Barron%2C+H">Helen Barron</a>, 
<a href="/search/q-bio?searchtype=author&query=Bogacz%2C+R">Rafal Bogacz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12283" title="Abstract">arXiv:2305.12283</a> (replaced) [<a href="/pdf/2305.12283" title="Download PDF">pdf</a>, <a href="/ps/2305.12283" title="Download PostScript">ps</a>, <a href="/format/2305.12283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution-Free Model-Agnostic Regression Calibration via  Nonparametric Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhongze Cai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaocheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 and update a camera-ready version; Add some experiments and literature reviews
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12945" title="Abstract">arXiv:2305.12945</a> (replaced) [<a href="/pdf/2305.12945" title="Download PDF">pdf</a>, <a href="/format/2305.12945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist  Examination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongfang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jindi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13082" title="Abstract">arXiv:2305.13082</a> (replaced) [<a href="/pdf/2305.13082" title="Download PDF">pdf</a>, <a href="/format/2305.13082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sketch-and-Project Meets Newton Method: Global $\mathcal O(k^{-2})$  Convergence with Low-Rank Updates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hanzely%2C+S">Slavom&#xed;r Hanzely</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13272" title="Abstract">arXiv:2305.13272</a> (replaced) [<a href="/pdf/2305.13272" title="Download PDF">pdf</a>, <a href="/format/2305.13272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLASS: A Design Framework for building Intelligent Tutoring Systems  based on Learning Science principles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sonkar%2C+S">Shashank Sonkar</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Naiming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mallick%2C+D+B">Debshila Basu Mallick</a>, 
<a href="/search/cs?searchtype=author&query=Baraniuk%2C+R+G">Richard G. Baraniuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13507" title="Abstract">arXiv:2305.13507</a> (replaced) [<a href="/pdf/2305.13507" title="Download PDF">pdf</a>, <a href="/format/2305.13507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Automated Fact-Checking: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+M">Mubashara Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Schlichtkrull%2C+M">Michael Schlichtkrull</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cocarascu%2C+O">Oana Cocarascu</a>, 
<a href="/search/cs?searchtype=author&query=Simperl%2C+E">Elena Simperl</a>, 
<a href="/search/cs?searchtype=author&query=Vlachos%2C+A">Andreas Vlachos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP): Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13552" title="Abstract">arXiv:2305.13552</a> (replaced) [<a href="/pdf/2305.13552" title="Download PDF">pdf</a>, <a href="/format/2305.13552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Squared Neural Families: A New Class of Tractable Density Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsuchida%2C+R">Russell Tsuchida</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+C+S">Cheng Soon Ong</a>, 
<a href="/search/cs?searchtype=author&query=Sejdinovic%2C+D">Dino Sejdinovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Spotlight award at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13632" title="Abstract">arXiv:2305.13632</a> (replaced) [<a href="/pdf/2305.13632" title="Download PDF">pdf</a>, <a href="/format/2305.13632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting and Mitigating Hallucinations in Multilingual Summarisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yifu Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Ziser%2C+Y">Yftah Ziser</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Ponti%2C+E+M">Edoardo M. Ponti</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S+B">Shay B. Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13757" title="Abstract">arXiv:2305.13757</a> (replaced) [<a href="/pdf/2305.13757" title="Download PDF">pdf</a>, <a href="/format/2305.13757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Field Channel Estimation for Ultra Massive-MIMO THz Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tarboush%2C+S">Simon Tarboush</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+A">Anum Ali</a>, 
<a href="/search/cs?searchtype=author&query=Al-Naffouri%2C+T+Y">Tareq Y. Al-Naffouri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13850" title="Abstract">arXiv:2305.13850</a> (replaced) [<a href="/pdf/2305.13850" title="Download PDF">pdf</a>, <a href="/format/2305.13850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Structure Knowledge-Guided Relation Extraction Method for  Visually-Rich Document
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangnan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Q">Qian Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juncheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+D">Duo Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaozhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14077" title="Abstract">arXiv:2305.14077</a> (replaced) [<a href="/pdf/2305.14077" title="Download PDF">pdf</a>, <a href="/format/2305.14077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mind the spikes: Benign overfitting of kernels and neural networks in  fixed dimension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Haas%2C+M">Moritz Haas</a>, 
<a href="/search/stat?searchtype=author&query=Holzm%C3%BCller%2C+D">David Holzm&#xfc;ller</a>, 
<a href="/search/stat?searchtype=author&query=von+Luxburg%2C+U">Ulrike von Luxburg</a>, 
<a href="/search/stat?searchtype=author&query=Steinwart%2C+I">Ingo Steinwart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We provide Python code to reproduce all of our experimental results at <a href="https://github.com/moritzhaas/mind-the-spikes">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14267" title="Abstract">arXiv:2305.14267</a> (replaced) [<a href="/pdf/2305.14267" title="Download PDF">pdf</a>, <a href="/format/2305.14267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+M">Martin Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+N">Nelson Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T">Thuy Tran</a>, 
<a href="/search/cs?searchtype=author&query=Gherbi%2C+E">Elies Gherbi</a>, 
<a href="/search/cs?searchtype=author&query=Hajri%2C+H">Hatem Hajri</a>, 
<a href="/search/cs?searchtype=author&query=Masmoudi%2C+N">Nader Masmoudi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages. Camera-Ready version for the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14327" title="Abstract">arXiv:2305.14327</a> (replaced) [<a href="/pdf/2305.14327" title="Download PDF">pdf</a>, <a href="/format/2305.14327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Da Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+F">Fan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+M">Ming Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+H">Hritik Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. Code and data are available at <a href="https://github.com/WadeYin9712/Dynosaur">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14331" title="Abstract">arXiv:2305.14331</a> (replaced) [<a href="/pdf/2305.14331" title="Download PDF">pdf</a>, <a href="/format/2305.14331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Else Do I Need to Know? The Effect of Background Information on  Users&#x27; Reliance on QA Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goyal%2C+N">Navita Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Briakou%2C+E">Eleftheria Briakou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Amanda Liu</a>, 
<a href="/search/cs?searchtype=author&query=Baumler%2C+C">Connor Baumler</a>, 
<a href="/search/cs?searchtype=author&query=Bonial%2C+C">Claire Bonial</a>, 
<a href="/search/cs?searchtype=author&query=Micher%2C+J">Jeffrey Micher</a>, 
<a href="/search/cs?searchtype=author&query=Voss%2C+C+R">Clare R. Voss</a>, 
<a href="/search/cs?searchtype=author&query=Carpuat%2C+M">Marine Carpuat</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14702" title="Abstract">arXiv:2305.14702</a> (replaced) [<a href="/pdf/2305.14702" title="Download PDF">pdf</a>, <a href="/format/2305.14702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DecipherPref: Analyzing Influential Factors in Human Preference  Judgments via GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yebowen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kaiqiang Song</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Sangwoo Cho</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Foroosh%2C+H">Hassan Foroosh</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14802" title="Abstract">arXiv:2305.14802</a> (replaced) [<a href="/pdf/2305.14802" title="Download PDF">pdf</a>, <a href="/format/2305.14802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Large Language Model Capabilities without Labeled Test Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+H+Y">Harvey Yiyun Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinyuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+A">Albert Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Robin Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings. Camera-ready version. Code: <a href="https://github.com/harvey-fin/icl-estimate">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14858" title="Abstract">arXiv:2305.14858</a> (replaced) [<a href="/pdf/2305.14858" title="Download PDF">pdf</a>, <a href="/format/2305.14858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient  Pre-LN Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zixuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiaqi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+D+Z">David Z. Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight. Code is available at <a href="https://github.com/ZixuanJiang/pre-rmsnorm-transformer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14907" title="Abstract">arXiv:2305.14907</a> (replaced) [<a href="/pdf/2305.14907" title="Download PDF">pdf</a>, <a href="/format/2305.14907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coverage-based Example Selection for In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shivanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Gardner%2C+M">Matt Gardner</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sameer Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14943" title="Abstract">arXiv:2305.14943</a> (replaced) [<a href="/pdf/2305.14943" title="Download PDF">pdf</a>, <a href="/format/2305.14943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Rate Free Bayesian Inference in Constrained Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sharrock%2C+L">Louis Sharrock</a>, 
<a href="/search/stat?searchtype=author&query=Mackey%2C+L">Lester Mackey</a>, 
<a href="/search/stat?searchtype=author&query=Nemeth%2C+C">Christopher Nemeth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14956" title="Abstract">arXiv:2305.14956</a> (replaced) [<a href="/pdf/2305.14956" title="Download PDF">pdf</a>, <a href="/format/2305.14956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Editing Common Sense in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Anshita Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+D">Debanjan Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Sheshadri%2C+A+K">Akshay Krishna Sheshadri</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenlong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X+L">Xiang Lorraine Li</a>, 
<a href="/search/cs?searchtype=author&query=Wiegreffe%2C+S">Sarah Wiegreffe</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+N">Niket Tandon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference. Anshita, Debanjan, Akshay are co-first authors. Code and datasets for all experiments are available at <a href="https://github.com/anshitag/memit_csk">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15064" title="Abstract">arXiv:2305.15064</a> (replaced) [<a href="/pdf/2305.15064" title="Download PDF">pdf</a>, <a href="/format/2305.15064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Siqi Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15080" title="Abstract">arXiv:2305.15080</a> (replaced) [<a href="/pdf/2305.15080" title="Download PDF">pdf</a>, <a href="/format/2305.15080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visually-Situated Natural Language Understanding with Contrastive  Reading Model and Frozen Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Geewook Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hodong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Daehee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Haeji Jung</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sanghee Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoonsik Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>, 
<a href="/search/cs?searchtype=author&query=Kil%2C+T">Taeho Kil</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Bado Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seunghyun Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages; To appear at EMNLP 2023 Main Conference (Project page: <a href="https://naver-ai.github.io/cream">this https URL</a> )
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15134" title="Abstract">arXiv:2305.15134</a> (replaced) [<a href="/pdf/2305.15134" title="Download PDF">pdf</a>, <a href="/format/2305.15134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Networks are Slacking Off: Understanding Generalization Problem in Image  Deraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xianzheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+X">Xiangtao Kong</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chao Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article has been accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15807" title="Abstract">arXiv:2305.15807</a> (replaced) [<a href="/pdf/2305.15807" title="Download PDF">pdf</a>, <a href="/format/2305.15807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with  Application to Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chzhen%2C+E">Evgenii Chzhen</a> (LMO, CELESTE), 
<a href="/search/stat?searchtype=author&query=Giraud%2C+C">Christophe Giraud</a> (LMO, CELESTE), 
<a href="/search/stat?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/stat?searchtype=author&query=Stoltz%2C+G">Gilles Stoltz</a> (LMO, CELESTE, HEC Paris)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems, Dec 2023, New
  Orleans, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15814" title="Abstract">arXiv:2305.15814</a> (replaced) [<a href="/pdf/2305.15814" title="Download PDF">pdf</a>, <a href="/format/2305.15814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bhasha-Abhijnaanam: Native-script and romanized Language Identification  for 22 Indic languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madhani%2C+Y">Yash Madhani</a>, 
<a href="/search/cs?searchtype=author&query=Khapra%2C+M+M">Mitesh M. Khapra</a>, 
<a href="/search/cs?searchtype=author&query=Kunchukuttan%2C+A">Anoop Kunchukuttan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16150" title="Abstract">arXiv:2305.16150</a> (replaced) [<a href="/pdf/2305.16150" title="Download PDF">pdf</a>, <a href="/format/2305.16150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying GANs and Score-Based Diffusion as Generative Particle Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Franceschi%2C+J">Jean-Yves Franceschi</a>, 
<a href="/search/cs?searchtype=author&query=Gartrell%2C+M">Mike Gartrell</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+L+D">Ludovic Dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Issenhuth%2C+T">Thibaut Issenhuth</a>, 
<a href="/search/cs?searchtype=author&query=de+B%C3%A9zenac%2C+E">Emmanuel de B&#xe9;zenac</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Micka&#xeb;l Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rakotomamonjy%2C+A">Alain Rakotomamonjy</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Thirty-seventh Conference on Neural Information Processing
  Systems, Neural Information Processing Systems Foundation, Dec. 2023, New
  Orleans, LA, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16264" title="Abstract">arXiv:2305.16264</a> (replaced) [<a href="/pdf/2305.16264" title="Download PDF">pdf</a>, <a href="/format/2305.16264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Data-Constrained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
<a href="/search/cs?searchtype=author&query=Rush%2C+A+M">Alexander M. Rush</a>, 
<a href="/search/cs?searchtype=author&query=Barak%2C+B">Boaz Barak</a>, 
<a href="/search/cs?searchtype=author&query=Scao%2C+T+L">Teven Le Scao</a>, 
<a href="/search/cs?searchtype=author&query=Piktus%2C+A">Aleksandra Piktus</a>, 
<a href="/search/cs?searchtype=author&query=Tazi%2C+N">Nouamane Tazi</a>, 
<a href="/search/cs?searchtype=author&query=Pyysalo%2C+S">Sampo Pyysalo</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+T">Thomas Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Raffel%2C+C">Colin Raffel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages (9 main), 39 figures, 15 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16427" title="Abstract">arXiv:2305.16427</a> (replaced) [<a href="/pdf/2305.16427" title="Download PDF">pdf</a>, <a href="/format/2305.16427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural (Tangent Kernel) Collapse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seleznova%2C+M">Mariia Seleznova</a>, 
<a href="/search/cs?searchtype=author&query=Weitzner%2C+D">Dana Weitzner</a>, 
<a href="/search/cs?searchtype=author&query=Giryes%2C+R">Raja Giryes</a>, 
<a href="/search/cs?searchtype=author&query=Kutyniok%2C+G">Gitta Kutyniok</a>, 
<a href="/search/cs?searchtype=author&query=Chou%2C+H">Hung-Hsu Chou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16905" title="Abstract">arXiv:2305.16905</a> (replaced) [<a href="/pdf/2305.16905" title="Download PDF">pdf</a>, <a href="/format/2305.16905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Neural Additive Models with Bayesian Principles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bouchiat%2C+K">Kouroche Bouchiat</a>, 
<a href="/search/stat?searchtype=author&query=Immer%2C+A">Alexander Immer</a>, 
<a href="/search/stat?searchtype=author&query=Y%C3%A8che%2C+H">Hugo Y&#xe8;che</a>, 
<a href="/search/stat?searchtype=author&query=R%C3%A4tsch%2C+G">Gunnar R&#xe4;tsch</a>, 
<a href="/search/stat?searchtype=author&query=Fortuin%2C+V">Vincent Fortuin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17380" title="Abstract">arXiv:2305.17380</a> (replaced) [<a href="/pdf/2305.17380" title="Download PDF">pdf</a>, <a href="/ps/2305.17380" title="Download PostScript">ps</a>, <a href="/format/2305.17380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No-Regret Online Reinforcement Learning with Adversarial Losses and  Transitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+T">Tiancheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junyan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Rouyer%2C+C">Chlo&#xe9; Rouyer</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+W">William Chang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chen-Yu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haipeng Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update the camera-ready version for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17836" title="Abstract">arXiv:2305.17836</a> (replaced) [<a href="/pdf/2305.17836" title="Download PDF">pdf</a>, <a href="/format/2305.17836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Optimal Filtering for Linear Systems with Unknown Noise  Covariances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Talebi%2C+S">Shahriar Talebi</a>, 
<a href="/search/eess?searchtype=author&query=Taghvaei%2C+A">Amirhossein Taghvaei</a>, 
<a href="/search/eess?searchtype=author&query=Mesbahi%2C+M">Mehran Mesbahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2210.14878">arXiv:2210.14878</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18353" title="Abstract">arXiv:2305.18353</a> (replaced) [<a href="/pdf/2305.18353" title="Download PDF">pdf</a>, <a href="/format/2305.18353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergent representations in networks trained with the Forward-Forward  algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tosato%2C+N">Niccol&#xf2; Tosato</a>, 
<a href="/search/cs?searchtype=author&query=Basile%2C+L">Lorenzo Basile</a>, 
<a href="/search/cs?searchtype=author&query=Ballarin%2C+E">Emanuele Ballarin</a>, 
<a href="/search/cs?searchtype=author&query=de+Alteriis%2C+G">Giuseppe de Alteriis</a>, 
<a href="/search/cs?searchtype=author&query=Cazzaniga%2C+A">Alberto Cazzaniga</a>, 
<a href="/search/cs?searchtype=author&query=Ansuini%2C+A">Alessio Ansuini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19434" title="Abstract">arXiv:2305.19434</a> (replaced) [<a href="/pdf/2305.19434" title="Download PDF">pdf</a>, <a href="/ps/2305.19434" title="Download PostScript">ps</a>, <a href="/format/2305.19434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arbitrary Lagrangian-Eulerian finite element approximations for  axisymmetric two-phase flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Garcke%2C+H">Harald Garcke</a>, 
<a href="/search/math?searchtype=author&query=N%C3%BCrnberg%2C+R">Robert N&#xfc;rnberg</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+Q">Quan Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19470" title="Abstract">arXiv:2305.19470</a> (replaced) [<a href="/pdf/2305.19470" title="Download PDF">pdf</a>, <a href="/format/2305.19470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Embedding via Low-Coherence Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Scott%2C+C">Clayton Scott</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19693" title="Abstract">arXiv:2305.19693</a> (replaced) [<a href="/pdf/2305.19693" title="Download PDF">pdf</a>, <a href="/format/2305.19693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spontaneous Symmetry Breaking in Generative Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raya%2C+G">Gabriel Raya</a>, 
<a href="/search/cs?searchtype=author&query=Ambrogioni%2C+L">Luca Ambrogioni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> As published at NeurIPS 2023, and the size of the file has been optimized for fast downloading
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.20081" title="Abstract">arXiv:2305.20081</a> (replaced) [<a href="/pdf/2305.20081" title="Download PDF">pdf</a>, <a href="/format/2305.20081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Diffusion Policies for Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+B">Bingyi Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shuicheng Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00595" title="Abstract">arXiv:2306.00595</a> (replaced) [<a href="/pdf/2306.00595" title="Download PDF">pdf</a>, <a href="/format/2306.00595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yingying Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yutian Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00612" title="Abstract">arXiv:2306.00612</a> (replaced) [<a href="/pdf/2306.00612" title="Download PDF">pdf</a>, <a href="/format/2306.00612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud  Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiakang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiangchao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Botian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yikang Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023. Project page: <a href="https://jiakangyuan.github.io/AD-PT.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00650" title="Abstract">arXiv:2306.00650</a> (replaced) [<a href="/pdf/2306.00650" title="Download PDF">pdf</a>, <a href="/format/2306.00650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Test-time Adaptation through Weight Ensembling, Diversity  Weighting, and Prior Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marsden%2C+R+A">Robert A. Marsden</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%B6bler%2C+M">Mario D&#xf6;bler</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bin Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00984" title="Abstract">arXiv:2306.00984</a> (replaced) [<a href="/pdf/2306.00984" title="Download PDF">pdf</a>, <a href="/format/2306.00984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual  Representation Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonglong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lijie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Isola%2C+P">Phillip Isola</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Huiwen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+D">Dilip Krishnan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> code is available at: <a href="https://github.com/google-research/syn-rep-learn">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01665" title="Abstract">arXiv:2306.01665</a> (replaced) [<a href="/pdf/2306.01665" title="Download PDF">pdf</a>, <a href="/format/2306.01665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SourceP: Detecting Ponzi Schemes on Ethereum with Source Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pengcheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+L">Liang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+K">Keting Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01874" title="Abstract">arXiv:2306.01874</a> (replaced) [<a href="/pdf/2306.01874" title="Download PDF">pdf</a>, <a href="/format/2306.01874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SACSoN: Scalable Autonomous Control for Social Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hirose%2C+N">Noriaki Hirose</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Dhruv Shah</a>, 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+A">Ajay Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 15 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02358" title="Abstract">arXiv:2306.02358</a> (replaced) [<a href="/pdf/2306.02358" title="Download PDF">pdf</a>, <a href="/format/2306.02358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Transitive Are Real-World Group Interactions? -- Measurement and  Reproduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sunwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+F">Fanchen Bu</a>, 
<a href="/search/cs?searchtype=author&query=Choe%2C+M">Minyoung Choe</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jaemin Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+K">Kijung Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in KDD 2023. 12 pages, 7 figures, and 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04220" title="Abstract">arXiv:2306.04220</a> (replaced) [<a href="/pdf/2306.04220" title="Download PDF">pdf</a>, <a href="/format/2306.04220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Look Beneath the Surface: Exploiting Fundamental Symmetry for  Sample-Efficient Offline RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Peng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+X">Xianyuan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shoucheng Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Youfang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Li Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04249" title="Abstract">arXiv:2306.04249</a> (replaced) [<a href="/pdf/2306.04249" title="Download PDF">pdf</a>, <a href="/format/2306.04249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEMIST: A deep-learning-based task-specific denoising approach for  myocardial perfusion SPECT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Rahman%2C+M+A">Md Ashequr Rahman</a>, 
<a href="/search/physics?searchtype=author&query=Yu%2C+Z">Zitong Yu</a>, 
<a href="/search/physics?searchtype=author&query=Laforest%2C+R">Richard Laforest</a>, 
<a href="/search/physics?searchtype=author&query=Abbey%2C+C+K">Craig K. Abbey</a>, 
<a href="/search/physics?searchtype=author&query=Siegel%2C+B+A">Barry A. Siegel</a>, 
<a href="/search/physics?searchtype=author&query=Jha%2C+A+K">Abhinav K. Jha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04618" title="Abstract">arXiv:2306.04618</a> (replaced) [<a href="/pdf/2306.04618" title="Download PDF">pdf</a>, <a href="/format/2306.04618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis,  and LLMs Evaluations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lifan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+G">Ganqu Cui</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hongcheng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+F">Fangyuan Zou</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xingyi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 Dataset and Benchmark Track. Code is available at \url{<a href="https://github.com/lifan-yuan/OOD_NLP">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05557" title="Abstract">arXiv:2306.05557</a> (replaced) [<a href="/pdf/2306.05557" title="Download PDF">pdf</a>, <a href="/format/2306.05557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Performance Discrepancies Across Local Homophily Levels in Graph  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loveland%2C+D">Donald Loveland</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Heimann%2C+M">Mark Heimann</a>, 
<a href="/search/cs?searchtype=author&query=Fish%2C+B">Benjamin Fish</a>, 
<a href="/search/cs?searchtype=author&query=Shaub%2C+M+T">Michael T. Shaub</a>, 
<a href="/search/cs?searchtype=author&query=Koutra%2C+D">Danai Koutra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07092" title="Abstract">arXiv:2306.07092</a> (replaced) [<a href="/pdf/2306.07092" title="Download PDF">pdf</a>, <a href="/format/2306.07092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning Legged Locomotion Controllers via Safe Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Widmer%2C+D">Daniel Widmer</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongho Kang</a>, 
<a href="/search/cs?searchtype=author&query=Sukhija%2C+B">Bhavya Sukhija</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCbotter%2C+J">Jonas H&#xfc;botter</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>, 
<a href="/search/cs?searchtype=author&query=Coros%2C+S">Stelian Coros</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted to the 2023 Conference on Robot Learning (CoRL 2023.) The first two authors contributed equally. The supplementary video is available at <a href="https://youtu.be/zDBouUgegrU">this https URL</a> and the code implementation is available at <a href="https://github.com/lasgroup/gosafeopt">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07163" title="Abstract">arXiv:2306.07163</a> (replaced) [<a href="/pdf/2306.07163" title="Download PDF">pdf</a>, <a href="/format/2306.07163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Batch-to-Online Transformation under Random-Order Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jing Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+Y">Yuichi Yoshida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07273" title="Abstract">arXiv:2306.07273</a> (replaced) [<a href="/pdf/2306.07273" title="Download PDF">pdf</a>, <a href="/format/2306.07273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaussian Membership Inference Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leemann%2C+T">Tobias Leemann</a>, 
<a href="/search/cs?searchtype=author&query=Pawelczyk%2C+M">Martin Pawelczyk</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+G">Gjergji Kasneci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready. The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07923" title="Abstract">arXiv:2306.07923</a> (replaced) [<a href="/pdf/2306.07923" title="Download PDF">pdf</a>, <a href="/format/2306.07923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual  Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lequn Wang</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Akshay Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Slivkins%2C+A">Aleksandrs Slivkins</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08247" title="Abstract">arXiv:2306.08247</a> (replaced) [<a href="/pdf/2306.08247" title="Download PDF">pdf</a>, <a href="/format/2306.08247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion in Diffusion: Cyclic One-Way Diffusion for  Text-Vision-Conditioned Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yongqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhihao Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Ye Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08645" title="Abstract">arXiv:2306.08645</a> (replaced) [<a href="/pdf/2306.08645" title="Download PDF">pdf</a>, <a href="/format/2306.08645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training-free Diffusion Model Adaptation for Variable-Sized  Text-to-Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhiyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xuli Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+X">Xiangyang Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023. 23 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08937" title="Abstract">arXiv:2306.08937</a> (replaced) [<a href="/pdf/2306.08937" title="Download PDF">pdf</a>, <a href="/format/2306.08937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DocumentNet: Bridging the Data Gap in Document Pre-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lijun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+J">Jin Miao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hauptmann%2C+A+G">Alexander G. Hauptmann</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hanjun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09497" title="Abstract">arXiv:2306.09497</a> (replaced) [<a href="/pdf/2306.09497" title="Download PDF">pdf</a>, <a href="/format/2306.09497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel-in-time integration of the shallow water equations on the  rotating sphere using Parareal and MGRIT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Steinstraesser%2C+J+G+C">Jo&#xe3;o Guilherme Caldas Steinstraesser</a>, 
<a href="/search/math?searchtype=author&query=da+Silva+Peixoto%2C+P">Pedro da Silva Peixoto</a>, 
<a href="/search/math?searchtype=author&query=Schreiber%2C+M">Martin Schreiber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 23 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13004" title="Abstract">arXiv:2306.13004</a> (replaced) [<a href="/pdf/2306.13004" title="Download PDF">pdf</a>, <a href="/format/2306.13004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Differentiable Decision Trees Learn Interpretable Reward Functions?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalra%2C+A">Akansha Kalra</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+D+S">Daniel S. Brown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is under submission at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14534" title="Abstract">arXiv:2306.14534</a> (replaced) [<a href="/pdf/2306.14534" title="Download PDF">pdf</a>, <a href="/format/2306.14534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CEIL: Generalized Contextual Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Li He</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yachen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zifeng Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Donglin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14685" title="Abstract">arXiv:2306.14685</a> (replaced) [<a href="/pdf/2306.14685" title="Download PDF">pdf</a>, <a href="/format/2306.14685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffSketcher: Text Guided Vector Sketch Synthesis through Latent  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+X">Ximing Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chuang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haitao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dong Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NIPS 2023. Project page: <a href="https://ximinng.github.io/DiffSketcher-project/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15012" title="Abstract">arXiv:2306.15012</a> (replaced) [<a href="/pdf/2306.15012" title="Download PDF">pdf</a>, <a href="/format/2306.15012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Component Separation for Targeted Signal Recovery in Noisy  Mixtures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Blancard%2C+B+R">Bruno R&#xe9;galdo-Saint Blancard</a>, 
<a href="/search/stat?searchtype=author&query=Eickenberg%2C+M">Michael Eickenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11+12 pages, 5+5 figures, submitted to TMLR, code: <a href="https://github.com/bregaldo/stat_comp_sep">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16899" title="Abstract">arXiv:2306.16899</a> (replaced) [<a href="/pdf/2306.16899" title="Download PDF">pdf</a>, <a href="/format/2306.16899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An improved kernelization algorithm for Trivially Perfect Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dumas%2C+M">Ma&#xeb;l Dumas</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+A">Anthony Perez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01458" title="Abstract">arXiv:2307.01458</a> (replaced) [<a href="/pdf/2307.01458" title="Download PDF">pdf</a>, <a href="/format/2307.01458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity  and Infant Care
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tong Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wangyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+M">Mingbai Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+L">Lu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+N">Noa Garcia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01951" title="Abstract">arXiv:2307.01951</a> (replaced) [<a href="/pdf/2307.01951" title="Download PDF">pdf</a>, <a href="/format/2307.01951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neural Collapse Perspective on Feature Evolution in Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kothapalli%2C+V">Vignesh Kothapalli</a>, 
<a href="/search/cs?searchtype=author&query=Tirer%2C+T">Tom Tirer</a>, 
<a href="/search/cs?searchtype=author&query=Bruna%2C+J">Joan Bruna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04110" title="Abstract">arXiv:2307.04110</a> (replaced) [<a href="/pdf/2307.04110" title="Download PDF">pdf</a>, <a href="/format/2307.04110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Space-Time Continuous Neural PDEs from Partially Observed  States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iakovlev%2C+V">Valerii Iakovlev</a>, 
<a href="/search/cs?searchtype=author&query=Heinonen%2C+M">Markus Heinonen</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%A4hdesm%C3%A4ki%2C+H">Harri L&#xe4;hdesm&#xe4;ki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04204" title="Abstract">arXiv:2307.04204</a> (replaced) [<a href="/pdf/2307.04204" title="Download PDF">pdf</a>, <a href="/format/2307.04204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory Alignment: Understanding the Edge of Stability Phenomenon via  Bifurcation Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Minhak Song</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+C">Chulhee Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready; 51 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04721" title="Abstract">arXiv:2307.04721</a> (replaced) [<a href="/pdf/2307.04721" title="Download PDF">pdf</a>, <a href="/format/2307.04721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as General Pattern Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirchandani%2C+S">Suvir Mirchandani</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Florence%2C+P">Pete Florence</a>, 
<a href="/search/cs?searchtype=author&query=Ichter%2C+B">Brian Ichter</a>, 
<a href="/search/cs?searchtype=author&query=Driess%2C+D">Danny Driess</a>, 
<a href="/search/cs?searchtype=author&query=Arenas%2C+M+G">Montserrat Gonzalez Arenas</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+K">Kanishka Rao</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Andy Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 25 figures. To appear at Conference on Robot Learning (CoRL) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05167" title="Abstract">arXiv:2307.05167</a> (replaced) [<a href="/pdf/2307.05167" title="Download PDF">pdf</a>, <a href="/format/2307.05167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Non-Custodial Wallet for CBDC: Design Challenges and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bowler%2C+R">Ryan Bowler</a>, 
<a href="/search/cs?searchtype=author&query=Goodell%2C+G">Geoffrey Goodell</a>, 
<a href="/search/cs?searchtype=author&query=Revans%2C+J">Joe Revans</a>, 
<a href="/search/cs?searchtype=author&query=Bizama%2C+G">Gabriel Bizama</a>, 
<a href="/search/cs?searchtype=author&query=Speed%2C+C">Chris Speed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05891" title="Abstract">arXiv:2307.05891</a> (replaced) [<a href="/pdf/2307.05891" title="Download PDF">pdf</a>, <a href="/format/2307.05891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PID-Inspired Inductive Biases for Deep Reinforcement Learning in  Partially Observable Control Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Char%2C+I">Ian Char</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Jeff Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07063" title="Abstract">arXiv:2307.07063</a> (replaced) [<a href="/pdf/2307.07063" title="Download PDF">pdf</a>, <a href="/format/2307.07063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bootstrapping Vision-Language Learning with Decoupled Language  Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jian%2C+Y">Yiren Jian</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chongyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+S">Soroush Vosoughi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (spotlight). The code is available at <a href="https://github.com/yiren-jian/BLIText">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07813" title="Abstract">arXiv:2307.07813</a> (replaced) [<a href="/pdf/2307.07813" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision In-Sensor for  Gaze Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonazzi%2C+P">Pietro Bonazzi</a>, 
<a href="/search/cs?searchtype=author&query=Ruegg%2C+T">Thomas Ruegg</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+S">Sizhen Bian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Magno%2C+M">Michele Magno</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07907" title="Abstract">arXiv:2307.07907</a> (replaced) [<a href="/pdf/2307.07907" title="Download PDF">pdf</a>, <a href="/format/2307.07907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeing is not Believing: Robust Reinforcement Learning against Spurious  Correlation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenhao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Laixi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08701" title="Abstract">arXiv:2307.08701</a> (replaced) [<a href="/pdf/2307.08701" title="Download PDF">pdf</a>, <a href="/format/2307.08701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AlpaGasus: Training A Better Alpaca with Fewer Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gunaratna%2C+K">Kalpa Gunaratna</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+V">Vikas Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+V">Vijay Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hongxia Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 Pages; 29 Figures; 15 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10350" title="Abstract">arXiv:2307.10350</a> (replaced) [<a href="/pdf/2307.10350" title="Download PDF">pdf</a>, <a href="/format/2307.10350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Multimodal Datasets with Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thao Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Gadre%2C+S+Y">Samir Yitzhak Gadre</a>, 
<a href="/search/cs?searchtype=author&query=Ilharco%2C+G">Gabriel Ilharco</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sewoong Oh</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+L">Ludwig Schmidt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 Datasets &amp; Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10922" title="Abstract">arXiv:2307.10922</a> (replaced) [<a href="/pdf/2307.10922" title="Download PDF">pdf</a>, <a href="/format/2307.10922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-based Action Concept Spaces Improve Video Self-Supervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ranasinghe%2C+K">Kanchana Ranasinghe</a>, 
<a href="/search/cs?searchtype=author&query=Ryoo%2C+M">Michael Ryoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11221" title="Abstract">arXiv:2307.11221</a> (replaced) [<a href="/pdf/2307.11221" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-tuned Coaxial-transmission-line RF coils for Hyperpolarized 13C and  Deuterium 2H Metabolic MRS Imaging at Ultrahigh Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Payne%2C+K">Komlan Payne</a>, 
<a href="/search/physics?searchtype=author&query=Zhao%2C+Y">Yunkun Zhao</a>, 
<a href="/search/physics?searchtype=author&query=Bhosale%2C+A+A">Aditya Ashok Bhosale</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+X">Xiaoliang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14338" title="Abstract">arXiv:2307.14338</a> (replaced) [<a href="/pdf/2307.14338" title="Download PDF">pdf</a>, <a href="/format/2307.14338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorishniy%2C+Y">Yury Gorishniy</a>, 
<a href="/search/cs?searchtype=author&query=Rubachev%2C+I">Ivan Rubachev</a>, 
<a href="/search/cs?searchtype=author&query=Kartashev%2C+N">Nikolay Kartashev</a>, 
<a href="/search/cs?searchtype=author&query=Shlenskii%2C+D">Daniil Shlenskii</a>, 
<a href="/search/cs?searchtype=author&query=Kotelnikov%2C+A">Akim Kotelnikov</a>, 
<a href="/search/cs?searchtype=author&query=Babenko%2C+A">Artem Babenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/yandex-research/tabular-dl-tabr">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16055" title="Abstract">arXiv:2307.16055</a> (replaced) [<a href="/pdf/2307.16055" title="Download PDF">pdf</a>, <a href="/format/2307.16055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relational correspondences for L-fuzzy rough approximations defined on  De Morgan Heyting algebras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=J%C3%A4rvinen%2C+J">Jouni J&#xe4;rvinen</a>, 
<a href="/search/math?searchtype=author&query=Kondo%2C+M">Michiro Kondo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Rings and Algebras (math.RA)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16200" title="Abstract">arXiv:2307.16200</a> (replaced) [<a href="/pdf/2307.16200" title="Download PDF">pdf</a>, <a href="/format/2307.16200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue  Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zefa Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Z">Ziyi Ni</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shuang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Machine Intelligence Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00855" title="Abstract">arXiv:2308.00855</a> (replaced) [<a href="/pdf/2308.00855" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Study of Groundbreaking Machine Learning Research:  Analyzing highly cited and impactful publications across six decades
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ezugwu%2C+A+E">Absalom E. Ezugwu</a>, 
<a href="/search/cs?searchtype=author&query=Greeff%2C+J">Japie Greeff</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+Y">Yuh-Shan Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal of Engineering Research (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04826" title="Abstract">arXiv:2308.04826</a> (replaced) [<a href="/pdf/2308.04826" title="Download PDF">pdf</a>, <a href="/format/2308.04826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Muyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+F">Fangneng Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiahui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yingchen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoqin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Theobalt%2C+C">Christian Theobalt</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+L">Ling Shao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023. Project website: <a href="https://mxuai.github.io/WaveNeRF/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07818" title="Abstract">arXiv:2308.07818</a> (replaced) [<a href="/pdf/2308.07818" title="Download PDF">pdf</a>, <a href="/format/2308.07818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Riemannian geometry for efficient analysis of protein dynamics data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Diepeveen%2C+W">Willem Diepeveen</a>, 
<a href="/search/q-bio?searchtype=author&query=Esteve-Yag%C3%BCe%2C+C">Carlos Esteve-Yag&#xfc;e</a>, 
<a href="/search/q-bio?searchtype=author&query=Lellmann%2C+J">Jan Lellmann</a>, 
<a href="/search/q-bio?searchtype=author&query=%C3%96ktem%2C+O">Ozan &#xd6;ktem</a>, 
<a href="/search/q-bio?searchtype=author&query=Sch%C3%B6nlieb%2C+C">Carola-Bibiane Sch&#xf6;nlieb</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Differential Geometry (math.DG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07983" title="Abstract">arXiv:2308.07983</a> (replaced) [<a href="/pdf/2308.07983" title="Download PDF">pdf</a>, <a href="/format/2308.07983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monte Carlo guided Diffusion for Bayesian linear inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cardoso%2C+G">Gabriel Cardoso</a>, 
<a href="/search/stat?searchtype=author&query=Idrissi%2C+Y+J+E">Yazid Janati El Idrissi</a>, 
<a href="/search/stat?searchtype=author&query=Corff%2C+S+L">Sylvain Le Corff</a>, 
<a href="/search/stat?searchtype=author&query=Moulines%2C+E">Eric Moulines</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08231" title="Abstract">arXiv:2308.08231</a> (replaced) [<a href="/pdf/2308.08231" title="Download PDF">pdf</a>, <a href="/format/2308.08231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DDF-HO: Hand-Held Object Reconstruction via Conditional Directed  Distance Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenyangguang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+Y">Yan Di</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruida Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangyao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Manhardt%2C+F">Fabian Manhardt</a>, 
<a href="/search/cs?searchtype=author&query=Tombari%2C+F">Federico Tombari</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiangyang Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09275" title="Abstract">arXiv:2308.09275</a> (replaced) [<a href="/pdf/2308.09275" title="Download PDF">pdf</a>, <a href="/format/2308.09275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Opinion Dynamics under Social Pressure in Arbitrary Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tang%2C+J">Jennifer Tang</a>, 
<a href="/search/eess?searchtype=author&query=Adler%2C+A">Aviv Adler</a>, 
<a href="/search/eess?searchtype=author&query=Ajorlou%2C+A">Amir Ajorlou</a>, 
<a href="/search/eess?searchtype=author&query=Jadbabaie%2C+A">Ali Jadbabaie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> fixed typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Social and Information Networks (cs.SI); Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10576" title="Abstract">arXiv:2308.10576</a> (replaced) [<a href="/pdf/2308.10576" title="Download PDF">pdf</a>, <a href="/format/2308.10576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorprating Prompt tuning for Commit classification with prior  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+J">Jiajun Tong</a>, 
<a href="/search/cs?searchtype=author&query=Rui%2C+X">Xiaobin Rui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13056" title="Abstract">arXiv:2308.13056</a> (replaced) [<a href="/pdf/2308.13056" title="Download PDF">pdf</a>, <a href="/format/2308.13056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lexical Diversity in Kinship Across Languages and Dialects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalilia%2C+H">Hadi Khalilia</a>, 
<a href="/search/cs?searchtype=author&query=Bella%2C+G">G&#xe1;bor Bella</a>, 
<a href="/search/cs?searchtype=author&query=Freihat%2C+A+A">Abed Alhakim Freihat</a>, 
<a href="/search/cs?searchtype=author&query=Darma%2C+S">Shandy Darma</a>, 
<a href="/search/cs?searchtype=author&query=Giunchiglia%2C+F">Fausto Giunchiglia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13633" title="Abstract">arXiv:2308.13633</a> (replaced) [<a href="/pdf/2308.13633" title="Download PDF">pdf</a>, <a href="/format/2308.13633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive whitening with fast gain modulation and slow synaptic  plasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Duong%2C+L+R">Lyndon R. Duong</a>, 
<a href="/search/q-bio?searchtype=author&query=Simoncelli%2C+E+P">Eero P. Simoncelli</a>, 
<a href="/search/q-bio?searchtype=author&query=Chklovskii%2C+D+B">Dmitri B. Chklovskii</a>, 
<a href="/search/q-bio?searchtype=author&query=Lipshutz%2C+D">David Lipshutz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Spotlight; 18 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14284" title="Abstract">arXiv:2308.14284</a> (replaced) [<a href="/pdf/2308.14284" title="Download PDF">pdf</a>, <a href="/format/2308.14284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Powered Sim-to-real Transfer for Traffic Signal Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Da%2C+L">Longchao Da</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Minchiuan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Hao Mei</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hua Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00359" title="Abstract">arXiv:2309.00359</a> (replaced) [<a href="/pdf/2309.00359" title="Download PDF">pdf</a>, <a href="/format/2309.00359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Content And Behavior Models To Understand, Simulate, And Optimize  Content And Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+A">Ashmit Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Aditya Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+A">Aanisha Bhattacharyya</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+Y+K">Yaman K Singla</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Somesh Singh</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+U">Uttaran Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+I">Ishita Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Petrangeli%2C+S">Stefano Petrangeli</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+R+R">Rajiv Ratn Shah</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changyou Chen</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+B">Balaji Krishnamurthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01270" title="Abstract">arXiv:2309.01270</a> (replaced) [<a href="/pdf/2309.01270" title="Download PDF">pdf</a>, <a href="/format/2309.01270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action  Spotting using Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Denize%2C+J">Julien Denize</a>, 
<a href="/search/cs?searchtype=author&query=Liashuha%2C+M">Mykola Liashuha</a>, 
<a href="/search/cs?searchtype=author&query=Rabarisoa%2C+J">Jaonary Rabarisoa</a>, 
<a href="/search/cs?searchtype=author&query=Orcesi%2C+A">Astrid Orcesi</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A9rault%2C+R">Romain H&#xe9;rault</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source code is available here: <a href="https://github.com/juliendenize/eztorch">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01482" title="Abstract">arXiv:2309.01482</a> (replaced) [<a href="/pdf/2309.01482" title="Download PDF">pdf</a>, <a href="/ps/2309.01482" title="Download PostScript">ps</a>, <a href="/format/2309.01482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thick Forests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dyer%2C+M">Martin Dyer</a>, 
<a href="/search/math?searchtype=author&query=M%C3%BCller%2C+H">Haiko M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01522" title="Abstract">arXiv:2309.01522</a> (replaced) [<a href="/e-print/2309.01522" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural  Topic Model Tells You
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haiping Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper requires major revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02732" title="Abstract">arXiv:2309.02732</a> (replaced) [<a href="/pdf/2309.02732" title="Download PDF">pdf</a>, <a href="/format/2309.02732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A study on fault diagnosis in nonlinear dynamic systems with  uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ding%2C+S+X">Steven X. Ding</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Linlin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03004" title="Abstract">arXiv:2309.03004</a> (replaced) [<a href="/pdf/2309.03004" title="Download PDF">pdf</a>, <a href="/format/2309.03004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theoretical Explanation of Activation Sparsity through Flat Minima and  Adversarial Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Ze Peng</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+L">Lei Qi</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yinghuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03805" title="Abstract">arXiv:2309.03805</a> (replaced) [<a href="/pdf/2309.03805" title="Download PDF">pdf</a>, <a href="/format/2309.03805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping of CNNs on multi-core RRAM-based CIM architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pelke%2C+R">Rebecca Pelke</a>, 
<a href="/search/cs?searchtype=author&query=Bosbach%2C+N">Nils Bosbach</a>, 
<a href="/search/cs?searchtype=author&query=Cubero%2C+J">Jose Cubero</a>, 
<a href="/search/cs?searchtype=author&query=Staudigl%2C+F">Felix Staudigl</a>, 
<a href="/search/cs?searchtype=author&query=Leupers%2C+R">Rainer Leupers</a>, 
<a href="/search/cs?searchtype=author&query=Joseph%2C+J+M">Jan Moritz Joseph</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04056" title="Abstract">arXiv:2309.04056</a> (replaced) [<a href="/pdf/2309.04056" title="Download PDF">pdf</a>, <a href="/ps/2309.04056" title="Download PostScript">ps</a>, <a href="/format/2309.04056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-discontinuous Functional based Sliding Mode Cascade Observer for  Estimation and Closed-loop Compensation Controller
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+Y">Yiyong Sun</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhang Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhai%2C+G">Guang Zhai</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+B">Bin Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05035" title="Abstract">arXiv:2309.05035</a> (replaced) [<a href="/pdf/2309.05035" title="Download PDF">pdf</a>, <a href="/format/2309.05035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Duplicate Question Retrieval and Confirmation Time Prediction in  Software Communities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hazra%2C+R">Rima Hazra</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+D">Debanjan Saha</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+A">Amruit Sahoo</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Somnath Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full paper accepted at ASONAM 2023: The 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Software Engineering (cs.SE); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06118" title="Abstract">arXiv:2309.06118</a> (replaced) [<a href="/pdf/2309.06118" title="Download PDF">pdf</a>, <a href="/format/2309.06118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHITNet: A Complementary to Harmonious Information Transfer Network for  Infrared and Visible Image Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yafei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+K">Keying Du</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huafeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhengtao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06367" title="Abstract">arXiv:2309.06367</a> (replaced) [<a href="/pdf/2309.06367" title="Download PDF">pdf</a>, <a href="/format/2309.06367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Cognitive-Affective Processes with Appraisal and Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Broekens%2C+J">Joost Broekens</a>, 
<a href="/search/cs?searchtype=author&query=Jokinen%2C+J">Jussi Jokinen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07254" title="Abstract">arXiv:2309.07254</a> (replaced) [<a href="/pdf/2309.07254" title="Download PDF">pdf</a>, <a href="/format/2309.07254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigate Replication and Copying in Diffusion Models with Generalized  Caption and Dual Fusion Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dake Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuke Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Beerel%2C+P+A">Peter A. Beerel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07510" title="Abstract">arXiv:2309.07510</a> (replaced) [<a href="/pdf/2309.07510" title="Download PDF">pdf</a>, <a href="/format/2309.07510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Environment-Aware Affordance for 3D Articulated Object  Manipulation under Occlusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruihai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+K">Kai Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+C">Chuanruo Ning</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+G">Guanqi Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Website at <a href="https://chengkaiacademycity.github.io/EnvAwareAfford/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07593" title="Abstract">arXiv:2309.07593</a> (replaced) [<a href="/pdf/2309.07593" title="Download PDF">pdf</a>, <a href="/format/2309.07593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistically Valid Variable Importance Assessment through Conditional  Permutations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chamma%2C+A">Ahmad Chamma</a> (1 and 2 and 3), 
<a href="/search/cs?searchtype=author&query=Engemann%2C+D+A">Denis A. Engemann</a> (4), 
<a href="/search/cs?searchtype=author&query=Thirion%2C+B">Bertrand Thirion</a> (1 and 2 and 3) ((1) Inria, (2) Universite Paris Saclay, (3) CEA, (4) Roche Pharma Research and Early Development, Neuroscience and Rare Diseases, Roche Innovation Center Basel, F. Hoffmann-La Roche Ltd., Basel, Switzerland)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08200" title="Abstract">arXiv:2309.08200</a> (replaced) [<a href="/pdf/2309.08200" title="Download PDF">pdf</a>, <a href="/format/2309.08200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TF-SepNet: An Efficient 1D Kernel Design in CNNs for Low-Complexity  Acoustic Scene Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yiqiang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peihong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shengchen Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13847" title="Abstract">arXiv:2309.13847</a> (replaced) [<a href="/pdf/2309.13847" title="Download PDF">pdf</a>, <a href="/format/2309.13847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning Multi-mode Token-level Prompt Alignment across Modalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dongsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Miaoge Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">MingSheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14098" title="Abstract">arXiv:2309.14098</a> (replaced) [<a href="/pdf/2309.14098" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computer Science Framework to Teach Community-Based Environmental  Literacy and Data Literacy to Diverse Students
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baek%2C+C">Clare Baek</a>, 
<a href="/search/cs?searchtype=author&query=Saito-Stehberger%2C+D">Dana Saito-Stehberger</a>, 
<a href="/search/cs?searchtype=author&query=Jacob%2C+S">Sharin Jacob</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+A">Adam Nam</a>, 
<a href="/search/cs?searchtype=author&query=Warschauer%2C+M">Mark Warschauer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14597" title="Abstract">arXiv:2309.14597</a> (replaced) [<a href="/pdf/2309.14597" title="Download PDF">pdf</a>, <a href="/format/2309.14597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Optimization in a Noisy Neighborhood: On Return Landscapes in  Continuous Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahn%2C+N">Nate Rahn</a>, 
<a href="/search/cs?searchtype=author&query=D%27Oro%2C+P">Pierluca D&#x27;Oro</a>, 
<a href="/search/cs?searchtype=author&query=Wiltzer%2C+H">Harley Wiltzer</a>, 
<a href="/search/cs?searchtype=author&query=Bacon%2C+P">Pierre-Luc Bacon</a>, 
<a href="/search/cs?searchtype=author&query=Bellemare%2C+M+G">Marc G. Bellemare</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Accepted Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14660" title="Abstract">arXiv:2309.14660</a> (replaced) [<a href="/pdf/2309.14660" title="Download PDF">pdf</a>, <a href="/format/2309.14660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud  Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Shuhao Kang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Youqi Liao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianping Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+F">Fuxun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fangning Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bisheng Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> demo video: <a href="https://youtu.be/ovbedasXuZE">this https URL</a>; source code (will be public): <a href="https://github.com/kang-1-2-3/CoFiI2P">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17310" title="Abstract">arXiv:2309.17310</a> (replaced) [<a href="/pdf/2309.17310" title="Download PDF">pdf</a>, <a href="/format/2309.17310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leave-one-out Distinguishability in Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jiayuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Borovykh%2C+A">Anastasia Borovykh</a>, 
<a href="/search/cs?searchtype=author&query=Hayou%2C+S">Soufiane Hayou</a>, 
<a href="/search/cs?searchtype=author&query=Shokri%2C+R">Reza Shokri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00100" title="Abstract">arXiv:2310.00100</a> (replaced) [<a href="/e-print/2310.00100" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Natural Language Processing Model for Radiology Reports --  The Summary is all you need!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lindo%2C+M">Mariana Lindo</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+A+S">Ana Sofia Santos</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+A">Andr&#xe9; Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianning Li</a>, 
<a href="/search/cs?searchtype=author&query=Luijten%2C+G">Gijs Luijten</a>, 
<a href="/search/cs?searchtype=author&query=Correia%2C+G">Gustavo Correia</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Moon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kleesiek%2C+J">Jens Kleesiek</a>, 
<a href="/search/cs?searchtype=author&query=Egger%2C+J">Jan Egger</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+V">Victor Alves</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Problems with the model
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00270" title="Abstract">arXiv:2310.00270</a> (replaced) [<a href="/pdf/2310.00270" title="Download PDF">pdf</a>, <a href="/format/2310.00270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpatialRank: Urban Event Ranking with NDCG Optimization on  Spatiotemporal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yongjian Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianbao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00917" title="Abstract">arXiv:2310.00917</a> (replaced) [<a href="/pdf/2310.00917" title="Download PDF">pdf</a>, <a href="/format/2310.00917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Power of Multi-Lingual Datasets for Pre-training: Towards  Enhancing Text Spotting Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Alloy Das</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+S">Sanket Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Ayan Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Saumik Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Llad%C3%B3s%2C+J">Josep Llad&#xf3;s</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+U">Umapada Pal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01080" title="Abstract">arXiv:2310.01080</a> (replaced) [<a href="/pdf/2310.01080" title="Download PDF">pdf</a>, <a href="/format/2310.01080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rel2Graph: Automated Mapping From Relational Databases to a Unified  Property Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=French%2C+T">Tim French</a>, 
<a href="/search/cs?searchtype=author&query=Stewart%2C+M">Michael Stewart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01164" title="Abstract">arXiv:2310.01164</a> (replaced) [<a href="/pdf/2310.01164" title="Download PDF">pdf</a>, <a href="/format/2310.01164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Any Building
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CGI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01551" title="Abstract">arXiv:2310.01551</a> (replaced) [<a href="/pdf/2310.01551" title="Download PDF">pdf</a>, <a href="/format/2310.01551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Power of Choices in Decision Tree Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blanc%2C+G">Guy Blanc</a>, 
<a href="/search/cs?searchtype=author&query=Lange%2C+J">Jane Lange</a>, 
<a href="/search/cs?searchtype=author&query=Pabbaraju%2C+C">Chirag Pabbaraju</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+C">Colin Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+L">Li-Yang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+M">Mo Tiwari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02255" title="Abstract">arXiv:2310.02255</a> (replaced) [<a href="/pdf/2310.02255" title="Download PDF">pdf</a>, <a href="/format/2310.02255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V,  Bard, and Other Large Multimodal Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+H">Hritik Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+T">Tony Xia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Galley%2C+M">Michel Galley</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 112 pages, 117 figures. Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02987" title="Abstract">arXiv:2310.02987</a> (replaced) [<a href="/pdf/2310.02987" title="Download PDF">pdf</a>, <a href="/format/2310.02987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xufeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Alacaoglu%2C+A">Ahmet Alacaoglu</a>, 
<a href="/search/cs?searchtype=author&query=Diakonikolas%2C+J">Jelena Diakonikolas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05858" title="Abstract">arXiv:2310.05858</a> (replaced) [<a href="/pdf/2310.05858" title="Download PDF">pdf</a>, <a href="/format/2310.05858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSAC-T: Distributional Soft Actor-Critic with Three Refinements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jingliang Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Liming Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jiaxin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+E">Shengbo Eben Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08416" title="Abstract">arXiv:2310.08416</a> (replaced) [<a href="/pdf/2310.08416" title="Download PDF">pdf</a>, <a href="/format/2310.08416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying reducible $k$-tuples of vectors with subspace-proximity  sensitive hashing/filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Holden%2C+G">Gabriella Holden</a>, 
<a href="/search/math?searchtype=author&query=Shiu%2C+D">Daniel Shiu</a>, 
<a href="/search/math?searchtype=author&query=Strutt%2C+L">Lauren Strutt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08872" title="Abstract">arXiv:2310.08872</a> (replaced) [<a href="/pdf/2310.08872" title="Download PDF">pdf</a>, <a href="/format/2310.08872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R&amp;B: Region and Boundary Aware Zero-shot Grounded Text-to-image  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jiayu Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+H">Henglei Lv</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingming Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review. Project page: <a href="https://sagileo.github.io/Region-and-Boundary">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08896" title="Abstract">arXiv:2310.08896</a> (replaced) [<a href="/pdf/2310.08896" title="Download PDF">pdf</a>, <a href="/format/2310.08896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Migrant Resettlement by Evolutionary Multi-objective Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dan-Xuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yu-Ran Gu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chao Qian</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+X">Xin Mu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Ke Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09478" title="Abstract">arXiv:2310.09478</a> (replaced) [<a href="/pdf/2310.09478" title="Download PDF">pdf</a>, <a href="/format/2310.09478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiniGPT-v2: large language model as a unified interface for  vision-language multi-task learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Deyao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiaoqian Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zechun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengchuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamoorthi%2C+R">Raghuraman Krishnamoorthi</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+V">Vikas Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yunyang Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Elhoseiny%2C+M">Mohamed Elhoseiny</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> fix small typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11191" title="Abstract">arXiv:2310.11191</a> (replaced) [<a href="/pdf/2310.11191" title="Download PDF">pdf</a>, <a href="/format/2310.11191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Medical Text Simplification: Optimizing for Readability with  Unlikelihood Training and Reranked Beam Search Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Flores%2C+L+J+Y">Lorenzo Jaime Yu Flores</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+K">Kejian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chheang%2C+S">Sophie Chheang</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12279" title="Abstract">arXiv:2310.12279</a> (replaced) [<a href="/pdf/2310.12279" title="Download PDF">pdf</a>, <a href="/ps/2310.12279" title="Download PostScript">ps</a>, <a href="/format/2310.12279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adjoint-based inversion for stress and frictional parameters in  earthquake modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Stiernstr%C3%B6m%2C+V">Vidar Stiernstr&#xf6;m</a>, 
<a href="/search/math?searchtype=author&query=Almquist%2C+M">Martin Almquist</a>, 
<a href="/search/math?searchtype=author&query=Dunham%2C+E+M">Eric M. Dunham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated title, added additional references, provided additional details in sections 1 and 5, fixed typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12350" title="Abstract">arXiv:2310.12350</a> (replaced) [<a href="/pdf/2310.12350" title="Download PDF">pdf</a>, <a href="/format/2310.12350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equipping Federated Graph Neural Networks with Structure-aware Group  Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+N">Nan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiuling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+H">Wendy Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+V">Violet Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+Y">Yue Ning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12880" title="Abstract">arXiv:2310.12880</a> (replaced) [<a href="/pdf/2310.12880" title="Download PDF">pdf</a>, <a href="/format/2310.12880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yigit%2C+Y">Yagmur Yigit</a>, 
<a href="/search/cs?searchtype=author&query=Kinaci%2C+O+K">Omer Kemal Kinaci</a>, 
<a href="/search/cs?searchtype=author&query=Duong%2C+T+Q">Trung Q. Duong</a>, 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted on WS01 IEEE ICC 2023 Workshop on The Evolution of Digital Twin Paradigm in Wireless Communications
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Communications Workshops
  (ICC Workshops), Rome, Italy, 2023, pp. 740-745
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12914" title="Abstract">arXiv:2310.12914</a> (replaced) [<a href="/pdf/2310.12914" title="Download PDF">pdf</a>, <a href="/format/2310.12914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network-Aware AutoML Framework for Software-Defined Sensor Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Horsanali%2C+E">Emre Horsanali</a>, 
<a href="/search/cs?searchtype=author&query=Yigit%2C+Y">Yagmur Yigit</a>, 
<a href="/search/cs?searchtype=author&query=Secinti%2C+G">Gokhan Secinti</a>, 
<a href="/search/cs?searchtype=author&query=Karameseoglu%2C+A">Aytac Karameseoglu</a>, 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2021 17th International Conference on Distributed Computing in
  Sensor Systems (DCOSS), Pafos, Cyprus, 2021, pp. 451-457
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12924" title="Abstract">arXiv:2310.12924</a> (replaced) [<a href="/pdf/2310.12924" title="Download PDF">pdf</a>, <a href="/format/2310.12924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous  Core Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yigit%2C+Y">Yagmur Yigit</a>, 
<a href="/search/cs?searchtype=author&query=Bal%2C+B">Bahadir Bal</a>, 
<a href="/search/cs?searchtype=author&query=Karameseoglu%2C+A">Aytac Karameseoglu</a>, 
<a href="/search/cs?searchtype=author&query=Duong%2C+T+Q">Trung Q. Duong</a>, 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Communications Standards Magazine, vol. 6, no. 3, pp. 38-44,
  September 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13268" title="Abstract">arXiv:2310.13268</a> (replaced) [<a href="/pdf/2310.13268" title="Download PDF">pdf</a>, <a href="/format/2310.13268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model  Statistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kaiwen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13452" title="Abstract">arXiv:2310.13452</a> (replaced) [<a href="/pdf/2310.13452" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quadrotor Dead Reckoning with Multiple Inertial Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hurwitz%2C+D">Dror Hurwitz</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+I">Itzik Klein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13564" title="Abstract">arXiv:2310.13564</a> (replaced) [<a href="/pdf/2310.13564" title="Download PDF">pdf</a>, <a href="/format/2310.13564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $hp$-optimal convergence of the original DG method for linear hyperbolic  problems on special simplicial meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dong%2C+Z">Zhaonan Dong</a>, 
<a href="/search/math?searchtype=author&query=Mascotto%2C+L">Lorenzo Mascotto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13588" title="Abstract">arXiv:2310.13588</a> (replaced) [<a href="/pdf/2310.13588" title="Download PDF">pdf</a>, <a href="/format/2310.13588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simultaneous Machine Translation with Tailored Reference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shoutao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaolei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023; 15 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13788" title="Abstract">arXiv:2310.13788</a> (replaced) [<a href="/pdf/2310.13788" title="Download PDF">pdf</a>, <a href="/ps/2310.13788" title="Download PostScript">ps</a>, <a href="/format/2310.13788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster Integer Points Counting in Parametric Polyhedra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gribanov%2C+D">D. Gribanov</a>, 
<a href="/search/cs?searchtype=author&query=Malyshev%2C+D">D. Malyshev</a>, 
<a href="/search/cs?searchtype=author&query=Pardalos%2C+P">P. Pardalos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13923" title="Abstract">arXiv:2310.13923</a> (replaced) [<a href="/pdf/2310.13923" title="Download PDF">pdf</a>, <a href="/format/2310.13923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversified Outlier Exposure for Out-of-Distribution Detection via  Informative Extrapolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Geng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiangchao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+G">Gang Niu</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13947" title="Abstract">arXiv:2310.13947</a> (replaced) [<a href="/pdf/2310.13947" title="Download PDF">pdf</a>, <a href="/format/2310.13947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extreme Learning Machine-Assisted Solution of Biharmonic Equations via  Its Coupled Schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+X">Xi&#x27;an Li</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+J">Jinran Wu</a>, 
<a href="/search/math?searchtype=author&query=Deng%2C+J">Jiaxin Deng</a>, 
<a href="/search/math?searchtype=author&query=Ding%2C+Z">Zhe Ding</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">You-Gan Wang</a>, 
<a href="/search/math?searchtype=author&query=Tai%2C+X">Xin Tai</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+L">Liang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14159" title="Abstract">arXiv:2310.14159</a> (replaced) [<a href="/pdf/2310.14159" title="Download PDF">pdf</a>, <a href="/format/2310.14159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Language Models Laugh at YouTube Short-form Videos?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+D">Dayoon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gunhee Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14613" title="Abstract">arXiv:2310.14613</a> (replaced) [<a href="/pdf/2310.14613" title="Download PDF">pdf</a>, <a href="/format/2310.14613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Modulation Current for Gain-Switching Lasers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Borisevich%2C+A">Alex Borisevich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC); Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14814" title="Abstract">arXiv:2310.14814</a> (replaced) [<a href="/pdf/2310.14814" title="Download PDF">pdf</a>, <a href="/format/2310.14814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Ensemble Diversity for Robust Self-Training in the Presence  of Sample Selection Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Odonnat%2C+A">Ambroise Odonnat</a>, 
<a href="/search/cs?searchtype=author&query=Feofanov%2C+V">Vasilii Feofanov</a>, 
<a href="/search/cs?searchtype=author&query=Redko%2C+I">Ievgen Redko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15105" title="Abstract">arXiv:2310.15105</a> (replaced) [<a href="/pdf/2310.15105" title="Download PDF">pdf</a>, <a href="/format/2310.15105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained  Models in Few-Shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kun Song</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huimin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+B">Bochao Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huishuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15113" title="Abstract">arXiv:2310.15113</a> (replaced) [<a href="/pdf/2310.15113" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counting the Bugs in ChatGPT&#x27;s Wugs: A Multilingual Investigation into  the Morphological Capabilities of a Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weissweiler%2C+L">Leonie Weissweiler</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+V">Valentin Hofmann</a>, 
<a href="/search/cs?searchtype=author&query=Kantharuban%2C+A">Anjali Kantharuban</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+A">Anna Cai</a>, 
<a href="/search/cs?searchtype=author&query=Dutt%2C+R">Ritam Dutt</a>, 
<a href="/search/cs?searchtype=author&query=Hengle%2C+A">Amey Hengle</a>, 
<a href="/search/cs?searchtype=author&query=Kabra%2C+A">Anubha Kabra</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Atharva Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Vijayakumar%2C+A">Abhishek Vijayakumar</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haofei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>, 
<a href="/search/cs?searchtype=author&query=Oflazer%2C+K">Kemal Oflazer</a>, 
<a href="/search/cs?searchtype=author&query=Mortensen%2C+D+R">David R. Mortensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15116" title="Abstract">arXiv:2310.15116</a> (replaced) [<a href="/pdf/2310.15116" title="Download PDF">pdf</a>, <a href="/ps/2310.15116" title="Download PostScript">ps</a>, <a href="/format/2310.15116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hermite-Pad&#xe9; approximation, multiple orthogonal polynomials, and  multidimensional Toda equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nlin?searchtype=author&query=Doliwa%2C+A">Adam Doliwa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, XL Workshop on Geometric Methods in Physics, Bia{\l}owie\.za 2023; typos corrected v2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Exactly Solvable and Integrable Systems (nlin.SI)</span>; Mathematical Physics (math-ph); Classical Analysis and ODEs (math.CA); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15447" title="Abstract">arXiv:2310.15447</a> (replaced) [<a href="/pdf/2310.15447" title="Download PDF">pdf</a>, <a href="/format/2310.15447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepIron: Predicting Unwarped Garment Texture from a Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+H">Hyun-Song Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sung-Hee Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15495" title="Abstract">arXiv:2310.15495</a> (replaced) [<a href="/pdf/2310.15495" title="Download PDF">pdf</a>, <a href="/format/2310.15495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMG: Automated Efficient Approximate Multiplier Generator for FPGAs via  Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingli Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2023 IEEE International Conference on Field-Programmable Technology (ICFPT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15581" title="Abstract">arXiv:2310.15581</a> (replaced) [<a href="/pdf/2310.15581" title="Download PDF">pdf</a>, <a href="/format/2310.15581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep ReLU neural networks overcome the curse of dimensionality when  approximating semilinear partial integro-differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Neufeld%2C+A">Ariel Neufeld</a>, 
<a href="/search/math?searchtype=author&query=Nguyen%2C+T+A">Tuan Anh Nguyen</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+S">Sizhou Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15694" title="Abstract">arXiv:2310.15694</a> (replaced) [<a href="/pdf/2310.15694" title="Download PDF">pdf</a>, <a href="/format/2310.15694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COPF: Continual Learning Human Preference through Optimal Policy Fitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yuanzhao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15712" title="Abstract">arXiv:2310.15712</a> (replaced) [<a href="/pdf/2310.15712" title="Download PDF">pdf</a>, <a href="/format/2310.15712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNeSF: Generalizable Neural Semantic Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hanlin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chen Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Mengqi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiwen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G+H">Gim Hee Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15890" title="Abstract">arXiv:2310.15890</a> (replaced) [<a href="/pdf/2310.15890" title="Download PDF">pdf</a>, <a href="/format/2310.15890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-feature Contrastive Loss for Decentralized Deep Learning on  Heterogeneous Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aketi%2C+S+A">Sai Aparna Aketi</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures, 11 tables. arXiv admin note: text overlap with <a href="/abs/2305.04792">arXiv:2305.04792</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16002" title="Abstract">arXiv:2310.16002</a> (replaced) [<a href="/pdf/2310.16002" title="Download PDF">pdf</a>, <a href="/format/2310.16002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating View Conditions for Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jinbin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+A">Aosong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+T">Tian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaicheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16020" title="Abstract">arXiv:2310.16020</a> (replaced) [<a href="/pdf/2310.16020" title="Download PDF">pdf</a>, <a href="/format/2310.16020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConvBKI: Real-Time Probabilistic Semantic Mapping Network with  Quantifiable Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilson%2C+J">Joey Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yuewei Fu</a>, 
<a href="/search/cs?searchtype=author&query=Friesen%2C+J">Joshua Friesen</a>, 
<a href="/search/cs?searchtype=author&query=Ewen%2C+P">Parker Ewen</a>, 
<a href="/search/cs?searchtype=author&query=Capodieci%2C+A">Andrew Capodieci</a>, 
<a href="/search/cs?searchtype=author&query=Jayakumar%2C+P">Paramsothy Jayakumar</a>, 
<a href="/search/cs?searchtype=author&query=Barton%2C+K">Kira Barton</a>, 
<a href="/search/cs?searchtype=author&query=Ghaffari%2C+M">Maani Ghaffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2209.10663">arXiv:2209.10663</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16190" title="Abstract">arXiv:2310.16190</a> (replaced) [<a href="/pdf/2310.16190" title="Download PDF">pdf</a>, <a href="/format/2310.16190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilayer Environment and Toolchain for Holistic NetwOrk Design and  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rezabek%2C+F">Filip Rezabek</a>, 
<a href="/search/cs?searchtype=author&query=Glas%2C+K">Kilian Glas</a>, 
<a href="/search/cs?searchtype=author&query=von+Seck%2C+R">Richard von Seck</a>, 
<a href="/search/cs?searchtype=author&query=Aroua%2C+A">Achraf Aroua</a>, 
<a href="/search/cs?searchtype=author&query=Leonhardt%2C+T">Tizian Leonhardt</a>, 
<a href="/search/cs?searchtype=author&query=Carle%2C+G">Georg Carle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Cryptography and Security (cs.CR); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16218" title="Abstract">arXiv:2310.16218</a> (replaced) [<a href="/pdf/2310.16218" title="Download PDF">pdf</a>, <a href="/format/2310.16218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Editing for Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yaochen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haochen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zaiyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jundong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16350" title="Abstract">arXiv:2310.16350</a> (replaced) [<a href="/pdf/2310.16350" title="Download PDF">pdf</a>, <a href="/format/2310.16350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unraveling Feature Extraction Mechanisms in Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaobing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16436" title="Abstract">arXiv:2310.16436</a> (replaced) [<a href="/pdf/2310.16436" title="Download PDF">pdf</a>, <a href="/format/2310.16436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning  in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Ge Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiajin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong-Yu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sibei Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 13 figures, to be published in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16605" title="Abstract">arXiv:2310.16605</a> (replaced) [<a href="/pdf/2310.16605" title="Download PDF">pdf</a>, <a href="/format/2310.16605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Robust Unsupervised Dense Retrieval Training on Web  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+P">Peixuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Chenyan Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16638" title="Abstract">arXiv:2310.16638</a> (replaced) [<a href="/pdf/2310.16638" title="Download PDF">pdf</a>, <a href="/format/2310.16638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Covariate Shift Adaptation for Density-Ratio Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kato%2C+M">Masahiro Kato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16639" title="Abstract">arXiv:2310.16639</a> (replaced) [<a href="/pdf/2310.16639" title="Download PDF">pdf</a>, <a href="/format/2310.16639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Driving through the Concept Gridlock: Unraveling Explainability  Bottlenecks in Automated Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Echterhoff%2C+J">Jessica Echterhoff</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+A">An Yan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kyungtae Han</a>, 
<a href="/search/cs?searchtype=author&query=Abdelraouf%2C+A">Amr Abdelraouf</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Rohit Gupta</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16713" title="Abstract">arXiv:2310.16713</a> (replaced) [<a href="/pdf/2310.16713" title="Download PDF">pdf</a>, <a href="/format/2310.16713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SkyMath: Technical Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haihua Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wenjun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lunan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jianfei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianwen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Biye Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guoliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuejie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xilin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Rui Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16776" title="Abstract">arXiv:2310.16776</a> (replaced) [<a href="/pdf/2310.16776" title="Download PDF">pdf</a>, <a href="/format/2310.16776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEFT: Data Efficient Fine-Tuning for Large Language Models via  Unsupervised Core-Set Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+D">Devleena Das</a>, 
<a href="/search/cs?searchtype=author&query=Khetan%2C+V">Vivek Khetan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16779" title="Abstract">arXiv:2310.16779</a> (replaced) [<a href="/pdf/2310.16779" title="Download PDF">pdf</a>, <a href="/format/2310.16779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-scale Diffusion Denoised Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jongheon Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jinwoo Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at NeurIPS 2023; Code is available at <a href="https://github.com/jh-jeong/smoothing-multiscale">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16818" title="Abstract">arXiv:2310.16818</a> (replaced) [<a href="/pdf/2310.16818" title="Download PDF">pdf</a>, <a href="/format/2310.16818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion  Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+R">Ruizhi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lizhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhenda Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yebin Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://mrtornado24.github.io/DreamCraft3D/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16826" title="Abstract">arXiv:2310.16826</a> (replaced) [<a href="/pdf/2310.16826" title="Download PDF">pdf</a>, <a href="/format/2310.16826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep machine learning for meteor monitoring: advances with transfer  learning and gradient-weighted class activation mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Pe%C3%B1a-Asensio%2C+E">Eloy Pe&#xf1;a-Asensio</a>, 
<a href="/search/astro-ph?searchtype=author&query=Trigo-Rodr%C3%ADguez%2C+J+M">Josep M. Trigo-Rodr&#xed;guez</a>, 
<a href="/search/astro-ph?searchtype=author&query=Gr%C3%A8bol-Tom%C3%A0s%2C+P">Pau Gr&#xe8;bol-Tom&#xe0;s</a>, 
<a href="/search/astro-ph?searchtype=author&query=Regordosa-Avellana%2C+D">David Regordosa-Avellana</a>, 
<a href="/search/astro-ph?searchtype=author&query=Rimola%2C+A">Albert Rimola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Planetary and Space Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item347">Cross-lists</a></li>
<li><a href="#item385">Replacements</a></li>
</ul>
<small>[ total of 659 entries:  <b>1-659</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
