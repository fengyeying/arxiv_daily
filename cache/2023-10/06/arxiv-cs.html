<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Wed  4 Oct 23  to  Thu  5 Oct 23, announced Fri,  6 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item310">Cross-lists</a></li>
<li><a href="#item353">Replacements</a></li>
</ul>
<small>[ total of 580 entries:  <b>1-580</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Fri,  6 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03031" title="Abstract">arXiv:2310.03031</a> [<a href="/pdf/2310.03031" title="Download PDF">pdf</a>, <a href="/format/2310.03031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English  ChatGPT Responses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Urchs%2C+S">Stefanie Urchs</a>, 
<a href="/search/cs?searchtype=author&query=Thurner%2C+V">Veronika Thurner</a>, 
<a href="/search/cs?searchtype=author&query=A%C3%9Fenmacher%2C+M">Matthias A&#xdf;enmacher</a>, 
<a href="/search/cs?searchtype=author&query=Heumann%2C+C">Christian Heumann</a>, 
<a href="/search/cs?searchtype=author&query=Thiemichen%2C+S">Stephanie Thiemichen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted @ "1st Workshop on Biased Data in Conversational Agents" (co-located with ECML PKDD 2023). This is the author's version of the work. The definite version of record will be published in the proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the introduction of ChatGPT, OpenAI made large language models (LLM)
accessible to users with limited IT expertise. However, users with no
background in natural language processing (NLP) might lack a proper
understanding of LLMs. Thus the awareness of their inherent limitations, and
therefore will take the systems' output at face value. In this paper, we
systematically analyse prompts and the generated responses to identify possible
problematic issues with a special focus on gender biases, which users need to
be aware of when processing the system's output. We explore how ChatGPT reacts
in English and German if prompted to answer from a female, male, or neutral
perspective. In an in-depth investigation, we examine selected prompts and
analyse to what extent responses differ if the system is prompted several times
in an identical way. On this basis, we show that ChatGPT is indeed useful for
helping non-IT users draft texts for their daily work. However, it is
absolutely crucial to thoroughly check the system's responses for biases as
well as for syntactic and grammatical mistakes.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03032" title="Abstract">arXiv:2310.03032</a> [<a href="/pdf/2310.03032" title="Download PDF">pdf</a>, <a href="/format/2310.03032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-enhanced Optimizers for Structure-aware Recommendation Embedding  Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Cong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Embedding plays a critical role in modern recommender systems because they
are virtual representations of real-world entities and the foundation for
subsequent decision models. In this paper, we propose a novel embedding update
mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage
related nodes to evolve similarly at each step. Unlike GNN (Graph Neural
Network) that typically serves as an intermediate part, SEvo is able to
directly inject the graph structure information into embedding with negligible
computational overhead in training. The convergence properties of SEvo as well
as its possible variants are theoretically analyzed to justify the validity of
the designs. Moreover, SEvo can be seamlessly integrated into existing
optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW
with moment estimate correction demonstrates consistent improvements across a
spectrum of models and datasets, suggesting a novel technical route to
effectively utilize graph structure information beyond explicit GNN modules.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03033" title="Abstract">arXiv:2310.03033</a> [<a href="/pdf/2310.03033" title="Download PDF">pdf</a>, <a href="/format/2310.03033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Local Robustness of High-Accuracy Binary Neural Networks  for Enhanced Traffic Sign Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Postovan%2C+A">Andreea Postovan</a>, 
<a href="/search/cs?searchtype=author&query=Era%C5%9Fcu%2C+M">M&#x103;d&#x103;lina Era&#x15f;cu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FROM 2023, <a href="/abs/2309.12959">arXiv:2309.12959</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 389, 2023, pp. 120-130
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Traffic signs play a critical role in road safety and traffic management for
autonomous driving systems. Accurate traffic sign classification is essential
but challenging due to real-world complexities like adversarial examples and
occlusions. To address these issues, binary neural networks offer promise in
constructing classifiers suitable for resource-constrained devices.
<br />In our previous work, we proposed high-accuracy BNN models for traffic sign
recognition, focusing on compact size for limited computation and energy
resources. To evaluate their local robustness, this paper introduces a set of
benchmark problems featuring layers that challenge state-of-the-art
verification tools. These layers include binarized convolutions, max pooling,
batch normalization, fully connected. The difficulty of the verification
problem is given by the high number of network parameters (905k - 1.7 M), of
the input dimension (2.7k-12k), and of the number of regions (43) as well by
the fact that the neural networks are not sparse.
<br />The proposed BNN models and local robustness properties can be checked at
https://github.com/ChristopherBrix/vnncomp2023_benchmarks/tree/main/benchmarks/traffic_signs_recognition.
<br />The results of the 4th International Verification of Neural Networks
Competition (VNN-COMP'23) revealed the fact that 4, out of 7, solvers can
handle many of our benchmarks randomly selected (minimum is 6, maximum is 36,
out of 45). Surprisingly, tools output also wrong results or missing
counterexample (ranging from 1 to 4). Currently, our focus lies in exploring
the possibility of achieving a greater count of solved instances by extending
the allotted time (previously set at 8 minutes). Furthermore, we are intrigued
by the reasons behind the erroneous outcomes provided by the tools for certain
benchmarks.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03036" title="Abstract">arXiv:2310.03036</a> [<a href="/pdf/2310.03036" title="Download PDF">pdf</a>, <a href="/format/2310.03036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A quantum system control method based on enhanced reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bosi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jihao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yebo Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zidan%2C+M">Mohammed Zidan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Soft Computing, 2022. 26(14): p. 6567-6575
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Artificial Intelligence (cs.AI); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Traditional quantum system control methods often face different constraints,
and are easy to cause both leakage and stochastic control errors under the
condition of limited resources. Reinforcement learning has been proved as an
efficient way to complete the quantum system control task. To learn a
satisfactory control strategy under the condition of limited resources, a
quantum system control method based on enhanced reinforcement learning
(QSC-ERL) is proposed. The states and actions in reinforcement learning are
mapped to quantum states and control operations in quantum systems. By using
new enhanced neural networks, reinforcement learning can quickly achieve the
maximization of long-term cumulative rewards, and a quantum state can be
evolved accurately from an initial state to a target state. According to the
number of candidate unitary operations, the three-switch control is used for
simulation experiments. Compared with other methods, the QSC-ERL achieves close
to 1 fidelity learning control of quantum systems, and takes fewer episodes to
quantum state evolution under the condition of limited resources.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03037" title="Abstract">arXiv:2310.03037</a> [<a href="/pdf/2310.03037" title="Download PDF">pdf</a>, <a href="/format/2310.03037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum image edge detection based on eight-direction Sobel operator for  NEQR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 20 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Quantum Information Processing, 2022. 21(5): p. 190
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Emerging Technologies (cs.ET); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Quantum Sobel edge detection (QSED) is a kind of algorithm for image edge
detection using quantum mechanism, which can solve the real-time problem
encountered by classical algorithms. However, the existing QSED algorithms only
consider two- or four-direction Sobel operator, which leads to a certain loss
of edge detail information in some high-definition images. In this paper, a
novel QSED algorithm based on eight-direction Sobel operator is proposed, which
not only reduces the loss of edge information, but also simultaneously
calculates eight directions' gradient values of all pixel in a quantum image.
In addition, the concrete quantum circuits, which consist of gradient
calculation, non-maximum suppression, double threshold detection and edge
tracking units, are designed in details. For a 2^n x 2^n image with q gray
scale, the complexity of our algorithm can be reduced to O(n^2 + q^2), which is
lower than other existing classical or quantum algorithms. And the simulation
experiment demonstrates that our algorithm can detect more edge information,
especially diagonal edges, than the two- and four-direction QSED algorithms.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03038" title="Abstract">arXiv:2310.03038</a> [<a href="/pdf/2310.03038" title="Download PDF">pdf</a>, <a href="/format/2310.03038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A quantum moving target segmentation algorithm for grayscale video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingshan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 15 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advanced Quantum Technologies, 2023, In Production
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Emerging Technologies (cs.ET); Quantum Physics (quant-ph)

</div>
<p class="mathjax">The moving target segmentation (MTS) aims to segment out moving targets in
the video, however, the classical algorithm faces the huge challenge of
real-time processing in the current video era. Some scholars have successfully
demonstrated the quantum advantages in some video processing tasks, but not
concerning moving target segmentation. In this paper, a quantum moving target
segmentation algorithm for grayscale video is proposed, which can use quantum
mechanism to simultaneously calculate the difference of all pixels in all
adjacent frames and then quickly segment out the moving target. In addition, a
feasible quantum comparator is designed to distinguish the grayscale values
with the threshold. Then several quantum circuit units, including three-frame
difference, binarization and AND operation, are designed in detail, and then
are combined together to construct the complete quantum circuits for segmenting
the moving target. For a quantum video with $2^m$ frames (every frame is a
$2^n\times 2^n$ image with $q$ grayscale levels), the complexity of our
algorithm can be reduced to O$(n^2 + q)$. Compared with the classic
counterpart, it is an exponential speedup, while its complexity is also
superior to the existing quantum algorithms. Finally, the experiment is
conducted on IBM Q to show the feasibility of our algorithm in the noisy
intermediate-scale quantum (NISQ) era.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03043" title="Abstract">arXiv:2310.03043</a> [<a href="/pdf/2310.03043" title="Download PDF">pdf</a>, <a href="/format/2310.03043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Reinforcement Learning Approach for Interactive Search with  Sentence-level Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jianghong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+J+C">Joyce C. Ho</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Agichtein%2C+E">Eugene Agichtein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures, DRL4IR@CIKM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Interactive search can provide a better experience by incorporating
interaction feedback from the users. This can significantly improve search
accuracy as it helps avoid irrelevant information and captures the users'
search intents. Existing state-of-the-art (SOTA) systems use reinforcement
learning (RL) models to incorporate the interactions but focus on item-level
feedback, ignoring the fine-grained information found in sentence-level
feedback. Yet such feedback requires extensive RL action space exploration and
large amounts of annotated data. This work addresses these challenges by
proposing a new deep Q-learning (DQ) approach, DQrank. DQrank adapts BERT-based
models, the SOTA in natural language processing, to select crucial sentences
based on users' engagement and rank the items to obtain more satisfactory
responses. We also propose two mechanisms to better explore optimal actions.
DQrank further utilizes the experience replay mechanism in DQ to store the
feedback sentences to obtain a better initial ranking performance. We validate
the effectiveness of DQrank on three search datasets. The results show that
DQRank performs at least 12% better than the previous SOTA RL approaches. We
also conduct detailed ablation studies. The ablation results demonstrate that
each model component can efficiently extract and accumulate long-term
engagement effects from the users' sentence-level feedback. This structure
offers new technologies with promised performance to construct a search system
with sentence-level interaction.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03044" title="Abstract">arXiv:2310.03044</a> [<a href="/pdf/2310.03044" title="Download PDF">pdf</a>, <a href="/format/2310.03044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> scg-cli -- a Tool Supporting Software Comprehension via Extraction and  Analysis of Semantic Code Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borowski%2C+K">Krzysztof Borowski</a>, 
<a href="/search/cs?searchtype=author&query=Bali%C5%9B%2C+B">Bartosz Bali&#x15b;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">We present scg-cli, a~command line tool facilitating software comprehension.
The tool extracts semantic information about code structure and dependencies
from the Java and Scala projects, and structures it as a~Semantic Code Graph
(SCG), an information model underlying scg-cli. The SCG data, once written into
a~portable, open protobuf-based format, can be used by the scg-cli command line
tool to obtain project metrics, find the most critical code entities, and
compute project partitionings. The results of this analysis and the SCG data
can be exported for further investigation by external tools such as Gephi
software (visualization) and, notably, as a Jupyter Notebook environment with
helper APIs to enable advanced analysis of the project using data analytics
methods. We explain functionalities of the scg-cli tool and demonstrate its
capabilities by showing an example analysis of an open-source Java project
commons-io.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03046" title="Abstract">arXiv:2310.03046</a> [<a href="/pdf/2310.03046" title="Download PDF">pdf</a>, <a href="/format/2310.03046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EcoAssistant: Using LLM Assistant More Affordably and Accurately
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A+H">Ahmed H. Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Today, users ask Large language models (LLMs) as assistants to answer queries
that require external knowledge; they ask about the weather in a specific city,
about stock prices, and even about where specific locations are within their
neighborhood. These queries require the LLM to produce code that invokes
external APIs to answer the user's question, yet LLMs rarely produce correct
code on the first try, requiring iterative code refinement upon execution
results. In addition, using LLM assistants to support high query volumes can be
expensive. In this work, we contribute a framework, EcoAssistant, that enables
LLMs to answer code-driven queries more affordably and accurately. EcoAssistant
contains three components. First, it allows the LLM assistants to converse with
an automatic code executor to iteratively refine code or to produce answers
based on the execution results. Second, we use a hierarchy of LLM assistants,
which attempts to answer the query with weaker, cheaper LLMs before backing off
to stronger, expensive ones. Third, we retrieve solutions from past successful
queries as in-context demonstrations to help subsequent queries. Empirically,
we show that EcoAssistant offers distinct advantages for affordability and
accuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of
GPT-4's cost.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03049" title="Abstract">arXiv:2310.03049</a> [<a href="/pdf/2310.03049" title="Download PDF">pdf</a>, <a href="/format/2310.03049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuATON: Quantization Aware Training of Optical Neurons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kariyawasam%2C+H">Hasindu Kariyawasam</a>, 
<a href="/search/cs?searchtype=author&query=Hettiarachchi%2C+R">Ramith Hettiarachchi</a>, 
<a href="/search/cs?searchtype=author&query=Wadduwage%2C+D">Dushan Wadduwage</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Image and Video Processing (eess.IV); Optics (physics.optics)

</div>
<p class="mathjax">Optical neural architectures (ONAs) use coding elements with optimized
physical parameters to perform intelligent measurements. However, fabricating
ONAs while maintaining design performances is challenging. Limitations in
fabrication techniques often limit the realizable precision of the trained
parameters. Physical constraints may also limit the range of values the
physical parameters can hold. Thus, ONAs should be trained within the
implementable constraints. However, such physics-based constraints reduce the
training objective to a constrained optimization problem, making it harder to
optimize with existing gradient-based methods. To alleviate these critical
issues that degrade performance from simulation to realization we propose a
physics-informed quantization-aware training framework. Our approach accounts
for the physical constraints during the training process, leading to robust
designs. We evaluate our approach on an ONA proposed in the literature, named a
diffractive deep neural network (D2NN), for all-optical phase imaging and for
classification of phase objects. With extensive experiments on different
quantization levels and datasets, we show that our approach leads to ONA
designs that are robust to quantization noise.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03051" title="Abstract">arXiv:2310.03051</a> [<a href="/pdf/2310.03051" title="Download PDF">pdf</a>, <a href="/format/2310.03051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How FaR Are Large Language Models From Agents with Theory-of-Mind?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+A">Aman Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Potharaju%2C+S+P">Srividya Pranavi Potharaju</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aditya Gupta</a>, 
<a href="/search/cs?searchtype=author&query=McKee%2C+K+R">Kevin R. McKee</a>, 
<a href="/search/cs?searchtype=author&query=Holtzman%2C+A">Ari Holtzman</a>, 
<a href="/search/cs?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Nematzadeh%2C+A">Aida Nematzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Upadhyay%2C+S">Shyam Upadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Faruqui%2C+M">Manaal Faruqui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, 18 pages, 6 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">"Thinking is for Doing." Humans can infer other people's mental states from
observations--an ability called Theory-of-Mind (ToM)--and subsequently act
pragmatically on those inferences. Existing question answering benchmarks such
as ToMi ask models questions to make inferences about beliefs of characters in
a story, but do not test whether models can then use these inferences to guide
their actions. We propose a new evaluation paradigm for large language models
(LLMs): Thinking for Doing (T4D), which requires models to connect inferences
about others' mental states to actions in social scenarios. Experiments on T4D
demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking
characters' beliefs in stories, but they struggle to translate this capability
into strategic action. Our analysis reveals the core challenge for LLMs lies in
identifying the implicit inferences about mental states without being
explicitly asked about as in ToMi, that lead to choosing the correct action in
T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee
and Reflect (FaR), which provides a reasoning structure that encourages LLMs to
anticipate future challenges and reason about potential actions. FaR boosts
GPT-4's performance from 50% to 71% on T4D, outperforming other prompting
methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to
diverse out-of-distribution story structures and scenarios that also require
ToM inferences to choose an action, consistently outperforming other methods
including few-shot in-context learning.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03052" title="Abstract">arXiv:2310.03052</a> [<a href="/pdf/2310.03052" title="Download PDF">pdf</a>, <a href="/format/2310.03052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memoria: Hebbian Memory Architecture for Human-Like Sequential  Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sangjun Park</a>, 
<a href="/search/cs?searchtype=author&query=Bak%2C+J">JinYeong Bak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review as a conference paper at ICLR 2024. 20 pages, 9 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Transformers have demonstrated their success in various domains and tasks.
However, Transformers struggle with long input sequences due to their limited
capacity. While one solution is to increase input length, endlessly stretching
the length is unrealistic. Furthermore, humans selectively remember and use
only relevant information from inputs, unlike Transformers which process all
raw data from start to end. We introduce Memoria, a general memory network that
applies Hebbian theory which is a major theory explaining human memory
formulation to enhance long-term dependencies in neural networks. Memoria
stores and retrieves information called engram at multiple memory levels of
working memory, short-term memory, and long-term memory, using connection
weights that change according to Hebb's rule. Through experiments with popular
Transformer-based models like BERT and GPT, we present that Memoria
significantly improves the ability to consider long-term dependencies in
various tasks. Results show that Memoria outperformed existing methodologies in
sorting and language modeling, and long text classification.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03055" title="Abstract">arXiv:2310.03055</a> [<a href="/pdf/2310.03055" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modified LAB Algorithm with Clustering-based Search Space Reduction  Method for solving Engineering Design Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reddy%2C+R">Ruturaj Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+U">Utkarsh Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Kale%2C+I">Ishaan Kale</a>, 
<a href="/search/cs?searchtype=author&query=Shastri%2C+A">Apoorva Shastri</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A+J">Anand J Kulkarni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A modified LAB algorithm is introduced in this paper. It builds upon the
original LAB algorithm (Reddy et al. 2023), which is a socio-inspired algorithm
that models competitive and learning behaviours within a group, establishing
hierarchical roles. The proposed algorithm incorporates the roulette wheel
approach and a reduction factor introducing inter-group competition and
iteratively narrowing down the sample space. The algorithm is validated by
solving the benchmark test problems from CEC 2005 and CEC 2017. The solutions
are validated using standard statistical tests such as two-sided and pairwise
signed rank Wilcoxon test and Friedman rank test. The algorithm exhibited
improved and superior robustness as well as search space exploration
capabilities. Furthermore, a Clustering-Based Search Space Reduction (C-SSR)
method is proposed, making the algorithm capable to solve constrained problems.
The C-SSR method enables the algorithm to identify clusters of feasible
regions, satisfying the constraints and contributing to achieve the optimal
solution. This method demonstrates its effectiveness as a potential alternative
to traditional constraint handling techniques. The results obtained using the
Modified LAB algorithm are then compared with those achieved by other recent
metaheuristic algorithms.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03056" title="Abstract">arXiv:2310.03056</a> [<a href="/pdf/2310.03056" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-carbon optimal dispatch of integrated energy system considering  demand response under the tiered carbon trading mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Limeng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xuemeng Liu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+D">Duo Chang</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+X">Xing Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Electric Power Construction [in Chinese]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In the operation of the integrated energy system (IES), considering further
reducing carbon emissions, improving its energy utilization rate, and
optimizing and improving the overall operation of IES, an optimal dispatching
strategy of integrated energy system considering demand response under the
stepped carbon trading mechanism is proposed. Firstly, from the perspective of
demand response (DR), considering the synergistic complementarity and flexible
conversion ability of multiple energy sources, the lateral time-shifting and
vertical complementary alternative strategies of electricity-gas-heat are
introduced and the DR model is constructed. Secondly, from the perspective of
life cycle assessment, the initial quota model of carbon emission allowances is
elaborated and revised. Then introduce a tiered carbon trading mechanism, which
has a certain degree of constraint on the carbon emissions of IES. Finally, the
sum of energy purchase cost, carbon emission transaction cost, equipment
maintenance cost and demand response cost is minimized, and a low-carbon
optimal scheduling model is constructed under the consideration of safety
constraints. This model transforms the original problem into a mixed integer
linear problem using Matlab software, and optimizes the model using the CPLEX
solver. The example results show that considering the carbon trading cost and
demand response under the tiered carbon trading mechanism, the total operating
cost of IES is reduced by 5.69% and the carbon emission is reduced by 17.06%,
which significantly improves the reliability, economy and low carbon
performance of IES.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03057" title="Abstract">arXiv:2310.03057</a> [<a href="/pdf/2310.03057" title="Download PDF">pdf</a>, <a href="/ps/2310.03057" title="Download PostScript">ps</a>, <a href="/format/2310.03057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithm for finding new identifiable reparametrizations of parametric  ODEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Meshkat%2C+N">Nicolette Meshkat</a>, 
<a href="/search/eess?searchtype=author&query=Ovchinnikov%2C+A">Alexey Ovchinnikov</a>, 
<a href="/search/eess?searchtype=author&query=Scanlon%2C+T">Thomas Scanlon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Algebraic Geometry (math.AG); Dynamical Systems (math.DS); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">The parameter identifiability problem for a dynamical system is to determine
whether the parameters of the system can be found from data for the outputs of
the system. Verifying whether the parameters are identifiable is a necessary
first step before a meaningful parameter estimation can take place.
Non-identifiability occurs in practical models. To reparametrize a model to
achieve identifiability is a challenge. The existing approaches have been shown
to be useful for many important examples. However, these approaches are either
limited to linear models and scaling parametrizations or are not guaranteed to
find a reparametrization even if it exists. In the present paper, we prove that
there always exists a locally identifiable model with the same input-output
behaviour as the original one obtained from a given one by a partial
specialization of the parameters. Furthermore, we give a sufficient
observability condition for the existence of a state space transformation from
the original model to the new one. Our proof is constructive and can be
translated to an algorithm, which we illustrate by several examples.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03059" title="Abstract">arXiv:2310.03059</a> [<a href="/pdf/2310.03059" title="Download PDF">pdf</a>, <a href="/format/2310.03059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+I">Ivan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Eric Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+R">Ray Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages. The specialized PEFT framework for 3D pre-trained models, which achieves competitive performance to full fine-tuning, and significantly reduces the computational resources. Project page: <a href="https://github.com/EvenJoker/Point-PEFT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/EvenJoker/Point-PEFT.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03084" title="Abstract">arXiv:2310.03084</a> [<a href="/pdf/2310.03084" title="Download PDF">pdf</a>, <a href="/format/2310.03084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Knowledge-Critical Subnetworks in Pretrained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bayazit%2C+D">Deniz Bayazit</a>, 
<a href="/search/cs?searchtype=author&query=Foroutan%2C+N">Negar Foroutan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zeming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+G">Gail Weiss</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Pretrained language models (LMs) encode implicit representations of knowledge
in their parameters. However, localizing these representations and
disentangling them from each other remains an open problem. In this work, we
investigate whether pretrained language models contain various
knowledge-critical subnetworks: particular sparse computational subgraphs
responsible for encoding specific knowledge the model has memorized. We propose
a multi-objective differentiable weight masking scheme to discover these
subnetworks and show that we can use them to precisely remove specific
knowledge from models while minimizing adverse effects on the behavior of the
original language model. We demonstrate our method on multiple GPT2 variants,
uncovering highly sparse subnetworks (98%+) that are solely responsible for
specific collections of relational knowledge. When these subnetworks are
removed, the remaining network maintains most of its initial capacity (modeling
language and other memorized relational knowledge) but struggles to express the
removed knowledge, and suffers performance drops on examples needing this
removed knowledge on downstream tasks after finetuning.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03085" title="Abstract">arXiv:2310.03085</a> [<a href="/pdf/2310.03085" title="Download PDF">pdf</a>, <a href="/format/2310.03085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Batch-less stochastic gradient descent for compressive learning of deep  regularization for image denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hui Shi</a> (IMB), 
<a href="/search/cs?searchtype=author&query=Traonmilin%2C+Y">Yann Traonmilin</a> (IMB), 
<a href="/search/cs?searchtype=author&query=Aujol%2C+J">J-F Aujol</a> (IMB)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">We consider the problem of denoising with the help of prior information taken
from a database of clean signals or images. Denoising with variational methods
is very efficient if a regularizer well adapted to the nature of the data is
available. Thanks to the maximum a posteriori Bayesian framework, such
regularizer can be systematically linked with the distribution of the data.
With deep neural networks (DNN), complex distributions can be recovered from a
large training database.To reduce the computational burden of this task, we
adapt the compressive learning framework to the learning of regularizers
parametrized by DNN. We propose two variants of stochastic gradient descent
(SGD) for the recovery of deep regularization parameters from a heavily
compressed database. These algorithms outperform the initially proposed method
that was limited to low-dimensional signals, each iteration using information
from the whole database. They also benefit from classical SGD convergence
guarantees. Thanks to these improvements we show that this method can be
applied for patch based image denoising.}
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03086" title="Abstract">arXiv:2310.03086</a> [<a href="/pdf/2310.03086" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning in Computational Biology: Advancements, Challenges, and  Future Outlook
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Suresh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Guruparan%2C+D">Dhanyashri Guruparan</a>, 
<a href="/search/cs?searchtype=author&query=Aaron%2C+P">Pavithren Aaron</a>, 
<a href="/search/cs?searchtype=author&query=Telajan%2C+P">Philemon Telajan</a>, 
<a href="/search/cs?searchtype=author&query=Mahadevan%2C+K">Kavinesh Mahadevan</a>, 
<a href="/search/cs?searchtype=author&query=Davagandhi%2C+D">Dinesh Davagandhi</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+O+X">Ong Xin Yue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, 1 figure, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Deep learning has become a powerful tool in computational biology,
revolutionising the analysis and interpretation of biological data over time.
In our article review, we delve into various aspects of deep learning in
computational biology. Specifically, we examine its history, advantages, and
challenges. Our focus is on two primary applications: DNA sequence
classification and prediction, as well as protein structure prediction from
sequence data. Additionally, we provide insights into the outlook for this
field. To fully harness the potential of deep learning in computational
biology, it is crucial to address the challenges that come with it. These
challenges include the requirement for large, labelled datasets and the
interpretability of deep learning models. The use of deep learning in the
analysis of DNA sequences has brought about a significant transformation in the
detection of genomic variants and the analysis of gene expression. This has
greatly contributed to the advancement of personalised medicine and drug
discovery. Convolutional neural networks (CNNs) have been shown to be highly
accurate in predicting genetic variations and gene expression levels. Deep
learning techniques are used for analysing epigenetic data, including DNA
methylation and histone modifications. This provides valuable insights into
metabolic conditions and gene regulation. The field of protein structure
prediction has been significantly impacted by deep learning, which has enabled
accurate determination of the three-dimensional shape of proteins and
prediction of their interactions. The future of deep learning in computational
biology looks promising. With the development of advanced deep learning models
and interpretation techniques, there is potential to overcome current
challenges and further our understanding of biological systems.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03088" title="Abstract">arXiv:2310.03088</a> [<a href="/pdf/2310.03088" title="Download PDF">pdf</a>, <a href="/format/2310.03088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Informed Neural Networks for Accelerating Power System State  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Falas%2C+S">Solon Falas</a>, 
<a href="/search/cs?searchtype=author&query=Asprou%2C+M">Markos Asprou</a>, 
<a href="/search/cs?searchtype=author&query=Konstantinou%2C+C">Charalambos Konstantinou</a>, 
<a href="/search/cs?searchtype=author&query=Michael%2C+M+K">Maria K. Michael</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">State estimation is the cornerstone of the power system control center since
it provides the operating condition of the system in consecutive time
intervals. This work investigates the application of physics-informed neural
networks (PINNs) for accelerating power systems state estimation in monitoring
the operation of power systems. Traditional state estimation techniques often
rely on iterative algorithms that can be computationally intensive,
particularly for large-scale power systems. In this paper, a novel approach
that leverages the inherent physical knowledge of power systems through the
integration of PINNs is proposed. By incorporating physical laws as prior
knowledge, the proposed method significantly reduces the computational
complexity associated with state estimation while maintaining high accuracy.
The proposed method achieves up to 11% increase in accuracy, 75% reduction in
standard deviation of results, and 30% faster convergence, as demonstrated by
comprehensive experiments on the IEEE 14-bus system.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03089" title="Abstract">arXiv:2310.03089</a> [<a href="/pdf/2310.03089" title="Download PDF">pdf</a>, <a href="/format/2310.03089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An adaptive stabilized trace finite element method for surface PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Heister%2C+T">Timo Heister</a>, 
<a href="/search/math?searchtype=author&query=Olshanskii%2C+M+A">Maxim A. Olshanskii</a>, 
<a href="/search/math?searchtype=author&query=Yushutin%2C+V">Vladimir Yushutin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The paper introduces an adaptive version of the stabilized Trace Finite
Element Method (TraceFEM) designed to solve low-regularity elliptic problems on
level-set surfaces using a shape-regular bulk mesh in the embedding space. Two
stabilization variants, gradient-jump face and normal-gradient volume, are
considered for continuous trace spaces of the first and second degrees, based
on the polynomial families $Q_1$ and $Q_2$. We propose a practical error
indicator that estimates the `jumps' of finite element solution derivatives
across background mesh faces and it avoids integration of any quantities along
implicitly defined curvilinear edges of the discrete surface elements. For the
$Q_1$ family of piecewise trilinear polynomials on bulk cells, the
solve-estimate-mark-refine strategy, combined with the suggested error
indicator, achieves optimal convergence rates typical of two-dimensional
problems. We also provide a posteriori error estimates, establishing the
reliability of the error indicator for the $Q_1$ and $Q_2$ elements and for two
types of stabilization. In numerical experiments, we assess the reliability and
efficiency of the error indicator. While both stabilizations are found to
deliver comparable performance,the lowest degree finite element space appears
to be the more robust choice for the adaptive TraceFEM framework.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03091" title="Abstract">arXiv:2310.03091</a> [<a href="/pdf/2310.03091" title="Download PDF">pdf</a>, <a href="/format/2310.03091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-preserving Multi-biometric Indexing based on Frequent Binary  Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osorio-Roig%2C+D">Daile Osorio-Roig</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez-Soler%2C+L+J">Lazaro J. Gonzalez-Soler</a>, 
<a href="/search/cs?searchtype=author&query=Rathgeb%2C+C">Christian Rathgeb</a>, 
<a href="/search/cs?searchtype=author&query=Busch%2C+C">Christoph Busch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The development of large-scale identification systems that ensure the privacy
protection of enrolled subjects represents a major challenge. Biometric
deployments that provide interoperability and usability by including efficient
multi-biometric solutions are a recent requirement. In the context of privacy
protection, several template protection schemes have been proposed in the past.
However, these schemes seem inadequate for indexing (workload reduction) in
biometric identification systems. More specifically, they have been used in
identification systems that perform exhaustive searches, leading to a
degradation of computational efficiency. To overcome these limitations, we
propose an efficient privacy-preserving multi-biometric identification system
that retrieves protected deep cancelable templates and is agnostic with respect
to biometric characteristics and biometric template protection schemes. To this
end, a multi-biometric binning scheme is designed to exploit the low
intra-class variation properties contained in the frequent binary patterns
extracted from different types of biometric characteristics. Experimental
results reported on publicly available databases using state-of-the-art Deep
Neural Network (DNN)-based embedding extractors show that the protected
multi-biometric identification system can reduce the computational workload to
approximately 57\% (indexing up to three types of biometric characteristics)
and 53% (indexing up to two types of biometric characteristics), while
simultaneously improving the biometric performance of the baseline biometric
system at the high-security thresholds. The source code of the proposed
multi-biometric indexing approach together with the composed multi-biometric
dataset, will be made available to the research community once the article is
accepted.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03094" title="Abstract">arXiv:2310.03094</a> [<a href="/pdf/2310.03094" title="Download PDF">pdf</a>, <a href="/format/2310.03094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Cascades with Mixture of Thoughts Representations  for Cost-efficient Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+M">Murong Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Liang Du</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Ziyu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) such as GPT-4 have exhibited remarkable
performance in a variety of tasks, but this strong performance often comes with
the high expense of using paid API services. In this paper, we are motivated to
study building an LLM cascade to save the cost of using LLMs, particularly for
performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline
follows the intuition that simpler questions can be addressed by a weaker but
more affordable LLM, whereas only the challenging questions necessitate the
stronger and more expensive LLM. To realize this decision-making, we consider
the "answer consistency" of the weaker LLM as a signal of the question
difficulty and propose several methods for the answer sampling and consistency
checking, including one leveraging a mixture of two thought representations
(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six
reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and
stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can
achieve performance comparable to using solely the stronger LLM but require
only 40% of its cost.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03095" title="Abstract">arXiv:2310.03095</a> [<a href="/pdf/2310.03095" title="Download PDF">pdf</a>, <a href="/ps/2310.03095" title="Download PostScript">ps</a>, <a href="/format/2310.03095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Opinion Dynamics Optimization Through Noncooperative Differential Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jond%2C+H+B">Hossein B. Jond</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was presented at 2023 9th International Conference on Control, Decision and Information Technologies (CoDIT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">In this paper, I study optimizing the opinion formation of a social network
of a population of individuals on a graph whose opinion evolves according to
the Hegselmann-Krause model for opinion dynamics. I propose an optimization
problem based on a differential game for a population of individuals who are
not stubborn. The objective of each individual is to seek an optimal control
policy for her own opinion evolution by optimizing a personal performance
index. The Nash equilibrium actions and the associated opinion trajectory with
the equilibrium actions are derived for the opinion optimization model using
Pontryagin's principle. The game strategies were executed on the well-known
Zachary's Karate Club social network. The resulting opinion trajectories
associated with the game strategies showed that in non-stubborn Zachary's
network, the opinions moved toward the average opinion of the network, but a
consensus of final opinions did not necessarily emerge.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03098" title="Abstract">arXiv:2310.03098</a> [<a href="/pdf/2310.03098" title="Download PDF">pdf</a>, <a href="/format/2310.03098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NOCAP: Near-Optimal Correlation-Aware Partitioning Joins
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zichen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Athanassoulis%2C+M">Manos Athanassoulis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Storage-based joins are still commonly used today because the memory budget
does not always scale with the data size. One of the many join algorithms
developed that has been widely deployed and proven to be efficient is the
Hybrid Hash Join (HHJ), which is designed to exploit any available memory to
maximize the data that is joined directly in memory. However, HHJ cannot fully
exploit detailed knowledge of the join attribute correlation distribution.
<br />In this paper, we show that given a correlation skew in the join attributes,
HHJ partitions data in a suboptimal way. To do that, we derive the optimal
partitioning using a new cost-based analysis of partitioning-based joins that
is tailored for primary key - foreign key (PK-FK) joins, one of the most common
join types. This optimal partitioning strategy has a high memory cost, thus, we
further derive an approximate algorithm that has tunable memory cost and leads
to near-optimal results. Our algorithm, termed NOCAP (Near-Optimal
Correlation-Aware Partitioning) join, outperforms the state-of-the-art for
skewed correlations by up to $30\%$, and the textbook Grace Hash Join by up to
$4\times$. Further, for a limited memory budget, NOCAP outperforms HHJ by up to
$10\%$, even for uniform correlation. Overall, NOCAP dominates state-of-the-art
algorithms and mimics the best algorithm for a memory budget varying from below
$\sqrt{\|\text{relation}\|}$ to more than $\|\text{relation}\|$.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03103" title="Abstract">arXiv:2310.03103</a> [<a href="/pdf/2310.03103" title="Download PDF">pdf</a>, <a href="/format/2310.03103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Prompt Tuning for Domain-Aware Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Guoyizhe Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Anshul Shah</a>, 
<a href="/search/cs?searchtype=author&query=Chellappa%2C+R">Rama Chellappa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning is a distributed machine learning paradigm that allows
multiple clients to collaboratively train a shared model with their local data.
Nonetheless, conventional federated learning algorithms often struggle to
generalize well due to the ubiquitous domain shift across clients. In this
work, we consider a challenging yet realistic federated learning scenario where
the training data of each client originates from different domains. We address
the challenges of domain shift by leveraging the technique of prompt learning,
and propose a novel method called Federated Dual Prompt Tuning (Fed-DPT).
Specifically, Fed-DPT employs a pre-trained vision-language model and then
applies both visual and textual prompt tuning to facilitate domain adaptation
over decentralized data. Extensive experiments of Fed-DPT demonstrate its
significant effectiveness in domain-aware federated learning. With a
pre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT
attains 68.4% average accuracy over six domains in the DomainNet dataset, which
improves the original CLIP by a large margin of 14.8%.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03104" title="Abstract">arXiv:2310.03104</a> [<a href="/pdf/2310.03104" title="Download PDF">pdf</a>, <a href="/format/2310.03104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DP-SGD for non-decomposable objective functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+W">William Kong</a>, 
<a href="/search/cs?searchtype=author&query=Medina%2C+A+M">Andr&#xe9;s Mu&#xf1;oz Medina</a>, 
<a href="/search/cs?searchtype=author&query=Ribero%2C+M">M&#xf3;nica Ribero</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Unsupervised pre-training is a common step in developing computer vision
models and large language models. In this setting, the absence of labels
requires the use of similarity-based loss functions, such as contrastive loss,
that favor minimizing the distance between similar inputs and maximizing the
distance between distinct inputs. As privacy concerns mount, training these
models using differential privacy has become more important. However, due to
how inputs are generated for these losses, one of their undesirable properties
is that their $L_2$ sensitivity can grow with increasing batch size. This
property is particularly disadvantageous for differentially private training
methods, such as DP-SGD. To overcome this issue, we develop a new DP-SGD
variant for similarity based loss functions -- in particular the commonly used
contrastive loss -- that manipulates gradients of the objective function in a
novel way to obtain a senstivity of the summed gradient that is $O(1)$ for
batch size $n$. We test our DP-SGD variant on some preliminary CIFAR-10
pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, our
method's performance comes close to that of a non-private model and generally
outperforms DP-SGD applied directly to the contrastive loss.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03105" title="Abstract">arXiv:2310.03105</a> [<a href="/pdf/2310.03105" title="Download PDF">pdf</a>, <a href="/format/2310.03105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficiency of the Generalized Second-Price Auction for Value Maximizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuan Deng</a>, 
<a href="/search/cs?searchtype=author&query=Mahdian%2C+M">Mohammad Mahdian</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jieming Mao</a>, 
<a href="/search/cs?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+S">Song Zuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study the price of anarchy of the generalized second-price auction where
bidders are value maximizers (i.e., autobidders). We show that in general the
price of anarchy can be as bad as $0$. For comparison, the price of anarchy of
running VCG is $1/2$ in the autobidding world. We further show a fined-grained
price of anarchy with respect to the discount factors (i.e., the ratios of
click probabilities between lower slots and the highest slot in each auction)
in the generalized second-price auction, which highlights the qualitative
relation between the smoothness of the discount factors and the efficiency of
the generalized second-price auction.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03108" title="Abstract">arXiv:2310.03108</a> [<a href="/pdf/2310.03108" title="Download PDF">pdf</a>, <a href="/format/2310.03108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning-based Mixture of Vision Transformers for Video  Violence Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+H">Hamid Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Nazerfard%2C+E">Ehsan Nazerfard</a>, 
<a href="/search/cs?searchtype=author&query=Firoozi%2C+T">Tahereh Firoozi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video violence recognition based on deep learning concerns accurate yet
scalable human violence recognition. Currently, most state-of-the-art video
violence recognition studies use CNN-based models to represent and categorize
videos. However, recent studies suggest that pre-trained transformers are more
accurate than CNN-based models on various video analysis benchmarks. Yet these
models are not thoroughly evaluated for video violence recognition. This paper
introduces a novel transformer-based Mixture of Experts (MoE) video violence
recognition system. Through an intelligent combination of large vision
transformers and efficient transformer architectures, the proposed system not
only takes advantage of the vision transformer architecture but also reduces
the cost of utilizing large vision transformers. The proposed architecture
maximizes violence recognition system accuracy while actively reducing
computational costs through a reinforcement learning-based router. The
empirical results show the proposed MoE architecture's superiority over
CNN-based models by achieving 92.4% accuracy on the RWF dataset.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03111" title="Abstract">arXiv:2310.03111</a> [<a href="/pdf/2310.03111" title="Download PDF">pdf</a>, <a href="/format/2310.03111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-modal Gaussian Process Variational Autoencoders for Neural and  Behavioral Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gondur%2C+R">Rabia Gondur</a>, 
<a href="/search/cs?searchtype=author&query=Sikandar%2C+U+B">Usama Bin Sikandar</a>, 
<a href="/search/cs?searchtype=author&query=Schaffer%2C+E">Evan Schaffer</a>, 
<a href="/search/cs?searchtype=author&query=Aoi%2C+M+C">Mikio Christian Aoi</a>, 
<a href="/search/cs?searchtype=author&query=Keeley%2C+S+L">Stephen L Keeley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Characterizing the relationship between neural population activity and
behavioral data is a central goal of neuroscience. While latent variable models
(LVMs) are successful in describing high-dimensional time-series data, they are
typically only designed for a single type of data, making it difficult to
identify structure shared across different experimental data modalities. Here,
we address this shortcoming by proposing an unsupervised LVM which extracts
temporally evolving shared and independent latents for distinct, simultaneously
recorded experimental modalities. We do this by combining Gaussian Process
Factor Analysis (GPFA), an interpretable LVM for neural spiking data with
temporally smooth latent space, with Gaussian Process Variational Autoencoders
(GP-VAEs), which similarly use a GP prior to characterize correlations in a
latent space, but admit rich expressivity due to a deep neural network mapping
to observations. We achieve interpretability in our model by partitioning
latent variability into components that are either shared between or
independent to each modality. We parameterize the latents of our model in the
Fourier domain, and show improved latent identification using this approach
over standard GP-VAE methods. We validate our model on simulated multi-modal
data consisting of Poisson spike counts and MNIST images that scale and rotate
smoothly over time. We show that the multi-modal GP-VAE (MM-GPVAE) is able to
not only identify the shared and independent latent structure across modalities
accurately, but provides good reconstructions of both images and neural rates
on held-out trials. Finally, we demonstrate our framework on two real world
multi-modal experimental settings: Drosophila whole-brain calcium imaging
alongside tracked limb positions, and Manduca sexta spike train measurements
from ten wing muscles as the animal tracks a visual stimulus.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03119" title="Abstract">arXiv:2310.03119</a> [<a href="/pdf/2310.03119" title="Download PDF">pdf</a>, <a href="/format/2310.03119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crossed-IoT device portability of Electromagnetic Side Channel Analysis:  Challenges and Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yasarathna%2C+T+L">Tharindu Lakshan Yasarathna</a>, 
<a href="/search/cs?searchtype=author&query=Navanesan%2C+L">Lojenaa Navanesan</a>, 
<a href="/search/cs?searchtype=author&query=Barque%2C+S">Simon Barque</a>, 
<a href="/search/cs?searchtype=author&query=Sayakkara%2C+A">Assanka Sayakkara</a>, 
<a href="/search/cs?searchtype=author&query=Le-Khac%2C+N">Nhien-An Le-Khac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> for associated dataset file, see <a href="https://aseados.ucd.ie/datasets/EMSCA-2023-Latest/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">IoT (Internet of Things) refers to the network of interconnected physical
devices, vehicles, home appliances, and other items embedded with sensors,
software, and connectivity, enabling them to collect and exchange data. IoT
Forensics is collecting and analyzing digital evidence from IoT devices to
investigate cybercrimes, security breaches, and other malicious activities that
may have taken place on these connected devices. In particular, EM-SCA has
become an essential tool for IoT forensics due to its ability to reveal
confidential information about the internal workings of IoT devices without
interfering these devices or wiretapping their networks. However, the accuracy
and reliability of EM-SCA results can be limited by device variability,
environmental factors, and data collection and processing methods. Besides,
there is very few research on these limitations that affects significantly the
accuracy of EM-SCA approaches for the crossed-IoT device portability as well as
limited research on the possible solutions to address such challenge.
Therefore, this empirical study examines the impact of device variability on
the accuracy and reliability of EM-SCA approaches, in particular
machine-learning (ML) based approaches for EM-SCA. We firstly presents the
background, basic concepts and techniques used to evaluate the limitations of
current EM-SCA approaches and datasets. Our study then addresses one of the
most important limitation, which is caused by the multi-core architecture of
the processors (SoC). We present an approach to collect the EM-SCA datasets and
demonstrate the feasibility of using transfer learning to obtain more
meaningful and reliable results from EM-SCA in IoT forensics of crossed-IoT
devices. Our study moreover contributes a new dataset for using deep learning
models in analysing Electromagnetic Side-Channel data with regards to the
cross-device portability matter.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03122" title="Abstract">arXiv:2310.03122</a> [<a href="/pdf/2310.03122" title="Download PDF">pdf</a>, <a href="/format/2310.03122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPH-based framework for modelling fluid-structure interaction problems  with finite deformation and fracturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+R+I">Md Rushdie Ibne Islam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Understanding crack propagation in structures subjected to fluid loads is
crucial in various engineering applications, ranging from underwater pipelines
to aircraft components. This study investigates the dynamic response of
structures, including their damage and fracture behaviour under hydrodynamic
load, emphasizing the fluid-structure interaction (FSI) phenomena by applying
Smoothed Particle Hydrodynamics (SPH). The developed framework employs weakly
compressible SPH (WCSPH) to model the fluid flow and a pseudo-spring-based SPH
solver for modelling the structural response. For improved accuracy in FSI
modelling, the $\delta$-SPH technique is implemented to enhance pressure
calculations within the fluid phase. The pseudo-spring analogy is employed for
modelling material damage, where particle interactions are confined to their
immediate neighbours. These particles are linked by springs, which don't
contribute to system stiffness but determine the interaction strength between
connected pairs. It is assumed that a crack propagates through a spring
connecting a particle pair when the damage indicator of that spring exceeds a
predefined threshold. The developed framework is extensively validated through
a dam break case, oscillation of a deformable solid beam, dam break through a
deformable elastic solid, and breaking dam impact on a deformable solid
obstacle. Numerical outcomes are subsequently compared with the findings from
existing literature. The ability of the framework to accurately depict material
damage and fracture is showcased through a simulation of water impact on a
deformable solid obstacle with an initial notch.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03123" title="Abstract">arXiv:2310.03123</a> [<a href="/pdf/2310.03123" title="Download PDF">pdf</a>, <a href="/format/2310.03123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zihao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yifan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lifu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the blowout development of pre-trained models (PTMs), the efficient
tuning of these models for diverse downstream applications has emerged as a
pivotal research concern. Although recent investigations into prompt tuning
have provided promising avenues, three salient challenges persist: (1) memory
constraint: the continuous growth in the size of open-source PTMs renders
fine-tuning, even a fraction of their parameters, challenging for many
practitioners. (2) model privacy: existing PTMs often function as public API
services, with their parameters inaccessible for effective or tailored
fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates
high-quality datasets, which are typically localized and not shared to public.
To optimally harness each local dataset while navigating memory constraints and
preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT).
This innovative approach eschews reliance on parameter architectures and
private dataset access, instead capitalizing on a central server that aids
local users in collaboratively training a prompt generator through regular
aggregation. Local users leverage API-driven learning via a zero-order
optimizer, obviating the need for PTM deployment. Relative to extensive
fine-tuning, Fed-BBPT proficiently sidesteps memory challenges tied to PTM
storage and fine-tuning on local machines, tapping into comprehensive,
high-quality, yet private training datasets. A thorough evaluation across 40
datasets spanning CV and NLP tasks underscores the robustness of our proposed
model.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03125" title="Abstract">arXiv:2310.03125</a> [<a href="/pdf/2310.03125" title="Download PDF">pdf</a>, <a href="/format/2310.03125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shielding the Unseen: Privacy Protection through Poisoning NeRF with  Spatial Deformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yihan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+B+Y">Brandon Y. Feng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we introduce an innovative method of safeguarding user privacy
against the generative capabilities of Neural Radiance Fields (NeRF) models.
Our novel poisoning attack method induces changes to observed views that are
imperceptible to the human eye, yet potent enough to disrupt NeRF's ability to
accurately reconstruct a 3D scene. To achieve this, we devise a bi-level
optimization algorithm incorporating a Projected Gradient Descent (PGD)-based
spatial deformation. We extensively test our approach on two common NeRF
benchmark datasets consisting of 29 real-world scenes with high-quality images.
Our results compellingly demonstrate that our privacy-preserving method
significantly impairs NeRF's performance across these benchmark datasets.
Additionally, we show that our method is adaptable and versatile, functioning
across various perturbation strengths and NeRF architectures. This work offers
valuable insights into NeRF's vulnerabilities and emphasizes the need to
account for such potential privacy risks when developing robust 3D scene
reconstruction algorithms. Our study contributes to the larger conversation
surrounding responsible AI and generative machine learning, aiming to protect
user privacy and respect creative ownership in the digital age.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03128" title="Abstract">arXiv:2310.03128</a> [<a href="/pdf/2310.03128" title="Download PDF">pdf</a>, <a href="/format/2310.03128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiawen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Chenrui Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qihui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have garnered significant attention due to their
impressive natural language processing (NLP) capabilities. Recently, many
studies have focused on the tool utilization ability of LLMs. They primarily
investigated how LLMs effectively collaborate with given specific tools.
However, in scenarios where LLMs serve as intelligent agents, as seen in
applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate
decision-making processes that involve deciding whether to employ a tool and
selecting the most suitable tool(s) from a collection of available tools to
fulfill user requests. Therefore, in this paper, we introduce MetaTool, a
benchmark designed to evaluate whether LLMs have tool usage awareness and can
correctly choose tools. Specifically, we create a dataset called ToolE within
the benchmark. This dataset contains various types of user queries in the form
of prompts that trigger LLMs to use tools, including both single-tool and
multi-tool scenarios. Subsequently, we set the tasks for both tool usage
awareness and tool selection. We define four subtasks from different
perspectives in tool selection, including tool selection with similar choices,
tool selection in specific scenarios, tool selection with possible reliability
issues, and multi-tool selection. We conduct experiments involving nine popular
LLMs and find that the majority of them still struggle to effectively select
tools, highlighting the existing gaps between LLMs and genuine intelligent
agents. However, through the error analysis, we found there is still
significant room for improvement. Finally, we conclude with insights for tool
developers that follow ChatGPT to provide detailed descriptions that can
enhance the tool selection performance of LLMs.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03131" title="Abstract">arXiv:2310.03131</a> [<a href="/pdf/2310.03131" title="Download PDF">pdf</a>, <a href="/ps/2310.03131" title="Download PostScript">ps</a>, <a href="/format/2310.03131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Axiomatic Aggregations of Abductive Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biradar%2C+G">Gagan Biradar</a>, 
<a href="/search/cs?searchtype=author&query=Izza%2C+Y">Yacine Izza</a>, 
<a href="/search/cs?searchtype=author&query=Lobo%2C+E">Elita Lobo</a>, 
<a href="/search/cs?searchtype=author&query=Viswanathan%2C+V">Vignesh Viswanathan</a>, 
<a href="/search/cs?searchtype=author&query=Zick%2C+Y">Yair Zick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The recent criticisms of the robustness of post hoc model approximation
explanation methods (like LIME and SHAP) have led to the rise of model-precise
abductive explanations. For each data point, abductive explanations provide a
minimal subset of features that are sufficient to generate the outcome. While
theoretically sound and rigorous, abductive explanations suffer from a major
issue -- there can be several valid abductive explanations for the same data
point. In such cases, providing a single abductive explanation can be
insufficient; on the other hand, providing all valid abductive explanations can
be incomprehensible due to their size. In this work, we solve this issue by
aggregating the many possible abductive explanations into feature importance
scores. We propose three aggregation methods: two based on power indices from
cooperative game theory and a third based on a well-known measure of causal
strength. We characterize these three methods axiomatically, showing that each
of them uniquely satisfies a set of desirable properties. We also evaluate them
on multiple datasets and show that these explanations are robust to the attacks
that fool SHAP and LIME.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03132" title="Abstract">arXiv:2310.03132</a> [<a href="/pdf/2310.03132" title="Download PDF">pdf</a>, <a href="/ps/2310.03132" title="Download PostScript">ps</a>, <a href="/format/2310.03132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application-Oriented Co-Design of Motors and Motions for a 6DOF Robot  Manipulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stein%2C+A">Adrian Stein</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yebin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sakamoto%2C+Y">Yusuke Sakamoto</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bingnan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Huazhen Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This work investigates an application-driven co-design problem where the
motion and motors of a six degrees of freedom robotic manipulator are optimized
simultaneously, and the application is characterized by a set of tasks. Unlike
the state-of-the-art which selects motors from a product catalogue and performs
co-design for a single task, this work designs the motor geometry as well as
motion for a specific application. Contributions are made towards solving the
proposed co-design problem in a computationally-efficient manner. First, a
two-step process is proposed, where multiple motor designs are identified by
optimizing motions and motors for multiple tasks one by one, and then are
reconciled to determine the final motor design. Second, magnetic equivalent
circuit modeling is exploited to establish the analytic mapping from motor
design parameters to dynamic models and objective functions to facilitate the
subsequent differentiable simulation. Third, a direct-collocation-based
differentiable simulator of motor and robotic arm dynamics is developed to
balance the computational complexity and numerical stability. Simulation
verifies that higher performance for a specific application can be achieved
with the multi-task method, compared to several benchmark co-design methods.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03137" title="Abstract">arXiv:2310.03137</a> [<a href="/pdf/2310.03137" title="Download PDF">pdf</a>, <a href="/format/2310.03137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speech-Based Human-Exoskeleton Interaction for Lower Limb Motion  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+E">Eddie Guo</a>, 
<a href="/search/cs?searchtype=author&query=Perlette%2C+C">Christopher Perlette</a>, 
<a href="/search/cs?searchtype=author&query=Sharifi%2C+M">Mojtaba Sharifi</a>, 
<a href="/search/cs?searchtype=author&query=Grasse%2C+L">Lukas Grasse</a>, 
<a href="/search/cs?searchtype=author&query=Tata%2C+M">Matthew Tata</a>, 
<a href="/search/cs?searchtype=author&query=Mushahwar%2C+V+K">Vivian K. Mushahwar</a>, 
<a href="/search/cs?searchtype=author&query=Tavakoli%2C+M">Mahdi Tavakoli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This study presents a speech-based motion planning strategy (SBMP) developed
for lower limb exoskeletons to facilitate safe and compliant human-robot
interaction. A speech processing system, finite state machine, and central
pattern generator are the building blocks of the proposed strategy for online
planning of the exoskeleton's trajectory. According to experimental
evaluations, this speech-processing system achieved low levels of word and
intent errors. Regarding locomotion, the completion time for users with voice
commands was 54% faster than that using a mobile app interface. With the
proposed SBMP, users are able to maintain their postural stability with both
hands-free. This supports its use as an effective motion planning method for
the assistance and rehabilitation of individuals with lower-limb impairments.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03140" title="Abstract">arXiv:2310.03140</a> [<a href="/pdf/2310.03140" title="Download PDF">pdf</a>, <a href="/format/2310.03140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViFiT: Reconstructing Vision Trajectories from IMU and Wi-Fi Fine Time  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+B+B">Bryan Bo Cao</a>, 
<a href="/search/cs?searchtype=author&query=Alali%2C+A">Abrar Alali</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hansi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Meegan%2C+N">Nicholas Meegan</a>, 
<a href="/search/cs?searchtype=author&query=Gruteser%2C+M">Marco Gruteser</a>, 
<a href="/search/cs?searchtype=author&query=Dana%2C+K">Kristin Dana</a>, 
<a href="/search/cs?searchtype=author&query=Ashok%2C+A">Ashwin Ashok</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shubham Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 12 figures, 9 tables. MobiCom 2023 ISACom
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Tracking subjects in videos is one of the most widely used functions in
camera-based IoT applications such as security surveillance, smart city traffic
safety enhancement, vehicle to pedestrian communication and so on. In the
computer vision domain, tracking is usually achieved by first detecting
subjects with bounding boxes, then associating detected bounding boxes across
video frames. For many IoT systems, images captured by cameras are usually sent
over the network to be processed at a different site that has more powerful
computing resources than edge devices. However, sending entire frames through
the network causes significant bandwidth consumption that may exceed the system
bandwidth constraints. To tackle this problem, we propose ViFiT, a
transformer-based model that reconstructs vision bounding box trajectories from
phone data (IMU and Fine Time Measurements). It leverages a transformer ability
of better modeling long-term time series data. ViFiT is evaluated on Vi-Fi
Dataset, a large-scale multimodal dataset in 5 diverse real world scenes,
including indoor and outdoor environments. To fill the gap of proper metrics of
jointly capturing the system characteristics of both tracking quality and video
bandwidth reduction, we propose a novel evaluation framework dubbed Minimum
Required Frames (MRF) and Minimum Required Frames Ratio (MRFR). ViFiT achieves
an MRFR of 0.65 that outperforms the state-of-the-art approach for cross-modal
reconstruction in LSTM Encoder-Decoder architecture X-Translator of 0.98,
resulting in a high frame reduction rate as 97.76%.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03142" title="Abstract">arXiv:2310.03142</a> [<a href="/pdf/2310.03142" title="Download PDF">pdf</a>, <a href="/ps/2310.03142" title="Download PostScript">ps</a>, <a href="/format/2310.03142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Optimization of Heterogeneous Coded Distributed Computing  with Nonuniform File Popularity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+M">Min Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper studies MapReduce-based heterogeneous coded distributed computing
(CDC) where, besides different computing capabilities at workers, input files
to be accessed by computing jobs have nonuniform popularity. We propose a file
placement strategy that can handle an arbitrary number of input files.
Furthermore, we design a nested coded shuffling strategy that can efficiently
manage the nonuniformity of file popularity to maximize the coded multicasting
opportunity. We then formulate the joint optimization of the proposed file
placement and nested shuffling design variables to optimize the proposed CDC
scheme. To reduce the high computational complexity in solving the resulting
mixed-integer linear programming (MILP) problem, we propose a simple
two-file-group-based file placement approach to obtain an approximate solution.
Numerical results show that the optimized CDC scheme outperforms other
alternatives. Also, the proposed two-file-group-based approach achieves nearly
the same performance as the conventional branch-and-cut method in solving the
MILP problem but with substantially lower computational complexity that is
scalable over the number of files and workers. For computing jobs with
aggregate target functions that commonly appear in machine learning
applications, we propose a heterogeneous compressed CDC (C-CDC) scheme to
further improve the shuffling efficiency. The C-CDC scheme uses a local data
aggregation technique to compress the data to be shuffled for the shuffling
load reduction. We again optimize the proposed C-CDC scheme and explore the
two-file-group-based low-complexity approach for an approximate solution.
Numerical results show the proposed C-CDC scheme provides a considerable
shuffling load reduction over the CDC scheme, and also, the
two-file-group-based file placement approach maintains good performance.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03146" title="Abstract">arXiv:2310.03146</a> [<a href="/pdf/2310.03146" title="Download PDF">pdf</a>, <a href="/ps/2310.03146" title="Download PostScript">ps</a>, <a href="/format/2310.03146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness-enhancing mixed effects deep learning improves fairness on in-  and out-of-distribution clustered (non-iid) data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Adam Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+S">Son Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Montillo%2C+A">Albert Montillo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Traditional deep learning (DL) suffers from two core problems. Firstly, it
assumes training samples are independent and identically distributed. However,
numerous real-world datasets group samples by shared measurements (e.g., study
participants or cells), violating this assumption. In these scenarios, DL can
show compromised performance, limited generalization, and interpretability
issues, coupled with cluster confounding causing Type 1 and 2 errors. Secondly,
models are typically trained for overall accuracy, often neglecting
underrepresented groups and introducing biases in crucial areas like loan
approvals or determining health insurance rates, such biases can significantly
impact one's quality of life. To address both of these challenges
simultaneously, we present a mixed effects deep learning (MEDL) framework. MEDL
separately quantifies cluster-invariant fixed effects (FE) and cluster-specific
random effects (RE) through the introduction of: 1) a cluster adversary which
encourages the learning of cluster-invariant FE, 2) a Bayesian neural network
which quantifies the RE, and a mixing function combining the FE an RE into a
mixed-effect prediction. We marry this MEDL with adversarial debiasing, which
promotes equality-of-odds fairness across FE, RE, and ME predictions for
fairness-sensitive variables. We evaluated our approach using three datasets:
two from census/finance focusing on income classification and one from
healthcare predicting hospitalization duration, a regression task. Our
framework notably enhances fairness across all sensitive variables-increasing
fairness up to 82% for age, 43% for race, 86% for sex, and 27% for
marital-status. Besides promoting fairness, our method maintains the robust
performance and clarity of MEDL. It's versatile, suitable for various dataset
types and tasks, making it broadly applicable. Our GitHub repository houses the
implementation.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03147" title="Abstract">arXiv:2310.03147</a> [<a href="/pdf/2310.03147" title="Download PDF">pdf</a>, <a href="/ps/2310.03147" title="Download PostScript">ps</a>, <a href="/format/2310.03147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-Based Tweet Engagement Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeromela%2C+J">Jovan Jeromela</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted as a Diploma Thesis at TU Wien on 2023-05-25. Advisor: Peter Knees. 198 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Twitter is currently one of the biggest social media platforms. Its users may
share, read, and engage with short posts called tweets. For the ACM Recommender
Systems Conference 2020, Twitter published a dataset around 70 GB in size for
the annual RecSys Challenge. In 2020, the RecSys Challenge invited
participating teams to create models that would predict engagement likelihoods
for given user-tweet combinations. The submitted models predicting like, reply,
retweet, and quote engagements were evaluated based on two metrics: area under
the precision-recall curve (PRAUC) and relative cross-entropy (RCE).
<br />In this diploma thesis, we used the RecSys 2020 Challenge dataset and
evaluation procedure to investigate how well context alone may be used to
predict tweet engagement likelihood. In doing so, we employed the Spark engine
on TU Wien's Little Big Data Cluster to create scalable data preprocessing,
feature engineering, feature selection, and machine learning pipelines. We
manually created just under 200 additional features to describe tweet context.
<br />The results indicate that features describing users' prior engagement history
and the popularity of hashtags and links in the tweet were the most
informative. We also found that factors such as the prediction algorithm,
training dataset size, training dataset sampling method, and feature selection
significantly affect the results. After comparing the best results of our
context-only prediction models with content-only models and with models
developed by the Challenge winners, we identified that the context-based models
underperformed in terms of the RCE score. This work thus concludes by situating
this discrepancy and proposing potential improvements to our implementation,
which is shared in a public git repository.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03148" title="Abstract">arXiv:2310.03148</a> [<a href="/pdf/2310.03148" title="Download PDF">pdf</a>, <a href="/format/2310.03148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Task Learning For Reduced Popularity Bias In Multi-Territory Video  Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gampa%2C+P">Phanideep Gampa</a>, 
<a href="/search/cs?searchtype=author&query=Javadi%2C+F">Farnoosh Javadi</a>, 
<a href="/search/cs?searchtype=author&query=Bayar%2C+B">Belhassen Bayar</a>, 
<a href="/search/cs?searchtype=author&query=Yessenalina%2C+A">Ainur Yessenalina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Recsys CARS 2023 Workshop paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Various data imbalances that naturally arise in a multi-territory
personalized recommender system can lead to a significant item bias for
globally prevalent items. A locally popular item can be overshadowed by a
globally prevalent item. Moreover, users' viewership patterns/statistics can
drastically change from one geographic location to another which may suggest to
learn specific user embeddings. In this paper, we propose a multi-task learning
(MTL) technique, along with an adaptive upsampling method to reduce popularity
bias in multi-territory recommendations. Our proposed framework is designed to
enrich training examples with active users representation through upsampling,
and capable of learning geographic-based user embeddings by leveraging MTL.
Through experiments, we demonstrate the effectiveness of our framework in
multiple territories compared to a baseline not incorporating our proposed
techniques.~Noticeably, we show improved relative gain of up to $65.27\%$ in
PR-AUC metric. A case study is presented to demonstrate the advantages of our
methods in attenuating the popularity bias of global items.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03149" title="Abstract">arXiv:2310.03149</a> [<a href="/pdf/2310.03149" title="Download PDF">pdf</a>, <a href="/format/2310.03149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attributing Learned Concepts in Neural Networks to Training Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konz%2C+N">Nicholas Konz</a>, 
<a href="/search/cs?searchtype=author&query=Godfrey%2C+C">Charles Godfrey</a>, 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+M">Madelyn Shapiro</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+J">Jonathan Tu</a>, 
<a href="/search/cs?searchtype=author&query=Kvinge%2C+H">Henry Kvinge</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+D">Davis Brown</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model's original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03150" title="Abstract">arXiv:2310.03150</a> [<a href="/pdf/2310.03150" title="Download PDF">pdf</a>, <a href="/format/2310.03150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the  Ugly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woisetschl%C3%A4ger%2C+H">Herbert Woisetschl&#xe4;ger</a>, 
<a href="/search/cs?searchtype=author&query=Isenko%2C+A">Alexander Isenko</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mayer%2C+R">Ruben Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Jacobsen%2C+H">Hans-Arno Jacobsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF)

</div>
<p class="mathjax">Large Language Models (LLM) and foundation models are popular as they offer
new opportunities for individuals and businesses to improve natural language
processing, interact with data, and retrieve information faster. However,
training or fine-tuning LLMs requires a vast amount of data, which can be
challenging to access due to legal or technical restrictions and may require
private computing resources. Federated Learning (FL) is a solution designed to
overcome these challenges and expand data access for deep learning
applications.
<br />This paper takes a hardware-centric approach to explore how LLMs can be
brought to modern edge computing systems. Our study fine-tunes the FLAN-T5
model family, ranging from 80M to 3B parameters, using FL for a text
summarization task. We provide a micro-level hardware benchmark, compare the
model FLOP utilization to a state-of-the-art data center GPU, and study the
network utilization in realistic conditions. Our contribution is twofold:
First, we evaluate the current capabilities of edge computing systems and their
potential for LLM FL workloads. Second, by comparing these systems with a
data-center GPU, we demonstrate the potential for improvement and the next
steps toward achieving greater computational efficiency at the edge.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03152" title="Abstract">arXiv:2310.03152</a> [<a href="/pdf/2310.03152" title="Download PDF">pdf</a>, <a href="/format/2310.03152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards out-of-distribution generalizable predictions of chemical  kinetics properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yang Duan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weijiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">James Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+H">Hanghang Tong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. 11 pages, 1 figure, and 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine Learning (ML) techniques have found applications in estimating
chemical kinetics properties. With the accumulated drug molecules identified
through "AI4drug discovery", the next imperative lies in AI-driven design for
high-throughput chemical synthesis processes, with the estimation of properties
of unseen reactions with unexplored molecules. To this end, the existing ML
approaches for kinetics property prediction are required to be
Out-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD
kinetic property prediction into three levels (structure, condition, and
mechanism), revealing unique aspects of such problems. Under this framework, we
create comprehensive datasets to benchmark (1) the state-of-the-art ML
approaches for reaction prediction in the OOD setting and (2) the
state-of-the-art graph OOD methods in kinetics property prediction problems.
Our results demonstrated the challenges and opportunities in OOD kinetics
property prediction. Our datasets and benchmarks can further support research
in this direction.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03156" title="Abstract">arXiv:2310.03156</a> [<a href="/pdf/2310.03156" title="Download PDF">pdf</a>, <a href="/format/2310.03156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedHyper: A Universal and Robust Learning Rate Scheduler for Federated  Learning with Hypergradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The theoretical landscape of federated learning (FL) undergoes rapid
evolution, but its practical application encounters a series of intricate
challenges, and hyperparameter optimization is one of these critical
challenges. Amongst the diverse adjustments in hyperparameters, the adaptation
of the learning rate emerges as a crucial component, holding the promise of
significantly enhancing the efficacy of FL systems. In response to this
critical need, this paper presents FedHyper, a novel hypergradient-based
learning rate adaptation algorithm specifically designed for FL. FedHyper
serves as a universal learning rate scheduler that can adapt both global and
local rates as the training progresses. In addition, FedHyper not only
showcases unparalleled robustness to a spectrum of initial learning rate
configurations but also significantly alleviates the necessity for laborious
empirical learning rate adjustments. We provide a comprehensive theoretical
analysis of FedHyper's convergence rate and conduct extensive experiments on
vision and language benchmark datasets. The results demonstrate that FEDHYPER
consistently converges 1.1-3x faster than FedAvg and the competing baselines
while achieving superior final accuracy. Moreover, FedHyper catalyzes a
remarkable surge in accuracy, augmenting it by up to 15% compared to FedAvg
under suboptimal initial learning rate settings.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03158" title="Abstract">arXiv:2310.03158</a> [<a href="/pdf/2310.03158" title="Download PDF">pdf</a>, <a href="/format/2310.03158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessment of Prediction Intervals Using Uncertainty Characteristics  Curves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Navratil%2C+J">Jiri Navratil</a>, 
<a href="/search/cs?searchtype=author&query=Elder%2C+B">Benjamin Elder</a>, 
<a href="/search/cs?searchtype=author&query=Arnold%2C+M">Matthew Arnold</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Soumya Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Sattigeri%2C+P">Prasanna Sattigeri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at Workshop on Distribution-Free Uncertainty Quantification, International Conference on Machine Learning (ICML), July 2022. arXiv admin note: substantial text overlap with <a href="/abs/2106.00858">arXiv:2106.00858</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Accurate quantification of model uncertainty has long been recognized as a
fundamental requirement for trusted AI. In regression tasks, uncertainty is
typically quantified using prediction intervals calibrated to an ad-hoc
operating point, making evaluation and comparison across different studies
relatively difficult. Our work leverages: (1) the concept of operating
characteristics curves and (2) the notion of a gain over a null reference, to
derive a novel operating point agnostic assessment methodology for prediction
intervals. The paper defines the Uncertainty Characteristics Curve and
demonstrates its utility in selected scenarios. We argue that the proposed
method addresses the current need for comprehensive assessment of prediction
intervals and thus represents a valuable addition to the uncertainty
quantification toolbox.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03159" title="Abstract">arXiv:2310.03159</a> [<a href="/pdf/2310.03159" title="Download PDF">pdf</a>, <a href="/ps/2310.03159" title="Download PostScript">ps</a>, <a href="/format/2310.03159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Auction Algorithms for the Assignment Problem and Extensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertsekas%2C+D">Dimitri Bertsekas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We consider the classical linear assignment problem, and we introduce new
auction algorithms for its optimal and suboptimal solution. The algorithms are
founded on duality theory, and are related to ideas of competitive bidding by
persons for objects and the attendant market equilibrium, which underlie
real-life auction processes. We distinguish between two fundamentally different
types of bidding mechanisms: aggressive and cooperative. Mathematically,
aggressive bidding relies on a notion of approximate coordinate descent in dual
space, an epsilon-complementary slackness condition to regulate the amount of
descent approximation, and the idea of epsilon-scaling to resolve efficiently
the price wars that occur naturally as multiple bidders compete for a smaller
number of valuable objects. Cooperative bidding avoids price wars through
detection and cooperative resolution of any competitive impasse that involves a
group of persons.
<br />We discuss the relations between the aggressive and the cooperative bidding
approaches, we derive new algorithms and variations that combine ideas from
both of them, and we also make connections with other primal-dual methods,
including the Hungarian method. Furthermore, our discussion points the way to
algorithmic extensions that apply more broadly to network optimization,
including shortest path, max-flow, transportation, and minimum cost flow
problems with both linear and convex cost functions.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03161" title="Abstract">arXiv:2310.03161</a> [<a href="/pdf/2310.03161" title="Download PDF">pdf</a>, <a href="/format/2310.03161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural architecture impact on identifying temporally extended  Reinforcement Learning tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=George%2C+V+V">Victor Vadakechirayath George</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Master's thesis at Albert-Ludwigs-University, Freiburg Faculty of Engineering, Department of Computer Science Chair for Machine Learning. Advisor: Raghu Rajan, Examiners: Prof. Dr. Frank Hutter, Prof. Dr. Thomas Brox
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Inspired by recent developments in attention models for image classification
and natural language processing, we present various Attention based
architectures in reinforcement learning (RL) domain, capable of performing well
on OpenAI Gym Atari-2600 game suite. In spite of the recent success of Deep
Reinforcement learning techniques in various fields like robotics, gaming and
healthcare, they suffer from a major drawback that neural networks are
difficult to interpret. We try to get around this problem with the help of
Attention based models. In Attention based models, extracting and overlaying of
attention map onto images allows for direct observation of information used by
agent to select actions and easier interpretation of logic behind the chosen
actions. Our models in addition to playing well on gym-Atari environments, also
provide insights on how agent perceives its environment. In addition, motivated
by recent developments in attention based video-classification models using
Vision Transformer, we come up with an architecture based on Vision
Transformer, for image-based RL domain too. Compared to previous works in
Vision Transformer, our model is faster to train and requires fewer
computational resources. 3
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03162" title="Abstract">arXiv:2310.03162</a> [<a href="/pdf/2310.03162" title="Download PDF">pdf</a>, <a href="/format/2310.03162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metaverse CAN: Embracing Continuous, Active, and Non-intrusive Biometric  Authentication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Hui Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chenpei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+M">Miao Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The Metaverse is a virtual world, an immersive experience, a new
human-computer interaction, built upon various advanced technologies. How to
protect Metaverse personal information and virtual properties is also facing
new challenges, such as new attacks and new expectations of user experiences.
While traditional methods (e.g., those employed in smartphone authentication)
generally pass the basic design considerations, they are repeatedly reported to
be either unsafe or inconvenient in the Metaverse. In this paper, we address
this discrepancy by introducing CAN: a new design consideration especially for
the Metaverse. Specifically, we focus on the legacy and novel biometric
authentication systems and evaluate them thoroughly with basic and CAN
considerations. We also propose an ear-based method as one example of CAN
systems. To conclude, a continuous, active and non-intrusive biometric system
is suggested for Metaverse authentication for its capability in continuous
sessions, against imposters, and immersive experience.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03163" title="Abstract">arXiv:2310.03163</a> [<a href="/pdf/2310.03163" title="Download PDF">pdf</a>, <a href="/format/2310.03163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedNAR: Federated Optimization with Normalized Annealing Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junbo Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+C">Chong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+Q">Qirong Ho</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E+P">Eric P. Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongyi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Thirty-seventh Conference on Neural Information Processing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Weight decay is a standard technique to improve generalization performance in
modern deep neural network optimization, and is also widely adopted in
federated learning (FL) to prevent overfitting in local clients. In this paper,
we first explore the choices of weight decay and identify that weight decay
value appreciably influences the convergence of existing FL algorithms. While
preventing overfitting is crucial, weight decay can introduce a different
optimization goal towards the global objective, which is further amplified in
FL due to multiple local updates and heterogeneous data distribution. To
address this challenge, we develop {\it Federated optimization with Normalized
Annealing Regularization} (FedNAR), a simple yet effective and versatile
algorithmic plug-in that can be seamlessly integrated into any existing FL
algorithms. Essentially, we regulate the magnitude of each update by performing
co-clipping of the gradient and weight decay. We provide a comprehensive
theoretical analysis of FedNAR's convergence rate and conduct extensive
experiments on both vision and language datasets with different backbone
federated optimization algorithms. Our experimental results consistently
demonstrate that incorporating FedNAR into existing FL algorithms leads to
accelerated convergence and heightened model accuracy. Moreover, FedNAR
exhibits resilience in the face of various hyperparameter configurations.
Specifically, FedNAR has the ability to self-adjust the weight decay when the
initial specification is not optimal, while the accuracy of traditional FL
algorithms would markedly decline. Our codes are released at
\href{https://github.com/ljb121002/fednar}{https://github.com/ljb121002/fednar}.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03165" title="Abstract">arXiv:2310.03165</a> [<a href="/pdf/2310.03165" title="Download PDF">pdf</a>, <a href="/format/2310.03165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Accuracy in Deep Learning Using Random Matrix Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berlyand%2C+L">Leonid Berlyand</a>, 
<a href="/search/cs?searchtype=author&query=Sandier%2C+E">Etienne Sandier</a>, 
<a href="/search/cs?searchtype=author&query=Shmalo%2C+Y">Yitzchak Shmalo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">In this study, we explore the applications of random matrix theory (RMT) in
the training of deep neural networks (DNNs), focusing on layer pruning to
simplify DNN architecture and loss landscape. RMT, recently used to address
overfitting in deep learning, enables the examination of DNN's weight layer
spectra. We use these techniques to optimally determine the number of singular
values to be removed from the weight layers of a DNN during training via
singular value decomposition (SVD). This process aids in DNN simplification and
accuracy enhancement, as evidenced by training simple DNN models on the MNIST
and Fashion MNIST datasets.
<br />Our method can be applied to any fully connected or convolutional layer of a
pretrained DNN, decreasing the layer's parameters and simplifying the DNN
architecture while preserving or even enhancing the model's accuracy. By
discarding small singular values based on RMT criteria, the accuracy of the
test set remains consistent, facilitating more efficient DNN training without
compromising performance.
<br />We provide both theoretical and empirical evidence supporting our claim that
the elimination of small singular values based on RMT does not negatively
impact the DNN's accuracy. Our results offer valuable insights into the
practical application of RMT for the creation of more efficient and accurate
deep-learning models.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03166" title="Abstract">arXiv:2310.03166</a> [<a href="/pdf/2310.03166" title="Download PDF">pdf</a>, <a href="/format/2310.03166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Raze to the Ground: Query-Efficient Adversarial HTML Attacks on  Machine-Learning Phishing Webpage Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Montaruli%2C+B">Biagio Montaruli</a>, 
<a href="/search/cs?searchtype=author&query=Demetrio%2C+L">Luca Demetrio</a>, 
<a href="/search/cs?searchtype=author&query=Pintor%2C+M">Maura Pintor</a>, 
<a href="/search/cs?searchtype=author&query=Compagna%2C+L">Luca Compagna</a>, 
<a href="/search/cs?searchtype=author&query=Balzarotti%2C+D">Davide Balzarotti</a>, 
<a href="/search/cs?searchtype=author&query=Biggio%2C+B">Battista Biggio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security (AISec '23), November 30, 2023, Copenhagen, Denmark
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine-learning phishing webpage detectors (ML-PWD) have been shown to
suffer from adversarial manipulations of the HTML code of the input webpage.
Nevertheless, the attacks recently proposed have demonstrated limited
effectiveness due to their lack of optimizing the usage of the adopted
manipulations, and they focus solely on specific elements of the HTML code. In
this work, we overcome these limitations by first designing a novel set of
fine-grained manipulations which allow to modify the HTML code of the input
phishing webpage without compromising its maliciousness and visual appearance,
i.e., the manipulations are functionality- and rendering-preserving by design.
We then select which manipulations should be applied to bypass the target
detector by a query-efficient black-box optimization algorithm. Our experiments
show that our attacks are able to raze to the ground the performance of current
state-of-the-art ML-PWD using just 30 queries, thus overcoming the weaker
attacks developed in previous work, and enabling a much fairer robustness
evaluation of ML-PWD.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03171" title="Abstract">arXiv:2310.03171</a> [<a href="/pdf/2310.03171" title="Download PDF">pdf</a>, <a href="/format/2310.03171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Knowledge Production in Polarized Political Memes: The Case of  Critical Race Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Walters%2C+A">Alyvia Walters</a>, 
<a href="/search/cs?searchtype=author&query=Ammari%2C+T">Tawfiq Ammari</a>, 
<a href="/search/cs?searchtype=author&query=Garimella%2C+K">Kiran Garimella</a>, 
<a href="/search/cs?searchtype=author&query=Jhaver%2C+S">Shagun Jhaver</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Visual culture has long been deployed by actors across the political spectrum
as tools of political mobilization, and have recently incorporated new
communication tools, such as memes, GIFs, and emojis. In this study, we analyze
the top-circulated Facebook memes relating to critical race theory (CRT) from
May 2021 - May 2022 to investigate their visual and textual appeals. Using
image clustering techniques and critical discourse analysis, we find that both
pro- and anti-CRT memes deploy similar rhetorical tactics to make bifurcating
arguments, most of which do not pertain to the academic formulations of CRT.
Instead, these memes manipulate definitions of racism and antiracism to appeal
to their respective audiences. We argue that labeling such discursive practices
as simply a symptom of "post-truth" politics is a potentially unproductive
stance. Instead, theorizing the knowledge-building practices of these memes
through a lens of political epistemology allows us to understand how they
produce meaning.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03172" title="Abstract">arXiv:2310.03172</a> [<a href="/pdf/2310.03172" title="Download PDF">pdf</a>, <a href="/format/2310.03172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization and Evaluation of Multi Robot Surface Inspection Through  Particle Swarm Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiu%2C+D">Darren Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Nagpal%2C+R">Radhika Nagpal</a>, 
<a href="/search/cs?searchtype=author&query=Haghighat%2C+B">Bahar Haghighat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robot swarms can be tasked with a variety of automated sensing and inspection
applications in aerial, aquatic, and surface environments. In this paper, we
study a simplified two-outcome surface inspection task. We task a group of
robots to inspect and collectively classify a 2D surface section based on a
binary pattern projected on the surface. We use a decentralized Bayesian
decision-making algorithm and deploy a swarm of miniature 3-cm sized wheeled
robots to inspect randomized black and white tiles of $1m\times 1m$. We first
describe the model parameters that characterize our simulated environment, the
robot swarm, and the inspection algorithm. We then employ a noise-resistant
heuristic optimization scheme based on the Particle Swarm Optimization (PSO)
using a fitness evaluation that combines decision accuracy and decision time.
We use our fitness measure definition to asses the optimized parameters through
100 randomized simulations that vary surface pattern and initial robot poses.
The optimized algorithm parameters show up to a 55% improvement in median of
fitness evaluations against an empirically chosen parameter set.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03173" title="Abstract">arXiv:2310.03173</a> [<a href="/pdf/2310.03173" title="Download PDF">pdf</a>, <a href="/format/2310.03173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program  Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zishun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yunzhe Tao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Program synthesis aims to create accurate, executable code from natural
language descriptions. This field has leveraged the power of reinforcement
learning (RL) in conjunction with large language models (LLMs), significantly
enhancing code generation capabilities. This integration focuses on directly
optimizing functional correctness, transcending conventional supervised losses.
While current literature predominantly favors policy-based algorithms,
attributes of program synthesis suggest a natural compatibility with
value-based methods. This stems from rich collection of off-policy programs
developed by human programmers, and the straightforward verification of
generated programs through automated unit testing (i.e. easily obtainable
rewards in RL language). Diverging from the predominant use of policy-based
algorithms, our work explores the applicability of value-based approaches,
leading to the development of our $\mathcal{B}$-Coder (pronounced Bellman
coder). Yet, training value-based methods presents challenges due to the
enormous search space inherent to program synthesis. To this end, we propose an
initialization protocol for RL agents utilizing pre-trained LMs and a
conservative Bellman operator to reduce training complexities. Moreover, we
demonstrate how to leverage the learned value functions as a dual strategy to
post-process generated programs. Our empirical evaluations demonstrated
$\mathcal{B}$-Coder's capability in achieving state-of-the-art performance
compared with policy-based methods. Remarkably, this achievement is reached
with minimal reward engineering effort, highlighting the effectiveness of
value-based RL, independent of reward designs.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03174" title="Abstract">arXiv:2310.03174</a> [<a href="/pdf/2310.03174" title="Download PDF">pdf</a>, <a href="/format/2310.03174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test Case Recommendations with Distributed Representation of Code  Syntactic Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+M">Mosab Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Alhoori%2C+H">Hamed Alhoori</a>, 
<a href="/search/cs?searchtype=author&query=Rahimi%2C+M">Mona Rahimi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 14th Workshop on Automating Test Case Design, Selection and Evaluation (A-TEST 2023) co-located with 38th IEEE/ACM International Conference on ASE 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Frequent modifications of unit test cases are inevitable due to software's
continuous underlying changes in source code, design, and requirements. Since
manually maintaining software test suites is tedious, timely, and costly,
automating the process of generation and maintenance of test units will
significantly impact the effectiveness and efficiency of software testing
processes.
<br />To this end, we propose an automated approach which exploits both structural
and semantic properties of source code methods and test cases to recommend the
most relevant and useful unit tests to the developers. The proposed approach
initially trains a neural network to transform method-level source code, as
well as unit tests, into distributed representations (embedded vectors) while
preserving the importance of the structure in the code. Retrieving the semantic
and structural properties of a given method, the approach computes cosine
similarity between the method's embedding and the previously-embedded training
instances. Further, according to the similarity scores between the embedding
vectors, the model identifies the closest methods of embedding and the
associated unit tests as the most similar recommendations.
<br />The results on the Methods2Test dataset showed that, while there is no
guarantee to have similar relevant test cases for the group of similar methods,
the proposed approach extracts the most similar existing test cases for a given
method in the dataset, and evaluations show that recommended test cases
decrease the developers' effort to generating expected test cases.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03175" title="Abstract">arXiv:2310.03175</a> [<a href="/pdf/2310.03175" title="Download PDF">pdf</a>, <a href="/format/2310.03175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impedance Leakage Vulnerability and its Utilization in  Reverse-engineering Embedded Software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Awal%2C+M+S">Md Sadik Awal</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+T">Md Tauhidur Rahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Retrieval (cs.IR); Signal Processing (eess.SP)

</div>
<p class="mathjax">Discovering new vulnerabilities and implementing security and privacy
measures are important to protect systems and data against physical attacks.
One such vulnerability is impedance, an inherent property of a device that can
be exploited to leak information through an unintended side channel, thereby
posing significant security and privacy risks. Unlike traditional
vulnerabilities, impedance is often overlooked or narrowly explored, as it is
typically treated as a fixed value at a specific frequency in research and
design endeavors. Moreover, impedance has never been explored as a source of
information leakage. This paper demonstrates that the impedance of an embedded
device is not constant and directly relates to the programs executed on the
device. We define this phenomenon as impedance leakage and use this as a side
channel to extract software instructions from protected memory. Our experiment
on the ATmega328P microcontroller and the Artix 7 FPGA indicates that the
impedance side channel can detect software instructions with 96.1% and 92.6%
accuracy, respectively. Furthermore, we explore the dual nature of the
impedance side channel, highlighting the potential for beneficial purposes and
the associated risk of intellectual property theft. Finally, potential
countermeasures that specifically address impedance leakage are discussed.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03178" title="Abstract">arXiv:2310.03178</a> [<a href="/pdf/2310.03178" title="Download PDF">pdf</a>, <a href="/format/2310.03178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Ethics in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Liangqi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Distributed, Parallel, and Cluster Computing (cs.DC); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">The Internet of Things (IoT) consistently generates vast amounts of data,
sparking increasing concern over the protection of data privacy and the
limitation of data misuse. Federated learning (FL) facilitates collaborative
capabilities among multiple parties by sharing machine learning (ML) model
parameters instead of raw user data, and it has recently gained significant
attention for its potential in privacy preservation and learning efficiency
enhancement. In this paper, we highlight the digital ethics concerns that arise
when human-centric devices serve as clients in FL. More specifically,
challenges of game dynamics, fairness, incentive, and continuity arise in FL
due to differences in perspectives and objectives between clients and the
server. We analyze these challenges and their solutions from the perspectives
of both the client and the server, and through the viewpoints of centralized
and decentralized FL. Finally, we explore the opportunities in FL for
human-centric IoT as directions for future development.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03179" title="Abstract">arXiv:2310.03179</a> [<a href="/pdf/2310.03179" title="Download PDF">pdf</a>, <a href="/format/2310.03179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Domain Walking with Reduced-Order Models of Locomotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+M">Min Dai</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaemin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ames%2C+A+D">Aaron D. Ames</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ACC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Drawing inspiration from human multi-domain walking, this work presents a
novel reduced-order model based framework for realizing multi-domain robotic
walking. At the core of our approach is the viewpoint that human walking can be
represented by a hybrid dynamical system, with continuous phases that are
fully-actuated, under-actuated, and over-actuated and discrete changes in
actuation type occurring with changes in contact. Leveraging this perspective,
we synthesize a multi-domain linear inverted pendulum (MLIP) model of
locomotion. Utilizing the step-to-step dynamics of the MLIP model, we
successfully demonstrate multi-domain walking behaviors on the bipedal robot
Cassie -- a high degree of freedom 3D bipedal robot. Thus, we show the ability
to bridge the gap between multi-domain reduced order models and full-order
multi-contact locomotion. Additionally, our results showcase the ability of the
proposed method to achieve versatile speed-tracking performance and robust push
recovery behaviors.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03182" title="Abstract">arXiv:2310.03182</a> [<a href="/pdf/2310.03182" title="Download PDF">pdf</a>, <a href="/format/2310.03182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust and Interpretable Medical Image Classifiers via Concept  Bottleneck Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+A">An Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yiwu Zhong</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zexue He</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+P">Petros Karypis</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chengyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Gentili%2C+A">Amilcare Gentili</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chun-Nan Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Medical image classification is a critical problem for healthcare, with the
potential to alleviate the workload of doctors and facilitate diagnoses of
patients. However, two challenges arise when deploying deep learning models to
real-world healthcare applications. First, neural models tend to learn spurious
correlations instead of desired features, which could fall short when
generalizing to new domains (e.g., patients with different ages). Second, these
black-box models lack interpretability. When making diagnostic predictions, it
is important to understand why a model makes a decision for trustworthy and
safety considerations. In this paper, to address these two limitations, we
propose a new paradigm to build robust and interpretable medical image
classifiers with natural language concepts. Specifically, we first query
clinical concepts from GPT-4, then transform latent image features into
explicit concepts with a vision-language model. We systematically evaluate our
method on eight medical image classification datasets to verify its
effectiveness. On challenging datasets with strong confounding factors, our
method can mitigate spurious correlations thus substantially outperform
standard visual encoders and other baselines. Finally, we show how
classification with a small number of concepts brings a level of
interpretability for understanding model decisions through case studies in real
medical data.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03184" title="Abstract">arXiv:2310.03184</a> [<a href="/pdf/2310.03184" title="Download PDF">pdf</a>, <a href="/format/2310.03184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-augmented Generation to Improve Math Question-Answering:  Trade-offs Between Groundedness and Human Preference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levonian%2C+Z">Zachary Levonian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenglu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wangda Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gade%2C+A">Anoushka Gade</a>, 
<a href="/search/cs?searchtype=author&query=Henkel%2C+O">Owen Henkel</a>, 
<a href="/search/cs?searchtype=author&query=Postle%2C+M">Millie-Ellen Postle</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+W">Wanli Xing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">For middle-school math students, interactive question-answering (QA) with
tutors is an effective way to learn. The flexibility and emergent capabilities
of generative large language models (LLMs) has led to a surge of interest in
automating portions of the tutoring process - including interactive QA to
support conceptual discussion of mathematical concepts. However, LLM responses
to math questions can be incorrect or mismatched to the educational context -
such as being misaligned with a school's curriculum. One potential solution is
retrieval-augmented generation (RAG), which involves incorporating a vetted
external knowledge source in the LLM prompt to increase response quality. In
this paper, we designed prompts that retrieve and use content from a
high-quality open-source math textbook to generate responses to real student
questions. We evaluate the efficacy of this RAG system for middle-school
algebra and geometry QA by administering a multi-condition survey, finding that
humans prefer responses generated using RAG, but not when responses are too
grounded in the textbook content. We argue that while RAG is able to improve
response quality, designers of math QA systems must consider trade-offs between
generating responses preferred by students and responses closely matched to
specific educational resources.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03185" title="Abstract">arXiv:2310.03185</a> [<a href="/pdf/2310.03185" title="Download PDF">pdf</a>, <a href="/format/2310.03185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Misusing Tools in Large Language Models With Visual Adversarial Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xiaohan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R+K">Rajesh K. Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Mireshghallah%2C+N">Niloofar Mireshghallah</a>, 
<a href="/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T">Taylor Berg-Kirkpatrick</a>, 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+E">Earlence Fernandes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) are being enhanced with the ability to use tools
and to process multiple modalities. These new capabilities bring new benefits
and also new security risks. In this work, we show that an attacker can use
visual adversarial examples to cause attacker-desired tool usage. For example,
the attacker could cause a victim LLM to delete calendar events, leak private
conversations and book hotels. Different from prior work, our attacks can
affect the confidentiality and integrity of user resources connected to the LLM
while being stealthy and generalizable to multiple input prompts. We construct
these attacks using gradient-based adversarial training and characterize
performance along multiple dimensions. We find that our adversarial images can
manipulate the LLM to invoke tools following real-world syntax almost always
(~98%) while maintaining high similarity to clean images (~0.9 SSIM).
Furthermore, using human scoring and automated metrics, we find that the
attacks do not noticeably affect the conversation (and its semantics) between
the user and the LLM.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03187" title="Abstract">arXiv:2310.03187</a> [<a href="/pdf/2310.03187" title="Download PDF">pdf</a>, <a href="/format/2310.03187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesis of Data-Driven Nonlinear State Observers using  Lipschitz-Bounded Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tang%2C+W">Wentao Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to American Control Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper focuses on the model-free synthesis of state observers for
nonlinear autonomous systems without knowing the governing equations.
Specifically, the Kazantzis-Kravaris/Luenberger (KKL) observer structure is
leveraged, where the outputs are fed into a linear time-invariant (LTI) system
to obtain the observer states, which can be viewed as the states nonlinearly
transformed by an immersion mapping, and a neural network is used to
approximate the inverse of the nonlinear immersion and estimate the states. In
view of the possible existence of noises in output measurements, this work
proposes to impose an upper bound on the Lipschitz constant of the neural
network for robust and safe observation. A relation that bounds the
generalization loss of state observation according to the Lipschitz constant,
as well as the $H_2$-norm of the LTI part in the KKL observer, is established,
thus reducing the model-free observer synthesis problem to that of
Lipschitz-bounded neural network training, for which a direct parameterization
technique is used. The proposed approach is demonstrated on a chaotic Lorenz
system.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03188" title="Abstract">arXiv:2310.03188</a> [<a href="/pdf/2310.03188" title="Download PDF">pdf</a>, <a href="/format/2310.03188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Talking Models: Distill Pre-trained Knowledge to Downstream Models via  Interactive Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingyun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+H">Huan Gui</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lichan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E+H">Ed H. Chi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Many recent breakthroughs in machine learning have been enabled by the
pre-trained foundation models. By scaling up model parameters, training data,
and computation resources, foundation models have significantly advanced the
state-of-the-art in many applications. However, it is still an open question of
how to use these models to perform downstream tasks efficiently. Knowledge
distillation (KD) has been explored to tackle this challenge. KD transfers
knowledge from a large teacher model to a smaller student model. While KD has
been successful in improving student model performance, recent research has
discovered that a powerful teacher does not necessarily lead to a powerful
student, due to their huge capacity gap. In addition, the potential
distribution shifts between the pre-training data and downstream tasks can make
knowledge transfer in KD sub-optimal for improving downstream task performance.
In this paper, we extend KD with an interactive communication process to help
students of downstream tasks learn effectively from pre-trained foundation
models. Our design is inspired by the way humans learn from teachers who can
explain knowledge in a way that meets the students' needs. Specifically, we let
each model (i.e., student and teacher) train two components: (1) an encoder
encoding the model's hidden states to a message and (2) a decoder decoding any
messages to its own hidden states. With encoder and decoder, not only can the
teacher transfer rich information by encoding its hidden states, but also the
student can send messages with information of downstream tasks to the teacher.
Therefore, knowledge passing from teacher to student can be tailored to the
student's capacity and downstream tasks' distributions. We conducted
experiments on benchmark datasets to show that our communication mechanism
outperforms state-of-the-art distillation techniques.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03191" title="Abstract">arXiv:2310.03191</a> [<a href="/pdf/2310.03191" title="Download PDF">pdf</a>, <a href="/format/2310.03191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sim-to-Real Learning for Humanoid Box Loco-Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dao%2C+J">Jeremy Dao</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Helei Duan</a>, 
<a href="/search/cs?searchtype=author&query=Fern%2C+A">Alan Fern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this work we propose a learning-based approach to box loco-manipulation
for a humanoid robot. This is a particularly challenging problem due to the
need for whole-body coordination in order to lift boxes of varying weight,
position, and orientation while maintaining balance. To address this challenge,
we present a sim-to-real reinforcement learning approach for training general
box pickup and carrying skills for the bipedal robot Digit. Our reward
functions are designed to produce the desired interactions with the box while
also valuing balance and gait quality. We combine the learned skills into a
full system for box loco-manipulation to achieve the task of moving boxes from
one table to another with a variety of sizes, weights, and initial
configurations. In addition to quantitative simulation results, we demonstrate
successful sim-to-real transfer on the humanoid r
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03192" title="Abstract">arXiv:2310.03192</a> [<a href="/pdf/2310.03192" title="Download PDF">pdf</a>, <a href="/ps/2310.03192" title="Download PostScript">ps</a>, <a href="/format/2310.03192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI in the Classroom: Can Students Remain Active Learners?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdelghani%2C+R">Rania Abdelghani</a>, 
<a href="/search/cs?searchtype=author&query=Sauz%C3%A9on%2C+H">H&#xe9;l&#xe8;ne Sauz&#xe9;on</a>, 
<a href="/search/cs?searchtype=author&query=Oudeyer%2C+P">Pierre-Yves Oudeyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Generative Artificial Intelligence (GAI) has high potential to help address a
diversity of educational challenges. In principle, GAI could facilitate the
implementation of interactive and empowering pedagogical activities to
complement the standard teaching strategies and favor students active
engagement, understanding and control over their learning processes. These
dimensions are indeed fundamental for a better learning experience and
longer-lasting cognitive outcomes. However, several characteristics of the
interactions with GAI such as continuous confidence in the generated answers,
and the lack of pedagogical stance in their behavior may lead students to poor
states of control over learning (e.g. over-reliance on pre-generated content,
over-estimation of one's own knowledge, loss of curious and critical-thinking
sense, etc).
<br />The fine line between the two settings seems to lie in how this technology is
used to carry out the pedagogical activities (e.g. types of interactions
allowed, level of controllability by students, level of involvement of
educators, etc) as well as to what extent students have the relevant skills
(cognitive, metacognitive and GAI literacy) that allow them to correctly
evaluate, analyze and interpret the system behaviors.
<br />In this context, this article proposes to identify some of the opportunities
and challenges that could arise wrt students control over their learning when
using GAI during formal pedagogical activities. In a second step, we also
discuss the types of trainings that could be relevant to offer students in
order to provide them with the appropriate set of skills that can help them use
GAI in informed ways, when pursuing a given learning goal.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03193" title="Abstract">arXiv:2310.03193</a> [<a href="/pdf/2310.03193" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Rise of Open Science: Tracking the Evolution and Perceived Value of  Data and Methods Link-Sharing Practices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hancheng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Dodge%2C+J">Jesse Dodge</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+K">Kyle Lo</a>, 
<a href="/search/cs?searchtype=author&query=McFarland%2C+D+A">Daniel A. McFarland</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L+L">Lucy Lu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY); History and Philosophy of Physics (physics.hist-ph); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">In recent years, funding agencies and journals increasingly advocate for open
science practices (e.g. data and method sharing) to improve the transparency,
access, and reproducibility of science. However, quantifying these practices at
scale has proven difficult. In this work, we leverage a large-scale dataset of
1.1M papers from arXiv that are representative of the fields of physics, math,
and computer science to analyze the adoption of data and method link-sharing
practices over time and their impact on article reception. To identify links to
data and methods, we train a neural text classification model to automatically
classify URL types based on contextual mentions in papers. We find evidence
that the practice of link-sharing to methods and data is spreading as more
papers include such URLs over time. Reproducibility efforts may also be
spreading because the same links are being increasingly reused across papers
(especially in computer science); and these links are increasingly concentrated
within fewer web domains (e.g. Github) over time. Lastly, articles that share
data and method links receive increased recognition in terms of citation count,
with a stronger effect when the shared links are active (rather than defunct).
Together, these findings demonstrate the increased spread and perceived value
of data and method sharing practices in open science.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03195" title="Abstract">arXiv:2310.03195</a> [<a href="/pdf/2310.03195" title="Download PDF">pdf</a>, <a href="/format/2310.03195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep reinforcement learning for machine scheduling: Methodology, the  state-of-the-art, and future directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khadivi%2C+M">Maziyar Khadivi</a>, 
<a href="/search/cs?searchtype=author&query=Charter%2C+T">Todd Charter</a>, 
<a href="/search/cs?searchtype=author&query=Yaghoubi%2C+M">Marjan Yaghoubi</a>, 
<a href="/search/cs?searchtype=author&query=Jalayer%2C+M">Masoud Jalayer</a>, 
<a href="/search/cs?searchtype=author&query=Ahang%2C+M">Maryam Ahang</a>, 
<a href="/search/cs?searchtype=author&query=Shojaeinasab%2C+A">Ardeshir Shojaeinasab</a>, 
<a href="/search/cs?searchtype=author&query=Najjaran%2C+H">Homayoun Najjaran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Machine scheduling aims to optimize job assignments to machines while
adhering to manufacturing rules and job specifications. This optimization leads
to reduced operational costs, improved customer demand fulfillment, and
enhanced production efficiency. However, machine scheduling remains a
challenging combinatorial problem due to its NP-hard nature. Deep Reinforcement
Learning (DRL), a key component of artificial general intelligence, has shown
promise in various domains like gaming and robotics. Researchers have explored
applying DRL to machine scheduling problems since 1995. This paper offers a
comprehensive review and comparison of DRL-based approaches, highlighting their
methodology, applications, advantages, and limitations. It categorizes these
approaches based on computational components: conventional neural networks,
encoder-decoder architectures, graph neural networks, and metaheuristic
algorithms. Our review concludes that DRL-based methods outperform exact
solvers, heuristics, and tabular reinforcement learning algorithms in terms of
computation speed and generating near-global optimal solutions. These DRL-based
approaches have been successfully applied to static and dynamic scheduling
across diverse machine environments and job characteristics. However, DRL-based
schedulers face limitations in handling complex operational constraints,
configurable multi-objective optimization, generalization, scalability,
interpretability, and robustness. Addressing these challenges will be a crucial
focus for future research in this field. This paper serves as a valuable
resource for researchers to assess the current state of DRL-based machine
scheduling and identify research gaps. It also aids experts and practitioners
in selecting the appropriate DRL approach for production scheduling.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03200" title="Abstract">arXiv:2310.03200</a> [<a href="/pdf/2310.03200" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amazon Books Rating prediction &amp; Recommendation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hsiu-Ping Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+S">Suman Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+Y">Yougender Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+N">Nagender Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+J">Jongwook Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">This paper uses the dataset of Amazon to predict the books ratings listed on
Amazon website. As part of this project, we predicted the ratings of the books,
and also built a recommendation cluster. This recommendation cluster provides
the recommended books based on the column's values from dataset, for instance,
category, description, author, price, reviews etc. This paper provides a flow
of handling big data files, data engineering, building models and providing
predictions. The models predict book ratings column using various PySpark
Machine Learning APIs. Additionally, we used hyper-parameters and parameters
tuning. Also, Cross Validation and TrainValidationSplit were used for
generalization. Finally, we performed a comparison between Binary
Classification and Multiclass Classification in their accuracies. We converted
our label from multiclass to binary to see if we could find any difference
between the two classifications. As a result, we found out that we get higher
accuracy in binary classification than in multiclass classification.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03202" title="Abstract">arXiv:2310.03202</a> [<a href="/pdf/2310.03202" title="Download PDF">pdf</a>, <a href="/format/2310.03202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResolverFuzz: Automated Discovery of DNS Resolver Vulnerabilities with  Query-Response Fuzzing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xuesong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Haixin Duan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version. Accepted by USENIX Security 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI); Software Engineering (cs.SE)

</div>
<p class="mathjax">Domain Name System (DNS) is a critical component of the Internet. DNS
resolvers, which act as the cache between DNS clients and DNS nameservers, are
the central piece of the DNS infrastructure, essential to the scalability of
DNS. However, finding the resolver vulnerabilities is non-trivial, and this
problem is not well addressed by the existing tools. To list a few reasons,
first, most of the known resolver vulnerabilities are non-crash bugs that
cannot be directly detected by the existing oracles (or sanitizers). Second,
there lacks rigorous specifications to be used as references to classify a test
case as a resolver bug. Third, DNS resolvers are stateful, and stateful fuzzing
is still challenging due to the large input space.
<br />In this paper, we present a new fuzzing system termed ResolverFuzz to address
the aforementioned challenges related to DNS resolvers, with a suite of new
techniques being developed. First, ResolverFuzz performs constrained stateful
fuzzing by focusing on the short query-response sequence, which has been
demonstrated as the most effective way to find resolver bugs, based on our
study of the published DNS CVEs. Second, to generate test cases that are more
likely to trigger resolver bugs, we combine probabilistic context-free grammar
(PCFG) based input generation with byte-level mutation for both queries and
responses. Third, we leverage differential testing and clustering to identify
non-crash bugs like cache poisoning bugs. We evaluated ResolverFuzz against 6
mainstream DNS software under 4 resolver modes. Overall, we identify 23
vulnerabilities that can result in cache poisoning, resource consumption, and
crash attacks. After responsible disclosure, 19 of them have been confirmed or
fixed, and 15 CVE numbers have been assigned.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03205" title="Abstract">arXiv:2310.03205</a> [<a href="/pdf/2310.03205" title="Download PDF">pdf</a>, <a href="/format/2310.03205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Youwang%2C+K">Kim Youwang</a>, 
<a href="/search/cs?searchtype=author&query=Hyun%2C+L">Lee Hyun</a>, 
<a href="/search/cs?searchtype=author&query=Sung-Bin%2C+K">Kim Sung-Bin</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+S">Suekyeong Nam</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+J">Janghoon Ju</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+T">Tae-Hyun Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures, and 3 tables for the main paper. 8 pages, 6 figures and 3 tables for the appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose NeuFace, a 3D face mesh pseudo annotation method on videos via
neural re-parameterized optimization. Despite the huge progress in 3D face
reconstruction methods, generating reliable 3D face labels for in-the-wild
dynamic videos remains challenging. Using NeuFace optimization, we annotate the
per-view/-frame accurate and consistent face meshes on large-scale face videos,
called the NeuFace-dataset. We investigate how neural re-parameterization helps
to reconstruct image-aligned facial details on 3D meshes via gradient analysis.
By exploiting the naturalness and diversity of 3D faces in our dataset, we
demonstrate the usefulness of our dataset for 3D face-related tasks: improving
the reconstruction accuracy of an existing 3D face reconstruction model and
learning 3D facial motion prior. Code and datasets will be available at
https://neuface-dataset.github.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03210" title="Abstract">arXiv:2310.03210</a> [<a href="/pdf/2310.03210" title="Download PDF">pdf</a>, <a href="/ps/2310.03210" title="Download PostScript">ps</a>, <a href="/format/2310.03210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Language Models Employ the Socratic Method? Experiments with Code  Debugging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Hossami%2C+E">Erfan Al-Hossami</a>, 
<a href="/search/cs?searchtype=author&query=Bunescu%2C+R">Razvan Bunescu</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+J">Justin Smith</a>, 
<a href="/search/cs?searchtype=author&query=Teehan%2C+R">Ryan Teehan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 tables. To be published in Proceedings of the 2024 Technical Symposium on Computer Science Education (SIGCSE'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">When employing the Socratic method of teaching, instructors guide students
toward solving a problem on their own rather than providing the solution
directly. While this strategy can substantially improve learning outcomes, it
is usually time-consuming and cognitively demanding. Automated Socratic
conversational agents can augment human instruction and provide the necessary
scale, however their development is hampered by the lack of suitable data for
training and evaluation. In this paper, we introduce a manually created dataset
of multi-turn Socratic advice that is aimed at helping a novice programmer fix
buggy solutions to simple computational problems. The dataset is then used for
benchmarking the Socratic debugging abilities of a number of language models,
ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5
to zero-shot and chain of thought prompting of the much larger GPT-4. The code
and datasets are made freely available for research at the link below.
https://github.com/taisazero/socratic-debugging-benchmark
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03211" title="Abstract">arXiv:2310.03211</a> [<a href="/pdf/2310.03211" title="Download PDF">pdf</a>, <a href="/format/2310.03211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Performance of Multimodal Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+U">Utsav Garg</a>, 
<a href="/search/cs?searchtype=author&query=Bas%2C+E">Erhan Bas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03212" title="Abstract">arXiv:2310.03212</a> [<a href="/pdf/2310.03212" title="Download PDF">pdf</a>, <a href="/format/2310.03212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PDR-CapsNet: an Energy-Efficient Parallel Approach to Dynamic Routing in  Capsule Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javadinia%2C+S">Samaneh Javadinia</a>, 
<a href="/search/cs?searchtype=author&query=Baniasadi%2C+A">Amirali Baniasadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Convolutional Neural Networks (CNNs) have produced state-of-the-art results
for image classification tasks. However, they are limited in their ability to
handle rotational and viewpoint variations due to information loss in
max-pooling layers. Capsule Networks (CapsNets) employ a
computationally-expensive iterative process referred to as dynamic routing to
address these issues. CapsNets, however, often fall short on complex datasets
and require more computational resources than CNNs. To overcome these
challenges, we introduce the Parallel Dynamic Routing CapsNet (PDR-CapsNet), a
deeper and more energy-efficient alternative to CapsNet that offers superior
performance, less energy consumption, and lower overfitting rates. By
leveraging a parallelization strategy, PDR-CapsNet mitigates the computational
complexity of CapsNet and increases throughput, efficiently using hardware
resources. As a result, we achieve 83.55\% accuracy while requiring 87.26\%
fewer parameters, 32.27\% and 47.40\% fewer MACs, and Flops, achieving 3x
faster inference and 7.29J less energy consumption on a 2080Ti GPU with 11GB
VRAM compared to CapsNet and for the CIFAR-10 dataset.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03214" title="Abstract">arXiv:2310.03214</a> [<a href="/pdf/2310.03214" title="Download PDF">pdf</a>, <a href="/format/2310.03214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FreshLLMs: Refreshing Large Language Models with Search Engine  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Tu Vu</a>, 
<a href="/search/cs?searchtype=author&query=Iyyer%2C+M">Mohit Iyyer</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuezhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Constant%2C+N">Noah Constant</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jerry Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jason Wei</a>, 
<a href="/search/cs?searchtype=author&query=Tar%2C+C">Chris Tar</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y">Yun-Hsuan Sung</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+Q">Quoc Le</a>, 
<a href="/search/cs?searchtype=author&query=Luong%2C+T">Thang Luong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, 22 pages, 7 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Most large language models (LLMs) are trained once and never updated; thus,
they lack the ability to dynamically adapt to our ever-changing world. In this
work, we perform a detailed study of the factuality of LLM-generated text in
the context of answering questions that test current world knowledge.
Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a
diverse range of question and answer types, including questions that require
fast-changing world knowledge as well as questions with false premises that
need to be debunked. We benchmark a diverse array of both closed and
open-source LLMs under a two-mode evaluation procedure that allows us to
measure both correctness and hallucination. Through human evaluations involving
more than 50K judgments, we shed light on limitations of these models and
demonstrate significant room for improvement: for instance, all models
(regardless of model size) struggle on questions that involve fast-changing
knowledge and false premises. Motivated by these results, we present
FreshPrompt, a simple few-shot prompting method that substantially boosts the
performance of an LLM on FreshQA by incorporating relevant and up-to-date
information retrieved from a search engine into the prompt. Our experiments
show that FreshPrompt outperforms both competing search engine-augmented
prompting methods such as Self-Ask (Press et al., 2022) as well as commercial
systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that
both the number of retrieved evidences and their order play a key role in
influencing the correctness of LLM-generated answers. Additionally, instructing
the LLM to generate concise and direct answers helps reduce hallucination
compared to encouraging more verbose answers. To facilitate future work, we
release FreshQA at github.com/freshllms/freshqa and commit to updating it at
regular intervals.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03217" title="Abstract">arXiv:2310.03217</a> [<a href="/pdf/2310.03217" title="Download PDF">pdf</a>, <a href="/format/2310.03217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal and Practical Elements for the Certification of Machine Learning  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Durand%2C+J">Jean-Guillaume Durand</a>, 
<a href="/search/cs?searchtype=author&query=Dubois%2C+A">Arthur Dubois</a>, 
<a href="/search/cs?searchtype=author&query=Moss%2C+R+J">Robert J. Moss</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Best of Conference at the 2023 Digital Avionics Systems Conference (DASC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Over the past decade, machine learning has demonstrated impressive results,
often surpassing human capabilities in sensing tasks relevant to autonomous
flight. Unlike traditional aerospace software, the parameters of machine
learning models are not hand-coded nor derived from physics but learned from
data. They are automatically adjusted during a training phase, and their values
do not usually correspond to physical requirements. As a result, requirements
cannot be directly traced to lines of code, hindering the current bottom-up
aerospace certification paradigm. This paper attempts to address this gap by 1)
demystifying the inner workings and processes to build machine learning models,
2) formally establishing theoretical guarantees given by those processes, and
3) complementing these formal elements with practical considerations to develop
a complete certification argument for safety-critical machine learning systems.
Based on a scalable statistical verifier, our proposed framework is
model-agnostic and tool-independent, making it adaptable to many use cases in
the industry. We demonstrate results on a widespread application in autonomous
flight: vision-based landing.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03218" title="Abstract">arXiv:2310.03218</a> [<a href="/pdf/2310.03218" title="Download PDF">pdf</a>, <a href="/format/2310.03218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Energy-Based Prior Model with Diffusion-Amortized MCMC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Peiyu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yaxuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Sirui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruiqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Latent space Energy-Based Models (EBMs), also known as energy-based priors,
have drawn growing interests in the field of generative modeling due to its
flexibility in the formulation and strong modeling power of the latent space.
However, the common practice of learning latent space EBMs with non-convergent
short-run MCMC for prior and posterior sampling is hindering the model from
further progress; the degenerate MCMC sampling quality in practice often leads
to degraded generation quality and instability in training, especially with
highly multi-modal and/or high-dimensional target distributions. To remedy this
sampling issue, in this paper we introduce a simple but effective
diffusion-based amortization method for long-run MCMC sampling and develop a
novel learning algorithm for the latent space EBM based on it. We provide
theoretical evidence that the learned amortization of MCMC is a valid long-run
MCMC sampler. Experiments on several image modeling benchmark datasets
demonstrate the superior performance of our method compared with strong
counterparts
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03221" title="Abstract">arXiv:2310.03221</a> [<a href="/pdf/2310.03221" title="Download PDF">pdf</a>, <a href="/format/2310.03221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical  Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yijia Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Steinecke%2C+D">Dylan Steinecke</a>, 
<a href="/search/cs?searchtype=author&query=Pelletier%2C+A+R">Alexander Russell Pelletier</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yushi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Ping%2C+P">Peipei Ping</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 2 figures, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Knowledge graphs (KGs) have emerged as a powerful framework for representing
and integrating complex biomedical information. However, assembling KGs from
diverse sources remains a significant challenge in several aspects, including
entity alignment, scalability, and the need for continuous updates to keep pace
with scientific advancements. Moreover, the representative power of KGs is
often limited by the scarcity of multi-modal data integration. To overcome
these challenges, we propose Know2BIO, a general-purpose heterogeneous KG
benchmark for the biomedical domain. Know2BIO integrates data from 30 diverse
sources, capturing intricate relationships across 11 biomedical categories. It
currently consists of ~219,000 nodes and ~6,200,000 edges. Know2BIO is capable
of user-directed automated updating to reflect the latest knowledge in
biomedical science. Furthermore, Know2BIO is accompanied by multi-modal data:
node features including text descriptions, protein and compound sequences and
structures, enabling the utilization of emerging natural language processing
methods and multi-modal data integration strategies. We evaluate KG
representation models on Know2BIO, demonstrating its effectiveness as a
benchmark for KG representation learning in the biomedical field. Data and
source code of Know2BIO are available at
https://github.com/Yijia-Xiao/Know2BIO/.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03222" title="Abstract">arXiv:2310.03222</a> [<a href="/pdf/2310.03222" title="Download PDF">pdf</a>, <a href="/ps/2310.03222" title="Download PostScript">ps</a>, <a href="/format/2310.03222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The bright side of simple heuristics for the TSP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frieze%2C+A">Alan Frieze</a>, 
<a href="/search/cs?searchtype=author&query=Pegden%2C+W">Wesley Pegden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computational Geometry (cs.CG); Combinatorics (math.CO)

</div>
<p class="mathjax">The greedy and nearest-neighbor TSP heuristics can both have $\log n$
approximation factors from optimal in worst case, even just for $n$ points in
Euclidean space. In this note, we show that this approximation factor is only
realized when the optimal tour is unusually short. In particular, for points
from any fixed $d$-Ahlfor's regular metric space (which includes any
$d$-manifold like the $d$-cube $[0,1]^d$ in the case $d$ is an integer but also
fractals of dimension $d$ when $d$ is real-valued), our results imply that the
greedy and nearest-neighbor heuristics have \emph{additive} errors from optimal
on the order of the \emph{optimal} tour length through \emph{random} points in
the same space, for $d&gt;1$.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03223" title="Abstract">arXiv:2310.03223</a> [<a href="/pdf/2310.03223" title="Download PDF">pdf</a>, <a href="/format/2310.03223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tony Shen</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+M">Mohit Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Ester%2C+M">Martin Ester</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We seek to automate the generation of drug-like compounds conditioned to
specific protein pocket targets. Most current methods approximate the
protein-molecule distribution of a finite dataset and, therefore struggle to
generate molecules with significant binding improvement over the training
dataset. We instead frame the pocket-conditioned molecular generation task as
an RL problem and develop TacoGFN, a target conditional Generative Flow Network
model. Our method is explicitly encouraged to generate molecules with desired
properties as opposed to fitting on a pre-existing data distribution. To this
end, we develop transformer-based docking score prediction to speed up docking
score computation and propose TacoGFN to explore molecule space efficiently.
Furthermore, we incorporate several rounds of active learning where generated
samples are queried using a docking oracle to improve the docking score
prediction. This approach allows us to accurately explore as much of the
molecule landscape as we can afford computationally. Empirically, molecules
generated using TacoGFN and its variants significantly outperform all baseline
methods across every property (Docking score, QED, SA, Lipinski), while being
orders of magnitude faster.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03224" title="Abstract">arXiv:2310.03224</a> [<a href="/pdf/2310.03224" title="Download PDF">pdf</a>, <a href="/ps/2310.03224" title="Download PostScript">ps</a>, <a href="/format/2310.03224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matrix Completion from One-Bit Dither Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eamaz%2C+A">Arian Eamaz</a>, 
<a href="/search/cs?searchtype=author&query=Yeganegi%2C+F">Farhang Yeganegi</a>, 
<a href="/search/cs?searchtype=author&query=Soltanalian%2C+M">Mojtaba Soltanalian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We explore the impact of coarse quantization on matrix completion in the
extreme scenario of dithered one-bit sensing, where the matrix entries are
compared with time-varying threshold levels. In particular, instead of
observing a subset of high-resolution entries of a low-rank matrix, we have
access to a small number of one-bit samples, generated as a result of these
comparisons. In order to recover the low-rank matrix using its coarsely
quantized known entries, we begin by transforming the problem of one-bit matrix
completion (one-bit MC) with time-varying thresholds into a nuclear norm
minimization problem. The one-bit sampled information is represented as linear
inequality feasibility constraints. We then develop the popular singular value
thresholding (SVT) algorithm to accommodate these inequality constraints,
resulting in the creation of the One-Bit SVT (OB-SVT). Our findings demonstrate
that incorporating multiple time-varying sampling threshold sequences in
one-bit MC can significantly improve the performance of the matrix completion
algorithm. In pursuit of achieving this objective, we utilize diverse
thresholding schemes, namely uniform, Gaussian, and discrete thresholds. To
accelerate the convergence of our proposed algorithm, we introduce three
variants of the OB-SVT algorithm. Among these variants is the randomized
sketched OB-SVT, which departs from using the entire information at each
iteration, opting instead to utilize sketched data. This approach effectively
reduces the dimension of the operational space and accelerates the convergence.
We perform numerical evaluations comparing our proposed algorithm with the
maximum likelihood estimation method previously employed for one-bit MC, and
demonstrate that our approach can achieve a better recovery performance.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03225" title="Abstract">arXiv:2310.03225</a> [<a href="/pdf/2310.03225" title="Download PDF">pdf</a>, <a href="/format/2310.03225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Exploration in Reinforcement Learning: A Generalized Formulation  and Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wachi%2C+A">Akifumi Wachi</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+W">Wataru Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+K">Kazumune Hashimoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Safe exploration is essential for the practical use of reinforcement learning
(RL) in many real-world scenarios. In this paper, we present a generalized safe
exploration (GSE) problem as a unified formulation of common safe exploration
problems. We then propose a solution of the GSE problem in the form of a
meta-algorithm for safe exploration, MASE, which combines an unconstrained RL
algorithm with an uncertainty quantifier to guarantee safety in the current
episode while properly penalizing unsafe explorations before actual safety
violation to discourage them in future episodes. The advantage of MASE is that
we can optimize a policy while guaranteeing with a high probability that no
safety constraint will be violated under proper assumptions. Specifically, we
present two variants of MASE with different constructions of the uncertainty
quantifier: one based on generalized linear models with theoretical guarantees
of safety and near-optimality, and another that combines a Gaussian process to
ensure safety with a deep RL algorithm to maximize the reward. Finally, we
demonstrate that our proposed algorithm achieves better performance than
state-of-the-art algorithms on grid-world and Safety Gym benchmarks without
violating any safety constraints, even during training.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03228" title="Abstract">arXiv:2310.03228</a> [<a href="/pdf/2310.03228" title="Download PDF">pdf</a>, <a href="/format/2310.03228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> History Matching for Geological Carbon Storage using Data-Space  Inversion with Spatio-Temporal Data Parameterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Su Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Durlofsky%2C+L+J">Louis J. Durlofsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">History matching based on monitoring data will enable uncertainty reduction,
and thus improved aquifer management, in industrial-scale carbon storage
operations. In traditional model-based data assimilation, geomodel parameters
are modified to force agreement between flow simulation results and
observations. In data-space inversion (DSI), history-matched quantities of
interest, e.g., posterior pressure and saturation fields conditioned to
observations, are inferred directly, without constructing posterior geomodels.
This is accomplished efficiently using a set of O(1000) prior simulation
results, data parameterization, and posterior sampling within a Bayesian
setting. In this study, we develop and implement (in DSI) a deep-learning-based
parameterization to represent spatio-temporal pressure and CO2 saturation
fields at a set of time steps. The new parameterization uses an adversarial
autoencoder (AAE) for dimension reduction and a convolutional long short-term
memory (convLSTM) network to represent the spatial distribution and temporal
evolution of the pressure and saturation fields. This parameterization is used
with an ensemble smoother with multiple data assimilation (ESMDA) in the DSI
framework to enable posterior predictions. A realistic 3D system characterized
by prior geological realizations drawn from a range of geological scenarios is
considered. A local grid refinement procedure is introduced to estimate the
error covariance term that appears in the history matching formulation.
Extensive history matching results are presented for various quantities, for
multiple synthetic true models. Substantial uncertainty reduction in posterior
pressure and saturation fields is achieved in all cases. The framework is
applied to efficiently provide posterior predictions for a range of error
covariance specifications. Such an assessment would be expensive using a
model-based approach.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03232" title="Abstract">arXiv:2310.03232</a> [<a href="/pdf/2310.03232" title="Download PDF">pdf</a>, <a href="/format/2310.03232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Representations of First-person Pronouns for Prediction of  Depression Symptom Severity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xinyang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Burkhardt%2C+H+A">Hannah A Burkhardt</a>, 
<a href="/search/cs?searchtype=author&query=Are%C3%A1n%2C+P+A">Patricia A Are&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Hull%2C+T+D">Thomas D Hull</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+T">Trevor Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted: AMIA Annual Symposium 2023. To appear as: Ren X, Burkhardt H, Are\'an P, Hull T, Cohen T. Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity. AMIA Annual Symposium Proceedings 2023. American Medical Informatics Association
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prior work has shown that analyzing the use of first-person singular pronouns
can provide insight into individuals' mental status, especially depression
symptom severity. These findings were generated by counting frequencies of
first-person singular pronouns in text data. However, counting doesn't capture
how these pronouns are used. Recent advances in neural language modeling have
leveraged methods generating contextual embeddings. In this study, we sought to
utilize the embeddings of first-person pronouns obtained from contextualized
language representation models to capture ways these pronouns are used, to
analyze mental status. De-identified text messages sent during online
psychotherapy with weekly assessment of depression severity were used for
evaluation. Results indicate the advantage of contextualized first-person
pronoun embeddings over standard classification token embeddings and
frequency-based pronoun analysis results in predicting depression symptom
severity. This suggests contextual representations of first-person pronouns can
enhance the predictive utility of language used by people with depression
symptoms.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03237" title="Abstract">arXiv:2310.03237</a> [<a href="/pdf/2310.03237" title="Download PDF">pdf</a>, <a href="/format/2310.03237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask for Alice: Online Covert Distress Signal in the Presence of a Strong  Adversary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imanda%2C+H">Hayyu Imanda</a>, 
<a href="/search/cs?searchtype=author&query=Rasmussen%2C+K">Kasper Rasmussen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In this paper we propose a protocol that can be used to covertly send a
distress signal through a seemingly normal webserver, even if the adversary is
monitoring both the network and the user's device. This allows a user to call
for help even when they are in the same physical space as their adversaries. We
model such a scenario by introducing a strong adversary model that captures a
high degree of access to the user's device and full control over the network.
<br />Our model fits into scenarios where a user is under surveillance and wishes
to inform a trusted party of the situation. To do this, our method uses
existing websites to act as intermediaries between the user and a trusted
backend; this enables the user to initiate the distress signal without arousing
suspicion, even while being actively monitored. We accomplish this by utilising
the TLS handshake to convey additional information; this means that any website
wishing to participate can do so with minimal effort and anyone monitoring the
traffic will just see common TLS connections. In order for websites to be
willing to host such a functionality the protocol must coexist gracefully with
users who use normal TLS and the computational overhead must be minimal. We
provide a full security analysis of the architecture and prove that the
adversary cannot distinguish between a set of communications which contains a
distress call and a normal communication.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03239" title="Abstract">arXiv:2310.03239</a> [<a href="/pdf/2310.03239" title="Download PDF">pdf</a>, <a href="/format/2310.03239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Roadmaps with Gaps over Controllers: Achieving Efficiency in Planning  under Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaramakrishnan%2C+A">Aravind Sivaramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Carver%2C+N+R">Noah R. Carver</a>, 
<a href="/search/cs?searchtype=author&query=Tangirala%2C+S">Sumanth Tangirala</a>, 
<a href="/search/cs?searchtype=author&query=Bekris%2C+K+E">Kostas E. Bekris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper aims to improve the computational efficiency of motion planning
for mobile robots with non-trivial dynamics by taking advantage of learned
controllers. It adopts a decoupled strategy, where a system-specific controller
is first trained offline in an empty environment to deal with the system's
dynamics. For an environment, the proposed approach constructs offline a data
structure, a "Roadmap with Gaps," to approximately learn how to solve planning
queries in this environment using the learned controller. Its nodes correspond
to local regions and edges correspond to applications of the learned control
policy that approximately connect these regions. Gaps arise due to the
controller not perfectly connecting pairs of individual states along edges.
Online, given a query, a tree sampling-based motion planner uses the roadmap so
that the tree's expansion is informed towards the goal region. The tree
expansion selects local subgoals given a wavefront on the roadmap that guides
towards the goal. When the controller cannot reach a subgoal region, the
planner resorts to random exploration to maintain probabilistic completeness
and asymptotic optimality. The experimental evaluation shows that the approach
significantly improves the computational efficiency of motion planning on
various benchmarks, including physics-based vehicular models on uneven and
varying friction terrains as well as a quadrotor under air pressure effects.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03240" title="Abstract">arXiv:2310.03240</a> [<a href="/pdf/2310.03240" title="Download PDF">pdf</a>, <a href="/format/2310.03240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relational Convolutional Networks: A framework for learning  representations of hierarchical relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altabaa%2C+A">Awni Altabaa</a>, 
<a href="/search/cs?searchtype=author&query=Lafferty%2C+J">John Lafferty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A maturing area of research in deep learning is the development of
architectures that can learn explicit representations of relational features.
In this paper, we focus on the problem of learning representations of
hierarchical relations, proposing an architectural framework we call
"relational convolutional networks". Given a sequence of objects, a
"multi-dimensional inner product relation" module produces a relation tensor
describing all pairwise relations. A "relational convolution" layer then
transforms the relation tensor into a sequence of new objects, each describing
the relations within some group of objects at the previous layer. Graphlet
filters, analogous to filters in convolutional neural networks, represent a
template of relations against which the relation tensor is compared at each
grouping. Repeating this yields representations of higher-order, hierarchical
relations. We present the motivation and details of the architecture, together
with a set of experiments to demonstrate how relational convolutional networks
can provide an effective framework for modeling relational tasks that have
hierarchical structure.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03246" title="Abstract">arXiv:2310.03246</a> [<a href="/pdf/2310.03246" title="Download PDF">pdf</a>, <a href="/format/2310.03246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ${\tt MORALS}$: Analysis of High-Dimensional Robot Controllers via  Topological Tools in a Latent Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vieira%2C+E+R">Ewerton R. Vieira</a>, 
<a href="/search/cs?searchtype=author&query=Sivaramakrishnan%2C+A">Aravind Sivaramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Tangirala%2C+S">Sumanth Tangirala</a>, 
<a href="/search/cs?searchtype=author&query=Granados%2C+E">Edgar Granados</a>, 
<a href="/search/cs?searchtype=author&query=Mischaikow%2C+K">Konstantin Mischaikow</a>, 
<a href="/search/cs?searchtype=author&query=Bekris%2C+K+E">Kostas E. Bekris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally to this paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Estimating the region of attraction (${\tt RoA}$) for a robotic system's
controller is essential for safe application and controller composition. Many
existing methods require access to a closed-form expression that limit
applicability to data-driven controllers. Methods that operate only over
trajectory rollouts tend to be data-hungry. In prior work, we have demonstrated
that topological tools based on Morse Graphs offer data-efficient ${\tt RoA}$
estimation without needing an analytical model. They struggle, however, with
high-dimensional systems as they operate over a discretization of the state
space. This paper presents ${\it Mo}$rse Graph-aided discovery of ${\it
R}$egions of ${\it A}$ttraction in a learned ${\it L}$atent ${\it S}$pace
(${\tt MORALS}$). The approach combines autoencoding neural networks with Morse
Graphs. ${\tt MORALS}$ shows promising predictive capabilities in estimating
attractors and their ${\tt RoA}$s for data-driven controllers operating over
high-dimensional systems, including a 67-dim humanoid robot and a 96-dim
3-fingered manipulator. It first projects the dynamics of the controlled system
into a learned latent space. Then, it constructs a reduced form of Morse Graphs
representing the bistability of the underlying dynamics, i.e., detecting when
the controller results in a desired versus an undesired behavior. The
evaluation on high-dimensional robotic datasets indicates the data efficiency
of the approach in ${\tt RoA}$ estimation.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03247" title="Abstract">arXiv:2310.03247</a> [<a href="/pdf/2310.03247" title="Download PDF">pdf</a>, <a href="/ps/2310.03247" title="Download PostScript">ps</a>, <a href="/format/2310.03247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust globally divergence-free Weak Galerkin finite element method for  incompressible Magnetohydrodynamics flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>, 
<a href="/search/math?searchtype=author&query=Xie%2C+X">Xiaoping Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">This paper develops a weak Galerkin (WG) finite element method of arbitrary
order for the steady incompressible Magnetohydrodynamics equations. The WG
scheme uses piecewise polynomials of degrees $k(k\geq 1),k,k-1$, and $k-1$
respectively for the approximations of the velocity, the magnetic field, the
pressure, and the magnetic pseudo-pressure in the interior of elements, and
uses piecewise polynomials of degree $k$ for their numerical traces on the
interfaces of elements. The method is shown to yield globally divergence-free
approximations of the velocity and magnetic fields. We give existence and
uniqueness results for the discrete scheme and derive optimal a priori error
estimates. We also present a convergent linearized iterative algorithm.
Numerical experiments are provided to verify the obtained theoretical results.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03248" title="Abstract">arXiv:2310.03248</a> [<a href="/pdf/2310.03248" title="Download PDF">pdf</a>, <a href="/format/2310.03248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Xcrum: A Synergistic Approach Integrating Extreme Programming with Scrum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+S">Siavash Hosseini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In today's modern world, software plays a pivotal role. Software development
is a highly complex and time-consuming process, demanding multidimensional
efforts. Companies continually adapt their requirements to align with the
evolving environment, with a specific emphasis on rapid delivery and the
acceptance of changing requirements. Traditional models, such as plan-driven
development, often fall short in meeting these demands. In the realm of
software development, Agile has been the focal point of global discourse for
both researchers and developers. Agile development is better suited to
customize and streamline the development process, offering a highly flexible,
early, and rapid delivery lifecycle conducive to efficient software
development. This article aims to provide an overview of two prominent Agile
methodologies: Scrum and Extreme Programming (XP). It achieves this by
reviewing relevant publications, analyzing their impact on software
development, exploring the distinctive features of each methodology, and
conducting a comparative assessment. Furthermore, the article offers personal
insights and recommendations. Notably, the integration of XP practices into
Scrum has given rise to a novel hybrid methodology known as "Xcrum," which
retains its agility. It should be highlighted that, given this new approach's
incorporation of the strengths of both methods, it holds the potential to
outperform the original frameworks.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03249" title="Abstract">arXiv:2310.03249</a> [<a href="/pdf/2310.03249" title="Download PDF">pdf</a>, <a href="/format/2310.03249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models be Good Path Planners? A Benchmark and  Investigation on Spatial-temporal Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aghzal%2C+M">Mohamed Aghzal</a>, 
<a href="/search/cs?searchtype=author&query=Plaku%2C+E">Erion Plaku</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Ziyu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have achieved remarkable success across a wide
spectrum of tasks; however, they still face limitations in scenarios that
demand long-term planning and spatial reasoning. To facilitate this line of
research, in this work, we propose a new benchmark, termed $\textbf{P}$ath
$\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage
($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by
formulating ''path planning'' tasks that require an LLM to navigate to target
locations while avoiding obstacles and adhering to constraints. Leveraging this
benchmark, we systematically investigate LLMs including GPT-4 via different
few-shot prompting methodologies and BART and T5 of various sizes via
fine-tuning. Our experimental results show the promise of few-shot GPT-4 in
spatial reasoning, when it is prompted to reason and act interleavedly,
although it still fails to make long-term temporal reasoning. In contrast,
while fine-tuned LLMs achieved impressive results on in-distribution reasoning
tasks, they struggled to generalize to larger environments or environments with
more obstacles.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03251" title="Abstract">arXiv:2310.03251</a> [<a href="/pdf/2310.03251" title="Download PDF">pdf</a>, <a href="/format/2310.03251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Video and Audio processing with Loihi 2
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+S+B">Sumit Bam Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Timcheck%2C+J">Jonathan Timcheck</a>, 
<a href="/search/cs?searchtype=author&query=Frady%2C+P">Paxon Frady</a>, 
<a href="/search/cs?searchtype=author&query=Campos-Macias%2C+L">Leobardo Campos-Macias</a>, 
<a href="/search/cs?searchtype=author&query=Davies%2C+M">Mike Davies</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Loihi 2 is an asynchronous, brain-inspired research processor that
generalizes several fundamental elements of neuromorphic architecture, such as
stateful neuron models communicating with event-driven spikes, in order to
address limitations of the first generation Loihi. Here we explore and
characterize some of these generalizations, such as sigma-delta encapsulation,
resonate-and-fire neurons, and integer-valued spikes, as applied to standard
video, audio, and signal processing tasks. We find that these new neuromorphic
approaches can provide orders of magnitude gains in combined efficiency and
latency (energy-delay-product) for feed-forward and convolutional neural
networks applied to video, audio denoising, and spectral transforms compared to
state-of-the-art solutions.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03252" title="Abstract">arXiv:2310.03252</a> [<a href="/pdf/2310.03252" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring age-related patterns in internet access: Insights from a  secondary analysis of New Zealand survey data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pacheco%2C+E">Edgar Pacheco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">For over two decades Internet access has been a topic of research and debate.
Up-to-date evidence about key predictors such as age is important considering
not only the complexities of access to the online medium but also the
ever-changing nature of the Internet. This paper attempts to provide a
stocktake of current trends in Internet access in New Zealand and their
association with age. It relies on secondary analysis of data from a larger
online panel survey of 1,001 adult users. Chi-square test of Independence and
Cramer's V were used for analysis. A key finding uncovers an emerging gap in
the quality of Internet access. While fibre is the predominant type of
broadband connection at home, older adults are significantly less likely to
have it, and more likely to adopt wireless broadband. Also, a large majority
across all age groups have a positive view of the Internet. This was higher
among older adults who, interestingly, were slightly more likely to say that
their concern about the security of their personal details online has increased
in the last year. The implications of the results are discussed and some
directions for future research are proposed.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03253" title="Abstract">arXiv:2310.03253</a> [<a href="/pdf/2310.03253" title="Download PDF">pdf</a>, <a href="/format/2310.03253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Molecule Design by Latent Prompt Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Deqian Kong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jianwen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper proposes a latent prompt Transformer model for solving challenging
optimization problems such as molecule design, where the goal is to find
molecules with optimal values of a target chemical or biological property that
can be computed by an existing software. Our proposed model consists of three
components. (1) A latent vector whose prior distribution is modeled by a Unet
transformation of a Gaussian white noise vector. (2) A molecule generation
model that generates the string-based representation of molecule conditional on
the latent vector in (1). We adopt the causal Transformer model that takes the
latent vector in (1) as prompt. (3) A property prediction model that predicts
the value of the target property of a molecule based on a non-linear regression
on the latent vector in (1). We call the proposed model the latent prompt
Transformer model. After initial training of the model on existing molecules
and their property values, we then gradually shift the model distribution
towards the region that supports desired values of the target property for the
purpose of molecule design. Our experiments show that our proposed model
achieves state of the art performances on several benchmark molecule design
tasks.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03254" title="Abstract">arXiv:2310.03254</a> [<a href="/pdf/2310.03254" title="Download PDF">pdf</a>, <a href="/format/2310.03254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-coresident family as a driver of migration change in a crisis: The  case of the COVID-19 pandemic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kan%2C+U">Unchitta Kan</a>, 
<a href="/search/cs?searchtype=author&query=McLeod%2C+J">Jericho McLeod</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez%2C+E">Eduardo L&#xf3;pez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Changes in U.S. migration trends during the COVID-19 pandemic show that many
moved to less populated cities from larger cities, deviating from previous
trends. In this study, building on prior work in the literature showing that
the abundance of family ties are inversely related to population size, we
analyze these migration changes with a focus on the crucial, yet overlooked
factor of extended family. Employing two large-scale data sets, census
microdata and mobile phone GPS relocation data, we show a collection of
empirical results that paint a picture of migration change affected by family.
Namely, we establish that people migrated closer to family at higher rates
after the COVID-19 pandemic started. Moreover, even controlling for factors
such as population density and costs of living, we find that changes in net
in-migration tended to be larger and positive in cities with larger proportions
of people who can be parents to adult children, our proxy for parental family
availability. Our study suggests an underexplored explanation for internal
migration patterns during a crisis and advances the demography-disaster nexus.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03256" title="Abstract">arXiv:2310.03256</a> [<a href="/pdf/2310.03256" title="Download PDF">pdf</a>, <a href="/format/2310.03256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward One-Second Latency: Evolution of Live Media Streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bentaleb%2C+A">Abdelhak Bentaleb</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+M">May Lim</a>, 
<a href="/search/cs?searchtype=author&query=Akcay%2C+M+N">Mehmet N. Akcay</a>, 
<a href="/search/cs?searchtype=author&query=Begen%2C+A+C">Ali C. Begen</a>, 
<a href="/search/cs?searchtype=author&query=Hammoudi%2C+S">Sarra Hammoudi</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+R">Roger Zimmermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">This survey presents the evolution of live media streaming and the
technological developments behind today's IP-based low-latency live streaming
systems. Live streaming primarily involves capturing, encoding, packaging and
delivering real-time events such as live sports, live news, personal broadcasts
and surveillance videos. Live streaming also involves concurrent streaming of
linear TV programming off the satellite, cable, over-the-air or IPTV broadcast,
where the programming is not necessarily a real-time event. The survey starts
with a discussion on the latency and latency continuum in streaming
applications. Then, it lays out the existing live streaming workflows and
protocols, followed by an in-depth analysis of the latency sources in these
workflows and protocols. The survey continues with the technology enablers,
low-latency extensions for the popular HTTP adaptive streaming methods and
enhancements for robust low-latency playback. An entire section is dedicated to
the detailed summary and findings of Twitch's grand challenge on low-latency
live streaming. The survey concludes with a discussion of ongoing research
problems in this space.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03258" title="Abstract">arXiv:2310.03258</a> [<a href="/pdf/2310.03258" title="Download PDF">pdf</a>, <a href="/format/2310.03258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Electricity Service Equity Issues with Transfer Counterfactual  Learning on Large-Scale Outage Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Song Wei</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+X">Xiangrui Kong</a>, 
<a href="/search/cs?searchtype=author&query=Huestis-Mitchell%2C+S+A">Sarah A Huestis-Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shixiang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xavier%2C+A+S">Alinson Santos Xavier</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+F">Feng Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">Energy justice is a growing area of interest in interdisciplinary energy
research. However, identifying systematic biases in the energy sector remains
challenging due to confounding variables, intricate heterogeneity in treatment
effects, and limited data availability. To address these challenges, we
introduce a novel approach for counterfactual causal analysis centered on
energy justice. We use subgroup analysis to manage diverse factors and leverage
the idea of transfer learning to mitigate data scarcity in each subgroup. In
our numerical analysis, we apply our method to a large-scale customer-level
power outage data set and investigate the counterfactual effect of demographic
factors, such as income and age of the population, on power outage durations.
Our results indicate that low-income and elderly-populated areas consistently
experience longer power outages, regardless of weather conditions. This points
to existing biases in the power system and highlights the need for focused
improvements in areas with economic challenges.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03260" title="Abstract">arXiv:2310.03260</a> [<a href="/pdf/2310.03260" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application Potential of a Hybrid Ground Source Heat Pump Array for the  UC Berkeley Campus Business and Law Node Energy System: A Preliminary Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+K">Kecheng Chen</a>, 
<a href="/search/math?searchtype=author&query=Soga%2C+K">Kenichi Soga</a>, 
<a href="/search/math?searchtype=author&query=Dobson%2C+P">Patrick Dobson</a>, 
<a href="/search/math?searchtype=author&query=Nico%2C+P">Peter Nico</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The current plan divides the UC Berkeley (UCB) campus energy system into five
nodes, where the Business and Law node was studied because of an open field
site for borehole installation. The Pacific Northwest National Laboratory's
Commercial Prototype Building Models were used to estimate heating and cooling
load requirements for UCB campus building types by considering model
characteristics (for example, high base load from hospitals, high DHW in
hotels) corresponding to the ASHRAE Standard 90.1-2013. Unscaled load profiles
were created from the EnergyPlus building energy simulation and scaled with
monitored peak load and annual energy use to generate the target node's hourly
heating and cooling load profiles. An optimization problem was solved to design
a hybrid GSHP system, where the objective function is the lifetime total cost
of the system, and the optimization variables are the portion of heating and
cooling loads covered by the GSHP system. Modelica models for air source and
ground source heat pump systems were built for detailed case studies based on
optimization results. In the Modelica model, the demand side is connected to
the radiators in the building to transfer heat, and the source side is
connected to GSHP, ASHP, or other heating and cooling facilities. The results
demonstrate that an appropriate hybrid GSHP system can help reduce both
borehole numbers and electricity consumption for the UCB campus site.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03262" title="Abstract">arXiv:2310.03262</a> [<a href="/pdf/2310.03262" title="Download PDF">pdf</a>, <a href="/format/2310.03262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlock Predictable Scaling from Emergent Abilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengding Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinrong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chaoqun He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+N">Ning Ding</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Z">Zebin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+G">Guoyang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages main paper, 8 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The scientific scale-up of large language models (LLMs) necessitates a
comprehensive understanding of their scaling properties. However, the existing
literature on the scaling properties only yields an incomplete answer:
optimization loss decreases predictably as the model size increases, in line
with established scaling law; yet no scaling law for task has been established
and the task performances are far from predictable during scaling. Task
performances typically show minor gains on small models until they improve
dramatically once models exceed a size threshold, exemplifying the ``emergent
abilities''. In this study, we discover that small models, although they
exhibit minor performance, demonstrate critical and consistent task performance
improvements that are not captured by conventional evaluation strategies due to
insufficient measurement resolution. To measure such improvements, we introduce
PassUntil, an evaluation strategy through massive sampling in the decoding
phase. We conduct quantitative investigations into the scaling law of task
performance. Firstly, a strict task scaling law is identified, enhancing the
predictability of task performances. Remarkably, we are able to predict the
performance of the 2.4B model on code generation with merely 0.05\% deviation
before training starts. Secondly, underpinned by PassUntil, we observe concrete
evidence of emergent abilities and ascertain that they are not in conflict with
the continuity of performance improvement. Their semblance to break-through is
that their scaling curve cannot be fitted by standard scaling law function. We
then introduce a mathematical definition for the emergent abilities. Through
the definition, we refute a prevalent ``multi-step reasoning hypothesis''
regarding the genesis of emergent abilities and propose a new hypothesis with a
satisfying fit to the observed scaling curve.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03265" title="Abstract">arXiv:2310.03265</a> [<a href="/pdf/2310.03265" title="Download PDF">pdf</a>, <a href="/format/2310.03265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrated Communication, Sensing, and Computation Framework for 6G  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhiyong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+A">J. Andrew Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xinxin He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Ping Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, submitted to IEEE VTM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In the sixth generation (6G) era, intelligent machine network (IMN)
applications, such as intelligent transportation, require collaborative
machines with communication, sensing, and computation (CSC) capabilities. This
article proposes an integrated communication, sensing, and computation (ICSAC)
framework for 6G to achieve the reciprocity among CSC functions to enhance the
reliability and latency of communication, accuracy and timeliness of sensing
information acquisition, and privacy and security of computing to realize the
IMN applications. Specifically, the sensing and communication functions can
merge into unified platforms using the same transmit signals, and the acquired
real-time sensing information can be exploited as prior information for
intelligent algorithms to enhance the performance of communication networks.
This is called the computing-empowered integrated sensing and communications
(ISAC) reciprocity. Such reciprocity can further improve the performance of
distributed computation with the assistance of networked sensing capability,
which is named the sensing-empowered integrated communications and computation
(ICAC) reciprocity. The above ISAC and ICAC reciprocities can enhance each
other iteratively and finally lead to the ICSAC reciprocity. To achieve these
reciprocities, we explore the potential enabling technologies for the ICSAC
framework. Finally, we present the evaluation results of crucial enabling
technologies to show the feasibility of the ICSAC framework.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03266" title="Abstract">arXiv:2310.03266</a> [<a href="/pdf/2310.03266" title="Download PDF">pdf</a>, <a href="/format/2310.03266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniPredict: Large Language Models are Universal Tabular Predictors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruiyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Tabular data prediction is a fundamental machine learning task for many
applications. Existing methods predominantly employ discriminative modeling and
operate under the assumption of a fixed target column, necessitating
re-training for every new predictive task. Inspired by the generative power of
large language models (LLMs), this paper exploits the idea of building
universal tabular data predictors based on generative modeling, namely
UniPredict. Here, we show that scaling up an LLM to extensive tabular datasets
with the capability of comprehending diverse tabular inputs and predicting for
target variables following the input instructions. Specifically, we train a
single LLM on an aggregation of 169 tabular datasets with diverse targets and
compare its performance against baselines that are trained on each dataset
separately. We observe this versatile UniPredict model demonstrates an
advantage over other models, ranging from 5.4% to 13.4%, when compared with the
best tree-boosting baseline and the best neural network baseline, respectively.
We further test UniPredict in few-shot learning settings on another 62 tabular
datasets. Our method achieves strong performance in quickly adapting to new
tasks, where our method outperforms XGBoost over 100% on the low-resource setup
and shows a significant margin over all baselines. We envision that UniPredict
sheds light on developing a universal tabular data prediction system that
learns from data at scale and serves a wide range of prediction tasks.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03268" title="Abstract">arXiv:2310.03268</a> [<a href="/pdf/2310.03268" title="Download PDF">pdf</a>, <a href="/format/2310.03268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Distribution of SINR for Cell-Free Massive MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chong%2C+B">Baolin Chong</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+F">Fengqian Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hancheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Langtian Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Cell-free (CF) massive multiple-input multiple-output (mMIMO) has been
considered as a potential technology for Beyond 5G communication systems.
However, the performance of CF mMIMO systems has not been well studied. Most
existing analytical work on CF mMIMO systems is based on the expected
signal-to-interference-plus-noise ratio (SINR). The statistical characteristics
of the SINR, which is critical for emerging applications that focus on extreme
events, have not been investigated. To address this issue, in this paper, we
attempt to obtain the distribution of SINR in CF mMIMO systems. Considering a
downlink CF mMIMO system with pilot contamination, we first give the
closed-form expression of the SINR. Based on our analytical work on the two
components of the SINR, i.e., desired signal and interference-plus-noise, we
then derive the probability density function and cumulative distribution
function of the SINR under maximum ratio transmission (MRT) and full-pilot
zero-forcing (FZF) precoding, respectively. Subsequently, the closed-form
expressions for two more sophisticated performance metrics, i.e., achievable
rate and outage probability, can be obtained. Finally, we perform Monte Carlo
simulations to validate our analytical work. The results demonstrate the
effectiveness of the derived SINR distribution, achievable rate, and outage
probability.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03270" title="Abstract">arXiv:2310.03270</a> [<a href="/pdf/2310.03270" title="Download PDF">pdf</a>, <a href="/format/2310.03270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yefei He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models have demonstrated remarkable capabilities in image synthesis
and related generative tasks. Nevertheless, their practicality for low-latency
real-world applications is constrained by substantial computational costs and
latency issues. Quantization is a dominant way to compress and accelerate
diffusion models, where post-training quantization (PTQ) and quantization-aware
training (QAT) are two main approaches, each bearing its own properties. While
PTQ exhibits efficiency in terms of both time and data usage, it may lead to
diminished performance in low bit-width. On the other hand, QAT can alleviate
performance degradation but comes with substantial demands on computational and
data resources. To capitalize on the advantages while avoiding their respective
drawbacks, we introduce a data-free and parameter-efficient fine-tuning
framework for low-bit diffusion models, dubbed EfficientDM, to achieve
QAT-level performance with PTQ-like efficiency. Specifically, we propose a
quantization-aware variant of the low-rank adapter (QALoRA) that can be merged
with model weights and jointly quantized to low bit-width. The fine-tuning
process distills the denoising capabilities of the full-precision model into
its quantized counterpart, eliminating the requirement for training data. We
also introduce scale-aware optimization and employ temporal learned step-size
quantization to further enhance performance. Extensive experimental results
demonstrate that our method significantly outperforms previous PTQ-based
diffusion models while maintaining similar time and data efficiency.
Specifically, there is only a marginal 0.05 sFID increase when quantizing both
weights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to
QAT-based methods, our EfficientDM also boasts a 16.2x faster quantization
speed with comparable generation quality.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03272" title="Abstract">arXiv:2310.03272</a> [<a href="/pdf/2310.03272" title="Download PDF">pdf</a>, <a href="/format/2310.03272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Alignment with Transferable Graph Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiashu He</a>, 
<a href="/search/cs?searchtype=author&query=Kanatsoulis%2C+C+I">Charilaos I. Kanatsoulis</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Network alignment is the task of establishing one-to-one correspondences
between the nodes of different graphs and finds a plethora of applications in
high-impact domains. However, this task is known to be NP-hard in its general
form, and existing algorithms do not scale up as the size of the graphs
increases. To tackle both challenges we propose a novel generalized graph
autoencoder architecture, designed to extract powerful and robust node
embeddings, that are tailored to the alignment task. We prove that the
generated embeddings are associated with the eigenvalues and eigenvectors of
the graphs and can achieve more accurate alignment compared to classical
spectral methods. Our proposed framework also leverages transfer learning and
data augmentation to achieve efficient network alignment at a very large scale
without retraining. Extensive experiments on both network and sub-network
alignment with real-world graphs provide corroborating evidence supporting the
effectiveness and scalability of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03273" title="Abstract">arXiv:2310.03273</a> [<a href="/pdf/2310.03273" title="Download PDF">pdf</a>, <a href="/format/2310.03273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ablation Study to Clarify the Mechanism of Object Segmentation in  Multi-Object Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Komatsu%2C+T">Takayuki Komatsu</a>, 
<a href="/search/cs?searchtype=author&query=Ohmura%2C+Y">Yoshiyuki Ohmura</a>, 
<a href="/search/cs?searchtype=author&query=Kuniyoshi%2C+Y">Yasuo Kuniyoshi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Multi-object representation learning aims to represent complex real-world
visual input using the composition of multiple objects. Representation learning
methods have often used unsupervised learning to segment an input image into
individual objects and encode these objects into each latent vector. However,
it is not clear how previous methods have achieved the appropriate segmentation
of individual objects. Additionally, most of the previous methods regularize
the latent vectors using a Variational Autoencoder (VAE). Therefore, it is not
clear whether VAE regularization contributes to appropriate object
segmentation. To elucidate the mechanism of object segmentation in multi-object
representation learning, we conducted an ablation study on MONet, which is a
typical method. MONet represents multiple objects using pairs that consist of
an attention mask and the latent vector corresponding to the attention mask.
Each latent vector is encoded from the input image and attention mask. Then,
the component image and attention mask are decoded from each latent vector. The
loss function of MONet consists of 1) the sum of reconstruction losses between
the input image and decoded component image, 2) the VAE regularization loss of
the latent vector, and 3) the reconstruction loss of the attention mask to
explicitly encode shape information. We conducted an ablation study on these
three loss functions to investigate the effect on segmentation performance. Our
results showed that the VAE regularization loss did not affect segmentation
performance and the others losses did affect it. Based on this result, we
hypothesize that it is important to maximize the attention mask of the image
region best represented by a single latent vector corresponding to the
attention mask. We confirmed this hypothesis by evaluating a new loss function
with the same mechanism as the hypothesis.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03274" title="Abstract">arXiv:2310.03274</a> [<a href="/pdf/2310.03274" title="Download PDF">pdf</a>, <a href="/format/2310.03274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fragment-based Pretraining and Finetuning on Molecular Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luong%2C+K">Kha-Dinh Luong</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Ambuj Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures, to be published in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Property prediction on molecular graphs is an important application of Graph
Neural Networks (GNNs). Recently, unlabeled molecular data has become abundant,
which facilitates the rapid development of self-supervised learning for GNNs in
the chemical domain. In this work, we propose pretraining GNNs at the fragment
level, which serves as a promising middle ground to overcome the limitations of
node-level and graph-level pretraining. Borrowing techniques from recent work
on principle subgraph mining, we obtain a compact vocabulary of prevalent
fragments that span a large pretraining dataset. From the extracted vocabulary,
we introduce several fragment-based contrastive and predictive pretraining
tasks. The contrastive learning task jointly pretrains two different GNNs: one
based on molecular graphs and one based on fragment graphs, which represents
high-order connectivity within molecules. By enforcing the consistency between
the fragment embedding and the aggregated embedding of the corresponding atoms
from the molecular graphs, we ensure that both embeddings capture structural
information at multiple resolutions. The structural information of the fragment
graphs is further exploited to extract auxiliary labels for the graph-level
predictive pretraining. We employ both the pretrained molecular-based and
fragment-based GNNs for downstream prediction, thus utilizing the fragment
information during finetuning. Our models advance the performances on 5 out of
8 common molecular benchmarks and improve the performances on long-range
biological benchmarks by at least 11.5%.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03275" title="Abstract">arXiv:2310.03275</a> [<a href="/pdf/2310.03275" title="Download PDF">pdf</a>, <a href="/format/2310.03275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Power Optimization in Multi-IRS Aided Delay-Constrained IoVT Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chong%2C+a">aolin Chong</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+H">Hancheng Lu</a>, 
<a href="/search/eess?searchtype=author&query=Qin%2C+L">Langtian Qin</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Chenwu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jiasen Li</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+C+W">Chang Wen Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">With the advancement of video sensors in the Internet of Things, Internet of
Video Things (IoVT) systems, capable of delivering abundant and diverse
information, have been increasingly deployed for various applications. However,
the extensive transmission of video data in IoVT poses challenges in terms of
delay and power consumption. Intelligent reconfigurable surface (IRS), as an
emerging technology, can enhance communication quality and consequently improve
system performance by reconfiguring wireless propagation environments. Inspired
by this, we propose a multi-IRS aided IoVT system that leverages IRS to enhance
communication quality, thereby reducing power consumption while satisfying
delay requirements. To fully leverage the benefits of IRS, we jointly optimize
power control for IoVT devices and passive beamforming for IRS to minimize
long-term total power consumption under delay constraints. To solve this
problem, we first utilize Lyapunov optimization to decouple the long-term
optimization problem into each time slot. Subsequently, an alternating
optimization algorithm employing optimal solution-seeking and fractional
programming is proposed to effectively solve the optimization problems at each
time slot. Simulation results demonstrate that the proposed algorithm
significantly outperforms benchmark algorithms in terms of long-term total
power consumption. Moreover, a trade-off between the number of IRS elements and
system performance is also proved.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03278" title="Abstract">arXiv:2310.03278</a> [<a href="/pdf/2310.03278" title="Download PDF">pdf</a>, <a href="/format/2310.03278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Pilot Contamination and Enabling IoT Scalability in Massive  MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saeed%2C+M+K">Muhammad Kamran Saeed</a>, 
<a href="/search/cs?searchtype=author&query=Kamal%2C+A+E">Ahmed E. Kamal</a>, 
<a href="/search/cs?searchtype=author&query=Khokhar%2C+A">Ashfaq Khokhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted At GLOBECOM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Massive MIMO is expected to play an important role in the development of 5G
networks. This paper addresses the issue of pilot contamination and scalability
in massive MIMO systems. The current practice of reusing orthogonal pilot
sequences in adjacent cells leads to difficulty in differentiating incoming
inter- and intra-cell pilot sequences. One possible solution is to increase the
number of orthogonal pilot sequences, which results in dedicating more space of
coherence block to pilot transmission than data transmission. This, in turn,
also hinders the scalability of massive MIMO systems, particularly in
accommodating a large number of IoT devices within a cell. To overcome these
challenges, this paper devises an innovative pilot allocation scheme based on
the data transfer patterns of IoT devices. The scheme assigns orthogonal pilot
sequences to clusters of devices instead of individual devices, allowing
multiple devices to utilize the same pilot for periodically transmitting data.
Moreover, we formulate the pilot assignment problem as a graph coloring problem
and use the max k-cut graph partitioning approach to overcome the pilot
contamination in a multicell massive MIMO system. The proposed scheme
significantly improves the spectral efficiency and enables the scalability of
massive MIMO systems; for instance, by using ten orthogonal pilot sequences, we
are able to accommodate 200 devices with only a 12.5% omission rate.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03279" title="Abstract">arXiv:2310.03279</a> [<a href="/pdf/2310.03279" title="Download PDF">pdf</a>, <a href="/format/2310.03279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classifying Whole Slide Images: What Matters?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+L">Long Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nibali%2C+A">Aiden Nibali</a>, 
<a href="/search/cs?searchtype=author&query=Millward%2C+J">Joshua Millward</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhen He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently there have been many algorithms proposed for the classification of
very high resolution whole slide images (WSIs). These new algorithms are mostly
focused on finding novel ways to combine the information from small local
patches extracted from the slide, with an emphasis on effectively aggregating
more global information for the final predictor. In this paper we thoroughly
explore different key design choices for WSI classification algorithms to
investigate what matters most for achieving high accuracy. Surprisingly, we
found that capturing global context information does not necessarily mean
better performance. A model that captures the most global information
consistently performs worse than a model that captures less global information.
In addition, a very simple multi-instance learning method that captures no
global information performs almost as well as models that capture a lot of
global information. These results suggest that the most important features for
effective WSI classification are captured at the local small patch level, where
cell and tissue micro-environment detail is most pronounced. Another surprising
finding was that unsupervised pre-training on a larger set of 33 cancers gives
significantly worse performance compared to pre-training on a smaller dataset
of 7 cancers (including the target cancer). We posit that pre-training on a
smaller, more focused dataset allows the feature extractor to make better use
of the limited feature space to better discriminate between subtle differences
in the input patch.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03281" title="Abstract">arXiv:2310.03281</a> [<a href="/pdf/2310.03281" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A 5&#x27; UTR Language Model for Decoding Untranslated Regions of mRNA and  Function Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+Y">Yanyi Chu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yupeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaixuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+L">Le Cong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jason Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The 5' UTR, a regulatory region at the beginning of an mRNA molecule, plays a
crucial role in regulating the translation process and impacts the protein
expression level. Language models have showcased their effectiveness in
decoding the functions of protein and genome sequences. Here, we introduced a
language model for 5' UTR, which we refer to as the UTR-LM. The UTR-LM is
pre-trained on endogenous 5' UTRs from multiple species and is further
augmented with supervised information including secondary structure and minimum
free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The
model outperformed the best-known benchmark by up to 42% for predicting the
Mean Ribosome Loading, and by up to 60% for predicting the Translation
Efficiency and the mRNA Expression Level. The model also applies to identifying
unannotated Internal Ribosome Entry Sites within the untranslated region and
improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we
designed a library of 211 novel 5' UTRs with high predicted values of
translation efficiency and evaluated them via a wet-lab assay. Experiment
results confirmed that our top designs achieved a 32.5% increase in protein
production level relative to well-established 5' UTR optimized for
therapeutics.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03283" title="Abstract">arXiv:2310.03283</a> [<a href="/pdf/2310.03283" title="Download PDF">pdf</a>, <a href="/format/2310.03283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Formalism and Approach for Improving Robustness of Large Language  Models Using Risk-Adjusted Confidence Scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+K">Ke Shen</a>, 
<a href="/search/cs?searchtype=author&query=Kejriwal%2C+M">Mayank Kejriwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs), such as ChatGPT, have achieved impressive
milestones in natural language processing (NLP). Despite their impressive
performance, the models are known to pose important risks. As these models are
deployed in real-world applications, a systematic understanding of different
risks posed by these models on tasks such as natural language inference (NLI),
is much needed. In this paper, we define and formalize two distinct types of
risk: decision risk and composite risk. We also propose a risk-centric
evaluation framework, and four novel metrics, for assessing LLMs on these risks
in both in-domain and out-of-domain settings. Finally, we propose a
risk-adjusted calibration method called DwD for helping LLMs minimize these
risks in an overall NLI architecture. Detailed experiments, using four NLI
benchmarks, three baselines and two LLMs, including ChatGPT, show both the
practical utility of the evaluation framework, and the efficacy of DwD in
reducing decision and composite risk. For instance, when using DwD, an
underlying LLM is able to address an extra 20.1% of low-risk inference tasks
(but which the LLM erroneously deems high-risk without risk adjustment) and
skip a further 19.8% of high-risk tasks, which would have been answered
incorrectly.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03285" title="Abstract">arXiv:2310.03285</a> [<a href="/pdf/2310.03285" title="Download PDF">pdf</a>, <a href="/format/2310.03285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Burning the Adversarial Bridges: Robust Windows Malware Detection  Against Binary-level Mutations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abusnaina%2C+A">Ahmed Abusnaina</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Sunpreet Arora</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Ke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Christodorescu%2C+M">Mihai Christodorescu</a>, 
<a href="/search/cs?searchtype=author&query=Mohaisen%2C+D">David Mohaisen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Toward robust malware detection, we explore the attack surface of existing
malware detection systems. We conduct root-cause analyses of the practical
binary-level black-box adversarial malware examples. Additionally, we uncover
the sensitivity of volatile features within the detection engines and exhibit
their exploitability. Highlighting volatile information channels within the
software, we introduce three software pre-processing steps to eliminate the
attack surface, namely, padding removal, software stripping, and inter-section
information resetting. Further, to counter the emerging section injection
attacks, we propose a graph-based section-dependent information extraction
scheme for software representation. The proposed scheme leverages aggregated
information within various sections in the software to enable robust malware
detection and mitigate adversarial settings. Our experimental results show that
traditional malware detection models are ineffective against adversarial
threats. However, the attack surface can be largely reduced by eliminating the
volatile information. Therefore, we propose simple-yet-effective methods to
mitigate the impacts of binary manipulation attacks. Overall, our graph-based
malware detection scheme can accurately detect malware with an area under the
curve score of 88.32\% and a score of 88.19% under a combination of binary
manipulation attacks, exhibiting the efficiency of our proposed scheme.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03288" title="Abstract">arXiv:2310.03288</a> [<a href="/pdf/2310.03288" title="Download PDF">pdf</a>, <a href="/format/2310.03288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PoseAction: Action Recognition for Patients in the Ward using Deep  Learning Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zherui Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeow%2C+R+C">Raye Chen-Hua Yeow</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Real-time intelligent detection and prediction of subjects' behavior
particularly their movements or actions is critical in the ward. This approach
offers the advantage of reducing in-hospital care costs and improving the
efficiency of healthcare workers, which is especially true for scenarios at
night or during peak admission periods. Therefore, in this work, we propose
using computer vision (CV) and deep learning (DL) methods for detecting
subjects and recognizing their actions. We utilize OpenPose as an accurate
subject detector for recognizing the positions of human subjects in the video
stream. Additionally, we employ AlphAction's Asynchronous Interaction
Aggregation (AIA) network to predict the actions of detected subjects. This
integrated model, referred to as PoseAction, is proposed. At the same time, the
proposed model is further trained to predict 12 common actions in ward areas,
such as staggering, chest pain, and falling down, using medical-related video
clips from the NTU RGB+D and NTU RGB+D 120 datasets. The results demonstrate
that PoseAction achieves the highest classification mAP of 98.72% (IoU@0.5).
Additionally, this study develops an online real-time mode for action
recognition, which strongly supports the clinical translation of PoseAction.
Furthermore, using OpenPose's function for recognizing face key points, we also
implement face blurring, which is a practical solution to address the privacy
protection concerns of patients and healthcare workers. Nevertheless, the
training data for PoseAction is currently limited, particularly in terms of
label diversity. Consequently, the subsequent step involves utilizing a more
diverse dataset (including general actions) to train the model's parameters for
improved generalization.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03291" title="Abstract">arXiv:2310.03291</a> [<a href="/pdf/2310.03291" title="Download PDF">pdf</a>, <a href="/format/2310.03291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimVLG: Simple and Efficient Pretraining of Visual Language Generative  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jian%2C+Y">Yiren Jian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tingkai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yunzhe Tao</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+S">Soroush Vosoughi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">HX Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose ``SimVLG'', a streamlined framework for the
pre-training of computationally intensive vision-language generative models,
leveraging frozen pre-trained large language models (LLMs). The prevailing
paradigm in vision-language pre-training (VLP) typically involves a two-stage
optimization process: an initial resource-intensive phase dedicated to
general-purpose vision-language representation learning, aimed at extracting
and consolidating pertinent visual features, followed by a subsequent phase
focusing on end-to-end alignment between visual and linguistic modalities. Our
one-stage, single-loss framework circumvents the aforementioned computationally
demanding first stage of training by gradually merging similar visual tokens
during training. This gradual merging process effectively compacts the visual
information while preserving the richness of semantic content, leading to fast
convergence without sacrificing performance. Our experiments show that our
approach can speed up the training of vision-language models by a factor
$\times 5$ without noticeable impact on the overall performance. Additionally,
we show that our models can achieve comparable performance to current
vision-language models with only $1/10$ of the data. Finally, we demonstrate
how our image-text models can be easily adapted to video-language generative
tasks through a novel soft attentive temporal token merging modules.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03292" title="Abstract">arXiv:2310.03292</a> [<a href="/pdf/2310.03292" title="Download PDF">pdf</a>, <a href="/format/2310.03292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoK: Access Control Policy Generation from High-level Natural Language  Requirements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jayasundara%2C+S+H">Sakuna Harinda Jayasundara</a>, 
<a href="/search/cs?searchtype=author&query=Arachchilage%2C+N+A+G">Nalin Asanka Gamagedara Arachchilage</a>, 
<a href="/search/cs?searchtype=author&query=Russello%2C+G">Giovanni Russello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Administrator-centered access control failures can cause data breaches,
putting organizations at risk of financial loss and reputation damage. Existing
graphical policy configuration tools and automated policy generation frameworks
attempt to help administrators configure and generate access control policies
by avoiding such failures. However, graphical policy configuration tools are
prone to human errors, making them unusable. On the other hand, automated
policy generation frameworks are prone to erroneous predictions, making them
unreliable. Therefore, to find ways to improve their usability and reliability,
we conducted a Systematic Literature Review analyzing 49 publications, to
identify those tools, frameworks, and their limitations. Identifying those
limitations will help develop effective access control policy generation
solutions while avoiding access control failures.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03293" title="Abstract">arXiv:2310.03293</a> [<a href="/pdf/2310.03293" title="Download PDF">pdf</a>, <a href="/format/2310.03293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Dialogue Response Generation Agent for Large Language Models by  Asking Questions to Detect User&#x27;s Intentions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiangqing Shen</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+R">Rui Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs), such as ChatGPT, have recently been applied to
various NLP tasks due to its open-domain generation capabilities. However,
there are two issues with applying LLMs to dialogue tasks. 1. During the
dialogue process, users may have implicit intentions that might be overlooked
by LLMs. Consequently, generated responses couldn't align with the user's
intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively.
In certain specific domains, their knowledge may be incomplete, and LLMs cannot
update the latest knowledge in real-time. To tackle these issues, we propose a
framework~\emph{using LLM to \textbf{E}nhance dialogue response generation by
asking questions to \textbf{D}etect user's \textbf{I}mplicit
in\textbf{T}entions} (\textbf{EDIT}). Firstly, EDIT generates open questions
related to the dialogue context as the potential user's intention; Then, EDIT
answers those questions by interacting with LLMs and searching in
domain-specific knowledge bases respectively, and use LLMs to choose the proper
answers to questions as extra knowledge; Finally, EDIT enhances response
generation by explicitly integrating those extra knowledge. Besides, previous
question generation works only focus on asking questions with answers in
context. In order to ask open questions, we construct a Context-Open-Question
(COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and
Holl-E), EDIT outperformed other LLMs.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03294" title="Abstract">arXiv:2310.03294</a> [<a href="/pdf/2310.03294" title="Download PDF">pdf</a>, <a href="/format/2310.03294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightSeq: Sequence Level Parallelism for Distributed Training of Long  Context Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dacheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+R">Rulin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+A">Anze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E+P">Eric P. Xing</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+E">Joseph E. Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Stoica%2C+I">Ion Stoica</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xuezhe Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Increasing the context length of large language models (LLMs) unlocks
fundamentally new capabilities, but also significantly increases the memory
footprints of training. Previous model-parallel systems such as Megatron-LM
partition and compute different attention heads in parallel, resulting in large
communication volumes, so they cannot scale beyond the number of attention
heads, thereby hindering its adoption. In this paper, we introduce a new
approach, LightSeq, for long-context LLMs training. LightSeq has many notable
advantages. First, LightSeq partitions over the sequence dimension, hence is
agnostic to model architectures and readily applicable for models with varying
numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query
attention. Second, LightSeq not only requires up to 4.7x less communication
than Megatron-LM on popular LLMs but also overlaps the communication with
computation. To further reduce the training time, LightSeq features a novel
gradient checkpointing scheme to bypass an forward computation for
memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants
with sequence lengths from 32K to 512K. Through comprehensive experiments on
single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x
end-to-end speedup, and a 2-8x longer sequence length on models with fewer
heads, compared to Megatron-LM. Codes will be available at
https://github.com/RulinShao/LightSeq.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03295" title="Abstract">arXiv:2310.03295</a> [<a href="/pdf/2310.03295" title="Download PDF">pdf</a>, <a href="/format/2310.03295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can pre-trained models assist in dataset distillation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jianyang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianle Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoniu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xuan%2C+Q">Qi Xuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Dataset Distillation (DD) is a prominent technique that encapsulates
knowledge from a large-scale original dataset into a small synthetic dataset
for efficient training. Meanwhile, Pre-trained Models (PTMs) function as
knowledge repositories, containing extensive information from the original
dataset. This naturally raises a question: Can PTMs effectively transfer
knowledge to synthetic datasets, guiding DD accurately? To this end, we conduct
preliminary experiments, confirming the contribution of PTMs to DD. Afterwards,
we systematically study different options in PTMs, including initialization
parameters, model architecture, training epoch and domain knowledge, revealing
that: 1) Increasing model diversity enhances the performance of synthetic
datasets; 2) Sub-optimal models can also assist in DD and outperform
well-trained ones in certain cases; 3) Domain-specific PTMs are not mandatory
for DD, but a reasonable domain match is crucial. Finally, by selecting optimal
options, we significantly improve the cross-architecture generalization over
baseline DD methods. We hope our work will facilitate researchers to develop
better DD techniques. Our code is available at
https://github.com/yaolu-zjut/DDInterpreter.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03301" title="Abstract">arXiv:2310.03301</a> [<a href="/pdf/2310.03301" title="Download PDF">pdf</a>, <a href="/format/2310.03301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Energy Decompositions for Partial Inference of GFlowNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+H">Hyosoon Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sungsoo Ahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper studies generative flow networks (GFlowNets) to sample objects
from the Boltzmann energy distribution via a sequence of actions. In
particular, we focus on improving GFlowNet with partial inference: training
flow functions with the evaluation of the intermediate states or transitions.
To this end, the recently developed forward-looking GFlowNet reparameterizes
the flow functions based on evaluating the energy of intermediate states.
However, such an evaluation of intermediate energies may (i) be too expensive
or impossible to evaluate and (ii) even provide misleading training signals
under large energy fluctuations along the sequence of actions. To resolve this
issue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our
main idea is to (i) decompose the energy of an object into learnable potential
functions defined on state transitions and (ii) reparameterize the flow
functions using the potential functions. In particular, to produce informative
local credits, we propose to regularize the potential to change smoothly over
the sequence of actions. It is also noteworthy that training GFlowNet with our
learned potential can preserve the optimal policy. We empirically verify the
superiority of LED-GFN in five problems including the generation of
unstructured and maximum independent sets, molecular graphs, and RNA sequences.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03302" title="Abstract">arXiv:2310.03302</a> [<a href="/pdf/2310.03302" title="Download PDF">pdf</a>, <a href="/format/2310.03302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Large Language Models As AI Research Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Vora%2C+J">Jian Vora</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>, 
<a href="/search/cs?searchtype=author&query=Leskovec%2C+J">Jure Leskovec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Scientific experimentation involves an iterative process of creating
hypotheses, designing experiments, running experiments, and analyzing the
results. Can we build AI research agents to perform these long-horizon tasks?
To take a step towards building and evaluating research agents on such
open-ended decision-making tasks, we focus on the problem of machine learning
engineering: given a task description and a dataset, build a high-performing
model. In this paper, we propose MLAgentBench, a suite of ML tasks for
benchmarking AI research agents. Agents can perform actions like
reading/writing files, executing code, and inspecting outputs. With these
actions, agents could run experiments, analyze the results, and modify the code
of entire machine learning pipelines, such as data processing, architecture,
training processes, etc. The benchmark then automatically evaluates the agent's
performance objectively over various metrics related to performance and
efficiency. We also design an LLM-based research agent to automatically perform
experimentation loops in such an environment. Empirically, we find that a
GPT-4-based research agent can feasibly build compelling ML models over many
tasks in MLAgentBench, displaying highly interpretable plans and actions.
However, the success rates vary considerably; they span from almost 90\% on
well-established older datasets to as low as 10\% on recent Kaggle Challenges
-- unavailable during the LLM model's pretraining -- and even 0\% on newer
research challenges like BabyLM. Finally, we identify several key challenges
for LLM-based research agents such as long-term planning and hallucination. Our
code is released at https://github.com/snap-stanford/MLAgentBench.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03303" title="Abstract">arXiv:2310.03303</a> [<a href="/pdf/2310.03303" title="Download PDF">pdf</a>, <a href="/format/2310.03303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Two-stage Based Social Preference Recognition in Multi-Agent  Autonomous Driving System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jintao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongkun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+R">Rong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+E">Eryun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Multi-Agent Reinforcement Learning (MARL) has become a promising solution for
constructing a multi-agent autonomous driving system (MADS) in complex and
dense scenarios. But most methods consider agents acting selfishly, which leads
to conflict behaviors. Some existing works incorporate the concept of social
value orientation (SVO) to promote coordination, but they lack the knowledge of
other agents' SVOs, resulting in conservative maneuvers. In this paper, we aim
to tackle the mentioned problem by enabling the agents to understand other
agents' SVOs. To accomplish this, we propose a two-stage system framework.
Firstly, we train a policy by allowing the agents to share their ground truth
SVOs to establish a coordinated traffic flow. Secondly, we develop a
recognition network that estimates agents' SVOs and integrates it with the
policy trained in the first stage. Experiments demonstrate that our developed
method significantly improves the performance of the driving policy in MADS
compared to two state-of-the-art MARL algorithms.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03304" title="Abstract">arXiv:2310.03304</a> [<a href="/pdf/2310.03304" title="Download PDF">pdf</a>, <a href="/format/2310.03304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Personalized Story Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Danqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kevin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaomeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A">Andrew Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While large language models (LLMs) have shown impressive results for more
objective tasks such as QA and retrieval, it remains nontrivial to evaluate
their performance on open-ended text generation for reasons including (1) data
contamination; (2) multi-dimensional evaluation criteria; and (3)
subjectiveness stemming from reviewers' personal preferences. To address such
issues, we propose to model personalization in an uncontaminated open-ended
generation assessment. We create two new datasets Per-MPST and Per-DOC for
personalized story evaluation, by re-purposing existing datasets with proper
anonymization and new personalized labels. We further develop a personalized
story evaluation model PERSE to infer reviewer preferences and provide a
personalized evaluation. Specifically, given a few exemplary reviews from a
particular reviewer, PERSE predicts either a detailed review or fine-grained
comparison in several aspects (such as interestingness and surprise) for that
reviewer on a new text input. Experimental results show that PERSE outperforms
GPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on
pairwise preference prediction accuracy. Both datasets and code will be
released at https://github.com/dqwang122/PerSE.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03309" title="Abstract">arXiv:2310.03309</a> [<a href="/pdf/2310.03309" title="Download PDF">pdf</a>, <a href="/format/2310.03309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concise and Organized Perception Facilitates Large Language Models for  Deductive Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shaotian Yan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jieping Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Exploiting large language models (LLMs) to tackle deductive reasoning has
garnered growing attention. It still remains highly challenging to achieve
satisfactory results in complex deductive problems, characterized by plenty of
premises (i.e., facts or rules) entailing intricate relationships among
entities and requiring multi-hop reasoning. One intuitive solution is to
decompose the original task into smaller sub-tasks, and then chain the multiple
casual reasoning steps together in a forward (e.g., Selection-Inference) or
backward (e.g., LAMBADA) direction. However, these techniques inevitably
necessitate a large number of overall stages, leading to computationally
expensive operations and a higher possibility of making misleading steps. In
addition to stage-by-stage decomposition, we draw inspiration from another
aspect of human problem-solving. Humans tend to distill the most relevant
information and organize their thoughts systematically (e.g., creating mind
maps), which assists them in answering questions or drawing conclusions
precisely and quickly. In light of this, we propose a novel reasoning approach
named Concise and Organized Perception (COP). COP carefully analyzes the given
statements to efficiently identify the most pertinent information while
eliminating redundancy. It then prompts the LLMs in a more organized form that
adapts to the model's inference process. By perceiving concise and organized
proofs, the deductive reasoning abilities of LLMs can be better elicited, and
the risk of acquiring errors caused by excessive reasoning stages is mitigated.
Furthermore, our approach can be combined with the aforementioned ones to
further boost their performance. Extensive experimental results on three
popular deductive benchmarks (i.e., ProofWriter, PrOntoQA and PrOntoQA-OOD)
show that COP significantly outperforms previous state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03311" title="Abstract">arXiv:2310.03311</a> [<a href="/pdf/2310.03311" title="Download PDF">pdf</a>, <a href="/format/2310.03311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Variational Multivariate Information Bottleneck -- A Framework for  Variational Losses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdelaleem%2C+E">Eslam Abdelaleem</a>, 
<a href="/search/cs?searchtype=author&query=Nemenman%2C+I">Ilya Nemenman</a>, 
<a href="/search/cs?searchtype=author&query=Martini%2C+K+M">K. Michael Martini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistical Mechanics (cond-mat.stat-mech); Information Theory (cs.IT); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">Variational dimensionality reduction methods are known for their high
accuracy, generative abilities, and robustness. These methods have many
theoretical justifications. Here we introduce a unifying principle rooted in
information theory to rederive and generalize existing variational methods and
design new ones. We base our framework on an interpretation of the multivariate
information bottleneck, in which two Bayesian networks are traded off against
one another. We interpret the first network as an encoder graph, which
specifies what information to keep when compressing the data. We interpret the
second network as a decoder graph, which specifies a generative model for the
data. Using this framework, we rederive existing dimensionality reduction
methods such as the deep variational information bottleneck (DVIB), beta
variational auto-encoders (beta-VAE), and deep variational canonical
correlation analysis (DVCCA). The framework naturally introduces a trade-off
parameter between compression and reconstruction in the DVCCA family of
algorithms, resulting in the new beta-DVCCA family. In addition, we derive a
new variational dimensionality reduction method, deep variational symmetric
informational bottleneck (DVSIB), which simultaneously compresses two variables
to preserve information between their compressed representations. We implement
all of these algorithms and evaluate their ability to produce shared low
dimensional latent spaces on a modified noisy MNIST dataset. We show that
algorithms that are better matched to the structure of the data (beta-DVCCA and
DVSIB) produce better latent spaces as measured by classification accuracy and
the dimensionality of the latent variables. We believe that this framework can
be used to unify other multi-view representation learning algorithms.
Additionally, it provides a straightforward framework for deriving
problem-specific loss functions.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03312" title="Abstract">arXiv:2310.03312</a> [<a href="/pdf/2310.03312" title="Download PDF">pdf</a>, <a href="/format/2310.03312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certifiably Robust Graph Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Minhua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Teng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+E">Enyan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph
representation learning method. However, it has been shown that GCL is
vulnerable to adversarial attacks on both the graph structure and node
attributes. Although empirical approaches have been proposed to enhance the
robustness of GCL, the certifiable robustness of GCL is still remain
unexplored. In this paper, we develop the first certifiably robust framework in
GCL. Specifically, we first propose a unified criteria to evaluate and certify
the robustness of GCL. We then introduce a novel technique, RES (Randomized
Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and
this certified robustness can be provably preserved in downstream tasks.
Furthermore, an effective training method is proposed for robust GCL. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed method in providing effective certifiable robustness and enhancing the
robustness of any GCL model. The source code of RES is available at
https://github.com/ventr1c/RES-GCL.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03314" title="Abstract">arXiv:2310.03314</a> [<a href="/pdf/2310.03314" title="Download PDF">pdf</a>, <a href="/format/2310.03314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Human-Robot Collaboration using Constrained Probabilistic  Human-Motion Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kothari%2C+A">Aadi Kothari</a>, 
<a href="/search/cs?searchtype=author&query=Tohme%2C+T">Tony Tohme</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaotong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Youcef-Toumi%2C+K">Kamal Youcef-Toumi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures. Associated video demonstration can be found at <a href="https://www.youtube.com/@MITMechatronics">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Human motion prediction is an essential step for efficient and safe
human-robot collaboration. Current methods either purely rely on representing
the human joints in some form of neural network-based architecture or use
regression models offline to fit hyper-parameters in the hope of capturing a
model encompassing human motion. While these methods provide good initial
results, they are missing out on leveraging well-studied human body kinematic
models as well as body and scene constraints which can help boost the efficacy
of these prediction frameworks while also explicitly avoiding implausible human
joint configurations. We propose a novel human motion prediction framework that
incorporates human joint constraints and scene constraints in a Gaussian
Process Regression (GPR) model to predict human motion over a set time horizon.
This formulation is combined with an online context-aware constraints model to
leverage task-dependent motions. It is tested on a human arm kinematic model
and implemented on a human-robot collaborative setup with a UR5 robot arm to
demonstrate the real-time capability of our approach. Simulations were also
performed on datasets like HA4M and ANDY. The simulation and experimental
results demonstrate considerable improvements in a Gaussian Process framework
when these constraints are explicitly considered.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03318" title="Abstract">arXiv:2310.03318</a> [<a href="/pdf/2310.03318" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Metaverse Application Dependability Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zong%2C+Y">Yingfan Zong</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jing Bai</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xiaolin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Machida%2C+F">Fumio Machida</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yingsi Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Metaverse as-a-Service (MaaS) enables Metaverse tenants to execute their
APPlications (MetaAPP) by allocating Metaverse resources in the form of
Metaverse service functions (MSF). Usually, each MSF is deployed in a virtual
machine (VM) for better resiliency and security. However, these MSFs along with
VMs and virtual machine monitors (VMM) running them may encounter software
aging after prolonged continuous operation. Then, there is a decrease in
MetaAPP dependability, namely, the dependability of the MSF chain (MSFC),
consisting of MSFs allocated to MetaAPP. This paper aims to investigate the
impact of both software aging and rejuvenation techniques on MetaAPP
dependability in the scenarios, where both active components (MSF, VM and VMM)
and their backup components are subject to software aging. We develop a
hierarchical model to capture behaviors of aging, failure, and recovery by
applying Semi-Markov process and reliability block diagram. Numerical analysis
and simulation experiments are conducted to evaluate the approximation accuracy
of the proposed model and dependability metrics. We then identify the key
parameters for improving the MetaAPP/MSFC dependability through sensitivity
analysis. The investigation is also made about the influence of various
parameters on MetaAPP/MSFC dependability.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03320" title="Abstract">arXiv:2310.03320</a> [<a href="/pdf/2310.03320" title="Download PDF">pdf</a>, <a href="/format/2310.03320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+B">Balasubramaniam Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Ioannidis%2C+V+N">Vassilis N. Ioannidis</a>, 
<a href="/search/cs?searchtype=author&query=Rangwala%2C+H">Huzefa Rangwala</a>, 
<a href="/search/cs?searchtype=author&query=Anubhai%2C+R">Rishita Anubhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Foundation models (FMs) are able to leverage large volumes of unlabeled data
to demonstrate superior performance across a wide range of tasks. However, FMs
developed for biomedical domains have largely remained unimodal, i.e.,
independently trained and used for tasks on protein sequences alone, small
molecule structures alone, or clinical data alone. To overcome this limitation
of biomedical FMs, we present BioBridge, a novel parameter-efficient learning
framework, to bridge independently trained unimodal FMs to establish multimodal
behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn
transformations between one unimodal FM and another without fine-tuning any
underlying unimodal FMs. Our empirical results demonstrate that BioBridge can
beat the best baseline KG embedding methods (on average by around 76.3%) in
cross-modal retrieval tasks. We also identify BioBridge demonstrates
out-of-domain generalization ability by extrapolating to unseen modalities or
relations. Additionally, we also show that BioBridge presents itself as a
general purpose retriever that can aid biomedical multimodal question answering
as well as enhance the guided generation of novel drugs.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03324" title="Abstract">arXiv:2310.03324</a> [<a href="/pdf/2310.03324" title="Download PDF">pdf</a>, <a href="/format/2310.03324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Limitation of CLIP Models: The Worst-Performing  Categories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jie-Jing Shao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiang-Xin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao-Wen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lan-Zhe Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu-Feng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Contrastive Language-Image Pre-training (CLIP) provides a foundation model by
integrating natural language into visual concepts, enabling zero-shot
recognition on downstream tasks. It is usually expected that satisfactory
overall accuracy can be achieved across numerous domains through well-designed
textual prompts. However, we found that their performance in the worst
categories is significantly inferior to the overall performance. For example,
on ImageNet, there are a total of 10 categories with class-wise accuracy as low
as 0\%, even though the overall performance has achieved 64.1\%. This
phenomenon reveals the potential risks associated with using CLIP models,
particularly in risk-sensitive applications where specific categories hold
significant importance. To address this issue, we investigate the alignment
between the two modalities in the CLIP model and propose the Class-wise
Matching Margin (\cmm) to measure the inference confusion. \cmm\ can
effectively identify the worst-performing categories and estimate the potential
performance of the candidate prompts. We further query large language models to
enrich descriptions of worst-performing categories and build a weighted
ensemble to highlight the efficient prompts. Experimental results clearly
verify the effectiveness of our proposal, where the accuracy on the worst-10
categories on ImageNet is boosted to 5.2\%, without manual prompt engineering,
laborious optimization, or access to labeled validation data.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03325" title="Abstract">arXiv:2310.03325</a> [<a href="/pdf/2310.03325" title="Download PDF">pdf</a>, <a href="/format/2310.03325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Concept-Based Visual Causal Transition and Symbolic Reasoning  for Visual Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yilue Qian</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Peiyu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lifeng Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories and unseen object categories.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03328" title="Abstract">arXiv:2310.03328</a> [<a href="/pdf/2310.03328" title="Download PDF">pdf</a>, <a href="/format/2310.03328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reformulating Domain Adaptation of Large Language Models as  Adapt-Retrieve-Revise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=wan%2C+Z">Zhen wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yating Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yexiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+F">Fei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Kurohashi%2C+S">Sadao Kurohashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While large language models (LLMs) like GPT-4 have recently demonstrated
astonishing zero-shot capabilities in general domain tasks, they often generate
content with hallucinations in specific domains such as Chinese law, hindering
their application in these areas. This is typically due to the absence of
training data that encompasses such a specific domain, preventing GPT-4 from
acquiring in-domain knowledge. A pressing challenge is that it's not plausible
to continue training LLMs of such scale on in-domain data.
<br />This paper introduces a simple and effective domain adaptation framework for
GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process.
The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain
by continuing learning on in-domain data. When solving a task, we leverage the
adapted LLM to generate a draft answer given a task query. Then, the draft
answer will be used to \textbf{retrieve} supporting evidence candidates from an
external in-domain knowledge base. Finally, the draft answer and retrieved
evidence are concatenated into a whole prompt to let GPT-4 assess the evidence
and \textbf{revise} the draft answer to generate the final answer.
<br />Our proposal combines the advantages of the efficiency of adapting a smaller
7B model with the evidence-assessing capability of GPT-4 and effectively
prevents GPT-4 from generating hallucinatory content. In the zero-shot setting
of four Chinese legal tasks, our method improves accuracy by 33.3\% compared to
the direct generation by GPT-4. When compared to two stronger retrieval-based
baselines, our method outperforms them by 15.4\% and 23.9\%. Our code will be
released
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03330" title="Abstract">arXiv:2310.03330</a> [<a href="/pdf/2310.03330" title="Download PDF">pdf</a>, <a href="/format/2310.03330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vehicle Cabin Climate MPC Parameter Tuning Using Constrained Contextual  Bayesian Optimization (C-CMES)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Stenger%2C+D">David Stenger</a>, 
<a href="/search/eess?searchtype=author&query=Reuscher%2C+T">Tim Reuscher</a>, 
<a href="/search/eess?searchtype=author&query=Vallery%2C+H">Heike Vallery</a>, 
<a href="/search/eess?searchtype=author&query=Abel%2C+D">Dirk Abel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the proceedings of the IEEE International Conference on Intelligent Transportation Systems (ITSC), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Climate-controlled cabins have for decades been standard in vehicles. Model
Predictive Controllers (MPCs) have shown promising results in achieving
temperature tracking in vehicle cabins and may improve upon model-free control
performance. However, for the multi-zone climate control case, proper
controller tuning is challenging, as externally, e.g., passenger-triggered
changes in compressor setting and thus mass flow lead to degraded control
performance. This paper presents a tuning method to automatically determine
robust MPC parameters, as a function of the blower mass flow. Constrained
contextual Bayesian optimization (BO) is used to derive policies minimizing a
high-level cost function subject to constraints in a defined scenario. The
proposed method leverages random disturbances and model-plant mismatch within
the training episodes to generate controller parameters achieving robust
disturbance rejection. The method contains a postprocessing step to achieve
smooth policies that can be utilized in real-world applications. First,
simulation results show that the mass flow-dependent policy outperforms a
constant parametrization, while achieving the desired closed-loop behavior.
Second, the robust tuning method greatly reduces worst-case overshoot and
produces consistent closed-loop behavior under varying operating conditions.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03331" title="Abstract">arXiv:2310.03331</a> [<a href="/pdf/2310.03331" title="Download PDF">pdf</a>, <a href="/format/2310.03331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tune Language Models to Approximate Unbiased In-context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+T">Timothy Chu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chiwun Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In-context learning (ICL) is an astonishing emergent ability of large
language models (LLMs). By presenting a prompt that includes multiple
input-output pairs as examples and introducing a new query input, models can
generate the corresponding output. However, the performance of models heavily
relies on the quality of the input prompt when implementing in-context
learning. Biased or imbalanced input prompts can significantly degrade the
performance of language models. To address this issue, we introduce a
reweighted algorithm called RICL (Reweighted In-context Learning). This
algorithm fine-tunes language models using an unbiased validation set to
determine the optimal weight for each input-output example to approximate
unbiased in-context learning. Furthermore, we also introduce a low-cost
reweighted algorithm, a linear optimal weight approximation algorithm called
LARICL (Linear Approximation of Reweighted In-context Learning). This algorithm
requires minimal training cost while providing effective results. We prove the
convergence of our algorithm and validate its performance through experiments
conducted on a numerical dataset. The experimental findings reveal a
substantial improvement in comparison to benchmarks including the performance
of casual prompt-based in-context learning and the performance of a classic
fine-tuning method.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03333" title="Abstract">arXiv:2310.03333</a> [<a href="/pdf/2310.03333" title="Download PDF">pdf</a>, <a href="/format/2310.03333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Multi-modal Object Detection and Tracking on Edge for  Regulatory Compliance Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+J+S">Jia Syuen Lim</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiajun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Khamis%2C+A">Abdelwahed Khamis</a>, 
<a href="/search/cs?searchtype=author&query=Arablouei%2C+R">Reza Arablouei</a>, 
<a href="/search/cs?searchtype=author&query=Barlow%2C+R">Robert Barlow</a>, 
<a href="/search/cs?searchtype=author&query=McAllister%2C+R">Ryan McAllister</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Regulatory compliance auditing across diverse industrial domains requires
heightened quality assurance and traceability. Present manual and intermittent
approaches to such auditing yield significant challenges, potentially leading
to oversights in the monitoring process. To address these issues, we introduce
a real-time, multi-modal sensing system employing 3D time-of-flight and RGB
cameras, coupled with unsupervised learning techniques on edge AI devices. This
enables continuous object tracking thereby enhancing efficiency in
record-keeping and minimizing manual interventions. While we validate the
system in a knife sanitization context within agrifood facilities, emphasizing
its prowess against occlusion and low-light issues with RGB cameras, its
potential spans various industrial monitoring settings.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03334" title="Abstract">arXiv:2310.03334</a> [<a href="/pdf/2310.03334" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Untargeted White-box Adversarial Attack with Heuristic Defence Methods  in Real-time Deep Learning based Network Intrusion Detection System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roshan%2C+K">Khushnaseeb Roshan</a>, 
<a href="/search/cs?searchtype=author&query=Zafar%2C+A">Aasim Zafar</a>, 
<a href="/search/cs?searchtype=author&query=Haque%2C+S+B+U">Sheikh Burhan Ul Haque</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Network Intrusion Detection System (NIDS) is a key component in securing the
computer network from various cyber security threats and network attacks.
However, consider an unfortunate situation where the NIDS is itself attacked
and vulnerable more specifically, we can say, How to defend the defender?. In
Adversarial Machine Learning (AML), the malicious actors aim to fool the
Machine Learning (ML) and Deep Learning (DL) models to produce incorrect
predictions with intentionally crafted adversarial examples. These adversarial
perturbed examples have become the biggest vulnerability of ML and DL based
systems and are major obstacles to their adoption in real-time and
mission-critical applications such as NIDS. AML is an emerging research domain,
and it has become a necessity for the in-depth study of adversarial attacks and
their defence strategies to safeguard the computer network from various cyber
security threads. In this research work, we aim to cover important aspects
related to NIDS, adversarial attacks and its defence mechanism to increase the
robustness of the ML and DL based NIDS. We implemented four powerful
adversarial attack techniques, namely, Fast Gradient Sign Method (FGSM),
Jacobian Saliency Map Attack (JSMA), Projected Gradient Descent (PGD) and
Carlini &amp; Wagner (C&amp;W) in NIDS. We analyzed its performance in terms of various
performance metrics in detail. Furthermore, the three heuristics defence
strategies, i.e., Adversarial Training (AT), Gaussian Data Augmentation (GDA)
and High Confidence (HC), are implemented to improve the NIDS robustness under
adversarial attack situations. The complete workflow is demonstrated in
real-time network with data packet flow. This research work provides the
overall background for the researchers interested in AML and its implementation
from a computer network security point of view.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03335" title="Abstract">arXiv:2310.03335</a> [<a href="/pdf/2310.03335" title="Download PDF">pdf</a>, <a href="/format/2310.03335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Test-time Domain Adaptation via Dynamic Sample Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanshuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Jie Hong</a>, 
<a href="/search/cs?searchtype=author&query=Cheraghian%2C+A">Ali Cheraghian</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+S">Shafin Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Ahmedt-Aristizabal%2C+D">David Ahmedt-Aristizabal</a>, 
<a href="/search/cs?searchtype=author&query=Petersson%2C+L">Lars Petersson</a>, 
<a href="/search/cs?searchtype=author&query=Harandi%2C+M">Mehrtash Harandi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The objective of Continual Test-time Domain Adaptation (CTDA) is to gradually
adapt a pre-trained model to a sequence of target domains without accessing the
source data. This paper proposes a Dynamic Sample Selection (DSS) method for
CTDA. DSS consists of dynamic thresholding, positive learning, and negative
learning processes. Traditionally, models learn from unlabeled unknown
environment data and equally rely on all samples' pseudo-labels to update their
parameters through self-training. However, noisy predictions exist in these
pseudo-labels, so all samples are not equally trustworthy. Therefore, in our
method, a dynamic thresholding module is first designed to select suspected
low-quality from high-quality samples. The selected low-quality samples are
more likely to be wrongly predicted. Therefore, we apply joint positive and
negative learning on both high- and low-quality samples to reduce the risk of
using wrong information. We conduct extensive experiments that demonstrate the
effectiveness of our proposed method for CTDA in the image domain,
outperforming the state-of-the-art results. Furthermore, our approach is also
evaluated in the 3D point cloud domain, showcasing its versatility and
potential for broader applicability.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03337" title="Abstract">arXiv:2310.03337</a> [<a href="/pdf/2310.03337" title="Download PDF">pdf</a>, <a href="/format/2310.03337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion Step-aware Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yukang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Luozhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingcong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for
data generation across various domains. However, a significant bottleneck is
the necessity for whole-network computation during every step of the generative
process, leading to high computational overheads. This paper presents a novel
framework, Denoising Diffusion Step-aware Models (DDSM), to address this
challenge. Unlike conventional approaches, DDSM employs a spectrum of neural
networks whose sizes are adapted according to the importance of each generative
step, as determined through evolutionary search. This step-wise network
variation effectively circumvents redundant computational efforts, particularly
in less critical steps, thereby enhancing the efficiency of the diffusion
model. Furthermore, the step-aware design can be seamlessly integrated with
other efficiency-geared diffusion models such as DDIMs and latent diffusion,
thus broadening the scope of computational savings. Empirical evaluations
demonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61%
for CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all
without compromising the generation quality. Our code and models will be
publicly available.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03339" title="Abstract">arXiv:2310.03339</a> [<a href="/pdf/2310.03339" title="Download PDF">pdf</a>, <a href="/format/2310.03339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Forecasting of Day-Ahead Electricity Prices and their  Volatility with LSTMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trebbien%2C+J">Julius Trebbien</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%BCtz%2C+S">Sebastian P&#xfc;tz</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4fer%2C+B">Benjamin Sch&#xe4;fer</a>, 
<a href="/search/cs?searchtype=author&query=Nyg%C3%A5rd%2C+H+S">Heidi S. Nyg&#xe5;rd</a>, 
<a href="/search/cs?searchtype=author&query=Gorj%C3%A3o%2C+L+R">Leonardo Rydin Gorj&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Witthaut%2C+D">Dirk Witthaut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Analysis, Statistics and Probability (physics.data-an); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Accurate forecasts of electricity prices are crucial for the management of
electric power systems and the development of smart applications. European
electricity prices have risen substantially and became highly volatile after
the Russian invasion of Ukraine, challenging established forecasting methods.
Here, we present a Long Short-Term Memory (LSTM) model for the
German-Luxembourg day-ahead electricity prices addressing these challenges. The
recurrent structure of the LSTM allows the model to adapt to trends, while the
joint prediction of both mean and standard deviation enables a probabilistic
prediction. Using a physics-inspired approach - superstatistics - to derive an
explanation for the statistics of prices, we show that the LSTM model
faithfully reproduces both prices and their volatility.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03342" title="Abstract">arXiv:2310.03342</a> [<a href="/pdf/2310.03342" title="Download PDF">pdf</a>, <a href="/format/2310.03342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LESSON: Learning to Integrate Exploration Strategies for Reinforcement  Learning via an Option Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Woojun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeonghye Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y">Youngchul Sung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, a unified framework for exploration in reinforcement learning
(RL) is proposed based on an option-critic model. The proposed framework learns
to integrate a set of diverse exploration strategies so that the agent can
adaptively select the most effective exploration strategy over time to realize
a relevant exploration-exploitation trade-off for each given task. The
effectiveness of the proposed exploration framework is demonstrated by various
experiments in the MiniGrid and Atari environments.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03344" title="Abstract">arXiv:2310.03344</a> [<a href="/pdf/2310.03344" title="Download PDF">pdf</a>, <a href="/format/2310.03344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Benders Decomposition with Continual Learning for Hybrid  Model Predictive Control in Dynamic Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xuan%2C+L">Lin Xuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Hybrid model predictive control (MPC) with both continuous and discrete
variables is widely applicable to robotic control tasks, especially those
involving contact with the environment. Due to the combinatorial complexity,
the solving speed of hybrid MPC can be insufficient for real-time applications.
In this paper, we proposed a hybrid MPC solver based on Generalized Benders
Decomposition (GBD) with continual learning. The algorithm accumulates cutting
planes from the invariant dual space of the subproblems. After a short
cold-start phase, the accumulated cuts provide warm-starts for the new problem
instances to increase the solving speed. Despite the randomly changing
environment that the control is unprepared for, the solving speed maintains. We
verified our solver on controlling a cart-pole system with randomly moving soft
contact walls and show that the solving speed is 2-3 times faster than the
off-the-shelf solver Gurobi.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03346" title="Abstract">arXiv:2310.03346</a> [<a href="/pdf/2310.03346" title="Download PDF">pdf</a>, <a href="/format/2310.03346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Datasets with Different Label Sets for Improved Nucleus  Segmentation and Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parulekar%2C+A">Amruta Parulekar</a>, 
<a href="/search/cs?searchtype=author&query=Kanwat%2C+U">Utkarsh Kanwat</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R+K">Ravi Kant Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Chippa%2C+M">Medha Chippa</a>, 
<a href="/search/cs?searchtype=author&query=Jacob%2C+T">Thomas Jacob</a>, 
<a href="/search/cs?searchtype=author&query=Bameta%2C+T">Tripti Bameta</a>, 
<a href="/search/cs?searchtype=author&query=Rane%2C+S">Swapnil Rane</a>, 
<a href="/search/cs?searchtype=author&query=Sethi%2C+A">Amit Sethi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Segmentation and classification of cell nuclei in histopathology images using
deep neural networks (DNNs) can save pathologists' time for diagnosing various
diseases, including cancers, by automating cell counting and morphometric
assessments. It is now well-known that the accuracy of DNNs increases with the
sizes of annotated datasets available for training. Although multiple datasets
of histopathology images with nuclear annotations and class labels have been
made publicly available, the set of class labels differ across these datasets.
We propose a method to train DNNs for instance segmentation and classification
on multiple datasets where the set of classes across the datasets are related
but not the same. Specifically, our method is designed to utilize a
coarse-to-fine class hierarchy, where the set of classes labeled and annotated
in a dataset can be at any level of the hierarchy, as long as the classes are
mutually exclusive. Within a dataset, the set of classes need not even be at
the same level of the class hierarchy tree. Our results demonstrate that
segmentation and classification metrics for the class set used by the test
split of a dataset can improve by pre-training on another dataset that may even
have a different set of classes due to the expansion of the training set
enabled by our method. Furthermore, generalization to previously unseen
datasets also improves by combining multiple other datasets with different sets
of classes for training. The improvement is both qualitative and quantitative.
The proposed method can be adapted for various loss functions, DNN
architectures, and application domains.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03347" title="Abstract">arXiv:2310.03347</a> [<a href="/pdf/2310.03347" title="Download PDF">pdf</a>, <a href="/format/2310.03347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Razumikhin-type ISS Lyapunov function and small gain theorem for  discrete time time-delay systems with application to a biased min-consensus  protocol
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mo%2C+Y">Yuanqiu Mo</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+W">Wenwu Yu</a>, 
<a href="/search/eess?searchtype=author&query=Hou%2C+H">Huazhou Hou</a>, 
<a href="/search/eess?searchtype=author&query=Dasgupta%2C+S">Soura Dasgupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper considers small gain theorems for the global asymptotic and
exponential input-to-state stability for discrete time time-delay systems using
Razumikhin-type Lyapunov function. Among other things, unlike the existing
literature, it provides both necessary and sufficient conditions for
exponential input-to-state stability in terms of the Razumikhin-type Lyapunov
function and the small gain theorem. Previous necessary ad sufficient
conditions were with the more computationally onerous, Krasovskii-type Lyapunov
functions. The result finds application in the robust stability analysis of a
graph-based distributed algorithm, namely, the biased min-consensus protocol,
which can be used to compute the length of the shortest path from each node to
its nearest source in a graph. We consider the biased min-consensus protocol
under perturbations that are common in communication networks, including noise,
delay and asynchronous communication. By converting such a perturbed protocol
into a discrete time time-delay nonlinear system, we prove its exponential
input-to-state stability under perturbations using our Razumikhin-type
Lyapunov-based small gain theorem. Simulations are provided to verify the
theoretical results.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03349" title="Abstract">arXiv:2310.03349</a> [<a href="/pdf/2310.03349" title="Download PDF">pdf</a>, <a href="/format/2310.03349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Integrated Algorithm for Robust and Imperceptible Audio Adversarial  Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ettenhofer%2C+A">Armin Ettenhofer</a>, 
<a href="/search/cs?searchtype=author&query=Schulze%2C+J">Jan-Philipp Schulze</a>, 
<a href="/search/cs?searchtype=author&query=Pizzi%2C+K">Karla Pizzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proc. 3rd Symposium on Security and Privacy in Speech Communication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Audio adversarial examples are audio files that have been manipulated to fool
an automatic speech recognition (ASR) system, while still sounding benign to a
human listener. Most methods to generate such samples are based on a two-step
algorithm: first, a viable adversarial audio file is produced, then, this is
fine-tuned with respect to perceptibility and robustness. In this work, we
present an integrated algorithm that uses psychoacoustic models and room
impulse responses (RIR) in the generation step. The RIRs are dynamically
created by a neural network during the generation process to simulate a
physical environment to harden our examples against transformations experienced
in over-the-air attacks. We compare the different approaches in three
experiments: in a simulated environment and in a realistic over-the-air
scenario to evaluate the robustness, and in a human study to evaluate the
perceptibility. Our algorithms considering psychoacoustics only or in addition
to the robustness show an improvement in the signal-to-noise ratio (SNR) as
well as in the human perception study, at the cost of an increased word error
rate (WER).
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03352" title="Abstract">arXiv:2310.03352</a> [<a href="/pdf/2310.03352" title="Download PDF">pdf</a>, <a href="/ps/2310.03352" title="Download PostScript">ps</a>, <a href="/format/2310.03352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tractable Bounding of Counterfactual Queries by Knowledge Compilation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huber%2C+D">David Huber</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yizuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Antonucci%2C+A">Alessandro Antonucci</a>, 
<a href="/search/cs?searchtype=author&query=Darwiche%2C+A">Adnan Darwiche</a>, 
<a href="/search/cs?searchtype=author&query=Zaffalon%2C+M">Marco Zaffalon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We discuss the problem of bounding partially identifiable queries, such as
counterfactuals, in Pearlian structural causal models. A recently proposed
iterated EM scheme yields an inner approximation of those bounds by sampling
the initialisation parameters. Such a method requires multiple (Bayesian
network) queries over models sharing the same structural equations and
topology, but different exogenous probabilities. This setup makes a compilation
of the underlying model to an arithmetic circuit advantageous, thus inducing a
sizeable inferential speed-up. We show how a single symbolic knowledge
compilation allows us to obtain the circuit structure with symbolic parameters
to be replaced by their actual values when computing the different queries. We
also discuss parallelisation techniques to further speed up the bound
computation. Experiments against standard Bayesian network inference show clear
computational advantages with up to an order of magnitude of speed-up.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03353" title="Abstract">arXiv:2310.03353</a> [<a href="/pdf/2310.03353" title="Download PDF">pdf</a>, <a href="/format/2310.03353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Geometric Learning with Monotonicity Constraints for Alzheimer&#x27;s  Disease Progression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+S">Seungwoo Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+W">Wonsik Jung</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+J">Junghyo Sohn</a>, 
<a href="/search/cs?searchtype=author&query=Suk%2C+H">Heung-Il Suk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Alzheimer's disease (AD) is a devastating neurodegenerative condition that
precedes progressive and irreversible dementia; thus, predicting its
progression over time is vital for clinical diagnosis and treatment. Numerous
studies have implemented structural magnetic resonance imaging (MRI) to model
AD progression, focusing on three integral aspects: (i) temporal variability,
(ii) incomplete observations, and (iii) temporal geometric characteristics.
However, deep learning-based approaches regarding data variability and sparsity
have yet to consider inherent geometrical properties sufficiently. The ordinary
differential equation-based geometric modeling method (ODE-RGRU) has recently
emerged as a promising strategy for modeling time-series data by intertwining a
recurrent neural network and an ODE in Riemannian space. Despite its
achievements, ODE-RGRU encounters limitations when extrapolating positive
definite symmetric metrics from incomplete samples, leading to feature reverse
occurrences that are particularly problematic, especially within the clinical
facet. Therefore, this study proposes a novel geometric learning approach that
models longitudinal MRI biomarkers and cognitive scores by combining three
modules: topological space shift, ODE-RGRU, and trajectory estimation. We have
also developed a training algorithm that integrates manifold mapping with
monotonicity constraints to reflect measurement transition irreversibility. We
verify our proposed method's efficacy by predicting clinical labels and
cognitive scores over time in regular and irregular settings. Furthermore, we
thoroughly analyze our proposed framework through an ablation study.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03354" title="Abstract">arXiv:2310.03354</a> [<a href="/pdf/2310.03354" title="Download PDF">pdf</a>, <a href="/format/2310.03354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed  Cooperative-Competitive Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zelai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yancheng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Self-play (SP) is a popular multi-agent reinforcement learning (MARL)
framework for solving competitive games, where each agent optimizes policy by
treating others as part of the environment. Despite the empirical successes,
the theoretical properties of SP-based methods are limited to two-player
zero-sum games. However, for mixed cooperative-competitive games where agents
on the same team need to cooperate with each other, we can show a simple
counter-example where SP-based methods cannot converge to a global Nash
equilibrium (NE) with high probability. Alternatively, Policy-Space Response
Oracles (PSRO) is an iterative framework for learning NE, where the best
responses w.r.t. previous policies are learned in each iteration. PSRO can be
directly extended to mixed cooperative-competitive settings by jointly learning
team best responses with all convergence properties unchanged. However, PSRO
requires repeatedly training joint policies from scratch till convergence,
which makes it hard to scale to complex games. In this work, we develop a novel
algorithm, Fictitious Cross-Play (FXP), which inherits the benefits from both
frameworks. FXP simultaneously trains an SP-based main policy and a counter
population of best response policies. The main policy is trained by fictitious
self-play and cross-play against the counter population, while the counter
policies are trained as the best responses to the main policy's past versions.
We validate our method in matrix games and show that FXP converges to global
NEs while SP methods fail. We also conduct experiments in a gridworld domain,
where FXP achieves higher Elo ratings and lower exploitabilities than
baselines, and a more challenging football game, where FXP defeats SOTA models
with over 94% win rate.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03355" title="Abstract">arXiv:2310.03355</a> [<a href="/pdf/2310.03355" title="Download PDF">pdf</a>, <a href="/ps/2310.03355" title="Download PostScript">ps</a>, <a href="/format/2310.03355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Note on the LogRank Conjecture in Communication Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grolmusz%2C+V">Vince Grolmusz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The LogRank conjecture of Lov\'asz and Saks from 1988 is the most famous open
problem in the communication complexity theory. The statement is as follows:
Suppose that two players intend to compute a Boolean function $f(x,y)$ when $x$
is known for the first and $y$ for the second player, and they may send and
receive messages encoded with bits, then they can compute $f(x,y)$ with
exchanging $(\log \rank (M_f))^c $ bits, where $M_f$ is a Boolean matrix,
determined by function $f$. The problem is widely open and very popular, and it
has resisted numerous attacks in the last 35 years. The best upper bound is
still exponential in the bound of the conjecture. Unfortunately, we cannot
prove the conjecture, but we present a communication protocol with $(\log \rank
(M_f))^c $ bits, which computes a -- somewhat -- related quantity to $f(x,y)$.
The relation is characterized by a representation of low-rank, multi-linear
polynomials modulo composite numbers. This result of ours may help to settle
this long-time open conjecture.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03358" title="Abstract">arXiv:2310.03358</a> [<a href="/pdf/2310.03358" title="Download PDF">pdf</a>, <a href="/format/2310.03358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Representation Learning via Asymmetric Negative Contrast and  Reverse Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+N">Nuoyan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Decheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Dawei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinbo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nannan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep neural networks are vulnerable to adversarial noise. Adversarial
training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two characteristics of robust
representation: (1) $\bf{exclusion}$: the feature of natural examples keeps
away from that of other classes; (2) $\bf{alignment}$: the feature of natural
and corresponding adversarial examples is close to each other. These motivate
us to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance. Code is
available at &lt;https://github.com/changzhang777/ANCRA&gt;.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03359" title="Abstract">arXiv:2310.03359</a> [<a href="/pdf/2310.03359" title="Download PDF">pdf</a>, <a href="/format/2310.03359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Optimal Trajectory Planning in Highway Scenarios using Basis-Spline  Parameterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dorpm%C3%BCller%2C+P">Philip Dorpm&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Schmitz%2C+T">Thomas Schmitz</a>, 
<a href="/search/cs?searchtype=author&query=Bejagam%2C+N">Naveen Bejagam</a>, 
<a href="/search/cs?searchtype=author&query=Bertram%2C+T">Torsten Bertram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Basis splines enable a time-continuous feasibility check with a finite number
of constraints. Constraints apply to the whole trajectory for motion planning
applications that require a collision-free and dynamically feasible trajectory.
Existing motion planners that rely on gradient-based optimization apply time
scaling to implement a shrinking planning horizon. They neither guarantee a
recursively feasible trajectory nor enable reaching two terminal manifold parts
at different time scales. This paper proposes a nonlinear optimization problem
that addresses the drawbacks of existing approaches. Therefore, the spline
breakpoints are included in the optimization variables. Transformations between
spline bases are implemented so a sparse problem formulation is achieved. A
strategy for breakpoint removal enables the convergence into a terminal
manifold. The evaluation in an overtaking scenario shows the influence of the
breakpoint number on the solution quality and the time required for
optimization.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03360" title="Abstract">arXiv:2310.03360</a> [<a href="/pdf/2310.03360" title="Download PDF">pdf</a>, <a href="/format/2310.03360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CSI: Enhancing the Robustness of 3D Point Cloud Recognition against  Corruption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhuoyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiachen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite recent advancements in deep neural networks for point cloud
recognition, real-world safety-critical applications present challenges due to
unavoidable data corruption. Current models often fall short in generalizing to
unforeseen distribution shifts. In this study, we harness the inherent set
property of point cloud data to introduce a novel critical subset
identification (CSI) method, aiming to bolster recognition robustness in the
face of data corruption. Our CSI framework integrates two pivotal components:
density-aware sampling (DAS) and self-entropy minimization (SEM), which cater
to static and dynamic CSI, respectively. DAS ensures efficient robust anchor
point sampling by factoring in local density, while SEM is employed during
training to accentuate the most salient point-to-point attention. Evaluations
reveal that our CSI approach yields error rates of 18.4\% and 16.3\% on
ModelNet40-C and PointCloud-C, respectively, marking a notable improvement over
state-of-the-art methods by margins of 5.2\% and 4.2\% on the respective
benchmarks. Code is available at
\href{https://github.com/masterwu2115/CSI/tree/main}{https://github.com/masterwu2115/CSI/tree/main}
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03362" title="Abstract">arXiv:2310.03362</a> [<a href="/pdf/2310.03362" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverter Pulse Width Modulation Control Techniques for Electric Motor  Drive Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pramod%2C+P">Prerit Pramod</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper provides a simple introduction to pulse width modulation control
techniques used for the control of power converters in the context of electric
motor drive systems. A summary of each technique is presented along with
analytical models that provide intuitive insight and enable their rapid
implementation for practical purposes.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03363" title="Abstract">arXiv:2310.03363</a> [<a href="/pdf/2310.03363" title="Download PDF">pdf</a>, <a href="/format/2310.03363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Realistic Speech-to-Face Generation with Speech-Conditioned Latent  Diffusion Model with Face Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Li Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H+V">Hei Victor Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Speech-to-face generation is an intriguing area of research that focuses on
generating realistic facial images based on a speaker's audio speech. However,
state-of-the-art methods employing GAN-based architectures lack stability and
cannot generate realistic face images. To fill this gap, we propose a novel
speech-to-face generation framework, which leverages a Speech-Conditioned
Latent Diffusion Model, called SCLDM. To the best of our knowledge, this is the
first work to harness the exceptional modeling capabilities of diffusion models
for speech-to-face generation. Preserving the shared identity information
between speech and face is crucial in generating realistic results. Therefore,
we employ contrastive pre-training for both the speech encoder and the face
encoder. This pre-training strategy facilitates effective alignment between the
attributes of speech, such as age and gender, and the corresponding facial
characteristics in the face images. Furthermore, we tackle the challenge posed
by excessive diversity in the synthesis process caused by the diffusion model.
To overcome this challenge, we introduce the concept of residuals by
integrating a statistical face prior to the diffusion process. This addition
helps to eliminate the shared component across the faces and enhances the
subtle variations captured by the speech condition. Extensive quantitative,
qualitative, and user study experiments demonstrate that our method can produce
more realistic face images while preserving the identity of the speaker better
than state-of-the-art methods. Highlighting the notable enhancements, our
method demonstrates significant gains in all metrics on the AVSpeech dataset
and Voxceleb dataset, particularly noteworthy are the improvements of 32.17 and
32.72 on the cosine distance metric for the two datasets, respectively.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03367" title="Abstract">arXiv:2310.03367</a> [<a href="/pdf/2310.03367" title="Download PDF">pdf</a>, <a href="/format/2310.03367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A paraxial approach for the inverse problem of vibro-acoustic imaging in  frequency domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rauscher%2C+T">Teresa Rauscher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">This paper presents a paraxial modeling approach for vibro-acoustography, a
high-frequency ultrasound imaging technique that makes use of the excited
low-frequency field to achieve a higher resolution while avoiding speckles. We
start from a general second order wave equation, introduce a second order
perturbation and make use of a paraxial change of variables to pass over to
directed beams. Depending on the relation between the second order perturbation
and the directivity of the beams this leads to different systems of PDEs that
involve a spatially variable parameter governing the interaction of the
incoming beams to generate the propagating acoustic field. We then consider the
inverse problem of determining this parameter for one of these models where we
present a Landweber-Kaczmarz reconstruction method to obtain a spatial image of
the region of interest.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03368" title="Abstract">arXiv:2310.03368</a> [<a href="/pdf/2310.03368" title="Download PDF">pdf</a>, <a href="/format/2310.03368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Hallucinations in Chinese Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qinyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiangyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mozhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junliang He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mianqiu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhangyue Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we establish a benchmark named HalluQA (Chinese Hallucination
Question-Answering) to measure the hallucination phenomenon in Chinese large
language models. HalluQA contains 450 meticulously designed adversarial
questions, spanning multiple domains, and takes into account Chinese historical
culture, customs, and social phenomena. During the construction of HalluQA, we
consider two types of hallucinations: imitative falsehoods and factual errors,
and we construct adversarial samples based on GLM-130B and ChatGPT. For
evaluation, we design an automated evaluation method using GPT-4 to judge
whether a model output is hallucinated. We conduct extensive experiments on 24
large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk
and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than
50%. This indicates that HalluQA is highly challenging. We analyze the primary
types of hallucinations in different types of models and their causes.
Additionally, we discuss which types of hallucinations should be prioritized
for different types of models.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03370" title="Abstract">arXiv:2310.03370</a> [<a href="/pdf/2310.03370" title="Download PDF">pdf</a>, <a href="/format/2310.03370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motivating Next-Generation OS Physical Memory Management for  Terabyte-Scale NVMMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Shivank Garg</a>, 
<a href="/search/cs?searchtype=author&query=Prasad%2C+A">Aravinda Prasad</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+D">Debadatta Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Subramoney%2C+S">Sreenivas Subramoney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 24 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>

</div>
<p class="mathjax">Software managed byte-addressable hybrid memory systems consisting of DRAMs
and NVMMs offer a lot of flexibility to design efficient large scale data
processing applications. Operating systems (OS) play an important role in
enabling the applications to realize the integrated benefits of DRAMs' low
access latency and NVMMs' large capacity along with its persistent
characteristics. In this paper, we comprehensively analyze the performance of
conventional OS physical memory management subsystems that were designed only
based on the DRAM memory characteristics in the context of modern hybrid
byte-addressable memory systems.
<br />To study the impact of high access latency and large capacity of NVMMs on
physical memory management, we perform an extensive evaluation on Linux with
Intel's Optane NVMM. We observe that the core memory management functionalities
such as page allocation are negatively impacted by high NVMM media latency,
while functionalities such as conventional fragmentation management are
rendered inadequate. We also demonstrate that certain traditional memory
management functionalities are affected by neither aspects of modern NVMMs. We
conclusively motivate the need to overhaul fundamental aspects of traditional
OS physical memory management in order to fully exploit terabyte-scale NVMMs.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03371" title="Abstract">arXiv:2310.03371</a> [<a href="/pdf/2310.03371" title="Download PDF">pdf</a>, <a href="/ps/2310.03371" title="Download PostScript">ps</a>, <a href="/format/2310.03371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Limits of Distributed Optimization over Multiple Access  Channel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Shubham Jha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We consider distributed optimization over a $d$-dimensional space, where $K$
remote clients send coded gradient estimates over an {\em additive Gaussian
Multiple Access Channel (MAC)} with noise variance $\sigma_z^2$.
<br />Furthermore, the codewords from the clients must satisfy the average power
constraint $P$, resulting in a signal-to-noise ratio (SNR) of $KP/\sigma_z^2$.
In this paper, we study the fundamental limits imposed by MAC on the
{convergence rate of any distributed optimization algorithm and design optimal
communication schemes to achieve these limits.} Our first result is a lower
bound for the convergence rate, showing that communicating over a MAC imposes a
slowdown of $\sqrt{d/\frac{1}{2}\log(1+\SNR)}$ on any protocol compared to the
centralized setting. Next, we design a computationally tractable {digital}
communication scheme that matches the lower bound to a logarithmic factor in
$K$ when combined with a projected stochastic gradient descent algorithm. At
the heart of our communication scheme is carefully combining several
compression and modulation ideas such as quantizing along random bases, {\em
Wyner-Ziv compression}, {\em modulo-lattice decoding}, and {\em amplitude shift
keying.} We also show that analog schemes, which are popular due to their ease
of implementation, can give close to optimal convergence rates at low $\SNR$
but experience a slowdown of roughly $\sqrt{d}$ at high $\SNR$.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03374" title="Abstract">arXiv:2310.03374</a> [<a href="/pdf/2310.03374" title="Download PDF">pdf</a>, <a href="/format/2310.03374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Optimizer for Planar Soft-Growing Robot Manipulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stroppa%2C+F">Fabio Stroppa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Soft-growing robots are innovative devices that feature plant-inspired growth
to navigate environments. Thanks to their embodied intelligence of adapting to
their surroundings and the latest innovation in actuation and manufacturing, it
is possible to employ them for specific manipulation tasks. The applications of
these devices include exploration of delicate/dangerous environments,
manipulation of items, or assistance in domestic environments.
<br />This work presents a novel approach for design optimization of soft-growing
robots, which will be used prior to manufacturing to suggest engineers -- or
robot designer enthusiasts -- the optimal dimension of the robot to be built
for solving a specific task. I modeled the design process as a multi-objective
optimization problem, in which I optimize the kinematic chain of a soft
manipulator to reach targets and avoid unnecessary overuse of material and
resources. The method exploits the advantages of population-based optimization
algorithms, in particular evolutionary algorithms, to transform the problem
from multi-objective into a single-objective thanks to an efficient
mathematical formulation, the novel rank-partitioning algorithm, and obstacle
avoidance integrated within the optimizer operators.
<br />I tested the proposed method on different tasks to access its optimality,
which showed significant performance in solving the problem. Finally,
comparative experiments showed that the proposed method works better than the
one existing in the literature in terms of precision, resource consumption, and
run time.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03375" title="Abstract">arXiv:2310.03375</a> [<a href="/pdf/2310.03375" title="Download PDF">pdf</a>, <a href="/format/2310.03375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point-Based Radiance Fields for Controllable Human Motion Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haitao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Deheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+P">Peiyuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">This paper proposes a novel controllable human motion synthesis method for
fine-level deformation based on static point-based radiance fields. Although
previous editable neural radiance field methods can generate impressive results
on novel-view synthesis and allow naive deformation, few algorithms can achieve
complex 3D human editing such as forward kinematics. Our method exploits the
explicit point cloud to train the static 3D scene and apply the deformation by
encoding the point cloud translation using a deformation MLP. To make sure the
rendering result is consistent with the canonical space training, we estimate
the local rotation using SVD and interpolate the per-point rotation to the
query view direction of the pre-trained radiance field. Extensive experiments
show that our approach can significantly outperform the state-of-the-art on
fine-level complex deformation which can be generalized to other 3D characters
besides humans.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03376" title="Abstract">arXiv:2310.03376</a> [<a href="/pdf/2310.03376" title="Download PDF">pdf</a>, <a href="/format/2310.03376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Procedural Text Mining with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rula%2C+A">Anisa Rula</a>, 
<a href="/search/cs?searchtype=author&query=D%27Souza%2C+J">Jennifer D&#x27;Souza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, Accepted to The Twelfth International Conference on Knowledge Capture (K-Cap 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
<p class="mathjax">Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03377" title="Abstract">arXiv:2310.03377</a> [<a href="/pdf/2310.03377" title="Download PDF">pdf</a>, <a href="/format/2310.03377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACT-Net: Anchor-context Action Detection in Surgery Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+L">Luoying Hao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wenjun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Heng Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Huazhu Fu</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jinming Duan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted early by MICCAI2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recognition and localization of surgical detailed actions is an essential
component of developing a context-aware decision support system. However, most
existing detection algorithms fail to provide high-accuracy action classes even
having their locations, as they do not consider the surgery procedure's
regularity in the whole video. This limitation hinders their application.
Moreover, implementing the predictions in clinical applications seriously needs
to convey model confidence to earn entrustment, which is unexplored in surgical
action prediction. In this paper, to accurately detect fine-grained actions
that happen at every moment, we propose an anchor-context action detection
network (ACTNet), including an anchor-context detection (ACD) module and a
class conditional diffusion (CCD) module, to answer the following questions: 1)
where the actions happen; 2) what actions are; 3) how confidence predictions
are. Specifically, the proposed ACD module spatially and temporally highlights
the regions interacting with the extracted anchor in surgery video, which
outputs action location and its class distribution based on anchor-context
interactions. Considering the full distribution of action classes in videos,
the CCD module adopts a denoising diffusion-based generative model conditioned
on our ACD estimator to further reconstruct accurately the action predictions.
Moreover, we utilize the stochastic nature of the diffusion model outputs to
access model confidence for each prediction. Our method reports the
state-of-the-art performance, with improvements of 4.0% mAP against baseline on
the surgical video dataset.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03379" title="Abstract">arXiv:2310.03379</a> [<a href="/pdf/2310.03379" title="Download PDF">pdf</a>, <a href="/format/2310.03379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive Adaptive Chance-Constrained Safeguards for Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaorun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Binhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tairan He</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+L">Liang Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengliang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Safety assurance of Reinforcement Learning (RL) is critical for exploration
in real-world scenarios. In handling the Constrained Markov Decision Process,
current approaches experience intrinsic difficulties in trading-off between
optimality and feasibility. Direct optimization methods cannot strictly
guarantee state-wise in-training safety while projection-based methods are
usually inefficient and correct actions through lengthy iterations. To address
these two challenges, this paper proposes an adaptive surrogate chance
constraint for the safety cost, and a hierarchical architecture that corrects
actions produced by the upper policy layer via a fast Quasi-Newton method.
Theoretical analysis indicates that the relaxed probabilistic constraint can
sufficiently guarantee forward invariance to the safe set. We validate the
proposed method on 4 simulated and real-world safety-critical robotic tasks.
Results indicate that the proposed method can efficiently enforce safety
(nearly zero-violation), while preserving optimality (+23.8%), robustness and
generalizability to stochastic real-world settings.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03380" title="Abstract">arXiv:2310.03380</a> [<a href="/pdf/2310.03380" title="Download PDF">pdf</a>, <a href="/format/2310.03380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StegGuard: Fingerprinting Self-supervised Pre-trained Encoders via  Secrets Embeder and Extractor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xingdong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hanzhou Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinpeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinggui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guangling Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In this work, we propose StegGuard, a novel fingerprinting mechanism to
verify the ownership of the suspect pre-trained encoder using steganography. A
critical perspective in StegGuard is that the unique characteristic of the
transformation from an image to an embedding, conducted by the pre-trained
encoder, can be equivalently exposed how an embeder embeds secrets into images
and how an extractor extracts the secrets from encoder's embeddings with a
tolerable error after the secrets are subjected to the encoder's
transformation. While each independent encoder has a distinct transformation,
the piracy encoder has a similar transformation to the victim. Based on these,
we learn a pair of secrets embeder and extractor as the fingerprint for the
victim encoder. We introduce a frequency-domain channel attention embedding
block into the embeder to adaptively embed secrets into suitable frequency
bands. During verification, if the secrets embedded into the query images can
be extracted with an acceptable error from the suspect encoder's embeddings,
the suspect encoder is determined as piracy, otherwise independent. Extensive
experiments demonstrate that depending on a very limited number of query
images, StegGuard can reliably identify across varied independent encoders, and
is robust against model stealing related attacks including model extraction,
fine-tuning, pruning, embedding noising and shuffle.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03388" title="Abstract">arXiv:2310.03388</a> [<a href="/pdf/2310.03388" title="Download PDF">pdf</a>, <a href="/format/2310.03388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rabino%2C+P">Paolo Rabino</a>, 
<a href="/search/cs?searchtype=author&query=Alliegro%2C+A">Antonio Alliegro</a>, 
<a href="/search/cs?searchtype=author&query=Borlino%2C+F+C">Francesco Cappio Borlino</a>, 
<a href="/search/cs?searchtype=author&query=Tommasi%2C+T">Tatiana Tommasi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Moving deep learning models from the laboratory setting to the open world
entails preparing them to handle unforeseen conditions. In several applications
the occurrence of novel classes during deployment poses a significant threat,
thus it is crucial to effectively detect them. Ideally, this skill should be
used when needed without requiring any further computational training effort at
every new task. Out-of-distribution detection has attracted significant
attention in the last years, however the majority of the studies deal with 2D
images ignoring the inherent 3D nature of the real-world and often confusing
between domain and semantic novelty. In this work, we focus on the latter,
considering the objects geometric structure captured by 3D point clouds
regardless of the specific domain. We advance the field by introducing
OpenPatch that builds on a large pre-trained model and simply extracts from its
intermediate features a set of patch representations that describe each known
class. For any new sample, we obtain a novelty score by evaluating whether it
can be recomposed mainly by patches of a single known class or rather via the
contribution of multiple classes. We present an extensive experimental
evaluation of our approach for the task of semantic novelty detection on
real-world point cloud samples when the reference known data are synthetic. We
demonstrate that OpenPatch excels in both the full and few-shot known sample
scenarios, showcasing its robustness across varying pre-training objectives and
network backbones. The inherent training-free nature of our method allows for
its immediate application to a wide array of real-world tasks, offering a
compelling advantage over approaches that need expensive retraining efforts.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03392" title="Abstract">arXiv:2310.03392</a> [<a href="/pdf/2310.03392" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unpacking Human-AI Interaction in Safety-Critical Industries: A  Systematic Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bach%2C+T+A">Tita A. Bach</a>, 
<a href="/search/cs?searchtype=author&query=Kristiansen%2C+J+K">Jenny K. Kristiansen</a>, 
<a href="/search/cs?searchtype=author&query=Babic%2C+A">Aleksandar Babic</a>, 
<a href="/search/cs?searchtype=author&query=Jacovi%2C+A">Alon Jacovi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review in IEEE Access, September 2023. 23 pages, 2 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Ensuring quality human-AI interaction (HAII) in safety-critical industries is
essential. Failure to do so can lead to catastrophic and deadly consequences.
Despite this urgency, what little research there is on HAII is fragmented and
inconsistent. We present here a survey of that literature and recommendations
for research best practices that will improve the field. We divided our
investigation into the following research areas: (1) terms used to describe
HAII, (2) primary roles of AI-enabled systems, (3) factors that influence HAII,
and (4) how HAII is measured. Additionally, we described the capabilities and
maturity of the AI-enabled systems used in safety-critical industries discussed
in these articles. We found that no single term is used across the literature
to describe HAII and some terms have multiple meanings. According to our
literature, five factors influence HAII: user characteristics and background
(e.g., user personality, perceptions), AI interface and features (e.g.,
interactive UI design), AI output (e.g., accuracy, actionable recommendations),
explainability and interpretability (e.g., level of detail, user
understanding), and usage of AI (e.g., heterogeneity of environments and user
needs). HAII is most commonly measured with user-related subjective metrics
(e.g., user perception, trust, and attitudes), and AI-assisted decision-making
is the most common primary role of AI-enabled systems. Based on this review, we
conclude that there are substantial research gaps in HAII. Researchers and
developers need to codify HAII terminology, involve users throughout the AI
lifecycle (especially during development), and tailor HAII in safety-critical
industries to the users and environments.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03393" title="Abstract">arXiv:2310.03393</a> [<a href="/pdf/2310.03393" title="Download PDF">pdf</a>, <a href="/ps/2310.03393" title="Download PostScript">ps</a>, <a href="/format/2310.03393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty quantification for deep learning-based schemes for solving  high-dimensional backward stochastic differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kapllani%2C+L">Lorenc Kapllani</a>, 
<a href="/search/math?searchtype=author&query=Teng%2C+L">Long Teng</a>, 
<a href="/search/math?searchtype=author&query=Rottmann%2C+M">Matthias Rottmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 23 figures and 15 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep learning-based numerical schemes for solving high-dimensional backward
stochastic differential equations (BSDEs) have recently raised plenty of
scientific interest. While they enable numerical methods to approximate very
high-dimensional BSDEs, their reliability has not been studied and is thus not
understood. In this work, we study uncertainty quantification (UQ) for a class
of deep learning-based BSDE schemes. More precisely, we review the sources of
uncertainty involved in the schemes and numerically study the impact of
different sources. Usually, the standard deviation (STD) of the approximate
solutions obtained from multiple runs of the algorithm with different datasets
is calculated to address the uncertainty. This approach is computationally
quite expensive, especially for high-dimensional problems. Hence, we develop a
UQ model that efficiently estimates the STD of the approximate solution using
only a single run of the algorithm. The model also estimates the mean of the
approximate solution, which can be leveraged to initialize the algorithm and
improve the optimization process. Our numerical experiments show that the UQ
model produces reliable estimates of the mean and STD of the approximate
solution for the considered class of deep learning-based BSDE schemes. The
estimated STD captures multiple sources of uncertainty, demonstrating its
effectiveness in quantifying the uncertainty. Additionally, the model
illustrates the improved performance when comparing different schemes based on
the estimated STD values. Furthermore, it can identify hyperparameter values
for which the scheme achieves good approximations.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03394" title="Abstract">arXiv:2310.03394</a> [<a href="/pdf/2310.03394" title="Download PDF">pdf</a>, <a href="/format/2310.03394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kinodynamic Motion Planning for a Team of Multirotors Transporting a  Cable-Suspended Payload in Cluttered Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wahba%2C+K">Khaled Wahba</a>, 
<a href="/search/cs?searchtype=author&query=Ortiz-Haro%2C+J">Joaquim Ortiz-Haro</a>, 
<a href="/search/cs?searchtype=author&query=Toussaint%2C+M">Marc Toussaint</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6nig%2C+W">Wolfgang H&#xf6;nig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We propose a motion planner for cable-driven payload transportation using
multiple unmanned aerial vehicles (UAVs) in an environment cluttered with
obstacles. Our planner is kinodynamic, i.e., it considers the full dynamics
model of the transporting system including actuation constraints. Due to the
high dimensionality of the planning problem, we use a hierarchical approach
where we first solve the geometric motion planning using a sampling-based
method with a novel sampler, followed by constrained trajectory optimization
that considers the full dynamics of the system. Both planning stages consider
inter-robot and robot/obstacle collisions. We demonstrate in a
software-in-the-loop simulation that there is a significant benefit in
kinodynamic motion planning for such payload transport systems with respect to
payload tracking error and energy consumption compared to the standard methods
of planning for the payload alone. Notably, we observe a significantly higher
success rate in scenarios where the team formation changes are needed to move
through tight spaces.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03396" title="Abstract">arXiv:2310.03396</a> [<a href="/pdf/2310.03396" title="Download PDF">pdf</a>, <a href="/format/2310.03396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Simplify Spatial-Temporal Graphs in Gait Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cosma%2C+A">Adrian Cosma</a>, 
<a href="/search/cs?searchtype=author&query=Radoi%2C+E">Emilian Radoi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 Figures, 1 Table. Short Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Gait analysis leverages unique walking patterns for person identification and
assessment across multiple domains. Among the methods used for gait analysis,
skeleton-based approaches have shown promise due to their robust and
interpretable features. However, these methods often rely on hand-crafted
spatial-temporal graphs that are based on human anatomy disregarding the
particularities of the dataset and task. This paper proposes a novel method to
simplify the spatial-temporal graph representation for gait-based gender
estimation, improving interpretability without losing performance. Our approach
employs two models, an upstream and a downstream model, that can adjust the
adjacency matrix for each walking instance, thereby removing the fixed nature
of the graph. By employing the Straight-Through Gumbel-Softmax trick, our model
is trainable end-to-end. We demonstrate the effectiveness of our approach on
the CASIA-B dataset for gait-based gender estimation. The resulting graphs are
interpretable and differ qualitatively from fixed graphs used in existing
models. Our research contributes to enhancing the explainability and
task-specific adaptability of gait recognition, promoting more efficient and
reliable gait-based biometrics.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03398" title="Abstract">arXiv:2310.03398</a> [<a href="/pdf/2310.03398" title="Download PDF">pdf</a>, <a href="/format/2310.03398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpolating between Clustering and Dimensionality Reduction with  Gromov-Wasserstein
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Assel%2C+H">Hugues Van Assel</a>, 
<a href="/search/cs?searchtype=author&query=Vincent-Cuaz%2C+C">C&#xe9;dric Vincent-Cuaz</a>, 
<a href="/search/cs?searchtype=author&query=Vayer%2C+T">Titouan Vayer</a>, 
<a href="/search/cs?searchtype=author&query=Flamary%2C+R">R&#xe9;mi Flamary</a>, 
<a href="/search/cs?searchtype=author&query=Courty%2C+N">Nicolas Courty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We present a versatile adaptation of existing dimensionality reduction (DR)
objectives, enabling the simultaneous reduction of both sample and feature
sizes. Correspondances between input and embedding samples are computed through
a semi-relaxed Gromov-Wasserstein optimal transport (OT) problem. When the
embedding sample size matches that of the input, our model recovers classical
popular DR models. When the embedding's dimensionality is unconstrained, we
show that the OT plan delivers a competitive hard clustering. We emphasize the
importance of intermediate stages that blend DR and clustering for summarizing
real data and apply our method to visualize datasets of images.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03399" title="Abstract">arXiv:2310.03399</a> [<a href="/pdf/2310.03399" title="Download PDF">pdf</a>, <a href="/format/2310.03399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Younesian%2C+T">Taraneh Younesian</a>, 
<a href="/search/cs?searchtype=author&query=Thanapalasingam%2C+T">Thiviyan Thanapalasingam</a>, 
<a href="/search/cs?searchtype=author&query=van+Krieken%2C+E">Emile van Krieken</a>, 
<a href="/search/cs?searchtype=author&query=Daza%2C+D">Daniel Daza</a>, 
<a href="/search/cs?searchtype=author&query=Bloem%2C+P">Peter Bloem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 appendix, 6 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph neural networks (GNNs) learn the representation of nodes in a graph by
aggregating the neighborhood information in various ways. As these networks
grow in depth, their receptive field grows exponentially due to the increase in
neighborhood sizes, resulting in high memory costs. Graph sampling solves
memory issues in GNNs by sampling a small ratio of the nodes in the graph. This
way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed
sampling heuristics, which may not generalize to different structures or tasks.
We introduce GRAPES, an adaptive graph sampling method that learns to identify
sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet
to learn node sampling probabilities given the classification objectives. We
evaluate GRAPES across several small- and large-scale graph benchmarks and
demonstrate its effectiveness in accuracy and scalability. In contrast to
existing sampling methods, GRAPES maintains high accuracy even with small
sample sizes and, therefore, can scale to very large graphs. Our code is
publicly available at https://github.com/dfdazac/grapes.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03400" title="Abstract">arXiv:2310.03400</a> [<a href="/pdf/2310.03400" title="Download PDF">pdf</a>, <a href="/format/2310.03400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting Large Language Models for Content Moderation: Pitfalls in Data  Engineering and Supervised Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Changqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Huazhu Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bingzhe Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Nowadays, billions of people engage in communication and express their
opinions on the internet daily. Unfortunately, not all of these expressions are
friendly or compliant, making content moderation an indispensable task. With
the successful development of Large Language Models (LLMs) in recent years,
LLM-based methods have become a feasible solution for handling tasks in various
domains. However, in the field of content moderation, there is still a lack of
detailed work that systematically introduces implementation details. In this
paper, we introduce how to fine-tune an LLM model that can be privately
deployed for content moderation. Specifically, we discuss whether incorporating
reasons during the fine-tuning process would be better or if it should be
treated as a classification task directly. We also explore the benefits of
utilizing reasons generated by more powerful LLMs for fine-tuning privately
deployed models and the impact of different processing approaches when the
answers generated by the more powerful LLMs are incorrect. We report the entire
research process and the key findings in this paper, hoping to provide valuable
experience for researchers who are fine-tuning privately deployed models in
their domain-specific research.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03401" title="Abstract">arXiv:2310.03401</a> [<a href="/pdf/2310.03401" title="Download PDF">pdf</a>, <a href="/format/2310.03401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IoTScent: Enhancing Forensic Capabilities in Internet of Things Gateways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boiano%2C+A">Antonio Boiano</a>, 
<a href="/search/cs?searchtype=author&query=Redondi%2C+A+E+C">Alessandro Enrico Cesare Redondi</a>, 
<a href="/search/cs?searchtype=author&query=Cesana%2C+M">Matteo Cesana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted at IEEE 9th World Forum on Internet of Things (WFIoT) Aveiro, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The widespread deployment of Consumer Internet of Things devices in proximity
to human activities makes them digital observers of our daily actions. This has
led to a new field of digital forensics, known as IoT Forensics, where digital
traces generated by IoT devices can serve as key evidence for forensic
investigations. Thus, there is a need to develop tools that can efficiently
acquire and store network traces from IoT ecosystems. This paper presents
IoTScent, an open-source IoT forensic tool that enables IoT gateways and Home
Automation platforms to perform IoT traffic capture and analysis. Unlike other
works focusing on IP-based protocols, IoTScent is specifically designed to
operate over IEEE 802.15.4-based traffic, which is the basis for many
IoT-specific protocols such as Zigbee, 6LoWPAN and Thread. IoTScent offers live
traffic capture and feature extraction capabilities, providing a framework for
forensic data collection that simplifies the task of setting up a data
collection pipeline, automating the data collection process, and providing
ready-made features that can be used for forensic evidence extraction. This
work provides a comprehensive description of the IoTScent tool, including a
practical use case that demonstrates the use of the tool to perform device
identification from Zigbee traffic. The study presented here significantly
contributes to the ongoing research in IoT Forensics by addressing the
challenges faced in the field and publicly releasing the IoTScent tool.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03402" title="Abstract">arXiv:2310.03402</a> [<a href="/pdf/2310.03402" title="Download PDF">pdf</a>, <a href="/format/2310.03402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Complementary Global and Local Knowledge Network for Ultrasound  denoising with Fine-grained Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bu%2C+Z">Zhenyu Bu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai-Ni Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fuxing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shengxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guang-Quan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Ultrasound imaging serves as an effective and non-invasive diagnostic tool
commonly employed in clinical examinations. However, the presence of speckle
noise in ultrasound images invariably degrades image quality, impeding the
performance of subsequent tasks, such as segmentation and classification.
Existing methods for speckle noise reduction frequently induce excessive image
smoothing or fail to preserve detailed information adequately. In this paper,
we propose a complementary global and local knowledge network for ultrasound
denoising with fine-grained refinement. Initially, the proposed architecture
employs the L-CSwinTransformer as encoder to capture global information,
incorporating CNN as decoder to fuse local features. We expand the resolution
of the feature at different stages to extract more global information compared
to the original CSwinTransformer. Subsequently, we integrate Fine-grained
Refinement Block (FRB) within the skip-connection stage to further augment
features. We validate our model on two public datasets, HC18 and BUSI.
Experimental results demonstrate that our model can achieve competitive
performance in both quantitative metrics and visual performance. Our code will
be available at https://github.com/AAlkaid/USDenoising.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03404" title="Abstract">arXiv:2310.03404</a> [<a href="/pdf/2310.03404" title="Download PDF">pdf</a>, <a href="/format/2310.03404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EAG-RS: A Novel Explainability-guided ROI-Selection Framework for ASD  Diagnosis via Inter-regional Relation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+W">Wonsik Jung</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+E">Eunjin Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+E">Eunsong Kang</a>, 
<a href="/search/cs?searchtype=author&query=Suk%2C+H">Heung-Il Suk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep learning models based on resting-state functional magnetic resonance
imaging (rs-fMRI) have been widely used to diagnose brain diseases,
particularly autism spectrum disorder (ASD). Existing studies have leveraged
the functional connectivity (FC) of rs-fMRI, achieving notable classification
performance. However, they have significant limitations, including the lack of
adequate information while using linear low-order FC as inputs to the model,
not considering individual characteristics (i.e., different symptoms or varying
stages of severity) among patients with ASD, and the non-explainability of the
decision process. To cover these limitations, we propose a novel
explainability-guided region of interest (ROI) selection (EAG-RS) framework
that identifies non-linear high-order functional associations among brain
regions by leveraging an explainable artificial intelligence technique and
selects class-discriminative regions for brain disease identification. The
proposed framework includes three steps: (i) inter-regional relation learning
to estimate non-linear relations through random seed-based network masking,
(ii) explainable connection-wise relevance score estimation to explore
high-order relations between functional connections, and (iii) non-linear
high-order FC-based diagnosis-informative ROI selection and classifier learning
to identify ASD. We validated the effectiveness of our proposed method by
conducting experiments using the Autism Brain Imaging Database Exchange (ABIDE)
dataset, demonstrating that the proposed method outperforms other comparative
methods in terms of various evaluation metrics. Furthermore, we qualitatively
analyzed the selected ROIs and identified ASD subtypes linked to previous
neuroscientific studies.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03406" title="Abstract">arXiv:2310.03406</a> [<a href="/pdf/2310.03406" title="Download PDF">pdf</a>, <a href="/format/2310.03406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RUSOpt: Robotic UltraSound Probe Normalization with Bayesian  Optimization for In-plane and Out-plane Scanning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raina%2C+D">Deepak Raina</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+A">Abhishek Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Voyles%2C+R+M">Richard M. Voyles</a>, 
<a href="/search/cs?searchtype=author&query=Wachs%2C+J">Juan Wachs</a>, 
<a href="/search/cs?searchtype=author&query=Chandrashekhara%2C+S">SH Chandrashekhara</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S+K">Subir Kumar Saha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE International Conference on Automation Science and Engineering (CASE) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The one of the significant challenges faced by autonomous robotic ultrasound
systems is acquiring high-quality images across different patients. The proper
orientation of the robotized probe plays a crucial role in governing the
quality of ultrasound images. To address this challenge, we propose a
sample-efficient method to automatically adjust the orientation of the
ultrasound probe normal to the point of contact on the scanning surface,
thereby improving the acoustic coupling of the probe and resulting image
quality. Our method utilizes Bayesian Optimization (BO) based search on the
scanning surface to efficiently search for the normalized probe orientation. We
formulate a novel objective function for BO that leverages the contact force
measurements and underlying mechanics to identify the normal. We further
incorporate a regularization scheme in BO to handle the noisy objective
function. The performance of the proposed strategy has been assessed through
experiments on urinary bladder phantoms. These phantoms included planar,
tilted, and rough surfaces, and were examined using both linear and convex
probes with varying search space limits. Further, simulation-based studies have
been carried out using 3D human mesh models. The results demonstrate that the
mean ($\pm$SD) absolute angular error averaged over all phantoms and 3D models
is $\boldsymbol{2.4\pm0.7^\circ}$ and $\boldsymbol{2.1\pm1.3^\circ}$,
respectively.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03410" title="Abstract">arXiv:2310.03410</a> [<a href="/pdf/2310.03410" title="Download PDF">pdf</a>, <a href="/format/2310.03410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Over-the-Air Federated Learning with Compressed Sensing: Is  Sparsification Necessary?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edin%2C+A">Adrian Edin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 8 figures, submitted for possible conference publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Over-the-Air (OtA) Federated Learning (FL) refers to an FL system where
multiple agents apply OtA computation for transmitting model updates to a
common edge server. Two important features of OtA computation, namely linear
processing and signal-level superposition, motivate the use of linear
compression with compressed sensing (CS) methods to reduce the number of data
samples transmitted over the channel. The previous works on applying CS methods
in OtA FL have primarily assumed that the original model update vectors are
sparse, or they have been sparsified before compression. However, it is unclear
whether linear compression with CS-based reconstruction is more effective than
directly sending the non-zero elements in the sparsified update vectors, under
the same total power constraint. In this study, we examine and compare several
communication designs with or without sparsification. Our findings demonstrate
that sparsification before compression is not necessary. Alternatively,
sparsification without linear compression can also achieve better performance
than the commonly considered setup that combines both.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03414" title="Abstract">arXiv:2310.03414</a> [<a href="/pdf/2310.03414" title="Download PDF">pdf</a>, <a href="/format/2310.03414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Based Multi-Document Summarization Exploiting Main-Event Biased  Monotone Submodular Content Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurisinkel%2C+L+J">Litton J Kurisinkel</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N+F">Nancy F. Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multi-document summarization is a challenging task due to its inherent
subjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4
among DUC-2004 reference summaries. In this work, we aim to enhance the
objectivity of news summarization by focusing on the main event of a group of
related news documents and presenting it coherently with sufficient context.
Our primary objective is to succinctly report the main event, ensuring that the
summary remains objective and informative. To achieve this, we employ an
extract-rewrite approach that incorporates a main-event biased
monotone-submodular function for content selection. This enables us to extract
the most crucial information related to the main event from the document
cluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for
rewriting the extracted content into a coherent text. The evaluation using
objective metrics and human evaluators confirms the effectiveness of our
approach, as it surpasses potential baselines, demonstrating excellence in both
content coverage, coherence, and informativeness.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03419" title="Abstract">arXiv:2310.03419</a> [<a href="/pdf/2310.03419" title="Download PDF">pdf</a>, <a href="/format/2310.03419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-Training and Fine-Tuning Generative Flow Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+M">Moksh Jain</a>, 
<a href="/search/cs?searchtype=author&query=Madan%2C+K">Kanika Madan</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative Flow Networks (GFlowNets) are amortized samplers that learn
stochastic policies to sequentially generate compositional objects from a given
unnormalized reward distribution. They can generate diverse sets of high-reward
objects, which is an important consideration in scientific discovery tasks.
However, as they are typically trained from a given extrinsic reward function,
it remains an important open challenge about how to leverage the power of
pre-training and train GFlowNets in an unsupervised fashion for efficient
adaptation to downstream tasks. Inspired by recent successes of unsupervised
pre-training in various domains, we introduce a novel approach for reward-free
pre-training of GFlowNets. By framing the training as a self-supervised
problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to
explore the candidate space. Specifically, OC-GFN learns to reach any targeted
outcomes, akin to goal-conditioned policies in reinforcement learning. We show
that the pre-trained OC-GFN model can allow for a direct extraction of a policy
capable of sampling from any new reward functions in downstream tasks.
Nonetheless, adapting OC-GFN on a downstream task-specific reward involves an
intractable marginalization over possible outcomes. We propose a novel way to
approximate this marginalization by learning an amortized predictor enabling
efficient fine-tuning. Extensive experimental results validate the efficacy of
our approach, demonstrating the effectiveness of pre-training the OC-GFN, and
its ability to swiftly adapt to downstream tasks and discover modes more
efficiently. This work may serve as a foundation for further exploration of
pre-training strategies in the context of GFlowNets.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03420" title="Abstract">arXiv:2310.03420</a> [<a href="/pdf/2310.03420" title="Download PDF">pdf</a>, <a href="/format/2310.03420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained  Diffusion Models and Monocular Depth Estimators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yujing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bisheng Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://whu-usi3dv.github.io/FreeReg/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Matching cross-modality features between images and point clouds is a
fundamental problem for image-to-point cloud registration. However, due to the
modality difference between images and points, it is difficult to learn robust
and discriminative cross-modality features by existing metric learning methods
for feature matching. Instead of applying metric learning on cross-modality
data, we propose to unify the modality between images and point clouds by
pretrained large-scale models first, and then establish robust correspondence
within the same modality. We show that the intermediate features, called
diffusion features, extracted by depth-to-image diffusion models are
semantically consistent between images and point clouds, which enables the
building of coarse but robust cross-modality correspondences. We further
extract geometric features on depth maps produced by the monocular depth
estimator. By matching such geometric features, we significantly improve the
accuracy of the coarse correspondences produced by diffusion features.
Extensive experiments demonstrate that without any task-specific training,
direct utilization of both features produces accurate image-to-point cloud
registration. On three public indoor and outdoor benchmarks, the proposed
method averagely achieves a 20.6 percent improvement in Inlier Ratio, a
three-fold higher Inlier Number, and a 48.6 percent improvement in Registration
Recall than existing state-of-the-arts.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03424" title="Abstract">arXiv:2310.03424</a> [<a href="/pdf/2310.03424" title="Download PDF">pdf</a>, <a href="/format/2310.03424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Language Model Pruning for Automatic Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emili%2C+L">Leonardo Emili</a>, 
<a href="/search/cs?searchtype=author&query=Fraga-Silva%2C+T">Thiago Fraga-Silva</a>, 
<a href="/search/cs?searchtype=author&query=Pusateri%2C+E">Ernest Pusateri</a>, 
<a href="/search/cs?searchtype=author&query=Nu%C3%9Fbaum-Thom%2C+M">Markus Nu&#xdf;baum-Thom</a>, 
<a href="/search/cs?searchtype=author&query=Oualil%2C+Y">Youssef Oualil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We study model pruning methods applied to Transformer-based neural network
language models for automatic speech recognition. We explore three aspects of
the pruning frame work, namely criterion, method and scheduler, analyzing their
contribution in terms of accuracy and inference speed. To the best of our
knowledge, such in-depth analyses on large-scale recognition systems has not
been reported in the literature. In addition, we propose a variant of low-rank
approximation suitable for incrementally compressing models, and delivering
multiple models with varied target sizes. Among other results, we show that a)
data-driven pruning outperforms magnitude-driven in several scenarios; b)
incremental pruning achieves higher accuracy compared to one-shot pruning,
especially when targeting smaller sizes; and c) low-rank approximation presents
the best trade-off between size reduction and inference speed-up for moderate
compression.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03431" title="Abstract">arXiv:2310.03431</a> [<a href="/pdf/2310.03431" title="Download PDF">pdf</a>, <a href="/format/2310.03431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on  Double Covering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+F">Fei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuhui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wencheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Hong Qin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Ying He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to ACM Transactions on Graphics (SIGGRAPH Asia 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose a new method, called DoubleCoverUDF, for extracting
the zero level-set from unsigned distance fields (UDFs). DoubleCoverUDF takes a
learned UDF and a user-specified parameter $r$ (a small positive real number)
as input and extracts an iso-surface with an iso-value $r$ using the
conventional marching cubes algorithm. We show that the computed iso-surface is
the boundary of the $r$-offset volume of the target zero level-set $S$, which
is an orientable manifold, regardless of the topology of $S$. Next, the
algorithm computes a covering map to project the boundary mesh onto $S$,
preserving the mesh's topology and avoiding folding. If $S$ is an orientable
manifold surface, our algorithm separates the double-layered mesh into a single
layer using a robust minimum-cut post-processing step. Otherwise, it keeps the
double-layered mesh as the output. We validate our algorithm by reconstructing
3D surfaces of open models and demonstrate its efficacy and effectiveness on
synthetic models and benchmark datasets. Our experimental results confirm that
our method is robust and produces meshes with better quality in terms of both
visual evaluation and quantitative measures than existing UDF-based methods.
The source code is available at https://github.com/jjjkkyz/DCUDF.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03432" title="Abstract">arXiv:2310.03432</a> [<a href="/pdf/2310.03432" title="Download PDF">pdf</a>, <a href="/format/2310.03432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating the Influence of Domain Shift in Skin Lesion Classification:  A Benchmark Study of Unsupervised Domain Adaptation Methods on Dermoscopic  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chamarthi%2C+S">Sireesha Chamarthi</a>, 
<a href="/search/cs?searchtype=author&query=Fogelberg%2C+K">Katharina Fogelberg</a>, 
<a href="/search/cs?searchtype=author&query=Maron%2C+R+C">Roman C. Maron</a>, 
<a href="/search/cs?searchtype=author&query=Brinker%2C+T+J">Titus J. Brinker</a>, 
<a href="/search/cs?searchtype=author&query=Niebling%2C+J">Julia Niebling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The potential of deep neural networks in skin lesion classification has
already been demonstrated to be on-par if not superior to the dermatologists
diagnosis. However, the performance of these models usually deteriorates when
the test data differs significantly from the training data (i.e. domain shift).
This concerning limitation for models intended to be used in real-world skin
lesion classification tasks poses a risk to patients. For example, different
image acquisition systems or previously unseen anatomical sites on the patient
can suffice to cause such domain shifts. Mitigating the negative effect of such
shifts is therefore crucial, but developing effective methods to address domain
shift has proven to be challenging. In this study, we carry out an in-depth
analysis of eight different unsupervised domain adaptation methods to analyze
their effectiveness in improving generalization for dermoscopic datasets. To
ensure robustness of our findings, we test each method on a total of ten
distinct datasets, thereby covering a variety of possible domain shifts. In
addition, we investigated which factors in the domain shifted datasets have an
impact on the effectiveness of domain adaptation methods. Our findings show
that all of the eight domain adaptation methods result in improved AUPRC for
the majority of analyzed datasets. Altogether, these results indicate that
unsupervised domain adaptations generally lead to performance improvements for
the binary melanoma-nevus classification task regardless of the nature of the
domain shift. However, small or heavily imbalanced datasets lead to a reduced
conformity of the results due to the influence of these factors on the methods
performance.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03441" title="Abstract">arXiv:2310.03441</a> [<a href="/pdf/2310.03441" title="Download PDF">pdf</a>, <a href="/ps/2310.03441" title="Download PostScript">ps</a>, <a href="/format/2310.03441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equalizer zero-determinant strategy in discounted repeated Stackelberg  asymmetric game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhaoyang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanpu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yiguang Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">This paper focuses on the performance of equalizer zero-determinant (ZD)
strategies in discounted repeated Stackerberg asymmetric games. In the
leader-follower adversarial scenario, the strong Stackelberg equilibrium (SSE)
deriving from the opponents' best response (BR), is technically the optimal
strategy for the leader. However, computing an SSE strategy may be difficult
since it needs to solve a mixed-integer program and has exponential complexity
in the number of states. To this end, we propose to adopt an equalizer ZD
strategy, which can unilaterally restrict the opponent's expected utility. We
first study the existence of an equalizer ZD strategy with one-to-one
situations, and analyze an upper bound of its performance with the baseline SSE
strategy. Then we turn to multi-player models, where there exists one player
adopting an equalizer ZD strategy. We give bounds of the sum of opponents'
utilities, and compare it with the SSE strategy. Finally, we give simulations
on unmanned aerial vehicles (UAVs) and the moving target defense (MTD) to
verify the effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03443" title="Abstract">arXiv:2310.03443</a> [<a href="/pdf/2310.03443" title="Download PDF">pdf</a>, <a href="/ps/2310.03443" title="Download PostScript">ps</a>, <a href="/format/2310.03443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The North System for Formosa Speech Recognition Challenge 2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li-Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+K">Kai-Chen Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-Shin Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This report provides a concise overview of the proposed North system, which
aims to achieve automatic word/syllable recognition for Taiwanese Hakka
(Sixian). The report outlines three key components of the system: the
acquisition, composition, and utilization of the training data; the
architecture of the model; and the hardware specifications and operational
statistics. The demonstration of the system can be found at
https://asrvm.iis.sinica.edu.tw/hakka_sixian.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03447" title="Abstract">arXiv:2310.03447</a> [<a href="/pdf/2310.03447" title="Download PDF">pdf</a>, <a href="/format/2310.03447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLAIM: AIM-based Synthetic Data Generation in the Federated Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maddock%2C+S">Samuel Maddock</a>, 
<a href="/search/cs?searchtype=author&query=Cormode%2C+G">Graham Cormode</a>, 
<a href="/search/cs?searchtype=author&query=Maple%2C+C">Carsten Maple</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Preserving individual privacy while enabling collaborative data sharing is
crucial for organizations. Synthetic data generation is one solution, producing
artificial data that mirrors the statistical properties of private data. While
numerous techniques have been devised under differential privacy, they
predominantly assume data is centralized. However, data is often distributed
across multiple clients in a federated manner. In this work, we initiate the
study of federated synthetic tabular data generation. Building upon a SOTA
central method known as AIM, we present DistAIM and FLAIM. We show it is
straightforward to distribute AIM, extending a recent approach based on secure
multi-party computation which necessitates additional overhead, making it less
suited to federated scenarios. We then demonstrate that naively federating AIM
can lead to substantial degradation in utility under the presence of
heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach
that maintains a private proxy of heterogeneity. We simulate our methods across
a range of benchmark datasets under different degrees of heterogeneity and show
this can improve utility while reducing overhead.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03456" title="Abstract">arXiv:2310.03456</a> [<a href="/pdf/2310.03456" title="Download PDF">pdf</a>, <a href="/format/2310.03456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Resolution Audio-Visual Feature Fusion for Temporal Action  Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fish%2C+E">Edward Fish</a>, 
<a href="/search/cs?searchtype=author&query=Weinbren%2C+J">Jon Weinbren</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+A">Andrew Gilbert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Temporal Action Localization (TAL) aims to identify actions' start, end, and
class labels in untrimmed videos. While recent advancements using transformer
networks and Feature Pyramid Networks (FPN) have enhanced visual feature
recognition in TAL tasks, less progress has been made in the integration of
audio features into such frameworks. This paper introduces the Multi-Resolution
Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge
audio-visual data across different temporal resolutions. Central to our
approach is a hierarchical gated cross-attention mechanism, which discerningly
weighs the importance of audio information at diverse temporal scales. Such a
technique not only refines the precision of regression boundaries but also
bolsters classification confidence. Importantly, MRAV-FF is versatile, making
it compatible with existing FPN TAL architectures and offering a significant
enhancement in performance when audio data is available.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03457" title="Abstract">arXiv:2310.03457</a> [<a href="/pdf/2310.03457" title="Download PDF">pdf</a>, <a href="/format/2310.03457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quantitatively Interpretable Model for Alzheimer&#x27;s Disease Prediction  Using Deep Counterfactuals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+K">Kwanseok Oh</a>, 
<a href="/search/cs?searchtype=author&query=Heo%2C+D">Da-Woon Heo</a>, 
<a href="/search/cs?searchtype=author&query=Mulyadi%2C+A+W">Ahmad Wisnu Mulyadi</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+W">Wonsik Jung</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+E">Eunsong Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K+H">Kun Ho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Suk%2C+H">Heung-Il Suk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Deep learning (DL) for predicting Alzheimer's disease (AD) has provided
timely intervention in disease progression yet still demands attentive
interpretability to explain how their DL models make definitive decisions.
Recently, counterfactual reasoning has gained increasing attention in medical
research because of its ability to provide a refined visual explanatory map.
However, such visual explanatory maps based on visual inspection alone are
insufficient unless we intuitively demonstrate their medical or neuroscientific
validity via quantitative features. In this study, we synthesize the
counterfactual-labeled structural MRIs using our proposed framework and
transform it into a gray matter density map to measure its volumetric changes
over the parcellated region of interest (ROI). We also devised a lightweight
linear classifier to boost the effectiveness of constructed ROIs, promoted
quantitative interpretation, and achieved comparable predictive performance to
DL methods. Throughout this, our framework produces an ``AD-relatedness index''
for each ROI and offers an intuitive understanding of brain status for an
individual patient and across patient groups with respect to AD progression.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03458" title="Abstract">arXiv:2310.03458</a> [<a href="/pdf/2310.03458" title="Download PDF">pdf</a>, <a href="/format/2310.03458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Attitudes to Content Moderation in Web Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Urman%2C+A">Aleksandra Urman</a>, 
<a href="/search/cs?searchtype=author&query=Hannak%2C+A">Aniko Hannak</a>, 
<a href="/search/cs?searchtype=author&query=Makhortykh%2C+M">Mykola Makhortykh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Internet users highly rely on and trust web search engines, such as Google,
to find relevant information online. However, scholars have documented numerous
biases and inaccuracies in search outputs. To improve the quality of search
results, search engines employ various content moderation practices such as
interface elements informing users about potentially dangerous websites and
algorithmic mechanisms for downgrading or removing low-quality search results.
While the reliance of the public on web search engines and their use of
moderation practices is well-established, user attitudes towards these
practices have not yet been explored in detail. To address this gap, we first
conducted an overview of content moderation practices used by search engines,
and then surveyed a representative sample of the US adult population (N=398) to
examine the levels of support for different moderation practices applied to
potentially misleading and/or potentially offensive content in web search. We
also analyzed the relationship between user characteristics and their support
for specific moderation practices. We find that the most supported practice is
informing users about potentially misleading or offensive content, and the
least supported one is the complete removal of search results. More
conservative users and users with lower levels of trust in web search results
are more likely to be against content moderation in web search.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03461" title="Abstract">arXiv:2310.03461</a> [<a href="/pdf/2310.03461" title="Download PDF">pdf</a>, <a href="/format/2310.03461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Which mode is better for federated learning? Centralized or  Decentralized
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Both centralized and decentralized approaches have shown excellent
performance and great application value in federated learning (FL). However,
current studies do not provide sufficient evidence to show which one performs
better. Although from the optimization perspective, decentralized methods can
approach the comparable convergence of centralized methods with less
communication, its test performance has always been inefficient in empirical
studies. To comprehensively explore their behaviors in FL, we study their
excess risks, including the joint analysis of both optimization and
generalization. We prove that on smooth non-convex objectives, 1) centralized
FL (CFL) always generalizes better than decentralized FL (DFL); 2) from
perspectives of the excess risk and test error in CFL, adopting partial
participation is superior to full participation; and, 3) there is a necessary
requirement for the topology in DFL to avoid performance collapse as the
training scale increases. Based on some simple hardware metrics, we could
evaluate which framework is better in practice. Extensive experiments are
conducted on common setups in FL to validate that our theoretical analysis is
contextually valid in practical scenarios.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03466" title="Abstract">arXiv:2310.03466</a> [<a href="/pdf/2310.03466" title="Download PDF">pdf</a>, <a href="/format/2310.03466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Blame Problem in Evaluating Local Explanations, and How to Tackle it
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahnama%2C+A+H+A">Amir Hossein Akhavan Rahnama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Workshop: XAI methods, challenges and applications, 26th European Conference on Artificial Intelligence (ECAI 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The number of local model-agnostic explanation techniques proposed has grown
rapidly recently. One main reason is that the bar for developing new
explainability techniques is low due to the lack of optimal evaluation
measures. Without rigorous measures, it is hard to have concrete evidence of
whether the new explanation techniques can significantly outperform their
predecessors. Our study proposes a new taxonomy for evaluating local
explanations: robustness, evaluation using ground truth from synthetic datasets
and interpretable models, model randomization, and human-grounded evaluation.
Using this proposed taxonomy, we highlight that all categories of evaluation
methods, except those based on the ground truth from interpretable models,
suffer from a problem we call the "blame problem." In our study, we argue that
this category of evaluation measure is a more reasonable method for evaluating
local model-agnostic explanations. However, we show that even this category of
evaluation measures has further limitations. The evaluation of local
explanations remains an open research problem.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03469" title="Abstract">arXiv:2310.03469</a> [<a href="/pdf/2310.03469" title="Download PDF">pdf</a>, <a href="/format/2310.03469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPT Approximations for Packing and Covering Problems Parameterized by  Elimination Distance and Even Less
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Inamdar%2C+T">Tanmay Inamdar</a>, 
<a href="/search/cs?searchtype=author&query=Kanesh%2C+L">Lawqueen Kanesh</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+M">Madhumita Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Ramanujan%2C+M+S">M. S. Ramanujan</a>, 
<a href="/search/cs?searchtype=author&query=Saurabh%2C+S">Saket Saurabh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version of FSTTCS 2023 paper. Abstract shortened to meet the character limit
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">For numerous graph problems in the realm of parameterized algorithms, using
the size of a smallest deletion set (called a modulator) into well-understood
graph families as parameterization has led to a long and successful line of
research. Recently, however, there has been an extensive study of structural
parameters that are potentially much smaller than the modulator size. In
particular, recent papers [Jansen et al. STOC 2021; Agrawal et al. SODA 2022]
have studied parameterization by the size of the modulator to a graph family
$\mathcal{H}$ ($\textbf{mod}_{\mathcal{H}}$), elimination distance to
$\mathcal{H}$ ($\textbf{ed}_{\mathcal{H}}$), and $\mathcal{H}$-treewidth
($\textbf{tw}_{\mathcal{H}}$). While these new parameters have been
successfully exploited to design fast exact algorithms their utility
(especially that of latter two) in the context of approximation algorithms is
mostly unexplored.
<br />The conceptual contribution of this paper is to present novel algorithmic
meta-theorems that expand the impact of these structural parameters to the area
of FPT Approximation, mirroring their utility in the design of exact FPT
algorithms. Precisely, we show that if a covering or packing problem is
definable in Monadic Second Order Logic and has a property called Finite
Integer Index, then the existence of an FPT Approximation Scheme (FPT-AS, i.e.,
($1\pm \epsilon$)-approximation) parameterized these three parameters is in
fact equivalent. As concrete exemplifications of our meta-theorems, we obtain
FPT-ASes for well-studied graph problems such as Vertex Cover, Feedback Vertex
Set, Cycle Packing and Dominating Set, parameterized by these three parameters.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03470" title="Abstract">arXiv:2310.03470</a> [<a href="/pdf/2310.03470" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyber Physical System Information Collection: Robot Location and  Navigation Method Based on QR Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+T">Tao Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper, we propose a method to estimate the exact location of a camera
in a cyber-physical system using the exact geographic coordinates of four
feature points stored in QR codes(Quick response codes) and the pixel
coordinates of four feature points analyzed from the QR code images taken by
the camera. Firstly, the P4P(Perspective 4 Points) algorithm is designed to
uniquely determine the initial pose estimation value of the QR coordinate
system relative to the camera coordinate system by using the four feature
points of the selected QR code. In the second step, the manifold gradient
optimization algorithm is designed. The rotation matrix and displacement vector
are taken as the initial values of iteration, and the iterative optimization is
carried out to improve the positioning accuracy and obtain the rotation matrix
and displacement vector with higher accuracy. The third step is to convert the
pose of the QR coordinate system with respect to the camera coordinate system
to the pose of the AGV(Automated Guided Vehicle) with respect to the world
coordinate system. Finally, the performance of manifold gradient optimization
algorithm and P4P analytical algorithm are simulated and compared under the
same conditions.One can see that the performance of the manifold gradient
optimization algorithm proposed in this paper is much better than that of the
P4P analytic algorithm when the signal-to-noise ratio is small.With the
increase of the signal-to-noise ratio,the performance of the P4P analytic
algorithm approaches that of the manifold gradient optimization algorithm.when
the noise is same,the performance of manifold gradient optimization algorithm
is better when there are more feature points.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03472" title="Abstract">arXiv:2310.03472</a> [<a href="/pdf/2310.03472" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ammonia-Net: A Multi-task Joint Learning Model for Multi-class  Segmentation and Classification in Tooth-marked Tongue Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shunkai Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qihui Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yiming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+M">Muhammad Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Melliou%2C+A">Aikaterini Melliou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dongmei Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In Traditional Chinese Medicine, the tooth marks on the tongue, stemming from
prolonged dental pressure, serve as a crucial indicator for assessing qi (yang)
deficiency, which is intrinsically linked to visceral health. Manual diagnosis
of tooth-marked tongue solely relies on experience. Nonetheless, the diversity
in shape, color, and type of tooth marks poses a challenge to diagnostic
accuracy and consistency. To address these problems, herein we propose a
multi-task joint learning model named Ammonia-Net. This model employs a
convolutional neural network-based architecture, specifically designed for
multi-class segmentation and classification of tongue images. Ammonia-Net
performs semantic segmentation of tongue images to identify tongue and tooth
marks. With the assistance of segmentation output, it classifies the images
into the desired number of classes: healthy tongue, light tongue, moderate
tongue, and severe tongue. As far as we know, this is the first attempt to
apply the semantic segmentation results of tooth marks for tooth-marked tongue
classification. To train Ammonia-Net, we collect 856 tongue images from 856
subjects. After a number of extensive experiments, the experimental results
show that the proposed model achieves 99.06% accuracy in the two-class
classification task of tooth-marked tongue identification and 80.02%. As for
the segmentation task, mIoU for tongue and tooth marks amounts to 71.65%.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03473" title="Abstract">arXiv:2310.03473</a> [<a href="/pdf/2310.03473" title="Download PDF">pdf</a>, <a href="/format/2310.03473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controllable Multi-document Summarization: Coverage &amp; Coherence  Intuitive Policy with Large Language Model Based Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurisinkel%2C+L+J">Litton J Kurisinkel</a>, 
<a href="/search/cs?searchtype=author&query=chen%2C+N+F">Nancy F chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Memory-efficient large language models are good at refining text input for
better readability. However, controllability is a matter of concern when it
comes to text generation tasks with long inputs, such as multi-document
summarization. In this work, we investigate for a generic controllable approach
for multi-document summarization that leverages the capabilities of LLMs to
refine the text. In particular, we train a controllable content extraction
scheme to extract the text that will be refined by an LLM. The scheme is
designed with a novel coverage and coherence intuitive policy, which is duly
rewarded by a passively trained LLM. Our approach yields competitive results in
the evaluation using ROUGE metrics and outperforms potential baselines in
coherence, as per human evaluation.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03475" title="Abstract">arXiv:2310.03475</a> [<a href="/pdf/2310.03475" title="Download PDF">pdf</a>, <a href="/ps/2310.03475" title="Download PostScript">ps</a>, <a href="/format/2310.03475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Division with Allocator&#x27;s Preference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bu%2C+X">Xiaolin Bu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiaxin Song</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+B">Biaoshuai Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears in the 19th Conference on Web and Internet Economics (WINE), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We consider the fair allocation problem of indivisible items. Most previous
work focuses on fairness and/or efficiency among agents given agents'
preferences. However, besides the agents, the allocator as the resource owner
may also be involved in many real-world scenarios, e.g., heritage division. The
allocator has the inclination to obtain a fair or efficient allocation based on
her own preference over the items and to whom each item is allocated. In this
paper, we propose a new model and focus on the following two problems: 1) Is it
possible to find an allocation that is fair for both the agents and the
allocator? 2) What is the complexity of maximizing the allocator's social
welfare while satisfying the agents' fairness?
<br />We consider the two fundamental fairness criteria: envy-freeness and
proportionality. For the first problem, we study the existence of an allocation
that is envy-free up to $c$ goods (EF-$c$) or proportional up to $c$ goods
(PROP-$c$) from both the agents' and the allocator's perspectives, in which
such an allocation is called doubly EF-$c$ or doubly PROP-$c$ respectively.
When the allocator's utility depends exclusively on the items (but not to whom
an item is allocated), we prove that a doubly EF-$1$ allocation always exists.
For the general setting where the allocator has a preference over the items and
to whom each item is allocated, we prove that a doubly EF-$1$ allocation always
exists for two agents, a doubly PROP-$2$ allocation always exists for binary
valuations, and a doubly PROP-$O(\log n)$ allocation always exists in general.
<br />For the second problem, we provide various (in)approximability results in
which the gaps between approximation and inapproximation ratios are
asymptotically closed under most settings.
<br />Most results are based on novel technical tools including the chromatic
numbers of the Kneser graphs and linear programming-based analysis.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03477" title="Abstract">arXiv:2310.03477</a> [<a href="/pdf/2310.03477" title="Download PDF">pdf</a>, <a href="/format/2310.03477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tik-to-Tok: Translating Language Models One Token at a Time: An  Embedding Initialization Strategy for Efficient Language Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Remy%2C+F">Fran&#xe7;ois Remy</a>, 
<a href="/search/cs?searchtype=author&query=Delobelle%2C+P">Pieter Delobelle</a>, 
<a href="/search/cs?searchtype=author&query=Berendt%2C+B">Bettina Berendt</a>, 
<a href="/search/cs?searchtype=author&query=Demuynck%2C+K">Kris Demuynck</a>, 
<a href="/search/cs?searchtype=author&query=Demeester%2C+T">Thomas Demeester</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> As first reviewed at TACL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Training monolingual language models for low and mid-resource languages is
made challenging by limited and often inadequate pretraining data. In this
study, we propose a novel model conversion strategy to address this issue,
adapting high-resources monolingual language models to a new target language.
By generalizing over a word translation dictionary encompassing both the source
and target languages, we map tokens from the target tokenizer to semantically
similar tokens from the source language tokenizer. This one-to-many token
mapping improves tremendously the initialization of the embedding table for the
target language. We conduct experiments to convert high-resource models to mid-
and low-resource languages, namely Dutch and Frisian. These converted models
achieve a new state-of-the-art performance on these languages across all sorts
of downstream tasks. By reducing significantly the amount of data and time
required for training state-of-the-art models, our novel model conversion
strategy has the potential to benefit many languages worldwide.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03478" title="Abstract">arXiv:2310.03478</a> [<a href="/pdf/2310.03478" title="Download PDF">pdf</a>, <a href="/format/2310.03478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RGBManip: Monocular Image-based Robotic Manipulation through Active  Object Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+B">Boshi An</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yiran Geng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Q">Qi Dou</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robotic manipulation requires accurate perception of the environment, which
poses a significant challenge due to its inherent complexity and constantly
changing nature. In this context, RGB image and point-cloud observations are
two commonly used modalities in visual-based robotic manipulation, but each of
these modalities have their own limitations. Commercial point-cloud
observations often suffer from issues like sparse sampling and noisy output due
to the limits of the emission-reception imaging principle. On the other hand,
RGB images, while rich in texture information, lack essential depth and 3D
information crucial for robotic manipulation. To mitigate these challenges, we
propose an image-only robotic manipulation framework that leverages an
eye-on-hand monocular camera installed on the robot's parallel gripper. By
moving with the robot gripper, this camera gains the ability to actively
perceive object from multiple perspectives during the manipulation process.
This enables the estimation of 6D object poses, which can be utilized for
manipulation. While, obtaining images from more and diverse viewpoints
typically improves pose estimation, it also increases the manipulation time. To
address this trade-off, we employ a reinforcement learning policy to
synchronize the manipulation strategy with active perception, achieving a
balance between 6D pose accuracy and manipulation efficiency. Our experimental
results in both simulated and real-world environments showcase the
state-of-the-art effectiveness of our approach. %, which, to the best of our
knowledge, is the first to achieve robust real-world robotic manipulation
through active pose estimation. We believe that our method will inspire further
research on real-world-oriented robotic manipulation.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03481" title="Abstract">arXiv:2310.03481</a> [<a href="/pdf/2310.03481" title="Download PDF">pdf</a>, <a href="/format/2310.03481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Transformer-based Ranking for e-Commerce at Yandex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khrylchenko%2C+K">Kirill Khrylchenko</a>, 
<a href="/search/cs?searchtype=author&query=Fritzler%2C+A">Alexander Fritzler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Personalizing the user experience with high-quality recommendations based on
user activities is vital for e-commerce platforms. This is particularly
important in scenarios where the user's intent is not explicit, such as on the
homepage. Recently, personalized embedding-based systems have significantly
improved the quality of recommendations and search results in the e-commerce
domain. However, most of these works focus on enhancing the retrieval stage.
<br />In this paper, we demonstrate that features produced by retrieval-focused
deep learning models are sub-optimal for ranking stage in e-commerce
recommendations. To address this issue, we propose a two-stage training process
that fine-tunes two-tower models to achieve optimal ranking performance. We
provide a detailed description of our transformer-based two-tower model
architecture, which is specifically designed for personalization in e-commerce.
<br />Additionally, we introduce a novel technique for debiasing context in offline
models and report significant improvements in ranking performance when using
web-search queries for e-commerce recommendations. Our model has been
successfully deployed at Yandex and has delivered strong performance in online
A/B testing.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03482" title="Abstract">arXiv:2310.03482</a> [<a href="/pdf/2310.03482" title="Download PDF">pdf</a>, <a href="/format/2310.03482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Geometric Structure of Fully-Connected ReLU-Layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vallin%2C+J">Jonatan Vallin</a>, 
<a href="/search/cs?searchtype=author&query=Larsson%2C+K">Karl Larsson</a>, 
<a href="/search/cs?searchtype=author&query=Larson%2C+M+G">Mats G. Larson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We formalize and interpret the geometric structure of $d$-dimensional fully
connected ReLU-layers in neural networks. The parameters of a ReLU-layer induce
a natural partition of the input domain, such that in each sector of the
partition, the ReLU-layer can be greatly simplified. This leads to a geometric
interpretation of a ReLU-layer as a projection onto a polyhedral cone followed
by an affine transformation, in line with the description in
[doi:10.48550/arXiv.<a href="/abs/1905.08922">1905.08922</a>] for convolutional networks with ReLU
activations. Further, this structure facilitates simplified expressions for
preimages of the intersection between partition sectors and hyperplanes, which
is useful when describing decision boundaries in a classification setting. We
investigate this in detail for a feed-forward network with one hidden
ReLU-layer, where we provide results on the geometric complexity of the
decision boundary generated by such networks, as well as proving that modulo an
affine transformation, such a network can only generate $d$ different decision
boundaries. Finally, the effect of adding more layers to the network is
discussed.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03491" title="Abstract">arXiv:2310.03491</a> [<a href="/pdf/2310.03491" title="Download PDF">pdf</a>, <a href="/format/2310.03491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TPDR: A Novel Two-Step Transformer-based Product and Class Description  Match and Retrieval Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cunha%2C+W">Washington Cunha</a>, 
<a href="/search/cs?searchtype=author&query=Fran%C3%A7a%2C+C">Celso Fran&#xe7;a</a>, 
<a href="/search/cs?searchtype=author&query=Rocha%2C+L">Leonardo Rocha</a>, 
<a href="/search/cs?searchtype=author&query=Gon%C3%A7alves%2C+M+A">Marcos Andr&#xe9; Gon&#xe7;alves</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">There is a niche of companies responsible for intermediating the purchase of
large batches of varied products for other companies, for which the main
challenge is to perform product description standardization, i.e., matching an
item described by a client with a product described in a catalog. The problem
is complex since the client's product description may be: (1) potentially
noisy; (2) short and uninformative (e.g., missing information about model and
size); and (3) cross-language. In this paper, we formalize this problem as a
ranking task: given an initial client product specification (query), return the
most appropriate standardized descriptions (response). In this paper, we
propose TPDR, a two-step Transformer-based Product and Class Description
Retrieval method that is able to explore the semantic correspondence between IS
and SD, by exploiting attention mechanisms and contrastive learning. First,
TPDR employs the transformers as two encoders sharing the embedding vector
space: one for encoding the IS and another for the SD, in which corresponding
pairs (IS, SD) must be close in the vector space. Closeness is further enforced
by a contrastive learning mechanism leveraging a specialized loss function.
TPDR also exploits a (second) re-ranking step based on syntactic features that
are very important for the exact matching (model, dimension) of certain
products that may have been neglected by the transformers. To evaluate our
proposal, we consider 11 datasets from a real company, covering different
application contexts. Our solution was able to retrieve the correct
standardized product before the 5th ranking position in 71% of the cases and
its correct category in the first position in 80% of the situations. Moreover,
the effectiveness gains over purely syntactic or semantic baselines reach up to
3.7 times, solving cases that none of the approaches in isolation can do by
themselves.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03494" title="Abstract">arXiv:2310.03494</a> [<a href="/pdf/2310.03494" title="Download PDF">pdf</a>, <a href="/format/2310.03494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How the level sampling process impacts zero-shot generalisation in deep  reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcin%2C+S">Samuel Garcin</a>, 
<a href="/search/cs?searchtype=author&query=Doran%2C+J">James Doran</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shangmin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+C+G">Christopher G. Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Currently under review, 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A key limitation preventing the wider adoption of autonomous agents trained
via deep reinforcement learning (RL) is their limited ability to generalise to
new environments, even when these share similar characteristics with
environments encountered during training. In this work, we investigate how a
non-uniform sampling strategy of individual environment instances, or levels,
affects the zero-shot generalisation (ZSG) ability of RL agents, considering
two failure modes: overfitting and over-generalisation. As a first step, we
measure the mutual information (MI) between the agent's internal representation
and the set of training levels, which we find to be well-correlated to instance
overfitting. In contrast to uniform sampling, adaptive sampling strategies
prioritising levels based on their value loss are more effective at maintaining
lower MI, which provides a novel theoretical justification for this class of
techniques. We then turn our attention to unsupervised environment design (UED)
methods, which adaptively generate new training levels and minimise MI more
effectively than methods sampling from a fixed set. However, we find UED
methods significantly shift the training distribution, resulting in
over-generalisation and worse ZSG performance over the distribution of
interest. To prevent both instance overfitting and over-generalisation, we
introduce self-supervised environment design (SSED). SSED generates levels
using a variational autoencoder, effectively reducing MI while minimising the
shift with the distribution of interest, and leads to statistically significant
improvements in ZSG over fixed-set level sampling strategies and UED methods.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03498" title="Abstract">arXiv:2310.03498</a> [<a href="/pdf/2310.03498" title="Download PDF">pdf</a>, <a href="/format/2310.03498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Anatomy of Deception: Technical and Human Perspectives on a  Large-scale Phishing Campaign
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chrysanthou%2C+A">Anargyros Chrysanthou</a>, 
<a href="/search/cs?searchtype=author&query=Pantis%2C+Y">Yorgos Pantis</a>, 
<a href="/search/cs?searchtype=author&query=Patsakis%2C+C">Constantinos Patsakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In an era dominated by digital interactions, phishing campaigns have evolved
to exploit not just technological vulnerabilities but also human traits. This
study takes an unprecedented deep dive into large-scale phishing campaigns
aimed at Meta's users, offering a dual perspective on the technical mechanics
and human elements involved. Analysing data from over 25,000 victims worldwide,
we highlight the nuances of these campaigns, from the intricate techniques
deployed by the attackers to the sentiments and behaviours of those who were
targeted. Unlike prior research conducted in controlled environments, this
investigation capitalises on the vast, diverse, and genuine data extracted
directly from active phishing campaigns, allowing for a more holistic
understanding of the drivers, facilitators, and human factors. Through the
application of advanced computational techniques, including natural language
processing and machine learning, this work unveils critical insights into the
psyche of victims and the evolving tactics of modern phishers. Our analysis
illustrates very poor password selection choices from the victims but also
persistence in the revictimisation of a significant part of the users. Finally,
we reveal many correlations regarding demographics, timing, sentiment, emotion,
and tone of the victims' responses.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03500" title="Abstract">arXiv:2310.03500</a> [<a href="/pdf/2310.03500" title="Download PDF">pdf</a>, <a href="/format/2310.03500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Generative Models of Music Expectation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masclef%2C+N+L">Ninon Liz&#xe9; Masclef</a>, 
<a href="/search/cs?searchtype=author&query=Keller%2C+T+A">T. Anderson Keller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">A prominent theory of affective response to music revolves around the
concepts of surprisal and expectation. In prior work, this idea has been
operationalized in the form of probabilistic models of music which allow for
precise computation of song (or note-by-note) probabilities, conditioned on a
'training set' of prior musical or cultural experiences. To date, however,
these models have been limited to compute exact probabilities through
hand-crafted features or restricted to linear models which are likely not
sufficient to represent the complex conditional distributions present in music.
In this work, we propose to use modern deep probabilistic generative models in
the form of a Diffusion Model to compute an approximate likelihood of a musical
input sequence. Unlike prior work, such a generative model parameterized by
deep neural networks is able to learn complex non-linear features directly from
a training set itself. In doing so, we expect to find that such models are able
to more accurately represent the 'surprisal' of music for human listeners. From
the literature, it is known that there is an inverted U-shaped relationship
between surprisal and the amount human subjects 'like' a given song. In this
work we show that pre-trained diffusion models indeed yield musical surprisal
values which exhibit a negative quadratic relationship with measured subject
'liking' ratings, and that the quality of this relationship is competitive with
state of the art methods such as IDyOM. We therefore present this model a
preliminary step in developing modern deep generative models of music
expectation and subjective likability.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03501" title="Abstract">arXiv:2310.03501</a> [<a href="/pdf/2310.03501" title="Download PDF">pdf</a>, <a href="/format/2310.03501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing Digital Voting Systems for Citizens: Achieving Fairness and  Legitimacy in Digital Participatory Budgeting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J+C">Joshua C. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hausladen%2C+C+I">Carina I. Hausladen</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+D">Dominik Peters</a>, 
<a href="/search/cs?searchtype=author&query=Pournaras%2C+E">Evangelos Pournaras</a>, 
<a href="/search/cs?searchtype=author&query=Fricker%2C+R+H">Regula Haenggli Fricker</a>, 
<a href="/search/cs?searchtype=author&query=Helbing%2C+D">Dirk Helbing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACM Digital Government: Research and Practice
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA); General Economics (econ.GN)

</div>
<p class="mathjax">Digital Participatory Budgeting (PB) has become a key democratic tool for
resource allocation in cities. Enabled by digital platforms, new voting input
formats and aggregation have been utilised. Yet, challenges in achieving
fairness and legitimacy persist. This study investigates the trade-offs in
various voting and aggregation methods within digital PB. Through behavioural
experiments, we identified favourable voting design combinations in terms of
cognitive load, proportionality, and perceived legitimacy. The research reveals
how design choices profoundly influence collective decision-making, citizen
perceptions, and outcome fairness. Our findings offer actionable insights for
human-computer interaction, mechanism design, and computational social choice,
contributing to the development of fairer and more transparent digital PB
systems and multi-winner collective decision-making process for citizens.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03502" title="Abstract">arXiv:2310.03502</a> [<a href="/pdf/2310.03502" title="Download PDF">pdf</a>, <a href="/format/2310.03502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and  Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Razzhigaev%2C+A">Anton Razzhigaev</a>, 
<a href="/search/cs?searchtype=author&query=Shakhmatov%2C+A">Arseniy Shakhmatov</a>, 
<a href="/search/cs?searchtype=author&query=Maltseva%2C+A">Anastasia Maltseva</a>, 
<a href="/search/cs?searchtype=author&query=Arkhipkin%2C+V">Vladimir Arkhipkin</a>, 
<a href="/search/cs?searchtype=author&query=Pavlov%2C+I">Igor Pavlov</a>, 
<a href="/search/cs?searchtype=author&query=Ryabov%2C+I">Ilya Ryabov</a>, 
<a href="/search/cs?searchtype=author&query=Kuts%2C+A">Angelina Kuts</a>, 
<a href="/search/cs?searchtype=author&query=Panchenko%2C+A">Alexander Panchenko</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+A">Andrey Kuznetsov</a>, 
<a href="/search/cs?searchtype=author&query=Dimitrov%2C+D">Denis Dimitrov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-to-image generation is a significant domain in modern computer vision
and has achieved substantial improvements through the evolution of generative
architectures. Among these, there are diffusion-based models that have
demonstrated essential quality enhancements. These models are generally split
into two categories: pixel-level and latent-level approaches. We present
Kandinsky1, a novel exploration of latent diffusion architecture, combining the
principles of the image prior models with latent diffusion techniques. The
image prior model is trained separately to map text embeddings to image
embeddings of CLIP. Another distinct feature of the proposed model is the
modified MoVQ implementation, which serves as the image autoencoder component.
Overall, the designed model contains 3.3B parameters. We also deployed a
user-friendly demo system that supports diverse generative modes such as
text-to-image generation, image fusion, text and image fusion, image variations
generation, and text-guided inpainting/outpainting. Additionally, we released
the source code and checkpoints for the Kandinsky models. Experimental
evaluations demonstrate a FID score of 8.03 on the COCO-30K dataset, marking
our model as the top open-source performer in terms of measurable image
generation quality.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03505" title="Abstract">arXiv:2310.03505</a> [<a href="/pdf/2310.03505" title="Download PDF">pdf</a>, <a href="/format/2310.03505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RadaRays: Real-time Simulation of Rotating FMCW Radar for Mobile  Robotics via Hardware-accelerated Ray Tracing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mock%2C+A">Alexander Mock</a>, 
<a href="/search/cs?searchtype=author&query=Magnusson%2C+M">Martin Magnusson</a>, 
<a href="/search/cs?searchtype=author&query=Hertzberg%2C+J">Joachim Hertzberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">RadaRays allows for the accurate modeling and simulation of rotating FMCW
radar sensors in complex environments, including the simulation of reflection,
refraction, and scattering of radar waves. Our software is able to handle large
numbers of objects and materials, making it suitable for use in a variety of
mobile robotics applications. We demonstrate the effectiveness of RadaRays
through a series of experiments and show that it can more accurately reproduce
the behavior of FMCW radar sensors in a variety of environments, compared to
the ray casting-based lidar-like simulations that are commonly used in
simulators for autonomous driving such as CARLA. Our experiments additionally
serve as valuable reference point for researchers to evaluate their own radar
simulations. By using RadaRays, developers can significantly reduce the time
and cost associated with prototyping and testing FMCW radar-based algorithms.
We also provide a Gazebo plugin that makes our work accessible to the mobile
robotics community.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03507" title="Abstract">arXiv:2310.03507</a> [<a href="/pdf/2310.03507" title="Download PDF">pdf</a>, <a href="/format/2310.03507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RL-based Stateful Neural Adaptive Sampling and Denoising for Real-Time  Path Tracing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scardigli%2C+A">Antoine Scardigli</a>, 
<a href="/search/cs?searchtype=author&query=Cavigelli%2C+L">Lukas Cavigelli</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+L+K">Lorenz K. M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to NeurIPS. <a href="https://openreview.net/forum?id=xNyR7DXUzJ">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Multimedia (cs.MM)

</div>
<p class="mathjax">Monte-Carlo path tracing is a powerful technique for realistic image
synthesis but suffers from high levels of noise at low sample counts, limiting
its use in real-time applications. To address this, we propose a framework with
end-to-end training of a sampling importance network, a latent space encoder
network, and a denoiser network. Our approach uses reinforcement learning to
optimize the sampling importance network, thus avoiding explicit numerically
approximated gradients. Our method does not aggregate the sampled values per
pixel by averaging but keeps all sampled values which are then fed into the
latent space encoder. The encoder replaces handcrafted spatiotemporal
heuristics by learned representations in a latent space. Finally, a neural
denoiser is trained to refine the output image. Our approach increases visual
quality on several challenging datasets and reduces rendering times for equal
quality by a factor of 1.6x compared to the previous state-of-the-art, making
it a promising solution for real-time applications.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03510" title="Abstract">arXiv:2310.03510</a> [<a href="/pdf/2310.03510" title="Download PDF">pdf</a>, <a href="/format/2310.03510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervising Smart Home Device Interactions: A Profile-Based Firewall  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Keersmaeker%2C+F">Fran&#xe7;ois De Keersmaeker</a>, 
<a href="/search/cs?searchtype=author&query=Sadre%2C+R">Ramin Sadre</a>, 
<a href="/search/cs?searchtype=author&query=Pelsser%2C+C">Cristel Pelsser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Internet of Things devices can now be found everywhere, including in our
households in the form of Smart Home networks. Despite their ubiquity, their
security is unsatisfactory, as demonstrated by recent attacks.
<br />The IETF's MUD standard has as goal to simplify and automate the secure
deployment of end devices in networks. A MUD file contains a device specific
description of allowed network activities (e.g., allowed IP ports or host
addresses) and can be used to configure for example a firewall. A major
weakness of MUD is that it is not expressive enough to describe traffic
patterns representing device interactions, which often occur in modern Smart
Home platforms.
<br />In this article, we present a new language for describing such traffic
patterns. The language allows writing device profiles that are more expressive
than MUD files and take into account the interdependencies of traffic
connections. We show how these profiles can be translated to efficient code for
a lightweight firewall leveraging NFTables to block non-conforming traffic. We
evaluate our approach on traffic generated by various Smart Home devices, and
show that our system can accurately block unwanted traffic while inducing
negligible latency.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03512" title="Abstract">arXiv:2310.03512</a> [<a href="/pdf/2310.03512" title="Download PDF">pdf</a>, <a href="/format/2310.03512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Otago Exercises Monitoring for Older Adults by a Single IMU and  Hierarchical Machine Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shang%2C+M">Meng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Dedeyne%2C+L">Lenore Dedeyne</a>, 
<a href="/search/cs?searchtype=author&query=Dupont%2C+J">Jolan Dupont</a>, 
<a href="/search/cs?searchtype=author&query=Vercauteren%2C+L">Laura Vercauteren</a>, 
<a href="/search/cs?searchtype=author&query=Amini%2C+N">Nadjia Amini</a>, 
<a href="/search/cs?searchtype=author&query=Lapauw%2C+L">Laurence Lapauw</a>, 
<a href="/search/cs?searchtype=author&query=Gielen%2C+E">Evelien Gielen</a>, 
<a href="/search/cs?searchtype=author&query=Verschueren%2C+S">Sabine Verschueren</a>, 
<a href="/search/cs?searchtype=author&query=Varon%2C+C">Carolina Varon</a>, 
<a href="/search/cs?searchtype=author&query=De+Raedt%2C+W">Walter De Raedt</a>, 
<a href="/search/cs?searchtype=author&query=Vanrumste%2C+B">Bart Vanrumste</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Otago Exercise Program (OEP) is a rehabilitation program for older adults to
improve frailty, sarcopenia, and balance. Accurate monitoring of patient
involvement in OEP is challenging, as self-reports (diaries) are often
unreliable. With the development of wearable sensors, Human Activity
Recognition (HAR) systems using wearable sensors have revolutionized
healthcare. However, their usage for OEP still shows limited performance. The
objective of this study is to build an unobtrusive and accurate system to
monitor OEP for older adults. Data was collected from older adults wearing a
single waist-mounted Inertial Measurement Unit (IMU). Two datasets were
collected, one in a laboratory setting, and one at the homes of the patients. A
hierarchical system is proposed with two stages: 1) using a deep learning model
to recognize whether the patients are performing OEP or activities of daily
life (ADLs) using a 10-minute sliding window; 2) based on stage 1, using a
6-second sliding window to recognize the OEP sub-classes performed. The results
showed that in stage 1, OEP could be recognized with window-wise f1-scores over
0.95 and Intersection-over-Union (IoU) f1-scores over 0.85 for both datasets.
In stage 2, for the home scenario, four activities could be recognized with
f1-scores over 0.8: ankle plantarflexors, abdominal muscles, knee bends, and
sit-to-stand. The results showed the potential of monitoring the compliance of
OEP using a single IMU in daily life. Also, some OEP sub-classes are possible
to be recognized for further analysis.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03513" title="Abstract">arXiv:2310.03513</a> [<a href="/pdf/2310.03513" title="Download PDF">pdf</a>, <a href="/format/2310.03513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring DINO: Emergent Properties and Limitations for Synthetic  Aperture Radar Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gallego-Mejia%2C+J+A">Joseph A. Gallego-Mejia</a>, 
<a href="/search/cs?searchtype=author&query=Jungbluth%2C+A">Anna Jungbluth</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Ferrer%2C+L">Laura Mart&#xed;nez-Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Allen%2C+M">Matt Allen</a>, 
<a href="/search/cs?searchtype=author&query=Dorr%2C+F">Francisco Dorr</a>, 
<a href="/search/cs?searchtype=author&query=Kalaitzis%2C+F">Freddie Kalaitzis</a>, 
<a href="/search/cs?searchtype=author&query=Ramos-Poll%C3%A1n%2C+R">Ra&#xfa;l Ramos-Poll&#xe1;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised learning (SSL) models have recently demonstrated remarkable
performance across various tasks, including image segmentation. This study
delves into the emergent characteristics of the Self-Distillation with No
Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR)
imagery. We pre-train a vision transformer (ViT)-based DINO model using
unlabeled SAR data, and later fine-tune the model to predict high-resolution
land cover maps. We rigorously evaluate the utility of attention maps generated
by the ViT backbone, and compare them with the model's token embedding space.
We observe a small improvement in model performance with pre-training compared
to training from scratch, and discuss the limitations and opportunities of SSL
for remote sensing and land cover segmentation. Beyond small performance
increases, we show that ViT attention maps hold great intrinsic value for
remote sensing, and could provide useful inputs to other algorithms. With this,
our work lays the ground-work for bigger and better SSL models for Earth
Observation.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03515" title="Abstract">arXiv:2310.03515</a> [<a href="/pdf/2310.03515" title="Download PDF">pdf</a>, <a href="/format/2310.03515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-dimensional Bayesian Optimization with Group Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hellsten%2C+E+O">Erik Orm Hellsten</a>, 
<a href="/search/cs?searchtype=author&query=Hvarfner%2C+C">Carl Hvarfner</a>, 
<a href="/search/cs?searchtype=author&query=Papenmeier%2C+L">Leonard Papenmeier</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+L">Luigi Nardi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Bayesian optimization is an effective method for optimizing
expensive-to-evaluate black-box functions. High-dimensional problems are
particularly challenging as the surrogate model of the objective suffers from
the curse of dimensionality, which makes accurate modeling difficult. We
propose a group testing approach to identify active variables to facilitate
efficient optimization in these domains. The proposed algorithm, Group Testing
Bayesian Optimization (GTBO), first runs a testing phase where groups of
variables are systematically selected and tested on whether they influence the
objective. To that end, we extend the well-established theory of group testing
to functions of continuous ranges. In the second phase, GTBO guides
optimization by placing more importance on the active dimensions. By exploiting
the axis-aligned subspace assumption, GTBO is competitive against
state-of-the-art methods on several synthetic and real-world high-dimensional
optimization tasks. Furthermore, GTBO aids in the discovery of active
parameters in applications, thereby enhancing practitioners' understanding of
the problem at hand.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03517" title="Abstract">arXiv:2310.03517</a> [<a href="/pdf/2310.03517" title="Download PDF">pdf</a>, <a href="/format/2310.03517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PrototypeFormer: Learning to Explore Prototype Relationships for  Few-shot Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+F">Feihong He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gang Li</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+L">Lingyu Si</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Leilei Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fanzhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fuchun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Few-shot image classification has received considerable attention for
addressing the challenge of poor classification performance with limited
samples in novel classes. However, numerous studies have employed sophisticated
learning strategies and diversified feature extraction methods to address this
issue. In this paper, we propose our method called PrototypeFormer, which aims
to significantly advance traditional few-shot image classification approaches
by exploring prototype relationships. Specifically, we utilize a transformer
architecture to build a prototype extraction module, aiming to extract class
representations that are more discriminative for few-shot classification.
Additionally, during the model training process, we propose a contrastive
learning-based optimization approach to optimize prototype features in few-shot
learning scenarios. Despite its simplicity, the method performs remarkably
well, with no bells and whistles. We have experimented with our approach on
several popular few-shot image classification benchmark datasets, which shows
that our method outperforms all current state-of-the-art methods. In
particular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way
1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with
accuracy of 7.27% and 8.72%, respectively. The code will be released later.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03518" title="Abstract">arXiv:2310.03518</a> [<a href="/pdf/2310.03518" title="Download PDF">pdf</a>, <a href="/format/2310.03518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust and Generalizable Training: An Empirical Study of Noisy  Slot Filling for Input Perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiachi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaoshuai Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zechen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+S">Shanglin Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinzheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+B">Bo Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In real dialogue scenarios, as there are unknown input noises in the
utterances, existing supervised slot filling models often perform poorly in
practical applications. Even though there are some studies on noise-robust
models, these works are only evaluated on rule-based synthetic datasets, which
is limiting, making it difficult to promote the research of noise-robust
methods. In this paper, we introduce a noise robustness evaluation dataset
named Noise-SF for slot filling task. The proposed dataset contains five types
of human-annotated noise, and all those noises are exactly existed in real
extensive robust-training methods of slot filling into the proposed framework.
By conducting exhaustive empirical evaluation experiments on Noise-SF, we find
that baseline models have poor performance in robustness evaluation, and the
proposed framework can effectively improve the robustness of models. Based on
the empirical experimental results, we make some forward-looking suggestions to
fuel the research in this direction. Our dataset Noise-SF will be released at
https://github.com/dongguanting/Noise-SF.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03522" title="Abstract">arXiv:2310.03522</a> [<a href="/pdf/2310.03522" title="Download PDF">pdf</a>, <a href="/format/2310.03522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Putting a Padlock on Lambda -- Integrating vTPMs into AWS Firecracker
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veltman%2C+M">Melker Veltman</a>, 
<a href="/search/cs?searchtype=author&query=Parkegren%2C+A">Alexandra Parkegren</a>, 
<a href="/search/cs?searchtype=author&query=Morel%2C+V">Victor Morel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at ITCCN-2023@TRUSTCOM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR); Computers and Society (cs.CY)

</div>
<p class="mathjax">When software services use cloud providers to run their workloads, they place
implicit trust in the cloud provider, without an explicit trust relationship.
One way to achieve such explicit trust in a computer system is to use a
hardware Trusted Platform Module (TPM), a coprocessor for trusted computing.
However, in the case of managed platform-as-a-service (PaaS) offerings, there
is currently no cloud provider that exposes TPM capabilities. In this paper, we
improve trust by integrating a virtual TPM device into the Firecracker
hypervisor, originally developed by Amazon Web Services. In addition to this,
multiple performance tests along with an attack surface analysis are performed
to evaluate the impact of the changes introduced. We discuss the results and
conclude that the slight performance decrease and attack surface increase are
acceptable trade-offs in order to enable trusted computing in PaaS offerings.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03525" title="Abstract">arXiv:2310.03525</a> [<a href="/pdf/2310.03525" title="Download PDF">pdf</a>, <a href="/format/2310.03525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> V2X Cooperative Perception for Autonomous Driving: Recent Advances and  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+C">Dinh C. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Azghadi%2C+M+R">Mostafa Rahimi Azghadi</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yuxuan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Q">Qing-Long Han</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Sumei Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Accurate perception is essential for advancing autonomous driving and
addressing safety challenges in modern transportation systems. Despite
significant advancements in computer vision for object recognition, current
perception methods still face difficulties in complex real-world traffic
environments. Challenges such as physical occlusion and limited sensor field of
view persist for individual vehicle systems. Cooperative Perception (CP) with
Vehicle-to-Everything (V2X) technologies has emerged as a solution to overcome
these obstacles and enhance driving automation systems. While some research has
explored CP's fundamental architecture and critical components, there remains a
lack of comprehensive summaries of the latest innovations, particularly in the
context of V2X communication technologies. To address this gap, this paper
provides a comprehensive overview of the evolution of CP technologies, spanning
from early explorations to recent developments, including advancements in V2X
communication technologies. Additionally, a contemporary generic framework is
proposed to illustrate the V2X-based CP workflow, aiding in the structured
understanding of CP system components. Furthermore, this paper categorizes
prevailing V2X-based CP methodologies based on the critical issues they
address. An extensive literature review is conducted within this taxonomy,
evaluating existing datasets and simulators. Finally, open challenges and
future directions in CP for autonomous driving are discussed by considering
both perception and V2X communication advancements.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03528" title="Abstract">arXiv:2310.03528</a> [<a href="/pdf/2310.03528" title="Download PDF">pdf</a>, <a href="/ps/2310.03528" title="Download PostScript">ps</a>, <a href="/format/2310.03528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best-Response Dynamics in Tullock Contests with Convex Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+A">Abheek Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages. arXiv admin note: text overlap with <a href="/abs/2305.10881">arXiv:2305.10881</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
<p class="mathjax">We study the convergence of best-response dynamics in Tullock contests with
convex cost functions (these games always have a unique pure-strategy Nash
equilibrium). We show that best-response dynamics rapidly converges to the
equilibrium for homogeneous agents. For two homogeneous agents, we show
convergence to an $\epsilon$-approximate equilibrium in
$\Theta(\log\log(1/\epsilon))$ steps. For $n \ge 3$ agents, the dynamics is not
unique because at each step $n-1 \ge 2$ agents can make non-trivial moves. We
consider the model proposed by \cite{ghosh2023best}, where the agent making the
move is randomly selected at each time step. We show convergence to an
$\epsilon$-approximate equilibrium in $O(\beta \log(n/(\epsilon\delta)))$ steps
with probability $1-\delta$, where $\beta$ is a parameter of the agent
selection process, e.g., $\beta = n^2 \log(n)$ if agents are selected uniformly
at random at each time step. We complement this result with a lower bound of
$\Omega(n + \log(1/\epsilon)/\log(n))$ applicable for any agent selection
process.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03529" title="Abstract">arXiv:2310.03529</a> [<a href="/pdf/2310.03529" title="Download PDF">pdf</a>, <a href="/format/2310.03529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality  of Formal Deep Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sonoda%2C+S">Sho Sonoda</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+Y">Yuka Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Ishikawa%2C+I">Isao Ishikawa</a>, 
<a href="/search/cs?searchtype=author&query=Ikeda%2C+M">Masahiro Ikeda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We identify hidden layers inside a DNN with group actions on the data space,
and formulate the DNN as a dual voice transform with respect to Koopman
operator, a linear representation of the group action. Based on the group
theoretic arguments, particularly by using Schur's lemma, we show a simple
proof of the universality of those DNNs.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03530" title="Abstract">arXiv:2310.03530</a> [<a href="/pdf/2310.03530" title="Download PDF">pdf</a>, <a href="/format/2310.03530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Group Invariant Functions on Data-Parameter Domain Induce  Universal Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sonoda%2C+S">Sho Sonoda</a>, 
<a href="/search/cs?searchtype=author&query=Ishi%2C+H">Hideyuki Ishi</a>, 
<a href="/search/cs?searchtype=author&query=Ishikawa%2C+I">Isao Ishikawa</a>, 
<a href="/search/cs?searchtype=author&query=Ikeda%2C+M">Masahiro Ikeda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The symmetry and geometry of input data are considered to be encoded in the
internal data representation inside the neural network, but the specific
encoding rule has been less investigated. By focusing on a joint group
invariant function on the data-parameter domain, we present a systematic rule
to find a dual group action on the parameter domain from a group action on the
data domain. Further, we introduce generalized neural networks induced from the
joint invariant functions, and present a new group theoretic proof of their
universality theorems by using Schur's lemma. Since traditional universality
theorems were demonstrated based on functional analytical methods, this study
sheds light on the group theoretic aspect of the approximation theory,
connecting geometric deep learning to abstract harmonic analysis.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03533" title="Abstract">arXiv:2310.03533</a> [<a href="/pdf/2310.03533" title="Download PDF">pdf</a>, <a href="/format/2310.03533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Software Engineering: Survey and Open Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+A">Angela Fan</a>, 
<a href="/search/cs?searchtype=author&query=Gokkaya%2C+B">Beliz Gokkaya</a>, 
<a href="/search/cs?searchtype=author&query=Harman%2C+M">Mark Harman</a>, 
<a href="/search/cs?searchtype=author&query=Lyubarskiy%2C+M">Mitya Lyubarskiy</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+S">Shubho Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Shin Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+M">Jie M. Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">This paper provides a survey of the emerging area of Large Language Models
(LLMs) for Software Engineering (SE). It also sets out open research challenges
for the application of LLMs to technical problems faced by software engineers.
LLMs' emergent properties bring novelty and creativity with applications right
across the spectrum of Software Engineering activities including coding,
design, requirements, repair, refactoring, performance improvement,
documentation and analytics. However, these very same emergent properties also
pose significant technical challenges; we need techniques that can reliably
weed out incorrect solutions, such as hallucinations. Our survey reveals the
pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in
the development and deployment of reliable, efficient and effective LLM-based
SE.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03534" title="Abstract">arXiv:2310.03534</a> [<a href="/pdf/2310.03534" title="Download PDF">pdf</a>, <a href="/format/2310.03534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D-Aware Hypothesis &amp; Verification for Generalizable Relative Object  Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Salzmann%2C+M">Mathieu Salzmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Prior methods that tackle the problem of generalizable object pose estimation
highly rely on having dense views of the unseen object. By contrast, we address
the scenario where only a single reference view of the object is available. Our
goal then is to estimate the relative object pose between this reference view
and a query image that depicts the object in a different pose. In this
scenario, robust generalization is imperative due to the presence of unseen
objects during testing and the large-scale object pose variation between the
reference and the query. To this end, we present a new
hypothesis-and-verification framework, in which we generate and evaluate
multiple pose hypotheses, ultimately selecting the most reliable one as the
relative object pose. To measure reliability, we introduce a 3D-aware
verification that explicitly applies 3D transformations to the 3D object
representations learned from the two input images. Our comprehensive
experiments on the Objaverse, LINEMOD, and CO3D datasets evidence the superior
accuracy of our approach in relative pose estimation and its robustness in
large-scale pose variations, when dealing with unseen objects.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03535" title="Abstract">arXiv:2310.03535</a> [<a href="/pdf/2310.03535" title="Download PDF">pdf</a>, <a href="/format/2310.03535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Unified Deep Image Deraining: A Survey and A New Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jinshan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jiangxin Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jinhui Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="http://www.deraining.tech/">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent years have witnessed significant advances in image deraining due to
the kinds of effective image priors and deep learning models. As each deraining
approach has individual settings (e.g., training and test datasets, evaluation
criteria), how to fairly evaluate existing approaches comprehensively is not a
trivial task. Although existing surveys aim to review of image deraining
approaches comprehensively, few of them focus on providing unify evaluation
settings to examine the deraining capability and practicality evaluation. In
this paper, we provide a comprehensive review of existing image deraining
method and provide a unify evaluation setting to evaluate the performance of
image deraining methods. We construct a new high-quality benchmark named
HQ-RAIN to further conduct extensive evaluation, consisting of 5,000 paired
high-resolution synthetic images with higher harmony and realism. We also
discuss the existing challenges and highlight several future research
opportunities worth exploring. To facilitate the reproduction and tracking of
the latest deraining technologies for general users, we build an online
platform to provide the off-the-shelf toolkit, involving the large-scale
performance evaluation. This online platform and the proposed new benchmark are
publicly available and will be regularly updated at <a href="http://www.deraining.tech/.">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03545" title="Abstract">arXiv:2310.03545</a> [<a href="/pdf/2310.03545" title="Download PDF">pdf</a>, <a href="/format/2310.03545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution-free risk assessment of regression-based machine learning  algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sukrita Singh</a>, 
<a href="/search/cs?searchtype=author&query=Sarna%2C+N">Neeraj Sarna</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Orfanoudaki%2C+A">Agni Orfanoudaki</a>, 
<a href="/search/cs?searchtype=author&query=Berger%2C+M">Michael Berger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Machine learning algorithms have grown in sophistication over the years and
are increasingly deployed for real-life applications. However, when using
machine learning techniques in practical settings, particularly in high-risk
applications such as medicine and engineering, obtaining the failure
probability of the predictive model is critical. We refer to this problem as
the risk-assessment task. We focus on regression algorithms and the
risk-assessment task of computing the probability of the true label lying
inside an interval defined around the model's prediction. We solve the
risk-assessment problem using the conformal prediction approach, which provides
prediction intervals that are guaranteed to contain the true label with a given
probability. Using this coverage property, we prove that our approximated
failure probability is conservative in the sense that it is not lower than the
true failure probability of the ML algorithm. We conduct extensive experiments
to empirically study the accuracy of the proposed method for problems with and
without covariate shift. Our analysis focuses on different modeling regimes,
dataset sizes, and conformal prediction methodologies.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03554" title="Abstract">arXiv:2310.03554</a> [<a href="/pdf/2310.03554" title="Download PDF">pdf</a>, <a href="/format/2310.03554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Twin-Empowered Smart Attack Detection System for 6G Edge of  Things Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yigit%2C+Y">Yagmur Yigit</a>, 
<a href="/search/cs?searchtype=author&query=Chrysoulas%2C+C">Christos Chrysoulas</a>, 
<a href="/search/cs?searchtype=author&query=Yurdakul%2C+G">Gokhan Yurdakul</a>, 
<a href="/search/cs?searchtype=author&query=Maglaras%2C+L">Leandros Maglaras</a>, 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">As global Internet of Things (IoT) devices connectivity surges, a significant
portion gravitates towards the Edge of Things (EoT) network. This shift prompts
businesses to deploy infrastructure closer to end-users, enhancing
accessibility. However, the growing EoT network expands the attack surface,
necessitating robust and proactive security measures. Traditional solutions
fall short against dynamic EoT threats, highlighting the need for proactive and
intelligent systems. We introduce a digital twin-empowered smart attack
detection system for 6G EoT networks. Leveraging digital twin and edge
computing, it monitors and simulates physical assets in real time, enhancing
security. An online learning module in the proposed system optimizes the
network performance. Our system excels in proactive threat detection, ensuring
6G EoT network security. The performance evaluations demonstrate its
effectiveness, robustness, and adaptability using real datasets.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03557" title="Abstract">arXiv:2310.03557</a> [<a href="/pdf/2310.03557" title="Download PDF">pdf</a>, <a href="/format/2310.03557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mobility Segregation Dynamics and Residual Isolation During Pandemic  Interventions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hilman%2C+R+M">Rafiazka Millanida Hilman</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Herranz%2C+M">Manuel Garc&#xed;a-Herranz</a>, 
<a href="/search/cs?searchtype=author&query=Sekara%2C+V">Vedran Sekara</a>, 
<a href="/search/cs?searchtype=author&query=Karsai%2C+M">M&#xe1;rton Karsai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Data Analysis, Statistics and Probability (physics.data-an); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">External shocks embody an unexpected and disruptive impact on the regular
life of people. This was the case during the COVID-19 outbreak that rapidly led
to changes in the typical mobility patterns in urban areas. In response, people
reorganised their daily errands throughout space. However, these changes might
not have been the same across socioeconomic classes leading to possibile
additional detrimental effects on inequality due to the pandemic. In this paper
we study the reorganisation of mobility segregation networks due to external
shocks and show that the diversity of visited places in terms of locations and
socioeconomic status is affected by the enforcement of mobility restriction
during pandemic. We use the case of COVID-19 as a natural experiment in several
cities to observe not only the effect of external shocks but also its mid-term
consequences and residual effects. We build on anonymised and privacy-preserved
mobility data in four cities: Bogota, Jakarta, London, and New York. We couple
mobility data with socioeconomic information to capture inequalities in
mobility among different socioeconomic groups and see how it changes
dynamically before, during, and after different lockdown periods. We find that
the first lockdowns induced considerable increases in mobility segregation in
each city, while loosening mobility restrictions did not necessarily diminished
isolation between different socioeconomic groups, as mobility mixing has not
recovered fully to its pre-pandemic level even weeks after the interruption of
interventions. Our results suggest that a one fits-all policy does not equally
affect the way people adjust their mobility, which calls for socioeconomically
informed intervention policies in the future.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03560" title="Abstract">arXiv:2310.03560</a> [<a href="/pdf/2310.03560" title="Download PDF">pdf</a>, <a href="/format/2310.03560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redefining Digital Health Interfaces with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imrie%2C+F">Fergus Imrie</a>, 
<a href="/search/cs?searchtype=author&query=Rauba%2C+P">Paulius Rauba</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Digital health tools have the potential to significantly improve the delivery
of healthcare services. However, their use remains comparatively limited due,
in part, to challenges surrounding usability and trust. Recently, Large
Language Models (LLMs) have emerged as general-purpose models with the ability
to process complex information and produce human-quality text, presenting a
wealth of potential applications in healthcare. Directly applying LLMs in
clinical settings is not straightforward, with LLMs susceptible to providing
inconsistent or nonsensical answers. We demonstrate how LLMs can utilize
external tools to provide a novel interface between clinicians and digital
technologies. This enhances the utility and practical impact of digital
healthcare tools and AI models while addressing current issues with using LLM
in clinical settings such as hallucinations. We illustrate our approach with
examples from cardiovascular disease and diabetes risk prediction, highlighting
the benefit compared to traditional interfaces for digital tools.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03563" title="Abstract">arXiv:2310.03563</a> [<a href="/pdf/2310.03563" title="Download PDF">pdf</a>, <a href="/format/2310.03563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance  Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Csehi%2C+%C3%81+I">&#xc1;goston Istv&#xe1;n Csehi</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%B3zsa%2C+C+M">Csaba M&#xe1;t&#xe9; J&#xf3;zsa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Nerf4ADR workshop of ICCV23 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which
defines the image pose estimation problem as a NeRF based iterative linear
optimization. NeRFs are novel neural space representation models that can
synthesize photorealistic novel views of real-world scenes or objects. Our
contributions are as follows: we extend the localization optimization objective
with a depth-based loss function, we introduce a multi-image based loss
function where a sequence of images with known relative poses are used without
increasing the computational complexity, we omit hierarchical sampling during
volumetric rendering, meaning only the coarse model is used for pose
estimation, and we how that by extending the sampling interval convergence can
be achieved even or higher initial pose estimate errors. With the proposed
modifications the convergence speed is significantly improved, and the basin of
convergence is substantially extended.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03567" title="Abstract">arXiv:2310.03567</a> [<a href="/pdf/2310.03567" title="Download PDF">pdf</a>, <a href="/format/2310.03567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimLOD: Simultaneous LOD Generation and Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtz%2C+M">Markus Sch&#xfc;tz</a>, 
<a href="/search/cs?searchtype=author&query=Herzberger%2C+L">Lukas Herzberger</a>, 
<a href="/search/cs?searchtype=author&query=Wimmer%2C+M">Michael Wimmer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">About: We propose an incremental LOD generation approach for point clouds
that allows us to simultaneously load points from disk, update an octree-based
level-of-detail representation, and render the intermediate results in real
time while additional points are still being loaded from disk. LOD construction
and rendering are both implemented in CUDA and share the GPU's processing
power, but each incremental update is lightweight enough to leave enough time
to maintain real-time frame rates.
<br />Background: LOD construction is typically implemented as a preprocessing step
that requires users to wait before they are able to view the results in real
time. This approach allows users to view intermediate results right away.
<br />Results: Our approach is able to stream points from an SSD and update the
octree on the GPU at rates of up to 580 million points per second (~9.3GB/s
from a PCIe 5.0 SSD) on an RTX 4090. Depending on the data set, our approach
spends an average of about 1 to 2 ms to incrementally insert 1 million points
into the octree, allowing us to insert several million points per frame into
the LOD structure and render the intermediate results within the same frame.
<br />Discussion/Limitations: We aim to provide near-instant, real-time
visualization of large data sets without preprocessing. Out-of-core processing
of arbitrarily large data sets and color-filtering for higher-quality LODs are
subject to future work.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03568" title="Abstract">arXiv:2310.03568</a> [<a href="/pdf/2310.03568" title="Download PDF">pdf</a>, <a href="/format/2310.03568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reverse-Mode AD of Reduce-by-Index and Scan in Futhark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruun%2C+L+M">Lotte Maria Bruun</a>, 
<a href="/search/cs?searchtype=author&query=Larsen%2C+U+S">Ulrik Stuhr Larsen</a>, 
<a href="/search/cs?searchtype=author&query=Hinnerskov%2C+N">Nikolaj Hinnerskov</a>, 
<a href="/search/cs?searchtype=author&query=Oancea%2C+C">Cosmin Oancea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at IFL'23 (i.e., 35th Symposium on Implementation and Application of Functional Languages, Aug. 29th - 31st, 2023, Braga, Portugal)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF)

</div>
<p class="mathjax">We present and evaluate the Futhark implementation of reverse-mode automatic
differentiation (AD) for the basic blocks of parallel programming: reduce,
prefix sum (scan), and reduce by index. We first present derivations of
general-case algorithms and then discuss several specializations that result in
efficient differentiation of most cases of practical interest. We report an
experiment that evaluates the performance of the differentiated code in the
context of GPU execution and highlights the impact of the proposed
specializations as well as the strengths and weaknesses of differentiating at
high level vs. low level (i.e., ``differentiating the memory'').
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03572" title="Abstract">arXiv:2310.03572</a> [<a href="/pdf/2310.03572" title="Download PDF">pdf</a>, <a href="/format/2310.03572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Residual Multi-Fidelity Neural Network Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davis%2C+O">Owen Davis</a>, 
<a href="/search/cs?searchtype=author&query=Motamed%2C+M">Mohammad Motamed</a>, 
<a href="/search/cs?searchtype=author&query=Tempone%2C+R">Raul Tempone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this work, we consider the general problem of constructing a neural
network surrogate model using multi-fidelity information. Given an inexpensive
low-fidelity and an expensive high-fidelity computational model, we present a
residual multi-fidelity computational framework that formulates the correlation
between models as a residual function, a possibly non-linear mapping between 1)
the shared input space of the models together with the low-fidelity model
output and 2) the discrepancy between the two model outputs. To accomplish
this, we train two neural networks to work in concert. The first network learns
the residual function on a small set of high-fidelity and low-fidelity data.
Once trained, this network is used to generate additional synthetic
high-fidelity data, which is used in the training of a second network. This
second network, once trained, acts as our surrogate for the high-fidelity
quantity of interest. We present three numerical examples to demonstrate the
power of the proposed framework. In particular, we show that dramatic savings
in computational cost may be achieved when the output predictions are desired
to be accurate within small tolerances.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03573" title="Abstract">arXiv:2310.03573</a> [<a href="/pdf/2310.03573" title="Download PDF">pdf</a>, <a href="/format/2310.03573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coupling OpenFOAM(R) with BoSSS, a discontinuous Galerkin solver written  in C#
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Klingenberg%2C+D">Dario Klingenberg</a>, 
<a href="/search/math?searchtype=author&query=Jasak%2C+H">Hrvoje Jasak</a>, 
<a href="/search/math?searchtype=author&query=Marschall%2C+H">Holger Marschall</a>, 
<a href="/search/math?searchtype=author&query=Kummer%2C+F">Florian Kummer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this article, we present the foam-dg project, which provides a bridge
between OpenFOAM(R) and the high-order DG (discontinuous Galerkin) framework
BoSSS. Thanks to the flexibility of the coupling approach, mixed calculations
where some parts of the equation system are solved in OpenFOAM(R) and others
are solved in BoSSS are easily possible. This is showcased using the convective
Cahn-Hilliard equation, where the Cahn-Hilliard part is solved in BoSSS and the
Navier-Stokes part is solved in OpenFOAM(R). The obtained results appear
reasonable, though the main focus of this paper is to present and document the
foam-dg project rather than on quantitative results.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03574" title="Abstract">arXiv:2310.03574</a> [<a href="/pdf/2310.03574" title="Download PDF">pdf</a>, <a href="/ps/2310.03574" title="Download PostScript">ps</a>, <a href="/format/2310.03574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on a gap in the proof of the minimum distance for Projective  Reed-Muller Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%B8rensen%2C+A+B">Anders Bj&#xe6;rt S&#xf8;rensen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Algebraic Geometry (math.AG)

</div>
<p class="mathjax">The note clarifies a gap in the proof of the minimum distance for Projective
Reed-Muller Codes. The gap was identified by S.Ghorpade and R.Ludhani in a
recent article. Here the original thoughts are explained and the gap closed.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03577" title="Abstract">arXiv:2310.03577</a> [<a href="/pdf/2310.03577" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Liquid Cooling System for a High Power, Medium Frequency, and Medium  Voltage Isolated Power Converter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Taghavi%2C+H">Hooman Taghavi</a>, 
<a href="/search/eess?searchtype=author&query=Shafei%2C+A+E">Ahmad El Shafei</a>, 
<a href="/search/eess?searchtype=author&query=Nasiri%2C+A">Adel Nasiri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Power electronics systems, widely used in various applications such as
industrial automation, electric cars, and renewable energy, have the primary
function of converting and controlling electrical power to the desired type of
load. Despite their reliability and efficiency, power losses in these systems
generate significant heat that must be dissipated to maintain performance and
prevent damage. Cooling systems play a crucial role in ensuring safe operating
temperatures for system components. Air and liquid cooling are the leading
technologies used in the power electronics world. Air cooling is simple and
cost-effective but is limited by ambient temperature and component thermal
resistance. While more efficient, liquid cooling requires more maintenance and
has higher upfront costs. Water-cooling systems have become famous for
regulating thermal loads as they can effectively remove heat from localized
high-temperature areas, such as the challenging hotspots in power electronics
systems. In addition to designing a cooling system for a power electronic
system, this study investigated the impact of three major parameters; cold
plate material, channel shape/size, and coolant inlet velocity. The research
examined and analyzed these factors and their trade-off analysis to obtain
cooling system design and optimization insights. This study might improve power
electronics system performance, reliability, and durability by improving heat
dissipation and thermal management.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03578" title="Abstract">arXiv:2310.03578</a> [<a href="/pdf/2310.03578" title="Download PDF">pdf</a>, <a href="/format/2310.03578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Targeted Adversarial Attacks on Generalizable Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Horvath%2C+A">Andras Horvath</a>, 
<a href="/search/cs?searchtype=author&query=Jozsa%2C+C+M">Csaba M. Jozsa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for
3D scene representation and rendering. These data-driven models can learn to
synthesize high-quality images from sparse 2D observations, enabling realistic
and interactive scene reconstructions. However, the growing usage of NeRFs in
critical applications such as augmented reality, robotics, and virtual
environments could be threatened by adversarial attacks.
<br />In this paper we present how generalizable NeRFs can be attacked by both
low-intensity adversarial attacks and adversarial patches, where the later
could be robust enough to be used in real world applications. We also
demonstrate targeted attacks, where a specific, predefined output scene is
generated by these attack with success.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03579" title="Abstract">arXiv:2310.03579</a> [<a href="/pdf/2310.03579" title="Download PDF">pdf</a>, <a href="/format/2310.03579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Inference in Gene Regulatory Networks with GFlowNet: Towards  Scalability in Large Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Trang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+A">Alexander Tong</a>, 
<a href="/search/cs?searchtype=author&query=Madan%2C+K">Kanika Madan</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dianbo Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Molecular Networks (q-bio.MN)

</div>
<p class="mathjax">Understanding causal relationships within Gene Regulatory Networks (GRNs) is
essential for unraveling the gene interactions in cellular processes. However,
causal discovery in GRNs is a challenging problem for multiple reasons
including the existence of cyclic feedback loops and uncertainty that yields
diverse possible causal structures. Previous works in this area either ignore
cyclic dynamics (assume acyclic structure) or struggle with scalability. We
introduce Swift-DynGFN as a novel framework that enhances causal structure
learning in GRNs while addressing scalability concerns. Specifically,
Swift-DynGFN exploits gene-wise independence to boost parallelization and to
lower computational cost. Experiments on real single-cell RNA velocity and
synthetic GRN datasets showcase the advancement in learning causal structure in
GRNs and scalability in larger systems.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03580" title="Abstract">arXiv:2310.03580</a> [<a href="/pdf/2310.03580" title="Download PDF">pdf</a>, <a href="/format/2310.03580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open RAN for 5G Supply Chain Diversification: The BEACON-5G Approach and  Key Achievements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aijaz%2C+A">Adnan Aijaz</a>, 
<a href="/search/cs?searchtype=author&query=Gufran%2C+S">Sajida Gufran</a>, 
<a href="/search/cs?searchtype=author&query=Farnham%2C+T">Tim Farnham</a>, 
<a href="/search/cs?searchtype=author&query=Chintalapati%2C+S">Sita Chintalapati</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez-Momp%C3%B3%2C+A">Adri&#xe1;n S&#xe1;nchez-Momp&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peizheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 15 figures. This paper has been accepted by the IEEE Conference on Standards for Communications and Networking (IEEE CSCN 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Open RAN brings multi-vendor diversity and interoperability to
mobile/cellular networks. It is becoming part of governmental strategies for
diversifying telecoms supply chains. This paper describes the approach and key
achievements of the BEACON-5G project, jointly funded by the UK government and
industry. The BEACON-5G project aims at developing a competitive edge for 5G
Open RAN and contributing toward its maturity. It addresses some of the key
challenges in this respect and provides various innovations for system
integration, network slicing, marketplace integration, cyber security, and
white-box RAN. It also conducts real-world technology trials for urban
use-cases. The paper also captures some of the key lessons learned during
delivery, the main outcomes, and highlights potential impact on the wider UK 5G
diversification strategy.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03581" title="Abstract">arXiv:2310.03581</a> [<a href="/pdf/2310.03581" title="Download PDF">pdf</a>, <a href="/format/2310.03581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resilient Legged Local Navigation: Learning to Traverse with Compromised  Perception End-to-End
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Jin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Frey%2C+J">Jonas Frey</a>, 
<a href="/search/cs?searchtype=author&query=Rudin%2C+N">Nikita Rudin</a>, 
<a href="/search/cs?searchtype=author&query=Mattamala%2C+M">Matias Mattamala</a>, 
<a href="/search/cs?searchtype=author&query=Cadena%2C+C">Cesar Cadena</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+M">Marco Hutter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website and videos are available at our Project Page: <a href="https://bit.ly/45NBTuh">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">Autonomous robots must navigate reliably in unknown environments even under
compromised exteroceptive perception, or perception failures. Such failures
often occur when harsh environments lead to degraded sensing, or when the
perception algorithm misinterprets the scene due to limited generalization. In
this paper, we model perception failures as invisible obstacles and pits, and
train a reinforcement learning (RL) based local navigation policy to guide our
legged robot. Unlike previous works relying on heuristics and anomaly detection
to update navigational information, we train our navigation policy to
reconstruct the environment information in the latent space from corrupted
perception and react to perception failures end-to-end. To this end, we
incorporate both proprioception and exteroception into our policy inputs,
thereby enabling the policy to sense collisions on different body parts and
pits, prompting corresponding reactions. We validate our approach in simulation
and on the real quadruped robot ANYmal running in real-time (&lt;10 ms CPU
inference). In a quantitative comparison with existing heuristic-based locally
reactive planners, our policy increases the success rate over 30% when facing
perception failures. Project Page: https://bit.ly/45NBTuh.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03583" title="Abstract">arXiv:2310.03583</a> [<a href="/pdf/2310.03583" title="Download PDF">pdf</a>, <a href="/format/2310.03583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CyMed: A Framework for Testing Cybersecurity of Connected Medical  Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scherb%2C+C">Christopher Scherb</a>, 
<a href="/search/cs?searchtype=author&query=Hadayah%2C+A">Adrian Hadayah</a>, 
<a href="/search/cs?searchtype=author&query=Heitz%2C+L+B">Luc Bryan Heitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Connected Medical Devices (CMDs) have a large impact on patients as they
allow them to lead a more normal life. Any malfunction could not only remove
the health benefits the CMDs provide, they could also cause further harm to the
patient. Due to this, there are many safety regulations which must be adhered
to prior to a CMD entering the market. However, while many detailed safety
regulations exist, there are a fundamental lack of cybersecurity frameworks
applicable to CMDs. While there are recent regulations which aim to enforce
cybersecurity practices, they are vague and do not contain the concrete steps
necessary to implement cybersecurity. This paper aims to fill that gap by
describing a framework, CyMed, to be used by vendors and ens-users, which
contains concrete measures to improve the resilience of CMDs against cyber
attack. The CyMed framework is subsequently evaluated based on practical tests
as well as expert interviews.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03585" title="Abstract">arXiv:2310.03585</a> [<a href="/pdf/2310.03585" title="Download PDF">pdf</a>, <a href="/format/2310.03585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smoothing Methods for Automatic Differentiation Across Conditional  Branches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kreikemeyer%2C+J+N">Justin N. Kreikemeyer</a>, 
<a href="/search/cs?searchtype=author&query=Andelfinger%2C+P">Philipp Andelfinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Mathematical Software (cs.MS); Optimization and Control (math.OC)

</div>
<p class="mathjax">Programs involving discontinuities introduced by control flow constructs such
as conditional branches pose challenges to mathematical optimization methods
that assume a degree of smoothness in the objective function's response
surface. Smooth interpretation (SI) is a form of abstract interpretation that
approximates the convolution of a program's output with a Gaussian kernel, thus
smoothing its output in a principled manner. Here, we combine SI with automatic
differentiation (AD) to efficiently compute gradients of smoothed programs. In
contrast to AD across a regular program execution, these gradients also capture
the effects of alternative control flow paths. The combination of SI with AD
enables the direct gradient-based parameter synthesis for branching programs,
allowing for instance the calibration of simulation models or their combination
with neural network models in machine learning pipelines. We detail the effects
of the approximations made for tractability in SI and propose a novel Monte
Carlo estimator that avoids the underlying assumptions by estimating the
smoothed programs' gradients through a combination of AD and sampling. Using
DiscoGrad, our tool for automatically translating simple C++ programs to a
smooth differentiable form, we perform an extensive evaluation. We compare the
combination of SI with AD and our Monte Carlo estimator to existing
gradient-free and stochastic methods on four non-trivial and originally
discontinuous problems ranging from classical simulation-based optimization to
neural network-driven control. While the optimization progress with the
SI-based estimator depends on the complexity of the programs' control flow, our
Monte Carlo estimator is competitive in all problems, exhibiting the fastest
convergence by a substantial margin in our highest-dimensional problem.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03586" title="Abstract">arXiv:2310.03586</a> [<a href="/pdf/2310.03586" title="Download PDF">pdf</a>, <a href="/format/2310.03586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Suspended Aerial Manipulation Avatar for Physical Interaction in  Unstructured Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+F">Fanyi Kong</a>, 
<a href="/search/cs?searchtype=author&query=Zambella%2C+G">Grazia Zambella</a>, 
<a href="/search/cs?searchtype=author&query=Monteleone%2C+S">Simone Monteleone</a>, 
<a href="/search/cs?searchtype=author&query=Grioli%2C+G">Giorgio Grioli</a>, 
<a href="/search/cs?searchtype=author&query=Catalano%2C+M+G">Manuel G. Catalano</a>, 
<a href="/search/cs?searchtype=author&query=Bicchi%2C+A">Antonio Bicchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper presents a floating robot capable of performing physically
interactive tasks in unstructured environments with human-like dexterity under
human supervision. The robot consists of a humanoid torso attached to a
hexacopter. A two-degree-of-freedom head and two five-degree-of-freedom arms
equipped with softhands provide the requisite dexterity to allow human
operators to carry out various tasks. A robust tendon-driven structure is
purposefully designed for the arms, considerably reducing the impact of arm
inertia on the floating base in motion. In addition, tendons provide
flexibility to the joints, which enhances the robustness of the arm preventing
damage in interaction with the environment. To increase the payload of the
aerial system and the battery life, we use the concept of Suspended Aerial
Manipulation, i.e., the flying humanoid can be connected with a tether to a
structure, e.g., a larger airborne carrier or a supporting crane. Importantly,
to maximize portability and applicability, we adopt a modular approach
exploiting commercial components for the drone hardware and autopilot, while
developing a whole-body outer control loop to stabilize the robot attitude,
compensating for the tether force and for the humanoid head and arm motions.
The humanoid can be controlled by a remote operator, thus effectively realizing
a Suspended Aerial Manipulation Avatar. The proposed system is validated
through experiments in indoor scenarios reproducing post-disaster tasks.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03589" title="Abstract">arXiv:2310.03589</a> [<a href="/pdf/2310.03589" title="Download PDF">pdf</a>, <a href="/format/2310.03589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TimeGPT-1
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garza%2C+A">Azul Garza</a>, 
<a href="/search/cs?searchtype=author&query=Mergenthaler-Canseco%2C+M">Max Mergenthaler-Canseco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP)

</div>
<p class="mathjax">In this paper, we introduce TimeGPT, the first foundation model for time
series, capable of generating accurate predictions for diverse datasets not
seen during training. We evaluate our pre-trained model against established
statistical, machine learning, and deep learning methods, demonstrating that
TimeGPT zero-shot inference excels in performance, efficiency, and simplicity.
Our study provides compelling evidence that insights from other domains of
artificial intelligence can be effectively applied to time series analysis. We
conclude that large-scale time series models offer an exciting opportunity to
democratize access to precise predictions and reduce uncertainty by leveraging
the capabilities of contemporary advancements in deep learning.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03591" title="Abstract">arXiv:2310.03591</a> [<a href="/pdf/2310.03591" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Artificial Intelligence on Electrical and Electronics  Engineering Productivity in the Construction Industry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Victor%2C+N+O+C">Nwosu Obinnaya Chikezie Victor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Artificial intelligence (AI) can revolutionize the development industry,
primarily electrical and electronics engineering. By automating recurring
duties, AI can grow productivity and efficiency in creating. For instance, AI
can research constructing designs, discover capability troubles, and generate
answers, reducing the effort and time required for manual analysis. AI also can
be used to optimize electricity consumption in buildings, which is a critical
difficulty in the construction enterprise. Via machines gaining knowledge of
algorithms to investigate electricity usage patterns, AI can discover areas
wherein power may be stored and offer guidelines for enhancements. This can
result in significant value financial savings and reduced carbon emissions.
Moreover, AI may be used to improve the protection of creation websites. By
studying statistics from sensors and cameras, AI can locate capacity dangers
and alert workers to take suitable action. This could help save you from
injuries and accidents on production sites, lowering the chance for workers and
enhancing overall safety in the enterprise. The impact of AI on electric and
electronics engineering productivity inside the creation industry is enormous.
AI can transform how we layout, build, and function buildings by automating
ordinary duties, optimising electricity intake, and enhancing safety. However,
ensuring that AI is used ethically and responsibly and that the advantages are
shared fairly throughout the enterprise is essential.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03598" title="Abstract">arXiv:2310.03598</a> [<a href="/pdf/2310.03598" title="Download PDF">pdf</a>, <a href="/format/2310.03598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divide, Conquer and Verify: Improving Symbolic Execution Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scherb%2C+C">Christopher Scherb</a>, 
<a href="/search/cs?searchtype=author&query=Heitz%2C+L+B">Luc Bryan Heitz</a>, 
<a href="/search/cs?searchtype=author&query=Grieder%2C+H">Hermann Grieder</a>, 
<a href="/search/cs?searchtype=author&query=Mattmann%2C+O">Olivier Mattmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Symbolic Computation (cs.SC); Systems and Control (eess.SY)

</div>
<p class="mathjax">Symbolic Execution is a formal method that can be used to verify the behavior
of computer programs and detect software vulnerabilities. Compared to other
testing methods such as fuzzing, Symbolic Execution has the advantage of
providing formal guarantees about the program. However, despite advances in
performance in recent years, Symbolic Execution is too slow to be applied to
real-world software. This is primarily caused by the \emph{path explosion
problem} as well as by the computational complexity of SMT solving. In this
paper, we present a divide-and-conquer approach for symbolic execution by
executing individual slices and later combining the side effects. This way, the
overall problem size is kept small, reducing the impact of computational
complexity on large problems.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03602" title="Abstract">arXiv:2310.03602</a> [<a href="/pdf/2310.03602" title="Download PDF">pdf</a>, <a href="/format/2310.03602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaotao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kunming Luo</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+P">Ping Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-driven 3D indoor scene generation could be useful for gaming, film
industry, and AR/VR applications. However, existing methods cannot faithfully
capture the room layout, nor do they allow flexible editing of individual
objects in the room. To address these problems, we present Ctrl-Room, which is
able to generate convincing 3D rooms with designer-style layouts and
high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables
versatile interactive editing operations such as resizing or moving individual
furniture items. Our key insight is to separate the modeling of layouts and
appearance. %how to model the room that takes into account both scene texture
and geometry at the same time. To this end, Our proposed method consists of two
stages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The
`Layout Generation Stage' trains a text-conditional diffusion model to learn
the layout distribution with our holistic scene code parameterization. Next,
the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a
vivid panoramic image of the room guided by the 3D scene layout and text
prompt. In this way, we achieve a high-quality 3D room with convincing layouts
and lively textures. Benefiting from the scene code parameterization, we can
easily edit the generated room model through our mask-guided editing module,
without expensive editing-specific training. Extensive experiments on the
Structured3D dataset demonstrate that our method outperforms existing methods
in producing more reasonable, view-consistent, and editable 3D rooms from
natural language prompts.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03605" title="Abstract">arXiv:2310.03605</a> [<a href="/pdf/2310.03605" title="Download PDF">pdf</a>, <a href="/format/2310.03605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FASER: Binary Code Similarity Search through the use of Intermediate  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Collyer%2C+J">Josh Collyer</a>, 
<a href="/search/cs?searchtype=author&query=Watson%2C+T">Tim Watson</a>, 
<a href="/search/cs?searchtype=author&query=Phillips%2C+I">Iain Phillips</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, To be presented as Conference on Applied Machine Learning for Information Security
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Being able to identify functions of interest in cross-architecture software
is useful whether you are analysing for malware, securing the software supply
chain or conducting vulnerability research. Cross-Architecture Binary Code
Similarity Search has been explored in numerous studies and has used a wide
range of different data sources to achieve its goals. The data sources
typically used draw on common structures derived from binaries such as function
control flow graphs or binary level call graphs, the output of the disassembly
process or the outputs of a dynamic analysis approach. One data source which
has received less attention is binary intermediate representations. Binary
Intermediate representations possess two interesting properties: they are cross
architecture by their very nature and encode the semantics of a function
explicitly to support downstream usage. Within this paper we propose Function
as a String Encoded Representation (FASER) which combines long document
transformers with the use of intermediate representations to create a model
capable of cross architecture function search without the need for manual
feature engineering, pre-training or a dynamic analysis step. We compare our
approach against a series of baseline approaches for two tasks; A general
function search task and a targeted vulnerability search task. Our approach
demonstrates strong performance across both tasks, performing better than all
baseline approaches.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03606" title="Abstract">arXiv:2310.03606</a> [<a href="/pdf/2310.03606" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Time-Series Analysis Approaches Utilized in Research Papers to  Forecast COVID-19 Cases in Africa: A Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ebadi%2C+A">Ali Ebadi</a>, 
<a href="/search/cs?searchtype=author&query=Sahafizadeh%2C+E">Ebrahim Sahafizadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This literature review aimed to compare various time-series analysis
approaches utilized in forecasting COVID-19 cases in Africa. The study involved
a methodical search for English-language research papers published between
January 2020 and July 2023, focusing specifically on papers that utilized
time-series analysis approaches on COVID-19 datasets in Africa. A variety of
databases including PubMed, Google Scholar, Scopus, and Web of Science were
utilized for this process. The research papers underwent an evaluation process
to extract relevant information regarding the implementation and performance of
the time-series analysis models. The study highlighted the different
methodologies employed, evaluating their effectiveness and limitations in
forecasting the spread of the virus. The result of this review could contribute
deeper insights into the field, and future research should consider these
insights to improve time series analysis models and explore the integration of
different approaches for enhanced public health decision-making.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03610" title="Abstract">arXiv:2310.03610</a> [<a href="/pdf/2310.03610" title="Download PDF">pdf</a>, <a href="/format/2310.03610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting the Value of Flexibility in AC Security-Constrained  Transmission Expansion Planning via a Cooperative Game Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Churkin%2C+A">Andrey Churkin</a>, 
<a href="/search/eess?searchtype=author&query=Kong%2C+W">Wangwei Kong</a>, 
<a href="/search/eess?searchtype=author&query=Alizadeh%2C+M+I">Mohammad Iman Alizadeh</a>, 
<a href="/search/eess?searchtype=author&query=Capitanescu%2C+F">Florin Capitanescu</a>, 
<a href="/search/eess?searchtype=author&query=Mancarella%2C+P">Pierluigi Mancarella</a>, 
<a href="/search/eess?searchtype=author&query=Cese%C3%B1a%2C+E+A+M">Eduardo A. Mart&#xed;nez Cese&#xf1;a</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to PSCC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Security-constrained transmission expansion planning (SCTEP) is an inherently
complex problem that requires simultaneously solving multiple contingency
states of the system (usually corresponding to N-1 security criterion).
Existing studies focus on effectively finding optimal solutions; however,
single optimal solutions are not sufficient to interpret the value of
flexibility (e.g., from energy storage systems) and support system planners in
well-informed decision making. In view of planning uncertainties, it is
necessary to estimate the contributions of flexibility to various objectives
and prioritise the most effective investments. In this regard, this work
introduces a SCTEP tool that enables interpreting the value of flexibility in
terms of contributions to avoided load curtailment and total expected system
cost reduction. Inspired by cooperative game theory, the tool ranks the
contributions of flexibility providers and compares them against traditional
line reinforcements. This information can be used by system planners to
prioritise investments with higher contributions and synergistic capabilities.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03611" title="Abstract">arXiv:2310.03611</a> [<a href="/pdf/2310.03611" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene  Interactions From Gene Expression Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elnaggar%2C+A+F">Ahmed Fakhry Elnaggar</a>, 
<a href="/search/cs?searchtype=author&query=Khafagy%2C+R+A">Raneem Ali Khafagy</a>, 
<a href="/search/cs?searchtype=author&query=Ludl%2C+A">Adriaan-Alexander Ludl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Detecting and discovering new gene interactions based on known gene
expressions and gene interaction data presents a significant challenge. Various
statistical and deep learning methods have attempted to tackle this challenge
by leveraging the topological structure of gene interactions and gene
expression patterns to predict novel gene interactions. In contrast, some
approaches have focused exclusively on utilizing gene expression profiles. In
this context, we introduce GENER, a parallel-layer deep learning network
designed exclusively for the identification of gene-gene relationships using
gene expression data. We conducted two training experiments and compared the
performance of our network with that of existing statistical and deep learning
approaches. Notably, our model achieved an average AUROC score of 0.834 on the
combined BioGRID&amp;DREAM5 dataset, outperforming competing methods in predicting
gene-gene interactions.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03613" title="Abstract">arXiv:2310.03613</a> [<a href="/pdf/2310.03613" title="Download PDF">pdf</a>, <a href="/format/2310.03613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving a Class of Non-Convex Minimax Optimization in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xidong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jianhui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhengmian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aidong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The minimax problems arise throughout machine learning applications, ranging
from adversarial training and policy evaluation in reinforcement learning to
AUROC maximization. To address the large-scale data challenges across multiple
clients with communication-efficient distributed training, federated learning
(FL) is gaining popularity. Many optimization algorithms for minimax problems
have been developed in the centralized setting (\emph{i.e.} single-machine).
Nonetheless, the algorithm for minimax problems under FL is still
underexplored. In this paper, we study a class of federated nonconvex minimax
optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and
reduce existing complexity results for the most common minimax problems. For
nonconvex-concave problems, we propose FedSGDA+ and reduce the communication
complexity to $O(\varepsilon^{-6})$. Under nonconvex-strongly-concave and
nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known
sample complexity of $O(\kappa^{3} N^{-1}\varepsilon^{-3})$ and the best-known
communication complexity of $O(\kappa^{2}\varepsilon^{-2})$. FedSGDA-M is the
first algorithm to match the best sample complexity $O(\varepsilon^{-3})$
achieved by the single-machine method under the nonconvex-strongly-concave
setting. Extensive experimental results on fair classification and AUROC
maximization show the efficiency of our algorithms.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03614" title="Abstract">arXiv:2310.03614</a> [<a href="/pdf/2310.03614" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Machine Learning for Social Good: Reframing the Adversary as  an Ally
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Maliki%2C+S">Shawqi Al-Maliki</a>, 
<a href="/search/cs?searchtype=author&query=Qayyum%2C+A">Adnan Qayyum</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+H">Hassan Ali</a>, 
<a href="/search/cs?searchtype=author&query=Abdallah%2C+M">Mohamed Abdallah</a>, 
<a href="/search/cs?searchtype=author&query=Qadir%2C+J">Junaid Qadir</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D+T">Dinh Thai Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Al-Fuqaha%2C+A">Ala Al-Fuqaha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Deep Neural Networks (DNNs) have been the driving force behind many of the
recent advances in machine learning. However, research has shown that DNNs are
vulnerable to adversarial examples -- input samples that have been perturbed to
force DNN-based models to make errors. As a result, Adversarial Machine
Learning (AdvML) has gained a lot of attention, and researchers have
investigated these vulnerabilities in various settings and modalities. In
addition, DNNs have also been found to incorporate embedded bias and often
produce unexplainable predictions, which can result in anti-social AI
applications. The emergence of new AI technologies that leverage Large Language
Models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing
anti-social applications at scale. AdvML for Social Good (AdvML4G) is an
emerging field that repurposes the AdvML bug to invent pro-social applications.
Regulators, practitioners, and researchers should collaborate to encourage the
development of pro-social applications and hinder the development of
anti-social ones. In this work, we provide the first comprehensive review of
the emerging field of AdvML4G. This paper encompasses a taxonomy that
highlights the emergence of AdvML4G, a discussion of the differences and
similarities between AdvML4G and AdvML, a taxonomy covering social good-related
concepts and aspects, an exploration of the motivations behind the emergence of
AdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the
works that utilize AdvML4G as an auxiliary tool for innovating pro-social
applications. Finally, we elaborate upon various challenges and open research
issues that require significant attention from the research community.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03615" title="Abstract">arXiv:2310.03615</a> [<a href="/pdf/2310.03615" title="Download PDF">pdf</a>, <a href="/format/2310.03615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Animatable Virtual Humans: Learning pose-dependent human representations  in UV space for interactive performance synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morgenstern%2C+W">Wieland Morgenstern</a>, 
<a href="/search/cs?searchtype=author&query=Bagdasarian%2C+M+T">Milena T. Bagdasarian</a>, 
<a href="/search/cs?searchtype=author&query=Hilsmann%2C+A">Anna Hilsmann</a>, 
<a href="/search/cs?searchtype=author&query=Eisert%2C+P">Peter Eisert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We propose a novel representation of virtual humans for highly realistic
real-time animation and rendering in 3D applications. We learn pose dependent
appearance and geometry from highly accurate dynamic mesh sequences obtained
from state-of-the-art multiview-video reconstruction. Learning pose-dependent
appearance and geometry from mesh sequences poses significant challenges, as it
requires the network to learn the intricate shape and articulated motion of a
human body. However, statistical body models like SMPL provide valuable
a-priori knowledge which we leverage in order to constrain the dimension of the
search space enabling more efficient and targeted learning and define
pose-dependency. Instead of directly learning absolute pose-dependent geometry,
we learn the difference between the observed geometry and the fitted SMPL
model. This allows us to encode both pose-dependent appearance and geometry in
the consistent UV space of the SMPL model. This approach not only ensures a
high level of realism but also facilitates streamlined processing and rendering
of virtual humans in real-time scenarios.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03616" title="Abstract">arXiv:2310.03616</a> [<a href="/pdf/2310.03616" title="Download PDF">pdf</a>, <a href="/format/2310.03616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoK: Decentralized Sequencers for Rollups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Motepalli%2C+S">Shashank Motepalli</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+L">Luciano Freitas</a>, 
<a href="/search/cs?searchtype=author&query=Livshits%2C+B">Benjamin Livshits</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA); Software Engineering (cs.SE)

</div>
<p class="mathjax">Rollups have emerged as a promising solution to enhance blockchain
scalability, offering increased throughput, reduced latency, and lower
transaction fees. However, they currently rely on a centralized sequencer to
determine transaction ordering, compromising the decentralization principle of
blockchain systems. Recognizing this, there is a clear need for decentralized
sequencers in rollups. However, designing such a system is intricate. This
paper presents a comprehensive exploration of decentralized sequencers in
rollups, formulating their ideal properties, dissecting their core components,
and synthesizing community insights. Our findings emphasize the imperative for
an adept sequencer design, harmonizing with the overarching goals of the
blockchain ecosystem, and setting a trajectory for subsequent research
endeavors.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03617" title="Abstract">arXiv:2310.03617</a> [<a href="/pdf/2310.03617" title="Download PDF">pdf</a>, <a href="/format/2310.03617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RouteKG: A knowledge graph-based framework for route prediction on road  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yihong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weipeng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+S">Shuyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuebing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhenliang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhan Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Short-term route prediction on road networks allows us to anticipate the
future trajectories of road users, enabling a plethora of intelligent
transportation applications such as dynamic traffic control or personalized
route recommendation. Despite the recent advances in this area, existing
methods focus primarily on learning sequential patterns, neglecting the
inherent spatial structure in road networks that can affect human routing
decisions. To fill the gap, this paper introduces RouteKG, a novel Knowledge
Graph-based framework for route prediction. Specifically, we construct a
Knowledge Graph on the road network, thereby learning and leveraging spatial
relations, especially moving directions, which are crucial for human
navigation. Moreover, an n-ary tree-based algorithm is introduced to
efficiently generate top-K routes in a batch mode, enhancing scalability and
computational efficiency. To further optimize the prediction performance, a
rank refinement module is incorporated to fine-tune the candidate route
rankings. The model performance is evaluated using two real-world vehicle
trajectory datasets from two Chinese cities, Chengdu and Shanghai, under
various practical scenarios. The results demonstrate a significant improvement
in accuracy over baseline methods, with an average increase of 6.2%, 7.8%, and
6.1% in top-1, 5, and 10 routes predictions, respectively. We further validate
our model through a case study that utilizes the pretrained model as a
simulator for real-time traffic flow estimation at the link level. The proposed
RouteKG promises wide-ranging applications in vehicle navigation, traffic
management, and other intelligent transportation tasks.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03618" title="Abstract">arXiv:2310.03618</a> [<a href="/pdf/2310.03618" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLASSify: A Web-Based Tool for Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mullen%2C+A+D">Aaron D. Mullen</a>, 
<a href="/search/cs?searchtype=author&query=Armstrong%2C+S+E">Samuel E. Armstrong</a>, 
<a href="/search/cs?searchtype=author&query=Talbert%2C+J">Jeff Talbert</a>, 
<a href="/search/cs?searchtype=author&query=Bumgardner%2C+V+K+C">V.K. Cody Bumgardner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 11 figures (3 images, 5 graphs, 3 tables)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Human-Computer Interaction (cs.HC); Software Engineering (cs.SE)

</div>
<p class="mathjax">Machine learning classification problems are widespread in bioinformatics,
but the technical knowledge required to perform model training, optimization,
and inference can prevent researchers from utilizing this technology. This
article presents an automated tool for machine learning classification problems
to simplify the process of training models and producing results while
providing informative visualizations and insights into the data. This tool
supports both binary and multiclass classification problems, and it provides
access to a variety of models and methods. Synthetic data can be generated
within the interface to fill missing values, balance class labels, or generate
entirely new datasets. It also provides support for feature evaluation and
generates explainability scores to indicate which features influence the output
the most. We present CLASSify, an open-source tool for simplifying the user
experience of solving classification problems without the need for knowledge of
machine learning.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03620" title="Abstract">arXiv:2310.03620</a> [<a href="/pdf/2310.03620" title="Download PDF">pdf</a>, <a href="/format/2310.03620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PeaTMOSS: Mining Pre-Trained Models in Open-Source Software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenxin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+J">Jason Jones</a>, 
<a href="/search/cs?searchtype=author&query=Yasmin%2C+J">Jerin Yasmin</a>, 
<a href="/search/cs?searchtype=author&query=Synovic%2C+N">Nicholas Synovic</a>, 
<a href="/search/cs?searchtype=author&query=Sashti%2C+R">Rajeev Sashti</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sophie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Thiruvathukal%2C+G+K">George K. Thiruvathukal</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J+C">James C. Davis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Developing and training deep learning models is expensive, so software
engineers have begun to reuse pre-trained deep learning models (PTMs) and
fine-tune them for downstream tasks. Despite the wide-spread use of PTMs, we
know little about the corresponding software engineering behaviors and
challenges.
<br />To enable the study of software engineering with PTMs, we present the
PeaTMOSS dataset: Pre-Trained Models in Open-Source Software. PeaTMOSS has
three parts: a snapshot of (1) 281,638 PTMs, (2) 27,270 open-source software
repositories that use PTMs, and (3) a mapping between PTMs and the projects
that use them. We challenge PeaTMOSS miners to discover software engineering
practices around PTMs. A demo and link to the full dataset are available at:
https://github.com/PurdueDualityLab/PeaTMOSS-Demos.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03624" title="Abstract">arXiv:2310.03624</a> [<a href="/pdf/2310.03624" title="Download PDF">pdf</a>, <a href="/format/2310.03624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling  and Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schulze%2C+L">Lennart Schulze</a>, 
<a href="/search/cs?searchtype=author&query=Lipson%2C+H">Hod Lipson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023 Workshop on Neural Fields for Autonomous Driving and Robotics (oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">A robot self-model is a task-agnostic representation of the robot's physical
morphology that can be used for motion planning tasks in absence of classical
geometric kinematic models. In particular, when the latter are hard to engineer
or the robot's kinematics change unexpectedly, human-free self-modeling is a
necessary feature of truly autonomous agents. In this work, we leverage neural
fields to allow a robot to self-model its kinematics as a neural-implicit query
model learned only from 2D images annotated with camera poses and
configurations. This enables significantly greater applicability than existing
approaches which have been dependent on depth images or geometry knowledge. To
this end, alongside a curricular data sampling strategy, we propose a new
encoder-based neural density field architecture for dynamic object-centric
scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF
robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2%
of the robot's workspace dimension. We demonstrate the capabilities of this
model on a motion planning task as an exemplary downstream application.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03627" title="Abstract">arXiv:2310.03627</a> [<a href="/pdf/2310.03627" title="Download PDF">pdf</a>, <a href="/ps/2310.03627" title="Download PostScript">ps</a>, <a href="/format/2310.03627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Belief Expansion in Subset Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lehmann%2C+E">Eveline Lehmann</a>, 
<a href="/search/cs?searchtype=author&query=Studer%2C+T">Thomas Studer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Subset models provide a new semantics for justifcation logic. The main idea
of subset models is that evidence terms are interpreted as sets of possible
worlds. A term then justifies a formula if that formula is true in each world
of the interpretation of the term. In this paper, we introduce a belief
expansion operator for subset models. We study the main properties of the
resulting logic as well as the differences to a previous (symbolic) approach to
belief expansion in justification logic.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03629" title="Abstract">arXiv:2310.03629</a> [<a href="/pdf/2310.03629" title="Download PDF">pdf</a>, <a href="/format/2310.03629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wasserstein Distortion: Unifying Fidelity and Realism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+A+B">Aaron B. Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Ball%C3%A9%2C+J">Johannes Ball&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Theis%2C+L">Lucas Theis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">We introduce a distortion measure for images, Wasserstein distortion, that
simultaneously generalizes pixel-level fidelity on the one hand and realism on
the other. We show how Wasserstein distortion reduces mathematically to a pure
fidelity constraint or a pure realism constraint under different parameter
choices. Pairs of images that are close under Wasserstein distortion illustrate
its utility. In particular, we generate random textures that have high fidelity
to a reference texture in one location of the image and smoothly transition to
an independent realization of the texture as one moves away from this point.
Connections between Wasserstein distortion and models of the human visual
system are noted.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03634" title="Abstract">arXiv:2310.03634</a> [<a href="/pdf/2310.03634" title="Download PDF">pdf</a>, <a href="/format/2310.03634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When a random tape is not enough: lower bounds for a problem in  adversarially robust streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakrabarti%2C+A">Amit Chakrabarti</a>, 
<a href="/search/cs?searchtype=author&query=Stoeckl%2C+M">Manuel Stoeckl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Adversarially robust streaming algorithms are required to process a stream of
elements and produce correct outputs, even when each stream element can be
chosen depending on earlier algorithm outputs. As with classic streaming
algorithms, which must only be correct for the worst-case fixed stream,
adversarially robust algorithms with access to randomness can use significantly
less space than deterministic algorithms. We prove that for the Missing Item
Finding problem in streaming, the space complexity also significantly depends
on how adversarially robust algorithms are permitted to use randomness. (In
contrast, the space complexity of classic streaming algorithms does not depend
as strongly on the way randomness is used.)
<br />For Missing Item Finding on streams of length $r$ with elements in
$\{1,...n\}$, and $\le 1/\text{poly}(n)$ error, we show that when $r =
O(2^{\sqrt{\log n}})$, "random seed" adversarially robust algorithms, which
only use randomness at initialization, require $r^{\Omega(1)}$ bits of space,
while "random tape" adversarially robust algorithms, which may make random
decisions at any time, may use $O(\text{polylog}(r))$ random bits. When $r =
\Theta(\sqrt{n})$, "random tape" adversarially robust algorithms need
$r^{\Omega(1)}$ space, while "random oracle" adversarially robust algorithms,
which can read from a long random string for free, may use
$O(\text{polylog}(r))$ space. The space lower bound for the "random seed" case
follows, by a reduction given in prior work, from a lower bound for
pseudo-deterministic streaming algorithms given in this paper.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03635" title="Abstract">arXiv:2310.03635</a> [<a href="/pdf/2310.03635" title="Download PDF">pdf</a>, <a href="/format/2310.03635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLEVRER-Humans: Describing Physical and Causal Events the Human Way
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jiayuan Mao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xuelin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Goodman%2C+N+D">Noah D. Goodman</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2022 (Dataset and Benchmark Track). First two authors contributed equally. Project page: <a href="https://sites.google.com/stanford.edu/clevrer-humans/home">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Building machines that can reason about physical events and their causal
relationships is crucial for flexible interaction with the physical world.
However, most existing physical and causal reasoning benchmarks are exclusively
based on synthetically generated events and synthetic natural language
descriptions of causal relationships. This design brings up two issues. First,
there is a lack of diversity in both event types and natural language
descriptions; second, causal relationships based on manually-defined heuristics
are different from human judgments. To address both shortcomings, we present
the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of
physical events with human labels. We employ two techniques to improve data
collection efficiency: first, a novel iterative event cloze task to elicit a
new representation of events in videos, which we term Causal Event Graphs
(CEGs); second, a data augmentation technique based on neural language
generative models. We convert the collected CEGs into questions and answers to
be consistent with prior work. Finally, we study a collection of baseline
approaches for CLEVRER-Humans question-answering, highlighting the great
challenges set forth by our benchmark.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03637" title="Abstract">arXiv:2310.03637</a> [<a href="/pdf/2310.03637" title="Download PDF">pdf</a>, <a href="/ps/2310.03637" title="Download PostScript">ps</a>, <a href="/format/2310.03637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Degree Bounds For Iterated Polynomial Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steiner%2C+M+J">Matthias Johann Steiner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Commutative Algebra (math.AC)

</div>
<p class="mathjax">For Arithmetization-Oriented ciphers and hash functions Gr\"obner basis
attacks are generally considered as the most competitive attack vector.
Unfortunately, the complexity of Gr\"obner basis algorithms is only understood
for special cases, and it is needless to say that these cases do not apply to
most cryptographic polynomial systems. Therefore, cryptographers have to resort
to experiments, extrapolations and hypotheses to assess the security of their
designs. One established measure to quantify the complexity of linear
algebra-based Gr\"obner basis algorithms is the so-called solving degree.
Caminata \&amp; Gorla revealed that under a certain genericity condition on a
polynomial system the solving degree is always upper bounded by the
Castelnuovo-Mumford regularity and henceforth by the Macaulay bound, which only
takes the degrees and number of variables of the input polynomials into
account. In this paper we extend their framework to iterated polynomial
systems, the standard polynomial model for symmetric ciphers and hash
functions. In particular, we prove solving degree bounds for various attacks on
MiMC, Feistel-MiMC, Feistel-MiMC-Hash, Hades and GMiMC. Our bounds fall in line
with the hypothesized complexity of Gr\"obner basis attacks on these designs,
and to the best of our knowledge this is the first time that a mathematical
proof for these complexities is provided.
<br />Moreover, by studying polynomials with degree falls we can prove lower bounds
on the Castelnuovo-Mumford regularity for attacks on MiMC, Feistel-MiMC and
Feistel-MiMC-Hash provided that only a few solutions of the corresponding
iterated polynomial system originate from the base field. Hence,
regularity-based solving degree estimations can never surpass a certain
threshold, a desirable property for cryptographic polynomial systems.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03639" title="Abstract">arXiv:2310.03639</a> [<a href="/pdf/2310.03639" title="Download PDF">pdf</a>, <a href="/ps/2310.03639" title="Download PostScript">ps</a>, <a href="/format/2310.03639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Self-Supervised Speech Representations for Indigenous  American Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chih-Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">William Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zevallos%2C+R">Rodolfo Zevallos</a>, 
<a href="/search/cs?searchtype=author&query=Ortega%2C+J">John Ortega</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The application of self-supervision to speech representation learning has
garnered significant interest in recent years, due to its scalability to large
amounts of unlabeled data. However, much progress, both in terms of
pre-training and downstream evaluation, has remained concentrated in
monolingual models that only consider English. Few models consider other
languages, and even fewer consider indigenous ones. In our submission to the
New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR
corpus for Quechua, an indigenous South American Language. We benchmark the
efficacy of large SSL models on Quechua, along with 6 other indigenous
languages such as Guarani and Bribri, on low-resource ASR. Our results show
surprisingly strong performance by state-of-the-art SSL models, showing the
potential generalizability of large-scale models to real-world data.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03641" title="Abstract">arXiv:2310.03641</a> [<a href="/pdf/2310.03641" title="Download PDF">pdf</a>, <a href="/format/2310.03641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional PAC-Learning from Nisan&#x27;s Natural Proofs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karchmer%2C+A">Ari Karchmer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">(Abridged) Carmosino et al. (2016) demonstrated that natural proofs of
circuit lower bounds for \Lambda imply efficient algorithms for learning
\Lambda-circuits, but only over the uniform distribution, with membership
queries, and provided \AC^0[p] \subseteq \Lambda. We consider whether this
implication can be generalized to \Lambda \not\supseteq \AC^0[p], and to
learning algorithms in Valiant's PAC model, which use only random examples and
learn over arbitrary example distributions. We give results of both positive
and negative flavor.
<br />On the negative side, we observe that if, for every circuit class \Lambda,
the implication from natural proofs for \Lambda to learning \Lambda-circuits in
Valiant's PAC model holds, then there is a polynomial time solution to
O(n^{1.5})-uSVP (unique Shortest Vector Problem), and polynomial time quantum
solutions to O(n^{1.5})-SVP (Shortest Vector Problem) and O(n^{1.5})-SIVP
(Shortest Independent Vector Problem). This indicates that whether natural
proofs for \Lambda imply efficient learning algorithms for \Lambda in Valiant's
PAC model may depend on \Lambda.
<br />On the positive side, our main result is that specific natural proofs arising
from a type of communication complexity argument (e.g., Nisan (1993), for
depth-2 majority circuits) imply PAC-learning algorithms in a new
distributional variant of Valiant's model. Our distributional PAC model is
stronger than the average-case prediction model of Blum et al (1993) and the
heuristic PAC model of Nanashima (2021), and has several important properties
which make it of independent interest, such as being boosting-friendly. The
main applications of our result are new distributional PAC-learning algorithms
for depth-2 majority circuits, polytopes and DNFs over natural target
distributions, as well as the nonexistence of encoded-input weak PRFs that can
be evaluated by depth-2 majority circuits.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03642" title="Abstract">arXiv:2310.03642</a> [<a href="/pdf/2310.03642" title="Download PDF">pdf</a>, <a href="/format/2310.03642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep surrogate model for learning Green&#x27;s function associated with  linear reaction-diffusion operator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ji%2C+J">Junqing Ji</a>, 
<a href="/search/math?searchtype=author&query=Ju%2C+L">Lili Ju</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xiaoping Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we present a deep surrogate model for learning the Green's
function associated with the reaction-diffusion operator in rectangular domain.
The U-Net architecture is utilized to effectively capture the mapping from
source to solution of the target partial differential equations (PDEs). To
enable efficient training of the model without relying on labeled data, we
propose a novel loss function that draws inspiration from traditional numerical
methods used for solving PDEs. Furthermore, a hard encoding mechanism is
employed to ensure that the predicted Green's function is perfectly matched
with the boundary conditions. Based on the learned Green's function from the
trained deep surrogate model, a fast solver is developed to solve the
corresponding PDEs with different sources and boundary conditions. Various
numerical examples are also provided to demonstrate the effectiveness of the
proposed model.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03646" title="Abstract">arXiv:2310.03646</a> [<a href="/pdf/2310.03646" title="Download PDF">pdf</a>, <a href="/ps/2310.03646" title="Download PostScript">ps</a>, <a href="/format/2310.03646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRAM: Bridging Trust Regions and Sharpness Aware Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sherborne%2C+T">Tom Sherborne</a>, 
<a href="/search/cs?searchtype=author&query=Saphra%2C+N">Naomi Saphra</a>, 
<a href="/search/cs?searchtype=author&query=Dasigi%2C+P">Pradeep Dasigi</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 tables, 1 figure. Submitted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">By reducing the curvature of the loss surface in the parameter space,
Sharpness-aware minimization (SAM) yields widespread robustness improvement
under domain transfer. Instead of focusing on parameters, however, this work
considers the transferability of representations as the optimization target for
out-of-domain generalization in a fine-tuning setup. To encourage the retention
of transferable representations, we consider trust region-based fine-tuning
methods, which exploit task-specific skills without forgetting task-agnostic
representations from pre-training. We unify parameter- and representation-space
smoothing approaches by using trust region bounds to inform SAM-style
regularizers on both of these optimization surfaces. We propose Trust Region
Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat
minima and smooth, informative representations without forgetting pre-trained
structure. We find that TRAM outperforms both sharpness-aware and trust
region-based optimization methods on cross-domain language modeling and
cross-lingual transfer, where robustness to domain transfer and representation
generality are critical for success. TRAM establishes a new standard in
training generalizable models with minimal additional computation.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03647" title="Abstract">arXiv:2310.03647</a> [<a href="/pdf/2310.03647" title="Download PDF">pdf</a>, <a href="/ps/2310.03647" title="Download PostScript">ps</a>, <a href="/format/2310.03647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Fairness for Human-AI Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+H">Haosen Ge</a>, 
<a href="/search/cs?searchtype=author&query=Bastani%2C+H">Hamsa Bastani</a>, 
<a href="/search/cs?searchtype=author&query=Bastani%2C+O">Osbert Bastani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Existing approaches to algorithmic fairness aim to ensure equitable outcomes
if human decision-makers comply perfectly with algorithmic decisions. However,
perfect compliance with the algorithm is rarely a reality or even a desirable
outcome in human-AI collaboration. Yet, recent studies have shown that
selective compliance with fair algorithms can amplify discrimination relative
to the prior human policy. As a consequence, ensuring equitable outcomes
requires fundamentally different algorithmic design principles that ensure
robustness to the decision-maker's (a priori unknown) compliance pattern. We
define the notion of compliance-robustly fair algorithmic recommendations that
are guaranteed to (weakly) improve fairness in decisions, regardless of the
human's compliance pattern. We propose a simple optimization strategy to
identify the best performance-improving compliance-robustly fair policy.
However, we show that it may be infeasible to design algorithmic
recommendations that are simultaneously fair in isolation, compliance-robustly
fair, and more accurate than the human policy; thus, if our goal is to improve
the equity and accuracy of human-AI collaboration, it may not be desirable to
enforce traditional fairness constraints.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03652" title="Abstract">arXiv:2310.03652</a> [<a href="/pdf/2310.03652" title="Download PDF">pdf</a>, <a href="/format/2310.03652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extreme sparsification of physics-augmented neural networks for  interpretable model discovery in mechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fuhg%2C+J+N">Jan N. Fuhg</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+R+E">Reese E. Jones</a>, 
<a href="/search/cs?searchtype=author&query=Bouklas%2C+N">Nikolaos Bouklas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 19 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Data-driven constitutive modeling with neural networks has received increased
interest in recent years due to its ability to easily incorporate physical and
mechanistic constraints and to overcome the challenging and time-consuming task
of formulating phenomenological constitutive laws that can accurately capture
the observed material response. However, even though neural network-based
constitutive laws have been shown to generalize proficiently, the generated
representations are not easily interpretable due to their high number of
trainable parameters. Sparse regression approaches exist that allow to
obtaining interpretable expressions, but the user is tasked with creating a
library of model forms which by construction limits their expressiveness to the
functional forms provided in the libraries. In this work, we propose to train
regularized physics-augmented neural network-based constitutive models
utilizing a smoothed version of $L^{0}$-regularization. This aims to maintain
the trustworthiness inherited by the physical constraints, but also enables
interpretability which has not been possible thus far on any type of machine
learning-based constitutive model where model forms were not assumed a-priory
but were actually discovered. During the training process, the network
simultaneously fits the training data and penalizes the number of active
parameters, while also ensuring constitutive constraints such as thermodynamic
consistency. We show that the method can reliably obtain interpretable and
trustworthy constitutive models for compressible and incompressible
hyperelasticity, yield functions, and hardening models for elastoplasticity,
for synthetic and experimental data.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03655" title="Abstract">arXiv:2310.03655</a> [<a href="/pdf/2310.03655" title="Download PDF">pdf</a>, <a href="/format/2310.03655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Evaluation: Subjects, Evaluators, and Society
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laufer%2C+B">Benjamin Laufer</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+J">Jon Kleinberg</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+K">Karen Levy</a>, 
<a href="/search/cs?searchtype=author&query=Nissenbaum%2C+H">Helen Nissenbaum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 figures, EAAMO 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 ACM Conference on Equity and Access in Algorithms,
  Mechanisms, and Optimization (EAAMO '23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
<p class="mathjax">A broad current application of algorithms is in formal and quantitative
measures of murky concepts -- like merit -- to make decisions. When people
strategically respond to these sorts of evaluations in order to gain favorable
decision outcomes, their behavior can be subjected to moral judgments. They may
be described as 'gaming the system' or 'cheating,' or (in other cases)
investing 'honest effort' or 'improving.' Machine learning literature on
strategic behavior has tried to describe these dynamics by emphasizing the
efforts expended by decision subjects hoping to obtain a more favorable
assessment -- some works offer ways to preempt or prevent such manipulations,
some differentiate 'gaming' from 'improvement' behavior, while others aim to
measure the effort burden or disparate effects of classification systems. We
begin from a different starting point: that the design of an evaluation itself
can be understood as furthering goals held by the evaluator which may be
misaligned with broader societal goals. To develop the idea that evaluation
represents a strategic interaction in which both the evaluator and the subject
of their evaluation are operating out of self-interest, we put forward a model
that represents the process of evaluation using three interacting agents: a
decision subject, an evaluator, and society, representing a bundle of values
and oversight mechanisms. We highlight our model's applicability to a number of
social systems where one or two players strategically undermine the others'
interests to advance their own. Treating evaluators as themselves strategic
allows us to re-cast the scrutiny directed at decision subjects, towards the
incentives that underpin institutional designs of evaluations. The moral
standing of strategic behaviors often depend on the moral standing of the
evaluations and incentives that provoke such behaviors.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03657" title="Abstract">arXiv:2310.03657</a> [<a href="/pdf/2310.03657" title="Download PDF">pdf</a>, <a href="/format/2310.03657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Load Forecasting of Distribution Power Systems based on  Empirical Copulas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Austnes%2C+P+F">P&#xe5;l Forr Austnes</a>, 
<a href="/search/eess?searchtype=author&query=Garc%C3%ADa-Pareja%2C+C">Celia Garc&#xed;a-Pareja</a>, 
<a href="/search/eess?searchtype=author&query=Nobile%2C+F">Fabio Nobile</a>, 
<a href="/search/eess?searchtype=author&query=Paolone%2C+M">Mario Paolone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to PSCC 2024. <a href="https://pscc2024.fr/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Accurate and reliable electricity load forecasts are becoming increasingly
important as the share of intermittent resources in the system increases.
Distribution System Operators (DSOs) are called to accurately forecast their
production and consumption to place optimal bids in the day-ahead market.
Violations of their dispatch-plan requires activation of reserve-power which
has a direct cost for the DSO, and also necessitate available reserve-capacity.
Forecasts must account for the volatility of weather-parameters that impacts
both the production and consumption of electricity. If DSO-loads are small or
lower-granularity forecasts are needed, traditional statistical methods may
fail to provide reliable performance since they rely on a priori statistical
distributions of the variables to forecast. In this paper we introduce a
probabilistic load forecast (PLF) method based on empirical copulas. Our model
is data-driven, does not need a priori assumption on parametric distribution
for variables, nor the dependence structure (copula), but employs a kernel
density estimate of the underlying distribution using beta kernels that have
bounded support on the unit hypercube. The method naturally supports variables
with widely different distributions, such as weather data (including forecasted
ones) and historic electricity consumption, and produces a conditional
probability distribution for every time step in the forecast, which allows
inferring the quantiles of interest. The proposed non-parametric approach is
highly flexible and can produce meaningful forecasts even at very low
aggregated levels (e.g. neighborhoods). We present results from an open dataset
and showcase the strength of the model with respect to Quantile Regression
using standard probabilistic evaluation metrics.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03658" title="Abstract">arXiv:2310.03658</a> [<a href="/pdf/2310.03658" title="Download PDF">pdf</a>, <a href="/format/2310.03658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual inspection for illicit items in X-ray images using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mademlis%2C+I">Ioannis Mademlis</a>, 
<a href="/search/cs?searchtype=author&query=Batsis%2C+G">Georgios Batsis</a>, 
<a href="/search/cs?searchtype=author&query=Chrysochoou%2C+A+A+R">Adamantia Anna Rebolledo Chrysochoou</a>, 
<a href="/search/cs?searchtype=author&query=Papadopoulos%2C+G+T">Georgios Th. Papadopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Automated detection of contraband items in X-ray images can significantly
increase public safety, by enhancing the productivity and alleviating the
mental load of security officers in airports, subways, customs/post offices,
etc. The large volume and high throughput of passengers, mailed parcels, etc.,
during rush hours practically make it a Big Data problem. Modern computer
vision algorithms relying on Deep Neural Networks (DNNs) have proven capable of
undertaking this task even under resource-constrained and embedded execution
scenarios, e.g., as is the case with fast, single-stage object detectors.
However, no comparative experimental assessment of the various relevant DNN
components/methods has been performed under a common evaluation protocol, which
means that reliable cross-method comparisons are missing. This paper presents
exactly such a comparative assessment, utilizing a public relevant dataset and
a well-defined methodology for selecting the specific DNN components/modules
that are being evaluated. The results indicate the superiority of Transformer
detectors, the obsolete nature of auxiliary neural modules that have been
developed in the past few years for security applications and the efficiency of
the CSP-DarkNet backbone CNN.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03659" title="Abstract">arXiv:2310.03659</a> [<a href="/pdf/2310.03659" title="Download PDF">pdf</a>, <a href="/format/2310.03659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for  Autonomous LLM-powered Multi-Agent Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%A4ndler%2C+T">Thorsten H&#xe4;ndler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA); Software Engineering (cs.SE)

</div>
<p class="mathjax">Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03661" title="Abstract">arXiv:2310.03661</a> [<a href="/pdf/2310.03661" title="Download PDF">pdf</a>, <a href="/format/2310.03661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness-Guided Image Synthesis for Data-Free Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jianhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuchen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+H">Huanpeng Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hualiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruizhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaoxuan He</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+L">Lianrui Mu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+C">Chengfei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haoji Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Quantization has emerged as a promising direction for model compression.
Recently, data-free quantization has been widely studied as a promising method
to avoid privacy concerns, which synthesizes images as an alternative to real
training data. Existing methods use classification loss to ensure the
reliability of the synthesized images. Unfortunately, even if these images are
well-classified by the pre-trained model, they still suffer from low semantics
and homogenization issues. Intuitively, these low-semantic images are sensitive
to perturbations, and the pre-trained model tends to have inconsistent output
when the generator synthesizes an image with poor semantics. To this end, we
propose Robustness-Guided Image Synthesis (RIS), a simple but effective method
to enrich the semantics of synthetic images and improve image diversity,
further boosting the performance of downstream data-free compression tasks.
Concretely, we first introduce perturbations on input and model weight, then
define the inconsistency metrics at feature and prediction levels before and
after perturbations. On the basis of inconsistency on two levels, we design a
robustness optimization objective to enhance the semantics of synthetic images.
Moreover, we also make our approach diversity-aware by forcing the generator to
synthesize images with small correlations in the label space. With RIS, we
achieve state-of-the-art performance for various settings on data-free
quantization and can be extended to other data-free compression tasks.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03665" title="Abstract">arXiv:2310.03665</a> [<a href="/pdf/2310.03665" title="Download PDF">pdf</a>, <a href="/format/2310.03665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> POLYLLA: Polygonal/Polyhedral meshing algorithm based on terminal-edge  regions and terminal-face regions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salinas-Fern%C3%A1ndez%2C+S">Sergio Salinas-Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Hitschfeld-Kahler%2C+N">Nancy Hitschfeld-Kahler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Polylla is a polygonal mesh algorithm that generates meshes with arbitrarily
shaped polygons using the concept of terminal-edge regions. Until now, Polylla
has been limited to 2D meshes, but in this work, we extend Polylla to 3D
volumetric meshes. We present two versions of Polylla 3D. The first version
generates terminal-edge regions, converts them into polyhedra, and repairs
polyhedra that are joined by only an edge. This version differs from the
original Polylla algorithm in that it does not have the same phases as the 2D
version. In the second version, we define two new concepts: longest-face
propagation path and terminal-face regions. We use these concepts to create an
almost direct extension of the 2D Polylla mesh with the same three phases:
label phase, traversal phase, and repair phase.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03666" title="Abstract">arXiv:2310.03666</a> [<a href="/pdf/2310.03666" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MapperGPT: Large Language Models for Linking and Mapping Entities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matentzoglu%2C+N">Nicolas Matentzoglu</a>, 
<a href="/search/cs?searchtype=author&query=Caufield%2C+J+H">J. Harry Caufield</a>, 
<a href="/search/cs?searchtype=author&query=Hegde%2C+H+B">Harshad B. Hegde</a>, 
<a href="/search/cs?searchtype=author&query=Reese%2C+J+T">Justin T. Reese</a>, 
<a href="/search/cs?searchtype=author&query=Moxon%2C+S">Sierra Moxon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeongsik Kim</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+N+L">Nomi L. Harris</a>, 
<a href="/search/cs?searchtype=author&query=Haendel%2C+M+A">Melissa A Haendel</a>, 
<a href="/search/cs?searchtype=author&query=Mungall%2C+C+J">Christopher J. Mungall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Aligning terminological resources, including ontologies, controlled
vocabularies, taxonomies, and value sets is a critical part of data integration
in many domains such as healthcare, chemistry, and biomedical research. Entity
mapping is the process of determining correspondences between entities across
these resources, such as gene identifiers, disease concepts, or chemical entity
identifiers. Many tools have been developed to compute such mappings based on
common structural features and lexical information such as labels and synonyms.
Lexical approaches in particular often provide very high recall, but low
precision, due to lexical ambiguity. As a consequence of this, mapping efforts
often resort to a labor intensive manual mapping refinement through a human
curator.
<br />Large Language Models (LLMs), such as the ones employed by ChatGPT, have
generalizable abilities to perform a wide range of tasks, including
question-answering and information extraction. Here we present MapperGPT, an
approach that uses LLMs to review and refine mapping relationships as a
post-processing step, in concert with existing high-recall methods that are
based on lexical and structural heuristics.
<br />We evaluated MapperGPT on a series of alignment tasks from different domains,
including anatomy, developmental biology, and renal diseases. We devised a
collection of tasks that are designed to be particularly challenging for
lexical methods. We show that when used in combination with high-recall
methods, MapperGPT can provide a substantial improvement in accuracy, beating
state-of-the-art (SOTA) methods such as LogMap.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03667" title="Abstract">arXiv:2310.03667</a> [<a href="/pdf/2310.03667" title="Download PDF">pdf</a>, <a href="/format/2310.03667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Exfiltration Path Analysis Using Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rishu%2C+R">Riddam Rishu</a>, 
<a href="/search/cs?searchtype=author&query=Kakkar%2C+A">Akshay Kakkar</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+A">Abdul Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Redino%2C+C">Christopher Redino</a>, 
<a href="/search/cs?searchtype=author&query=Nandakumar%2C+D">Dhruv Nandakumar</a>, 
<a href="/search/cs?searchtype=author&query=Cody%2C+T">Tyler Cody</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+R">Ryan Clark</a>, 
<a href="/search/cs?searchtype=author&query=Radke%2C+D">Daniel Radke</a>, 
<a href="/search/cs?searchtype=author&query=Bowen%2C+E">Edward Bowen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Building on previous work using reinforcement learning (RL) focused on
identification of exfiltration paths, this work expands the methodology to
include protocol and payload considerations. The former approach to
exfiltration path discovery, where reward and state are associated specifically
with the determination of optimal paths, are presented with these additional
realistic characteristics to account for nuances in adversarial behavior. The
paths generated are enhanced by including communication payload and protocol
into the Markov decision process (MDP) in order to more realistically emulate
attributes of network based exfiltration events. The proposed method will help
emulate complex adversarial considerations such as the size of a payload being
exported over time or the protocol on which it occurs, as is the case where
threat actors steal data over long periods of time using system native ports or
protocols to avoid detection. As such, practitioners will be able to improve
identification of expected adversary behavior under various payload and
protocol assumptions more comprehensively.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03668" title="Abstract">arXiv:2310.03668</a> [<a href="/pdf/2310.03668" title="Download PDF">pdf</a>, <a href="/format/2310.03668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sainz%2C+O">Oscar Sainz</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ferrero%2C+I">Iker Garc&#xed;a-Ferrero</a>, 
<a href="/search/cs?searchtype=author&query=Agerri%2C+R">Rodrigo Agerri</a>, 
<a href="/search/cs?searchtype=author&query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
<a href="/search/cs?searchtype=author&query=Rigau%2C+G">German Rigau</a>, 
<a href="/search/cs?searchtype=author&query=Agirre%2C+E">Eneko Agirre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) combined with instruction tuning have made
significant progress when generalizing to unseen tasks. However, they have been
less successful in Information Extraction (IE), lagging behind task-specific
models. Typically, IE tasks are characterized by complex annotation guidelines
which describe the task and give examples to humans. Previous attempts to
leverage such information have failed, even with the largest models, as they
are not able to follow the guidelines out-of-the-box. In this paper we propose
GoLLIE (Guideline-following Large Language Model for IE), a model able to
improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to
comply with annotation guidelines. Comprehensive evaluation empirically
demonstrates that GoLLIE is able to generalize to and follow unseen guidelines,
outperforming previous attempts at zero-shot information extraction. The
ablation study shows that detailed guidelines is key for good results.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03669" title="Abstract">arXiv:2310.03669</a> [<a href="/pdf/2310.03669" title="Download PDF">pdf</a>, <a href="/format/2310.03669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LumiNet: The Bright Side of Perceptual Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+M+I">Md. Ismail Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Elahi%2C+M+M+L">M M Lutfe Elahi</a>, 
<a href="/search/cs?searchtype=author&query=Ramasinghe%2C+S">Sameera Ramasinghe</a>, 
<a href="/search/cs?searchtype=author&query=Cheraghian%2C+A">Ali Cheraghian</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+F">Fuad Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Mohammed%2C+N">Nabeel Mohammed</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+S">Shafin Rahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In knowledge distillation research, feature-based methods have dominated due
to their ability to effectively tap into extensive teacher models. In contrast,
logit-based approaches are considered to be less adept at extracting hidden
'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novel
knowledge-transfer algorithm designed to enhance logit-based distillation. We
introduce a perception matrix that aims to recalibrate logits through
adjustments based on the model's representation capability. By meticulously
analyzing intra-class dynamics, LumiNet reconstructs more granular inter-class
relationships, enabling the student model to learn a richer breadth of
knowledge. Both teacher and student models are mapped onto this refined matrix,
with the student's goal being to minimize representational discrepancies.
Rigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO)
attests to LumiNet's efficacy, revealing its competitive edge over leading
feature-based methods. Moreover, in exploring the realm of transfer learning,
we assess how effectively the student model, trained using our method, adapts
to downstream tasks. Notably, when applied to Tiny ImageNet, the transferred
features exhibit remarkable performance, further underscoring LumiNet's
versatility and robustness in diverse settings. With LumiNet, we hope to steer
the research discourse towards a renewed interest in the latent capabilities of
logit-based knowledge distillation.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03670" title="Abstract">arXiv:2310.03670</a> [<a href="/pdf/2310.03670" title="Download PDF">pdf</a>, <a href="/ps/2310.03670" title="Download PostScript">ps</a>, <a href="/format/2310.03670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regress Before Construct: Regress Autoencoder for Point Cloud  Self-supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Can Wang</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+X">Xulin King</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengyuan Liu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the 31st ACM International Conference on
  Multimedia (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Masked Autoencoders (MAE) have demonstrated promising performance in
self-supervised learning for both 2D and 3D computer vision. Nevertheless,
existing MAE-based methods still have certain drawbacks. Firstly, the
functional decoupling between the encoder and decoder is incomplete, which
limits the encoder's representation learning ability. Secondly, downstream
tasks solely utilize the encoder, failing to fully leverage the knowledge
acquired through the encoder-decoder architecture in the pre-text task. In this
paper, we propose Point Regress AutoEncoder (Point-RAE), a new scheme for
regressive autoencoders for point cloud self-supervised learning. The proposed
method decouples functions between the decoder and the encoder by introducing a
mask regressor, which predicts the masked patch representation from the visible
patch representation encoded by the encoder and the decoder reconstructs the
target from the predicted masked patch representation. By doing so, we minimize
the impact of decoder updates on the representation space of the encoder.
Moreover, we introduce an alignment constraint to ensure that the
representations for masked patches, predicted from the encoded representations
of visible patches, are aligned with the masked patch presentations computed
from the encoder. To make full use of the knowledge learned in the pre-training
stage, we design a new finetune mode for the proposed Point-RAE. Extensive
experiments demonstrate that our approach is efficient during pre-training and
generalizes well on various downstream tasks. Specifically, our pre-trained
models achieve a high accuracy of \textbf{90.28\%} on the ScanObjectNN hardest
split and \textbf{94.1\%} accuracy on ModelNet40, surpassing all the other
self-supervised learning methods. Our code and pretrained model are public
available at: \url{https://github.com/liuyyy111/Point-RAE}.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03673" title="Abstract">arXiv:2310.03673</a> [<a href="/pdf/2310.03673" title="Download PDF">pdf</a>, <a href="/ps/2310.03673" title="Download PostScript">ps</a>, <a href="/format/2310.03673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Internal Software Metrics Have Relationship with Fault-proneness and  Change-proneness?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M">Md.Masudur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Ahammed%2C+T">Toukir Ahammed</a>, 
<a href="/search/cs?searchtype=author&query=Sakib%2C+K">Kazi Sakib</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages and two tables. arXiv admin note: text overlap with <a href="/abs/2305.05572">arXiv:2305.05572</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Change-proneness indicates the possibility of changes to a software system.
Both of these measures are related to software maintainability which impact
internal software metrics such as size, inheritance, coupling, etc. due to
making many changes to the system. In the literature, change- and
fault-proneness have been predicted using the internal software metrics which
are almost one decade earlier. Therefore, as software systems and structures
are evolving in nature, we present an empirical study to revisit the
relationship of the internal software metrics with change- and fault-proneness
to provide up-to-date insights. In particular, we identify 25 internal software
metrics, change-proneness and fault-proneness in the wellknown open source
systems from Apache and Eclipse ecosystems. Then we analyse the relationship
based on the statistical correlation method. The results show that almost all
of the metrics have no or low relationship with fault-proneness, while
inheritance, coupling and comments-related metrics have a moderate or high
relationship with change-proneness. These findings will assist developers to
minimize the higher related software metrics to enhance maintainability in
terms of change- and fault-proneness. In addition, these also help researchers
to innovate change and fault prediction approaches by incorporating the higher
related metrics.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03675" title="Abstract">arXiv:2310.03675</a> [<a href="/pdf/2310.03675" title="Download PDF">pdf</a>, <a href="/format/2310.03675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hadamard Domain Training with Integers for Class Incremental Quantized  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schiemer%2C+M">Martin Schiemer</a>, 
<a href="/search/cs?searchtype=author&query=Schaefer%2C+C+J">Clemens JS Schaefer</a>, 
<a href="/search/cs?searchtype=author&query=Vap%2C+J+P">Jayden Parker Vap</a>, 
<a href="/search/cs?searchtype=author&query=Horeni%2C+M+J">Mark James Horeni</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y+E">Yu Emma Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Juan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+S">Siddharth Joshi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Continual learning is a desirable feature in many modern machine learning
applications, which allows in-field adaptation and updating, ranging from
accommodating distribution shift, to fine-tuning, and to learning new tasks.
For applications with privacy and low latency requirements, the compute and
memory demands imposed by continual learning can be cost-prohibitive for
resource-constraint edge platforms. Reducing computational precision through
fully quantized training (FQT) simultaneously reduces memory footprint and
increases compute efficiency for both training and inference. However,
aggressive quantization especially integer FQT typically degrades model
accuracy to unacceptable levels. In this paper, we propose a technique that
leverages inexpensive Hadamard transforms to enable low-precision training with
only integer matrix multiplications. We further determine which tensors need
stochastic rounding and propose tiled matrix multiplication to enable low-bit
width accumulators. We demonstrate the effectiveness of our technique on
several human activity recognition datasets and CIFAR100 in a class incremental
learning setting. We achieve less than 0.5% and 3% accuracy degradation while
we quantize all matrix multiplications inputs down to 4-bits with 8-bit
accumulators.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03676" title="Abstract">arXiv:2310.03676</a> [<a href="/pdf/2310.03676" title="Download PDF">pdf</a>, <a href="/ps/2310.03676" title="Download PostScript">ps</a>, <a href="/format/2310.03676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PV-OSIMr: A Lowest Order Complexity Algorithm for Computing the Delassus  Matrix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sathya%2C+A+S">Ajay Suresha Sathya</a>, 
<a href="/search/cs?searchtype=author&query=Decre%2C+W">Wilm Decre</a>, 
<a href="/search/cs?searchtype=author&query=Swevers%2C+J">Jan Swevers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, submitted for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present PV-OSIMr, an efficient algorithm for computing the Delassus matrix
(also known as the inverse operational space inertia matrix) for a kinematic
tree, with the lowest order computational complexity known in literature.
PV-OSIMr is derived by optimizing the Popov-Vereshchagin (PV) solver
computations using the compositionality of the force and motion propagators. It
has a computational complexity of O(n + m^2 ) compared to O(n + m^2d) of the
original PV-OSIM algorithm and O(n+md+m^2 ) of the extended force propagator
algorithm (EFPA), where n is the number of joints, m is the number of
constraints and d is the depth of the kinematic tree. Since Delassus matrix
computation requires constructing an m x m sized matrix and must consider all
the n joints at least once, the asymptotic computational complexity of PV-OSIMr
is optimal. We further benchmark our algorithm and find it to be often more
efficient than the PV-OSIM and EFPA in practice.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03684" title="Abstract">arXiv:2310.03684</a> [<a href="/pdf/2310.03684" title="Download PDF">pdf</a>, <a href="/format/2310.03684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Robey%2C+A">Alexander Robey</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+E">Eric Wong</a>, 
<a href="/search/cs?searchtype=author&query=Hassani%2C+H">Hamed Hassani</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+G+J">George J. Pappas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Despite efforts to align large language models (LLMs) with human values,
widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to
jailbreaking attacks, wherein an adversary fools a targeted LLM into generating
objectionable content. To address this vulnerability, we propose SmoothLLM, the
first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our
finding that adversarially-generated prompts are brittle to character-level
changes, our defense first randomly perturbs multiple copies of a given input
prompt, and then aggregates the corresponding predictions to detect adversarial
inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to
below one percentage point, avoids unnecessary conservatism, and admits
provable guarantees on attack mitigation. Moreover, our defense uses
exponentially fewer queries than existing attacks and is compatible with any
LLM.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03686" title="Abstract">arXiv:2310.03686</a> [<a href="/pdf/2310.03686" title="Download PDF">pdf</a>, <a href="/format/2310.03686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Langedijk%2C+A">Anna Langedijk</a>, 
<a href="/search/cs?searchtype=author&query=Mohebbi%2C+H">Hosein Mohebbi</a>, 
<a href="/search/cs?searchtype=author&query=Sarti%2C+G">Gabriele Sarti</a>, 
<a href="/search/cs?searchtype=author&query=Zuidema%2C+W">Willem Zuidema</a>, 
<a href="/search/cs?searchtype=author&query=Jumelet%2C+J">Jaap Jumelet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In recent years, many interpretability methods have been proposed to help
interpret the internal states of Transformer-models, at different levels of
precision and complexity. Here, to analyze encoder-decoder Transformers, we
propose a simple, new method: DecoderLens. Inspired by the LogitLens (for
decoder-only Transformers), this method involves allowing the decoder to
cross-attend representations of intermediate encoder layers instead of using
the final encoder output, as is normally done in encoder-decoder models. The
method thus maps previously uninterpretable vector representations to
human-interpretable sequences of words or symbols. We report results from the
DecoderLens applied to models trained on question answering, logical reasoning,
speech recognition and machine translation. The DecoderLens reveals several
specific subtasks that are solved at low or intermediate layers, shedding new
light on the information flow inside the encoder component of this important
class of models.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03687" title="Abstract">arXiv:2310.03687</a> [<a href="/pdf/2310.03687" title="Download PDF">pdf</a>, <a href="/format/2310.03687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Generative Modeling for Procedural Roundabout Generation  for Developing Countries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ikram%2C+Z">Zarif Ikram</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dianbo Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages. Submitted to ReALML@NeurIPS (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Due to limited resources and fast economic growth, designing optimal
transportation road networks with traffic simulation and validation in a
cost-effective manner is vital for developing countries, where extensive manual
testing is expensive and often infeasible. Current rule-based road design
generators lack diversity, a key feature for design robustness. Generative Flow
Networks (GFlowNets) learn stochastic policies to sample from an unnormalized
reward distribution, thus generating high-quality solutions while preserving
their diversity. In this work, we formulate the problem of linking incident
roads to the circular junction of a roundabout by a Markov decision process,
and we leverage GFlowNets as the Junction-Art road generator. We compare our
method with related methods and our empirical results show that our method
achieves better diversity while preserving a high validity score.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03691" title="Abstract">arXiv:2310.03691</a> [<a href="/pdf/2310.03691" title="Download PDF">pdf</a>, <a href="/format/2310.03691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DirectGPT: A Direct Manipulation Interface to Interact with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masson%2C+D">Damien Masson</a>, 
<a href="/search/cs?searchtype=author&query=Malacria%2C+S">Sylvain Malacria</a>, 
<a href="/search/cs?searchtype=author&query=Casiez%2C+G">G&#xe9;ry Casiez</a>, 
<a href="/search/cs?searchtype=author&query=Vogel%2C+D">Daniel Vogel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">We characterize and demonstrate how the principles of direct manipulation can
improve interaction with large language models. This includes: continuous
representation of generated objects of interest; reuse of prompt syntax in a
toolbar of commands; manipulable outputs to compose or control the effect of
prompts; and undo mechanisms. This idea is exemplified in DirectGPT, a user
interface layer on top of ChatGPT that works by transforming direct
manipulation actions to engineered prompts. A study shows participants were 50%
faster and relied on 50% fewer and 72% shorter prompts to edit text, code, and
vector images compared to baseline ChatGPT. Our work contributes a validated
approach to integrate LLMs into traditional software using direct manipulation.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03693" title="Abstract">arXiv:2310.03693</a> [<a href="/pdf/2310.03693" title="Download PDF">pdf</a>, <a href="/format/2310.03693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning Aligned Language Models Compromises Safety, Even When Users  Do Not Intend To!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiangyu Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tinghao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Ruoxi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+P">Prateek Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+P">Peter Henderson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Optimizing large language models (LLMs) for downstream use cases often
involves the customization of pre-trained LLMs through further fine-tuning.
Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5
Turbo on custom datasets also encourage this practice. But, what are the safety
costs associated with such custom fine-tuning? We note that while existing
safety alignment infrastructures can restrict harmful behaviors of LLMs at
inference time, they do not cover safety risks when fine-tuning privileges are
extended to end-users. Our red teaming studies find that the safety alignment
of LLMs can be compromised by fine-tuning with only a few adversarially
designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety
guardrails by fine-tuning it on only 10 such examples at a cost of less than
$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful
instructions. Disconcertingly, our research also reveals that, even without
malicious intent, simply fine-tuning with benign and commonly used datasets can
also inadvertently degrade the safety alignment of LLMs, though to a lesser
extent. These findings suggest that fine-tuning aligned LLMs introduces new
safety risks that current safety infrastructures fall short of addressing --
even if a model's initial safety alignment is impeccable, it is not necessarily
to be maintained after custom fine-tuning. We outline and critically analyze
potential mitigations and advocate for further research efforts toward
reinforcing safety protocols for the custom fine-tuning of aligned LLMs.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03695" title="Abstract">arXiv:2310.03695</a> [<a href="/pdf/2310.03695" title="Download PDF">pdf</a>, <a href="/format/2310.03695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimarginal generative modeling with stochastic interpolants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Albergo%2C+M+S">Michael S. Albergo</a>, 
<a href="/search/cs?searchtype=author&query=Boffi%2C+N+M">Nicholas M. Boffi</a>, 
<a href="/search/cs?searchtype=author&query=Lindsey%2C+M">Michael Lindsey</a>, 
<a href="/search/cs?searchtype=author&query=Vanden-Eijnden%2C+E">Eric Vanden-Eijnden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR)

</div>
<p class="mathjax">Given a set of $K$ probability densities, we consider the multimarginal
generative modeling problem of learning a joint distribution that recovers
these densities as marginals. The structure of this joint distribution should
identify multi-way correspondences among the prescribed marginals. We formalize
an approach to this task within a generalization of the stochastic interpolant
framework, leading to efficient learning algorithms built upon dynamical
transport of measure. Our generative models are defined by velocity and score
fields that can be characterized as the minimizers of simple quadratic
objectives, and they are defined on a simplex that generalizes the time
variable in the usual dynamical transport framework. The resulting transport on
the simplex is influenced by all marginals, and we show that multi-way
correspondences can be extracted. The identification of such correspondences
has applications to style transfer, algorithmic fairness, and data
decorruption. In addition, the multimarginal perspective enables an efficient
algorithm for reducing the dynamical transport cost in the ordinary
two-marginal setting. We demonstrate these capacities with several numerical
examples.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03700" title="Abstract">arXiv:2310.03700</a> [<a href="/pdf/2310.03700" title="Download PDF">pdf</a>, <a href="/format/2310.03700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BrickStARt: Enabling In-situ Design and Tangible Exploration for  Personal Fabrication using Mixed Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stemasov%2C+E">Evgeny Stemasov</a>, 
<a href="/search/cs?searchtype=author&query=Hohn%2C+J">Jessica Hohn</a>, 
<a href="/search/cs?searchtype=author&query=Cordts%2C+M">Maurice Cordts</a>, 
<a href="/search/cs?searchtype=author&query=Schikorr%2C+A">Anja Schikorr</a>, 
<a href="/search/cs?searchtype=author&query=Rukzio%2C+E">Enrico Rukzio</a>, 
<a href="/search/cs?searchtype=author&query=Gugenheimer%2C+J">Jan Gugenheimer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 13 figures, to appear in: Proceedings of the ACM on Human-Computer Interaction, Vol. 7 Number ISS (PACM ISS), November 5-8, 2023, Pittsburgh, PA, USA
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the ACM on Human-Computer Interaction, Vol. 7, No.
  ISS (PACM ISS), 2023, Article 429
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">3D printers enable end-users to design and fabricate unique physical
artifacts but maintain an increased entry barrier and friction. End users must
design tangible artifacts through intangible media away from the main problem
space (ex-situ) and transfer spatial requirements to an abstract software
environment. To allow users to evaluate dimensions, balance, or fit early and
in-situ, we developed BrickStARt, a design tool using tangible construction
blocks paired with a mixed-reality headset. Users assemble a physical block
model at the envisioned location of the fabricated artifact. Designs can be
tested tangibly, refined, and digitally post-processed, remaining continuously
in-situ. We implemented BrickStARt using a Magic Leap headset and present
walkthroughs, highlighting novel interactions for 3D design. In a user study
(n=16), first-time 3D modelers succeeded more often using BrickStARt than
Tinkercad. Our results suggest that BrickStARt provides an accessible and
explorative process while facilitating quick, tangible design iterations that
allow users to detect physics-related issues (e.g., clearance) early on.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03702" title="Abstract">arXiv:2310.03702</a> [<a href="/pdf/2310.03702" title="Download PDF">pdf</a>, <a href="/ps/2310.03702" title="Download PostScript">ps</a>, <a href="/format/2310.03702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Analysis of Auction Equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hartline%2C+J">Jason Hartline</a>, 
<a href="/search/cs?searchtype=author&query=Hoy%2C+D">Darrell Hoy</a>, 
<a href="/search/cs?searchtype=author&query=Taggart%2C+S">Samuel Taggart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper provides an economic interpretation on results presented in an extended abstract under the title "Price of Anarchy for Auction Revenue" at the fifteenth ACM Conference on Economics and Computation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Equilibria in auctions can be very difficult to analyze, beyond the symmetric
environments where revenue equivalence renders the analysis straightforward.
This paper takes a robust approach to evaluating the equilibria of auctions.
Rather than identify the equilibria of an auction under specific environmental
conditions, it considers worst-case analysis, where an auction is evaluated
according to the worst environment and worst equilibrium in that environment.
It identifies a non-equilibrium property of auctions that governs whether or
not their worst-case equilibria are good for welfare and revenue. This property
is easy to analyze, can be refined from data, and composes across markets where
multiple auctions are run simultaneously.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03704" title="Abstract">arXiv:2310.03704</a> [<a href="/pdf/2310.03704" title="Download PDF">pdf</a>, <a href="/format/2310.03704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Drag View: Generalizable Novel View Synthesis with Unposed Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhiwen Fan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+P">Panwang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yifan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hanwen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dejia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zehao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dilin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce DragView, a novel and interactive framework for generating novel
views of unseen scenes. DragView initializes the new view from a single source
image, and the rendering is supported by a sparse set of unposed multi-view
images, all seamlessly executed within a single feed-forward pass. Our approach
begins with users dragging a source view through a local relative coordinate
system. Pixel-aligned features are obtained by projecting the sampled 3D points
along the target ray onto the source view. We then incorporate a view-dependent
modulation layer to effectively handle occlusion during the projection.
Additionally, we broaden the epipolar attention mechanism to encompass all
source pixels, facilitating the aggregation of initialized coordinate-aligned
point features from other unposed views. Finally, we employ another transformer
to decode ray features into final pixel intensities. Crucially, our framework
does not rely on either 2D prior models or the explicit estimation of camera
poses. During testing, DragView showcases the capability to generalize to new
scenes unseen during training, also utilizing only unposed support images,
enabling the generation of photo-realistic new views characterized by flexible
camera trajectories. In our experiments, we conduct a comprehensive comparison
of the performance of DragView with recent scene representation networks
operating under pose-free conditions, as well as with generalizable NeRFs
subject to noisy test camera poses. DragView consistently demonstrates its
superior performance in view synthesis quality, while also being more
user-friendly. Project page: https://zhiwenfan.github.io/DragView/.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03707" title="Abstract">arXiv:2310.03707</a> [<a href="/pdf/2310.03707" title="Download PDF">pdf</a>, <a href="/format/2310.03707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable  Evasion Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tal%2C+O+B">Ofir Bar Tal</a>, 
<a href="/search/cs?searchtype=author&query=Haviv%2C+A">Adi Haviv</a>, 
<a href="/search/cs?searchtype=author&query=Bermano%2C+A+H">Amit H. Bermano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023, AROW Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Evasion Attacks (EA) are used to test the robustness of trained neural
networks by distorting input data to misguide the model into incorrect
classifications. Creating these attacks is a challenging task, especially with
the ever-increasing complexity of models and datasets. In this work, we
introduce a self-supervised, computationally economical method for generating
adversarial examples, designed for the unseen black-box setting. Adapting
techniques from representation learning, our method generates on-manifold EAs
that are encouraged to resemble the data distribution. These attacks are
comparable in effectiveness compared to the state-of-the-art when attacking the
model trained on, but are significantly more effective when attacking unseen
models, as the attacks are more related to the data rather than the model
itself. Our experiments consistently demonstrate the method is effective across
various models, unseen data categories, and even defended models, suggesting a
significant role for on-manifold EAs when targeting unseen models.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03708" title="Abstract">arXiv:2310.03708</a> [<a href="/pdf/2310.03708" title="Download PDF">pdf</a>, <a href="/format/2310.03708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond One-Preference-for-All: Multi-Objective Direct Preference  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jing Shao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiangyu Yue</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language models (LMs), despite aligning well with an average labeler through
reinforcement learning from human feedback (RLHF), may not universally suit
diverse human preferences. Recent approaches therefore opt for customization by
collecting multi-dimensional feedback and creating distinct rewards for each
dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored
to different preferences using multi-objective RL (MORL) with different reward
weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for
MORLHF with diverse and usually conflicting objectives. In this paper, we
present Multi-Objective Direct Preference Optimization (MODPO), an RL-free
algorithm that extends Direct Preference Optimization (DPO) for multiple
alignment objectives. Essentially, MODPO trains different LMs to represent
different collective reward models that combine all objectives with specific
weightings. With a simple cross-entropy loss, the LMs optimized against the
MODPO objective are analytically the exact solutions of the original MORLHF
objective. Empirical results in safety alignment and long-form question
answering confirm that MODPO matches or outperforms existing methods,
efficiently producing a Pareto-optimal set of LMs that cater to diverse
preferences with 3 times less computational resources compared with MORLHF.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03710" title="Abstract">arXiv:2310.03710</a> [<a href="/pdf/2310.03710" title="Download PDF">pdf</a>, <a href="/format/2310.03710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agent Instructs Large Language Models to be General Zero-Shot Reasoners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crispino%2C+N">Nicholas Crispino</a>, 
<a href="/search/cs?searchtype=author&query=Montgomery%2C+K">Kyle Montgomery</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+F">Fankun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenguang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a method to improve the zero-shot reasoning abilities of large
language models on general language understanding tasks. Specifically, we build
an autonomous agent to instruct the reasoning process of large language models.
We show this approach further unleashes the zero-shot reasoning abilities of
large language models to more tasks. We study the performance of our method on
a wide set of datasets spanning generation, classification, and reasoning. We
show that our method generalizes to most tasks and obtains state-of-the-art
zero-shot performance on 20 of the 29 datasets that we evaluate. For instance,
our method boosts the performance of state-of-the-art large language models by
a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and
GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement
in reasoning is striking, with an average increase of 10.5%. With our method,
Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03714" title="Abstract">arXiv:2310.03714</a> [<a href="/pdf/2310.03714" title="Download PDF">pdf</a>, <a href="/format/2310.03714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSPy: Compiling Declarative Language Model Calls into Self-Improving  Pipelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khattab%2C+O">Omar Khattab</a>, 
<a href="/search/cs?searchtype=author&query=Singhvi%2C+A">Arnav Singhvi</a>, 
<a href="/search/cs?searchtype=author&query=Maheshwari%2C+P">Paridhi Maheshwari</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Santhanam%2C+K">Keshav Santhanam</a>, 
<a href="/search/cs?searchtype=author&query=Vardhamanan%2C+S">Sri Vardhamanan</a>, 
<a href="/search/cs?searchtype=author&query=Haq%2C+S">Saiful Haq</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Ashutosh Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+T+T">Thomas T. Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Moazam%2C+H">Hanna Moazam</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+H">Heather Miller</a>, 
<a href="/search/cs?searchtype=author&query=Zaharia%2C+M">Matei Zaharia</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The ML community is rapidly exploring techniques for prompting language
models (LMs) and for stacking them into pipelines that solve complex tasks.
Unfortunately, existing LM pipelines are typically implemented using hard-coded
"prompt templates", i.e. lengthy strings discovered via trial and error. Toward
a more systematic approach for developing and optimizing LM pipelines, we
introduce DSPy, a programming model that abstracts LM pipelines as text
transformation graphs, i.e. imperative computational graphs where LMs are
invoked through declarative modules. DSPy modules are parameterized, meaning
they can learn (by creating and collecting demonstrations) how to apply
compositions of prompting, finetuning, augmentation, and reasoning techniques.
We design a compiler that will optimize any DSPy pipeline to maximize a given
metric. We conduct two case studies, showing that succinct DSPy programs can
express and optimize sophisticated LM pipelines that reason about math word
problems, tackle multi-hop retrieval, answer complex questions, and control
agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and
llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot
prompting (generally by over 25% and 65%, respectively) and pipelines with
expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top
of that, DSPy programs compiled to open and relatively small LMs like
770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely
on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at
https://github.com/stanfordnlp/dspy
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03715" title="Abstract">arXiv:2310.03715</a> [<a href="/pdf/2310.03715" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Intelligence Index Report 2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maslej%2C+N">Nestor Maslej</a>, 
<a href="/search/cs?searchtype=author&query=Fattorini%2C+L">Loredana Fattorini</a>, 
<a href="/search/cs?searchtype=author&query=Brynjolfsson%2C+E">Erik Brynjolfsson</a>, 
<a href="/search/cs?searchtype=author&query=Etchemendy%2C+J">John Etchemendy</a>, 
<a href="/search/cs?searchtype=author&query=Ligett%2C+K">Katrina Ligett</a>, 
<a href="/search/cs?searchtype=author&query=Lyons%2C+T">Terah Lyons</a>, 
<a href="/search/cs?searchtype=author&query=Manyika%2C+J">James Manyika</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+H">Helen Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Niebles%2C+J+C">Juan Carlos Niebles</a>, 
<a href="/search/cs?searchtype=author&query=Parli%2C+V">Vanessa Parli</a>, 
<a href="/search/cs?searchtype=author&query=Shoham%2C+Y">Yoav Shoham</a>, 
<a href="/search/cs?searchtype=author&query=Wald%2C+R">Russell Wald</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+J">Jack Clark</a>, 
<a href="/search/cs?searchtype=author&query=Perrault%2C+R">Raymond Perrault</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Welcome to the sixth edition of the AI Index Report. This year, the report
introduces more original data than any previous edition, including a new
chapter on AI public opinion, a more thorough technical performance chapter,
original analysis about large language and multimodal models, detailed trends
in global AI legislation records, a study of the environmental impact of AI
systems, and more. The AI Index Report tracks, collates, distills, and
visualizes data related to artificial intelligence. Our mission is to provide
unbiased, rigorously vetted, broadly sourced data in order for policymakers,
researchers, executives, journalists, and the general public to develop a more
thorough and nuanced understanding of the complex field of AI. The report aims
to be the world's most credible and authoritative source for data and insights
about AI.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03716" title="Abstract">arXiv:2310.03716</a> [<a href="/pdf/2310.03716" title="Download PDF">pdf</a>, <a href="/format/2310.03716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Long Way to Go: Investigating Length Correlations in RLHF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singhal%2C+P">Prasann Singhal</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+T">Tanya Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiacheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Durrett%2C+G">Greg Durrett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Great successes have been reported using Reinforcement Learning from Human
Feedback (RLHF) to align large language models. Open-source preference datasets
and reward models have enabled wider experimentation beyond generic chat
settings, particularly to make systems more "helpful" for tasks like web
question answering, summarization, and multi-turn dialogue. When optimizing for
helpfulness, RLHF has been consistently observed to drive models to produce
longer outputs. This paper demonstrates that optimizing for response length is
a significant factor behind RLHF's reported improvements in these settings.
First, we study the relationship between reward and length for reward models
trained on three open-source preference datasets for helpfulness. Here, length
correlates strongly with reward, and improvements in reward score are driven in
large part by shifting the distribution over output lengths. We then explore
interventions during both RL and reward model learning to see if we can achieve
the same downstream improvements as RLHF without increasing length. While our
interventions mitigate length increases, they aren't uniformly effective across
settings. Furthermore, we find that even running RLHF with a reward based
solely on length can reproduce most of the downstream improvements over the
initial policy model, showing that reward models in these settings have a long
way to go.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03718" title="Abstract">arXiv:2310.03718</a> [<a href="/pdf/2310.03718" title="Download PDF">pdf</a>, <a href="/format/2310.03718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constraint-Conditioned Policy Optimization for Versatile Safe  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yihang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cen%2C+Z">Zhepeng Cen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiacheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tingnan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Safe reinforcement learning (RL) focuses on training reward-maximizing agents
subject to pre-defined safety constraints. Yet, learning versatile safe
policies that can adapt to varying safety constraint requirements during
deployment without retraining remains a largely unexplored and challenging
area. In this work, we formulate the versatile safe RL problem and consider two
primary requirements: training efficiency and zero-shot adaptation capability.
To address them, we introduce the Conditioned Constrained Policy Optimization
(CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation
(VVE) for approximating value functions under unseen threshold conditions, and
(2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint
thresholds during policy optimization. Our extensive experiments demonstrate
that CCPO outperforms the baselines in terms of safety and task performance
while preserving zero-shot adaptation capabilities to different constraint
thresholds data-efficiently. This makes our approach suitable for real-world
dynamic applications.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03720" title="Abstract">arXiv:2310.03720</a> [<a href="/pdf/2310.03720" title="Download PDF">pdf</a>, <a href="/format/2310.03720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HeaP: Hierarchical Policies for Web Actions using LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sodhi%2C+P">Paloma Sodhi</a>, 
<a href="/search/cs?searchtype=author&query=Branavan%2C+S+R+K">S.R.K. Branavan</a>, 
<a href="/search/cs?searchtype=author&query=McDonald%2C+R">Ryan McDonald</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated remarkable capabilities in
performing a range of instruction following tasks in few and zero-shot
settings. However, teaching LLMs to perform tasks on the web presents
fundamental challenges -- combinatorially large open-world tasks and variations
across web interfaces. We tackle these challenges by leveraging LLMs to
decompose web tasks into a collection of sub-tasks, each of which can be solved
by a low-level, closed-loop policy. These policies constitute a shared grammar
across tasks, i.e., new web tasks can be expressed as a composition of these
policies. We propose a novel framework, Hierarchical Policies for Web Actions
using LLMs (HeaP), that learns a set of hierarchical LLM prompts from
demonstrations for planning high-level tasks and executing them via a sequence
of low-level policies. We evaluate HeaP against a range of baselines on a suite
of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as
live website interactions, and show that it is able to outperform prior works
using orders of magnitude less data.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03724" title="Abstract">arXiv:2310.03724</a> [<a href="/pdf/2310.03724" title="Download PDF">pdf</a>, <a href="/format/2310.03724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duquenne%2C+P">Paul-Ambroise Duquenne</a>, 
<a href="/search/cs?searchtype=author&query=Schwenk%2C+H">Holger Schwenk</a>, 
<a href="/search/cs?searchtype=author&query=Sagot%2C+B">Beno&#xee;t Sagot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent research has shown that independently trained encoders and decoders,
combined through a shared fixed-size representation, can achieve competitive
performance in speech-to-text translation. In this work, we show that this type
of approach can be further improved with multilingual training. We observe
significant improvements in zero-shot cross-modal speech translation, even
outperforming a supervised approach based on XLSR for several languages.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03725" title="Abstract">arXiv:2310.03725</a> [<a href="/pdf/2310.03725" title="Download PDF">pdf</a>, <a href="/format/2310.03725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic interpolants with data-dependent couplings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Albergo%2C+M+S">Michael S. Albergo</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+M">Mark Goldstein</a>, 
<a href="/search/cs?searchtype=author&query=Boffi%2C+N+M">Nicholas M. Boffi</a>, 
<a href="/search/cs?searchtype=author&query=Ranganath%2C+R">Rajesh Ranganath</a>, 
<a href="/search/cs?searchtype=author&query=Vanden-Eijnden%2C+E">Eric Vanden-Eijnden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Generative models inspired by dynamical transport of measure -- such as flows
and diffusions -- construct a continuous-time map between two probability
densities. Conventionally, one of these is the target density, only accessible
through samples, while the other is taken as a simple base density that is
data-agnostic. In this work, using the framework of stochastic interpolants, we
formalize how to \textit{couple} the base and the target densities. This
enables us to incorporate information about class labels or continuous
embeddings to construct dynamical transport maps that serve as conditional
generative models. We show that these transport maps can be learned by solving
a simple square loss regression problem analogous to the standard independent
setting. We demonstrate the usefulness of constructing dependent couplings in
practice through experiments in super-resolution and in-painting.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03731" title="Abstract">arXiv:2310.03731</a> [<a href="/pdf/2310.03731" title="Download PDF">pdf</a>, <a href="/format/2310.03731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Ke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Houxing Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Aojun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zimu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Sichun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weikang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Renrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+M">Mingjie Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The state-of-the-art open-source language models for mathematical reasoning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The recently released GPT-4 Code Interpreter has demonstrated remarkable
proficiency in solving challenging math problems, primarily attributed to its
ability to seamlessly reason with natural language, generate code, execute
code, and continue reasoning based on the execution output. In this paper, we
present a method to fine-tune open-source language models, enabling them to use
code for modeling and deriving math equations and, consequently, enhancing
their mathematical reasoning abilities. We propose a method of generating novel
and high-quality datasets with math problems and their code-based solutions,
referred to as MathCodeInstruct. Each solution interleaves natural language,
code, and execution results. We also introduce a customized supervised
fine-tuning and inference approach. This approach yields the MathCoder models,
a family of models capable of generating code-based solutions for solving
challenging math problems. Impressively, the MathCoder models achieve
state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K
(83.9%) datasets, substantially outperforming other open-source alternatives.
Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K
and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The
dataset and models will be released at https://github.com/mathllm/MathCoder.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03734" title="Abstract">arXiv:2310.03734</a> [<a href="/pdf/2310.03734" title="Download PDF">pdf</a>, <a href="/format/2310.03734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Unpaired Data for Vision-Language Generative Models via Cycle  Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianhong Li</a>, 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+S">Sangnie Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonglong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Barber%2C+J">Jarred Barber</a>, 
<a href="/search/cs?searchtype=author&query=Katabi%2C+D">Dina Katabi</a>, 
<a href="/search/cs?searchtype=author&query=Lajoie%2C+G">Guillaume Lajoie</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Huiwen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+D">Dilip Krishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Current vision-language generative models rely on expansive corpora of paired
image-text data to attain optimal performance and generalization capabilities.
However, automatically collecting such data (e.g. via large-scale web scraping)
leads to low quality and poor image-text correlation, while human annotation is
more accurate but requires significant manual effort and expense. We introduce
$\textbf{ITIT}$ ($\textbf{I}$n$\textbf{T}$egrating $\textbf{I}$mage
$\textbf{T}$ext): an innovative training paradigm grounded in the concept of
cycle consistency which allows vision-language training on unpaired image and
text data. ITIT is comprised of a joint image-text encoder with disjoint image
and text decoders that enable bidirectional image-to-text and text-to-image
generation in a single framework. During training, ITIT leverages a small set
of paired image-text data to ensure its output matches the input reasonably
well in both directions. Simultaneously, the model is also trained on much
larger datasets containing only images or texts. This is achieved by enforcing
cycle consistency between the original unpaired samples and the cycle-generated
counterparts. For instance, it generates a caption for a given input image and
then uses the caption to create an output image, and enforces similarity
between the input and output images. Our experiments show that ITIT with
unpaired datasets exhibits similar scaling behavior as using high-quality
paired data. We demonstrate image generation and captioning performance on par
with state-of-the-art text-to-image and image-to-text models with orders of
magnitude fewer (only 3M) paired image-text data.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03736" title="Abstract">arXiv:2310.03736</a> [<a href="/pdf/2310.03736" title="Download PDF">pdf</a>, <a href="/format/2310.03736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recovering Single-Crossing Preferences From Approval Ballots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Constantinescu%2C+A">Andrei Constantinescu</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in WINE'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">An electorate with fully-ranked innate preferences casts approval votes over
a finite set of alternatives. As a result, only partial information about the
true preferences is revealed to the voting authorities. In an effort to
understand the nature of the true preferences given only partial information,
one might ask whether the unknown innate preferences could possibly be
single-crossing. The existence of a polynomial time algorithm to determine this
has been asked as an outstanding problem in the works of Elkind and Lackner. We
hereby give a polynomial time algorithm determining a single-crossing
collection of fully-ranked preferences that could have induced the elicited
approval ballots, or reporting the nonexistence thereof. Moreover, we consider
the problem of identifying negative instances with a set of forbidden
sub-ballots, showing that any such characterization requires infinitely many
forbidden configurations.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03738" title="Abstract">arXiv:2310.03738</a> [<a href="/pdf/2310.03738" title="Download PDF">pdf</a>, <a href="/format/2310.03738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stylist: Style-Driven Feature Ranking for Robust Novelty Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smeu%2C+S">Stefan Smeu</a>, 
<a href="/search/cs?searchtype=author&query=Burceanu%2C+E">Elena Burceanu</a>, 
<a href="/search/cs?searchtype=author&query=Haller%2C+E">Emanuela Haller</a>, 
<a href="/search/cs?searchtype=author&query=Nicolicioiu%2C+A+L">Andrei Liviu Nicolicioiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Novelty detection aims at finding samples that differ in some form from the
distribution of seen samples. But not all changes are created equal. Data can
suffer a multitude of distribution shifts, and we might want to detect only
some types of relevant changes. Similar to works in out-of-distribution
generalization, we propose to use the formalization of separating into semantic
or content changes, that are relevant to our task, and style changes, that are
irrelevant. Within this formalization, we define the robust novelty detection
as the task of finding semantic changes while being robust to style
distributional shifts. Leveraging pretrained, large-scale model
representations, we introduce Stylist, a novel method that focuses on dropping
environment-biased features. First, we compute a per-feature score based on the
feature distribution distances between environments. Next, we show that our
selection manages to remove features responsible for spurious correlations and
improve novelty detection performance. For evaluation, we adapt domain
generalization datasets to our task and analyze the methods behaviors. We
additionally built a large synthetic dataset where we have control over the
spurious correlations degree. We prove that our selection mechanism improves
novelty detection algorithms across multiple datasets, containing both
stylistic and content shifts.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03739" title="Abstract">arXiv:2310.03739</a> [<a href="/pdf/2310.03739" title="Download PDF">pdf</a>, <a href="/format/2310.03739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Text-to-Image Diffusion Models with Reward Backpropagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabhudesai%2C+M">Mihir Prabhudesai</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+A">Anirudh Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+D">Deepak Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Fragkiadaki%2C+K">Katerina Fragkiadaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://align-prop.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Text-to-image diffusion models have recently emerged at the forefront of
image generation, powered by very large-scale unsupervised or weakly supervised
text-to-image training datasets. Due to their unsupervised training,
controlling their behavior in downstream tasks, such as maximizing
human-perceived image quality, image-text alignment, or ethical image
generation, is difficult. Recent works finetune diffusion models to downstream
reward functions using vanilla reinforcement learning, notorious for the high
variance of the gradient estimators. In this paper, we propose AlignProp, a
method that aligns diffusion models to downstream reward functions using
end-to-end backpropagation of the reward gradient through the denoising
process. While naive implementation of such backpropagation would require
prohibitive memory resources for storing the partial derivatives of modern
text-to-image models, AlignProp finetunes low-rank adapter weight modules and
uses gradient checkpointing, to render its memory usage viable. We test
AlignProp in finetuning diffusion models to various objectives, such as
image-text semantic alignment, aesthetics, compressibility and controllability
of the number of objects present, as well as their combinations. We show
AlignProp achieves higher rewards in fewer training steps than alternatives,
while being conceptually simpler, making it a straightforward choice for
optimizing diffusion models for differentiable reward functions of interest.
Code and Visualization results are available at https://align-prop.github.io/.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03740" title="Abstract">arXiv:2310.03740</a> [<a href="/pdf/2310.03740" title="Download PDF">pdf</a>, <a href="/format/2310.03740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ContactGen: Generative Contact Modeling for Grasp Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shaowei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jimei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Saurabh Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenlong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023. Website: <a href="https://stevenlsw.github.io/contactgen/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">This paper presents a novel object-centric contact representation ContactGen
for hand-object interaction. The ContactGen comprises three components: a
contact map indicates the contact location, a part map represents the contact
hand part, and a direction map tells the contact direction within each part.
Given an input object, we propose a conditional generative model to predict
ContactGen and adopt model-based optimization to predict diverse and
geometrically feasible grasps. Experimental results demonstrate our method can
generate high-fidelity and diverse human grasps for various objects. Project
page: https://stevenlsw.github.io/contactgen/
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03742" title="Abstract">arXiv:2310.03742</a> [<a href="/pdf/2310.03742" title="Download PDF">pdf</a>, <a href="/format/2310.03742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A High-Performance Design, Implementation, Deployment, and Evaluation of  The Slim Fly Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blach%2C+N">Nils Blach</a>, 
<a href="/search/cs?searchtype=author&query=Besta%2C+M">Maciej Besta</a>, 
<a href="/search/cs?searchtype=author&query=De+Sensi%2C+D">Daniele De Sensi</a>, 
<a href="/search/cs?searchtype=author&query=Domke%2C+J">Jens Domke</a>, 
<a href="/search/cs?searchtype=author&query=Harake%2C+H">Hussein Harake</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shigang Li</a>, 
<a href="/search/cs?searchtype=author&query=Iff%2C+P">Patrick Iff</a>, 
<a href="/search/cs?searchtype=author&query=Konieczny%2C+M">Marek Konieczny</a>, 
<a href="/search/cs?searchtype=author&query=Lakhotia%2C+K">Kartik Lakhotia</a>, 
<a href="/search/cs?searchtype=author&query=Kubicek%2C+A">Ales Kubicek</a>, 
<a href="/search/cs?searchtype=author&query=Ferrari%2C+M">Marcel Ferrari</a>, 
<a href="/search/cs?searchtype=author&query=Petrini%2C+F">Fabrizio Petrini</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Novel low-diameter network topologies such as Slim Fly (SF) offer significant
cost and power advantages over the established Fat Tree, Clos, or Dragonfly. To
spearhead the adoption of low-diameter networks, we design, implement, deploy,
and evaluate the first real-world SF installation. We focus on deployment,
management, and operational aspects of our test cluster with 200 servers and
carefully analyze performance. We demonstrate techniques for simple cabling and
cabling validation as well as a novel high-performance routing architecture for
InfiniBand-based low-diameter topologies. Our real-world benchmarks show SF's
strong performance for many modern workloads such as deep neural network
training, graph analytics, or linear algebra kernels. SF outperforms
non-blocking Fat Trees in scalability while offering comparable or better
performance and lower cost for large network sizes. Our work can facilitate
deploying SF while the associated (open-source) routing architecture is fully
portable and applicable to accelerate any low-diameter interconnect.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03743" title="Abstract">arXiv:2310.03743</a> [<a href="/pdf/2310.03743" title="Download PDF">pdf</a>, <a href="/format/2310.03743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Un-Kidnappable Robot: Acoustic Localization of Sneaking People
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Grady%2C+P">Patrick Grady</a>, 
<a href="/search/cs?searchtype=author&query=Brahmbhatt%2C+S">Samarth Brahmbhatt</a>, 
<a href="/search/cs?searchtype=author&query=Vasudevan%2C+A+B">Arun Balajee Vasudevan</a>, 
<a href="/search/cs?searchtype=author&query=Kemp%2C+C+C">Charles C. Kemp</a>, 
<a href="/search/cs?searchtype=author&query=Hays%2C+J">James Hays</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://sites.google.com/view/unkidnappable-robot">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">How easy is it to sneak up on a robot? We examine whether we can detect
people using only the incidental sounds they produce as they move, even when
they try to be quiet. We collect a robotic dataset of high-quality 4-channel
audio paired with 360 degree RGB data of people moving in different indoor
settings. We train models that predict if there is a moving person nearby and
their location using only audio. We implement our method on a robot, allowing
it to track a single person moving quietly with only passive audio sensing. For
demonstration videos, see our project page:
https://sites.google.com/view/unkidnappable-robot
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03744" title="Abstract">arXiv:2310.03744</a> [<a href="/pdf/2310.03744" title="Download PDF">pdf</a>, <a href="/format/2310.03744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Baselines with Visual Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haotian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+J">Yong Jae Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech report, 4 pages. LLaVA project page: <a href="https://llava-vl.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large multimodal models (LMM) have recently shown encouraging progress with
visual instruction tuning. In this note, we show that the fully-connected
vision-language cross-modal connector in LLaVA is surprisingly powerful and
data-efficient. With simple modifications to LLaVA, namely, using
CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA
data with simple response formatting prompts, we establish stronger baselines
that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint
uses merely 1.2M publicly available data, and finishes full training in ~1 day
on a single 8-A100 node. We hope this can make state-of-the-art LMM research
more accessible. Code and model will be publicly available.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Fri,  6 Oct 23</h3>
<dl>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02671" title="Abstract">arXiv:2310.02671</a> (cross-list from math.OC) [<a href="/pdf/2310.02671" title="Download PDF">pdf</a>, <a href="/format/2310.02671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy  Gradient Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Klein%2C+S">Sara Klein</a>, 
<a href="/search/math?searchtype=author&query=Weissmann%2C+S">Simon Weissmann</a>, 
<a href="/search/math?searchtype=author&query=D%C3%B6ring%2C+L">Leif D&#xf6;ring</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Markov Decision Processes (MDPs) are a formal framework for modeling and
solving sequential decision-making problems. In finite-time horizons such
problems are relevant for instance for optimal stopping or specific supply
chain problems, but also in the training of large language models. In contrast
to infinite horizon MDPs optimal policies are not stationary, policies must be
learned for every single epoch. In practice all parameters are often trained
simultaneously, ignoring the inherent structure suggested by dynamic
programming. This paper introduces a combination of dynamic programming and
policy gradient called dynamic policy gradient, where the parameters are
trained backwards in time. For the tabular softmax parametrisation we carry out
the convergence analysis for simultaneous and dynamic policy gradient towards
global optima, both in the exact and sampled gradient settings without
regularisation. It turns out that the use of dynamic policy gradient training
much better exploits the structure of finite-time problems which is reflected
in improved convergence bounds.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03027" title="Abstract">arXiv:2310.03027</a> (cross-list from physics.chem-ph) [<a href="/pdf/2310.03027" title="Download PDF">pdf</a>, <a href="/format/2310.03027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synergistic Fusion of Graph and Transformer Features for Enhanced  Molecular Property Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Prakash%2C+M+V+S">M V Sai Prakash</a>, 
<a href="/search/physics?searchtype=author&query=N%2C+S+R">Siddartha Reddy N</a>, 
<a href="/search/physics?searchtype=author&query=Parab%2C+G">Ganesh Parab</a>, 
<a href="/search/physics?searchtype=author&query=V%2C+V">Varun V</a>, 
<a href="/search/physics?searchtype=author&query=Vaddina%2C+V">Vishal Vaddina</a>, 
<a href="/search/physics?searchtype=author&query=Gopalakrishnan%2C+S">Saisubramaniam Gopalakrishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Molecular property prediction is a critical task in computational drug
discovery. While recent advances in Graph Neural Networks (GNNs) and
Transformers have shown to be effective and promising, they face the following
limitations: Transformer self-attention does not explicitly consider the
underlying molecule structure while GNN feature representation alone is not
sufficient to capture granular and hidden interactions and characteristics that
distinguish similar molecules. To address these limitations, we propose SYN-
FUSION, a novel approach that synergistically combines pre-trained features
from GNNs and Transformers. This approach provides a comprehensive molecular
representation, capturing both the global molecule structure and the individual
atom characteristics. Experimental results on MoleculeNet benchmarks
demonstrate superior performance, surpassing previous models in 5 out of 7
classification datasets and 4 out of 6 regression datasets. The performance of
SYN-FUSION has been compared with other Graph-Transformer models that have been
jointly trained using a combination of transformer and graph features, and it
is found that our approach is on par with those models in terms of performance.
Extensive analysis of the learned fusion model across aspects such as loss,
latent space, and weight distribution further validates the effectiveness of
SYN-FUSION. Finally, an ablation study unequivocally demonstrates that the
synergy achieved by SYN-FUSION surpasses the performance of its individual
model components and their ensemble, offering a substantial improvement in
predicting molecular properties.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03028" title="Abstract">arXiv:2310.03028</a> (cross-list from physics.chem-ph) [<a href="/pdf/2310.03028" title="Download PDF">pdf</a>, <a href="/format/2310.03028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAF: Smart Aggregation Framework for Revealing Atoms Importance Rank and  Improving Prediction Rates in Drug Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Taub%2C+R">Ronen Taub</a>, 
<a href="/search/physics?searchtype=author&query=Savir%2C+Y">Yonatan Savir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning, and representation learning in particular, has the
potential to facilitate drug discovery by screening a large chemical space in
silico. A successful approach for representing molecules is to treat them as a
graph and utilize graph neural networks. One of the key limitations of such
methods is the necessity to represent compounds with different numbers of
atoms, which requires aggregating the atom's information. Common aggregation
operators, such as averaging, result in loss of information at the atom level.
In this work, we propose a novel aggregating approach where each atom is
weighted non-linearly using the Boltzmann distribution with a hyperparameter
analogous to temperature. We show that using this weighted aggregation improves
the ability of the gold standard message-passing neural network to predict
antibiotic activity. Moreover, by changing the temperature hyperparameter, our
approach can reveal the atoms that are important for activity prediction in a
smooth and consistent way, thus providing a novel, regulated attention
mechanism for graph neural networks. We further validate our method by showing
that it recapitulates the functional group in beta-Lactam antibiotics. The
ability of our approach to rank the atoms' importance for a desired function
can be used within any graph neural network to provide interpretability of the
results and predictions at the node level.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03030" title="Abstract">arXiv:2310.03030</a> (cross-list from physics.chem-ph) [<a href="/pdf/2310.03030" title="Download PDF">pdf</a>, <a href="/format/2310.03030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-MolBERTa: GPT Molecular Features Language Model for molecular  property prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Balaji%2C+S">Suryanarayanan Balaji</a>, 
<a href="/search/physics?searchtype=author&query=Magar%2C+R">Rishikesh Magar</a>, 
<a href="/search/physics?searchtype=author&query=Jadhav%2C+Y">Yayati Jadhav</a>, 
<a href="/search/physics?searchtype=author&query=BaratiFarimani%2C+a+A">and Amir BaratiFarimani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper has 17 pages, 4 figures and 4 tables, along with 71 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the emergence of Transformer architectures and their powerful
understanding of textual data, a new horizon has opened up to predict the
molecular properties based on text description. While SMILES are the most
common form of representation, they are lacking robustness, rich information
and canonicity, which limit their effectiveness in becoming generalizable
representations. Here, we present GPT-MolBERTa, a self-supervised large
language model (LLM) which uses detailed textual descriptions of molecules to
predict their properties. A text based description of 326000 molecules were
collected using ChatGPT and used to train LLM to learn the representation of
molecules. To predict the properties for the downstream tasks, both BERT and
RoBERTa models were used in the finetuning stage. Experiments show that
GPT-MolBERTa performs well on various molecule property benchmarks, and
approaching state of the art performance in regression tasks. Additionally,
further analysis of the attention mechanisms show that GPT-MolBERTa is able to
pick up important information from the input textual data, displaying the
interpretability of the model.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03047" title="Abstract">arXiv:2310.03047</a> (cross-list from physics.chem-ph) [<a href="/pdf/2310.03047" title="Download PDF">pdf</a>, <a href="/format/2310.03047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Chemical Physics by Geometric Deep Learning for  Gradient-based Property Optimization of Mixtures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zhu%2C+S">Shang Zhu</a>, 
<a href="/search/physics?searchtype=author&query=Ramsundar%2C+B">Bharath Ramsundar</a>, 
<a href="/search/physics?searchtype=author&query=Annevelink%2C+E">Emil Annevelink</a>, 
<a href="/search/physics?searchtype=author&query=Lin%2C+H">Hongyi Lin</a>, 
<a href="/search/physics?searchtype=author&query=Dave%2C+A">Adarsh Dave</a>, 
<a href="/search/physics?searchtype=author&query=Guan%2C+P">Pin-Wen Guan</a>, 
<a href="/search/physics?searchtype=author&query=Gering%2C+K">Kevin Gering</a>, 
<a href="/search/physics?searchtype=author&query=Viswanathan%2C+V">Venkatasubramanian Viswanathan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Chemical mixtures, satisfying multi-objective performance metrics and
constraints, enable their use in chemical processes and electrochemical
devices. In this work, we develop a differentiable chemical-physics framework
for modeling chemical mixtures, DiffMix, where geometric deep learning (GDL) is
leveraged to map from molecular species, compositions and environment
conditions, to physical coefficients in the mixture physics laws. In
particular, we extend mixture thermodynamic and transport laws by creating
learnable physical coefficients, where we use graph neural networks as the
molecule encoder and enforce component-wise permutation-invariance. We start
our model evaluations with thermodynamics of binary mixtures, and further
benchmarked multicomponent electrolyte mixtures on their transport properties,
in order to test the model generalizability. We show improved prediction
accuracy and model robustness of DiffMix than its purely data-driven variants.
Furthermore, we demonstrate the efficient optimization of electrolyte transport
properties, built on the gradient obtained using DiffMix auto-differentiation.
Our simulation runs are then backed up by the data generated by a robotic
experimentation setup, Clio. By combining mixture physics and GDL, DiffMix
expands the predictive modeling methods for chemical mixtures and provides
low-cost optimization approaches in large chemical spaces.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03054" title="Abstract">arXiv:2310.03054</a> (cross-list from stat.ML) [<a href="/pdf/2310.03054" title="Download PDF">pdf</a>, <a href="/format/2310.03054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posterior Sampling Based on Gradient Flows of the MMD with Negative  Distance Kernel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hagemann%2C+P">Paul Hagemann</a>, 
<a href="/search/stat?searchtype=author&query=Hertrich%2C+J">Johannes Hertrich</a>, 
<a href="/search/stat?searchtype=author&query=Altekr%C3%BCger%2C+F">Fabian Altekr&#xfc;ger</a>, 
<a href="/search/stat?searchtype=author&query=Beinert%2C+R">Robert Beinert</a>, 
<a href="/search/stat?searchtype=author&query=Chemseddine%2C+J">Jannis Chemseddine</a>, 
<a href="/search/stat?searchtype=author&query=Steidl%2C+G">Gabriele Steidl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Probability (math.PR)

</div>
<p class="mathjax">We propose conditional flows of the maximum mean discrepancy (MMD) with the
negative distance kernel for posterior sampling and conditional generative
modeling. This MMD, which is also known as energy distance, has several
advantageous properties like efficient computation via slicing and sorting. We
approximate the joint distribution of the ground truth and the observations
using discrete Wasserstein gradient flows and establish an error bound for the
posterior distributions. Further, we prove that our particle flow is indeed a
Wasserstein gradient flow of an appropriate functional. The power of our method
is demonstrated by numerical examples including conditional image generation
and inverse problems like superresolution, inpainting and computed tomography
in low-dose and limited-angle settings.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03106" title="Abstract">arXiv:2310.03106</a> (cross-list from eess.IV) [<a href="/pdf/2310.03106" title="Download PDF">pdf</a>, <a href="/format/2310.03106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Creating an Atlas of Normal Tissue for Pruning WSI Patching Through  Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nejat%2C+P">Peyman Nejat</a>, 
<a href="/search/eess?searchtype=author&query=Alsaafin%2C+A">Areej Alsaafin</a>, 
<a href="/search/eess?searchtype=author&query=Alabtah%2C+G">Ghazal Alabtah</a>, 
<a href="/search/eess?searchtype=author&query=Comfere%2C+N">Nneka Comfere</a>, 
<a href="/search/eess?searchtype=author&query=Mangold%2C+A">Aaron Mangold</a>, 
<a href="/search/eess?searchtype=author&query=Murphree%2C+D">Dennis Murphree</a>, 
<a href="/search/eess?searchtype=author&query=Zot%2C+P">Patricija Zot</a>, 
<a href="/search/eess?searchtype=author&query=Yasir%2C+S">Saba Yasir</a>, 
<a href="/search/eess?searchtype=author&query=Garcia%2C+J+J">Joaquin J. Garcia</a>, 
<a href="/search/eess?searchtype=author&query=Tizhoosh%2C+H+R">H.R. Tizhoosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Patching gigapixel whole slide images (WSIs) is an important task in
computational pathology. Some methods have been proposed to select a subset of
patches as WSI representation for downstream tasks. While most of the
computational pathology tasks are designed to classify or detect the presence
of pathological lesions in each WSI, the confounding role and redundant nature
of normal histology in tissue samples are generally overlooked in WSI
representations. In this paper, we propose and validate the concept of an
"atlas of normal tissue" solely using samples of WSIs obtained from normal
tissue biopsies. Such atlases can be employed to eliminate normal fragments of
tissue samples and hence increase the representativeness collection of patches.
We tested our proposed method by establishing a normal atlas using 107 normal
skin WSIs and demonstrated how established indexes and search engines like
Yottixel can be improved. We used 553 WSIs of cutaneous squamous cell carcinoma
(cSCC) to show the advantage. We also validated our method applied to an
external dataset of 451 breast WSIs. The number of selected WSI patches was
reduced by 30% to 50% after utilizing the proposed normal atlas while
maintaining the same indexing and search performance in leave-one-patinet-out
validation for both datasets. We show that the proposed normal atlas shows
promise for unsupervised selection of the most representative patches of the
abnormal/malignant WSI lesions.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03112" title="Abstract">arXiv:2310.03112</a> (cross-list from stat.ML) [<a href="/pdf/2310.03112" title="Download PDF">pdf</a>, <a href="/format/2310.03112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Model-based Trees as Interpretable Surrogate Models for Model  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Herbinger%2C+J">Julia Herbinger</a>, 
<a href="/search/stat?searchtype=author&query=Dandl%2C+S">Susanne Dandl</a>, 
<a href="/search/stat?searchtype=author&query=Ewald%2C+F+K">Fiona K. Ewald</a>, 
<a href="/search/stat?searchtype=author&query=Loibl%2C+S">Sofia Loibl</a>, 
<a href="/search/stat?searchtype=author&query=Casalicchio%2C+G">Giuseppe Casalicchio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Surrogate models play a crucial role in retrospectively interpreting complex
and powerful black box machine learning models via model distillation. This
paper focuses on using model-based trees as surrogate models which partition
the feature space into interpretable regions via decision rules. Within each
region, interpretable models based on additive main effects are used to
approximate the behavior of the black box model, striking for an optimal
balance between interpretability and performance. Four model-based tree
algorithms, namely SLIM, GUIDE, MOB, and CTree, are compared regarding their
ability to generate such surrogate models. We investigate fidelity,
interpretability, stability, and the algorithms' capability to capture
interaction effects through appropriate splits. Based on our comprehensive
analyses, we finally provide an overview of user-specific recommendations.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03118" title="Abstract">arXiv:2310.03118</a> (cross-list from eess.IV) [<a href="/pdf/2310.03118" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blind CT Image Quality Assessment Using DDPM-derived Content and  Transformer-based Evaluator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shi%2C+Y">Yongyi Shi</a>, 
<a href="/search/eess?searchtype=author&query=Xia%2C+W">Wenjun Xia</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+G">Ge Wang</a>, 
<a href="/search/eess?searchtype=author&query=Mou%2C+X">Xuanqin Mou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Lowering radiation dose per view and utilizing sparse views per scan are two
common CT scan modes, albeit often leading to distorted images characterized by
noise and streak artifacts. Blind image quality assessment (BIQA) strives to
evaluate perceptual quality in alignment with what radiologists perceive, which
plays an important role in advancing low-dose CT reconstruction techniques. An
intriguing direction involves developing BIQA methods that mimic the
operational characteristic of the human visual system (HVS). The internal
generative mechanism (IGM) theory reveals that the HVS actively deduces primary
content to enhance comprehension. In this study, we introduce an innovative
BIQA metric that emulates the active inference process of IGM. Initially, an
active inference module, implemented as a denoising diffusion probabilistic
model (DDPM), is constructed to anticipate the primary content. Then, the
dissimilarity map is derived by assessing the interrelation between the
distorted image and its primary content. Subsequently, the distorted image and
dissimilarity map are combined into a multi-channel image, which is inputted
into a transformer-based image quality evaluator. Remarkably, by exclusively
utilizing this transformer-based quality evaluator, we won the second place in
the MICCAI 2023 low-dose computed tomography perceptual image quality
assessment grand challenge. Leveraging the DDPM-derived primary content, our
approach further improves the performance on the challenge dataset.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03121" title="Abstract">arXiv:2310.03121</a> (cross-list from physics.chem-ph) [<a href="/pdf/2310.03121" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenMM 8: Molecular Dynamics Simulation with Machine Learning Potentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Eastman%2C+P">Peter Eastman</a>, 
<a href="/search/physics?searchtype=author&query=Galvelis%2C+R">Raimondas Galvelis</a>, 
<a href="/search/physics?searchtype=author&query=Pel%C3%A1ez%2C+R+P">Ra&#xfa;l P. Pel&#xe1;ez</a>, 
<a href="/search/physics?searchtype=author&query=Abreu%2C+C+R+A">Charlles R. A. Abreu</a>, 
<a href="/search/physics?searchtype=author&query=Farr%2C+S+E">Stephen E. Farr</a>, 
<a href="/search/physics?searchtype=author&query=Gallicchio%2C+E">Emilio Gallicchio</a>, 
<a href="/search/physics?searchtype=author&query=Gorenko%2C+A">Anton Gorenko</a>, 
<a href="/search/physics?searchtype=author&query=Henry%2C+M+M">Michael M. Henry</a>, 
<a href="/search/physics?searchtype=author&query=Hu%2C+F">Frank Hu</a>, 
<a href="/search/physics?searchtype=author&query=Huang%2C+J">Jing Huang</a>, 
<a href="/search/physics?searchtype=author&query=Kr%C3%A4mer%2C+A">Andreas Kr&#xe4;mer</a>, 
<a href="/search/physics?searchtype=author&query=Michel%2C+J">Julien Michel</a>, 
<a href="/search/physics?searchtype=author&query=Mitchell%2C+J+A">Joshua A. Mitchell</a>, 
<a href="/search/physics?searchtype=author&query=Pande%2C+V+S">Vijay S. Pande</a>, 
<a href="/search/physics?searchtype=author&query=Rodrigues%2C+J+P">Jo&#xe3;o PGLM Rodrigues</a>, 
<a href="/search/physics?searchtype=author&query=Rodriguez-Guerra%2C+J">Jaime Rodriguez-Guerra</a>, 
<a href="/search/physics?searchtype=author&query=Simmonett%2C+A+C">Andrew C. Simmonett</a>, 
<a href="/search/physics?searchtype=author&query=Swails%2C+J">Jason Swails</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+I">Ivy Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Chodera%2C+J+D">John D. Chodera</a>, 
<a href="/search/physics?searchtype=author&query=De+Fabritiis%2C+G">Gianni De Fabritiis</a>, 
<a href="/search/physics?searchtype=author&query=Markland%2C+T+E">Thomas E. Markland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning plays an important and growing role in molecular simulation.
The newest version of the OpenMM molecular dynamics toolkit introduces new
features to support the use of machine learning potentials. Arbitrary PyTorch
models can be added to a simulation and used to compute forces and energy. A
higher-level interface allows users to easily model their molecules of interest
with general purpose, pretrained potential functions. A collection of optimized
CUDA kernels and custom PyTorch operations greatly improves the speed of
simulations. We demonstrate these features on simulations of cyclin-dependent
kinase 8 (CDK8) and the green fluorescent protein (GFP) chromophore in water.
Taken together, these features make it practical to use machine learning to
improve the accuracy of simulations at only a modest increase in cost.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03138" title="Abstract">arXiv:2310.03138</a> (cross-list from physics.acc-ph) [<a href="/pdf/2310.03138" title="Download PDF">pdf</a>, <a href="/format/2310.03138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Magneto-Thermal Thin Shell Approximation for 3D Finite Element Analysis  of No-Insulation Coils
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Schnaubelt%2C+E">Erik Schnaubelt</a>, 
<a href="/search/physics?searchtype=author&query=Atalay%2C+S">Sina Atalay</a>, 
<a href="/search/physics?searchtype=author&query=Wozniak%2C+M">Mariusz Wozniak</a>, 
<a href="/search/physics?searchtype=author&query=Dular%2C+J">Julien Dular</a>, 
<a href="/search/physics?searchtype=author&query=Geuzaine%2C+C">Christophe Geuzaine</a>, 
<a href="/search/physics?searchtype=author&query=Vanderheyden%2C+B">Beno&#xee;t Vanderheyden</a>, 
<a href="/search/physics?searchtype=author&query=Marsic%2C+N">Nicolas Marsic</a>, 
<a href="/search/physics?searchtype=author&query=Verweij%2C+A">Arjan Verweij</a>, 
<a href="/search/physics?searchtype=author&query=Sch%C3%B6ps%2C+S">Sebastian Sch&#xf6;ps</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at EUCAS 2023, Bologna, Italy. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Accelerator Physics (physics.acc-ph)</span>; Superconductivity (cond-mat.supr-con); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">For finite element (FE) analysis of no-insulation (NI) high-temperature
superconducting (HTS) pancake coils, the high aspect ratio of the turn-to-turn
contact layer (T2TCL) leads to meshing difficulties which result in either poor
quality mesh elements resulting in a decrease of the solution accuracy or a
high number of degrees of freedom. We proposed to mitigate this issue by
collapsing the T2TCL volume into a surface and using a so-called thin shell
approximation (TSA). Previously, two TSA have been introduced, one to solve the
heat equation and the other for an $\vec{H}-\phi$ magnetodynamic formulation.
<br />In this work, we propose to combine the magnetodynamic and thermal TSA to
create a coupled magneto-thermal TSA for three-dimensional FE analysis.
Particular attention is paid to the detailed derivation of the coupling terms.
In the context of NI HTS pancake coils, the TSA represents the electric and
thermal contact resistance of the T2TCL. For the HTS coated conductor (CC)
itself, an anisotropic homogenization is used which represents its
multi-layered structure. In axial and azimuthal direction, it resolves the
current sharing between the HTS and other layers of the CC. The coupled TSA
formulation is verified against a reference model with volumetric T2TCL. The
coupled TSA is shown to significantly reduce the solution time as well as the
manual effort required for high-quality meshes of the T2TCL. The implementation
is open-source and a reference implementation is made publicly available.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03186" title="Abstract">arXiv:2310.03186</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.03186" title="Download PDF">pdf</a>, <a href="/format/2310.03186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Raju%2C+R+V">Rajkumar Vasudeva Raju</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Linderman%2C+S">Scott Linderman</a>, 
<a href="/search/q-bio?searchtype=author&query=Pitkow%2C+X">Xaq Pitkow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 4 figures and 1 supplementary figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Patterns of microcircuitry suggest that the brain has an array of repeated
canonical computational units. Yet neural representations are distributed, so
the relevant computations may only be related indirectly to single-neuron
transformations. It thus remains an open challenge how to define canonical
distributed computations. We integrate normative and algorithmic theories of
neural computation into a mathematical framework for inferring canonical
distributed computations from large-scale neural activity patterns. At the
normative level, we hypothesize that the brain creates a structured internal
model of its environment, positing latent causes that explain its sensory
inputs, and uses those sensory inputs to infer the latent causes. At the
algorithmic level, we propose that this inference process is a nonlinear
message-passing algorithm on a graph-structured model of the world. Given a
time series of neural activity during a perceptual inference task, our
framework finds (i) the neural representation of relevant latent variables,
(ii) interactions between these variables that define the brain's internal
model of the world, and (iii) message-functions specifying the inference
algorithm. These targeted computational properties are then statistically
distinguishable due to the symmetries inherent in any canonical computation, up
to a global transformation. As a demonstration, we simulate recordings for a
model brain that implicitly implements an approximate inference algorithm on a
probabilistic graphical model. Given its external inputs and noisy neural
activity, we recover the latent variables, their neural representation and
dynamics, and canonical message-functions. We highlight features of
experimental design needed to successfully extract canonical computations from
neural data. Overall, this framework provides a new tool for discovering
interpretable structure in neural recordings.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03206" title="Abstract">arXiv:2310.03206</a> (cross-list from math.OC) [<a href="/pdf/2310.03206" title="Download PDF">pdf</a>, <a href="/format/2310.03206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret Analysis of Distributed Online Control for LTI Systems with  Adversarial Disturbances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chang%2C+T">Ting-Jui Chang</a>, 
<a href="/search/math?searchtype=author&query=Shahrampour%2C+S">Shahin Shahrampour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper addresses the distributed online control problem over a network of
linear time-invariant (LTI) systems (with possibly unknown dynamics) in the
presence of adversarial perturbations. There exists a global network cost that
is characterized by a time-varying convex function, which evolves in an
adversarial manner and is sequentially and partially observed by local agents.
The goal of each agent is to generate a control sequence that can compete with
the best centralized control policy in hindsight, which has access to the
global cost. This problem is formulated as a regret minimization. For the case
of known dynamics, we propose a fully distributed disturbance feedback
controller that guarantees a regret bound of $O(\sqrt{T}\log T)$, where $T$ is
the time horizon. For the unknown dynamics case, we design a distributed
explore-then-commit approach, where in the exploration phase all agents jointly
learn the system dynamics, and in the learning phase our proposed control
algorithm is applied using each agent system estimate. We establish a regret
bound of $O(T^{2/3} \text{poly}(\log T))$ for this setting.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03234" title="Abstract">arXiv:2310.03234</a> (cross-list from math.OC) [<a href="/pdf/2310.03234" title="Download PDF">pdf</a>, <a href="/format/2310.03234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+Q">Quanqi Hu</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+D">Dixian Zhu</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+T">Tianbao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper investigates new families of compositional optimization problems,
called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf
w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf
c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC
FCCO). There has been a growing interest in FCCO due to its wide-ranging
applications in machine learning and AI, as well as its ability to address the
shortcomings of stochastic algorithms based on empirical risk minimization.
However, current research on FCCO presumes that both the inner and outer
functions are smooth, limiting their potential to tackle a more diverse set of
problems. Our research expands on this area by examining non-smooth
weakly-convex FCCO, where the outer function is weakly convex and
non-decreasing, and the inner function is weakly-convex. We analyze a
single-loop algorithm and establish its complexity for finding an
$\epsilon$-stationary point of the Moreau envelop of the objective function.
Additionally, we also extend the algorithm to solving novel non-smooth
weakly-convex tri-level finite-sum coupled compositional optimization problems,
which feature a nested arrangement of three functions. Lastly, we explore the
applications of our algorithms in deep learning for two-way partial AUC
maximization and multi-instance two-way partial AUC maximization, using
empirical studies to showcase the effectiveness of the proposed algorithms.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03243" title="Abstract">arXiv:2310.03243</a> (cross-list from stat.ML) [<a href="/pdf/2310.03243" title="Download PDF">pdf</a>, <a href="/format/2310.03243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Deep Learning for Time Series Data: Theory and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+M">Mingxuan Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+Y">Yan Sun</a>, 
<a href="/search/stat?searchtype=author&query=Liang%2C+F">Faming Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Sparse deep learning has become a popular technique for improving the
performance of deep neural networks in areas such as uncertainty
quantification, variable selection, and large-scale network compression.
However, most existing research has focused on problems where the observations
are independent and identically distributed (i.i.d.), and there has been little
work on the problems where the observations are dependent, such as time series
data and sequential data in natural language processing. This paper aims to
address this gap by studying the theory for sparse deep learning with dependent
data. We show that sparse recurrent neural networks (RNNs) can be consistently
estimated, and their predictions are asymptotically normally distributed under
appropriate assumptions, enabling the prediction uncertainty to be correctly
quantified. Our numerical results show that sparse deep learning outperforms
state-of-the-art methods, such as conformal predictions, in prediction
uncertainty quantification for time series data. Furthermore, our results
indicate that the proposed method can consistently identify the autoregressive
order for time series data and outperform existing methods in large-scale model
compression. Our proposed method has important practical implications in fields
such as finance, healthcare, and energy, where both accurate point estimates
and prediction uncertainty quantification are of concern.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03269" title="Abstract">arXiv:2310.03269</a> (cross-list from q-bio.BM) [<a href="/pdf/2310.03269" title="Download PDF">pdf</a>, <a href="/format/2310.03269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructProtein: Aligning Human and Protein Language via Knowledge  Instruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+Z">Zeyuan Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Q">Qiang Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Ding%2C+K">Keyan Ding</a>, 
<a href="/search/q-bio?searchtype=author&query=Qin%2C+M">Ming Qin</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhuang%2C+X">Xiang Zhuang</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+X">Xiaotong Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) have revolutionized the field of natural
language processing, but they fall short in comprehending biological sequences
such as proteins. To address this challenge, we propose InstructProtein, an
innovative LLM that possesses bidirectional generation capabilities in both
human and protein languages: (i) taking a protein sequence as input to predict
its textual function description and (ii) using natural language to prompt
protein sequence generation. To achieve this, we first pre-train an LLM on both
protein and natural language corpora, enabling it to comprehend individual
languages. Then supervised instruction tuning is employed to facilitate the
alignment of these two distinct languages. Herein, we introduce a knowledge
graph-based instruction generation framework to construct a high-quality
instruction dataset, addressing annotation imbalance and instruction deficits
in existing protein-text corpus. In particular, the instructions inherit the
structural relations between proteins and function annotations in knowledge
graphs, which empowers our model to engage in the causal modeling of protein
functions, akin to the chain-of-thought processes in natural languages.
Extensive experiments on bidirectional protein-text generation tasks show that
InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,
InstructProtein serves as a pioneering step towards text-based protein function
prediction and sequence design, effectively bridging the gap between protein
and human language understanding.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03289" title="Abstract">arXiv:2310.03289</a> (cross-list from math.OC) [<a href="/pdf/2310.03289" title="Download PDF">pdf</a>, <a href="/format/2310.03289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Collaborative Safety-Critical Control for Networked Dynamic  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Butler%2C+B+A">Brooks A. Butler</a>, 
<a href="/search/math?searchtype=author&query=Par%C3%A9%2C+P+E">Philip E. Par&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work is under review for publication in the IEEE Transactions on Automatic Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Multiagent Systems (cs.MA); Systems and Control (eess.SY); Dynamical Systems (math.DS)

</div>
<p class="mathjax">As modern systems become ever more connected with complex dynamic coupling
relationships, the development of safe control methods for such networked
systems becomes paramount. In this paper, we define a general networked model
with coupled dynamics and local control and discuss the relationship of
node-level safety definitions for individual agents with local neighborhood
dynamics. We define a node-level barrier function (NBF), node-level control
barrier function (NCBF), and collaborative node-level barrier function (cNCBF)
and provide conditions under which sets defined by these functions will be
forward invariant. We use collaborative node-level barrier functions to
construct a novel distributed algorithm for the safe control of collaborating
network agents and provide conditions under which the algorithm is guaranteed
to converge to a viable set of safe control actions for all agents or a
terminally infeasible state for at least one agent. We introduce the notion of
non-compliance of network neighbors as a metric of robustness for collaborative
safety for a given network state and chosen barrier function hyper-parameters.
We illustrate these results on a networked susceptible-infected-susceptible
(SIS) model.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03298" title="Abstract">arXiv:2310.03298</a> (cross-list from stat.ML) [<a href="/pdf/2310.03298" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+Y">Yi-Ping Chen</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+L">Liwei Wang</a>, 
<a href="/search/stat?searchtype=author&query=Comlek%2C+Y">Yigitcan Comlek</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+W">Wei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multi-fidelity (MF) methods are gaining popularity for enhancing surrogate
modeling and design optimization by incorporating data from various
low-fidelity (LF) models. While most existing MF methods assume a fixed
dataset, adaptive sampling methods that dynamically allocate resources among
fidelity models can achieve higher efficiency in the exploring and exploiting
the design space. However, most existing MF methods rely on the hierarchical
assumption of fidelity levels or fail to capture the intercorrelation between
multiple fidelity levels and utilize it to quantify the value of the future
samples and navigate the adaptive sampling. To address this hurdle, we propose
a framework hinged on a latent embedding for different fidelity models and the
associated pre-posterior analysis to explicitly utilize their correlation for
adaptive sampling. In this framework, each infill sampling iteration includes
two steps: We first identify the location of interest with the greatest
potential improvement using the high-fidelity (HF) model, then we search for
the next sample across all fidelity levels that maximize the improvement per
unit cost at the location identified in the first step. This is made possible
by a single Latent Variable Gaussian Process (LVGP) model that maps different
fidelity models into an interpretable latent space to capture their
correlations without assuming hierarchical fidelity levels. The LVGP enables us
to assess how LF sampling candidates will affect HF response with pre-posterior
analysis and determine the next sample with the best benefit-to-cost ratio.
Through test cases, we demonstrate that the proposed method outperforms the
benchmark methods in both MF global fitting (GF) and Bayesian Optimization (BO)
problems in convergence rate and robustness. Moreover, the method offers the
flexibility to switch between GF and BO by simply changing the acquisition
function.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03365" title="Abstract">arXiv:2310.03365</a> (cross-list from eess.IV) [<a href="/pdf/2310.03365" title="Download PDF">pdf</a>, <a href="/format/2310.03365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video  Sequences Using Swin Transformer-Enhanced UNet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jafari%2C+H">Hossein Jafari</a>, 
<a href="/search/eess?searchtype=author&query=Faez%2C+K">Karim Faez</a>, 
<a href="/search/eess?searchtype=author&query=Amindavar%2C+H">Hamidreza Amindavar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Lung cancer is highly lethal, emphasizing the critical need for early
detection. However, identifying lung nodules poses significant challenges for
radiologists, who rely heavily on their expertise and experience for accurate
diagnosis. To address this issue, computer-aided diagnosis systems based on
machine learning techniques have emerged to assist doctors in identifying lung
nodules from computed tomography (CT) scans. Unfortunately, existing networks
in this domain often suffer from computational complexity, leading to high
rates of false negatives and false positives, limiting their effectiveness. To
address these challenges, we present an innovative model that harnesses the
strengths of both convolutional neural networks and vision transformers.
Inspired by object detection in videos, we treat each 3D CT image as a video,
individual slices as frames, and lung nodules as objects, enabling a
time-series application. The primary objective of our work is to overcome
hardware limitations during model training, allowing for efficient processing
of 2D data while utilizing inter-slice information for accurate identification
based on 3D image context. We validated the proposed network by applying a
10-fold cross-validation technique to the publicly available Lung Nodule
Analysis 2016 dataset. Our proposed architecture achieves an average
sensitivity criterion of 97.84% and a competition performance metrics (CPM) of
96.0% with few parameters. Comparative analysis with state-of-the-art
advancements in lung nodule identification demonstrates the significant
accuracy achieved by our proposed model.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03378" title="Abstract">arXiv:2310.03378</a> (cross-list from math.DS) [<a href="/pdf/2310.03378" title="Download PDF">pdf</a>, <a href="/format/2310.03378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine learning the interaction network in coupled dynamical systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bhure%2C+P+R">Pawan R. Bhure</a>, 
<a href="/search/math?searchtype=author&query=Santhanam%2C+M+S">M. S. Santhanam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">The study of interacting dynamical systems continues to attract research
interest in various fields of science and engineering. In a collection of
interacting particles, the interaction network contains information about how
various components interact with one another. Inferring the information about
the interaction network from the dynamics of agents is a problem of
long-standing interest. In this work, we employ a self-supervised neural
network model to achieve two outcomes: to recover the interaction network and
to predict the dynamics of individual agents. Both these information are
inferred solely from the observed trajectory data. This work presents an
application of the Neural Relational Inference model to two dynamical systems:
coupled particles mediated by Hooke's law interaction and coupled phase
(Kuramoto) oscillators.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03384" title="Abstract">arXiv:2310.03384</a> (cross-list from physics.optics) [<a href="/pdf/2310.03384" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complex-valued universal linear transformations and image encryption  using spatially incoherent diffractive networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Yang%2C+X">Xilin Yang</a>, 
<a href="/search/physics?searchtype=author&query=Rahman%2C+M+S+S">Md Sadman Sakib Rahman</a>, 
<a href="/search/physics?searchtype=author&query=Bai%2C+B">Bijie Bai</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+J">Jingxi Li</a>, 
<a href="/search/physics?searchtype=author&query=Ozcan%2C+A">Aydogan Ozcan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 Pages, 3 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">As an optical processor, a Diffractive Deep Neural Network (D2NN) utilizes
engineered diffractive surfaces designed through machine learning to perform
all-optical information processing, completing its tasks at the speed of light
propagation through thin optical layers. With sufficient degrees-of-freedom,
D2NNs can perform arbitrary complex-valued linear transformations using
spatially coherent light. Similarly, D2NNs can also perform arbitrary linear
intensity transformations with spatially incoherent illumination; however,
under spatially incoherent light, these transformations are non-negative,
acting on diffraction-limited optical intensity patterns at the input
field-of-view (FOV). Here, we expand the use of spatially incoherent D2NNs to
complex-valued information processing for executing arbitrary complex-valued
linear transformations using spatially incoherent light. Through simulations,
we show that as the number of optimized diffractive features increases beyond a
threshold dictated by the multiplication of the input and output
space-bandwidth products, a spatially incoherent diffractive visual processor
can approximate any complex-valued linear transformation and be used for
all-optical image encryption using incoherent illumination. The findings are
important for the all-optical processing of information under natural light
using various forms of diffractive surface-based optical processors.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03435" title="Abstract">arXiv:2310.03435</a> (cross-list from stat.ML) [<a href="/pdf/2310.03435" title="Download PDF">pdf</a>, <a href="/format/2310.03435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Inference for GARCH-family Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Magris%2C+M">Martin Magris</a>, 
<a href="/search/stat?searchtype=author&query=Iosifidis%2C+A">Alexandros Iosifidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Econometrics (econ.EM)

</div>
<p class="mathjax">The Bayesian estimation of GARCH-family models has been typically addressed
through Monte Carlo sampling. Variational Inference is gaining popularity and
attention as a robust approach for Bayesian inference in complex machine
learning models; however, its adoption in econometrics and finance is limited.
This paper discusses the extent to which Variational Inference constitutes a
reliable and feasible alternative to Monte Carlo sampling for Bayesian
inference in GARCH-like models. Through a large-scale experiment involving the
constituents of the S&amp;P 500 index, several Variational Inference optimizers, a
variety of volatility models, and a case study, we show that Variational
Inference is an attractive, remarkably well-calibrated, and competitive method
for Bayesian learning.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03444" title="Abstract">arXiv:2310.03444</a> (cross-list from eess.AS) [<a href="/pdf/2310.03444" title="Download PDF">pdf</a>, <a href="/format/2310.03444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VaSAB: The variable size adaptive information bottleneck for  disentanglement on speech and singing voice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bous%2C+F">Frederik Bous</a>, 
<a href="/search/eess?searchtype=author&query=Roebel%2C+A">Axel Roebel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">The information bottleneck auto-encoder is a tool for disentanglement
commonly used for voice transformation. The successful disentanglement relies
on the right choice of bottleneck size. Previous bottleneck auto-encoders
created the bottleneck by the dimension of the latent space or through vector
quantization and had no means to change the bottleneck size of a specific
model. As the bottleneck removes information from the disentangled
representation, the choice of bottleneck size is a trade-off between
disentanglement and synthesis quality. We propose to build the information
bottleneck using dropout which allows us to change the bottleneck through the
dropout rate and investigate adapting the bottleneck size depending on the
context. We experimentally explore into using the adaptive bottleneck for pitch
transformation and demonstrate that the adaptive bottleneck leads to improved
disentanglement of the F0 parameter for both, speech and singing voice leading
to improved synthesis quality. Using the variable bottleneck size, we were able
to achieve disentanglement for singing voice including extremely high pitches
and create a universal voice model, that works on both speech and singing voice
with improved synthesis quality.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03445" title="Abstract">arXiv:2310.03445</a> (cross-list from math.CT) [<a href="/pdf/2310.03445" title="Download PDF">pdf</a>, <a href="/ps/2310.03445" title="Download PostScript">ps</a>, <a href="/format/2310.03445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relative fixed points of functors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Schoen%2C+E">Ezra Schoen</a>, 
<a href="/search/math?searchtype=author&query=Master%2C+J">Jade Master</a>, 
<a href="/search/math?searchtype=author&query=Kupke%2C+C">Clemens Kupke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We show how the relatively initial or relatively terminal fixed points for a
well-behaved functor $F$ form a pair of adjoint functors between $F$-coalgebras
and $F$-algebras. We use the language of locally presentable categories to find
sufficient conditions for existence of this adjunction. We show that relative
fixed points may be characterized as (co)equalizers of the free (co)monad on
$F$. In particular, when $F$ is a polynomial functor on $\mathsf{Set}$ the
relative fixed points are a quotient or subset of the free term algebra or the
cofree term coalgebra. We give examples of the relative fixed points for
polynomial functors and an example which is the Sierpinski carpet. Lastly, we
prove a general preservation result for relative fixed points.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03455" title="Abstract">arXiv:2310.03455</a> (cross-list from eess.AS) [<a href="/pdf/2310.03455" title="Download PDF">pdf</a>, <a href="/format/2310.03455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance and energy balance: a comprehensive study of  state-of-the-art sound event detection systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ronchini%2C+F">Francesca Ronchini</a>, 
<a href="/search/eess?searchtype=author&query=Serizel%2C+R">Romain Serizel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">In recent years, deep learning systems have shown a concerning trend toward
increased complexity and higher energy consumption. As researchers in this
domain and organizers of one of the Detection and Classification of Acoustic
Scenes and Events challenges tasks, we recognize the importance of addressing
the environmental impact of data-driven SED systems. In this paper, we propose
an analysis focused on SED systems based on the challenge submissions. This
includes a comparison across the past two years and a detailed analysis of this
year's SED systems. Through this research, we aim to explore how the SED
systems are evolving every year in relation to their energy efficiency
implications.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03480" title="Abstract">arXiv:2310.03480</a> (cross-list from eess.AS) [<a href="/pdf/2310.03480" title="Download PDF">pdf</a>, <a href="/format/2310.03480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Cadenza ICASSP 2024 Grand Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dabike%2C+G+R">Gerardo Roa Dabike</a>, 
<a href="/search/eess?searchtype=author&query=Akeroyd%2C+M+A">Michael A. Akeroyd</a>, 
<a href="/search/eess?searchtype=author&query=Bannister%2C+S">Scott Bannister</a>, 
<a href="/search/eess?searchtype=author&query=Barker%2C+J">Jon Barker</a>, 
<a href="/search/eess?searchtype=author&query=Cox%2C+T+J">Trevor J. Cox</a>, 
<a href="/search/eess?searchtype=author&query=Fazenda%2C+B">Bruno Fazenda</a>, 
<a href="/search/eess?searchtype=author&query=Firth%2C+J">Jennifer Firth</a>, 
<a href="/search/eess?searchtype=author&query=Graetzer%2C+S">Simone Graetzer</a>, 
<a href="/search/eess?searchtype=author&query=Greasley%2C+A">Alinka Greasley</a>, 
<a href="/search/eess?searchtype=author&query=Vos%2C+R">Rebecca Vos</a>, 
<a href="/search/eess?searchtype=author&query=Whitmer%2C+W">William Whitmer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages paper for ICASSP 2024 SP Grand Challenge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">The Cadenza project aims to enhance the audio quality of music for
individuals with hearing loss. As part of this, the project is organizing the
ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing Aids. The
challenge can be tackled by decomposing the music at the hearing aid
microphones into vocals, bass, drums, and other components. These can then be
intelligently remixed in a personalized manner to improve audio quality.
Alternatively, an end-to-end approach could be used. Processes need to consider
the music itself, the gain applied to each component, and the listener's
hearing loss. The submitted entries will be evaluated using the intrusive
objective metric, the Hearing Aid Audio Quality Index (HAAQI). This paper
outlines the challenge.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03485" title="Abstract">arXiv:2310.03485</a> (cross-list from eess.IV) [<a href="/pdf/2310.03485" title="Download PDF">pdf</a>, <a href="/format/2310.03485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kollias%2C+D">Dimitrios Kollias</a>, 
<a href="/search/eess?searchtype=author&query=Vendal%2C+K">Karanjot Vendal</a>, 
<a href="/search/eess?searchtype=author&query=Gadhavi%2C+P">Priyanka Gadhavi</a>, 
<a href="/search/eess?searchtype=author&query=Russom%2C+S">Solomon Russom</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Brain tumors pose significant health challenges worldwide, with glioblastoma
being one of the most aggressive forms. Accurate determination of the
O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation status is
crucial for personalized treatment strategies. However, traditional methods are
labor-intensive and time-consuming. This paper proposes a novel multi-modal
approach, BTDNet, leveraging multi-parametric MRI scans, including FLAIR, T1w,
T1wCE, and T2 3D volumes, to predict MGMT promoter methylation status. BTDNet
addresses two main challenges: the variable volume lengths (i.e., each volume
consists of a different number of slices) and the volume-level annotations
(i.e., the whole 3D volume is annotated and not the independent slices that it
consists of). BTDNet consists of four components: i) the data augmentation one
(that performs geometric transformations, convex combinations of data pairs and
test-time data augmentation); ii) the 3D analysis one (that performs global
analysis through a CNN-RNN); iii) the routing one (that contains a mask layer
that handles variable input feature lengths), and iv) the modality fusion one
(that effectively enhances data representation, reduces ambiguities and
mitigates data scarcity). The proposed method outperforms by large margins the
state-of-the-art methods in the RSNA-ASNR-MICCAI BraTS 2021 Challenge, offering
a promising avenue for enhancing brain tumor diagnosis and treatment.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03499" title="Abstract">arXiv:2310.03499</a> (cross-list from physics.ao-ph) [<a href="/pdf/2310.03499" title="Download PDF">pdf</a>, <a href="/format/2310.03499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IceCloudNet: Cirrus and mixed-phase cloud prediction from SEVIRI input  learned from sparse supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Jeggle%2C+K">Kai Jeggle</a>, 
<a href="/search/physics?searchtype=author&query=Czerkawski%2C+M">Mikolaj Czerkawski</a>, 
<a href="/search/physics?searchtype=author&query=Serva%2C+F">Federico Serva</a>, 
<a href="/search/physics?searchtype=author&query=Saux%2C+B+L">Bertrand Le Saux</a>, 
<a href="/search/physics?searchtype=author&query=Neubauer%2C+D">David Neubauer</a>, 
<a href="/search/physics?searchtype=author&query=Lohmann%2C+U">Ulrike Lohmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A Preprint. Submitted to Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Clouds containing ice particles play a crucial role in the climate system.
Yet they remain a source of great uncertainty in climate models and future
climate projections. In this work, we create a new observational constraint of
regime-dependent ice microphysical properties at the spatio-temporal coverage
of geostationary satellite instruments and the quality of active satellite
retrievals. We achieve this by training a convolutional neural network on three
years of SEVIRI and DARDAR data sets. This work will enable novel research to
improve ice cloud process understanding and hence, reduce uncertainties in a
changing climate and help assess geoengineering methods for cirrus clouds.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03514" title="Abstract">arXiv:2310.03514</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2310.03514" title="Download PDF">pdf</a>, <a href="/format/2310.03514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vocal Fold Reconstruction from Optical Velocity and Displacement  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zieger%2C+D">Daniel Zieger</a>, 
<a href="/search/physics?searchtype=author&query=N%C3%A4ger%2C+C">Christoph N&#xe4;ger</a>, 
<a href="/search/physics?searchtype=author&query=Becker%2C+S">Stefan Becker</a>, 
<a href="/search/physics?searchtype=author&query=G%C3%BCnther%2C+T">Tobias G&#xfc;nther</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">The three-dimensional reconstruction of vocal folds in medicine usually
involves endoscopy and an approach to extract depth information like structured
light or stereo matching of images. The resulting mesh can accurately represent
the superior area of the vocal folds, while new approaches also try to
reconstruct the inferior area. We propose a novel approach to extract the
time-dependent 3D geometry of the vocal fold from optical measurements on both
the superior and inferior side, requiring optical measurements only from the
superior side. First, a time-dependent, tri-variate surface velocity vector
field is reconstructed using a high-speed camera and a laser Doppler vibrometer
in an experimental environment. This vector field serves as target in an
inverse finite-element simulation that optimizes the forces applied to a
deformable vocal fold model such that the resulting movement after FEM
simulation matches the velocity observations on the superior side. The required
forces for the finite element method simulation are treated as unknowns and are
assembled using multiple scalar fields. We use tensor products in B\'ezier
Bernstein basis for our scalar fields to reduce the degrees of freedom for our
optimization. We use gradient descent to optimize the control points of the
force field polynomials. Our utilized error metric for gradient descent
consists of two terms. The first term is used to match the simulated velocities
to the observed measurements, while the second term measures the silhouette
difference between observation and simulation.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03546" title="Abstract">arXiv:2310.03546</a> (cross-list from stat.ML) [<a href="/pdf/2310.03546" title="Download PDF">pdf</a>, <a href="/format/2310.03546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Renaud%2C+M">Marien Renaud</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+J">Jiaming Liu</a>, 
<a href="/search/stat?searchtype=author&query=de+Bortoli%2C+V">Valentin de Bortoli</a>, 
<a href="/search/stat?searchtype=author&query=Almansa%2C+A">Andr&#xe9;s Almansa</a>, 
<a href="/search/stat?searchtype=author&query=Kamilov%2C+U+S">Ulugbek S. Kamilov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Posterior sampling has been shown to be a powerful Bayesian approach for
solving imaging inverse problems. The recent plug-and-play unadjusted Langevin
algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling
and minimum mean squared error (MMSE) estimation by combining physical
measurement models with deep-learning priors specified using image denoisers.
However, the intricate relationship between the sampling distribution of
PnP-ULA and the mismatched data-fidelity and denoiser has not been
theoretically analyzed. We address this gap by proposing a posterior-L2
pseudometric and using it to quantify an explicit error bound for PnP-ULA under
mismatched posterior distribution. We numerically validate our theory on
several inverse problems such as sampling from Gaussian mixture models and
image deblurring. Our results suggest that the sensitivity of the sampling
distribution of PnP-ULA to a mismatch in the measurement model and the denoiser
can be precisely characterized.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03556" title="Abstract">arXiv:2310.03556</a> (cross-list from stat.ML) [<a href="/pdf/2310.03556" title="Download PDF">pdf</a>, <a href="/format/2310.03556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Training of Probabilistic Models Using the Leave-One-Out Maximum  Log-Likelihood Objective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=B%C3%B6lat%2C+K">Kutay B&#xf6;lat</a>, 
<a href="/search/stat?searchtype=author&query=Tindemans%2C+S+H">Simon H. Tindemans</a>, 
<a href="/search/stat?searchtype=author&query=Palensky%2C+P">Peter Palensky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">Probabilistic modelling of power systems operation and planning processes
depends on data-driven methods, which require sufficiently large datasets. When
historical data lacks this, it is desired to model the underlying data
generation mechanism as a probability distribution to assess the data quality
and generate more data, if needed. Kernel density estimation (KDE) based models
are popular choices for this task, but they fail to adapt to data regions with
varying densities. In this paper, an adaptive KDE model is employed to
circumvent this, where each kernel in the model has an individual bandwidth.
The leave-one-out maximum log-likelihood (LOO-MLL) criterion is proposed to
prevent the singular solutions that the regular MLL criterion gives rise to,
and it is proven that LOO-MLL prevents these. Relying on this guaranteed
robustness, the model is extended by assigning learnable weights to the
kernels. In addition, a modified expectation-maximization algorithm is employed
to accelerate the optimization speed reliably. The performance of the proposed
method and models are exhibited on two power systems datasets using different
statistical tests and by comparison with Gaussian mixture models. Results show
that the proposed models have promising performance, in addition to their
singularity prevention guarantees.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03559" title="Abstract">arXiv:2310.03559</a> (cross-list from eess.IV) [<a href="/pdf/2310.03559" title="Download PDF">pdf</a>, <a href="/format/2310.03559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedSynV1: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yanwu Xu</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+L">Li Sun</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+W">Wei Peng</a>, 
<a href="/search/eess?searchtype=author&query=Visweswaran%2C+S">Shyam Visweswaran</a>, 
<a href="/search/eess?searchtype=author&query=Batmanghelich%2C+K">Kayhan Batmanghelich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper introduces an innovative methodology for producing high-quality 3D
lung CT images guided by textual information. While diffusion-based generative
models are increasingly used in medical imaging, current state-of-the-art
approaches are limited to low-resolution outputs and underutilize radiology
reports' abundant information. The radiology reports can enhance the generation
process by providing additional guidance and offering fine-grained control over
the synthesis of images. Nevertheless, expanding text-guided generation to
high-resolution 3D images poses significant memory and anatomical
detail-preserving challenges. Addressing the memory issue, we introduce a
hierarchical scheme that uses a modified UNet architecture. We start by
synthesizing low-resolution images conditioned on the text, serving as a
foundation for subsequent generators for complete volumetric data. To ensure
the anatomical plausibility of the generated samples, we provide further
guidance by generating vascular, airway, and lobular segmentation masks in
conjunction with the CT images. The model demonstrates the capability to use
textual input and segmentation tasks to generate synthesized images. The
results of comparative assessments indicate that our approach exhibits superior
performance compared to the most advanced models based on GAN and diffusion
techniques, especially in accurately retaining crucial anatomical features such
as fissure lines, airways, and vascular structures. This innovation introduces
novel possibilities. This study focuses on two main objectives: (1) the
development of a method for creating images based on textual prompts and
anatomical components, and (2) the capability to generate new images
conditioning on anatomical elements. The advancements in image generation can
be applied to enhance numerous downstream tasks.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03575" title="Abstract">arXiv:2310.03575</a> (cross-list from stat.ML) [<a href="/pdf/2310.03575" title="Download PDF">pdf</a>, <a href="/format/2310.03575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of learning a flow-based generative model from limited sample  complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cui%2C+H">Hugo Cui</a>, 
<a href="/search/stat?searchtype=author&query=Krzakala%2C+F">Florent Krzakala</a>, 
<a href="/search/stat?searchtype=author&query=Vanden-Eijnden%2C+E">Eric Vanden-Eijnden</a>, 
<a href="/search/stat?searchtype=author&query=Zdeborov%C3%A1%2C+L">Lenka Zdeborov&#xe1;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the problem of training a flow-based generative model, parametrized
by a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture.
We provide a sharp end-to-end analysis of the problem. First, we provide a
tight closed-form characterization of the learnt velocity field, when
parametrized by a shallow denoising auto-encoder trained on a finite number $n$
of samples from the target distribution. Building on this analysis, we provide
a sharp description of the corresponding generative flow, which pushes the base
Gaussian density forward to an approximation of the target density. In
particular, we provide closed-form formulae for the distance between the mean
of the generated mixture and the mean of the target mixture, which we show
decays as $\Theta_n(\frac{1}{n})$. Finally, this rate is shown to be in fact
Bayes-optimal.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03584" title="Abstract">arXiv:2310.03584</a> (cross-list from math.CO) [<a href="/pdf/2310.03584" title="Download PDF">pdf</a>, <a href="/ps/2310.03584" title="Download PostScript">ps</a>, <a href="/format/2310.03584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimum number of arcs in $k$-critical digraphs with order at most  $2k-1$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Picasarri-Arrieta%2C+L">Lucas Picasarri-Arrieta</a>, 
<a href="/search/math?searchtype=author&query=Stiebitz%2C+M">Michael Stiebitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The dichromatic number $\vec{\chi}(D)$ of a digraph $D$ is the least integer
$k$ for which $D$ has a coloring with $k$ colors such that there is no
monochromatic directed cycle in $D$. The digraphs considered here are finite
and may have antiparallel arcs, but no parallel arcs. A digraph $D$ is called
$k$-critical if each proper subdigraph $D'$ of $D$ satisfies
$\vec{\chi}(D')&lt;\vec{\chi}(D)=k$. For integers $k$ and $n$, let
$\overrightarrow{\mathrm{ext}}(k,n)$ denote the minimum number of arcs possible
in a $k$-critical digraph of order $n$. It is easy to show that
$\overrightarrow{\mathrm{ext}}(2,n)=n$ for all $n\geq 2$, and
$\overrightarrow{\mathrm{ext}}(3,n)\geq 2n$ for all possible $n$, where
equality holds if and only if $n$ is odd and $n\geq 3$. As a main result we
prove that if $n, k$ and $p$ are integers with $n=k+p$ and $2\leq p \leq k-1$,
then $\overrightarrow{\mathrm{ext}}(k,n)=2({\binom{n}{2}} - (p^2+1))$, and we
give an exact characterisation of $k$-critical digraphs for which equality
holds. This generalizes a result about critical graphs obtained in 1963 by
Tibor Gallai.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03597" title="Abstract">arXiv:2310.03597</a> (cross-list from stat.ML) [<a href="/pdf/2310.03597" title="Download PDF">pdf</a>, <a href="/format/2310.03597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling via Gradient Flows in the Space of Probability Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+Y">Yifan Chen</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+D+Z">Daniel Zhengyu Huang</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+J">Jiaoyang Huang</a>, 
<a href="/search/stat?searchtype=author&query=Reich%2C+S">Sebastian Reich</a>, 
<a href="/search/stat?searchtype=author&query=Stuart%2C+A+M">Andrew M Stuart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2302.11024">arXiv:2302.11024</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Sampling a target probability distribution with an unknown normalization
constant is a fundamental challenge in computational science and engineering.
Recent work shows that algorithms derived by considering gradient flows in the
space of probability measures open up new avenues for algorithm development.
This paper makes three contributions to this sampling approach by scrutinizing
the design components of such gradient flows. Any instantiation of a gradient
flow for sampling needs an energy functional and a metric to determine the
flow, as well as numerical approximations of the flow to derive algorithms. Our
first contribution is to show that the Kullback-Leibler divergence, as an
energy functional, has the unique property (among all f-divergences) that
gradient flows resulting from it do not depend on the normalization constant of
the target distribution. Our second contribution is to study the choice of
metric from the perspective of invariance. The Fisher-Rao metric is known as
the unique choice (up to scaling) that is diffeomorphism invariant. As a
computationally tractable alternative, we introduce a relaxed, affine
invariance property for the metrics and gradient flows. In particular, we
construct various affine invariant Wasserstein and Stein gradient flows. Affine
invariant gradient flows are shown to behave more favorably than their
non-affine-invariant counterparts when sampling highly anisotropic
distributions, in theory and by using particle methods. Our third contribution
is to study, and develop efficient algorithms based on Gaussian approximations
of the gradient flows; this leads to an alternative to particle methods. We
establish connections between various Gaussian approximate gradient flows,
discuss their relation to gradient methods arising from parametric variational
inference, and study their convergence properties both theoretically and
numerically.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03608" title="Abstract">arXiv:2310.03608</a> (cross-list from eess.IV) [<a href="/pdf/2310.03608" title="Download PDF">pdf</a>, <a href="/format/2310.03608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Good Are Synthetic Medical Images? An Empirical Study with Lung  Ultrasound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yu%2C+M">Menghan Yu</a>, 
<a href="/search/eess?searchtype=author&query=Kulhare%2C+S">Sourabh Kulhare</a>, 
<a href="/search/eess?searchtype=author&query=Mehanian%2C+C">Courosh Mehanian</a>, 
<a href="/search/eess?searchtype=author&query=Delahunt%2C+C+B">Charles B Delahunt</a>, 
<a href="/search/eess?searchtype=author&query=Shea%2C+D+E">Daniel E Shea</a>, 
<a href="/search/eess?searchtype=author&query=Laverriere%2C+Z">Zohreh Laverriere</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+I">Ishan Shah</a>, 
<a href="/search/eess?searchtype=author&query=Horning%2C+M+P">Matthew P Horning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted in Simulation and Synthesis in Medical Imaging (SASHIMI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Acquiring large quantities of data and annotations is known to be effective
for developing high-performing deep learning models, but is difficult and
expensive to do in the healthcare context. Adding synthetic training data using
generative models offers a low-cost method to deal effectively with the data
scarcity challenge, and can also address data imbalance and patient privacy
issues. In this study, we propose a comprehensive framework that fits
seamlessly into model development workflows for medical image analysis. We
demonstrate, with datasets of varying size, (i) the benefits of generative
models as a data augmentation method; (ii) how adversarial methods can protect
patient privacy via data substitution; (iii) novel performance metrics for
these use cases by testing models on real holdout data. We show that training
with both synthetic and real data outperforms training with real data alone,
and that models trained solely with synthetic data approach their real-only
counterparts. Code is available at
https://github.com/Global-Health-Labs/US-DCGAN.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03640" title="Abstract">arXiv:2310.03640</a> (cross-list from math.LO) [<a href="/pdf/2310.03640" title="Download PDF">pdf</a>, <a href="/ps/2310.03640" title="Download PostScript">ps</a>, <a href="/format/2310.03640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proof-theoretic methods in quantifier-free definability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kocsis%2C+Z+A">Zoltan A. Kocsis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, for associated Agda proofs see <a href="https://github.com/zaklogician/proof-theoretic-methods">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We introduce a proof-theoretic method for showing nondefinability of
second-order intuitionistic connectives by quantifier-free schemata. We apply
the method to confirm that Taranovsky's "realizability disjunction" connective
does not admit a quantifier-free definition, and use it to obtain new results
and more nuanced information about the nondefinability of Kreisel's and
Po{\l}acik's unary connectives. The finitary and combinatorial nature of our
method makes it more resilient to changes in metatheory than common semantic
approaches, whose robustness tends to waver once we pass to non-classical and
especially anti-classical settings. Furthermore, we can easily transcribe the
problem-specific subproofs into univalent type theory and check them using the
Agda proof assistant.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03664" title="Abstract">arXiv:2310.03664</a> (cross-list from eess.IV) [<a href="/pdf/2310.03664" title="Download PDF">pdf</a>, <a href="/format/2310.03664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certification of Deep Learning Models for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Laousy%2C+O">Othmane Laousy</a>, 
<a href="/search/eess?searchtype=author&query=Araujo%2C+A">Alexandre Araujo</a>, 
<a href="/search/eess?searchtype=author&query=Chassagnon%2C+G">Guillaume Chassagnon</a>, 
<a href="/search/eess?searchtype=author&query=Paragios%2C+N">Nikos Paragios</a>, 
<a href="/search/eess?searchtype=author&query=Revel%2C+M">Marie-Pierre Revel</a>, 
<a href="/search/eess?searchtype=author&query=Vakalopoulou%2C+M">Maria Vakalopoulou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In medical imaging, segmentation models have known a significant improvement
in the past decade and are now used daily in clinical practice. However,
similar to classification models, segmentation models are affected by
adversarial attacks. In a safety-critical field like healthcare, certifying
model predictions is of the utmost importance. Randomized smoothing has been
introduced lately and provides a framework to certify models and obtain
theoretical guarantees. In this paper, we present for the first time a
certified segmentation baseline for medical imaging based on randomized
smoothing and diffusion models. Our results show that leveraging the power of
denoising diffusion probabilistic models helps us overcome the limits of
randomized smoothing. We conduct extensive experiments on five public datasets
of chest X-rays, skin lesions, and colonoscopies, and empirically show that we
are able to maintain high certified Dice scores even for highly perturbed
images. Our work represents the first attempt to certify medical image
segmentation models, and we aspire for it to set a foundation for future
benchmarks in this crucial and largely uncharted area.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03688" title="Abstract">arXiv:2310.03688</a> (cross-list from eess.AS) [<a href="/pdf/2310.03688" title="Download PDF">pdf</a>, <a href="/format/2310.03688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speaker localization using direct path dominance test based on sound  field directivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rafaely%2C+B">Boaz Rafaely</a>, 
<a href="/search/eess?searchtype=author&query=Alhaiany%2C+K">Koby Alhaiany</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Signal Processing, vol. 143, pp. 42 - 47, 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Estimation of the direction-of-arrival (DoA) of a speaker in a room is
important in many audio signal processing applications. Environments with
reverberation that masks the DoA information are particularly challenging.
Recently, a DoA estimation method that is robust to reverberation has been
developed. This method identifies time-frequency bins dominated by the
contribution from the direct path, which carries the correct DoA information.
However, its implementation is computationally demanding as it requires
frequency smoothing to overcome the effect of coherent early reflections and
matrix decomposition to apply the direct-path dominance (DPD) test. In this
work, a novel computationally-efficient alternative to the DPD test is
proposed, based on the directivity measure for sensor arrays, which requires
neither frequency smoothing nor matrix decomposition, and which has been
reformulated for sound field directivity with spherical microphone arrays. The
paper presents the proposed method and a comparison to previous methods under a
range of reverberation and noise conditions. Result demonstrate that the
proposed method shows comparable performance to the original method in terms of
robustness to reverberation and noise, and is about four times more
computationally efficient for the given experiment.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03692" title="Abstract">arXiv:2310.03692</a> (cross-list from econ.TH) [<a href="/pdf/2310.03692" title="Download PDF">pdf</a>, <a href="/ps/2310.03692" title="Download PostScript">ps</a>, <a href="/format/2310.03692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Substitutes markets with budget constraints: solving for competitive and  optimal prices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Finster%2C+S">Simon Finster</a>, 
<a href="/search/econ?searchtype=author&query=Goldberg%2C+P">Paul Goldberg</a>, 
<a href="/search/econ?searchtype=author&query=Lock%2C+E">Edwin Lock</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WINE'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Markets with multiple divisible goods have been studied widely from the
perspective of revenue and welfare. In general, it is well known that envy-free
revenue-maximal outcomes can result in lower welfare than competitive
equilibrium outcomes. We study a market in which buyers have quasilinear
utilities with linear substitutes valuations and budget constraints, and the
seller must find prices and an envy-free allocation that maximise revenue or
welfare. Our setup mirrors markets such as ad auctions and auctions for the
exchange of financial assets. We prove that the unique competitive equilibrium
prices are also envy-free revenue-maximal. This coincidence of maximal revenue
and welfare is surprising and breaks down even when buyers have
piecewise-linear valuations. We present a novel characterisation of the set of
"feasible" prices at which demand does not exceed supply, show that this set
has an elementwise minimal price vector, and demonstrate that these prices
maximise revenue and welfare. The proof also implies an algorithm for finding
this unique price vector.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03696" title="Abstract">arXiv:2310.03696</a> (cross-list from stat.ML) [<a href="/pdf/2310.03696" title="Download PDF">pdf</a>, <a href="/format/2310.03696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Banach Space Optimality of Neural Architectures With Multivariate  Nonlinearities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Parhi%2C+R">Rahul Parhi</a>, 
<a href="/search/stat?searchtype=author&query=Unser%2C+M">Michael Unser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We investigate the variational optimality (specifically, the Banach space
optimality) of a large class of neural architectures with multivariate
nonlinearities/activation functions. To that end, we construct a new family of
Banach spaces defined via a regularization operator and the $k$-plane
transform. We prove a representer theorem that states that the solution sets to
learning problems posed over these Banach spaces are completely characterized
by neural architectures with multivariate nonlinearities. These optimal
architectures have skip connections and are tightly connected to orthogonal
weight normalization and multi-index models, both of which have received
considerable interest in the neural network community. Our framework is
compatible with a number of classical nonlinearities including the rectified
linear unit (ReLU) activation function, the norm activation function, and the
radial basis functions found in the theory of thin-plate/polyharmonic splines.
We also show that the underlying spaces are special instances of reproducing
kernel Banach spaces and variation spaces. Our results shed light on the
regularity of functions learned by neural networks trained on data,
particularly with multivariate nonlinearities, and provide new theoretical
motivation for several architectural choices found in practice.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03722" title="Abstract">arXiv:2310.03722</a> (cross-list from math.ST) [<a href="/pdf/2310.03722" title="Download PDF">pdf</a>, <a href="/format/2310.03722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anytime-valid t-tests and confidence sequences for Gaussian means with  unknown variance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+H">Hongjian Wang</a>, 
<a href="/search/math?searchtype=author&query=Ramdas%2C+A">Aaditya Ramdas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$
of a Gaussian distribution with unknown variance $\sigma$. Curiously, he
employed both an improper (right Haar) mixture over $\sigma$ and an improper
(flat) mixture over $\mu$. Here, we elaborate carefully on the details of his
construction, which use generalized nonintegrable martingales and an extended
Ville's inequality. While this does yield a sequential t-test, it does not
yield an ``e-process'' (due to the nonintegrability of his martingale). In this
paper, we develop two new e-processes and confidence sequences for the same
setting: one is a test martingale in a reduced filtration, while the other is
an e-process in the canonical data filtration. These are respectively obtained
by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right
Haar mixture over $\sigma$ with the maximum likelihood estimate under the null,
as done in universal inference. We also analyze the width of resulting
confidence sequences, which have a curious dependence on the error probability
$\alpha$. Numerical experiments are provided along the way to compare and
contrast the various approaches.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03735" title="Abstract">arXiv:2310.03735</a> (cross-list from quant-ph) [<a href="/pdf/2310.03735" title="Download PDF">pdf</a>, <a href="/format/2310.03735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Uncertainty Principle for the Curvelet Transform, and the  Infeasibility of Quantum Algorithms for Finding Short Lattice Vectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+Y">Yi-Kai Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The curvelet transform is a special type of wavelet transform, which is
useful for estimating the locations and orientations of waves propagating in
Euclidean space. We prove an uncertainty principle that lower-bounds the
variance of these estimates, for radial wave functions in n dimensions.
<br />As an application of this uncertainty principle, we show the infeasibility of
one approach to constructing quantum algorithms for solving lattice problems,
such as the approximate shortest vector problem (approximate-SVP), and bounded
distance decoding (BDD). This gives insight into the computational
intractability of approximate-SVP, which plays an important role in algorithms
for integer programming, and in post-quantum cryptosystems.
<br />In this approach to solving lattice problems, one prepares quantum
superpositions of Gaussian-like wave functions centered at lattice points. A
key step in this procedure requires finding the center of each Gaussian-like
wave function, using the quantum curvelet transform. We show that, for any
choice of the Gaussian-like wave function, the error in this step will be above
the threshold required to solve BDD and approximate-SVP.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Fri,  6 Oct 23</h3>
<dl>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1802.00810" title="Abstract">arXiv:1802.00810</a> (replaced) [<a href="/pdf/1802.00810" title="Download PDF">pdf</a>, <a href="/ps/1802.00810" title="Download PostScript">ps</a>, <a href="/format/1802.00810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Genomics: A Concise Overview
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Yue%2C+T">Tianwei Yue</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+Y">Yuanxin Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+L">Longxiang Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Gu%2C+C">Chunming Gu</a>, 
<a href="/search/q-bio?searchtype=author&query=Xue%2C+H">Haoru Xue</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+W">Wenping Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Lyu%2C+Q">Qi Lyu</a>, 
<a href="/search/q-bio?searchtype=author&query=Dun%2C+Y">Yujie Dun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1812.01911" title="Abstract">arXiv:1812.01911</a> (replaced) [<a href="/pdf/1812.01911" title="Download PDF">pdf</a>, <a href="/format/1812.01911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New reducible configurations for graph multicoloring with application to  the experimental resolution of McDiarmid-Reed&#x27;s Conjecture (extended version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Godin%2C+J">Jean-Christophe Godin</a>, 
<a href="/search/math?searchtype=author&query=Togni%2C+O">Olivier Togni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, new version with more reducible configurations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.05957" title="Abstract">arXiv:1912.05957</a> (replaced) [<a href="/pdf/1912.05957" title="Download PDF">pdf</a>, <a href="/format/1912.05957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text as Environment: A Deep Reinforcement Learning Text Readability  Assessment Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+H">Hamid Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Khasteh%2C+S+H">Seyed Hossein Khasteh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.11559" title="Abstract">arXiv:2010.11559</a> (replaced) [<a href="/pdf/2010.11559" title="Download PDF">pdf</a>, <a href="/format/2010.11559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Graph Laplacian with MCP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yangjing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Toh%2C+K">Kim-Chuan Toh</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Defeng Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.00696" title="Abstract">arXiv:2102.00696</a> (replaced) [<a href="/pdf/2102.00696" title="Download PDF">pdf</a>, <a href="/format/2102.00696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Weather Forecasting using Convolutional-LSTM with Attention  and Context Matcher Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tekin%2C+S+F">Selim Furkan Tekin</a>, 
<a href="/search/cs?searchtype=author&query=Fazla%2C+A">Arda Fazla</a>, 
<a href="/search/cs?searchtype=author&query=Kozat%2C+S+S">Suleyman Serdar Kozat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> - In our journal submission, we removed the integration of the observational data section since it was not used in the experiments. Thus, we also removed the authors from the paper who were responsible for that section. - In the second version, we also performed an experiment on WeatherBench. We compare our results with the Physical Weather Forecasting Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.04904" title="Abstract">arXiv:2103.04904</a> (replaced) [<a href="/pdf/2103.04904" title="Download PDF">pdf</a>, <a href="/ps/2103.04904" title="Download PostScript">ps</a>, <a href="/format/2103.04904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bipartite secret sharing and staircases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Csirmaz%2C+L">Laszlo Csirmaz</a>, 
<a href="/search/cs?searchtype=author&query=Mat%C3%BA%C5%A1%2C+F">Franti&#x161;ek Mat&#xfa;&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Padr%C3%B3%2C+C">Carles Padr&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Discrete Mathematics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.07454" title="Abstract">arXiv:2104.07454</a> (replaced) [<a href="/pdf/2104.07454" title="Download PDF">pdf</a>, <a href="/format/2104.07454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory Capacity of Recurrent Neural Networks with Matrix Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Renanse%2C+A">Animesh Renanse</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Alok Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+R">Rohitash Chandra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.06178" title="Abstract">arXiv:2105.06178</a> (replaced) [<a href="/pdf/2105.06178" title="Download PDF">pdf</a>, <a href="/format/2105.06178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Paying Attention to Astronomical Transients: Introducing the Time-series  Transformer for Photometric Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Allam%2C+T">Tarek Allam Jr.</a>, 
<a href="/search/astro-ph?searchtype=author&query=McEwen%2C+J+D">Jason D. McEwen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript Accepted to RAS Techniques and Instruments. 15 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.08086" title="Abstract">arXiv:2107.08086</a> (replaced) [<a href="/pdf/2107.08086" title="Download PDF">pdf</a>, <a href="/format/2107.08086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Information-state based Approach to the Optimal Output Feedback  Control of Nonlinear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goyal%2C+R">Raman Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+M+N+G">Mohamed Naveed Gul Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Aayushman Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Chakravorty%2C+S">Suman Chakravorty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.05641" title="Abstract">arXiv:2108.05641</a> (replaced) [<a href="/pdf/2108.05641" title="Download PDF">pdf</a>, <a href="/format/2108.05641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinpeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haiyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xudong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Senzhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+K">Kaimin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaqi Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.02494" title="Abstract">arXiv:2109.02494</a> (replaced) [<a href="/pdf/2109.02494" title="Download PDF">pdf</a>, <a href="/format/2109.02494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Gabor phase retrieval in Gaussian shift-invariant spaces via  biorthogonality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Grohs%2C+P">Philipp Grohs</a>, 
<a href="/search/math?searchtype=author&query=Liehr%2C+L">Lukas Liehr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 9 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Constr. Approx. (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.04939" title="Abstract">arXiv:2109.04939</a> (replaced) [<a href="/pdf/2109.04939" title="Download PDF">pdf</a>, <a href="/format/2109.04939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Human Sentence Processing with Left-Corner Recurrent Neural  Network Grammars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+R">Ryo Yoshida</a>, 
<a href="/search/cs?searchtype=author&query=Noji%2C+H">Hiroshi Noji</a>, 
<a href="/search/cs?searchtype=author&query=Oseki%2C+Y">Yohei Oseki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.03991" title="Abstract">arXiv:2110.03991</a> (replaced) [<a href="/pdf/2110.03991" title="Download PDF">pdf</a>, <a href="/format/2110.03991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Differential Privacy and Byzantine Resilience in Distributed  SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guerraoui%2C+R">Rachid Guerraoui</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+N">Nirupam Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Pinot%2C+R">Rafael Pinot</a>, 
<a href="/search/cs?searchtype=author&query=Rouault%2C+S">Sebastien Rouault</a>, 
<a href="/search/cs?searchtype=author&query=Stephan%2C+J">John Stephan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.14883" title="Abstract">arXiv:2110.14883</a> (replaced) [<a href="/pdf/2110.14883" title="Download PDF">pdf</a>, <a href="/format/2110.14883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel  Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shenggui Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Z">Zhengda Bian</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiarui Fang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haichen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.15497" title="Abstract">arXiv:2110.15497</a> (replaced) [<a href="/pdf/2110.15497" title="Download PDF">pdf</a>, <a href="/format/2110.15497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Foreground Extraction via Deep Region Competition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Peiyu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Sirui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.02062" title="Abstract">arXiv:2111.02062</a> (replaced) [<a href="/pdf/2111.02062" title="Download PDF">pdf</a>, <a href="/format/2111.02062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linking Across Data Granularity: Fitting Multivariate Hawkes Processes  to Partially Interval-Censored Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Calderon%2C+P">Pio Calderon</a>, 
<a href="/search/cs?searchtype=author&query=Soen%2C+A">Alexander Soen</a>, 
<a href="/search/cs?searchtype=author&query=Rizoiu%2C+M">Marian-Andrei Rizoiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.12232" title="Abstract">arXiv:2111.12232</a> (replaced) [<a href="/pdf/2111.12232" title="Download PDF">pdf</a>, <a href="/format/2111.12232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PMSSC: Parallelizable multi-subset based self-expressive model for  subspace clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hotta%2C+K">Katsuya Hotta</a>, 
<a href="/search/cs?searchtype=author&query=Akashi%2C+T">Takuya Akashi</a>, 
<a href="/search/cs?searchtype=author&query=Tokai%2C+S">Shogo Tokai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.05120" title="Abstract">arXiv:2112.05120</a> (replaced) [<a href="/pdf/2112.05120" title="Download PDF">pdf</a>, <a href="/format/2112.05120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Convergence of Federated Averaging Langevin Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Deng%2C+W">Wei Deng</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+Q">Qian Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Ma%2C+Y">Yi-An Ma</a>, 
<a href="/search/stat?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/stat?searchtype=author&query=Lin%2C+G">Guang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A polished proof without the federated formulation of Langevin diffusion to avoid confusion
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.08417" title="Abstract">arXiv:2112.08417</a> (replaced) [<a href="/pdf/2112.08417" title="Download PDF">pdf</a>, <a href="/format/2112.08417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterization of causal ancestral graphs for time series with latent  confounders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gerhardus%2C+A">Andreas Gerhardus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 67 pages (including supplement), 16 figures, accepted at The Annals of Statistics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.02824" title="Abstract">arXiv:2201.02824</a> (replaced) [<a href="/pdf/2201.02824" title="Download PDF">pdf</a>, <a href="/format/2201.02824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal 1-Wasserstein Distance for WGANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=St%C3%A9phanovitch%2C+A">Arthur St&#xe9;phanovitch</a>, 
<a href="/search/stat?searchtype=author&query=Tanielian%2C+U">Ugo Tanielian</a>, 
<a href="/search/stat?searchtype=author&query=Cadre%2C+B">Beno&#xee;t Cadre</a>, 
<a href="/search/stat?searchtype=author&query=Klutchnikoff%2C+N">Nicolas Klutchnikoff</a>, 
<a href="/search/stat?searchtype=author&query=Biau%2C+G">G&#xe9;rard Biau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.09573" title="Abstract">arXiv:2202.09573</a> (replaced) [<a href="/pdf/2202.09573" title="Download PDF">pdf</a>, <a href="/format/2202.09573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversity in deep generative models and generative AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Turinici%2C+G">Gabriel Turinici</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.13103" title="Abstract">arXiv:2202.13103</a> (replaced) [<a href="/pdf/2202.13103" title="Download PDF">pdf</a>, <a href="/ps/2202.13103" title="Download PostScript">ps</a>, <a href="/format/2202.13103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monotone Classes Beyond VNP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+P">Prerona Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Gajjar%2C+K">Kshitij Gajjar</a>, 
<a href="/search/cs?searchtype=author&query=Tengse%2C+A">Anamay Tengse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages; made changes suggested by reviewers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.06184" title="Abstract">arXiv:2203.06184</a> (replaced) [<a href="/pdf/2203.06184" title="Download PDF">pdf</a>, <a href="/format/2203.06184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GSDA: Generative Adversarial Network-based Semi-Supervised Data  Augmentation for Ultrasound Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhaoshan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lv%2C+Q">Qiujie Lv</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+C+H">Chau Hung Lee</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+L">Lei Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Heliyon Accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.08196" title="Abstract">arXiv:2203.08196</a> (replaced) [<a href="/pdf/2203.08196" title="Download PDF">pdf</a>, <a href="/ps/2203.08196" title="Download PostScript">ps</a>, <a href="/format/2203.08196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Damping with Hierarchical Adaptive Quadrature for Efficient  Fourier Pricing of Multi-Asset Options in L&#xe9;vy Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Samet%2C+M">Michael Samet</a>, 
<a href="/search/q-fin?searchtype=author&query=Bayer%2C+C">Christian Bayer</a>, 
<a href="/search/q-fin?searchtype=author&query=Hammouda%2C+C+B">Chiheb Ben Hammouda</a>, 
<a href="/search/q-fin?searchtype=author&query=Papapantoleon%2C+A">Antonis Papapantoleon</a>, 
<a href="/search/q-fin?searchtype=author&query=Tempone%2C+R">Ra&#xfa;l Tempone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Numerical Analysis (math.NA); Pricing of Securities (q-fin.PR)

</div>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.05923" title="Abstract">arXiv:2204.05923</a> (replaced) [<a href="/pdf/2204.05923" title="Download PDF">pdf</a>, <a href="/ps/2204.05923" title="Download PostScript">ps</a>, <a href="/format/2204.05923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Algebraically Converging Stochastic Gradient Descent Algorithm for  Global Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Engquist%2C+B">Bj&#xf6;rn Engquist</a>, 
<a href="/search/math?searchtype=author&query=Ren%2C+K">Kui Ren</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+Y">Yunan Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.03519" title="Abstract">arXiv:2205.03519</a> (replaced) [<a href="/pdf/2205.03519" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Deep Unrolled Reconstruction Using Regularization by  Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+P">Peizhou Huang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Chaoyi Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiaoliang Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xiaojuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+L">Liang Dong</a>, 
<a href="/search/eess?searchtype=author&query=Ying%2C+L">Leslie Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.05250" title="Abstract">arXiv:2205.05250</a> (replaced) [<a href="/pdf/2205.05250" title="Download PDF">pdf</a>, <a href="/ps/2205.05250" title="Download PostScript">ps</a>, <a href="/format/2205.05250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial-temporal associations representation and application for process  monitoring using graph convolution neural network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaojun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chunhua Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiwen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+W">Weihua Gui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.11119" title="Abstract">arXiv:2205.11119</a> (replaced) [<a href="/pdf/2205.11119" title="Download PDF">pdf</a>, <a href="/format/2205.11119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NPGA: A Unified Algorithmic Framework for Decentralized  Constraint-Coupled Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jingwang Li</a>, 
<a href="/search/math?searchtype=author&query=Su%2C+H">Housheng Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.05895" title="Abstract">arXiv:2206.05895</a> (replaced) [<a href="/pdf/2206.05895" title="Download PDF">pdf</a>, <a href="/format/2206.05895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Diffusion Energy-Based Model for Interpretable Text Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Peiyu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Sirui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+B">Baoxiong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+B">Bo Pang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruiqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.03299" title="Abstract">arXiv:2207.03299</a> (replaced) [<a href="/pdf/2207.03299" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Academic information retrieval using citation clusters: In-depth  evaluation based on systematic reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bascur%2C+J+P">Juan Pablo Bascur</a>, 
<a href="/search/cs?searchtype=author&query=Verberne%2C+S">Suzan Verberne</a>, 
<a href="/search/cs?searchtype=author&query=van+Eck%2C+N+J">Nees Jan van Eck</a>, 
<a href="/search/cs?searchtype=author&query=Waltman%2C+L">Ludo Waltman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.06399" title="Abstract">arXiv:2207.06399</a> (replaced) [<a href="/pdf/2207.06399" title="Download PDF">pdf</a>, <a href="/format/2207.06399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pattern recognition in the nucleation kinetics of non-equilibrium  self-assembly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Evans%2C+C+G">Constantine Glen Evans</a>, 
<a href="/search/cond-mat?searchtype=author&query=O%27Brien%2C+J">Jackson O&#x27;Brien</a>, 
<a href="/search/cond-mat?searchtype=author&query=Winfree%2C+E">Erik Winfree</a>, 
<a href="/search/cond-mat?searchtype=author&query=Murugan%2C+A">Arvind Murugan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 + 12 pages, 6 + 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Statistical Mechanics (cond-mat.stat-mech); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.11447" title="Abstract">arXiv:2207.11447</a> (replaced) [<a href="/pdf/2207.11447" title="Download PDF">pdf</a>, <a href="/format/2207.11447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Handling Data Heterogeneity in Federated Learning via Knowledge  Distillation and Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+X">Xinyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yichun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jingwen Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.03246" title="Abstract">arXiv:2208.03246</a> (replaced) [<a href="/pdf/2208.03246" title="Download PDF">pdf</a>, <a href="/ps/2208.03246" title="Download PostScript">ps</a>, <a href="/format/2208.03246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Asymptotic Analysis of Ensemble Kalman Updates: Effective Dimension  and Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ghattas%2C+O+A">Omar Al Ghattas</a>, 
<a href="/search/stat?searchtype=author&query=Sanz-Alonso%2C+D">Daniel Sanz-Alonso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Numerical Analysis (math.NA); Statistics Theory (math.ST); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.05241" title="Abstract">arXiv:2208.05241</a> (replaced) [<a href="/pdf/2208.05241" title="Download PDF">pdf</a>, <a href="/format/2208.05241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CANet: Channel Extending and Axial Attention Catching Network for  Multi-structure Kidney Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bu%2C+Z">Zhenyu Bu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+K">Kai-Ni Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+G">Guang-Quan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> KiPA2022 Challenge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.07708" title="Abstract">arXiv:2208.07708</a> (replaced) [<a href="/e-print/2208.07708" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction Methods for Galois LCD codes over Finite Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+G+K">Gyanendra K. Verma</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Astha Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R+K">R. K. Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There are many typos and mathematical typos as well
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12266" title="Abstract">arXiv:2208.12266</a> (replaced) [<a href="/pdf/2208.12266" title="Download PDF">pdf</a>, <a href="/format/2208.12266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding speech perception from non-invasive brain recordings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=D%C3%A9fossez%2C+A">Alexandre D&#xe9;fossez</a>, 
<a href="/search/eess?searchtype=author&query=Caucheteux%2C+C">Charlotte Caucheteux</a>, 
<a href="/search/eess?searchtype=author&query=Rapin%2C+J">J&#xe9;r&#xe9;my Rapin</a>, 
<a href="/search/eess?searchtype=author&query=Kabeli%2C+O">Ori Kabeli</a>, 
<a href="/search/eess?searchtype=author&query=King%2C+J">Jean-R&#xe9;mi King</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated version following publication in Nature Machine Intelligence (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05917" title="Abstract">arXiv:2209.05917</a> (replaced) [<a href="/pdf/2209.05917" title="Download PDF">pdf</a>, <a href="/format/2209.05917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpaDE: Improving Sparse Representations using a Dual Document Encoder  for First-stage Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Eunseong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sunkyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+M">Minjin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+H">Hyeseon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Young-In Song</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jongwuk Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 31st ACM International Conference on Information and Knowledge Management (CIKM '22). 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.12148" title="Abstract">arXiv:2209.12148</a> (replaced) [<a href="/pdf/2209.12148" title="Download PDF">pdf</a>, <a href="/format/2209.12148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Masked Convolutional Transformer Block for Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madan%2C+N">Neelu Madan</a>, 
<a href="/search/cs?searchtype=author&query=Ristea%2C+N">Nicolae-Catalin Ristea</a>, 
<a href="/search/cs?searchtype=author&query=Ionescu%2C+R+T">Radu Tudor Ionescu</a>, 
<a href="/search/cs?searchtype=author&query=Nasrollahi%2C+K">Kamal Nasrollahi</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
<a href="/search/cs?searchtype=author&query=Moeslund%2C+T+B">Thomas B. Moeslund</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+M">Mubarak Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE Transactions on Pattern Analysis and Machine Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.14951" title="Abstract">arXiv:2209.14951</a> (replaced) [<a href="/pdf/2209.14951" title="Download PDF">pdf</a>, <a href="/format/2209.14951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed decentralized receding horizon control for very large-scale  networks with application to satellite mega-constellations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pedroso%2C+L">Leonardo Pedroso</a>, 
<a href="/search/eess?searchtype=author&query=Batista%2C+P">Pedro Batista</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Control Engineering Practice, vol. 141, pp. 105728, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01422" title="Abstract">arXiv:2210.01422</a> (replaced) [<a href="/pdf/2210.01422" title="Download PDF">pdf</a>, <a href="/format/2210.01422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Varying Propensity Score to Bridge the Gap between the Past and  Present
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fakoor%2C+R">Rasool Fakoor</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+J">Jonas Mueller</a>, 
<a href="/search/cs?searchtype=author&query=Lipton%2C+Z+C">Zachary C. Lipton</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+P">Pratik Chaudhari</a>, 
<a href="/search/cs?searchtype=author&query=Smola%2C+A+J">Alexander J. Smola</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01944" title="Abstract">arXiv:2210.01944</a> (replaced) [<a href="/pdf/2210.01944" title="Download PDF">pdf</a>, <a href="/format/2210.01944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Large Scale Synthetic Graph Dataset Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darabi%2C+S">Sajad Darabi</a>, 
<a href="/search/cs?searchtype=author&query=Bigaj%2C+P">Piotr Bigaj</a>, 
<a href="/search/cs?searchtype=author&query=Majchrowski%2C+D">Dawid Majchrowski</a>, 
<a href="/search/cs?searchtype=author&query=Kasymov%2C+A">Artur Kasymov</a>, 
<a href="/search/cs?searchtype=author&query=Morkisz%2C+P">Pawel Morkisz</a>, 
<a href="/search/cs?searchtype=author&query=Fit-Florea%2C+A">Alex Fit-Florea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.17505" title="Abstract">arXiv:2210.17505</a> (replaced) [<a href="/pdf/2210.17505" title="Download PDF">pdf</a>, <a href="/format/2210.17505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Space-Fluid Adaptive Sampling by Self-Organisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casadei%2C+R">Roberto Casadei</a>, 
<a href="/search/cs?searchtype=author&query=Mariani%2C+S">Stefano Mariani</a>, 
<a href="/search/cs?searchtype=author&query=Pianini%2C+D">Danilo Pianini</a>, 
<a href="/search/cs?searchtype=author&query=Viroli%2C+M">Mirko Viroli</a>, 
<a href="/search/cs?searchtype=author&query=Zambonelli%2C+F">Franco Zambonelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.00635" title="Abstract">arXiv:2211.00635</a> (replaced) [<a href="/pdf/2211.00635" title="Download PDF">pdf</a>, <a href="/format/2211.00635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-stage LLM Fine-tuning with Less Specialization and More  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Si Si</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Daliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lukasik%2C+M">Michal Lukasik</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Felix Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Dhillon%2C+I+S">Inderjit S Dhillon</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sanjiv Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.01856" title="Abstract">arXiv:2211.01856</a> (replaced) [<a href="/pdf/2211.01856" title="Download PDF">pdf</a>, <a href="/format/2211.01856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Generative Models for Simulation of EMG During Naturalistic  Movements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shihan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+A+K">Alexander Kenneth Clarke</a>, 
<a href="/search/cs?searchtype=author&query=Maksymenko%2C+K">Kostiantyn Maksymenko</a>, 
<a href="/search/cs?searchtype=author&query=Deslauriers-Gauthier%2C+S">Samuel Deslauriers-Gauthier</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+X">Xinjun Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiangyang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+D">Dario Farina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Signal Processing (eess.SP); Biological Physics (physics.bio-ph)

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.03660" title="Abstract">arXiv:2211.03660</a> (replaced) [<a href="/pdf/2211.03660" title="Download PDF">pdf</a>, <a href="/format/2211.03660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SC-DepthV3: Robust Self-supervised Monocular Depth Estimation for  Dynamic Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Libo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jia-Wang Bian</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Huangying Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Reid%2C+I">Ian Reid</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in TPAMI; The code will be available at <a href="https://github.com/JiawangBian/sc_depth_pl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06300" title="Abstract">arXiv:2211.06300</a> (replaced) [<a href="/pdf/2211.06300" title="Download PDF">pdf</a>, <a href="/format/2211.06300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithmic analysis towards time-domain extended source waveform  inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yang%2C+P">Pengliang Yang</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07091" title="Abstract">arXiv:2211.07091</a> (replaced) [<a href="/pdf/2211.07091" title="Download PDF">pdf</a>, <a href="/format/2211.07091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiViT: Extremely Compressed Binary Vision Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yefei He</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Z">Zhenyu Lou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Luoming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11961" title="Abstract">arXiv:2211.11961</a> (replaced) [<a href="/pdf/2211.11961" title="Download PDF">pdf</a>, <a href="/ps/2211.11961" title="Download PostScript">ps</a>, <a href="/format/2211.11961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online facility location with timed-requests and congestion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+A">Arghya Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Vaze%2C+R">Rahul Vaze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13118" title="Abstract">arXiv:2211.13118</a> (replaced) [<a href="/pdf/2211.13118" title="Download PDF">pdf</a>, <a href="/format/2211.13118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision Diagram-Based Branch-and-Bound with Caching for Dominance and  Suboptimality Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Copp%C3%A9%2C+V">Vianney Copp&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Gillard%2C+X">Xavier Gillard</a>, 
<a href="/search/cs?searchtype=author&query=Schaus%2C+P">Pierre Schaus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to INFORMS Journal on Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02648" title="Abstract">arXiv:2212.02648</a> (replaced) [<a href="/pdf/2212.02648" title="Download PDF">pdf</a>, <a href="/format/2212.02648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moayeri%2C+M">Mazda Moayeri</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+S">Sahil Singla</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS '23 (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06074" title="Abstract">arXiv:2212.06074</a> (replaced) [<a href="/pdf/2212.06074" title="Download PDF">pdf</a>, <a href="/format/2212.06074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regression with Label Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghazi%2C+B">Badih Ghazi</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+P">Pritish Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Ravi Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Leeman%2C+E">Ethan Leeman</a>, 
<a href="/search/cs?searchtype=author&query=Manurangsi%2C+P">Pasin Manurangsi</a>, 
<a href="/search/cs?searchtype=author&query=Varadarajan%2C+A+V">Avinash V Varadarajan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chiyuan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appeared at ICLR '23, 28 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06921" title="Abstract">arXiv:2212.06921</a> (replaced) [<a href="/pdf/2212.06921" title="Download PDF">pdf</a>, <a href="/format/2212.06921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Losses over Labels: Weakly Supervised Learning via Direct Loss  Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sam%2C+D">Dylan Sam</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+J+Z">J. Zico Kolter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures, AAAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.12055" title="Abstract">arXiv:2212.12055</a> (replaced) [<a href="/pdf/2212.12055" title="Download PDF">pdf</a>, <a href="/ps/2212.12055" title="Download PostScript">ps</a>, <a href="/format/2212.12055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DRL-based Energy-Efficient Baseband Function Deployments for  Service-Oriented Open RAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haiyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Emami%2C+A">Amin Emami</a>, 
<a href="/search/cs?searchtype=author&query=Assis%2C+K">Karcius Assis</a>, 
<a href="/search/cs?searchtype=author&query=Vafeas%2C+A">Antonis Vafeas</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruizhi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Nejabati%2C+R">Reza Nejabati</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shuangyi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Simeonidou%2C+D">Dimitra Simeonidou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.01132" title="Abstract">arXiv:2301.01132</a> (replaced) [<a href="/pdf/2301.01132" title="Download PDF">pdf</a>, <a href="/format/2301.01132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Time Universal Hashing Quantum Digital Signatures without Perfect  Keys
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+B">Bing-Hong Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Xie%2C+Y">Yuan-Mei Xie</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cao%2C+X">Xiao-Yu Cao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+C">Chen-Long Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fu%2C+Y">Yao Fu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yin%2C+H">Hua-Lei Yin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+Z">Zeng-Bing Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. Appl. 20, 044011(2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02077" title="Abstract">arXiv:2301.02077</a> (replaced) [<a href="/pdf/2301.02077" title="Download PDF">pdf</a>, <a href="/format/2301.02077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the stability and convergence of Discontinuous Galerkin schemes for  incompressible flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gazca-Orozco%2C+P+A">Pablo Alexei Gazca-Orozco</a>, 
<a href="/search/math?searchtype=author&query=Kaltenbach%2C+A">Alex Kaltenbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> convergence proof has been added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04494" title="Abstract">arXiv:2301.04494</a> (replaced) [<a href="/pdf/2301.04494" title="Download PDF">pdf</a>, <a href="/format/2301.04494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-label Image Classification using Adaptive Graph Convolutional  Networks: from a Single Domain to Multiple Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+I+P">Indel Pal Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbel%2C+E">Enjie Ghorbel</a>, 
<a href="/search/cs?searchtype=author&query=Oyedotun%2C+O">Oyebade Oyedotun</a>, 
<a href="/search/cs?searchtype=author&query=Aouada%2C+D">Djamila Aouada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04554" title="Abstract">arXiv:2301.04554</a> (replaced) [<a href="/pdf/2301.04554" title="Download PDF">pdf</a>, <a href="/format/2301.04554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Detection of Backdoor Attacks via Density-based Clustering and  Centroids Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tondi%2C+B">Benedetta Tondi</a>, 
<a href="/search/cs?searchtype=author&query=Barni%2C+M">Mauro Barni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05603" title="Abstract">arXiv:2301.05603</a> (replaced) [<a href="/pdf/2301.05603" title="Download PDF">pdf</a>, <a href="/format/2301.05603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey of Dataset Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+S">Shiye Lei</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TPAMI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.06421" title="Abstract">arXiv:2301.06421</a> (replaced) [<a href="/e-print/2301.06421" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Alignment Dialogues: An Interactive Approach to AI Alignment in  Support Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pei-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tielman%2C+M+L">Myrthe L. Tielman</a>, 
<a href="/search/cs?searchtype=author&query=Heylen%2C+D+K+J">Dirk K.J. Heylen</a>, 
<a href="/search/cs?searchtype=author&query=Jonker%2C+C+M">Catholijn M. Jonker</a>, 
<a href="/search/cs?searchtype=author&query=van+Riemsdijk%2C+M+B">M. Birna van Riemsdijk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Withdraw because the content of the paper has been largely revised. The newest version is very different than the submitted one
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.07305" title="Abstract">arXiv:2301.07305</a> (replaced) [<a href="/pdf/2301.07305" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-Theoretic Approach for Manufacturing Cybersecurity Risk Modeling  and Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+H">Md Habibor Rahman</a> (1), 
<a href="/search/cs?searchtype=author&query=Hamedani%2C+E+Y">Erfan Yazdandoost Hamedani</a> (1), 
<a href="/search/cs?searchtype=author&query=Son%2C+Y">Young-Jun Son</a> (2), 
<a href="/search/cs?searchtype=author&query=Shafae%2C+M">Mohammed Shafae</a> (1) ((1) The University of Arizona, (2) Purdue University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09350" title="Abstract">arXiv:2301.09350</a> (replaced) [<a href="/pdf/2301.09350" title="Download PDF">pdf</a>, <a href="/format/2301.09350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-scale investigation of weakly-supervised deep learning for the  fine-grained semantic indexing of biomedical literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nentidis%2C+A">Anastasios Nentidis</a>, 
<a href="/search/cs?searchtype=author&query=Chatzopoulos%2C+T">Thomas Chatzopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Krithara%2C+A">Anastasia Krithara</a>, 
<a href="/search/cs?searchtype=author&query=Tsoumakas%2C+G">Grigorios Tsoumakas</a>, 
<a href="/search/cs?searchtype=author&query=Paliouras%2C+G">Georgios Paliouras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 figures, 4 tables. A more concise version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Biomedical Informatics, Volume 146, 2023, 104499, ISSN
  1532-0464
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Digital Libraries (cs.DL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02394" title="Abstract">arXiv:2302.02394</a> (replaced) [<a href="/pdf/2302.02394" title="Download PDF">pdf</a>, <a href="/format/2302.02394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eliminating Contextual Prior Bias for Semantic Image Editing via  Dual-Cycle Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zuopeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+T">Tianshu Chu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+E">Erdun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Daqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoyue Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by the IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02575" title="Abstract">arXiv:2302.02575</a> (replaced) [<a href="/pdf/2302.02575" title="Download PDF">pdf</a>, <a href="/ps/2302.02575" title="Download PostScript">ps</a>, <a href="/format/2302.02575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Energy-Harvesting Hybrid VLC/RF Networks with Random Receiver  Orientation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Raouf%2C+A+H+F">Amir Hossein Fahim Raouf</a>, 
<a href="/search/eess?searchtype=author&query=Anjinappa%2C+C+K">Chethan Kumar Anjinappa</a>, 
<a href="/search/eess?searchtype=author&query=Guvenc%2C+I">Ismail Guvenc</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02787" title="Abstract">arXiv:2302.02787</a> (replaced) [<a href="/pdf/2302.02787" title="Download PDF">pdf</a>, <a href="/format/2302.02787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative models for two-ground-truth partitions in networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mangold%2C+L">Lena Mangold</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+C">Camille Roth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02936" title="Abstract">arXiv:2302.02936</a> (replaced) [<a href="/pdf/2302.02936" title="Download PDF">pdf</a>, <a href="/format/2302.02936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private GANs, Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bie%2C+A">Alex Bie</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+G">Gautam Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guojun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages; revisions and new experiments from TMLR camera-ready + code release at <a href="https://github.com/alexbie98/dpgan-revisit">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11024" title="Abstract">arXiv:2302.11024</a> (replaced) [<a href="/pdf/2302.11024" title="Download PDF">pdf</a>, <a href="/format/2302.11024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient Flows for Sampling: Mean-Field Models, Gaussian Approximations  and Affine Invariance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+Y">Yifan Chen</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+D+Z">Daniel Zhengyu Huang</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+J">Jiaoyang Huang</a>, 
<a href="/search/stat?searchtype=author&query=Reich%2C+S">Sebastian Reich</a>, 
<a href="/search/stat?searchtype=author&query=Stuart%2C+A+M">Andrew M. Stuart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 82 pages, 8 figures (Welcome any feedback!)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11791" title="Abstract">arXiv:2302.11791</a> (replaced) [<a href="/e-print/2302.11791" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Additive complementary dual codes over $\mathbb{F}_{q^2}$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+G+K">Gyanendra K. Verma</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R+K">R. K. Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There has been major changes in this manuscript we will submit new one
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14296" title="Abstract">arXiv:2302.14296</a> (replaced) [<a href="/pdf/2302.14296" title="Download PDF">pdf</a>, <a href="/format/2302.14296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete-time Optimal Covariance Steering via Semidefinite Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rapakoulias%2C+G">George Rapakoulias</a>, 
<a href="/search/eess?searchtype=author&query=Tsiotras%2C+P">Panagiotis Tsiotras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in CDC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00047" title="Abstract">arXiv:2303.00047</a> (replaced) [<a href="/pdf/2303.00047" title="Download PDF">pdf</a>, <a href="/format/2303.00047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online On-Demand Multi-Robot Coverage Path Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitra%2C+R">Ratijit Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+I">Indranil Saha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01338" title="Abstract">arXiv:2303.01338</a> (replaced) [<a href="/pdf/2303.01338" title="Download PDF">pdf</a>, <a href="/format/2303.01338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdvRain: Adversarial Raindrops to Attack Camera-based Smart Vision  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guesmi%2C+A">Amira Guesmi</a>, 
<a href="/search/cs?searchtype=author&query=Hanif%2C+M+A">Muhammad Abdullah Hanif</a>, 
<a href="/search/cs?searchtype=author&query=Shafique%2C+M">Muhammad Shafique</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01751" title="Abstract">arXiv:2303.01751</a> (replaced) [<a href="/pdf/2303.01751" title="Download PDF">pdf</a>, <a href="/format/2303.01751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Momentum Multi-Marginal Schr&#xf6;dinger Bridge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+T">Tianrong Chen</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+G">Guan-Horng Liu</a>, 
<a href="/search/stat?searchtype=author&query=Tao%2C+M">Molei Tao</a>, 
<a href="/search/stat?searchtype=author&query=Theodorou%2C+E+A">Evangelos A. Theodorou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02950" title="Abstract">arXiv:2303.02950</a> (replaced) [<a href="/pdf/2303.02950" title="Download PDF">pdf</a>, <a href="/format/2303.02950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Intelligent Reflecting Surfaces for Interference Channels  with SWIPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Ying Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Celimuge Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+D+W+K">Derrick Wing Kwan Ng</a>, 
<a href="/search/cs?searchtype=author&query=Al-Dhahir%2C+N">Naofal Al-Dhahir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, accepted by IEEE Transactions on Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06088" title="Abstract">arXiv:2303.06088</a> (replaced) [<a href="/pdf/2303.06088" title="Download PDF">pdf</a>, <a href="/format/2303.06088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards domain-invariant Self-Supervised Learning with Batch Styles  Standardization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scalbert%2C+M">Marin Scalbert</a>, 
<a href="/search/cs?searchtype=author&query=Vakalopoulou%2C+M">Maria Vakalopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Couzini%C3%A9-Devy%2C+F">Florent Couzini&#xe9;-Devy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review as conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09234" title="Abstract">arXiv:2303.09234</a> (replaced) [<a href="/pdf/2303.09234" title="Download PDF">pdf</a>, <a href="/format/2303.09234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NAISR: A 3D Neural Additive Model for Interpretable Shape Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+Y">Yining Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Zdanski%2C+C">Carlton Zdanski</a>, 
<a href="/search/cs?searchtype=author&query=Kimbell%2C+J">Julia Kimbell</a>, 
<a href="/search/cs?searchtype=author&query=Prince%2C+A">Andrew Prince</a>, 
<a href="/search/cs?searchtype=author&query=Worden%2C+C">Cameron Worden</a>, 
<a href="/search/cs?searchtype=author&query=Kirse%2C+S">Samuel Kirse</a>, 
<a href="/search/cs?searchtype=author&query=Rutter%2C+C">Christopher Rutter</a>, 
<a href="/search/cs?searchtype=author&query=Shields%2C+B">Benjamin Shields</a>, 
<a href="/search/cs?searchtype=author&query=Dunn%2C+W">William Dunn</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+J">Jisan Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Niethammer%2C+M">Marc Niethammer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09874" title="Abstract">arXiv:2303.09874</a> (replaced) [<a href="/pdf/2303.09874" title="Download PDF">pdf</a>, <a href="/format/2303.09874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling the Link Between Image Statistics and Human Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hepburn%2C+A">Alexander Hepburn</a>, 
<a href="/search/cs?searchtype=author&query=Laparra%2C+V">Valero Laparra</a>, 
<a href="/search/cs?searchtype=author&query=Santos-Rodriguez%2C+R">Ra&#xfa;l Santos-Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Malo%2C+J">Jes&#xfa;s Malo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10650" title="Abstract">arXiv:2303.10650</a> (replaced) [<a href="/pdf/2303.10650" title="Download PDF">pdf</a>, <a href="/format/2303.10650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logic of Differentiable Logics: Towards a Uniform Semantics of DL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C5%9Alusarz%2C+N">Natalia &#x15a;lusarz</a>, 
<a href="/search/cs?searchtype=author&query=Komendantskaya%2C+E">Ekaterina Komendantskaya</a>, 
<a href="/search/cs?searchtype=author&query=Daggitt%2C+M+L">Matthew L. Daggitt</a>, 
<a href="/search/cs?searchtype=author&query=Stewart%2C+R">Robert Stewart</a>, 
<a href="/search/cs?searchtype=author&query=Stark%2C+K">Kathrin Stark</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LPAR'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12214" title="Abstract">arXiv:2303.12214</a> (replaced) [<a href="/pdf/2303.12214" title="Download PDF">pdf</a>, <a href="/format/2303.12214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-MIL: Boosting Multi-Instance Learning Schemes via Task-specific  Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kapse%2C+S">Saarthak Kapse</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Ke Ma</a>, 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+P">Prateek Prasanna</a>, 
<a href="/search/cs?searchtype=author&query=Saltz%2C+J">Joel Saltz</a>, 
<a href="/search/cs?searchtype=author&query=Vakalopoulou%2C+M">Maria Vakalopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Samaras%2C+D">Dimitris Samaras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to MICCAI 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14655" title="Abstract">arXiv:2303.14655</a> (replaced) [<a href="/pdf/2303.14655" title="Download PDF">pdf</a>, <a href="/format/2303.14655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for  Real-time Soccer Commentary Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Ji Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+T">Teng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+K">Kunyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yifan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+X">Xinyu Guan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaozhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Weidong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yu Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15375" title="Abstract">arXiv:2303.15375</a> (replaced) [<a href="/pdf/2303.15375" title="Download PDF">pdf</a>, <a href="/format/2303.15375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zeduo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kuper%2C+R">Reese Kuper</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chihun Song</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jinghan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Houxiang Ji</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Siddharth Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jiaqi Lou</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+I">Ipoom Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ren Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+J+H">Jung Ho Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianyin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N+S">Nam Sung Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by MICRO'23. Please refer to the <a href="https://doi.org/10.1145/3613424.3614256">this https URL</a> for the official version of this paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16887" title="Abstract">arXiv:2303.16887</a> (replaced) [<a href="/pdf/2303.16887" title="Download PDF">pdf</a>, <a href="/format/2303.16887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding the Effect of Pretraining Label Granularity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+G+Z">Guan Zhe Hong</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yin Cui</a>, 
<a href="/search/cs?searchtype=author&query=Fuxman%2C+A">Ariel Fuxman</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+S+H">Stanley H. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+E">Enming Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00195" title="Abstract">arXiv:2304.00195</a> (replaced) [<a href="/pdf/2304.00195" title="Download PDF">pdf</a>, <a href="/format/2304.00195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Abstractors and relational cross-attention: An inductive bias for  explicit relational reasoning in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Altabaa%2C+A">Awni Altabaa</a>, 
<a href="/search/stat?searchtype=author&query=Webb%2C+T">Taylor Webb</a>, 
<a href="/search/stat?searchtype=author&query=Cohen%2C+J">Jonathan Cohen</a>, 
<a href="/search/stat?searchtype=author&query=Lafferty%2C+J">John Lafferty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01150" title="Abstract">arXiv:2304.01150</a> (replaced) [<a href="/pdf/2304.01150" title="Download PDF">pdf</a>, <a href="/format/2304.01150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algebraic and Geometric Models for Space Networking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bernardoni%2C+W">William Bernardoni</a>, 
<a href="/search/math?searchtype=author&query=Cardona%2C+R">Robert Cardona</a>, 
<a href="/search/math?searchtype=author&query=Cleveland%2C+J">Jacob Cleveland</a>, 
<a href="/search/math?searchtype=author&query=Curry%2C+J">Justin Curry</a>, 
<a href="/search/math?searchtype=author&query=Green%2C+R">Robert Green</a>, 
<a href="/search/math?searchtype=author&query=Heller%2C+B">Brian Heller</a>, 
<a href="/search/math?searchtype=author&query=Hylton%2C+A">Alan Hylton</a>, 
<a href="/search/math?searchtype=author&query=Lam%2C+T">Tung Lam</a>, 
<a href="/search/math?searchtype=author&query=Kassouf-Short%2C+R">Robert Kassouf-Short</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Figures updated and improved based on more exhaustive simulations. Conjecture 2.27 now has weak and strong variations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI); Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01728" title="Abstract">arXiv:2304.01728</a> (replaced) [<a href="/pdf/2304.01728" title="Download PDF">pdf</a>, <a href="/format/2304.01728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable DPG Multigrid Solver for Helmholtz Problems: A Study on  Convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Badger%2C+J">Jacob Badger</a>, 
<a href="/search/math?searchtype=author&query=Henneking%2C+S">Stefan Henneking</a>, 
<a href="/search/math?searchtype=author&query=Petrides%2C+S">Socratis Petrides</a>, 
<a href="/search/math?searchtype=author&query=Demkowicz%2C+L">Leszek Demkowicz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03752" title="Abstract">arXiv:2304.03752</a> (replaced) [<a href="/pdf/2304.03752" title="Download PDF">pdf</a>, <a href="/format/2304.03752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> V3Det: Vast Vocabulary Visual Detection Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+T">Tao Chu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yuhang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yujie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Conghui He</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023 Oral Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04327" title="Abstract">arXiv:2304.04327</a> (replaced) [<a href="/pdf/2304.04327" title="Download PDF">pdf</a>, <a href="/format/2304.04327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Networks of Support in Distressed Environments: Solidarity and  Mobilization during the Russian Invasion of Ukraine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jinyi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Jindal%2C+N">Nikhil Jindal</a>, 
<a href="/search/cs?searchtype=author&query=Pierri%2C+F">Francesco Pierri</a>, 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at ICWSM2023 Workshop "Data for the Wellbeing of Most Vulnerable"
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the ICWSM Workshops 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05128" title="Abstract">arXiv:2304.05128</a> (replaced) [<a href="/pdf/2304.05128" title="Download PDF">pdf</a>, <a href="/format/2304.05128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teaching Large Language Models to Self-Debug
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Maxwell Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4rli%2C+N">Nathanael Sch&#xe4;rli</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06715" title="Abstract">arXiv:2304.06715</a> (replaced) [<a href="/pdf/2304.06715" title="Download PDF">pdf</a>, <a href="/format/2304.06715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Robustness of Interpretability Methods through  Explanation Invariance and Equivariance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crabb%C3%A9%2C+J">Jonathan Crabb&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08247" title="Abstract">arXiv:2304.08247</a> (replaced) [<a href="/pdf/2304.08247" title="Download PDF">pdf</a>, <a href="/ps/2304.08247" title="Download PostScript">ps</a>, <a href="/format/2304.08247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedAlpaca -- An Open-Source Collection of Medical Conversational AI  Models and Training Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tianyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+L+C">Lisa C. Adams</a>, 
<a href="/search/cs?searchtype=author&query=Papaioannou%2C+J">Jens-Michalis Papaioannou</a>, 
<a href="/search/cs?searchtype=author&query=Grundmann%2C+P">Paul Grundmann</a>, 
<a href="/search/cs?searchtype=author&query=Oberhauser%2C+T">Tom Oberhauser</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B6ser%2C+A">Alexander L&#xf6;ser</a>, 
<a href="/search/cs?searchtype=author&query=Truhn%2C+D">Daniel Truhn</a>, 
<a href="/search/cs?searchtype=author&query=Bressem%2C+K+K">Keno K. Bressem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08979" title="Abstract">arXiv:2304.08979</a> (replaced) [<a href="/pdf/2304.08979" title="Download PDF">pdf</a>, <a href="/format/2304.08979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In ChatGPT We Trust? Measuring and Characterizing the Reliability of  ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xinyue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zeyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11004" title="Abstract">arXiv:2304.11004</a> (replaced) [<a href="/pdf/2304.11004" title="Download PDF">pdf</a>, <a href="/format/2304.11004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Distillation Under Ideal Joint Classifier Assumption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huayu Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiwen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ditzler%2C+G">Gregory Ditzler</a>, 
<a href="/search/cs?searchtype=author&query=Roveda%2C+J">Janet Roveda</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13699" title="Abstract">arXiv:2304.13699</a> (replaced) [<a href="/pdf/2304.13699" title="Download PDF">pdf</a>, <a href="/ps/2304.13699" title="Download PostScript">ps</a>, <a href="/format/2304.13699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Covering simple orthogonal polygons with $r$-stars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mezei%2C+T+R">Tam&#xe1;s R&#xf3;bert Mezei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14420" title="Abstract">arXiv:2304.14420</a> (replaced) [<a href="/pdf/2304.14420" title="Download PDF">pdf</a>, <a href="/format/2304.14420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Cascade Vulnerability using Constrained Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lam%2C+A">Albert Lam</a>, 
<a href="/search/cs?searchtype=author&query=Anitescu%2C+M">Mihai Anitescu</a>, 
<a href="/search/cs?searchtype=author&query=Subramanyam%2C+A">Anirudh Subramanyam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14993" title="Abstract">arXiv:2304.14993</a> (replaced) [<a href="/pdf/2304.14993" title="Download PDF">pdf</a>, <a href="/ps/2304.14993" title="Download PostScript">ps</a>, <a href="/format/2304.14993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT in the Classroom: An Analysis of Its Strengths and Weaknesses  for Solving Undergraduate Computer Science Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joshi%2C+I">Ishika Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Budhiraja%2C+R">Ritvik Budhiraja</a>, 
<a href="/search/cs?searchtype=author&query=Dev%2C+H">Harshal Dev</a>, 
<a href="/search/cs?searchtype=author&query=Kadia%2C+J">Jahnvi Kadia</a>, 
<a href="/search/cs?searchtype=author&query=Ataullah%2C+M+O">M. Osama Ataullah</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+S">Sayan Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dhruv Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Akolekar%2C+H+D">Harshal D. Akolekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in SIGCSE TS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06410" title="Abstract">arXiv:2305.06410</a> (replaced) [<a href="/pdf/2305.06410" title="Download PDF">pdf</a>, <a href="/format/2305.06410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surface Simplification using Intrinsic Error Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H+D">Hsueh-Ti Derek Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gillespie%2C+M">Mark Gillespie</a>, 
<a href="/search/cs?searchtype=author&query=Chislett%2C+B">Benjamin Chislett</a>, 
<a href="/search/cs?searchtype=author&query=Sharp%2C+N">Nicholas Sharp</a>, 
<a href="/search/cs?searchtype=author&query=Jacobson%2C+A">Alec Jacobson</a>, 
<a href="/search/cs?searchtype=author&query=Crane%2C+K">Keenan Crane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM Transactions on Graphics, Vol.42, No. 4, August 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07962" title="Abstract">arXiv:2305.07962</a> (replaced) [<a href="/pdf/2305.07962" title="Download PDF">pdf</a>, <a href="/format/2305.07962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved List Decoding for Polar-Coded Probabilistic Shaping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Runge%2C+C">Constantin Runge</a>, 
<a href="/search/cs?searchtype=author&query=Wiegart%2C+T">Thomas Wiegart</a>, 
<a href="/search/cs?searchtype=author&query=Lentner%2C+D">Diego Lentner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures; as presented at ISTC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08262" title="Abstract">arXiv:2305.08262</a> (replaced) [<a href="/pdf/2305.08262" title="Download PDF">pdf</a>, <a href="/format/2305.08262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stochastic Compound Failure Model for Testing Resilience of Autonomous  Fixed-Wing Aircraft I: Formulation and Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cooper%2C+T">Thelonious Cooper</a>, 
<a href="/search/eess?searchtype=author&query=Ravela%2C+S">Sai Ravela</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12081" title="Abstract">arXiv:2305.12081</a> (replaced) [<a href="/pdf/2305.12081" title="Download PDF">pdf</a>, <a href="/format/2305.12081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation,  Enrichment, and Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chufan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Cao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12766" title="Abstract">arXiv:2305.12766</a> (replaced) [<a href="/pdf/2305.12766" title="Download PDF">pdf</a>, <a href="/format/2305.12766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining Emergent In-Context Learning as Kernel Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chi Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13673" title="Abstract">arXiv:2305.13673</a> (replaced) [<a href="/pdf/2305.13673" title="Download PDF">pdf</a>, <a href="/format/2305.13673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics of Language Models: Part 1, Context-Free Grammar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allen-Zhu%2C+Z">Zeyuan Allen-Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> V2 polishes writing and adds Appendix G
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13716" title="Abstract">arXiv:2305.13716</a> (replaced) [<a href="/pdf/2305.13716" title="Download PDF">pdf</a>, <a href="/format/2305.13716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuhao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangze Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Pengcheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by INTERSPEECH 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14979" title="Abstract">arXiv:2305.14979</a> (replaced) [<a href="/pdf/2305.14979" title="Download PDF">pdf</a>, <a href="/format/2305.14979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessment of the Reliablity of a Model&#x27;s Decision by Generalizing  Attribution to the Wavelet Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kasmi%2C+G">Gabriel Kasmi</a>, 
<a href="/search/cs?searchtype=author&query=Dubus%2C+L">Laurent Dubus</a>, 
<a href="/search/cs?searchtype=author&query=Drenan%2C+Y+S">Yves-Marie Saint Drenan</a>, 
<a href="/search/cs?searchtype=author&query=Blanc%2C+P">Philippe Blanc</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 10 figures, 2 tables. v1 of the manuscript rejected from NeurIPS 2023, mainly due to the lack of quantitative evidence of the relevance of the proposed methodology. In the v2, we propose steps to address this issue and also plan on expanding the insertion and deletion scores for our method
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15070" title="Abstract">arXiv:2305.15070</a> (replaced) [<a href="/pdf/2305.15070" title="Download PDF">pdf</a>, <a href="/format/2305.15070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Annotation Imputation to Individualize Predictions: Initial Studies on  Distribution Dynamics and Model Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lowmanstone%2C+L">London Lowmanstone</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+R">Ruyuan Wan</a>, 
<a href="/search/cs?searchtype=author&query=Owan%2C+R">Risako Owan</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaehyung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NLPerspectives - 2nd Workshop on Perspectivist Approaches to NLP, 39 pages, 13 figures, 13 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2nd Workshop on Perspectivist Approaches to NLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15086" title="Abstract">arXiv:2305.15086</a> (replaced) [<a href="/pdf/2305.15086" title="Download PDF">pdf</a>, <a href="/format/2305.15086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unpaired Image-to-Image Translation via Neural Schr&#xf6;dinger Bridge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Beomsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+G">Gihyun Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kwanyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15357" title="Abstract">arXiv:2305.15357</a> (replaced) [<a href="/pdf/2305.15357" title="Download PDF">pdf</a>, <a href="/format/2305.15357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image  Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ma%2C+Y">Yiyang Ma</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+H">Huan Yang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+W">Wenhan Yang</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+J">Jianlong Fu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jiaying Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15871" title="Abstract">arXiv:2305.15871</a> (replaced) [<a href="/pdf/2305.15871" title="Download PDF">pdf</a>, <a href="/format/2305.15871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Robust Statistics for Simulation-based Inference under Model  Misspecification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Huang%2C+D">Daolang Huang</a>, 
<a href="/search/stat?searchtype=author&query=Bharti%2C+A">Ayush Bharti</a>, 
<a href="/search/stat?searchtype=author&query=Souza%2C+A">Amauri Souza</a>, 
<a href="/search/stat?searchtype=author&query=Acerbi%2C+L">Luigi Acerbi</a>, 
<a href="/search/stat?searchtype=author&query=Kaski%2C+S">Samuel Kaski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 13 figures, Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16102" title="Abstract">arXiv:2305.16102</a> (replaced) [<a href="/pdf/2305.16102" title="Download PDF">pdf</a>, <a href="/format/2305.16102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Oversmoothing in Attention-Based Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xinyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ajorlou%2C+A">Amir Ajorlou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jadbabaie%2C+A">Ali Jadbabaie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight. New remarks added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17455" title="Abstract">arXiv:2305.17455</a> (replaced) [<a href="/pdf/2305.17455" title="Download PDF">pdf</a>, <a href="/format/2305.17455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrossGET: Cross-Guided Ensemble of Tokens for Accelerating  Vision-Language Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+D">Dachuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chaofan Tao</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+A">Anyi Rao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhendong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chun Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.20057" title="Abstract">arXiv:2305.20057</a> (replaced) [<a href="/pdf/2305.20057" title="Download PDF">pdf</a>, <a href="/format/2305.20057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three-Way Trade-Off in Multi-Objective Learning: Optimization,  Generalization and Conflict-Avoidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lisha Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fernando%2C+H">Heshan Fernando</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+Y">Yiming Ying</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianyi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.20062" title="Abstract">arXiv:2305.20062</a> (replaced) [<a href="/pdf/2305.20062" title="Download PDF">pdf</a>, <a href="/format/2305.20062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chatting Makes Perfect: Chat-based Image Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levy%2C+M">Matan Levy</a>, 
<a href="/search/cs?searchtype=author&query=Ben-Ari%2C+R">Rami Ben-Ari</a>, 
<a href="/search/cs?searchtype=author&query=Darshan%2C+N">Nir Darshan</a>, 
<a href="/search/cs?searchtype=author&query=Lischinski%2C+D">Dani Lischinski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready version for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00709" title="Abstract">arXiv:2306.00709</a> (replaced) [<a href="/pdf/2306.00709" title="Download PDF">pdf</a>, <a href="/format/2306.00709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Social Context of Eating with Multimodal Smartphone  Sensing: The Role of Country Diversity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kammoun%2C+N">Nathan Kammoun</a>, 
<a href="/search/cs?searchtype=author&query=Meegahapola%2C+L">Lakmal Meegahapola</a>, 
<a href="/search/cs?searchtype=author&query=Gatica-Perez%2C+D">Daniel Gatica-Perez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25th ACM International Conference on Multimodal Interaction (ICMI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00966" title="Abstract">arXiv:2306.00966</a> (replaced) [<a href="/pdf/2306.00966" title="Download PDF">pdf</a>, <a href="/format/2306.00966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hidden Language of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chefer%2C+H">Hila Chefer</a>, 
<a href="/search/cs?searchtype=author&query=Lang%2C+O">Oran Lang</a>, 
<a href="/search/cs?searchtype=author&query=Geva%2C+M">Mor Geva</a>, 
<a href="/search/cs?searchtype=author&query=Polosukhin%2C+V">Volodymyr Polosukhin</a>, 
<a href="/search/cs?searchtype=author&query=Shocher%2C+A">Assaf Shocher</a>, 
<a href="/search/cs?searchtype=author&query=Irani%2C+M">Michal Irani</a>, 
<a href="/search/cs?searchtype=author&query=Mosseri%2C+I">Inbar Mosseri</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+L">Lior Wolf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01297" title="Abstract">arXiv:2306.01297</a> (replaced) [<a href="/pdf/2306.01297" title="Download PDF">pdf</a>, <a href="/ps/2306.01297" title="Download PostScript">ps</a>, <a href="/format/2306.01297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear Boundary Conditions for Initial Boundary Value Problems with  Applications in Computational Fluid Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nordstr%C3%B6m%2C+J">Jan Nordstr&#xf6;m</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2301.04568">arXiv:2301.04568</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01879" title="Abstract">arXiv:2306.01879</a> (replaced) [<a href="/pdf/2306.01879" title="Download PDF">pdf</a>, <a href="/format/2306.01879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Role of Language Priors in Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhiqiu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyue Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+D">Deepak Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengchuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+D">Deva Ramanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://linzhiqiu.github.io/papers/visual_gpt_score/">this https URL</a> Code: <a href="https://github.com/linzhiqiu/visual_gpt_score/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03116" title="Abstract">arXiv:2306.03116</a> (replaced) [<a href="/pdf/2306.03116" title="Download PDF">pdf</a>, <a href="/format/2306.03116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transferring Annotator- and Instance-dependent Transition Matrix for  Learning from Crowds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shikun Li</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xiaobo Xia</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jiankang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Shiming Ge</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03364" title="Abstract">arXiv:2306.03364</a> (replaced) [<a href="/pdf/2306.03364" title="Download PDF">pdf</a>, <a href="/format/2306.03364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Representations on the Unit Sphere: Investigating Angular  Gaussian and von Mises-Fisher Distributions for Online Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michel%2C+N">Nicolas Michel</a>, 
<a href="/search/cs?searchtype=author&query=Chierchia%2C+G">Giovanni Chierchia</a>, 
<a href="/search/cs?searchtype=author&query=Negrel%2C+R">Romain Negrel</a>, 
<a href="/search/cs?searchtype=author&query=Bercher%2C+J">Jean-Fran&#xe7;ois Bercher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, under review, update title
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04018" title="Abstract">arXiv:2306.04018</a> (replaced) [<a href="/pdf/2306.04018" title="Download PDF">pdf</a>, <a href="/format/2306.04018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PyTrial: Machine Learning Software and Benchmark for Clinical Trial  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Theodorou%2C+B">Brandon Theodorou</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tianfan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Cao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07629" title="Abstract">arXiv:2306.07629</a> (replaced) [<a href="/pdf/2306.07629" title="Download PDF">pdf</a>, <a href="/format/2306.07629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SqueezeLLM: Dense-and-Sparse Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sehoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hooper%2C+C">Coleman Hooper</a>, 
<a href="/search/cs?searchtype=author&query=Gholami%2C+A">Amir Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiuyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Sheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08586" title="Abstract">arXiv:2306.08586</a> (replaced) [<a href="/pdf/2306.08586" title="Download PDF">pdf</a>, <a href="/format/2306.08586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedJETs: Efficient Just-In-Time Personalization with Federated Mixture  of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dun%2C+C">Chen Dun</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+M+H">Mirian Hipolito Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A+H">Ahmed Hassan Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+R">Robert Sim</a>, 
<a href="/search/cs?searchtype=author&query=Kyrillidis%2C+A">Anastasios Kyrillidis</a>, 
<a href="/search/cs?searchtype=author&query=Dimitriadis%2C+D">Dimitrios Dimitriadis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08827" title="Abstract">arXiv:2306.08827</a> (replaced) [<a href="/pdf/2306.08827" title="Download PDF">pdf</a>, <a href="/format/2306.08827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks  for Solving PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhongkai Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiachen Yao</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+C">Chang Su</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+F">Fanzhi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Z">Zeyu Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09376" title="Abstract">arXiv:2306.09376</a> (replaced) [<a href="/pdf/2306.09376" title="Download PDF">pdf</a>, <a href="/format/2306.09376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modularizing while Training: A New Paradigm for Modularizing DNN Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+B">Binhang Qi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hailong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ruobing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICSE'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10840" title="Abstract">arXiv:2306.10840</a> (replaced) [<a href="/pdf/2306.10840" title="Download PDF">pdf</a>, <a href="/format/2306.10840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RedMotion: Motion Prediction via Redundancy Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wagner%2C+R">Royden Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Tas%2C+O+S">Omer Sahin Tas</a>, 
<a href="/search/cs?searchtype=author&query=Klemp%2C+M">Marvin Klemp</a>, 
<a href="/search/cs?searchtype=author&query=Lopez%2C+C+F">Carlos Fernandez Lopez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report, 13 pages, 8 figures; v2: focus on transformer model
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10913" title="Abstract">arXiv:2306.10913</a> (replaced) [<a href="/pdf/2306.10913" title="Download PDF">pdf</a>, <a href="/format/2306.10913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semilinear fractional elliptic PDEs with gradient nonlinearities on open  balls: existence of solutions and probabilistic representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Penent%2C+G">Guillaume Penent</a>, 
<a href="/search/math?searchtype=author&query=Privault%2C+N">Nicolas Privault</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2110.09941">arXiv:2110.09941</a>, <a href="/abs/2106.12127">arXiv:2106.12127</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13512" title="Abstract">arXiv:2306.13512</a> (replaced) [<a href="/pdf/2306.13512" title="Download PDF">pdf</a>, <a href="/format/2306.13512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DISCO-10M: A Large-Scale Music Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanzend%C3%B6rfer%2C+L+A">Luca A. Lanzend&#xf6;rfer</a>, 
<a href="/search/cs?searchtype=author&query=Gr%C3%B6tschla%2C+F">Florian Gr&#xf6;tschla</a>, 
<a href="/search/cs?searchtype=author&query=Funke%2C+E">Emil Funke</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13933" title="Abstract">arXiv:2306.13933</a> (replaced) [<a href="/pdf/2306.13933" title="Download PDF">pdf</a>, <a href="/format/2306.13933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boost Video Frame Interpolation via Motion Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haoning Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by BMVC 2023 (Oral Presentation) Project Page: <a href="https://haoningwu3639.github.io/VFI_Adapter_Webpage/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14840" title="Abstract">arXiv:2306.14840</a> (replaced) [<a href="/pdf/2306.14840" title="Download PDF">pdf</a>, <a href="/format/2306.14840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Flyweight FLIM-based CNNs with Adaptive Decoding for Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=de+Melo+Joao%2C+L">Leonardo de Melo Joao</a>, 
<a href="/search/eess?searchtype=author&query=de+Melo+e+Sousa%2C+A">Azael de Melo e Sousa</a>, 
<a href="/search/eess?searchtype=author&query=Santos%2C+B+M+d">Bianca Martins dos Santos</a>, 
<a href="/search/eess?searchtype=author&query=Guimaraes%2C+S+J+F">Silvio Jamil Ferzoli Guimaraes</a>, 
<a href="/search/eess?searchtype=author&query=Gomes%2C+J+F">Jancarlo Ferreira Gomes</a>, 
<a href="/search/eess?searchtype=author&query=Kijak%2C+E">Ewa Kijak</a>, 
<a href="/search/eess?searchtype=author&query=Falcao%2C+A+X">Alexandre Xavier Falcao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15295" title="Abstract">arXiv:2306.15295</a> (replaced) [<a href="/pdf/2306.15295" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesis of Quantum Vector Databases Based on Grovers Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Pronin%2C+C+B">Cesar Borisovich Pronin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ostroukh%2C+A+V">Andrey Vladimirovich Ostroukh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 11 figures, Accepted as part of publication "Environmental, Social, and Corporate Governance Perspectives in Development of Quantum Vector Databases Based on Grover's Algorithm" in IEEE Conference "2023 Intelligent technologies and electronic devices in vehicle and road transport complex (TIRVED-2023)"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04333" title="Abstract">arXiv:2307.04333</a> (replaced) [<a href="/pdf/2307.04333" title="Download PDF">pdf</a>, <a href="/format/2307.04333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Adversarial Robustness via Score-Based Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Boya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Weijian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihua Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05435" title="Abstract">arXiv:2307.05435</a> (replaced) [<a href="/pdf/2307.05435" title="Download PDF">pdf</a>, <a href="/format/2307.05435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Versus-Others Attention: Scalable Multimodal Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golovanevsky%2C+M">Michal Golovanevsky</a>, 
<a href="/search/cs?searchtype=author&query=Schiller%2C+E">Eva Schiller</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+A">Akira Nair</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Ritambhara Singh</a>, 
<a href="/search/cs?searchtype=author&query=Eickhoff%2C+C">Carsten Eickhoff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06092" title="Abstract">arXiv:2307.06092</a> (replaced) [<a href="/pdf/2307.06092" title="Download PDF">pdf</a>, <a href="/ps/2307.06092" title="Download PostScript">ps</a>, <a href="/format/2307.06092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantitative CLTs in Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Favaro%2C+S">Stefano Favaro</a>, 
<a href="/search/cs?searchtype=author&query=Hanin%2C+B">Boris Hanin</a>, 
<a href="/search/cs?searchtype=author&query=Marinucci%2C+D">Domenico Marinucci</a>, 
<a href="/search/cs?searchtype=author&query=Nourdin%2C+I">Ivan Nourdin</a>, 
<a href="/search/cs?searchtype=author&query=Peccati%2C+G">Giovanni Peccati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Probability (math.PR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06125" title="Abstract">arXiv:2307.06125</a> (replaced) [<a href="/pdf/2307.06125" title="Download PDF">pdf</a>, <a href="/format/2307.06125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Hierarchical Interactive Multi-Object Search for Mobile  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmalstieg%2C+F">Fabian Schmalstieg</a>, 
<a href="/search/cs?searchtype=author&query=Honerkamp%2C+D">Daniel Honerkamp</a>, 
<a href="/search/cs?searchtype=author&query=Welschehold%2C+T">Tim Welschehold</a>, 
<a href="/search/cs?searchtype=author&query=Valada%2C+A">Abhinav Valada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06435" title="Abstract">arXiv:2307.06435</a> (replaced) [<a href="/pdf/2307.06435" title="Download PDF">pdf</a>, <a href="/format/2307.06435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Overview of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naveed%2C+H">Humza Naveed</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A+U">Asad Ullah Khan</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Saqib%2C+M">Muhammad Saqib</a>, 
<a href="/search/cs?searchtype=author&query=Anwar%2C+S">Saeed Anwar</a>, 
<a href="/search/cs?searchtype=author&query=Usman%2C+M">Muhammad Usman</a>, 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+N">Naveed Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Barnes%2C+N">Nick Barnes</a>, 
<a href="/search/cs?searchtype=author&query=Mian%2C+A">Ajmal Mian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in-progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07726" title="Abstract">arXiv:2307.07726</a> (replaced) [<a href="/pdf/2307.07726" title="Download PDF">pdf</a>, <a href="/ps/2307.07726" title="Download PostScript">ps</a>, <a href="/format/2307.07726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Optimal Neural Networks: the Role of Sample Splitting in  Hyperparameter Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gong%2C+S">Shijin Gong</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08493" title="Abstract">arXiv:2307.08493</a> (replaced) [<a href="/pdf/2307.08493" title="Download PDF">pdf</a>, <a href="/format/2307.08493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Occupancy Grid Mapping without Ray-Casting for High-resolution LiDAR  Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yixi Cai</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+F">Fanze Kong</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yunfan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fangcheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiarong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplementary material included. Accepted for publication in IEEE Transactions on Robotics (T-RO)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10784" title="Abstract">arXiv:2307.10784</a> (replaced) [<a href="/pdf/2307.10784" title="Download PDF">pdf</a>, <a href="/format/2307.10784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with  4D Imaging Radar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qiuchi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Weiyi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Q">Qing-Long Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bing Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Intelligent Vehicles
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10972" title="Abstract">arXiv:2307.10972</a> (replaced) [<a href="/pdf/2307.10972" title="Download PDF">pdf</a>, <a href="/format/2307.10972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptively Weighted Audits of Instant-Runoff Voting Elections: AWAIRE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ek%2C+A">Alexander Ek</a>, 
<a href="/search/stat?searchtype=author&query=Stark%2C+P+B">Philip B. Stark</a>, 
<a href="/search/stat?searchtype=author&query=Stuckey%2C+P+J">Peter J. Stuckey</a>, 
<a href="/search/stat?searchtype=author&query=Vukcevic%2C+D">Damjan Vukcevic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures. Presented at E-Vote-ID 2023. This version contains minor corrections to match the final published version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Electronic Voting, E-Vote-ID 2023, Lecture Notes in Computer
  Science 14230 (2023) 35-51
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11546" title="Abstract">arXiv:2307.11546</a> (replaced) [<a href="/pdf/2307.11546" title="Download PDF">pdf</a>, <a href="/format/2307.11546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards practical reinforcement learning for tokamak magnetic control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Tracey%2C+B+D">Brendan D. Tracey</a>, 
<a href="/search/physics?searchtype=author&query=Michi%2C+A">Andrea Michi</a>, 
<a href="/search/physics?searchtype=author&query=Chervonyi%2C+Y">Yuri Chervonyi</a>, 
<a href="/search/physics?searchtype=author&query=Davies%2C+I">Ian Davies</a>, 
<a href="/search/physics?searchtype=author&query=Paduraru%2C+C">Cosmin Paduraru</a>, 
<a href="/search/physics?searchtype=author&query=Lazic%2C+N">Nevena Lazic</a>, 
<a href="/search/physics?searchtype=author&query=Felici%2C+F">Federico Felici</a>, 
<a href="/search/physics?searchtype=author&query=Ewalds%2C+T">Timo Ewalds</a>, 
<a href="/search/physics?searchtype=author&query=Donner%2C+C">Craig Donner</a>, 
<a href="/search/physics?searchtype=author&query=Galperti%2C+C">Cristian Galperti</a>, 
<a href="/search/physics?searchtype=author&query=Buchli%2C+J">Jonas Buchli</a>, 
<a href="/search/physics?searchtype=author&query=Neunert%2C+M">Michael Neunert</a>, 
<a href="/search/physics?searchtype=author&query=Huber%2C+A">Andrea Huber</a>, 
<a href="/search/physics?searchtype=author&query=Evens%2C+J">Jonathan Evens</a>, 
<a href="/search/physics?searchtype=author&query=Kurylowicz%2C+P">Paula Kurylowicz</a>, 
<a href="/search/physics?searchtype=author&query=Mankowitz%2C+D+J">Daniel J. Mankowitz</a>, 
<a href="/search/physics?searchtype=author&query=Riedmiller%2C+M">Martin Riedmiller</a>, 
The <a href="/search/physics?searchtype=author&query=TCV+Team">TCV Team</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Plasma Physics (physics.plasm-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11932" title="Abstract">arXiv:2307.11932</a> (replaced) [<a href="/pdf/2307.11932" title="Download PDF">pdf</a>, <a href="/format/2307.11932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kasahara%2C+I">Isaac Kasahara</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Shubham Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Engin%2C+S">Selim Engin</a>, 
<a href="/search/cs?searchtype=author&query=Chavan-Dafle%2C+N">Nikhil Chavan-Dafle</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>, 
<a href="/search/cs?searchtype=author&query=Isler%2C+V">Volkan Isler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14322" title="Abstract">arXiv:2307.14322</a> (replaced) [<a href="/pdf/2307.14322" title="Download PDF">pdf</a>, <a href="/format/2307.14322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Inverse Demand Function with Explainable Dual Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Cao%2C+Z">Zhiyu Cao</a>, 
<a href="/search/q-fin?searchtype=author&query=Chen%2C+Z">Zihan Chen</a>, 
<a href="/search/q-fin?searchtype=author&query=Mishra%2C+P">Prerna Mishra</a>, 
<a href="/search/q-fin?searchtype=author&query=Amini%2C+H">Hamed Amini</a>, 
<a href="/search/q-fin?searchtype=author&query=Feinstein%2C+Z">Zachary Feinstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and selected for oral presentation at ICAIF 2023, NY, US
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Computational Engineering, Finance, and Science (cs.CE); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00143" title="Abstract">arXiv:2308.00143</a> (replaced) [<a href="/pdf/2308.00143" title="Download PDF">pdf</a>, <a href="/format/2308.00143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formally Explaining Neural Networks within Reactive Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bassan%2C+S">Shahaf Bassan</a>, 
<a href="/search/cs?searchtype=author&query=Amir%2C+G">Guy Amir</a>, 
<a href="/search/cs?searchtype=author&query=Corsi%2C+D">Davide Corsi</a>, 
<a href="/search/cs?searchtype=author&query=Refaeli%2C+I">Idan Refaeli</a>, 
<a href="/search/cs?searchtype=author&query=Katz%2C+G">Guy Katz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Proc. 23rd Int. Conf. on Formal Methods in Computer-Aided Design (FMCAD)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00436" title="Abstract">arXiv:2308.00436</a> (replaced) [<a href="/pdf/2308.00436" title="Download PDF">pdf</a>, <a href="/format/2308.00436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+N">Ning Miao</a>, 
<a href="/search/cs?searchtype=author&query=Teh%2C+Y+W">Yee Whye Teh</a>, 
<a href="/search/cs?searchtype=author&query=Rainforth%2C+T">Tom Rainforth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08303" title="Abstract">arXiv:2308.08303</a> (replaced) [<a href="/pdf/2308.08303" title="Download PDF">pdf</a>, <a href="/format/2308.08303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Next-Active Objects for Context-Aware Anticipation in  Egocentric Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thakur%2C+S">Sanket Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Beyan%2C+C">Cigdem Beyan</a>, 
<a href="/search/cs?searchtype=author&query=Morerio%2C+P">Pietro Morerio</a>, 
<a href="/search/cs?searchtype=author&query=Murino%2C+V">Vittorio Murino</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bue%2C+A">Alessio Del Bue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in WACV'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08903" title="Abstract">arXiv:2308.08903</a> (replaced) [<a href="/pdf/2308.08903" title="Download PDF">pdf</a>, <a href="/ps/2308.08903" title="Download PostScript">ps</a>, <a href="/format/2308.08903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Incentive Guarantees Behind Nash Welfare in Divisible Resources  Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bei%2C+X">Xiaohui Bei</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+B">Biaoshuai Tao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingwei Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10094" title="Abstract">arXiv:2308.10094</a> (replaced) [<a href="/pdf/2308.10094" title="Download PDF">pdf</a>, <a href="/format/2308.10094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning and Communications Co-Design for Remote Inference Systems:  Feature Length Selection and Transmission Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shisher%2C+M+K+C">Md Kamran Chowdhury Shisher</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+B">Bo Ji</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+I">I-Hong Hou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yin Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures. The manuscript has been accepted by IEEE Journal on Selected Areas in Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12439" title="Abstract">arXiv:2308.12439</a> (replaced) [<a href="/pdf/2308.12439" title="Download PDF">pdf</a>, <a href="/format/2308.12439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tinghao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiangyu Qi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Ping He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiming Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J+T">Jiachen T. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+P">Prateek Mittal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12960" title="Abstract">arXiv:2308.12960</a> (replaced) [<a href="/pdf/2308.12960" title="Download PDF">pdf</a>, <a href="/format/2308.12960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Realistic Zero-Shot Classification via Self Structural Semantic  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Naseer%2C+M">Muzammal Naseer</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqiang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F">Fahad Khan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16150" title="Abstract">arXiv:2308.16150</a> (replaced) [<a href="/pdf/2308.16150" title="Download PDF">pdf</a>, <a href="/format/2308.16150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modality Cycles with Masked Conditional Diffusion for Unsupervised  Anomaly Segmentation in MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liang%2C+Z">Ziyun Liang</a>, 
<a href="/search/eess?searchtype=author&query=Anthony%2C+H">Harry Anthony</a>, 
<a href="/search/eess?searchtype=author&query=Wagner%2C+F">Felix Wagner</a>, 
<a href="/search/eess?searchtype=author&query=Kamnitsas%2C+K">Konstantinos Kamnitsas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Multiscale Multimodal Medical Imaging workshop in MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16738" title="Abstract">arXiv:2308.16738</a> (replaced) [<a href="/pdf/2308.16738" title="Download PDF">pdf</a>, <a href="/format/2308.16738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SFUSNet: A Spatial-Frequency domain-based Multi-branch Network for  diagnosis of Cervical Lymph Node Lesions in Ultrasound Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yue%2C+Y">Yubiao Yue</a>, 
<a href="/search/eess?searchtype=author&query=Xue%2C+J">Jun Xue</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+H">Haihua Liang</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+B">Bingchun Luo</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhenzhang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00079" title="Abstract">arXiv:2309.00079</a> (replaced) [<a href="/pdf/2309.00079" title="Download PDF">pdf</a>, <a href="/format/2309.00079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Implicit Bias of Adam
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cattaneo%2C+M+D">Matias D. Cattaneo</a>, 
<a href="/search/cs?searchtype=author&query=Klusowski%2C+J+M">Jason M. Klusowski</a>, 
<a href="/search/cs?searchtype=author&query=Shigida%2C+B">Boris Shigida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00616" title="Abstract">arXiv:2309.00616</a> (replaced) [<a href="/pdf/2309.00616" title="Download PDF">pdf</a>, <a href="/format/2309.00616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhening Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hengshuang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lasenby%2C+J">Joan Lasenby</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 17 figures, 13 tables. Project page: <a href="https://zheninghuang.github.io/OpenIns3D/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01807" title="Abstract">arXiv:2309.01807</a> (replaced) [<a href="/pdf/2309.01807" title="Download PDF">pdf</a>, <a href="/format/2309.01807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Marginalized Importance Sampling for Off-Environment Policy Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katdare%2C+P">Pulkit Katdare</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Driggs-Campbell%2C+K">Katherine Driggs-Campbell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02092" title="Abstract">arXiv:2309.02092</a> (replaced) [<a href="/pdf/2309.02092" title="Download PDF">pdf</a>, <a href="/format/2309.02092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Emotion Role Labeling and Appraisal-based Emotion Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klinger%2C+R">Roman Klinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to the Big Picture Workshop (<a href="https://bigpictureworkshop.com/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02156" title="Abstract">arXiv:2309.02156</a> (replaced) [<a href="/pdf/2309.02156" title="Download PDF">pdf</a>, <a href="/ps/2309.02156" title="Download PostScript">ps</a>, <a href="/format/2309.02156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subspace Acceleration for a Sequence of Linear Systems and Application  to Plasma Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guido%2C+M">Margherita Guido</a>, 
<a href="/search/math?searchtype=author&query=Kressner%2C+D">Daniel Kressner</a>, 
<a href="/search/math?searchtype=author&query=Ricci%2C+P">Paolo Ricci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Plasma Physics (physics.plasm-ph)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05395" title="Abstract">arXiv:2309.05395</a> (replaced) [<a href="/pdf/2309.05395" title="Download PDF">pdf</a>, <a href="/format/2309.05395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Homomorphic Aggregation for Byzantine ML
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choffrut%2C+A">Antoine Choffrut</a>, 
<a href="/search/cs?searchtype=author&query=Guerraoui%2C+R">Rachid Guerraoui</a>, 
<a href="/search/cs?searchtype=author&query=Pinot%2C+R">Rafael Pinot</a>, 
<a href="/search/cs?searchtype=author&query=Sirdey%2C+R">Renaud Sirdey</a>, 
<a href="/search/cs?searchtype=author&query=Stephan%2C+J">John Stephan</a>, 
<a href="/search/cs?searchtype=author&query=Zuber%2C+M">Martin Zuber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06262" title="Abstract">arXiv:2309.06262</a> (replaced) [<a href="/pdf/2309.06262" title="Download PDF">pdf</a>, <a href="/format/2309.06262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modality Unifying Network for Visible-Infrared Person Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wei Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Guoying Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures. Accepted as the poster paper in ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06330" title="Abstract">arXiv:2309.06330</a> (replaced) [<a href="/pdf/2309.06330" title="Download PDF">pdf</a>, <a href="/format/2309.06330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Constraint-Coupled Optimization with Inexact Oracle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jingwang Li</a>, 
<a href="/search/math?searchtype=author&query=Su%2C+H">Housheng Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07056" title="Abstract">arXiv:2309.07056</a> (replaced) [<a href="/pdf/2309.07056" title="Download PDF">pdf</a>, <a href="/format/2309.07056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into  Quantum Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Jaouni%2C+T">Tareq Jaouni</a>, 
<a href="/search/quant-ph?searchtype=author&query=Arlt%2C+S">S&#xf6;ren Arlt</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ruiz-Gonzalez%2C+C">Carlos Ruiz-Gonzalez</a>, 
<a href="/search/quant-ph?searchtype=author&query=Karimi%2C+E">Ebrahim Karimi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gu%2C+X">Xuemei Gu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Krenn%2C+M">Mario Krenn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Modified Figure 2. Fixed minor typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07936" title="Abstract">arXiv:2309.07936</a> (replaced) [<a href="/pdf/2309.07936" title="Download PDF">pdf</a>, <a href="/format/2309.07936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate  Optimization Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Monteiro%2C+R">Rafael Monteiro</a>, 
<a href="/search/cs?searchtype=author&query=Sau%2C+K">Kartik Sau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Git-hub on <a href="https://github.com/rafael-a-monteiro-math/landscape_sketch_and_step/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08337" title="Abstract">arXiv:2309.08337</a> (replaced) [<a href="/pdf/2309.08337" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proceedings of the XII International Workshop on Locational Analysis and  Related Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baldomero-Naranjo%2C+M">Marta Baldomero-Naranjo</a>, 
<a href="/search/cs?searchtype=author&query=Blanco%2C+V">V&#xed;ctor Blanco</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa%2C+S">Sergio Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%A1zquez%2C+R">Ricardo G&#xe1;zquez</a>, 
<a href="/search/cs?searchtype=author&query=Kalcsics%2C+J">J&#xf6;rg Kalcsics</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Merino%2C+L+I">Luisa I. Mart&#xed;nez-Merino</a>, 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1oz-Oca%C3%B1a%2C+J+M">Juan M. Mu&#xf1;oz-Oca&#xf1;a</a>, 
<a href="/search/cs?searchtype=author&query=Temprano%2C+F">Francisco Temprano</a>, 
<a href="/search/cs?searchtype=author&query=Torrej%C3%B3n%2C+A">Alberto Torrej&#xf3;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The proceedings book of the previous editions can be found at <a href="/abs/2002.08287">arXiv:2002.08287</a> <a href="/abs/2002.08293">arXiv:2002.08293</a> <a href="/abs/2002.08300">arXiv:2002.08300</a> <a href="/abs/2002.01702">arXiv:2002.01702</a> <a href="/abs/2202.13878">arXiv:2202.13878</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08568" title="Abstract">arXiv:2309.08568</a> (replaced) [<a href="/pdf/2309.08568" title="Download PDF">pdf</a>, <a href="/format/2309.08568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion Probabilistic Models for Hardware-Impaired  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Letafati%2C+M">Mehdi Letafati</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Samad Ali</a>, 
<a href="/search/cs?searchtype=author&query=Latva-aho%2C+M">Matti Latva-aho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08621" title="Abstract">arXiv:2309.08621</a> (replaced) [<a href="/pdf/2309.08621" title="Download PDF">pdf</a>, <a href="/format/2309.08621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aird%2C+A">Amanda Aird</a>, 
<a href="/search/cs?searchtype=author&query=All%2C+C">Cassidy All</a>, 
<a href="/search/cs?searchtype=author&query=Farastu%2C+P">Paresha Farastu</a>, 
<a href="/search/cs?searchtype=author&query=Stefancova%2C+E">Elena Stefancova</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Joshua Sun</a>, 
<a href="/search/cs?searchtype=author&query=Mattei%2C+N">Nicholas Mattei</a>, 
<a href="/search/cs?searchtype=author&query=Burke%2C+R">Robin Burke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08916" title="Abstract">arXiv:2309.08916</a> (replaced) [<a href="/pdf/2309.08916" title="Download PDF">pdf</a>, <a href="/format/2309.08916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BGGAN: Generative AI Enables Representing Brain Structure-Function  Connections for Alzheimer&#x27;s Disease
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Chen Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuqiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Image and Video Processing (eess.IV); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09472" title="Abstract">arXiv:2309.09472</a> (replaced) [<a href="/pdf/2309.09472" title="Download PDF">pdf</a>, <a href="/format/2309.09472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing Existing Levels through Level Inpainting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+J">Johor Jara Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Guzdial%2C+M">Matthew Guzdial</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, Artificial Intelligence and Interactive Digital Entertainment
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09476" title="Abstract">arXiv:2309.09476</a> (replaced) [<a href="/pdf/2309.09476" title="Download PDF">pdf</a>, <a href="/format/2309.09476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated  Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+J">Johor Jara Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Cooper%2C+S">Seth Cooper</a>, 
<a href="/search/cs?searchtype=author&query=Guzdial%2C+M">Matthew Guzdial</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, Artificial Intelligence and Interactive Digital Entertainment
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09566" title="Abstract">arXiv:2309.09566</a> (replaced) [<a href="/pdf/2309.09566" title="Download PDF">pdf</a>, <a href="/ps/2309.09566" title="Download PostScript">ps</a>, <a href="/format/2309.09566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synchronous orders on the set of integers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choffrut%2C+C">Christian Choffrut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10097" title="Abstract">arXiv:2309.10097</a> (replaced) [<a href="/pdf/2309.10097" title="Download PDF">pdf</a>, <a href="/ps/2309.10097" title="Download PostScript">ps</a>, <a href="/format/2309.10097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Co-rotational Virtual Element Method for 2D Elasticity and Plasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yaw%2C+L+L">L. L. Yaw</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 26 Figures, corrected few typos, minor wording, equations (8) and (76) corrected to now include the area, A_E
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10514" title="Abstract">arXiv:2309.10514</a> (replaced) [<a href="/pdf/2309.10514" title="Download PDF">pdf</a>, <a href="/format/2309.10514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partially Specified Causal Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zamanian%2C+A">A. Zamanian</a>, 
<a href="/search/stat?searchtype=author&query=Mareis%2C+L">L. Mareis</a>, 
<a href="/search/stat?searchtype=author&query=Ahmidi%2C+N">N. Ahmidi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10719" title="Abstract">arXiv:2309.10719</a> (replaced) [<a href="/pdf/2309.10719" title="Download PDF">pdf</a>, <a href="/format/2309.10719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harmony and Duality: An introduction to Music Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lipyanskiy%2C+M">Maksim Lipyanskiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 70 pages, 72 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11686" title="Abstract">arXiv:2309.11686</a> (replaced) [<a href="/pdf/2309.11686" title="Download PDF">pdf</a>, <a href="/format/2309.11686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SE-PEF: a Resource for Personalized Expert Finding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kasela%2C+P">Pranav Kasela</a>, 
<a href="/search/cs?searchtype=author&query=Pasi%2C+G">Gabriella Pasi</a>, 
<a href="/search/cs?searchtype=author&query=Perego%2C+R">Raffaele Perego</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGIR-AP '23 Conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11745" title="Abstract">arXiv:2309.11745</a> (replaced) [<a href="/pdf/2309.11745" title="Download PDF">pdf</a>, <a href="/format/2309.11745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PIE: Simulating Disease Progression via Progressive Image Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liang%2C+K">Kaizhao Liang</a>, 
<a href="/search/eess?searchtype=author&query=Cao%2C+X">Xu Cao</a>, 
<a href="/search/eess?searchtype=author&query=Liao%2C+K">Kuei-Da Liao</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+T">Tianren Gao</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+W">Wenqian Ye</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/eess?searchtype=author&query=Cao%2C+J">Jianguo Cao</a>, 
<a href="/search/eess?searchtype=author&query=Nama%2C+T">Tejas Nama</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and checkpoints for replicating our results can be found at <a href="https://github.com/IrohXu/PIE">this https URL</a> and <a href="https://huggingface.co/IrohXu/stable-diffusion-mimic-cxr-v0.1">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11880" title="Abstract">arXiv:2309.11880</a> (replaced) [<a href="/pdf/2309.11880" title="Download PDF">pdf</a>, <a href="/format/2309.11880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Four universal growth regimes in degree-dependent first passage  percolation on spatial random graphs II
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Komj%C3%A1thy%2C+J">J&#xfa;lia Komj&#xe1;thy</a>, 
<a href="/search/math?searchtype=author&query=Lapinskas%2C+J">John Lapinskas</a>, 
<a href="/search/math?searchtype=author&query=Lengler%2C+J">Johannes Lengler</a>, 
<a href="/search/math?searchtype=author&query=Schaller%2C+U">Ulysse Schaller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Social and Information Networks (cs.SI); Combinatorics (math.CO); Populations and Evolution (q-bio.PE)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11981" title="Abstract">arXiv:2309.11981</a> (replaced) [<a href="/pdf/2309.11981" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking the Evaluating Framework for Natural Language Understanding  in AI Systems: Language Acquisition as a Core for Future Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vera%2C+P">Patricio Vera</a>, 
<a href="/search/cs?searchtype=author&query=Moya%2C+P">Pedro Moya</a>, 
<a href="/search/cs?searchtype=author&query=Barraza%2C+L">Lisa Barraza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 table, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12340" title="Abstract">arXiv:2309.12340</a> (replaced) [<a href="/e-print/2309.12340" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security for Children in the Digital Society -- A Rights-based and  Research Ethics Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schelenz%2C+L">Laura Schelenz</a>, 
<a href="/search/cs?searchtype=author&query=Stapf%2C+I">Ingrid Stapf</a>, 
<a href="/search/cs?searchtype=author&query=Heesen%2C+J">Jessica Heesen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version included false figures and technical difficulties made it difficult to replace the current version with another one that does not include the false figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12488" title="Abstract">arXiv:2309.12488</a> (replaced) [<a href="/pdf/2309.12488" title="Download PDF">pdf</a>, <a href="/format/2309.12488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharpness-Aware Minimization and the Edge of Stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+P+M">Philip M. Long</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+P+L">Peter L. Bartlett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12766" title="Abstract">arXiv:2309.12766</a> (replaced) [<a href="/pdf/2309.12766" title="Download PDF">pdf</a>, <a href="/format/2309.12766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study on Incorporating Whisper for Robust Speech Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zezario%2C+R+E">Ryandhimas E. Zezario</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yu-Wen Chen</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+S">Szu-Wei Fu</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hsin-Min Wang</a>, 
<a href="/search/eess?searchtype=author&query=Fuh%2C+C">Chiou-Shann Fuh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12871" title="Abstract">arXiv:2309.12871</a> (replaced) [<a href="/pdf/2309.12871" title="Download PDF">pdf</a>, <a href="/format/2309.12871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnglE-optimized Text Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianming Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NLP, Text Embedding, Semantic Textual Similarity
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13573" title="Abstract">arXiv:2309.13573</a> (replaced) [<a href="/pdf/2309.13573" title="Download PDF">pdf</a>, <a href="/format/2309.13573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The second multi-channel multi-party meeting transcription challenge  (M2MeT) 2.0): A benchmark for speaker-attributed ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuhao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Mohan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangze Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhihao Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yanmin Qian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K+A">Kong Aik Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhijie Yan</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+H">Hui Bu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, Accepted by ASRU2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13777" title="Abstract">arXiv:2309.13777</a> (replaced) [<a href="/pdf/2309.13777" title="Download PDF">pdf</a>, <a href="/format/2309.13777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffeomorphic Multi-Resolution Deep Learning Registration for  Applications in Breast MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=French%2C+M+G">Matthew G. French</a>, 
<a href="/search/eess?searchtype=author&query=Talou%2C+G+D+M">Gonzalo D. Maso Talou</a>, 
<a href="/search/eess?searchtype=author&query=Gamage%2C+T+P+B">Thiranja P. Babarenda Gamage</a>, 
<a href="/search/eess?searchtype=author&query=Nash%2C+M+P">Martyn P. Nash</a>, 
<a href="/search/eess?searchtype=author&query=Nielsen%2C+P+M">Poul M. Nielsen</a>, 
<a href="/search/eess?searchtype=author&query=Doyle%2C+A+J">Anthony J. Doyle</a>, 
<a href="/search/eess?searchtype=author&query=Iglesias%2C+J+E">Juan Eugenio Iglesias</a>, 
<a href="/search/eess?searchtype=author&query=Balbastre%2C+Y">Ya&#xeb;l Balbastre</a>, 
<a href="/search/eess?searchtype=author&query=Young%2C+S+I">Sean I. Young</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14073" title="Abstract">arXiv:2309.14073</a> (replaced) [<a href="/pdf/2309.14073" title="Download PDF">pdf</a>, <a href="/ps/2309.14073" title="Download PostScript">ps</a>, <a href="/format/2309.14073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum Likelihood Estimation of Latent Variable Structural Equation  Models: A Neural Network Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Saremi%2C+M">Mehrzad Saremi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14331" title="Abstract">arXiv:2309.14331</a> (replaced) [<a href="/pdf/2309.14331" title="Download PDF">pdf</a>, <a href="/format/2309.14331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LinGCN: Structural Linearized Graph Convolutional Network for  Homomorphically Encrypted Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hongwu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+R">Ran Ran</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yukui Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiahui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaoyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Thorat%2C+K">Kiran Thorat</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+T">Tong Geng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenghong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaolin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Wujie Wen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Caiwen Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 accepted publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15294" title="Abstract">arXiv:2309.15294</a> (replaced) [<a href="/pdf/2309.15294" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple Case Physics-Informed Neural Network for Biomedical Tube Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wong%2C+H+S">Hong Shen Wong</a>, 
<a href="/search/physics?searchtype=author&query=Chan%2C+W+X">Wei Xuan Chan</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+B+H">Bing Huan Li</a>, 
<a href="/search/physics?searchtype=author&query=Yap%2C+C+H">Choon Hwai Yap</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 8 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15325" title="Abstract">arXiv:2309.15325</a> (replaced) [<a href="/pdf/2309.15325" title="Download PDF">pdf</a>, <a href="/format/2309.15325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Operators for Accelerating Scientific Simulations and Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azzizadenesheli%2C+K">Kamyar Azzizadenesheli</a>, 
<a href="/search/cs?searchtype=author&query=Kovachki%2C+N">Nikola Kovachki</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu-Schiaffini%2C+M">Miguel Liu-Schiaffini</a>, 
<a href="/search/cs?searchtype=author&query=Kossaifi%2C+J">Jean Kossaifi</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15523" title="Abstract">arXiv:2309.15523</a> (replaced) [<a href="/pdf/2309.15523" title="Download PDF">pdf</a>, <a href="/format/2309.15523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Facade Parsing with Vision Transformers and Line Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunqin Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Nakashima%2C+Y">Yuta Nakashima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15630" title="Abstract">arXiv:2309.15630</a> (replaced) [<a href="/pdf/2309.15630" title="Download PDF">pdf</a>, <a href="/format/2309.15630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLPBench: Evaluating Large Language Models on Solving NLP Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linxin Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lechao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pengyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+I">Irene Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16163" title="Abstract">arXiv:2309.16163</a> (replaced) [<a href="/pdf/2309.16163" title="Download PDF">pdf</a>, <a href="/format/2309.16163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Doppler Time-of-Flight Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juhyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jarosz%2C+W">Wojciech Jarosz</a>, 
<a href="/search/cs?searchtype=author&query=Gkioulekas%2C+I">Ioannis Gkioulekas</a>, 
<a href="/search/cs?searchtype=author&query=Pediredla%2C+A">Adithya Pediredla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 28 Figures, SIGGRAPH Asia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16206" title="Abstract">arXiv:2309.16206</a> (replaced) [<a href="/pdf/2309.16206" title="Download PDF">pdf</a>, <a href="/format/2309.16206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alzheimer&#x27;s Disease Prediction via Brain Structural-Functional Deep  Fusing Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zuo%2C+Q">Qiankun Zuo</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+J">Junren Pan</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shuqiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17147" title="Abstract">arXiv:2309.17147</a> (replaced) [<a href="/pdf/2309.17147" title="Download PDF">pdf</a>, <a href="/format/2309.17147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Large Language Models for Qualitative Analysis can Introduce  Serious Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ashwin%2C+J">Julian Ashwin</a>, 
<a href="/search/cs?searchtype=author&query=Chhabra%2C+A">Aditya Chhabra</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+V">Vijayendra Rao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17167" title="Abstract">arXiv:2309.17167</a> (replaced) [<a href="/pdf/2309.17167" title="Download PDF">pdf</a>, <a href="/format/2309.17167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DyVal: Graph-informed Dynamic Evaluation of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kaijie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report; 36 pages; code will be released at aka.ms/dyval
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17260" title="Abstract">arXiv:2309.17260</a> (replaced) [<a href="/pdf/2309.17260" title="Download PDF">pdf</a>, <a href="/format/2309.17260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PlaceNav: Topological Navigation through Place Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suomela%2C+L">Lauri Suomela</a>, 
<a href="/search/cs?searchtype=author&query=Kalliola%2C+J">Jussi Kalliola</a>, 
<a href="/search/cs?searchtype=author&query=Edelman%2C+H">Harry Edelman</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4m%C3%A4r%C3%A4inen%2C+J">Joni-Kristian K&#xe4;m&#xe4;r&#xe4;inen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17329" title="Abstract">arXiv:2309.17329</a> (replaced) [<a href="/pdf/2309.17329" title="Download PDF">pdf</a>, <a href="/format/2309.17329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Anatomical Labeling of Pulmonary Tree Structures via Implicit  Point-Graph Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+K">Kangxian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiancheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Donglai Wei</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+Z">Ziqiao Weng</a>, 
<a href="/search/cs?searchtype=author&query=Fua%2C+P">Pascal Fua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17348" title="Abstract">arXiv:2309.17348</a> (replaced) [<a href="/pdf/2309.17348" title="Download PDF">pdf</a>, <a href="/format/2309.17348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Biologically Plausible Adversarial Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farinha%2C+M+T">Matilde Tristany Farinha</a>, 
<a href="/search/cs?searchtype=author&query=Ortner%2C+T">Thomas Ortner</a>, 
<a href="/search/cs?searchtype=author&query=Dellaferrera%2C+G">Giorgia Dellaferrera</a>, 
<a href="/search/cs?searchtype=author&query=Grewe%2C+B">Benjamin Grewe</a>, 
<a href="/search/cs?searchtype=author&query=Pantazi%2C+A">Angeliki Pantazi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17357" title="Abstract">arXiv:2309.17357</a> (replaced) [<a href="/pdf/2309.17357" title="Download PDF">pdf</a>, <a href="/format/2309.17357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Module-wise Training of Neural Networks via the Minimizing Movement  Scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karkar%2C+S">Skander Karkar</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+I">Ibrahim Ayed</a>, 
<a href="/search/cs?searchtype=author&query=de+B%C3%A9zenac%2C+E">Emmanuel de B&#xe9;zenac</a>, 
<a href="/search/cs?searchtype=author&query=Gallinari%2C+P">Patrick Gallinari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. arXiv admin note: text overlap with <a href="/abs/2210.00949">arXiv:2210.00949</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00015" title="Abstract">arXiv:2310.00015</a> (replaced) [<a href="/pdf/2310.00015" title="Download PDF">pdf</a>, <a href="/format/2310.00015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Communication with Probability Graph: A Joint Communication and  Computation Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhouxiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+Q">Quoc-Viet Pham</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qianqian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00031" title="Abstract">arXiv:2310.00031</a> (replaced) [<a href="/pdf/2310.00031" title="Download PDF">pdf</a>, <a href="/format/2310.00031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-image Alignment for Diffusion-based Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kondapaneni%2C+N">Neehar Kondapaneni</a>, 
<a href="/search/cs?searchtype=author&query=Marks%2C+M">Markus Marks</a>, 
<a href="/search/cs?searchtype=author&query=Knott%2C+M">Manuel Knott</a>, 
<a href="/search/cs?searchtype=author&query=Guimar%C3%A3es%2C+R">Rog&#xe9;rio Guimar&#xe3;es</a>, 
<a href="/search/cs?searchtype=author&query=Perona%2C+P">Pietro Perona</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://www.vision.caltech.edu/tadp/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00032" title="Abstract">arXiv:2310.00032</a> (replaced) [<a href="/pdf/2310.00032" title="Download PDF">pdf</a>, <a href="/format/2310.00032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretrain, Prompt, and Transfer: Evolving Digital Twins for Time-to-Event  Analysis in Cyber-physical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qinghua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+T">Tao Yue</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Shaukat Ali</a>, 
<a href="/search/cs?searchtype=author&query=Arratibel%2C+M">Maite Arratibel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00035" title="Abstract">arXiv:2310.00035</a> (replaced) [<a href="/pdf/2310.00035" title="Download PDF">pdf</a>, <a href="/format/2310.00035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRA ensembles for large language model fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Aitchison%2C+L">Laurence Aitchison</a>, 
<a href="/search/cs?searchtype=author&query=Rudolph%2C+M">Maja Rudolph</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update the title in the PDF file
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00247" title="Abstract">arXiv:2310.00247</a> (replaced) [<a href="/pdf/2310.00247" title="Download PDF">pdf</a>, <a href="/format/2310.00247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Gap Between Foundation Models and Heterogeneous Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sixing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1oz%2C+J+P">J. Pablo Mu&#xf1;oz</a>, 
<a href="/search/cs?searchtype=author&query=Jannesari%2C+A">Ali Jannesari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00334" title="Abstract">arXiv:2310.00334</a> (replaced) [<a href="/pdf/2310.00334" title="Download PDF">pdf</a>, <a href="/ps/2310.00334" title="Download PostScript">ps</a>, <a href="/format/2310.00334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounded Simultaneous Messages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogdanov%2C+A">Andrej Bogdanov</a>, 
<a href="/search/cs?searchtype=author&query=Dinesh%2C+K">Krishnamoorthy Dinesh</a>, 
<a href="/search/cs?searchtype=author&query=Filmus%2C+Y">Yuval Filmus</a>, 
<a href="/search/cs?searchtype=author&query=Ishai%2C+Y">Yuval Ishai</a>, 
<a href="/search/cs?searchtype=author&query=Kaplan%2C+A">Avi Kaplan</a>, 
<a href="/search/cs?searchtype=author&query=Sekar%2C+S">Sruthi Sekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 1 table, accepted to FSTTCS 2023 (improved exposition)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00704" title="Abstract">arXiv:2310.00704</a> (replaced) [<a href="/pdf/2310.00704" title="Download PDF">pdf</a>, <a href="/format/2310.00704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniAudio: An Audio Foundation Model Toward Universal Audio Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dongchao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jinchuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Rongjie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songxiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xuankai Chang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiatong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xixin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhou Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H">Helen Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00818" title="Abstract">arXiv:2310.00818</a> (replaced) [<a href="/pdf/2310.00818" title="Download PDF">pdf</a>, <a href="/format/2310.00818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method  for ECG signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huiyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sano%2C+A">Akane Sano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00901" title="Abstract">arXiv:2310.00901</a> (replaced) [<a href="/pdf/2310.00901" title="Download PDF">pdf</a>, <a href="/format/2310.00901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TADIS: Steering Models for Deep-Thinking about Demonstration Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+T">Tianci Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yixia Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanhua Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00944" title="Abstract">arXiv:2310.00944</a> (replaced) [<a href="/pdf/2310.00944" title="Download PDF">pdf</a>, <a href="/format/2310.00944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust 3D Object Detection In Rainy Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piroli%2C+A">Aldi Piroli</a>, 
<a href="/search/cs?searchtype=author&query=Dallabetta%2C+V">Vinzenz Dallabetta</a>, 
<a href="/search/cs?searchtype=author&query=Kopp%2C+J">Johannes Kopp</a>, 
<a href="/search/cs?searchtype=author&query=Walessa%2C+M">Marc Walessa</a>, 
<a href="/search/cs?searchtype=author&query=Meissner%2C+D">Daniel Meissner</a>, 
<a href="/search/cs?searchtype=author&query=Dietmayer%2C+K">Klaus Dietmayer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at IEEE International Conference on Intelligent Transportation Systems ITSC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01254" title="Abstract">arXiv:2310.01254</a> (replaced) [<a href="/pdf/2310.01254" title="Download PDF">pdf</a>, <a href="/format/2310.01254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Containment for Binary Guarded Monotone SNP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barsukov%2C+A">Alexey Barsukov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Databases (cs.DB); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01278" title="Abstract">arXiv:2310.01278</a> (replaced) [<a href="/pdf/2310.01278" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open and Linked Data Model for Carbon Footprint Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruf%2C+B">Boris Ruf</a>, 
<a href="/search/cs?searchtype=author&query=Detyniecki%2C+M">Marcin Detyniecki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at International Conference on Renewable Energy and Conservation (ICREC) 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01425" title="Abstract">arXiv:2310.01425</a> (replaced) [<a href="/pdf/2310.01425" title="Download PDF">pdf</a>, <a href="/ps/2310.01425" title="Download PostScript">ps</a>, <a href="/format/2310.01425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Borges and AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bottou%2C+L">L&#xe9;on Bottou</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01662" title="Abstract">arXiv:2310.01662</a> (replaced) [<a href="/pdf/2310.01662" title="Download PDF">pdf</a>, <a href="/format/2310.01662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SYRAC: Synthesize, Rank, and Count
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Alessandro%2C+A">Adriano D&#x27;Alessandro</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavi-Amiri%2C+A">Ali Mahdavi-Amiri</a>, 
<a href="/search/cs?searchtype=author&query=Hamarneh%2C+G">Ghassan Hamarneh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01690" title="Abstract">arXiv:2310.01690</a> (replaced) [<a href="/pdf/2310.01690" title="Download PDF">pdf</a>, <a href="/format/2310.01690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecasting Tropical Cyclones with Cascaded Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Nath%2C+P">Pritthijit Nath</a>, 
<a href="/search/physics?searchtype=author&query=Shukla%2C+P">Pancham Shukla</a>, 
<a href="/search/physics?searchtype=author&query=Quilodr%C3%A1n-Casas%2C+C">C&#xe9;sar Quilodr&#xe1;n-Casas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, Submitted to Tackling Climate Change with Machine Learning workshop at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01791" title="Abstract">arXiv:2310.01791</a> (replaced) [<a href="/pdf/2310.01791" title="Download PDF">pdf</a>, <a href="/format/2310.01791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online POMDP Planning with Anytime Deterministic Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barenboim%2C+M">Moran Barenboim</a>, 
<a href="/search/cs?searchtype=author&query=Indelman%2C+V">Vadim Indelman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01889" title="Abstract">arXiv:2310.01889</a> (replaced) [<a href="/pdf/2310.01889" title="Download PDF">pdf</a>, <a href="/format/2310.01889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ring Attention with Blockwise Transformers for Near-Infinite Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zaharia%2C+M">Matei Zaharia</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02027" title="Abstract">arXiv:2310.02027</a> (replaced) [<a href="/pdf/2310.02027" title="Download PDF">pdf</a>, <a href="/format/2310.02027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaxu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+X">Xinping Yi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaowei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages including appendix and reference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02156" title="Abstract">arXiv:2310.02156</a> (replaced) [<a href="/pdf/2310.02156" title="Download PDF">pdf</a>, <a href="/format/2310.02156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistically Rewired Message-Passing Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chendi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Manolache%2C+A">Andrei Manolache</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+K">Kareem Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhe Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Broeck%2C+G">Guy Van den Broeck</a>, 
<a href="/search/cs?searchtype=author&query=Niepert%2C+M">Mathias Niepert</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+C">Christopher Morris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02244" title="Abstract">arXiv:2310.02244</a> (replaced) [<a href="/pdf/2310.02244" title="Download PDF">pdf</a>, <a href="/format/2310.02244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Greg Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dingli Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hayou%2C+S">Soufiane Hayou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02298" title="Abstract">arXiv:2310.02298</a> (replaced) [<a href="/pdf/2310.02298" title="Download PDF">pdf</a>, <a href="/format/2310.02298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Audios Using Acoustic Properties For Emotion Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhamyal%2C+H">Hira Dhamyal</a>, 
<a href="/search/cs?searchtype=author&query=Elizalde%2C+B">Benjamin Elizalde</a>, 
<a href="/search/cs?searchtype=author&query=Deshmukh%2C+S">Soham Deshmukh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huaming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2211.07737">arXiv:2211.07737</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02357" title="Abstract">arXiv:2310.02357</a> (replaced) [<a href="/pdf/2310.02357" title="Download PDF">pdf</a>, <a href="/ps/2310.02357" title="Download PostScript">ps</a>, <a href="/format/2310.02357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the definition of toxicity in NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berezin%2C+S">Sergey Berezin</a>, 
<a href="/search/cs?searchtype=author&query=Farahbakhsh%2C+R">Reza Farahbakhsh</a>, 
<a href="/search/cs?searchtype=author&query=Crespi%2C+N">Noel Crespi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02591" title="Abstract">arXiv:2310.02591</a> (replaced) [<a href="/pdf/2310.02591" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Inception Architecture with Residual Connection: Fine-tuned  Inception-ResNet Deep Learning Model for Lung Inflammation Diagnosis from  Chest Radiographs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neshat%2C+M">Mehdi Neshat</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+M">Muktar Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Askari%2C+H">Hossein Askari</a>, 
<a href="/search/cs?searchtype=author&query=Thilakaratne%2C+M">Menasha Thilakaratne</a>, 
<a href="/search/cs?searchtype=author&query=Mirjalili%2C+S">Seyedali Mirjalili</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at International Conference on Machine Learning and Data Engineering (ICMLDE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02601" title="Abstract">arXiv:2310.02601</a> (replaced) [<a href="/pdf/2310.02601" title="Download PDF">pdf</a>, <a href="/format/2310.02601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MagicDrive: Street View Generation with Diverse 3D Geometry Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruiyuan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://flymin.github.io/magicdrive">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02676" title="Abstract">arXiv:2310.02676</a> (replaced) [<a href="/pdf/2310.02676" title="Download PDF">pdf</a>, <a href="/format/2310.02676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PostRainBench: A comprehensive benchmark and a new model for  precipitation forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yujin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiaming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zeying Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Junwei Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02754" title="Abstract">arXiv:2310.02754</a> (replaced) [<a href="/pdf/2310.02754" title="Download PDF">pdf</a>, <a href="/format/2310.02754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LC-Score: Reference-less estimation of Text Comprehension Difficulty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tardy%2C+P">Paul Tardy</a>, 
<a href="/search/cs?searchtype=author&query=Roze%2C+C">Charlotte Roze</a>, 
<a href="/search/cs?searchtype=author&query=Poupet%2C+P">Paul Poupet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02766" title="Abstract">arXiv:2310.02766</a> (replaced) [<a href="/pdf/2310.02766" title="Download PDF">pdf</a>, <a href="/format/2310.02766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Likelihood-Based Methods Improve Parameter Estimation in Opinion  Dynamics Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lenti%2C+J">Jacopo Lenti</a>, 
<a href="/search/cs?searchtype=author&query=Monti%2C+C">Corrado Monti</a>, 
<a href="/search/cs?searchtype=author&query=De+Francisci+Morales%2C+G">Gianmarco De Francisci Morales</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02800" title="Abstract">arXiv:2310.02800</a> (replaced) [<a href="/pdf/2310.02800" title="Download PDF">pdf</a>, <a href="/format/2310.02800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Everest: GPU-Accelerated System For Mining Temporal Motifs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yichao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haojie Ye</a>, 
<a href="/search/cs?searchtype=author&query=Vedula%2C+S">Sanketh Vedula</a>, 
<a href="/search/cs?searchtype=author&query=Kaza%2C+W">Wynn Kaza</a>, 
<a href="/search/cs?searchtype=author&query=Talati%2C+N">Nishil Talati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02817" title="Abstract">arXiv:2310.02817</a> (replaced) [<a href="/pdf/2310.02817" title="Download PDF">pdf</a>, <a href="/format/2310.02817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicit Runge Kutta Methods that Alleviate Order Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Biswas%2C+A">Abhijit Biswas</a>, 
<a href="/search/math?searchtype=author&query=Ketcheson%2C+D+I">David I. Ketcheson</a>, 
<a href="/search/math?searchtype=author&query=Roberts%2C+S">Steven Roberts</a>, 
<a href="/search/math?searchtype=author&query=Seibold%2C+B">Benjamin Seibold</a>, 
<a href="/search/math?searchtype=author&query=Shirokoff%2C+D">David Shirokoff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02859" title="Abstract">arXiv:2310.02859</a> (replaced) [<a href="/pdf/2310.02859" title="Download PDF">pdf</a>, <a href="/format/2310.02859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Sampling in Unbounded Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaglan%2C+K">Kshitijaa Jaglan</a>, 
<a href="/search/cs?searchtype=author&query=Chaitanya%2C+M">Meher Chaitanya</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+T">Triansh Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Singam%2C+A">Abhijeeth Singam</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+N">Nidhi Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>, 
<a href="/search/cs?searchtype=author&query=Brandes%2C+U">Ulrik Brandes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02861" title="Abstract">arXiv:2310.02861</a> (replaced) [<a href="/pdf/2310.02861" title="Download PDF">pdf</a>, <a href="/ps/2310.02861" title="Download PostScript">ps</a>, <a href="/format/2310.02861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiangyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sibo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02898" title="Abstract">arXiv:2310.02898</a> (replaced) [<a href="/pdf/2310.02898" title="Download PDF">pdf</a>, <a href="/format/2310.02898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some bidding games converging to their unique pure equilibrium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heymann%2C+B">Benjamin Heymann</a>, 
<a href="/search/cs?searchtype=author&query=Jofr%C3%A9%2C+A">Alejandro Jofr&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02911" title="Abstract">arXiv:2310.02911</a> (replaced) [<a href="/pdf/2310.02911" title="Download PDF">pdf</a>, <a href="/format/2310.02911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciphering the Crypto-shopper: Knowledge and Preferences of Consumers  Using Cryptocurrencies for Purchases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silenzi%2C+M">Massimiliano Silenzi</a>, 
<a href="/search/cs?searchtype=author&query=Cabuk%2C+U+C">Umut Can Cabuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Cryptorefills Labs Whitepaper. 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computational Engineering, Finance, and Science (cs.CE); Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02954" title="Abstract">arXiv:2310.02954</a> (replaced) [<a href="/pdf/2310.02954" title="Download PDF">pdf</a>, <a href="/format/2310.02954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for  In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jiong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yichun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhicheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingxing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiongwei Han</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02964" title="Abstract">arXiv:2310.02964</a> (replaced) [<a href="/pdf/2310.02964" title="Download PDF">pdf</a>, <a href="/format/2310.02964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-modeling the Sequential and Graphical Routes for Peptide  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Ge Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiangbin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02995" title="Abstract">arXiv:2310.02995</a> (replaced) [<a href="/e-print/2310.02995" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IBCL: Zero-shot Model Generation for Task Trade-offs in Continual  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pengyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Caprio%2C+M">Michele Caprio</a>, 
<a href="/search/cs?searchtype=author&query=Eaton%2C+E">Eric Eaton</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Insup Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Duplicate submission to arxiv
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03006" title="Abstract">arXiv:2310.03006</a> (replaced) [<a href="/pdf/2310.03006" title="Download PDF">pdf</a>, <a href="/format/2310.03006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COOLer: Class-Incremental Learning for Appearance-Based Multiple Object  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhizheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Segu%2C+M">Mattia Segu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fisher Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> GCPR 2023 Oral
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item310">Cross-lists</a></li>
<li><a href="#item353">Replacements</a></li>
</ul>
<small>[ total of 580 entries:  <b>1-580</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
