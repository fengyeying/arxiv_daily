<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Tue 17 Oct 23  to  Wed 18 Oct 23, announced Thu, 19 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item305">Cross-lists</a></li>
<li><a href="#item337">Replacements</a></li>
</ul>
<small>[ total of 567 entries:  <b>1-567</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Thu, 19 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11465" title="Abstract">arXiv:2310.11465</a> [<a href="/pdf/2310.11465" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in  Bangla with Multi-Feature and Multi-Modal Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imran%2C+A+A">Abdullah Al Imran</a>, 
<a href="/search/cs?searchtype=author&query=Shovon%2C+M+S+H">Md Sakib Hossain Shovon</a>, 
<a href="/search/cs?searchtype=author&query=Mridha%2C+M+F">M. F. Mridha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">This study presents a large multi-modal Bangla YouTube clickbait dataset
consisting of 253,070 data points collected through an automated process using
the YouTube API and Python web automation frameworks. The dataset contains 18
diverse features categorized into metadata, primary content, engagement
statistics, and labels for individual videos from 58 Bangla YouTube channels. A
rigorous preprocessing step has been applied to denoise, deduplicate, and
remove bias from the features, ensuring unbiased and reliable analysis. As the
largest and most robust clickbait corpus in Bangla to date, this dataset
provides significant value for natural language processing and data science
researchers seeking to advance modeling of clickbait phenomena in low-resource
languages. Its multi-modal nature allows for comprehensive analyses of
clickbait across content, user interactions, and linguistic dimensions to
develop more sophisticated detection methods with cross-linguistic
applications.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11466" title="Abstract">arXiv:2310.11466</a> [<a href="/pdf/2310.11466" title="Download PDF">pdf</a>, <a href="/format/2310.11466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Protein 3D Graph Structure Learning for Robust Structure-based Protein  Property Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jin Su</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lirong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+O">Odin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haitao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Jingqi Qi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiangbin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan.ZQ.Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Protein structure-based property prediction has emerged as a promising
approach for various biological tasks, such as protein function prediction and
sub-cellular location estimation. The existing methods highly rely on
experimental protein structure data and fail in scenarios where these data are
unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were
utilized as alternatives. However, we observed that current practices, which
simply employ accurately predicted structures during inference, suffer from
notable degradation in prediction accuracy. While similar phenomena have been
extensively studied in general fields (e.g., Computer Vision) as model
robustness, their impact on protein property prediction remains unexplored. In
this paper, we first investigate the reason behind the performance decrease
when utilizing predicted structures, attributing it to the structure embedding
bias from the perspective of structure representation learning. To study this
problem, we identify a Protein 3D Graph Structure Learning Problem for Robust
Protein Property Prediction (PGSL-RP3), collect benchmark datasets, and present
a protein Structure embedding Alignment Optimization framework (SAO) to
mitigate the problem of structure embedding bias between the predicted and
experimental protein structures. Extensive experiments have shown that our
framework is model-agnostic and effective in improving the property prediction
of both predicted structures and experimental structures. The benchmark
datasets and codes will be released to benefit the community.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11467" title="Abstract">arXiv:2310.11467</a> [<a href="/pdf/2310.11467" title="Download PDF">pdf</a>, <a href="/format/2310.11467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Binary Code Comment Quality Classification: Integrating  Generative AI for Improved Accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%2C+R+A">Rohith Arumugam S</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+A+D">Angel Deborah S</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures, 2 tables, Has been accepted for the Information Retrieval in Software Engineering track at Forum for Information Retrieval Evaluation 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This report focuses on enhancing a binary code comment quality classification
model by integrating generated code and comment pairs, to improve model
accuracy. The dataset comprises 9048 pairs of code and comments written in the
C programming language, each annotated as "Useful" or "Not Useful."
Additionally, code and comment pairs are generated using a Large Language Model
Architecture, and these generated pairs are labeled to indicate their utility.
The outcome of this effort consists of two classification models: one utilizing
the original dataset and another incorporating the augmented dataset with the
newly generated code comment pairs and labels.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11470" title="Abstract">arXiv:2310.11470</a> [<a href="/pdf/2310.11470" title="Download PDF">pdf</a>, <a href="/format/2310.11470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classic machine learning methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faouzi%2C+J">Johann Faouzi</a>, 
<a href="/search/cs?searchtype=author&query=Colliot%2C+O">Olivier Colliot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this chapter, we present the main classic machine learning methods. A
large part of the chapter is devoted to supervised learning techniques for
classification and regression, including nearest-neighbor methods, linear and
logistic regressions, support vector machines and tree-based algorithms. We
also describe the problem of overfitting as well as strategies to overcome it.
We finally provide a brief overview of unsupervised learning methods, namely
for clustering and dimensionality reduction.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11475" title="Abstract">arXiv:2310.11475</a> [<a href="/pdf/2310.11475" title="Download PDF">pdf</a>, <a href="/format/2310.11475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking and Mapping in Medical Computer Vision: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+A">Adam Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Mohareri%2C+O">Omid Mohareri</a>, 
<a href="/search/cs?searchtype=author&query=DiMaio%2C+S">Simon DiMaio</a>, 
<a href="/search/cs?searchtype=author&query=Yip%2C+M">Michael Yip</a>, 
<a href="/search/cs?searchtype=author&query=Salcudean%2C+S+E">Septimiu E. Salcudean</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">As computer vision algorithms are becoming more capable, their applications
in clinical systems will become more pervasive. These applications include
diagnostics such as colonoscopy and bronchoscopy, guiding biopsies and
minimally invasive interventions and surgery, automating instrument motion and
providing image guidance using pre-operative scans. Many of these applications
depend on the specific visual nature of medical scenes and require designing
and applying algorithms to perform in this environment.
<br />In this review, we provide an update to the field of camera-based tracking
and scene mapping in surgery and diagnostics in medical computer vision. We
begin with describing our review process, which results in a final list of 515
papers that we cover. We then give a high-level summary of the state of the art
and provide relevant background for those who need tracking and mapping for
their clinical applications. We then review datasets provided in the field and
the clinical needs therein. Then, we delve in depth into the algorithmic side,
and summarize recent developments, which should be especially useful for
algorithm designers and to those looking to understand the capability of
off-the-shelf methods. We focus on algorithms for deformable environments while
also reviewing the essential building blocks in rigid tracking and mapping
since there is a large amount of crossover in methods. Finally, we discuss the
current state of the tracking and mapping methods along with needs for future
algorithms, needs for quantification, and the viability of clinical
applications in the field. We conclude that new methods need to be designed or
combined to support clinical applications in deformable environments, and more
focus needs to be put into collecting datasets for training and evaluation.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11476" title="Abstract">arXiv:2310.11476</a> [<a href="/pdf/2310.11476" title="Download PDF">pdf</a>, <a href="/format/2310.11476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Program Translation via Code Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+M">Mengnan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yongqiang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Maoquan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+B">Bin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Clement%2C+C">Colin Clement</a>, 
<a href="/search/cs?searchtype=author&query=Sundaresan%2C+N">Neel Sundaresan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Software version migration and program translation are an important and
costly part of the lifecycle of large codebases. Traditional machine
translation relies on parallel corpora for supervised translation, which is not
feasible for program translation due to a dearth of aligned data. Recent
unsupervised neural machine translation techniques have overcome data
limitations by included techniques such as back translation and low level
compiler intermediate representations (IR). These methods face significant
challenges due to the noise in code snippet alignment and the diversity of IRs
respectively. In this paper we propose a novel model called Code Distillation
(CoDist) whereby we capture the semantic and structural equivalence of code in
a language agnostic intermediate representation. Distilled code serves as a
translation pivot for any programming language, leading by construction to
parallel corpora which scale to all available source code by simply applying
the distillation compiler. We demonstrate that our approach achieves
state-of-the-art performance on CodeXGLUE and TransCoder GeeksForGeeks
translation benchmarks, with an average absolute increase of 12.7% on the
TransCoder GeeksforGeeks translation benchmark compare to TransCoder-ST.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11477" title="Abstract">arXiv:2310.11477</a> [<a href="/pdf/2310.11477" title="Download PDF">pdf</a>, <a href="/format/2310.11477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults  Detection Using Multiple Deep Learning Training Strategies and A Novel Double  Loss Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+K">Khoa Tran</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+L">Lam Pham</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+H">Hai-Canh Vu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents a comprehensive analysis of motor bearing fault detection
(MBFD), which involves the task of identifying faults in a motor bearing based
on its vibration. To this end, we first propose and evaluate various machine
learning based systems for the MBFD task. Furthermore, we propose three deep
learning based systems for the MBFD task, each of which explores one of the
following training strategies: supervised learning, semi-supervised learning,
and unsupervised learning. The proposed machine learning based systems and deep
learning based systems are evaluated, compared, and then they are used to
identify the best model for the MBFD task. We conducted extensive experiments
on various benchmark datasets of motor bearing faults, including those from the
American Society for Mechanical Failure Prevention Technology (MFPT), Case
Western Reserve University Bearing Center (CWRU), and the Condition Monitoring
of Bearing Damage in Electromechanical Drive Systems from Paderborn University
(PU). The experimental results on different datasets highlight two main
contributions of this study. First, we prove that deep learning based systems
are more effective than machine learning based systems for the MBFD task.
Second, we achieve a robust and general deep learning based system with a novel
loss function for the MBFD task on several benchmark datasets, demonstrating
its potential for real-life MBFD applications.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11478" title="Abstract">arXiv:2310.11478</a> [<a href="/pdf/2310.11478" title="Download PDF">pdf</a>, <a href="/format/2310.11478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASP: Automatic Selection of Proxy dataset for efficient AutoML
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+P">Peng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+C">Chao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jiyuan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jianchao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chengru Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was actually finished in 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep neural networks have gained great success due to the increasing amounts
of data, and diverse effective neural network designs. However, it also brings
a heavy computing burden as the amount of training data is proportional to the
training time. In addition, a well-behaved model requires repeated trials of
different structure designs and hyper-parameters, which may take a large amount
of time even with state-of-the-art (SOTA) hyper-parameter optimization (HPO)
algorithms and neural architecture search (NAS) algorithms. In this paper, we
propose an Automatic Selection of Proxy dataset framework (ASP) aimed to
dynamically find the informative proxy subsets of training data at each epoch,
reducing the training data size as well as saving the AutoML processing time.
We verify the effectiveness and generalization of ASP on CIFAR10, CIFAR100,
ImageNet16-120, and ImageNet-1k, across various public model benchmarks. The
experiment results show that ASP can obtain better results than other data
selection methods at all selection ratios. ASP can also enable much more
efficient AutoML processing with a speedup of 2x-20x while obtaining better
architectures and better hyper-parameters compared to utilizing the entire
dataset.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11479" title="Abstract">arXiv:2310.11479</a> [<a href="/pdf/2310.11479" title="Download PDF">pdf</a>, <a href="/format/2310.11479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Temperature of Bayesian Graph Neural Networks for Conformal  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cha%2C+S">Seohyeon Cha</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Honggu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Joonhyuk Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Accurate uncertainty quantification in graph neural networks (GNNs) is
essential, especially in high-stakes domains where GNNs are frequently
employed. Conformal prediction (CP) offers a promising framework for
quantifying uncertainty by providing $\textit{valid}$ prediction sets for any
black-box model. CP ensures formal probabilistic guarantees that a prediction
set contains a true label with a desired probability. However, the size of
prediction sets, known as $\textit{inefficiency}$, is influenced by the
underlying model and data generating process. On the other hand, Bayesian
learning also provides a credible region based on the estimated posterior
distribution, but this region is $\textit{well-calibrated}$ only when the model
is correctly specified. Building on a recent work that introduced a scaling
parameter for constructing valid credible regions from posterior estimate, our
study explores the advantages of incorporating a temperature parameter into
Bayesian GNNs within CP framework. We empirically demonstrate the existence of
temperatures that result in more efficient prediction sets. Furthermore, we
conduct an analysis to identify the factors contributing to inefficiency and
offer valuable insights into the relationship between CP performance and model
calibration.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11481" title="Abstract">arXiv:2310.11481</a> [<a href="/pdf/2310.11481" title="Download PDF">pdf</a>, <a href="/format/2310.11481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contracting Tsetlin Machine with Absorbing Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+B">Bimal Bhattarai</a>, 
<a href="/search/cs?searchtype=author&query=Granmo%2C+O">Ole-Christoffer Granmo</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+L">Lei Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Andersen%2C+P">Per-Arne Andersen</a>, 
<a href="/search/cs?searchtype=author&query=Tunheim%2C+S+A">Svein Anders Tunheim</a>, 
<a href="/search/cs?searchtype=author&query=Shafik%2C+R">Rishad Shafik</a>, 
<a href="/search/cs?searchtype=author&query=Yakovlev%2C+A">Alex Yakovlev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ISTM2023. 7 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we introduce a sparse Tsetlin Machine (TM) with absorbing
Tsetlin Automata (TA) states. In brief, the TA of each clause literal has both
an absorbing Exclude- and an absorbing Include state, making the learning
scheme absorbing instead of ergodic. When a TA reaches an absorbing state, it
will never leave that state again. If the absorbing state is an Exclude state,
both the automaton and the literal can be removed from further consideration.
The literal will as a result never participates in that clause. If the
absorbing state is an Include state, on the other hand, the literal is stored
as a permanent part of the clause while the TA is discarded. A novel sparse
data structure supports these updates by means of three action lists: Absorbed
Include, Include, and Exclude. By updating these lists, the TM gets smaller and
smaller as the literals and their TA withdraw. In this manner, the computation
accelerates during learning, leading to faster learning and less energy
consumption.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11482" title="Abstract">arXiv:2310.11482</a> [<a href="/pdf/2310.11482" title="Download PDF">pdf</a>, <a href="/format/2310.11482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Class-incremental Learning in the Era of Large Pre-trained  Models via Test-Time Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marouf%2C+I+E">Imad Eddine Marouf</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Subhankar Roy</a>, 
<a href="/search/cs?searchtype=author&query=Tartaglione%2C+E">Enzo Tartaglione</a>, 
<a href="/search/cs?searchtype=author&query=Lathuili%C3%A8re%2C+S">St&#xe9;phane Lathuili&#xe8;re</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages,5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Class-incremental learning (CIL) is a challenging task that involves
continually learning to categorize classes into new tasks without forgetting
previously learned information. The advent of the large pre-trained models
(PTMs) has fast-tracked the progress in CIL due to the highly transferable PTM
representations, where tuning a small set of parameters results in
state-of-the-art performance when compared with the traditional CIL methods
that are trained from scratch. However, repeated fine-tuning on each task
destroys the rich representations of the PTMs and further leads to forgetting
previous tasks. To strike a balance between the stability and plasticity of
PTMs for CIL, we propose a novel perspective of eliminating training on every
new task and instead performing test-time adaptation (TTA) directly on the test
instances. Concretely, we propose "Test-Time Adaptation for Class-Incremental
Learning" (TTACIL) that first fine-tunes Layer Norm parameters of the PTM on
each test instance for learning task-specific features, and then resets them
back to the base model to preserve stability. As a consequence, TTACIL does not
undergo any forgetting, while benefiting each task with the rich PTM features.
Additionally, by design, our method is robust to common data corruptions. Our
TTACIL outperforms several state-of-the-art CIL methods when evaluated on
multiple CIL benchmarks under both clean and corrupted data.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11501" title="Abstract">arXiv:2310.11501</a> [<a href="/pdf/2310.11501" title="Download PDF">pdf</a>, <a href="/format/2310.11501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Myra Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Piccardi%2C+T">Tiziano Piccardi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023 (Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Recent work has aimed to capture nuances of human behavior by using LLMs to
simulate responses from particular demographics in settings like social science
experiments and public opinion surveys. However, there are currently no
established ways to discuss or evaluate the quality of such LLM simulations.
Moreover, there is growing concern that these LLM simulations are flattened
caricatures of the personas that they aim to simulate, failing to capture the
multidimensionality of people and perpetuating stereotypes. To bridge these
gaps, we present CoMPosT, a framework to characterize LLM simulations using
four dimensions: Context, Model, Persona, and Topic. We use this framework to
measure open-ended LLM simulations' susceptibility to caricature, defined via
two criteria: individuation and exaggeration. We evaluate the level of
caricature in scenarios from existing work on LLM simulations. We find that for
GPT-4, simulations of certain demographics (political and marginalized groups)
and topics (general, uncontroversial) are highly susceptible to caricature.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11506" title="Abstract">arXiv:2310.11506</a> [<a href="/pdf/2310.11506" title="Download PDF">pdf</a>, <a href="/ps/2310.11506" title="Download PostScript">ps</a>, <a href="/format/2310.11506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Kripke-Lewis semantics for belief update and belief revision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonanno%2C+G">Giacomo Bonanno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We provide a new characterization of both belief update and belief revision
in terms of a Kripke-Lewis semantics. We consider frames consisting of a set of
states, a Kripke belief relation and a Lewis selection function. Adding a
valuation to a frame yields a model. Given a model and a state, we identify the
initial belief set K with the set of formulas that are believed at that state
and we identify either the updated belief set or the revised belief set,
prompted by the input represented by formula A, as the set of formulas that are
the consequent of conditionals that (1) are believed at that state and (2) have
A as antecedent. We show that this class of models characterizes both the
Katsuno-Mendelzon (KM) belief update functions and the AGM belief revision
functions, in the following sense: (1) each model gives rise to a partial
belief function that can be completed into a full KM/AGM update/revision
function, and (2) for every KM/AGM update/revision function there is a model
whose associated belief function coincides with it. The difference between
update and revision can be reduced to two semantic properties that appear in a
stronger form in revision relative to update, thus confirming the finding by
Peppas et al. (1996) that, "for a fixed theory K, revising K is much the same
as updating K"
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11511" title="Abstract">arXiv:2310.11511</a> [<a href="/pdf/2310.11511" title="Download PDF">pdf</a>, <a href="/format/2310.11511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asai%2C+A">Akari Asai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zeqiu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sil%2C+A">Avirup Sil</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 2 figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite their remarkable capabilities, large language models (LLMs) often
produce responses containing factual inaccuracies due to their sole reliance on
the parametric knowledge they encapsulate. Retrieval-Augmented Generation
(RAG), an ad hoc approach that augments LMs with retrieval of relevant
knowledge, decreases such issues. However, indiscriminately retrieving and
incorporating a fixed number of retrieved passages, regardless of whether
retrieval is necessary, or passages are relevant, diminishes LM versatility or
can lead to unhelpful response generation. We introduce a new framework called
Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's
quality and factuality through retrieval and self-reflection. Our framework
trains a single arbitrary LM that adaptively retrieves passages on-demand, and
generates and reflects on retrieved passages and its own generations using
special tokens, called reflection tokens. Generating reflection tokens makes
the LM controllable during the inference phase, enabling it to tailor its
behavior to diverse task requirements. Experiments show that Self-RAG (7B and
13B parameters) significantly outperforms state-of-the-art LLMs and
retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in
improving factuality and citation accuracy for long-form generations relative
to these models.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11513" title="Abstract">arXiv:2310.11513</a> [<a href="/pdf/2310.11513" title="Download PDF">pdf</a>, <a href="/format/2310.11513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenEval: An Object-Focused Framework for Evaluating Text-to-Image  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+D">Dhruba Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hanna Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+L">Ludwig Schmidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent breakthroughs in diffusion models, multimodal pretraining, and
efficient finetuning have led to an explosion of text-to-image generative
models. Given human evaluation is expensive and difficult to scale, automated
methods are critical for evaluating the increasingly large number of new
models. However, most current automated evaluation metrics like FID or
CLIPScore only offer a holistic measure of image quality or image-text
alignment, and are unsuited for fine-grained or instance-level analysis. In
this paper, we introduce GenEval, an object-focused framework to evaluate
compositional image properties such as object co-occurrence, position, count,
and color. We show that current object detection models can be leveraged to
evaluate text-to-image models on a variety of generation tasks with strong
human agreement, and that other discriminative vision models can be linked to
this pipeline to further verify properties like object color. We then evaluate
several open-source text-to-image models and analyze their relative generative
capabilities on our benchmark. We find that recent models demonstrate
significant improvement on these tasks, though they are still lacking in
complex capabilities such as spatial relations and attribute binding. Finally,
we demonstrate how GenEval might be used to help discover existing failure
modes, in order to inform development of the next generation of text-to-image
models. Our code to run the GenEval framework is publicly available at
https://github.com/djghosh13/geneval.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11515" title="Abstract">arXiv:2310.11515</a> [<a href="/pdf/2310.11515" title="Download PDF">pdf</a>, <a href="/ps/2310.11515" title="Download PostScript">ps</a>, <a href="/format/2310.11515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement  Learning in Discounted Linear MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hung%2C+Y">Yu-Heng Hung</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+P">Ping-Chun Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Mete%2C+A">Akshay Mete</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P+R">P. R. Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We consider the infinite-horizon linear Markov Decision Processes (MDPs),
where the transition probabilities of the dynamic model can be linearly
parameterized with the help of a predefined low-dimensional feature mapping.
While the existing regression-based approaches have been theoretically shown to
achieve nearly-optimal regret, they are computationally rather inefficient due
to the need for a large number of optimization runs in each time step,
especially when the state and action spaces are large. To address this issue,
we propose to solve linear MDPs through the lens of Value-Biased Maximum
Likelihood Estimation (VBMLE), which is a classic model-based exploration
principle in the adaptive control literature for resolving the well-known
closed-loop identification problem of Maximum Likelihood Estimation. We
formally show that (i) VBMLE enjoys $\widetilde{O}(d\sqrt{T})$ regret, where
$T$ is the time horizon and $d$ is the dimension of the model parameter, and
(ii) VBMLE is computationally more efficient as it only requires solving one
optimization problem in each time step. In our regret analysis, we offer a
generic convergence result of MLE in linear MDPs through a novel
supermartingale construct and uncover an interesting connection between linear
MDPs and online learning, which could be of independent interest. Finally, the
simulation results show that VBMLE significantly outperforms the benchmark
method in terms of both empirical regret and computation time.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11516" title="Abstract">arXiv:2310.11516</a> [<a href="/pdf/2310.11516" title="Download PDF">pdf</a>, <a href="/format/2310.11516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Field Robot for High-throughput and High-resolution 3D Plant Phenotyping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esser%2C+F">Felix Esser</a>, 
<a href="/search/cs?searchtype=author&query=Rosu%2C+R+A">Radu Alexandru Rosu</a>, 
<a href="/search/cs?searchtype=author&query=Corneli%C3%9Fen%2C+A">Andr&#xe9; Corneli&#xdf;en</a>, 
<a href="/search/cs?searchtype=author&query=Klingbeil%2C+L">Lasse Klingbeil</a>, 
<a href="/search/cs?searchtype=author&query=Kuhlmann%2C+H">Heiner Kuhlmann</a>, 
<a href="/search/cs?searchtype=author&query=Behnke%2C+S">Sven Behnke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">With the need to feed a growing world population, the efficiency of crop
production is of paramount importance. To support breeding and field
management, various characteristics of the plant phenotype need to be measured
-- a time-consuming process when performed manually. We present a robotic
platform equipped with multiple laser and camera sensors for high-throughput,
high-resolution in-field plant scanning. We create digital twins of the plants
through 3D reconstruction. This allows the estimation of phenotypic traits such
as leaf area, leaf angle, and plant height. We validate our system on a real
field, where we reconstruct accurate point clouds and meshes of sugar beet,
soybean, and maize.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11518" title="Abstract">arXiv:2310.11518</a> [<a href="/pdf/2310.11518" title="Download PDF">pdf</a>, <a href="/format/2310.11518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guarantees for Self-Play in Multiplayer Games via Polymatrix  Decomposability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=MacQueen%2C+R">Revan MacQueen</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+J+R">James R. Wright</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Self-play is a technique for machine learning in multi-agent systems where a
learning algorithm learns by interacting with copies of itself. Self-play is
useful for generating large quantities of data for learning, but has the
drawback that the agents the learner will face post-training may have
dramatically different behavior than the learner came to expect by interacting
with itself. For the special case of two-player constant-sum games, self-play
that reaches Nash equilibrium is guaranteed to produce strategies that perform
well against any post-training opponent; however, no such guarantee exists for
multi-player games. We show that in games that approximately decompose into a
set of two-player constant-sum games (called polymatrix games) where global
$\epsilon$-Nash equilibria are boundedly far from Nash-equilibria in each
subgame, any no-external-regret algorithm that learns by self-play will produce
a strategy with bounded vulnerability. For the first time, our results identify
a structural property of multi-player games that enable performance guarantees
for the strategies produced by a broad class of self-play algorithms. We
demonstrate our findings through experiments on Leduc poker.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11520" title="Abstract">arXiv:2310.11520</a> [<a href="/pdf/2310.11520" title="Download PDF">pdf</a>, <a href="/format/2310.11520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic News Summerization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dheer%2C+K">Kavach Dheer</a>, 
<a href="/search/cs?searchtype=author&query=Dhankhar%2C+A">Arpit Dhankhar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Natural Language Processing is booming with its applications in the real
world, one of which is Text Summarization for large texts including news
articles. This research paper provides an extensive comparative evaluation of
extractive and abstractive approaches for news text summarization, with an
emphasis on the ROUGE score analysis. The study employs the CNN-Daily Mail
dataset, which consists of news articles and human-generated reference
summaries. The evaluation employs ROUGE scores to assess the efficacy and
quality of generated summaries. After Evaluation, we integrate the
best-performing models on a web application to assess their real-world
capabilities and user experience.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11521" title="Abstract">arXiv:2310.11521</a> [<a href="/pdf/2310.11521" title="Download PDF">pdf</a>, <a href="/format/2310.11521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DataGarden: Exploring our Community in a VR Data Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kondo%2C+J">Joy Kondo</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Justin Park</a>, 
<a href="/search/cs?searchtype=author&query=Kondo%2C+J">Josiah Kondo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N+W">Nam Wook Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">As our society is becoming increasingly data-dependent, more and more people
rely on charts and graphs to understand and communicate complex data. While
such visualizations effectively reveal meaningful trends, they unavoidably
aggregate data into points and bars that are overly simplified depictions of
ourselves and our communities. We present DataGarden, a system that supports
embodied interactions with humane data representations in an immersive VR
environment. Through the system, we explore ways to rethink the traditional
visualization approach and allow people to empathize more deeply with the
people behind the data.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11523" title="Abstract">arXiv:2310.11523</a> [<a href="/pdf/2310.11523" title="Download PDF">pdf</a>, <a href="/format/2310.11523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group Preference Optimization: Few-Shot Alignment of Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Siyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+J">John Dang</a>, 
<a href="/search/cs?searchtype=author&query=Grover%2C+A">Aditya Grover</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Many applications of large language models (LLMs), ranging from chatbots to
creative writing, require nuanced subjective judgments that can differ
significantly across different groups. Existing alignment algorithms can be
expensive to align for each group, requiring prohibitive amounts of
group-specific preference data and computation for real-world use cases. We
introduce Group Preference Optimization (GPO), an alignment framework that
steers language models to preferences of individual groups in a few-shot
manner. In GPO, we augment the base LLM with an independent transformer module
trained to predict the preferences of a group for the LLM generations. For
few-shot learning, we parameterize this module as an in-context autoregressive
transformer and train it via meta-learning on several groups. We empirically
validate the efficacy of GPO through rigorous evaluations using LLMs with
varied sizes on three human opinion adaptation tasks. These tasks involve
adapting to the preferences of US demographic groups, global countries, and
individual users. Our results demonstrate that GPO not only aligns models more
accurately but also requires fewer group-specific preferences, and less
training and inference computing resources, outperforming existing strategies
such as in-context steering and fine-tuning methods.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11524" title="Abstract">arXiv:2310.11524</a> [<a href="/pdf/2310.11524" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward an Ontology for Third Generation Systems Thinking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levenchuk%2C+A">Anatoly Levenchuk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Systems thinking is a way of making sense about the world in terms of
multilevel, nested, interacting systems, their environment, and the boundaries
between the systems and the environment. In this paper we discuss the evolution
of systems thinking and discuss what is needed for an ontology of the current
generation of systems thinking.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11531" title="Abstract">arXiv:2310.11531</a> [<a href="/pdf/2310.11531" title="Download PDF">pdf</a>, <a href="/ps/2310.11531" title="Download PostScript">ps</a>, <a href="/format/2310.11531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Online Learning with Offline Datasets for Infinite Horizon  MDPs: A Bayesian Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Dengwang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rahul Jain</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+B">Botao Hao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zheng Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we study the problem of efficient online reinforcement
learning in the infinite horizon setting when there is an offline dataset to
start with. We assume that the offline dataset is generated by an expert but
with unknown level of competence, i.e., it is not perfect and not necessarily
using the optimal policy. We show that if the learning agent models the
behavioral policy (parameterized by a competence parameter) used by the expert,
it can do substantially better in terms of minimizing cumulative regret, than
if it doesn't do that. We establish an upper bound on regret of the exact
informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a
novel prior-dependent regret analysis of Bayesian online learning algorithms
for the infinite horizon setting. We then propose an approximate Informed RLSVI
algorithm that we can interpret as performing imitation learning with the
offline dataset, and then performing online learning.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11532" title="Abstract">arXiv:2310.11532</a> [<a href="/pdf/2310.11532" title="Download PDF">pdf</a>, <a href="/format/2310.11532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-stage Large Language Model Correction for Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pu%2C+J">Jie Pu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thai-Son Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=St%C3%BCker%2C+S">Sebastian St&#xfc;ker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we investigate the usage of large language models (LLMs) to
improve the performance of competitive speech recognition systems. Different
from traditional language models that focus on one single data domain, the rise
of LLMs brings us the opportunity to push the limit of state-of-the-art ASR
performance, and at the same time to achieve higher robustness and generalize
effectively across multiple domains. Motivated by this, we propose a novel
multi-stage approach to combine traditional language model re-scoring and LLM
prompting. Specifically, the proposed method has two stages: the first stage
uses a language model to re-score an N-best list of ASR hypotheses and run a
confidence check; The second stage uses prompts to a LLM to perform ASR error
correction on less confident results from the first stage. Our experimental
results demonstrate the effectiveness of the proposed method by showing a 10% ~
20% relative improvement in WER over a competitive ASR system -- across
multiple test domains.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11534" title="Abstract">arXiv:2310.11534</a> [<a href="/pdf/2310.11534" title="Download PDF">pdf</a>, <a href="/format/2310.11534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HMN: Generalization of Heterogeneous and Multi-layered Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S+K">Shraban Kumar Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Suman Kundu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">A network may have different types of entities and their relations. Further,
there could be additional layers of ties. The former is referred to as
Heterogeneous networks, while the latter is known as Multi-layer networks. The
present paper provides a generalized network model, namely, a Heterogeneous
Multi-layered Network (HMN), which can simultaneously be multi-layered and
heterogeneous. The model can represent homogeneous networks as well. We define
different structural measures in an HMN. We proved that the sets of all
homogeneous, heterogeneous and multi-layered networks are subsets of the set of
all HMNs. Accordingly, we established the equivalency of the proposed
structural measures of HMNs with that of homogeneous, heterogeneous, and
multi-layered networks. Following that, we show how our proposed HMN is more
efficient in tasks such as link prediction. In addition, we present a novel
parameterized algorithm (with complexity analysis) for generating synthetic
HMNs. The networks generated from our proposed algorithm are more consistent in
modelling the layer-wise degree distribution of a real-world Twitter network
(represented as HMN) than those generated by existing models. Moreover, we also
show that our algorithm is more effective in modelling an air-transportation
multiplex network when compared to an algorithm designed specifically for the
task.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11536" title="Abstract">arXiv:2310.11536</a> [<a href="/pdf/2310.11536" title="Download PDF">pdf</a>, <a href="/format/2310.11536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diver Interest via Pointing in Three Dimensions: 3D Pointing  Reconstruction for Diver-AUV Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edge%2C+C">Chelsey Edge</a>, 
<a href="/search/cs?searchtype=author&query=Kutzke%2C+D">Demetrious Kutzke</a>, 
<a href="/search/cs?searchtype=author&query=Bromhal%2C+M">Megdalia Bromhal</a>, 
<a href="/search/cs?searchtype=author&query=Sattar%2C+J">Junaed Sattar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review International Conference of Robotics and Automation 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents Diver Interest via Pointing in Three Dimensions (DIP-3D),
a method to relay an object of interest from a diver to an autonomous
underwater vehicle (AUV) by pointing that includes three-dimensional distance
information to discriminate between multiple objects in the AUV's camera image.
Traditional dense stereo vision for distance estimation underwater is
challenging because of the relative lack of saliency of scene features and
degraded lighting conditions. Yet, including distance information is necessary
for robotic perception of diver pointing when multiple objects appear within
the robot's image plane. We subvert the challenges of underwater distance
estimation by using sparse reconstruction of keypoints to perform pose
estimation on both the left and right images from the robot's stereo camera.
Triangulated pose keypoints, along with a classical object detection method,
enable DIP-3D to infer the location of an object of interest when multiple
objects are in the AUV's field of view. By allowing the scuba diver to point at
an arbitrary object of interest and enabling the AUV to autonomously decide
which object the diver is pointing to, this method will permit more natural
interaction between AUVs and human scuba divers in underwater-human robot
collaborative tasks.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11541" title="Abstract">arXiv:2310.11541</a> [<a href="/pdf/2310.11541" title="Download PDF">pdf</a>, <a href="/format/2310.11541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUST&amp;P-SRL: Multi-lingual and Unified Syllabification in Text and  Phonetic Domains for Speech Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tits%2C+N">No&#xe9; Tits</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we present a methodology for linguistic feature extraction,
focusing particularly on automatically syllabifying words in multiple
languages, with a design to be compatible with a forced-alignment tool, the
Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our
method focuses on the extraction of phonetic transcriptions from text, stress
marks, and a unified automatic syllabification (in text and phonetic domains).
The system was built with open-source components and resources. Through an
ablation study, we demonstrate the efficacy of our approach in automatically
syllabifying words from several languages (English, French and Spanish).
Additionally, we apply the technique to the transcriptions of the CMU ARCTIC
dataset, generating valuable annotations available
online\footnote{\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for
speech representation learning, speech unit discovery, and disentanglement of
speech factors in several speech-related fields.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11546" title="Abstract">arXiv:2310.11546</a> [<a href="/pdf/2310.11546" title="Download PDF">pdf</a>, <a href="/ps/2310.11546" title="Download PostScript">ps</a>, <a href="/format/2310.11546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bias and Error Mitigation in Software-Generated Data: An Advanced Search  and Optimization Framework Leveraging Generative Code Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez%2C+E+G">Ernesto Giralt Hern&#xe1;ndez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Data generation and analysis is a fundamental aspect of many industries and
disciplines, from strategic decision making in business to research in the
physical and social sciences. However, data generated using software and
algorithms can be subject to biases and errors. These can be due to problems
with the original software, default settings that do not align with the
specific needs of the situation, or even deeper problems with the underlying
theories and models. This paper proposes an advanced search and optimization
framework aimed at generating and choosing optimal source code capable of
correcting errors and biases from previous versions to address typical problems
in software systems specializing in data analysis and generation, especially
those in the corporate and data science world. Applying this framework multiple
times on the same software system would incrementally improve the quality of
the output results. It uses Solomonoff Induction as a sound theoretical basis,
extending it with Kolmogorov Conditional Complexity, a novel adaptation, to
evaluate a set of candidate programs. We propose the use of generative models
for the creation of this set of programs, with special emphasis on the
capabilities of Large Language Models (LLMs) to generate high quality code.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11548" title="Abstract">arXiv:2310.11548</a> [<a href="/pdf/2310.11548" title="Download PDF">pdf</a>, <a href="/format/2310.11548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Data Generation with Missing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohapatra%2C+S">Shubhankar Mohapatra</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+J">Jianqiao Zong</a>, 
<a href="/search/cs?searchtype=author&query=Kerschbaum%2C+F">Florian Kerschbaum</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xi He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Despite several works that succeed in generating synthetic data with
differential privacy (DP) guarantees, they are inadequate for generating
high-quality synthetic data when the input data has missing values. In this
work, we formalize the problems of DP synthetic data with missing values and
propose three effective adaptive strategies that significantly improve the
utility of the synthetic data on four real-world datasets with different types
and levels of missing data and privacy requirements. We also identify the
relationship between privacy impact for the complete ground truth data and
incomplete data for these DP synthetic data generation algorithms. We model the
missing mechanisms as a sampling process to obtain tighter upper bounds for the
privacy guarantees to the ground truth data. Overall, this study contributes to
a better understanding of the challenges and opportunities for using private
synthetic data generation algorithms in the presence of missing data.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11550" title="Abstract">arXiv:2310.11550</a> [<a href="/pdf/2310.11550" title="Download PDF">pdf</a>, <a href="/ps/2310.11550" title="Download PostScript">ps</a>, <a href="/format/2310.11550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haolin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chen-Yu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zimmert%2C+J">Julian Zimmert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study online reinforcement learning in linear Markov decision processes
with adversarial losses and bandit feedback, without prior knowledge on
transitions or access to simulators. We introduce two algorithms that achieve
improved regret performance compared to existing approaches. The first
algorithm, although computationally inefficient, ensures a regret of
$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$, where $K$ is the number of
episodes. This is the first result with the optimal $K$ dependence in the
considered setting. The second algorithm, which is based on the policy
optimization framework, guarantees a regret of
$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$ and is computationally
efficient. Both our results significantly improve over the state-of-the-art: a
computationally inefficient algorithm by Kong et al. [2023] with
$\widetilde{\mathcal{O}}\left(K^{\frac{4}{5}}+poly\left(\frac{1}{\lambda_{\min}}\right)
\right)$ regret, for some problem-dependent constant $\lambda_{\min}$ that can
be arbitrarily close to zero, and a computationally efficient algorithm by
Sherman et al. [2023b] with $\widetilde{\mathcal{O}}\left(K^{\frac{6}{7}}
\right)$ regret.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11551" title="Abstract">arXiv:2310.11551</a> [<a href="/pdf/2310.11551" title="Download PDF">pdf</a>, <a href="/format/2310.11551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WaveFlex: A Smart Surface for Private CBRS Wireless Cellular Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+F">Fan Yi</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K+W">Kun Woo Cho</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yaxiong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+K">Kyle Jamieson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We present the design and implementation of WaveFlex, the first smart surface
that enhances Private LTE/5G networks operating under the shared-license
framework in the Citizens Broadband Radio Service frequency band. WaveFlex
works in the presence of frequency diversity: multiple nearby base stations
operating on different frequencies, as dictated by a Spectrum Access System
coordinator. It also handles time dynamism: due to the dynamic sharing rules of
the band, base stations occasionally switch channels, especially when priority
users enter the network. Finally, WaveFlex operates independently of the
network itself, not requiring access to nor modification of the base station or
mobile users, yet it remain compliant with and effective on prevailing cellular
protocols. We have designed and fabricated WaveFlex on a custom multi-layer
PCB, software defined radio-based network monitor, and supporting control
software and hardware. Our experimental evaluation benchmarks an operational
Private LTE network running at full line rate. Results demonstrate an 8.50 dB
average SNR gain, and an average throughput gain of 4.36 Mbps for a single
small cell, and 3.19 Mbps for four small cells, in a realistic indoor office
scenario.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11555" title="Abstract">arXiv:2310.11555</a> [<a href="/pdf/2310.11555" title="Download PDF">pdf</a>, <a href="/format/2310.11555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating 3D City Data through Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Linfang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+G">Guohui Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Pano%2C+A">Albulen Pano</a>, 
<a href="/search/cs?searchtype=author&query=Fumagalli%2C+M">Mattia Fumagalli</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongsheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Calvanese%2C+D">Diego Calvanese</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Hongchao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Liqiu Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">CityGML is a widely adopted standard by the Open Geospatial Consortium (OGC)
for representing and exchanging 3D city models. The representation of semantic
and topological properties in CityGML makes it possible to query such 3D city
data to perform analysis in various applications, e.g., security management and
emergency response, energy consumption and estimation, and occupancy
measurement. However, the potential of querying CityGML data has not been fully
exploited. The official GML/XML encoding of CityGML is only intended as an
exchange format but is not suitable for query answering. The most common way of
dealing with CityGML data is to store them in the 3DCityDB system as relational
tables and then query them with the standard SQL query language. Nevertheless,
for end users, it remains a challenging task to formulate queries over 3DCityDB
directly for their ad-hoc analytical tasks, because there is a gap between the
conceptual semantics of CityGML and the relational schema adopted in 3DCityDB.
In fact, the semantics of CityGML itself can be modeled as a suitable ontology.
The technology of Knowledge Graphs (KGs), where an ontology is at the core, is
a good solution to bridge such a gap. Moreover, embracing KGs makes it easier
to integrate with other spatial data sources, e.g., OpenStreetMap and existing
(Geo)KGs (e.g., Wikidata, DBPedia, and GeoNames), and to perform queries
combining information from multiple data sources. In this work, we describe a
CityGML KG framework to populate the concepts in the CityGML ontology using
declarative mappings to 3DCityDB, thus exposing the CityGML data therein as a
KG. To demonstrate the feasibility of our approach, we use CityGML data from
the city of Munich as test data and integrate OpenStreeMap data in the same
area.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11557" title="Abstract">arXiv:2310.11557</a> [<a href="/pdf/2310.11557" title="Download PDF">pdf</a>, <a href="/format/2310.11557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Musical, Lyrical, and Network Dimensions of Music Sharing  Among Depression Individuals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tahir%2C+A">Anique Tahir</a>, 
<a href="/search/cs?searchtype=author&query=Alghamdi%2C+Z">Zeyad Alghamdi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2007.03137">arXiv:2007.03137</a>, <a href="/abs/2205.03459">arXiv:2205.03459</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Depression has emerged as a significant mental health concern due to a
variety of factors, reflecting broader societal and individual challenges.
Within the digital era, social media has become an important platform for
individuals navigating through depression, enabling them to express their
emotional and mental states through various mediums, notably music.
Specifically, their music preferences, manifested through sharing practices,
inadvertently offer a glimpse into their psychological and emotional
landscapes. This work seeks to study the differences in music preferences
between individuals diagnosed with depression and non-diagnosed individuals,
exploring numerous facets of music, including musical features, lyrics, and
musical networks. The music preferences of individuals with depression through
music sharing on social media, reveal notable differences in musical features
and topics and language use of lyrics compared to non-depressed individuals. We
find the network information enhances understanding of the link between music
listening patterns. The result highlights a potential echo-chamber effect,
where depression individual's musical choices may inadvertently perpetuate
depressive moods and emotions. In sum, this study underscores the significance
of examining music's various aspects to grasp its relationship with mental
health, offering insights for personalized music interventions and
recommendation algorithms that could benefit individuals with depression.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11558" title="Abstract">arXiv:2310.11558</a> [<a href="/pdf/2310.11558" title="Download PDF">pdf</a>, <a href="/format/2310.11558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Algorithms with Uncertainty-Quantified Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Bo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jerry Huang</a>, 
<a href="/search/cs?searchtype=author&query=Christianson%2C+N">Nicolas Christianson</a>, 
<a href="/search/cs?searchtype=author&query=Hajiesmaili%2C+M">Mohammad Hajiesmaili</a>, 
<a href="/search/cs?searchtype=author&query=Wierman%2C+A">Adam Wierman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Online algorithms with predictions have become a trending topic in the field
of beyond worst-case analysis of algorithms. These algorithms incorporate
predictions about the future to obtain performance guarantees that are of high
quality when the predictions are good, while still maintaining bounded
worst-case guarantees when predictions are arbitrarily poor. In general, the
algorithm is assumed to be unaware of the prediction's quality. However, recent
developments in the machine learning literature have studied techniques for
providing uncertainty quantification on machine-learned predictions, which
describes how certain a model is about its quality. This paper examines the
question of how to optimally utilize uncertainty-quantified predictions in the
design of online algorithms. In particular, we consider predictions augmented
with uncertainty quantification describing the likelihood of the ground truth
falling in a certain range, designing online algorithms with these
probabilistic predictions for two classic online problems: ski rental and
online search. In each case, we demonstrate that non-trivial modifications to
algorithm design are needed to fully leverage the probabilistic predictions.
Moreover, we consider how to utilize more general forms of uncertainty
quantification, proposing a framework based on online learning that learns to
exploit uncertainty quantification to make optimal decisions in multi-instance
settings.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11559" title="Abstract">arXiv:2310.11559</a> [<a href="/pdf/2310.11559" title="Download PDF">pdf</a>, <a href="/format/2310.11559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidential Consortium Framework: Secure Multiparty Applications with  Confidentiality, Integrity, and High Availability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Howard%2C+H">Heidi Howard</a>, 
<a href="/search/cs?searchtype=author&query=Alder%2C+F">Fritz Alder</a>, 
<a href="/search/cs?searchtype=author&query=Ashton%2C+E">Edward Ashton</a>, 
<a href="/search/cs?searchtype=author&query=Chamayou%2C+A">Amaury Chamayou</a>, 
<a href="/search/cs?searchtype=author&query=Clebsch%2C+S">Sylvan Clebsch</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+M">Manuel Costa</a>, 
<a href="/search/cs?searchtype=author&query=Delignat-Lavaud%2C+A">Antoine Delignat-Lavaud</a>, 
<a href="/search/cs?searchtype=author&query=Fournet%2C+C">Cedric Fournet</a>, 
<a href="/search/cs?searchtype=author&query=Jeffery%2C+A">Andrew Jeffery</a>, 
<a href="/search/cs?searchtype=author&query=Kerner%2C+M">Matthew Kerner</a>, 
<a href="/search/cs?searchtype=author&query=Kounelis%2C+F">Fotios Kounelis</a>, 
<a href="/search/cs?searchtype=author&query=Kuppe%2C+M+A">Markus A. Kuppe</a>, 
<a href="/search/cs?searchtype=author&query=Maffre%2C+J">Julien Maffre</a>, 
<a href="/search/cs?searchtype=author&query=Russinovich%2C+M">Mark Russinovich</a>, 
<a href="/search/cs?searchtype=author&query=Wintersteiger%2C+C+M">Christoph M. Wintersteiger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures. To appear in the Proceedings of the VLDB Endowment, Volume 17
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Confidentiality, integrity protection, and high availability, abbreviated to
CIA, are essential properties for trustworthy data systems. The rise of cloud
computing and the growing demand for multiparty applications however means that
building modern CIA systems is more challenging than ever. In response, we
present the Confidential Consortium Framework (CCF), a general-purpose
foundation for developing secure stateful CIA applications. CCF combines
centralized compute with decentralized trust, supporting deployment on
untrusted cloud infrastructure and transparent governance by mutually untrusted
parties. CCF leverages hardware-based trusted execution environments for
remotely verifiable confidentiality and code integrity. This is coupled with
state machine replication backed by an auditable immutable ledger for data
integrity and high availability. CCF enables each service to bring its own
application logic, custom multiparty governance model, and deployment scenario,
decoupling the operators of nodes from the consortium that governs them. CCF is
open-source and available now at https://github.com/microsoft/CCF.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11562" title="Abstract">arXiv:2310.11562</a> [<a href="/pdf/2310.11562" title="Download PDF">pdf</a>, <a href="/format/2310.11562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RekomGNN: Visualizing, Contextualizing and Evaluating Graph Neural  Networks Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brumar%2C+C+D">Camelia D. Brumar</a>, 
<a href="/search/cs?searchtype=author&query=Appleby%2C+G">Gabriel Appleby</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+J">Jen Rogers</a>, 
<a href="/search/cs?searchtype=author&query=Matinde%2C+T">Teddy Matinde</a>, 
<a href="/search/cs?searchtype=author&query=Thompson%2C+L">Lara Thompson</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+R">Remco Chang</a>, 
<a href="/search/cs?searchtype=author&query=Crisan%2C+A">Anamaria Crisan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Content recommendation tasks increasingly use Graph Neural Networks, but it
remains challenging for machine learning experts to assess the quality of their
outputs. Visualization systems for GNNs that could support this interrogation
are few. Moreover, those that do exist focus primarily on exposing GNN
architectures for tuning and prediction tasks and do not address the challenges
of recommendation tasks. We developed RekomGNN, a visual analytics system that
supports ML experts in exploring GNN recommendations across several dimensions
and making annotations about their quality. RekomGNN straddles the design space
between Neural Network and recommender system visualization to arrive at a set
of encoding and interaction choices for recommendation tasks. We found that
RekomGNN helps experts make qualitative assessments of the GNN's results, which
they can use for model refinement. Overall, our contributions and findings add
to the growing understanding of visualizing GNNs for increasingly complex
tasks.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11564" title="Abstract">arXiv:2310.11564</a> [<a href="/pdf/2310.11564" title="Download PDF">pdf</a>, <a href="/format/2310.11564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Soups: Personalized Large Language Model Alignment via  Post-hoc Parameter Merging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Joel Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungone Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B+Y">Bill Yuchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Ammanabrolu%2C+P">Prithviraj Ammanabrolu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language
Models (LLMs) with general, aggregate human preferences, it is suboptimal for
learning diverse, individual perspectives. In this work, we study Reinforcement
Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are
aligned to multiple (sometimes conflicting) preferences by modeling alignment
as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong
single-objective baselines, we show that we can achieve personalized alignment
by decomposing preferences into multiple dimensions. These dimensions are
defined based on personalizations that are declared as desirable by the user.
In this work, we show that they can be efficiently trained independently in a
distributed manner and combined effectively post-hoc through parameter merging.
The code is available at https://github.com/joeljang/RLPHF.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11566" title="Abstract">arXiv:2310.11566</a> [<a href="/pdf/2310.11566" title="Download PDF">pdf</a>, <a href="/format/2310.11566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partially Observable Stochastic Games with Neural Perception Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+G">Gabriel Santos</a>, 
<a href="/search/cs?searchtype=author&query=Norman%2C+G">Gethin Norman</a>, 
<a href="/search/cs?searchtype=author&query=Parker%2C+D">David Parker</a>, 
<a href="/search/cs?searchtype=author&query=Kwiatkowska%2C+M">Marta Kwiatkowska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Stochastic games are a well established model for multi-agent sequential
decision making under uncertainty. In reality, though, agents have only partial
observability of their environment, which makes the problem computationally
challenging, even in the single-agent setting of partially observable Markov
decision processes. Furthermore, in practice, agents increasingly perceive
their environment using data-driven approaches such as neural networks trained
on continuous data. To tackle this problem, we propose the model of
neuro-symbolic partially-observable stochastic games (NS-POSGs), a variant of
continuous-space concurrent stochastic games that explicitly incorporates
perception mechanisms. We focus on a one-sided setting, comprising a
partially-informed agent with discrete, data-driven observations and a
fully-informed agent with continuous observations. We present a new point-based
method, called one-sided NS-HSVI, for approximating values of one-sided
NS-POSGs and implement it based on the popular particle-based beliefs, showing
that it has closed forms for computing values of interest. We provide
experimental results to demonstrate the practical applicability of our method
for neural networks whose preimage is in polyhedral form.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11569" title="Abstract">arXiv:2310.11569</a> [<a href="/pdf/2310.11569" title="Download PDF">pdf</a>, <a href="/format/2310.11569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Rigidity Hurts: Soft Consistency Regularization for Probabilistic  Hierarchical Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamarthi%2C+H">Harshavardhan Kamarthi</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingkai Kong</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez%2C+A">Alexander Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+B+A">B. Aditya Prakash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at KDD 2023, 16 pages, 4 figures Updated Conference version of <a href="/abs/2206.07940">2206.07940</a> with added baselines. arXiv admin note: text overlap with <a href="/abs/2206.07940">arXiv:2206.07940</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Probabilistic hierarchical time-series forecasting is an important variant of
time-series forecasting, where the goal is to model and forecast multivariate
time-series that have underlying hierarchical relations. Most methods focus on
point predictions and do not provide well-calibrated probabilistic forecasts
distributions. Recent state-of-art probabilistic forecasting methods also
impose hierarchical relations on point predictions and samples of distribution
which does not account for coherency of forecast distributions. Previous works
also silently assume that datasets are always consistent with given
hierarchical relations and do not adapt to real-world datasets that show
deviation from this assumption. We close both these gap and propose PROFHiT,
which is a fully probabilistic hierarchical forecasting model that jointly
models forecast distribution of entire hierarchy. PROFHiT uses a flexible
probabilistic Bayesian approach and introduces a novel Distributional Coherency
regularization to learn from hierarchical relations for entire forecast
distribution that enables robust and calibrated forecasts as well as adapt to
datasets of varying hierarchical consistency. On evaluating PROFHiT over wide
range of datasets, we observed 41-88% better performance in accuracy and
significantly better calibration. Due to modeling the coherency over full
distribution, we observed that PROFHiT can robustly provide reliable forecasts
even if up to 10% of input time-series data is missing where other methods'
performance severely degrade by over 70%.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11571" title="Abstract">arXiv:2310.11571</a> [<a href="/pdf/2310.11571" title="Download PDF">pdf</a>, <a href="/format/2310.11571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What is a good question? Task-oriented asking with fact-level masking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toles%2C+M">Matthew Toles</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yukun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gravano%2C+L">Luis Gravano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Asking questions is an important element of real-life collaboration on
reasoning tasks like question answering. For example, a legal assistant chatbot
may be unable to make accurate recommendations without specific information on
the user's circumstances. However, large language models are usually deployed
to solve reasoning tasks directly without asking follow-up questions to the
user or third parties. We term this problem task-oriented asking (TOA).
Zero-shot chat models can perform TOA, but their training is primarily based on
next-token prediction rather than whether questions contribute to successful
collaboration. To enable the training and evaluation of TOA models, we present
a definition and framework for natural language task-oriented asking, the
problem of generating questions that result in answers useful for a reasoning
task. We also present fact-level masking (FLM), a procedure for converting
natural language datasets into self-supervised TOA datasets by omitting
particular critical facts. Finally, we generate a TOA dataset from the HotpotQA
dataset using FLM and evaluate several zero-shot language models on it. Our
experiments show that current zero-shot models struggle to ask questions that
retrieve useful information, as compared to human annotators. These results
demonstrate an opportunity to use FLM datasets and the TOA framework to train
and evaluate better TOA models.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11573" title="Abstract">arXiv:2310.11573</a> [<a href="/pdf/2310.11573" title="Download PDF">pdf</a>, <a href="/ps/2310.11573" title="Download PostScript">ps</a>, <a href="/format/2310.11573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A polynomial bound on the number of minimal separators and potential  maximal cliques in $P_6$-free graphs of bounded clique number
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Marcin Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Rz%C4%85%C5%BCewski%2C+P">Pawe&#x142; Rz&#x105;&#x17c;ewski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">In this note we show a polynomial bound on the number of minimal separators
and potential maximal cliques in $P_6$-free graphs of bounded clique number.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11575" title="Abstract">arXiv:2310.11575</a> [<a href="/pdf/2310.11575" title="Download PDF">pdf</a>, <a href="/ps/2310.11575" title="Download PostScript">ps</a>, <a href="/format/2310.11575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simpler Reductions from Exact Triangle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+M">Timothy M. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yinzhan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in SOSA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In this paper, we provide simpler reductions from Exact Triangle to two
important problems in fine-grained complexity: Exact Triangle with Few
Zero-Weight $4$-Cycles and All-Edges Sparse Triangle.
<br />Exact Triangle instances with few zero-weight $4$-cycles was considered by
Jin and Xu [STOC 2023], who used it as an intermediate problem to show $3$SUM
hardness of All-Edges Sparse Triangle with few $4$-cycles (independently
obtained by Abboud, Bringmann and Fischer [STOC 2023]), which is further used
to show $3$SUM hardness of a variety of problems, including $4$-Cycle
Enumeration, Offline Approximate Distance Oracle, Dynamic Approximate Shortest
Paths and All-Nodes Shortest Cycles. We provide a simple reduction from Exact
Triangle to Exact Triangle with few zero-weight $4$-cycles. Our new reduction
not only simplifies Jin and Xu's previous reduction, but also strengthens the
conditional lower bounds from being under the $3$SUM hypothesis to the even
more believable Exact Triangle hypothesis. As a result, all conditional lower
bounds shown by Jin and Xu [STOC 2023] and by Abboud, Bringmann and Fischer
[STOC 2023] using All-Edges Sparse Triangle with few $4$-cycles as an
intermediate problem now also hold under the Exact Triangle hypothesis.
<br />We also provide two alternative proofs of the conditional lower bound of the
All-Edges Sparse Triangle problem under the Exact Triangle hypothesis, which
was originally proved by Vassilevska Williams and Xu [FOCS 2020]. Both of our
new reductions are simpler, and one of them is also deterministic -- all
previous reductions from Exact Triangle or 3SUM to All-Edges Sparse Triangle
(including P\u{a}tra\c{s}cu's seminal work [STOC 2010]) were randomized.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11584" title="Abstract">arXiv:2310.11584</a> [<a href="/pdf/2310.11584" title="Download PDF">pdf</a>, <a href="/format/2310.11584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment  in Central Philippine Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imperial%2C+J+M">Joseph Marvin Imperial</a>, 
<a href="/search/cs?searchtype=author&query=Kochmar%2C+E">Ekaterina Kochmar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final camera-ready paper for EMNLP 2023 (Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Current research on automatic readability assessment (ARA) has focused on
improving the performance of models in high-resource languages such as English.
In this work, we introduce and release BasahaCorpus as part of an initiative
aimed at expanding available corpora and baseline models for readability
assessment in lower resource languages in the Philippines. We compiled a corpus
of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and
Rinconada -- languages belonging to the Central Philippine family tree subgroup
-- to train ARA models using surface-level, syllable-pattern, and n-gram
overlap features. We also propose a new hierarchical cross-lingual modeling
approach that takes advantage of a language's placement in the family tree to
increase the amount of available training data. Our study yields encouraging
results that support previous work showcasing the efficacy of cross-lingual
models in low-resource settings, as well as similarities in highly informative
linguistic features for mutually intelligible languages.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11589" title="Abstract">arXiv:2310.11589</a> [<a href="/pdf/2310.11589" title="Download PDF">pdf</a>, <a href="/format/2310.11589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eliciting Human Preferences with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B+Z">Belinda Z. Li</a>, 
<a href="/search/cs?searchtype=author&query=Tamkin%2C+A">Alex Tamkin</a>, 
<a href="/search/cs?searchtype=author&query=Goodman%2C+N">Noah Goodman</a>, 
<a href="/search/cs?searchtype=author&query=Andreas%2C+J">Jacob Andreas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Language models (LMs) can be directed to perform target tasks by using
labeled examples or natural language prompts. But selecting examples or writing
prompts for can be challenging--especially in tasks that involve unusual edge
cases, demand precise articulation of nebulous preferences, or require an
accurate mental model of LM behavior. We propose to use *LMs themselves* to
guide the task specification process. In this paper, we introduce **Generative
Active Task Elicitation (GATE)**: a learning framework in which models elicit
and infer intended behavior through free-form, language-based interaction with
users. We study GATE in three domains: email validation, content
recommendation, and moral reasoning. In preregistered experiments, we show that
LMs prompted to perform GATE (e.g., by generating open-ended questions or
synthesizing informative edge cases) elicit responses that are often more
informative than user-written prompts or labels. Users report that interactive
task elicitation requires less effort than prompting or example labeling and
surfaces novel considerations not initially anticipated by users. Our findings
suggest that LM-driven elicitation can be a powerful tool for aligning models
to complex human preferences and values.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11590" title="Abstract">arXiv:2310.11590</a> [<a href="/pdf/2310.11590" title="Download PDF">pdf</a>, <a href="/format/2310.11590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Inferring Users&#x27; Impressions of Robot Performance in Navigation  Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tsoi%2C+N">Nathan Tsoi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+B">Booyeon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jie Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+H+L">Hao-Tien Lewis Chiang</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%A1zquez%2C+M">Marynel V&#xe1;zquez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Human impressions of robot performance are often measured through surveys. As
a more scalable and cost-effective alternative, we study the possibility of
predicting people's impressions of robot behavior using non-verbal behavioral
cues and machine learning techniques. To this end, we first contribute the SEAN
TOGETHER Dataset consisting of observations of an interaction between a person
and a mobile robot in a Virtual Reality simulation, together with impressions
of robot performance provided by users on a 5-point scale. Second, we
contribute analyses of how well humans and supervised learning techniques can
predict perceived robot performance based on different combinations of
observation types (e.g., facial, spatial, and map features). Our results show
that facial expressions alone provide useful information about human
impressions of robot performance; but in the navigation scenarios we tested,
spatial features are the most critical piece of information for this inference
task. Also, when evaluating results as binary classification (rather than
multiclass classification), the F1-Score of human predictions and machine
learning models more than doubles, showing that both are better at telling the
directionality of robot performance than predicting exact performance ratings.
Based on our findings, we provide guidelines for implementing these predictions
models in real-world navigation scenarios.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11593" title="Abstract">arXiv:2310.11593</a> [<a href="/pdf/2310.11593" title="Download PDF">pdf</a>, <a href="/format/2310.11593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Evaluation of Personalized Text Generation using Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiepu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Personalized text generation presents a specialized mechanism for delivering
content that is specific to a user's personal context. While the research
progress in this area has been rapid, evaluation still presents a challenge.
Traditional automated metrics such as BLEU and ROUGE primarily measure lexical
similarity to human-written references, and are not able to distinguish
personalization from other subtle semantic aspects, thus falling short of
capturing the nuances of personalized generated content quality. On the other
hand, human judgments are costly to obtain, especially in the realm of
personalized evaluation. Inspired by these challenges, we explore the use of
large language models (LLMs) for evaluating personalized text generation, and
examine their ability to understand nuanced user context. We present AuPEL, a
novel evaluation method that distills three major semantic aspects of the
generated text: personalization, quality and relevance, and automatically
measures these aspects. To validate the effectiveness of AuPEL, we design
carefully controlled experiments and compare the accuracy of the evaluation
judgments made by LLMs versus that of judgements made by human annotators, and
conduct rigorous analyses of the consistency and sensitivity of the proposed
metric. We find that, compared to existing evaluation metrics, AuPEL not only
distinguishes and ranks models based on their personalization abilities more
accurately, but also presents commendable consistency and efficiency for this
task. Our work suggests that using LLMs as the evaluators of personalized text
generation is superior to traditional text similarity metrics, even though
interesting new challenges still remain.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11594" title="Abstract">arXiv:2310.11594</a> [<a href="/pdf/2310.11594" title="Download PDF">pdf</a>, <a href="/format/2310.11594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Robustness Unhardening via Backdoor Attacks in Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taejin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiarui Li</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shubhranshu Singh</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+N">Nikhil Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Joe-Wong%2C+C">Carlee Joe-Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 main pages of text, 4 figures, 2 tables. Made for a Neurips workshop on backdoor attacks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In today's data-driven landscape, the delicate equilibrium between
safeguarding user privacy and unleashing data potential stands as a paramount
concern. Federated learning, which enables collaborative model training without
necessitating data sharing, has emerged as a privacy-centric solution. This
decentralized approach brings forth security challenges, notably poisoning and
backdoor attacks where malicious entities inject corrupted data. Our research,
initially spurred by test-time evasion attacks, investigates the intersection
of adversarial training and backdoor attacks within federated learning,
introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a
subset of adversaries to intentionally undermine model robustness during
decentralized training, rendering models susceptible to a broader range of
evasion attacks. We present extensive empirical experiments evaluating ARU's
impact on adversarial training and existing robust aggregation defenses against
poisoning and backdoor attacks. Our findings inform strategies for enhancing
ARU to counter current defensive measures and highlight the limitations of
existing defenses, offering insights into bolstering defenses against ARU.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11595" title="Abstract">arXiv:2310.11595</a> [<a href="/pdf/2310.11595" title="Download PDF">pdf</a>, <a href="/format/2310.11595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks  Against Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Z">Zhihao Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhiwei Ling</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xian Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingsong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Due to the popularity of Artificial Intelligence (AI) technology, numerous
backdoor attacks are designed by adversaries to mislead deep neural network
predictions by manipulating training samples and training processes. Although
backdoor attacks are effective in various real scenarios, they still suffer
from the problems of both low fidelity of poisoned samples and non-negligible
transfer in latent space, which make them easily detectable by existing
backdoor detection algorithms. To overcome the weakness, this paper proposes a
novel frequency-based backdoor attack method named WaveAttack, which obtains
image high-frequency features through Discrete Wavelet Transform (DWT) to
generate backdoor triggers. Furthermore, we introduce an asymmetric frequency
obfuscation method, which can add an adaptive residual in the training and
inference stage to improve the impact of triggers and further enhance the
effectiveness of WaveAttack. Comprehensive experimental results show that
WaveAttack not only achieves higher stealthiness and effectiveness, but also
outperforms state-of-the-art (SOTA) backdoor attack methods in the fidelity of
images by up to 28.27\% improvement in PSNR, 1.61\% improvement in SSIM, and
70.59\% reduction in IS. Our code is available at
https://anonymous.4open.science/r/AnonymousRep-701D.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11597" title="Abstract">arXiv:2310.11597</a> [<a href="/pdf/2310.11597" title="Download PDF">pdf</a>, <a href="/format/2310.11597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Efficacy of Transformer-based Adversarial Attacks in Security  Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kunyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Domico%2C+K">Kyle Domico</a>, 
<a href="/search/cs?searchtype=author&query=Ferrand%2C+J+N">Jean-Charles Noirot Ferrand</a>, 
<a href="/search/cs?searchtype=author&query=McDaniel%2C+P">Patrick McDaniel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Military Communications Conference (MILCOM), AI for Cyber Workshop, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Today, the security of many domains rely on the use of Machine Learning to
detect threats, identify vulnerabilities, and safeguard systems from attacks.
Recently, transformer architectures have improved the state-of-the-art
performance on a wide range of tasks such as malware detection and network
intrusion detection. But, before abandoning current approaches to transformers,
it is crucial to understand their properties and implications on cybersecurity
applications. In this paper, we evaluate the robustness of transformers to
adversarial samples for system defenders (i.e., resiliency to adversarial
perturbations generated on different types of architectures) and their
adversarial strength for system attackers (i.e., transferability of adversarial
samples generated by transformers to other target models). To that effect, we
first fine-tune a set of pre-trained transformer, Convolutional Neural Network
(CNN), and hybrid (an ensemble of transformer and CNN) models to solve
different downstream image-based tasks. Then, we use an attack algorithm to
craft 19,367 adversarial examples on each model for each task. The
transferability of these adversarial examples is measured by evaluating each
set on other models to determine which models offer more adversarial strength,
and consequently, more robustness against these attacks. We find that the
adversarial examples crafted on transformers offer the highest transferability
rate (i.e., 25.7% higher than the average) onto other models. Similarly,
adversarial examples crafted on other models have the lowest rate of
transferability (i.e., 56.7% lower than the average) onto transformers. Our
work emphasizes the importance of studying transformer architectures for
attacking and defending models in security domains, and suggests using them as
the primary architecture in transfer attack settings.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11598" title="Abstract">arXiv:2310.11598</a> [<a href="/pdf/2310.11598" title="Download PDF">pdf</a>, <a href="/format/2310.11598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Neural Implicit through Volume Rendering with Attentive Depth  Fusion Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+P">Pengchong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhizhong Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Learning neural implicit representations has achieved remarkable performance
in 3D reconstruction from multi-view images. Current methods use volume
rendering to render implicit representations into either RGB or depth images
that are supervised by multi-view ground truth. However, rendering a view each
time suffers from incomplete depth at holes and unawareness of occluded
structures from the depth supervision, which severely affects the accuracy of
geometry inference via volume rendering. To resolve this issue, we propose to
learn neural implicit representations from multi-view RGBD images through
volume rendering with an attentive depth fusion prior. Our prior allows neural
networks to perceive coarse 3D structures from the Truncated Signed Distance
Function (TSDF) fused from all depth images available for rendering. The TSDF
enables accessing the missing depth at holes on one depth image and the
occluded parts that are invisible from the current view. By introducing a novel
attention mechanism, we allow neural networks to directly use the depth fusion
prior with the inferred occupancy as the learned implicit function. Our
attention mechanism works with either a one-time fused TSDF that represents a
whole scene or an incrementally fused TSDF that represents a partial scene in
the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on
widely used benchmarks including synthetic and real-world scans show our
superiority over the latest neural implicit methods. Project page:
https://machineperceptionlab.github.io/Attentive_DF_Prior/
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11599" title="Abstract">arXiv:2310.11599</a> [<a href="/pdf/2310.11599" title="Download PDF">pdf</a>, <a href="/format/2310.11599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Trajectory Optimization of Simple Skateboarding Tricks through  Contact
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burgess%2C+M">Michael Burgess</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Trajectories are optimized for a two-dimensional simplified skateboarding
system to allow it to perform a fundamental skateboarding trick called an
"ollie". A methodology for generating trick trajectories by controlling the
position of a point-mass relative to a board is presented and demonstrated over
a range of peak jump heights. A hybrid dynamics approach is taken to perform
this optimization, with contact constraints applied along a sequence of
discrete timesteps based on the board's position throughout designated sections
of the trick. These constraints introduce explicit and implicit discontinuities
between chosen sections of the trick sequence. The approach has been shown to
be successful for a set of realistic system parameters.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11602" title="Abstract">arXiv:2310.11602</a> [<a href="/pdf/2310.11602" title="Download PDF">pdf</a>, <a href="/format/2310.11602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FreSh: A Lock-Free Data Series Index
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fatourou%2C+P">Panagiota Fatourou</a>, 
<a href="/search/cs?searchtype=author&query=Kosmas%2C+E">Eleftherios Kosmas</a>, 
<a href="/search/cs?searchtype=author&query=Palpanas%2C+T">Themis Palpanas</a>, 
<a href="/search/cs?searchtype=author&query=Paterakis%2C+G">George Paterakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 18 figures, Conference: Symposium on Reliable Distributed Systems (SRDS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We present FreSh, a lock-free data series index that exhibits good
performance (while being robust). FreSh is based on Refresh, which is a generic
approach we have developed for supporting lock-freedom in an efficient way on
top of any localityaware data series index. We believe Refresh is of
independent interest and can be used to get well-performed lock-free versions
of other locality-aware blocking data structures. For developing FreSh, we
first studied in depth the design decisions of current state-of-the-art data
series indexes, and the principles governing their performance. This led to a
theoretical framework, which enables the development and analysis of data
series indexes in a modular way. The framework allowed us to apply Refresh,
repeatedly, to get lock-free versions of the different phases of a family of
data series indexes. Experiments with several synthetic and real datasets
illustrate that FreSh achieves performance that is as good as that of the
state-of-the-art blocking in-memory data series index. This shows that the
helping mechanisms of FreSh are light-weight, respecting certain principles
that are crucial for performance in locality-aware data structures.This paper
was published in SRDS 2023.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11604" title="Abstract">arXiv:2310.11604</a> [<a href="/pdf/2310.11604" title="Download PDF">pdf</a>, <a href="/format/2310.11604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models as Zero-Shot Trajectory Generators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Teyun Kwon</a> (1), 
<a href="/search/cs?searchtype=author&query=Di+Palo%2C+N">Norman Di Palo</a> (1), 
<a href="/search/cs?searchtype=author&query=Johns%2C+E">Edward Johns</a> (1) ((1) Imperial College London)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have recently shown promise as high-level
planners for robots when given access to a selection of low-level skills.
However, it is often assumed that LLMs do not possess sufficient knowledge to
be used for the low-level trajectories themselves. In this work, we address
this assumption thoroughly, and investigate if an LLM (GPT-4) can directly
predict a dense sequence of end-effector poses for manipulation skills, when
given access to only object detection and segmentation vision models. We study
how well a single task-agnostic prompt, without any in-context examples, motion
primitives, or external trajectory optimisers, can perform across 26 real-world
language-based tasks, such as "open the bottle cap" and "wipe the plate with
the sponge", and we investigate which design choices in this prompt are the
most effective. Our conclusions raise the assumed limit of LLMs for robotics,
and we reveal for the first time that LLMs do indeed possess an understanding
of low-level robot control sufficient for a range of common tasks, and that
they can additionally detect failures and then re-plan trajectories
accordingly. Videos, code, and prompts are available at:
https://www.robot-learning.uk/language-models-trajectory-generators.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11605" title="Abstract">arXiv:2310.11605</a> [<a href="/pdf/2310.11605" title="Download PDF">pdf</a>, <a href="/format/2310.11605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIAR: Deep Image Alignment and Reconstruction using Swin Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwiatkowski%2C+M">Monika Kwiatkowski</a>, 
<a href="/search/cs?searchtype=author&query=Matern%2C+S">Simon Matern</a>, 
<a href="/search/cs?searchtype=author&query=Hellwich%2C+O">Olaf Hellwich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">When taking images of some occluded content, one is often faced with the
problem that every individual image frame contains unwanted artifacts, but a
collection of images contains all relevant information if properly aligned and
aggregated. In this paper, we attempt to build a deep learning pipeline that
simultaneously aligns a sequence of distorted images and reconstructs them. We
create a dataset that contains images with image distortions, such as lighting,
specularities, shadows, and occlusion. We create perspective distortions with
corresponding ground-truth homographies as labels. We use our dataset to train
Swin transformer models to analyze sequential image data. The attention maps
enable the model to detect relevant image content and differentiate it from
outliers and artifacts. We further explore using neural feature maps as
alternatives to classical key point detectors. The feature maps of trained
convolutional layers provide dense image descriptors that can be used to find
point correspondences between images. We utilize this to compute coarse image
alignments and explore its limitations.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11607" title="Abstract">arXiv:2310.11607</a> [<a href="/pdf/2310.11607" title="Download PDF">pdf</a>, <a href="/format/2310.11607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for  Semi-Supervised Intent Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Botzer%2C+N">Nicholas Botzer</a>, 
<a href="/search/cs?searchtype=author&query=Vasquez%2C+D">David Vasquez</a>, 
<a href="/search/cs?searchtype=author&query=Weninger%2C+T">Tim Weninger</a>, 
<a href="/search/cs?searchtype=author&query=Laradji%2C+I">Issam Laradji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The ability to detect intent in dialogue systems has become increasingly
important in modern technology. These systems often generate a large amount of
unlabeled data, and manually labeling this data requires substantial human
effort. Semi-supervised methods attempt to remedy this cost by using a model
trained on a few labeled examples and then by assigning pseudo-labels to
further a subset of unlabeled examples that has a model prediction confidence
higher than a certain threshold. However, one particularly perilous consequence
of these methods is the risk of picking an imbalanced set of examples across
classes, which could lead to poor labels. In the present work, we describe
Top-K K-Nearest Neighbor (TK-KNN), which uses a more robust pseudo-labeling
approach based on distance in the embedding space while maintaining a balanced
set of pseudo-labeled examples across classes through a ranking-based approach.
Experiments on several datasets show that TK-KNN outperforms existing models,
particularly when labeled data is scarce on popular datasets such as CLINC150
and Banking77. Code is available at https://github.com/ServiceNow/tk-knn
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11608" title="Abstract">arXiv:2310.11608</a> [<a href="/pdf/2310.11608" title="Download PDF">pdf</a>, <a href="/format/2310.11608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification of Safety Driver Attention During Autonomous Vehicle  Operation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konrad%2C+S+G">Santiago Gerling Konrad</a>, 
<a href="/search/cs?searchtype=author&query=Berrio%2C+J+S">Julie Stephany Berrio</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+M">Mao Shan</a>, 
<a href="/search/cs?searchtype=author&query=Masson%2C+F">Favio Masson</a>, 
<a href="/search/cs?searchtype=author&query=Worrall%2C+S">Stewart Worrall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Despite the continual advances in Advanced Driver Assistance Systems (ADAS)
and the development of high-level autonomous vehicles (AV), there is a general
consensus that for the short to medium term, there is a requirement for a human
supervisor to handle the edge cases that inevitably arise. Given this
requirement, it is essential that the state of the vehicle operator is
monitored to ensure they are contributing to the vehicle's safe operation. This
paper introduces a dual-source approach integrating data from an infrared
camera facing the vehicle operator and vehicle perception systems to produce a
metric for driver alertness in order to promote and ensure safe operator
behaviour. The infrared camera detects the driver's head, enabling the
calculation of head orientation, which is relevant as the head typically moves
according to the individual's focus of attention. By incorporating
environmental data from the perception system, it becomes possible to determine
whether the vehicle operator observes objects in the surroundings. Experiments
were conducted using data collected in Sydney, Australia, simulating AV
operations in an urban environment. Our results demonstrate that the proposed
system effectively determines a metric for the attention levels of the vehicle
operator, enabling interventions such as warnings or reducing autonomous
functionality as appropriate. This comprehensive solution shows promise in
contributing to ADAS and AVs' overall safety and efficiency in a real-world
setting.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11609" title="Abstract">arXiv:2310.11609</a> [<a href="/pdf/2310.11609" title="Download PDF">pdf</a>, <a href="/format/2310.11609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reflection-Equivariant Diffusion for 3D Structure Determination from  Isotopologue Rotational Spectra in Natural Abundance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+A">Austin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+A">Alston Lo</a>, 
<a href="/search/cs?searchtype=author&query=Miret%2C+S">Santiago Miret</a>, 
<a href="/search/cs?searchtype=author&query=Pate%2C+B">Brooks Pate</a>, 
<a href="/search/cs?searchtype=author&query=Aspuru-Guzik%2C+A">Al&#xe1;n Aspuru-Guzik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Astrophysics of Galaxies (astro-ph.GA); Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Structure determination is necessary to identify unknown organic molecules,
such as those in natural products, forensic samples, the interstellar medium,
and laboratory syntheses. Rotational spectroscopy enables structure
determination by providing accurate 3D information about small organic
molecules via their moments of inertia. Using these moments, Kraitchman
analysis determines isotopic substitution coordinates, which are the unsigned
$|x|,|y|,|z|$ coordinates of all atoms with natural isotopic abundance,
including carbon, nitrogen, and oxygen. While unsigned substitution coordinates
can verify guesses of structures, the missing $+/-$ signs make it challenging
to determine the actual structure from the substitution coordinates alone. To
tackle this inverse problem, we develop KREED (Kraitchman
REflection-Equivariant Diffusion), a generative diffusion model that infers a
molecule's complete 3D structure from its molecular formula, moments of
inertia, and unsigned substitution coordinates of heavy atoms. KREED's top-1
predictions identify the correct 3D structure with &gt;98% accuracy on the QM9 and
GEOM datasets when provided with substitution coordinates of all heavy atoms
with natural isotopic abundance. When substitution coordinates are restricted
to only a subset of carbons, accuracy is retained at 91% on QM9 and 32% on
GEOM. On a test set of experimentally measured substitution coordinates
gathered from the literature, KREED predicts the correct all-atom 3D structure
in 25 of 33 cases, demonstrating experimental applicability for context-free 3D
structure determination with rotational spectroscopy.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11611" title="Abstract">arXiv:2310.11611</a> [<a href="/pdf/2310.11611" title="Download PDF">pdf</a>, <a href="/format/2310.11611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In defense of parameter sharing for model-compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Desai%2C+A">Aditya Desai</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+A">Anshumali Shrivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">When considering a model architecture, there are several ways to reduce its
memory footprint. Historically, popular approaches included selecting smaller
architectures and creating sparse networks through pruning. More recently,
randomized parameter-sharing (RPS) methods have gained traction for model
compression at start of training. In this paper, we comprehensively assess the
trade-off between memory and accuracy across RPS, pruning techniques, and
building smaller models. Our findings demonstrate that RPS, which is both data
and model-agnostic, consistently outperforms/matches smaller models and all
moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP,
across the entire compression range. This advantage becomes particularly
pronounced in higher compression scenarios. Notably, even when compared to
highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS
exhibits superior performance in high compression settings. This points out
inherent capacity advantage that RPS enjoys over sparse models. Theoretically,
we establish RPS as a superior technique in terms of memory-efficient
representation when compared to pruning for linear models. This paper argues in
favor of paradigm shift towards RPS based models. During our rigorous
evaluation of RPS, we identified issues in the state-of-the-art RPS technique
ROAST, specifically regarding stability (ROAST's sensitivity to initialization
hyperparameters, often leading to divergence) and Pareto-continuity (ROAST's
inability to recover the accuracy of the original model at zero compression).
We provably address both of these issues. We refer to the modified RPS, which
incorporates our improvements, as STABLE-RPS.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11612" title="Abstract">arXiv:2310.11612</a> [<a href="/pdf/2310.11612" title="Download PDF">pdf</a>, <a href="/format/2310.11612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and  Gallery Banks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yimu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jian%2C+X">Xiangru Jian</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Bo Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we present a post-processing solution to address the hubness
problem in cross-modal retrieval, a phenomenon where a small number of gallery
data points are frequently retrieved, resulting in a decline in retrieval
performance. We first theoretically demonstrate the necessity of incorporating
both the gallery and query data for addressing hubness as hubs always exhibit
high similarity with gallery and query data. Second, building on our
theoretical results, we propose a novel framework, Dual Bank Normalization
(DBNorm). While previous work has attempted to alleviate hubness by only
utilizing the query samples, DBNorm leverages two banks constructed from the
query and gallery samples to reduce the occurrence of hubs during inference.
Next, to complement DBNorm, we introduce two novel methods, dual inverted
softmax and dual dynamic inverted softmax, for normalizing similarity based on
the two banks. Specifically, our proposed methods reduce the similarity between
hubs and queries while improving the similarity between non-hubs and queries.
Finally, we present extensive experimental results on diverse language-grounded
benchmarks, including text-image, text-video, and text-audio, demonstrating the
superior performance of our approaches compared to previous methods in
addressing hubness and boosting retrieval performance. Our code is available at
https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11614" title="Abstract">arXiv:2310.11614</a> [<a href="/pdf/2310.11614" title="Download PDF">pdf</a>, <a href="/format/2310.11614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning a Hierarchical Planner from Humans in Multiple Generations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cano%2C+L+H">Leonardo Hernandez Cano</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+Y">Yewen Pu</a>, 
<a href="/search/cs?searchtype=author&query=Hawkins%2C+R+D">Robert D. Hawkins</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J">Josh Tenenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Solar-Lezama%2C+A">Armando Solar-Lezama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">A typical way in which a machine acquires knowledge from humans is by
programming. Compared to learning from demonstrations or experiences,
programmatic learning allows the machine to acquire a novel skill as soon as
the program is written, and, by building a library of programs, a machine can
quickly learn how to perform complex tasks. However, as programs often take
their execution contexts for granted, they are brittle when the contexts
change, making it difficult to adapt complex programs to new contexts. We
present natural programming, a library learning system that combines
programmatic learning with a hierarchical planner. Natural programming
maintains a library of decompositions, consisting of a goal, a linguistic
description of how this goal decompose into sub-goals, and a concrete instance
of its decomposition into sub-goals. A user teaches the system via curriculum
building, by identifying a challenging yet not impossible goal along with
linguistic hints on how this goal may be decomposed into sub-goals. The system
solves for the goal via hierarchical planning, using the linguistic hints to
guide its probability distribution in proposing the right plans. The system
learns from this interaction by adding newly found decompositions in the
successful search into its library. Simulated studies and a human experiment
(n=360) on a controlled environment demonstrate that natural programming can
robustly compose programs learned from different users and contexts, adapting
faster and solving more complex tasks when compared to programmatic baselines.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11616" title="Abstract">arXiv:2310.11616</a> [<a href="/pdf/2310.11616" title="Download PDF">pdf</a>, <a href="/format/2310.11616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the General Intelligence Factor in Language Models: A  Psychometric Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ili%C4%87%2C+D">David Ili&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages (including appendix), 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study uncovers the factor of general intelligence, or g, in language
models, extending the psychometric theory traditionally applied to humans and
certain animal species. Utilizing factor analysis on two extensive datasets -
Open LLM Leaderboard with 1,232 models and General Language Understanding
Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for
a unidimensional, highly stable g factor that accounts for 85% of the variance
in model performance. The study also finds a moderate correlation of .48
between model size and g. The discovery of g in language models offers a
unified metric for model evaluation and opens new avenues for more robust,
g-based model ability assessment. These findings lay the foundation for
understanding and future research on artificial general intelligence from a
psychometric perspective and have practical implications for model evaluation
and development.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11622" title="Abstract">arXiv:2310.11622</a> [<a href="/pdf/2310.11622" title="Download PDF">pdf</a>, <a href="/format/2310.11622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Resolution Building and Road Detection from Sentinel-2
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sirko%2C+W">Wojciech Sirko</a>, 
<a href="/search/cs?searchtype=author&query=Brempong%2C+E+A">Emmanuel Asiedu Brempong</a>, 
<a href="/search/cs?searchtype=author&query=Marcos%2C+J+T+C">Juliana T. C. Marcos</a>, 
<a href="/search/cs?searchtype=author&query=Annkah%2C+A">Abigail Annkah</a>, 
<a href="/search/cs?searchtype=author&query=Korme%2C+A">Abel Korme</a>, 
<a href="/search/cs?searchtype=author&query=Hassen%2C+M+A">Mohammed Alewi Hassen</a>, 
<a href="/search/cs?searchtype=author&query=Sapkota%2C+K">Krishna Sapkota</a>, 
<a href="/search/cs?searchtype=author&query=Shekel%2C+T">Tomer Shekel</a>, 
<a href="/search/cs?searchtype=author&query=Diack%2C+A">Abdoulaye Diack</a>, 
<a href="/search/cs?searchtype=author&query=Nevo%2C+S">Sella Nevo</a>, 
<a href="/search/cs?searchtype=author&query=Hickey%2C+J">Jason Hickey</a>, 
<a href="/search/cs?searchtype=author&query=Quinn%2C+J">John Quinn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Mapping buildings and roads automatically with remote sensing typically
requires high-resolution imagery, which is expensive to obtain and often
sparsely available. In this work we demonstrate how multiple 10 m resolution
Sentinel-2 images can be used to generate 50 cm resolution building and road
segmentation masks. This is done by training a `student' model with access to
Sentinel-2 images to reproduce the predictions of a `teacher' model which has
access to corresponding high-resolution imagery. While the predictions do not
have all the fine detail of the teacher model, we find that we are able to
retain much of the performance: for building segmentation we achieve 78.3%
mIoU, compared to the high-resolution teacher model accuracy of 85.3% mIoU. We
also describe a related method for counting individual buildings in a
Sentinel-2 patch which achieves R^2 = 0.91 against true counts. This work opens
up new possibilities for using freely available Sentinel-2 imagery for a range
of tasks that previously could only be done with high-resolution satellite
imagery.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11626" title="Abstract">arXiv:2310.11626</a> [<a href="/pdf/2310.11626" title="Download PDF">pdf</a>, <a href="/format/2310.11626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperNetX: A Python package for modeling complex network data as  hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Praggastis%2C+B">Brenda Praggastis</a>, 
<a href="/search/cs?searchtype=author&query=Aksoy%2C+S">Sinan Aksoy</a>, 
<a href="/search/cs?searchtype=author&query=Arendt%2C+D">Dustin Arendt</a>, 
<a href="/search/cs?searchtype=author&query=Bonicillo%2C+M">Mark Bonicillo</a>, 
<a href="/search/cs?searchtype=author&query=Joslyn%2C+C">Cliff Joslyn</a>, 
<a href="/search/cs?searchtype=author&query=Purvine%2C+E">Emilie Purvine</a>, 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+M">Madelyn Shapiro</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+J+Y">Ji Young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>

</div>
<p class="mathjax">HyperNetX (HNX) is an open source Python library for the analysis and
visualization of complex network data modeled as hypergraphs. Initially
released in 2019, HNX facilitates exploratory data analysis of complex networks
using algebraic topology, combinatorics, and generalized hypergraph and graph
theoretical methods on structured data inputs. With its 2023 release, the
library supports attaching metadata, numerical and categorical, to nodes
(vertices) and hyperedges, as well as to node-hyperedge pairings (incidences).
HNX has a customizable Matplotlib-based visualization module as well as
HypernetX-Widget, its JavaScript addon for interactive exploration and
visualization of hypergraphs within Jupyter Notebooks. Both packages are
available on GitHub and PyPI. With a growing community of users and
collaborators, HNX has become a preeminent tool for hypergraph analysis.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11628" title="Abstract">arXiv:2310.11628</a> [<a href="/pdf/2310.11628" title="Download PDF">pdf</a>, <a href="/format/2310.11628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learn Your Tokens: Word-Pooled Tokenization for Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thawani%2C+A">Avijit Thawani</a>, 
<a href="/search/cs?searchtype=author&query=Ghanekar%2C+S">Saurabh Ghanekar</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaoyuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language models typically tokenize text into subwords, using a deterministic,
hand-engineered heuristic of combining characters into longer surface-level
strings such as 'ing' or whole words. Recent literature has repeatedly shown
the limitations of such a tokenization strategy, particularly for documents not
written in English and for representing numbers. On the other extreme,
byte/character-level language models are much less restricted but suffer from
increased sequence description lengths and a subsequent quadratic expansion in
self-attention computation. Recent attempts to compress and limit these context
lengths with fixed size convolutions is helpful but completely ignores the word
boundary. This paper considers an alternative 'learn your tokens' scheme which
utilizes the word boundary to pool bytes/characters into word representations,
which are fed to the primary language model, before again decoding individual
characters/bytes per word in parallel. We find that our moderately expressive
and moderately fast end-to-end tokenizer outperform by over 300% both subwords
and byte/character models over the intrinsic language modeling metric of
next-word prediction across datasets. It particularly outshines on rare words,
outperforming by a factor of 30! We extensively study the language modeling
setup for all three categories of tokenizers and theoretically analyze how our
end-to-end models can also be a strong trade-off in efficiency and robustness.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11629" title="Abstract">arXiv:2310.11629</a> [<a href="/pdf/2310.11629" title="Download PDF">pdf</a>, <a href="/format/2310.11629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Holistic Parking Slot Detection with Polygon-Shaped Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Musabini%2C+A">Antonyo Musabini</a>, 
<a href="/search/cs?searchtype=author&query=Leonet%2C+C">Christel Leonet</a>, 
<a href="/search/cs?searchtype=author&query=Benmokhtar%2C+R">Rachid Benmokhtar</a>, 
<a href="/search/cs?searchtype=author&query=Breheret%2C+A">Amaury Breheret</a>, 
<a href="/search/cs?searchtype=author&query=Yedes%2C+C">Chaima Yedes</a>, 
<a href="/search/cs?searchtype=author&query=Burger%2C+F">Fabian Burger</a>, 
<a href="/search/cs?searchtype=author&query=Boulay%2C+T">Thomas Boulay</a>, 
<a href="/search/cs?searchtype=author&query=Perrotton%2C+X">Xavier Perrotton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Current parking slot detection in advanced driver-assistance systems (ADAS)
primarily relies on ultrasonic sensors. This method has several limitations
such as the need to scan the entire parking slot before detecting it, the
incapacity of detecting multiple slots in a row, and the difficulty of
classifying them. Due to the complex visual environment, vehicles are equipped
with surround view camera systems to detect vacant parking slots. Previous
research works in this field mostly use image-domain models to solve the
problem. These two-stage approaches separate the 2D detection and 3D pose
estimation steps using camera calibration. In this paper, we propose one-step
Holistic Parking Slot Network (HPS-Net), a tailor-made adaptation of the You
Only Look Once (YOLO)v4 algorithm. This camera-based approach directly outputs
the four vertex coordinates of the parking slot in topview domain, instead of a
bounding box in raw camera images. Several visible points and shapes can be
proposed from different angles. A novel regression loss function named
polygon-corner Generalized Intersection over Union (GIoU) for polygon vertex
position optimization is also proposed to manage the slot orientation and to
distinguish the entrance line. Experiments show that HPS-Net can detect various
vacant parking slots with a F1-score of 0.92 on our internal Valeo Parking
Slots Dataset (VPSD) and 0.99 on the public dataset PS2.0. It provides a
satisfying generalization and robustness in various parking scenarios, such as
indoor (F1: 0.86) or paved ground (F1: 0.91). Moreover, it achieves a real-time
detection speed of 17 FPS on Nvidia Drive AGX Xavier. A demo video can be found
at https://streamable.com/75j7sj.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11634" title="Abstract">arXiv:2310.11634</a> [<a href="/pdf/2310.11634" title="Download PDF">pdf</a>, <a href="/format/2310.11634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language  Models to Generalize to Novel Interpretations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Arkil Patel</a>, 
<a href="/search/cs?searchtype=author&query=Bhattamishra%2C+S">Satwik Bhattamishra</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Siva Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Bahdanau%2C+D">Dzmitry Bahdanau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Humans possess a remarkable ability to assign novel interpretations to
linguistic expressions, enabling them to learn new words and understand
community-specific connotations. However, Large Language Models (LLMs) have a
knowledge cutoff and are costly to finetune repeatedly. Therefore, it is
crucial for LLMs to learn novel interpretations in-context. In this paper, we
systematically analyse the ability of LLMs to acquire novel interpretations
using in-context learning. To facilitate our study, we introduce MAGNIFICo, an
evaluation suite implemented within a text-to-SQL semantic parsing framework
that incorporates diverse tokens and prompt settings to simulate real-world
complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a
surprisingly robust capacity for comprehending novel interpretations from
natural language descriptions as well as from discussions within long
conversations. Nevertheless, our findings also highlight the need for further
improvements, particularly when interpreting unfamiliar words or when composing
multiple novel interpretations simultaneously in the same example.
Additionally, our analysis uncovers the semantic predispositions in LLMs and
reveals the impact of recency bias for information presented in long contexts.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11636" title="Abstract">arXiv:2310.11636</a> [<a href="/pdf/2310.11636" title="Download PDF">pdf</a>, <a href="/format/2310.11636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Symbolic Language for Interpreting Decision Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arenas%2C+M">Marcelo Arenas</a>, 
<a href="/search/cs?searchtype=author&query=Barcelo%2C+P">Pablo Barcelo</a>, 
<a href="/search/cs?searchtype=author&query=Bustamente%2C+D">Diego Bustamente</a>, 
<a href="/search/cs?searchtype=author&query=Caraball%2C+J">Jose Caraball</a>, 
<a href="/search/cs?searchtype=author&query=Subercaseaux%2C+B">Bernardo Subercaseaux</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The recent development of formal explainable AI has disputed the folklore
claim that "decision trees are readily interpretable models", showing different
interpretability queries that are computationally hard on decision trees, as
well as proposing different methods to deal with them in practice. Nonetheless,
no single explainability query or score works as a "silver bullet" that is
appropriate for every context and end-user. This naturally suggests the
possibility of "interpretability languages" in which a wide variety of queries
can be expressed, giving control to the end-user to tailor queries to their
particular needs. In this context, our work presents ExplainDT, a symbolic
language for interpreting decision trees. ExplainDT is rooted in a carefully
constructed fragment of first-ordered logic that we call StratiFOILed.
StratiFOILed balances expressiveness and complexity of evaluation, allowing for
the computation of many post-hoc explanations--both local (e.g., abductive and
contrastive explanations) and global ones (e.g., feature relevancy)--while
remaining in the Boolean Hierarchy over NP. Furthermore, StratiFOILed queries
can be written as a Boolean combination of NP-problems, thus allowing us to
evaluate them in practice with a constant number of calls to a SAT solver. On
the theoretical side, our main contribution is an in-depth analysis of the
expressiveness and complexity of StratiFOILed, while on the practical side, we
provide an optimized implementation for encoding StratiFOILed queries as
propositional formulas, together with an experimental study on its efficiency.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11638" title="Abstract">arXiv:2310.11638</a> [<a href="/pdf/2310.11638" title="Download PDF">pdf</a>, <a href="/format/2310.11638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Assessment of Factual Knowledge in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Linhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Thuy-Trang Vu</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2024 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Previous studies have relied on existing question-answering benchmarks to
evaluate the knowledge stored in large language models (LLMs). However, this
approach has limitations regarding factual knowledge coverage, as it mostly
focuses on generic domains which may overlap with the pretraining data. This
paper proposes a framework to systematically assess the factual knowledge of
LLMs by leveraging knowledge graphs (KGs). Our framework automatically
generates a set of questions and expected answers from the facts stored in a
given KG, and then evaluates the accuracy of LLMs in answering these questions.
We systematically evaluate the state-of-the-art LLMs with KGs in generic and
specific domains. The experiment shows that ChatGPT is consistently the top
performer across all domains. We also find that LLMs performance depends on the
instruction finetuning, domain and question complexity and is prone to
adversarial context.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11639" title="Abstract">arXiv:2310.11639</a> [<a href="/pdf/2310.11639" title="Download PDF">pdf</a>, <a href="/format/2310.11639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrossData: Leveraging Text-Data Connections for Authoring Data Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haijun Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Data documents play a central role in recording, presenting, and
disseminating data. Despite the proliferation of applications and systems
designed to support the analysis, visualization, and communication of data,
writing data documents remains a laborious process, requiring a constant
back-and-forth between data processing and writing tools. Interviews with eight
professionals revealed that their workflows contained numerous tedious,
repetitive, and error-prone operations. The key issue that we identified is the
lack of persistent connection between text and data. Thus, we developed
CrossData, a prototype that treats text-data connections as persistent,
interactive, first-class objects. By automatically identifying, establishing,
and leveraging text-data connections, CrossData enables rich interactions to
assist in the authoring of data documents. An expert evaluation with eight
users demonstrated the usefulness of CrossData, showing that it not only
reduced the manual effort in writing data documents but also opened new
possibilities to bridge the gap between data exploration and writing.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11640" title="Abstract">arXiv:2310.11640</a> [<a href="/pdf/2310.11640" title="Download PDF">pdf</a>, <a href="/format/2310.11640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Free-text Keystroke Authentication using Transformers: A Comparative  Study of Architectures and Loss Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Momeni%2C+S">Saleh Momeni</a>, 
<a href="/search/cs?searchtype=author&query=BabaAli%2C+B">Bagher BabaAli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Keystroke biometrics is a promising approach for user identification and
verification, leveraging the unique patterns in individuals' typing behavior.
In this paper, we propose a Transformer-based network that employs
self-attention to extract informative features from keystroke sequences,
surpassing the performance of traditional Recurrent Neural Networks. We explore
two distinct architectures, namely bi-encoder and cross-encoder, and compare
their effectiveness in keystroke authentication. Furthermore, we investigate
different loss functions, including triplet, batch-all triplet, and WDCL loss,
along with various distance metrics such as Euclidean, Manhattan, and cosine
distances. These experiments allow us to optimize the training process and
enhance the performance of our model. To evaluate our proposed model, we employ
the Aalto desktop keystroke dataset. The results demonstrate that the
bi-encoder architecture with batch-all triplet loss and cosine distance
achieves the best performance, yielding an exceptional Equal Error Rate of
0.0186%. Furthermore, alternative algorithms for calculating similarity scores
are explored to enhance accuracy. Notably, the utilization of a one-class
Support Vector Machine reduces the Equal Error Rate to an impressive 0.0163%.
The outcomes of this study indicate that our model surpasses the previous
state-of-the-art in free-text keystroke authentication. These findings
contribute to advancing the field of keystroke authentication and offer
practical implications for secure user verification systems.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11643" title="Abstract">arXiv:2310.11643</a> [<a href="/pdf/2310.11643" title="Download PDF">pdf</a>, <a href="/format/2310.11643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Opinion Change or Differential Turnout: Changing Opinions on the Austin  Police Department in a Budget Feedback Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gelauff%2C+L+L">Lodewijk L. Gelauff</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+A">Ashish Goel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This preprint is an extended version of a previously published conference paper: <a href="https://dl.acm.org/doi/10.1145/3551624.3555295">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In 2020 the tragic murder of George Floyd at the hands of law enforcement
ignited and intensified nationwide protests, demanding changes in police
funding and allocation. This happened during a budgeting feedback exercise
where residents of Austin, Texas were invited to share opinions on the budgets
of various city service areas, including the Police Department, on an online
platform designed by our team. Daily responses increased by a hundredfold and
responses registered after the "exogenous shock" overwhelmingly advocated for
reducing police funding. This opinion shift far exceeded what we observed in 14
other Participatory Budgeting elections on our Participatory Budgeting
Platform, and can't be explained by shifts in the respondent demographics.
Analysis of the results from an Austin budgetary feedback exercise in 2021 and
a follow-up survey indicates that the opinion shift from 2020 persisted, with
the opinion gap on police funding widening. We conclude that there was an
actual change of opinion regarding police funding. This study not only sheds
light on the enduring impact of the 2020 events and protests on public opinion,
but also showcases the value of analysis of clustered opinions as a tool in the
evaluation toolkit of survey organizers.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11644" title="Abstract">arXiv:2310.11644</a> [<a href="/pdf/2310.11644" title="Download PDF">pdf</a>, <a href="/format/2310.11644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Information Extraction: A Review of Baseline Techniques,  Approaches, and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamp%2C+S">Serafina Kamp</a>, 
<a href="/search/cs?searchtype=author&query=Fayazi%2C+M">Morteza Fayazi</a>, 
<a href="/search/cs?searchtype=author&query=Benameur-El%2C+Z">Zineb Benameur-El</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shuyan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Dreslinski%2C+R">Ronald Dreslinski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">With the abundant amount of available online and offline text data, there
arises a crucial need to extract the relation between phrases and summarize the
main content of each document in a few words. For this purpose, there have been
many studies recently in Open Information Extraction (OIE). OIE improves upon
relation extraction techniques by analyzing relations across different domains
and avoids requiring hand-labeling pre-specified relations in sentences. This
paper surveys recent approaches of OIE and its applications on Knowledge Graph
(KG), text summarization, and Question Answering (QA). Moreover, the paper
describes OIE basis methods in relation extraction. It briefly discusses the
main approaches and the pros and cons of each method. Finally, it gives an
overview about challenges, open issues, and future work opportunities for OIE,
relation extraction, and OIE applications.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11645" title="Abstract">arXiv:2310.11645</a> [<a href="/pdf/2310.11645" title="Download PDF">pdf</a>, <a href="/format/2310.11645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos  using NeRFs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K+T">Khoa Tuan Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tozzi%2C+F">Francesca Tozzi</a>, 
<a href="/search/cs?searchtype=author&query=Rashidian%2C+N">Nikdokht Rashidian</a>, 
<a href="/search/cs?searchtype=author&query=Willaert%2C+W">Wouter Willaert</a>, 
<a href="/search/cs?searchtype=author&query=Vankerschaver%2C+J">Joris Vankerschaver</a>, 
<a href="/search/cs?searchtype=author&query=De+Neve%2C+W">Wesley De Neve</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The Version of Record of this contribution is published in MLMI 2023 Part I, and is available online at <a href="https://doi.org/10.1007/978-3-031-45673-2_9">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Given that a conventional laparoscope only provides a two-dimensional (2-D)
view, the detection and diagnosis of medical ailments can be challenging. To
overcome the visual constraints associated with laparoscopy, the use of
laparoscopic images and videos to reconstruct the three-dimensional (3-D)
anatomical structure of the abdomen has proven to be a promising approach.
Neural Radiance Fields (NeRFs) have recently gained attention thanks to their
ability to generate photorealistic images from a 3-D static scene, thus
facilitating a more comprehensive exploration of the abdomen through the
synthesis of new views. This distinguishes NeRFs from alternative methods such
as Simultaneous Localization and Mapping (SLAM) and depth estimation. In this
paper, we present a comprehensive examination of NeRFs in the context of
laparoscopy surgical videos, with the goal of rendering abdominal scenes in
3-D. Although our experimental results are promising, the proposed approach
encounters substantial challenges, which require further exploration in future
research.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11648" title="Abstract">arXiv:2310.11648</a> [<a href="/pdf/2310.11648" title="Download PDF">pdf</a>, <a href="/format/2310.11648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot Faithfulness Evaluation for Text Summarization with Foundation  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+Q">Qi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Siyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yizhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K+Q">Kenny Q. Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite tremendous improvements in natural language generation, summarization
models still suffer from the unfaithfulness issue. Previous work evaluates
faithfulness either using models trained on the other tasks or in-domain
synthetic data, or prompting a large model such as ChatGPT. This paper proposes
to do zero-shot faithfulness evaluation simply with a moderately-sized
foundation language model. We introduce a new metric FFLM, which is a
combination of probability changes based on the intuition that prefixing a
piece of text that is consistent with the output will increase the probability
of predicting the output. Experiments show that FFLM performs competitively
with or even outperforms ChatGPT on both inconsistency detection and
faithfulness rating with 24x fewer parameters. FFLM also achieves improvements
over other strong baselines.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11650" title="Abstract">arXiv:2310.11650</a> [<a href="/pdf/2310.11650" title="Download PDF">pdf</a>, <a href="/format/2310.11650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VKIE: The Application of Key Information Extraction on Video Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+S">Siyu An</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ye Liu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Haoyuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Di Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
<p class="mathjax">Extracting structured information from videos is critical for numerous
downstream applications in the industry. In this paper, we define a significant
task of extracting hierarchical key information from visual texts on videos. To
fulfill this task, we decouples it into four subtasks and introduce two
implementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially
completes the four subtasks in continuous stages, while UniVKIE is improved by
unifying all the subtasks into one backbone. Both PipVKIE and UniVKIE leverage
multimodal information from vision, text, and coordinates for feature
representation. Extensive experiments on one well-defined dataset demonstrate
that our solutions can achieve remarkable performance and efficient inference
speed. The code and dataset will be publicly available.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11651" title="Abstract">arXiv:2310.11651</a> [<a href="/pdf/2310.11651" title="Download PDF">pdf</a>, <a href="/format/2310.11651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> US Microelectronics Packaging Ecosystem: Challenges and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Noor%2C+R">Rouhan Noor</a>, 
<a href="/search/eess?searchtype=author&query=Kottur%2C+H+R">Himanandhan Reddy Kottur</a>, 
<a href="/search/eess?searchtype=author&query=Craig%2C+P+J">Patrick J Craig</a>, 
<a href="/search/eess?searchtype=author&query=Biswas%2C+L+K">Liton Kumar Biswas</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+M+S+M">M Shafkat M Khan</a>, 
<a href="/search/eess?searchtype=author&query=Varshney%2C+N">Nitin Varshney</a>, 
<a href="/search/eess?searchtype=author&query=Dalir%2C+H">Hamed Dalir</a>, 
<a href="/search/eess?searchtype=author&query=Ak%C3%A7al%C4%B1%2C+E">Elif Ak&#xe7;al&#x131;</a>, 
<a href="/search/eess?searchtype=author&query=Motlagh%2C+B">Bahar Motlagh</a>, 
<a href="/search/eess?searchtype=author&query=Woychik%2C+C">Charles Woychik</a>, 
<a href="/search/eess?searchtype=author&query=Yoon%2C+Y">Yong-Kyu Yoon</a>, 
<a href="/search/eess?searchtype=author&query=Asadizanjani%2C+N">Navid Asadizanjani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The semiconductor industry is experiencing a significant shift from
traditional methods of shrinking devices and reducing costs. Chip designers
actively seek new technological solutions to enhance cost-effectiveness while
incorporating more features into the silicon footprint. One promising approach
is Heterogeneous Integration (HI), which involves advanced packaging techniques
to integrate independently designed and manufactured components using the most
suitable process technology. However, adopting HI introduces design and
security challenges. To enable HI, research and development of advanced
packaging is crucial. The existing research raises the possible security
threats in the advanced packaging supply chain, as most of the Outsourced
Semiconductor Assembly and Test (OSAT) facilities/vendors are offshore. To deal
with the increasing demand for semiconductors and to ensure a secure
semiconductor supply chain, there are sizable efforts from the United States
(US) government to bring semiconductor fabrication facilities onshore. However,
the US-based advanced packaging capabilities must also be ramped up to fully
realize the vision of establishing a secure, efficient, resilient semiconductor
supply chain. Our effort was motivated to identify the possible bottlenecks and
weak links in the advanced packaging supply chain based in the US.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11654" title="Abstract">arXiv:2310.11654</a> [<a href="/pdf/2310.11654" title="Download PDF">pdf</a>, <a href="/format/2310.11654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subject-specific Deep Neural Networks for Count Data with  High-cardinality Categorical Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hangbin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+I+D">Il Do Ha</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+C">Changha Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Youngjo Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">There is a growing interest in subject-specific predictions using deep neural
networks (DNNs) because real-world data often exhibit correlations, which has
been typically overlooked in traditional DNN frameworks. In this paper, we
propose a novel hierarchical likelihood learning framework for introducing
gamma random effects into the Poisson DNN, so as to improve the prediction
performance by capturing both nonlinear effects of input variables and
subject-specific cluster effects. The proposed method simultaneously yields
maximum likelihood estimators for fixed parameters and best unbiased predictors
for random effects by optimizing a single objective function. This approach
enables a fast end-to-end algorithm for handling clustered count data, which
often involve high-cardinality categorical features. Furthermore,
state-of-the-art network architectures can be easily implemented into the
proposed h-likelihood framework. As an example, we introduce multi-head
attention layer and a sparsemax function, which allows feature selection in
high-dimensional settings. To enhance practical performance and learning
efficiency, we present an adjustment procedure for prediction of random
parameters and a method-of-moments estimator for pretraining of variance
component. Various experiential studies and real data analyses confirm the
advantages of our proposed methods.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11655" title="Abstract">arXiv:2310.11655</a> [<a href="/pdf/2310.11655" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Field-testing items using artificial intelligence: Natural language  processing with transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maeda%2C+H">Hotaka Maeda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Five thousand variations of the RoBERTa model, an artificially intelligent
"transformer" that can understand text language, completed an English literacy
exam with 29 multiple-choice questions. Data were used to calculate the
psychometric properties of the items, which showed some degree of agreement to
those obtained from human examinee data.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11657" title="Abstract">arXiv:2310.11657</a> [<a href="/pdf/2310.11657" title="Download PDF">pdf</a>, <a href="/format/2310.11657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT-guided Semantics for Zero-shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shubho%2C+F+H">Fahimul Hoque Shubho</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+T+F">Townim Faisal Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Cheraghian%2C+A">Ali Cheraghian</a>, 
<a href="/search/cs?searchtype=author&query=Saberi%2C+M">Morteza Saberi</a>, 
<a href="/search/cs?searchtype=author&query=Mohammed%2C+N">Nabeel Mohammed</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+S">Shafin Rahman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in International Conference on Digital Image Computing: Techniques and Applications (DICTA), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Zero-shot learning (ZSL) aims to classify objects that are not observed or
seen during training. It relies on class semantic description to transfer
knowledge from the seen classes to the unseen classes. Existing methods of
obtaining class semantics include manual attributes or automatic word vectors
from language models (like word2vec). We know attribute annotation is costly,
whereas automatic word-vectors are relatively noisy. To address this problem,
we explore how ChatGPT, a large language model, can enhance class semantics for
ZSL tasks. ChatGPT can be a helpful source to obtain text descriptions for each
class containing related attributes and semantics. We use the word2vec model to
get a word vector using the texts from ChatGPT. Then, we enrich word vectors by
combining the word embeddings from class names and descriptions generated by
ChatGPT. More specifically, we leverage ChatGPT to provide extra supervision
for the class description, eventually benefiting ZSL models. We evaluate our
approach on various 2D image (CUB and AwA) and 3D point cloud (ModelNet10,
ModelNet40, and ScanObjectNN) datasets and show that it improves ZSL
performance. Our work contributes to the ZSL literature by applying ChatGPT for
class semantics enhancement and proposing a novel word vector fusion method.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11658" title="Abstract">arXiv:2310.11658</a> [<a href="/pdf/2310.11658" title="Download PDF">pdf</a>, <a href="/format/2310.11658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Set-Based Approach for Robust Control Co-Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bird%2C+T+J">Trevor J. Bird</a>, 
<a href="/search/eess?searchtype=author&query=Siefert%2C+J+A">Jacob A. Siefert</a>, 
<a href="/search/eess?searchtype=author&query=Pangborn%2C+H+C">Herschel C. Pangborn</a>, 
<a href="/search/eess?searchtype=author&query=Jain%2C+N">Neera Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Control Co-Design (CCD) considers the coupled effects of both the plant and
control parameters to optimize a system's closed-loop transient performance
during the design stage. This paper presents a new method for CCD with
guarantees on robustness to nondeterministic disturbances for all initial
conditions within a specified region of operation. This is accomplished by
calculating the reachable sets of a candidate closed-loop system directly
within the optimization problem. Using this approach, the plant and control
parameters are simultaneously chosen to shape these reachable sets to be
robustly positive invariant and thus safe for all time. Compared to
conventional approaches that perform the optimization for a single initial
condition and an a priori chosen sequence of disturbances, the proposed
set-based method avoids sensitivity to variations in the assumed design
scenario. As a representative example, the proposed method is applied to an
active suspension system.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11659" title="Abstract">arXiv:2310.11659</a> [<a href="/pdf/2310.11659" title="Download PDF">pdf</a>, <a href="/format/2310.11659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flymation: Interactive Animation for Flying Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yunlong Song</a>, 
<a href="/search/cs?searchtype=author&query=Scaramuzza%2C+D">Davide Scaramuzza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was presented at Workshop at ICRA 2023 ( The Role of Robotics Simulators for Unmanned Aerial Vehicles)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Trajectory visualization and animation play critical roles in robotics
research. However, existing data visualization and animation tools often lack
flexibility, scalability, and versatility, resulting in limited capability to
fully explore and analyze flight data. To address this limitation, we introduce
Flymation, a new flight trajectory visualization and animation tool. Built on
the Unity3D engine, Flymation is an intuitive and interactive tool that allows
users to visualize and analyze flight data in real time. Users can import data
from various sources, including flight simulators and real-world data, and
create customized visualizations with high-quality rendering. With Flymation,
users can choose between trajectory snapshot and animation; both provide
valuable insights into the behavior of the underlying autonomous system.
Flymation represents an exciting step toward visualizing and interacting with
large-scale data in robotics research.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11663" title="Abstract">arXiv:2310.11663</a> [<a href="/pdf/2310.11663" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High Efficiency Polymer based Direct Multi-jet Impingement Cooling  Solution for High Power Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wei%2C+T">Tiwei Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Liquid jet impingement cooling is an efficient cooling technique where the
liquid coolant is directly ejected from nozzles on the chip backside resulting
in a high cooling efficiency due to the absence of the TIM and the lateral
temperature gradient. In literature, several Si-fabrication based impingement
coolers with nozzle diameters of a few distributed returns or combination of
micro-channels and impingement nozzles. The drawback of this Si processing of
the cooler is the high fabrication cost. Other fabrication methods for nozzle
diameters for ceramic and metal. Low cost fabrication methods, including
injection molding and 3D printing have been introduced for much larger nozzle
diameters (mm range) with larger cooler dimensions. These dimensions and
processes are however not compatible with the chip packaging process flow. This
PhD focuses on the modeling, design, fabrication and characterization of a
micro-scale liquid impingement cooler using advanced, yet cost efficient,
fabrication techniques. The main objectives are: (a) development of a modeling
methodology to optimize the cooler geometry; (b) exploring low cost fabrication
methods for the package level impingement jet cooler; (c) experimental thermal
and hydraulic characterization and analysis of the fabricated coolers; (d)
applying the direct impingement jet cooling solutions to different
applications.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11664" title="Abstract">arXiv:2310.11664</a> [<a href="/pdf/2310.11664" title="Download PDF">pdf</a>, <a href="/format/2310.11664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hetero$^2$Net: Heterophily-aware Representation Learning on  Heterogenerous Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jintang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Dan%2C+J">Jiawang Dan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jing Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuchang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruofan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baokun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhen%2C+Z">Zhang Zhen</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Changhua Meng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Real-world graphs are typically complex, exhibiting heterogeneity in the
global structure, as well as strong heterophily within local neighborhoods.
While a growing body of literature has revealed the limitations of common graph
neural networks (GNNs) in handling homogeneous graphs with heterophily, little
work has been conducted on investigating the heterophily properties in the
context of heterogeneous graphs. To bridge this research gap, we identify the
heterophily in heterogeneous graphs using metapaths and propose two practical
metrics to quantitatively describe the levels of heterophily. Through in-depth
investigations on several real-world heterogeneous graphs exhibiting varying
levels of heterophily, we have observed that heterogeneous graph neural
networks (HGNNs), which inherit many mechanisms from GNNs designed for
homogeneous graphs, fail to generalize to heterogeneous graphs with heterophily
or low level of homophily. To address the challenge, we present Hetero$^2$Net,
a heterophily-aware HGNN that incorporates both masked metapath prediction and
masked label prediction tasks to effectively and flexibly handle both
homophilic and heterophilic heterogeneous graphs. We evaluate the performance
of Hetero$^2$Net on five real-world heterogeneous graph benchmarks with varying
levels of heterophily. The results demonstrate that Hetero$^2$Net outperforms
strong baselines in the semi-supervised node classification task, providing
valuable insights into effectively handling more complex heterogeneous graphs.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11665" title="Abstract">arXiv:2310.11665</a> [<a href="/pdf/2310.11665" title="Download PDF">pdf</a>, <a href="/format/2310.11665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forward Kinematics of Object Transport by a Multi-Robot System with  Deformable Sheet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiawei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenhang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+J">Jingang Yi</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhenhua Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, has been submitted to IEEE Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present object handling and transport by a multi-robot team with a
deformable sheet as a carrier. Due to the deformability of the sheet and the
high dimension of the whole system, it is challenging to clearly describe all
the possible positions of the object on the sheet for a given formation of the
multi-robot system. A complete forward kinematics (FK) method is proposed in
this paper for object handling by an $N$-mobile robot team with a deformable
sheet. Based on the virtual variable cables model, a constrained quadratic
problem (CQP) is formulated by combining the form closure and minimum potential
energy conditions of the system. Analytical solutions to the CQP are presented
and then further verified with the force closure condition. With the proposed
FK method, all possible solutions are obtained with the given initial sheet
shape and the robot team formation. We demonstrate the effectiveness,
completeness, and efficiency of the FK method with simulation and experimental
results.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11667" title="Abstract">arXiv:2310.11667</a> [<a href="/pdf/2310.11667" title="Download PDF">pdf</a>, <a href="/format/2310.11667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SOTOPIA: Interactive Evaluation for Social Intelligence in Language  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+L">Leena Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruohong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haofei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zhengyang Qi</a>, 
<a href="/search/cs?searchtype=author&query=Morency%2C+L">Louis-Philippe Morency</a>, 
<a href="/search/cs?searchtype=author&query=Bisk%2C+Y">Yonatan Bisk</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+D">Daniel Fried</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>, 
<a href="/search/cs?searchtype=author&query=Sap%2C+M">Maarten Sap</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, 43 pages. The first two authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Humans are social beings; we pursue social goals in our daily interactions,
which is a crucial aspect of social intelligence. Yet, AI systems' abilities in
this realm remain elusive. We present SOTOPIA, an open-ended environment to
simulate complex social interactions between artificial agents and evaluate
their social intelligence. In our environment, agents role-play and interact
under a wide variety of scenarios; they coordinate, collaborate, exchange, and
compete with each other to achieve complex social goals. We simulate the
role-play interaction between LLM-based agents and humans within this task
space and evaluate their performance with a holistic evaluation framework
called SOTOPIA-Eval. With SOTOPIA, we find significant differences between
these models in terms of their social intelligence, and we identify a subset of
SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models.
We find that on this subset, GPT-4 achieves a significantly lower goal
completion rate than humans and struggles to exhibit social commonsense
reasoning and strategic communication skills. These findings demonstrate
SOTOPIA's promise as a general platform for research on evaluating and
improving social intelligence in artificial agents.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11670" title="Abstract">arXiv:2310.11670</a> [<a href="/pdf/2310.11670" title="Download PDF">pdf</a>, <a href="/format/2310.11670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in
adapting the pre-trained language models to downstream tasks while only
updating a small number of parameters. Despite the success, most existing
methods independently adapt to each task without considering knowledge transfer
between tasks and are limited to low-data regimes. To overcome this issue, we
propose Prototype-based HyperAdapter (PHA), a novel framework built on the
adapter-tuning and hypernetwork. It introduces an instance-dense retriever and
a prototypical hypernetwork to generate the conditional modules in a
sample-efficient manner. This leads to comparable performance improvements
against existing PEFT methods on multi-task learning and few-shot transfer
learning. More importantly, when the available data size gets smaller, our
method outperforms other strong baselines by a large margin. Based on our
extensive empirical experiments across various datasets, we demonstrate that
PHA strikes a better trade-off between trainable parameters, accuracy on stream
tasks, and sample efficiency.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11671" title="Abstract">arXiv:2310.11671</a> [<a href="/pdf/2310.11671" title="Download PDF">pdf</a>, <a href="/format/2310.11671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error  Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jingheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangning Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hai-Tao Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Data Augmentation through generating pseudo data has been proven effective in
mitigating the challenge of data scarcity in the field of Grammatical Error
Correction (GEC). Various augmentation strategies have been widely explored,
most of which are motivated by two heuristics, i.e., increasing the
distribution similarity and diversity of pseudo data. However, the underlying
mechanism responsible for the effectiveness of these strategies remains poorly
understood. In this paper, we aim to clarify how data augmentation improves GEC
models. To this end, we introduce two interpretable and computationally
efficient measures: Affinity and Diversity. Our findings indicate that an
excellent GEC data augmentation strategy characterized by high Affinity and
appropriate Diversity can better improve the performance of GEC models. Based
on this observation, we propose MixEdit, a data augmentation approach that
strategically and dynamically augments realistic data, without requiring extra
monolingual corpora. To verify the correctness of our findings and the
effectiveness of the proposed MixEdit, we conduct experiments on mainstream
English and Chinese GEC datasets. The results show that MixEdit substantially
improves GEC models and is complementary to traditional data augmentation
methods.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11672" title="Abstract">arXiv:2310.11672</a> [<a href="/pdf/2310.11672" title="Download PDF">pdf</a>, <a href="/format/2310.11672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-ended Commonsense Reasoning with Unrestricted Answer Scope
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+C">Chen Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuchao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xujiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Osaki%2C+T">Takao Osaki</a>, 
<a href="/search/cs?searchtype=author&query=Matsuda%2C+K">Katsushi Matsuda</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Open-ended Commonsense Reasoning is defined as solving a commonsense question
without providing 1) a short list of answer candidates and 2) a pre-defined
answer scope. Conventional ways of formulating the commonsense question into a
question-answering form or utilizing external knowledge to learn
retrieval-based methods are less applicable in the open-ended setting due to an
inherent challenge. Without pre-defining an answer scope or a few candidates,
open-ended commonsense reasoning entails predicting answers by searching over
an extremely large searching space. Moreover, most questions require implicit
multi-hop reasoning, which presents even more challenges to our problem. In
this work, we leverage pre-trained language models to iteratively retrieve
reasoning paths on the external knowledge base, which does not require
task-specific supervision. The reasoning paths can help to identify the most
precise answer to the commonsense question. We conduct experiments on two
commonsense benchmark datasets. Compared to other approaches, our proposed
method achieves better performance both quantitatively and qualitatively.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11675" title="Abstract">arXiv:2310.11675</a> [<a href="/pdf/2310.11675" title="Download PDF">pdf</a>, <a href="/format/2310.11675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Relevance to Utility: Evidence Retrieval with Feedback for Fact  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiafeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yixing Fan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Acctepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Retrieval-enhanced methods have become a primary approach in fact
verification (FV); it requires reasoning over multiple retrieved pieces of
evidence to verify the integrity of a claim. To retrieve evidence, existing
work often employs off-the-shelf retrieval models whose design is based on the
probability ranking principle. We argue that, rather than relevance, for FV we
need to focus on the utility that a claim verifier derives from the retrieved
evidence. We introduce the feedback-based evidence retriever(FER) that
optimizes the evidence retrieval process by incorporating feedback from the
claim verifier. As a feedback signal we use the divergence in utility between
how effectively the verifier utilizes the retrieved evidence and the
ground-truth evidence to produce the final claim label. Empirical studies
demonstrate the superiority of FER over prevailing baselines.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11676" title="Abstract">arXiv:2310.11676</a> [<a href="/pdf/2310.11676" title="Download PDF">pdf</a>, <a href="/format/2310.11676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Junjun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yizhen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Node-level graph anomaly detection (GAD) plays a critical role in identifying
anomalous nodes from graph-structured data in various domains such as medicine,
social networks, and e-commerce. However, challenges have arisen due to the
diversity of anomalies and the dearth of labeled data. Existing methodologies -
reconstruction-based and contrastive learning - while effective, often suffer
from efficiency issues, stemming from their complex objectives and elaborate
modules. To improve the efficiency of GAD, we introduce a simple method termed
PREprocessing and Matching (PREM for short). Our approach streamlines GAD,
reducing time and memory consumption while maintaining powerful anomaly
detection capabilities. Comprising two modules - a pre-processing module and an
ego-neighbor matching module - PREM eliminates the necessity for
message-passing propagation during training, and employs a simple contrastive
loss, leading to considerable reductions in training time and memory usage.
Moreover, through rigorous evaluations of five real-world datasets, our method
demonstrated robustness and effectiveness. Notably, when validated on the ACM
dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training
speed, and sharply reduce memory usage compared to the most efficient baseline.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11677" title="Abstract">arXiv:2310.11677</a> [<a href="/pdf/2310.11677" title="Download PDF">pdf</a>, <a href="/ps/2310.11677" title="Download PostScript">ps</a>, <a href="/format/2310.11677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm  with General Parameterization for Infinite Horizon Discounted Reward Markov  Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+W+U">Washim Uddin Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We consider the problem of designing sample efficient learning algorithms for
infinite horizon discounted reward Markov Decision Process. Specifically, we
propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes
an accelerated stochastic gradient descent process to obtain the natural policy
gradient. ANPG achieves $\mathcal{O}({\epsilon^{-2}})$ sample complexity and
$\mathcal{O}(\epsilon^{-1})$ iteration complexity with general parameterization
where $\epsilon$ defines the optimality error. This improves the
state-of-the-art sample complexity by a $\log(\frac{1}{\epsilon})$ factor. ANPG
is a first-order algorithm and unlike some existing literature, does not
require the unverifiable assumption that the variance of importance sampling
(IS) weights is upper bounded. In the class of Hessian-free and IS-free
algorithms, ANPG beats the best-known sample complexity by a factor of
$\mathcal{O}(\epsilon^{-\frac{1}{2}})$ and simultaneously matches their
state-of-the-art iteration complexity.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11678" title="Abstract">arXiv:2310.11678</a> [<a href="/pdf/2310.11678" title="Download PDF">pdf</a>, <a href="/format/2310.11678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Experience Classification for Training Non-Markovian Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+R">Ruixuan Miao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+C">Cong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Z">Zhenhua Duan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Formal Languages and Automata Theory (cs.FL); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Unlike the standard Reinforcement Learning (RL) model, many real-world tasks
are non-Markovian, whose rewards are predicated on state history rather than
solely on the current state. Solving a non-Markovian task, frequently applied
in practical applications such as autonomous driving, financial trading, and
medical diagnosis, can be quite challenging. We propose a novel RL approach to
achieve non-Markovian rewards expressed in temporal logic LTL$_f$ (Linear
Temporal Logic over Finite Traces). To this end, an encoding of linear
complexity from LTL$_f$ into MDPs (Markov Decision Processes) is introduced to
take advantage of advanced RL algorithms. Then, a prioritized experience replay
technique based on the automata structure (semantics equivalent to LTL$_f$
specification) is utilized to improve the training process. We empirically
evaluate several benchmark problems augmented with non-Markovian tasks to
demonstrate the feasibility and effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11681" title="Abstract">arXiv:2310.11681</a> [<a href="/pdf/2310.11681" title="Download PDF">pdf</a>, <a href="/format/2310.11681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Descriptive Knowledge Graph in Biomedical Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kerui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Demo
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present a novel system that automatically extracts and generates
informative and descriptive sentences from the biomedical corpus and
facilitates the efficient search for relational knowledge. Unlike previous
search engines or exploration systems that retrieve unconnected passages, our
system organizes descriptive sentences as a relational graph, enabling
researchers to explore closely related biomedical entities (e.g., diseases
treated by a chemical) or indirectly connected entities (e.g., potential drugs
for treating a disease). Our system also uses ChatGPT and a fine-tuned relation
synthesis model to generate concise and reliable descriptive sentences from
retrieved information, reducing the need for extensive human reading effort.
With our system, researchers can easily obtain both high-level knowledge and
detailed references and interactively steer to the information of interest. We
spotlight the application of our system in COVID-19 research, illustrating its
utility in areas such as drug repurposing and literature curation.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11684" title="Abstract">arXiv:2310.11684</a> [<a href="/pdf/2310.11684" title="Download PDF">pdf</a>, <a href="/format/2310.11684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+B">Bhargav Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantum Physics (quant-ph)

</div>
<p class="mathjax">This paper investigates the potential of quantum acceleration in addressing
infinite horizon Markov Decision Processes (MDPs) to enhance average reward
outcomes. We introduce an innovative quantum framework for the agent's
engagement with an unknown MDP, extending the conventional interaction
paradigm. Our approach involves the design of an optimism-driven tabular
Reinforcement Learning algorithm that harnesses quantum signals acquired by the
agent through efficient quantum mean estimation techniques. Through thorough
theoretical analysis, we demonstrate that the quantum advantage in mean
estimation leads to exponential advancements in regret guarantees for infinite
horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm
achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement
over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical
counterparts.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11685" title="Abstract">arXiv:2310.11685</a> [<a href="/pdf/2310.11685" title="Download PDF">pdf</a>, <a href="/format/2310.11685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Superiority of Softmax: Unveiling the Performance Edge Over Linear  Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yichuan Deng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large transformer models have achieved state-of-the-art results in numerous
natural language processing tasks. Among the pivotal components of the
transformer architecture, the attention mechanism plays a crucial role in
capturing token interactions within sequences through the utilization of
softmax function.
<br />Conversely, linear attention presents a more computationally efficient
alternative by approximating the softmax operation with linear complexity.
However, it exhibits substantial performance degradation when compared to the
traditional softmax attention mechanism.
<br />In this paper, we bridge the gap in our theoretical understanding of the
reasons behind the practical performance gap between softmax and linear
attention. By conducting a comprehensive comparative analysis of these two
attention mechanisms, we shed light on the underlying reasons for why softmax
attention outperforms linear attention in most scenarios.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11687" title="Abstract">arXiv:2310.11687</a> [<a href="/pdf/2310.11687" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are research contributions assigned differently under the two  contributorship classification systems in PLoS ONE?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kai Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Larivi%C3%A8re%2C+V">Vincent Larivi&#xe8;re</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">Contributorship statements have been effective at recording granular author
contributions in research articles and have been broadly used to understand how
labor is divided across research teams. However, one major limitation in
existing empirical studies is that two classification systems have been
adopted, especially from its most important data source, journals published by
the Public Library of Science (PLoS). This research aims to address this
limitation by developing a mapping scheme between the two systems and using it
to understand whether there are differences in the assignment of contribution
by authors under the two systems. We use all research articles published in
PLoS ONE between 2012 to 2020, divided into two five-year publication windows
centered by the shift of the classification systems in 2016. Our results show
that most tasks (except for writing- and resource-related tasks) are used
similarly under the two systems. Moreover, notable differences between how
researchers used the two systems are also examined and discussed. This research
offers an important foundation for empirical research on division of labor in
the future, by enabling a larger dataset that crosses both, and potentially
other, classification systems.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11689" title="Abstract">arXiv:2310.11689</a> [<a href="/pdf/2310.11689" title="Download PDF">pdf</a>, <a href="/format/2310.11689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiefeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jinsung Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Ebrahimi%2C+S">Sayna Ebrahimi</a>, 
<a href="/search/cs?searchtype=author&query=Arik%2C+S+O">Sercan O Arik</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+T">Tomas Pfister</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper published at Findings of the Association for Computational Linguistics: EMNLP, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have recently shown great advances in a variety
of tasks, including natural language understanding and generation. However,
their use in high-stakes decision-making scenarios is still limited due to the
potential for errors. Selective prediction is a technique that can be used to
improve the reliability of the LLMs by allowing them to abstain from making
predictions when they are unsure of the answer. In this work, we propose a
novel framework for adaptation with self-evaluation to improve the selective
prediction performance of LLMs. Our framework is based on the idea of using
parameter-efficient tuning to adapt the LLM to the specific task at hand while
improving its ability to perform self-evaluation. We evaluate our method on a
variety of question-answering (QA) datasets and show that it outperforms
state-of-the-art selective prediction methods. For example, on the CoQA
benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the
AUROC from 74.61% to 80.25%.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11690" title="Abstract">arXiv:2310.11690</a> [<a href="/pdf/2310.11690" title="Download PDF">pdf</a>, <a href="/format/2310.11690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep learning based on Transformer architecture for power system  short-term voltage stability assessment with class imbalance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/eess?searchtype=author&query=Cao%2C+J">Jiting Cao</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yan Xu</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Lipeng Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+Z+Y">Zhao Yang Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Renewable and Sustainable Energy Reviews
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Most existing data-driven power system short-term voltage stability
assessment (STVSA) approaches presume class-balanced input data. However, in
practical applications, the occurrence of short-term voltage instability
following a disturbance is minimal, leading to a significant class imbalance
problem and a consequent decline in classifier performance. This work proposes
a Transformer-based STVSA method to address this challenge. By utilizing the
basic Transformer architecture, a stability assessment Transformer (StaaT) is
developed {as a classification model to reflect the correlation between the
operational states of the system and the resulting stability outcomes}. To
combat the negative impact of imbalanced datasets, this work employs a
conditional Wasserstein generative adversarial network with gradient penalty
(CWGAN-GP) for synthetic data generation, aiding in the creation of a balanced,
representative training set for the classifier. Semi-supervised clustering
learning is implemented to enhance clustering quality, addressing the lack of a
unified quantitative criterion for short-term voltage stability. {Numerical
tests on the IEEE 39-bus test system extensively demonstrate that the proposed
method exhibits robust performance under class imbalances up to 100:1 and noisy
environments, and maintains consistent effectiveness even with an increased
penetration of renewable energy}. Comparative results reveal that the CWGAN-GP
generates more balanced datasets than traditional oversampling methods and that
the StaaT outperforms other deep learning algorithms. This study presents a
compelling solution for real-world STVSA applications that often face class
imbalance and data noise challenges.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11692" title="Abstract">arXiv:2310.11692</a> [<a href="/pdf/2310.11692" title="Download PDF">pdf</a>, <a href="/format/2310.11692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Sampling of Bandlimited Graph Signals from Local Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Lili Shen</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+J">Jun Xian</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Cheng Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The random sampling on graph signals is one of the fundamental topics in
graph signal processing. In this letter, we consider the random sampling of
k-bandlimited signals from the local measurements and show that no more than
O(klogk) measurements with replacement are sufficient for the accurate and
stable recovery of any k-bandlimited graph signals. We propose two random
sampling strategies based on the minimum measurements, i.e., the optimal
sampling and the estimated sampling. The geodesic distance between vertices is
introduced to design the sampling probability distribution. Numerical
experiments are included to show the effectiveness of the proposed methods.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11693" title="Abstract">arXiv:2310.11693</a> [<a href="/pdf/2310.11693" title="Download PDF">pdf</a>, <a href="/format/2310.11693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AUC-mixup: Deep AUC Maximization with Mixup
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xv%2C+J">Jianzhi Xv</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianbao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">While deep AUC maximization (DAM) has shown remarkable success on imbalanced
medical tasks, e.g., chest X-rays classification and skin lesions
classification, it could suffer from severe overfitting when applied to small
datasets due to its aggressive nature of pushing prediction scores of positive
data away from that of negative data. This paper studies how to improve
generalization of DAM by mixup data augmentation -- an approach that is widely
used for improving generalization of the cross-entropy loss based deep learning
methods. %For overfitting issues arising from limited data, the common approach
is to employ mixup data augmentation to boost the models' generalization
performance by enriching the training data. However, AUC is defined over
positive and negative pairs, which makes it challenging to incorporate mixup
data augmentation into DAM algorithms. To tackle this challenge, we employ the
AUC margin loss and incorporate soft labels into the formulation to effectively
learn from data generated by mixup augmentation, which is referred to as the
AUC-mixup loss. Our experimental results demonstrate the effectiveness of the
proposed AUC-mixup methods on imbalanced benchmark and medical image datasets
compared to standard DAM training methods.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11696" title="Abstract">arXiv:2310.11696</a> [<a href="/pdf/2310.11696" title="Download PDF">pdf</a>, <a href="/format/2310.11696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOHO: Learning Single-view Hand-held Object Reconstruction with  Multi-view Occlusion-Aware Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenyangguang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+G">Guanlong Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+Y">Yan Di</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Ziqin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruida Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+B">Bowen Fu</a>, 
<a href="/search/cs?searchtype=author&query=Tombari%2C+F">Federico Tombari</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiangyang Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Previous works concerning single-view hand-held object reconstruction
typically utilize supervision from 3D ground truth models, which are hard to
collect in real world. In contrast, abundant videos depicting hand-object
interactions can be accessed easily with low cost, although they only give
partial object observations with complex occlusion. In this paper, we present
MOHO to reconstruct hand-held object from a single image with multi-view
supervision from hand-object videos, tackling two predominant challenges
including object's self-occlusion and hand-induced occlusion. MOHO inputs
semantic features indicating visible object parts and geometric embeddings
provided by hand articulations as partial-to-full cues to resist object's
self-occlusion, so as to recover full shape of the object. Meanwhile, a novel
2D-3D hand-occlusion-aware training scheme following the synthetic-to-real
paradigm is proposed to release hand-induced occlusion. In the synthetic
pre-training stage, 2D-3D hand-object correlations are constructed by
supervising MOHO with rendered images to complete the hand-concealed regions of
the object in both 2D and 3D space. Subsequently, MOHO is finetuned in real
world by the mask-weighted volume rendering supervision adopting hand-object
correlations obtained during pre-training. Extensive experiments on HO3D and
DexYCB datasets demonstrate that 2D-supervised MOHO gains superior results
against 3D-supervised methods by a large margin. Codes and key assets will be
released soon.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11699" title="Abstract">arXiv:2310.11699</a> [<a href="/pdf/2310.11699" title="Download PDF">pdf</a>, <a href="/format/2310.11699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MISAR: A Multimodal Instructional System with Augmented Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+J">Jing Bi</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+N+M">Nguyen Manh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+A">Ali Vosoughi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenliang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023 - AV4D, 6 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Augmented reality (AR) requires the seamless integration of visual, auditory,
and linguistic channels for optimized human-computer interaction. While
auditory and visual inputs facilitate real-time and contextual user guidance,
the potential of large language models (LLMs) in this landscape remains largely
untapped. Our study introduces an innovative method harnessing LLMs to
assimilate information from visual, auditory, and contextual modalities.
Focusing on the unique challenge of task performance quantification in AR, we
utilize egocentric video, speech, and context analysis. The integration of LLMs
facilitates enhanced state estimation, marking a step towards more adaptive AR
systems. Code, dataset, and demo will be available at
https://github.com/nguyennm1024/misar.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11700" title="Abstract">arXiv:2310.11700</a> [<a href="/pdf/2310.11700" title="Download PDF">pdf</a>, <a href="/format/2310.11700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Runner re-identification from single-view video in the open-world  setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">Tomohiro Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Tsutsui%2C+K">Kazushi Tsutsui</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+K">Kazuya Takeda</a>, 
<a href="/search/cs?searchtype=author&query=Fujii%2C+K">Keisuke Fujii</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In many sports, player re-identification is crucial for automatic video
processing and analysis. However, most of the current studies on player
re-identification in multi- or single-view sports videos focus on
re-identification in the closed-world setting using labeled image dataset, and
player re-identification in the open-world setting for automatic video analysis
is not well developed. In this paper, we propose a runner re-identification
system that directly processes single-view video to address the open-world
setting. In the open-world setting, we cannot use labeled dataset and have to
process video directly. The proposed system automatically processes raw video
as input to identify runners, and it can identify runners even when they are
framed out multiple times. For the automatic processing, we first detect the
runners in the video using the pre-trained YOLOv8 and the fine-tuned
EfficientNet. We then track the runners using ByteTrack and detect their shoes
with the fine-tuned YOLOv8. Finally, we extract the image features of the
runners using an unsupervised method using the gated recurrent unit autoencoder
model. To improve the accuracy of runner re-identification, we use dynamic
features of running sequence images. We evaluated the system on a running
practice video dataset and showed that the proposed method identified runners
with higher accuracy than one of the state-of-the-art models in unsupervised
re-identification. We also showed that our unsupervised running dynamic feature
extractor was effective for runner re-identification. Our runner
re-identification system can be useful for the automatic analysis of running
videos.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11702" title="Abstract">arXiv:2310.11702</a> [<a href="/pdf/2310.11702" title="Download PDF">pdf</a>, <a href="/format/2310.11702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPF-Nutrition: Food Nutrition Estimation via Depth Prediction and Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yuzhe Han</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qimin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wenjin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Ziyang Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A reasonable and balanced diet is essential for maintaining good health. With
the advancements in deep learning, automated nutrition estimation method based
on food images offers a promising solution for monitoring daily nutritional
intake and promoting dietary health. While monocular image-based nutrition
estimation is convenient, efficient, and economical, the challenge of limited
accuracy remains a significant concern. To tackle this issue, we proposed
DPF-Nutrition, an end-to-end nutrition estimation method using monocular
images. In DPF-Nutrition, we introduced a depth prediction module to generate
depth maps, thereby improving the accuracy of food portion estimation.
Additionally, we designed an RGB-D fusion module that combined monocular images
with the predicted depth information, resulting in better performance for
nutrition estimation. To the best of our knowledge, this was the pioneering
effort that integrated depth prediction and RGB-D fusion techniques in food
nutrition estimation. Comprehensive experiments performed on Nutrition5k
evaluated the effectiveness and efficiency of DPF-Nutrition.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11703" title="Abstract">arXiv:2310.11703</a> [<a href="/pdf/2310.11703" title="Download PDF">pdf</a>, <a href="/format/2310.11703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on Vector Database: Storage and Retrieval  Technique, Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yikun Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chunjiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengfei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A vector database is used to store high-dimensional data that cannot be
characterized by traditional DBMS. Although there are not many articles
describing existing or introducing new vector database architectures, the
approximate nearest neighbor search problem behind vector databases has been
studied for a long time, and considerable related algorithmic articles can be
found in the literature. This article attempts to comprehensively review
relevant algorithms to provide a general understanding of this booming research
area. The basis of our framework categorises these studies by the approach of
solving ANNS problem, respectively hash-based, tree-based, graph-based and
quantization-based approaches. Then we present an overview of existing
challenges for vector databases. Lastly, we sketch how vector databases can be
combined with large language models and provide new possibilities.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11706" title="Abstract">arXiv:2310.11706</a> [<a href="/pdf/2310.11706" title="Download PDF">pdf</a>, <a href="/format/2310.11706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MalDICT: Benchmark Datasets on Malware Behaviors, Platforms,  Exploitation, and Packers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joyce%2C+R+J">Robert J. Joyce</a>, 
<a href="/search/cs?searchtype=author&query=Raff%2C+E">Edward Raff</a>, 
<a href="/search/cs?searchtype=author&query=Nicholas%2C+C">Charles Nicholas</a>, 
<a href="/search/cs?searchtype=author&query=Holt%2C+J">James Holt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Existing research on malware classification focuses almost exclusively on two
tasks: distinguishing between malicious and benign files and classifying
malware by family. However, malware can be categorized according to many other
types of attributes, and the ability to identify these attributes in
newly-emerging malware using machine learning could provide significant value
to analysts. In particular, we have identified four tasks which are
under-represented in prior work: classification by behaviors that malware
exhibit, platforms that malware run on, vulnerabilities that malware exploit,
and packers that malware are packed with. To obtain labels for training and
evaluating ML classifiers on these tasks, we created an antivirus (AV) tagging
tool called ClarAVy. ClarAVy's sophisticated AV label parser distinguishes
itself from prior AV-based taggers, with the ability to accurately parse 882
different AV label formats used by 90 different AV products. We are releasing
benchmark datasets for each of these four classification tasks, tagged using
ClarAVy and comprising nearly 5.5 million malicious files in total. Our malware
behavior dataset includes 75 distinct tags - nearly 7x more than the only prior
benchmark dataset with behavioral tags. To our knowledge, we are the first to
release datasets with malware platform and packer tags.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11707" title="Abstract">arXiv:2310.11707</a> [<a href="/pdf/2310.11707" title="Download PDF">pdf</a>, <a href="/format/2310.11707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning under Label Proportions for Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+J">Jatin Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted as long paper in Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present one of the preliminary NLP works under the challenging setup of
Learning from Label Proportions (LLP), where the data is provided in an
aggregate form called bags and only the proportion of samples in each class as
the ground truth. This setup is inline with the desired characteristics of
training models under Privacy settings and Weakly supervision. By
characterizing some irregularities of the most widely used baseline technique
DLLP, we propose a novel formulation that is also robust. This is accompanied
with a learnability result that provides a generalization bound under LLP.
Combining this formulation with a self-supervised objective, our method
achieves better results as compared to the baselines in almost 87% of the
experimental configurations which include large scale models for both long and
short range texts across multiple metrics.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11709" title="Abstract">arXiv:2310.11709</a> [<a href="/pdf/2310.11709" title="Download PDF">pdf</a>, <a href="/format/2310.11709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with  NFT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bingqiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shengliang Lu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bingsheng He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023, Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Numerous studies have been conducted to investigate the properties of
large-scale temporal graphs. Despite the ubiquity of these graphs in real-world
scenarios, it's usually impractical for us to obtain the whole real-time graphs
due to privacy concerns and technical limitations. In this paper, we introduce
the concept of {\it Live Graph Lab} for temporal graphs, which enables open,
dynamic and real transaction graphs from blockchains. Among them, Non-fungible
tokens (NFTs) have become one of the most prominent parts of blockchain over
the past several years. With more than \$40 billion market capitalization, this
decentralized ecosystem produces massive, anonymous and real transaction
activities, which naturally forms a complicated transaction network. However,
there is limited understanding about the characteristics of this emerging NFT
ecosystem from a temporal graph analysis perspective. To mitigate this gap, we
instantiate a live graph with NFT transaction network and investigate its
dynamics to provide new observations and insights. Specifically, through
downloading and parsing the NFT transaction activities, we obtain a temporal
graph with more than 4.5 million nodes and 124 million edges. Then, a series of
measurements are presented to understand the properties of the NFT ecosystem.
Through comparisons with social, citation, and web networks, our analyses give
intriguing findings and point out potential directions for future exploration.
Finally, we also study machine learning models in this live graph to enrich the
current datasets and provide new opportunities for the graph community. The
source codes and dataset are available at https://livegraphlab.github.io.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11710" title="Abstract">arXiv:2310.11710</a> [<a href="/pdf/2310.11710" title="Download PDF">pdf</a>, <a href="/format/2310.11710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Co-Speech Gesture for Multimodal Aphasia Type Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Daeun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Son%2C+S">Sejung Son</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyolim Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungbae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jinyoung Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Aphasia, a language disorder resulting from brain damage, requires accurate
identification of specific aphasia types, such as Broca's and Wernicke's
aphasia, for effective treatment. However, little attention has been paid to
developing methods to detect different types of aphasia. Recognizing the
importance of analyzing co-speech gestures for distinguish aphasia types, we
propose a multimodal graph neural network for aphasia type detection using
speech and corresponding gesture patterns. By learning the correlation between
the speech and gesture modalities for each aphasia type, our model can generate
textual representations sensitive to gesture information, leading to accurate
aphasia type detection. Extensive experiments demonstrate the superiority of
our approach over existing methods, achieving state-of-the-art results (F1
84.2\%). We also show that gesture features outperform acoustic features,
highlighting the significance of gesture expression in detecting aphasia types.
We provide the codes for reproducibility purposes\footnote{Code:
\url{https://github.com/DSAIL-SKKU/Multimodal-Aphasia-Type-Detection_EMNLP_2023}}.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11713" title="Abstract">arXiv:2310.11713</a> [<a href="/pdf/2310.11713" title="Download PDF">pdf</a>, <a href="/format/2310.11713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware  Sound Separation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yiyang Su</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+A">Ali Vosoughi</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shijian Deng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yapeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenliang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023 - AV4D, 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The audio-visual sound separation field assumes visible sources in videos,
but this excludes invisible sounds beyond the camera's view. Current methods
struggle with such sounds lacking visible cues. This paper introduces a novel
"Audio-Visual Scene-Aware Separation" (AVSA-Sep) framework. It includes a
semantic parser for visible and invisible sounds and a separator for
scene-informed separation. AVSA-Sep successfully separates both sound types,
with joint training and cross-modal alignment enhancing effectiveness.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11714" title="Abstract">arXiv:2310.11714</a> [<a href="/pdf/2310.11714" title="Download PDF">pdf</a>, <a href="/format/2310.11714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Evaluation of Generative Models in Distributed Learning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Farnia%2C+F">Farzan Farnia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhenghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yunheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bei Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The evaluation of deep generative models including generative adversarial
networks (GANs) and diffusion models has been extensively studied in the
literature. While the existing evaluation methods mainly target a centralized
learning problem with training data stored by a single client, many
applications of generative models concern distributed learning settings, e.g.
the federated learning scenario, where training data are collected by and
distributed among several clients. In this paper, we study the evaluation of
generative models in distributed learning tasks with heterogeneous data
distributions. First, we focus on the Fr\'echet inception distance (FID) and
consider the following FID-based aggregate scores over the clients: 1) FID-avg
as the mean of clients' individual FID scores, 2) FID-all as the FID distance
of the trained model to the collective dataset containing all clients' data. We
prove that the model rankings according to the FID-all and FID-avg scores could
be inconsistent, which can lead to different optimal generative models
according to the two aggregate scores. Next, we consider the kernel inception
distance (KID) and similarly define the KID-avg and KID-all aggregations.
Unlike the FID case, we prove that KID-all and KID-avg result in the same
rankings of generative models. We perform several numerical experiments on
standard image datasets and training schemes to support our theoretical
findings on the evaluation of generative models in distributed learning
problems.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11715" title="Abstract">arXiv:2310.11715</a> [<a href="/pdf/2310.11715" title="Download PDF">pdf</a>, <a href="/format/2310.11715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Low-resource Fine-grained Named Entity Recognition by  Leveraging Coarse-grained Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+A">Su Ah Lee</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Seokjin Oh</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+W">Woohwan Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Named Entity Recognition (NER) frequently suffers from the problem of
insufficient labeled data, particularly in fine-grained NER scenarios. Although
$K$-shot learning techniques can be applied, their performance tends to
saturate when the number of annotations exceeds several tens of labels. To
overcome this problem, we utilize existing coarse-grained datasets that offer a
large number of annotations. A straightforward approach to address this problem
is pre-finetuning, which employs coarse-grained data for representation
learning. However, it cannot directly utilize the relationships between
fine-grained and coarse-grained entities, although a fine-grained entity type
is likely to be a subcategory of a coarse-grained entity type. We propose a
fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage
the hierarchical structure explicitly. In addition, we present an inconsistency
filtering method to eliminate coarse-grained entities that are inconsistent
with fine-grained entity types to avoid performance degradation. Our
experimental results show that our method outperforms both $K$-shot learning
and supervised learning methods when dealing with a small number of
fine-grained annotations.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11716" title="Abstract">arXiv:2310.11716</a> [<a href="/pdf/2310.11716" title="Download PDF">pdf</a>, <a href="/format/2310.11716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiuhai Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shwai He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiuxiang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent advancements in Large Language Models (LLMs) have expanded the
horizons of natural language understanding and generation. Notably, the output
control and alignment with the input of LLMs can be refined through instruction
tuning. However, as highlighted in several studies, low-quality data in the
training set are usually detrimental to instruction tuning, resulting in
inconsistent or even misleading LLM outputs. We propose a novel method, termed
"reflection-tuning," which addresses the problem by self-improvement and
judging capabilities of LLMs. This approach utilizes an oracle LLM to recycle
the original training data by introspecting and enhancing the quality of
instructions and responses in the data. Extensive experiments on widely used
evaluation benchmarks show that LLMs trained with our recycled data outperform
those trained with existing datasets in various benchmarks.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11721" title="Abstract">arXiv:2310.11721</a> [<a href="/pdf/2310.11721" title="Download PDF">pdf</a>, <a href="/format/2310.11721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Thought Tuning: Masked Language Models can also Think Step By  Step in Natural Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Caoyun Fan</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jidong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yitian Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenqing Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hao He</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaohui Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Chain-of-Thought (CoT) is a technique that guides Large Language Models
(LLMs) to decompose complex tasks into multi-step reasoning through
intermediate steps in natural language form. Briefly, CoT enables LLMs to think
step by step. However, although many Natural Language Understanding (NLU) tasks
also require thinking step by step, LLMs perform less well than small-scale
Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose
Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt
tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the
perspective of CoT, CoTT's two-step framework enables MLMs to implement task
decomposition; CoTT's prompt tuning allows intermediate steps to be used in
natural language form. Thereby, the success of CoT can be extended to NLU tasks
through MLMs. To verify the effectiveness of CoTT, we conduct experiments on
two NLU tasks: hierarchical classification and relation extraction, and the
results show that CoTT outperforms baselines and achieves state-of-the-art
performance.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11722" title="Abstract">arXiv:2310.11722</a> [<a href="/pdf/2310.11722" title="Download PDF">pdf</a>, <a href="/format/2310.11722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantify Health-Related Atomic Knowledge in Chinese Medical Large  Language Models: A Computational Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yaxin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Feng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have the potential to revolutionize the way
users self-diagnose through search engines by offering direct and efficient
suggestions. Recent studies primarily focused on the quality of LLMs evaluated
by GPT-4 or their ability to pass medical exams, no studies have quantified the
extent of health-related atomic knowledge stored in LLMs' memory, which is the
basis of LLMs to provide more factual suggestions. In this paper, we first
constructed a benchmark, including the most common types of atomic knowledge in
user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces
of atomic knowledge. Then, we evaluated both generic and specialized LLMs on
the benchmark. The experimental results showcased that generic LLMs perform
better than specialized LLMs in terms of atomic knowledge and
instruction-following ability. Error analysis revealed that both generic and
specialized LLMs are sycophantic, e.g., always catering to users' claims when
it comes to unknown knowledge. Besides, generic LLMs showed stronger safety,
which can be learned by specialized LLMs through distilled data. We further
explored different types of data commonly adopted for fine-tuning specialized
LLMs, i.e., real-world, semi-distilled, and distilled data, and found that
distilled data can benefit LLMs most.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11723" title="Abstract">arXiv:2310.11723</a> [<a href="/pdf/2310.11723" title="Download PDF">pdf</a>, <a href="/format/2310.11723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty in Automated Ontology Matching: Lessons Learned from an  Empirical Experimentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osman%2C+I">In&#xe8;s Osman</a>, 
<a href="/search/cs?searchtype=author&query=Pileggi%2C+S+F">Salvatore F. Pileggi</a>, 
<a href="/search/cs?searchtype=author&query=Yahia%2C+S+B">Sadok Ben Yahia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Data integration is considered a classic research field and a pressing need
within the information science community. Ontologies play a critical role in
such a process by providing well-consolidated support to link and semantically
integrate datasets via interoperability. This paper approaches data integration
from an application perspective, looking at techniques based on ontology
matching. An ontology-based process may only be considered adequate by assuming
manual matching of different sources of information. However, since the
approach becomes unrealistic once the system scales up, automation of the
matching process becomes a compelling need. Therefore, we have conducted
experiments on actual data with the support of existing tools for automatic
ontology matching from the scientific community. Even considering a relatively
simple case study (i.e., the spatio-temporal alignment of global indicators),
outcomes clearly show significant uncertainty resulting from errors and
inaccuracies along the automated matching process. More concretely, this paper
aims to test on real-world data a bottom-up knowledge-building approach,
discuss the lessons learned from the experimental results of the case study,
and draw conclusions about uncertainty and uncertainty management in an
automated ontology matching process. While the most common evaluation metrics
clearly demonstrate the unreliability of fully automated matching solutions,
properly designed semi-supervised approaches seem to be mature for a more
generalized application.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11725" title="Abstract">arXiv:2310.11725</a> [<a href="/pdf/2310.11725" title="Download PDF">pdf</a>, <a href="/format/2310.11725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VST++: Efficient and Stronger Visual Saliency Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Ziyang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ni Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Junwei Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While previous CNN-based models have exhibited promising results for salient
object detection (SOD), their ability to explore global long-range dependencies
is restricted. Our previous work, the Visual Saliency Transformer (VST),
addressed this constraint from a transformer-based sequence-to-sequence
perspective, to unify RGB and RGB-D SOD. In VST, we developed a multi-task
transformer decoder that concurrently predicts saliency and boundary outcomes
in a pure transformer architecture. Moreover, we introduced a novel token
upsampling method called reverse T2T for predicting a high-resolution saliency
map effortlessly within transformer-based structures. Building upon the VST
model, we further propose an efficient and stronger VST version in this work,
i.e. VST++. To mitigate the computational costs of the VST model, we propose a
Select-Integrate Attention (SIA) module, partitioning foreground into
fine-grained segments and aggregating background information into a single
coarse-grained token. To incorporate 3D depth information with low cost, we
design a novel depth position encoding method tailored for depth maps.
Furthermore, we introduce a token-supervised prediction loss to provide
straightforward guidance for the task-related tokens. We evaluate our VST++
model across various transformer-based backbones on RGB, RGB-D, and RGB-T SOD
benchmark datasets. Experimental results show that our model outperforms
existing methods while achieving a 25% reduction in computational costs without
significant performance compromise. The demonstrated strong ability for
generalization, enhanced performance, and heightened efficiency of our VST++
model highlight its potential.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11728" title="Abstract">arXiv:2310.11728</a> [<a href="/pdf/2310.11728" title="Download PDF">pdf</a>, <a href="/format/2310.11728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EchoScan: Scanning Complex Indoor Geometries via Acoustic Echoes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeon%2C+I">Inmo Yeon</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+I">Iljoo Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seungchul Lee</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jung-Woo Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
<p class="mathjax">Accurate estimation of indoor space geometries is vital for constructing
precise digital twins, whose broad industrial applications include navigation
in unfamiliar environments and efficient evacuation planning, particularly in
low-light conditions. This study introduces EchoScan, a deep neural network
model that utilizes acoustic echoes to perform room geometry inference.
Conventional sound-based techniques rely on estimating geometry-related room
parameters such as wall position and room size, thereby limiting the diversity
of inferable room geometries. Contrarily, EchoScan overcomes this limitation by
directly inferring room floorplans and heights, thereby enabling it to handle
rooms with arbitrary shapes, including curved walls. The key innovation of
EchoScan is its ability to analyze the complex relationship between low- and
high-order reflections in room impulse responses (RIRs) using a
multi-aggregation module. The analysis of high-order reflections also enables
it to infer complex room shapes when echoes are unobservable from the position
of an audio device. Herein, EchoScan was trained and evaluated using RIRs
synthesized from complex environments, including the Manhattan and Atlanta
layouts, employing a practical audio device configuration compatible with
commercial, off-the-shelf devices. Compared with vision-based methods, EchoScan
demonstrated outstanding geometry estimation performance in rooms with various
shapes.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11730" title="Abstract">arXiv:2310.11730</a> [<a href="/pdf/2310.11730" title="Download PDF">pdf</a>, <a href="/format/2310.11730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Heterogeneous Graph Neural Network for Privacy-preserving  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bo Yan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenchuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Junping Du</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submit to WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Heterogeneous information network (HIN), which contains rich semantics
depicted by meta-paths, has become a powerful tool to alleviate data sparsity
in recommender systems. Existing HIN-based recommendations hold the data
centralized storage assumption and conduct centralized model training. However,
the real-world data is often stored in a distributed manner for privacy
concerns, resulting in the failure of centralized HIN-based recommendations. In
this paper, we suggest the HIN is partitioned into private HINs stored in the
client side and shared HINs in the server. Following this setting, we propose a
federated heterogeneous graph neural network (FedHGNN) based framework, which
can collaboratively train a recommendation model on distributed HINs without
leaking user privacy. Specifically, we first formalize the privacy definition
in the light of differential privacy for HIN-based federated recommendation,
which aims to protect user-item interactions of private HIN as well as user's
high-order patterns from shared HINs. To recover the broken meta-path based
semantics caused by distributed data storage and satisfy the proposed privacy,
we elaborately design a semantic-preserving user interactions publishing
method, which locally perturbs user's high-order patterns as well as related
user-item interactions for publishing. After that, we propose a HGNN model for
recommendation, which conducts node- and semantic-level aggregations to capture
recovered semantics. Extensive experiments on three datasets demonstrate our
model outperforms existing methods by a large margin (up to 34% in HR@10 and
42% in NDCG@10) under an acceptable privacy budget.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11731" title="Abstract">arXiv:2310.11731</a> [<a href="/pdf/2310.11731" title="Download PDF">pdf</a>, <a href="/format/2310.11731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Action-Quantized Offline Reinforcement Learning for Robotic Skill  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jianlan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+P">Perry Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jeffrey Wu</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aviral Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xinyang Geng</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The offline reinforcement learning (RL) paradigm provides a general recipe to
convert static behavior datasets into policies that can perform better than the
policy that collected the data. While policy constraints, conservatism, and
other methods for mitigating distributional shifts have made offline
reinforcement learning more effective, the continuous action setting often
necessitates various approximations for applying these techniques. Many of
these challenges are greatly alleviated in discrete action settings, where
offline RL constraints and regularizers can often be computed more precisely or
even exactly. In this paper, we propose an adaptive scheme for action
quantization. We use a VQ-VAE to learn state-conditioned action quantization,
avoiding the exponential blowup that comes with na\"ive discretization of the
action space. We show that several state-of-the-art offline RL methods such as
IQL, CQL, and BRAC improve in performance on benchmarks when combined with our
proposed discretization scheme. We further validate our approach on a set of
challenging long-horizon complex robotic manipulation tasks in the Robomimic
environment, where our discretized offline RL algorithms are able to improve
upon their continuous counterparts by 2-3x. Our project page is at
https://saqrl.github.io/
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11732" title="Abstract">arXiv:2310.11732</a> [<a href="/pdf/2310.11732" title="Download PDF">pdf</a>, <a href="/format/2310.11732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Uncertainty Calibration of Aligned Language Models under  the Multiple-Choice Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guande He</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+P">Peng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenbo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite the significant progress made in practical applications of aligned
language models (LMs), they tend to be overconfident in output answers compared
to the corresponding pre-trained LMs. In this work, we systematically evaluate
the impact of the alignment process on logit-based uncertainty calibration of
LMs under the multiple-choice setting. We first conduct a thoughtful empirical
study on how aligned LMs differ in calibration from their pre-trained
counterparts. Experimental results reveal that there are two distinct
uncertainties in LMs under the multiple-choice setting, which are responsible
for the answer decision and the format preference of the LMs, respectively.
Then, we investigate the role of these two uncertainties on aligned LM's
calibration through fine-tuning in simple synthetic alignment schemes and
conclude that one reason for aligned LMs' overconfidence is the conflation of
these two types of uncertainty. Furthermore, we examine the utility of common
post-hoc calibration methods for aligned LMs and propose an easy-to-implement
and sample-efficient method to calibrate aligned LMs. We hope our findings
could provide insights into the design of more reliable alignment processes for
LMs.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11733" title="Abstract">arXiv:2310.11733</a> [<a href="/pdf/2310.11733" title="Download PDF">pdf</a>, <a href="/format/2310.11733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DBDNet:Partial-to-Partial Point Cloud Registration with Dual Branches  Decoupling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jihua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yifan Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Point cloud registration plays a crucial role in various computer vision
tasks, and usually demands the resolution of partial overlap registration in
practice. Most existing methods perform a serial calculation of rotation and
translation, while jointly predicting overlap during registration, this
coupling tends to degenerate the registration performance. In this paper, we
propose an effective registration method with dual branches decoupling for
partial-to-partial registration, dubbed as DBDNet. Specifically, we introduce a
dual branches structure to eliminate mutual interference error between rotation
and translation by separately creating two individual correspondence matrices.
For partial-to-partial registration, we consider overlap prediction as a
preordering task before the registration procedure. Accordingly, we present an
overlap predictor that benefits from explicit feature interaction, which is
achieved by the powerful attention mechanism to accurately predict pointwise
masks. Furthermore, we design a multi-resolution feature extraction network to
capture both local and global patterns thus enhancing both overlap prediction
and registration module. Experimental results on both synthetic and real
datasets validate the effectiveness of our proposed method.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11738" title="Abstract">arXiv:2310.11738</a> [<a href="/pdf/2310.11738" title="Download PDF">pdf</a>, <a href="/format/2310.11738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Power of Clippy in Real-World Rust Projects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunmiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yijun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haitao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Carlig%2C+L">Luca Carlig</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+S">Shijie Nie</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lingxiao Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Clippy lints are considered as essential tools for Rust developers, as they
can be configured as gate-keeping rules for a Rust project during continuous
integration. Despite their availability, little was known about practical
application and cost-effectiveness of the lints in reducing code quality
issues. In this study, we embark on a comprehensive analysis to unveil the true
impact of Clippy lints in the Rust development landscape. The study is
structured around three interrelated components, each contributing to the
overall effectiveness of Clippy. Firstly, we conduct a comprehensive analysis
of Clippy lints in all idiomatic crates-io Rust projects with an average
warning density of 21/KLOC. The analysis identifies the most cost-effective
lint fixes, offering valuable opportunities for optimizing code quality.
Secondly, we actively engage Rust developers through a user survey to garner
invaluable feedback on their experiences with Clippy. User insights shed light
on two crucial concerns: the prevalence of false positives in warnings and the
need for auto-fix support for most warnings. Thirdly, building upon these
findings, we engineer three innovative automated refactoring techniques to
effectively fix the four most frequent Clippy lints. As a result, the warning
density in Rosetta benchmarks has significantly decreased from 195/KLOC to an
impressive 18/KLOC, already lower than the average density of the crates-io
Rust projects. These results demonstrate tangible benefit and impact of our
efforts in enhancing the overall code quality and maintainability for Rust
developers.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11739" title="Abstract">arXiv:2310.11739</a> [<a href="/pdf/2310.11739" title="Download PDF">pdf</a>, <a href="/format/2310.11739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unintended Memorization in Large ASR Models, and How to Mitigate It
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+O">Om Thakkar</a>, 
<a href="/search/cs?searchtype=author&query=Mathews%2C+R">Rajiv Mathews</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">It is well-known that neural networks can unintentionally memorize their
training examples, causing privacy concerns. However, auditing memorization in
large non-auto-regressive automatic speech recognition (ASR) models has been
challenging due to the high compute cost of existing methods such as hardness
calibration. In this work, we design a simple auditing method to measure
memorization in large ASR models without the extra compute overhead.
Concretely, we speed up randomly-generated utterances to create a mapping
between vocal and text information that is difficult to learn from typical
training examples. Hence, accurate predictions only for sped-up training
examples can serve as clear evidence for memorization, and the corresponding
accuracy can be used to measure memorization. Using the proposed method, we
showcase memorization in the state-of-the-art ASR models. To mitigate
memorization, we tried gradient clipping during training to bound the influence
of any individual example on the final model. We empirically show that clipping
each example's gradient can mitigate memorization for sped-up training examples
with up to 16 repetitions in the training set. Furthermore, we show that in
large-scale distributed training, clipping the average gradient on each compute
core maintains neutral model quality and compute cost while providing strong
privacy protection.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11740" title="Abstract">arXiv:2310.11740</a> [<a href="/pdf/2310.11740" title="Download PDF">pdf</a>, <a href="/format/2310.11740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A fast normal splitting preconditioner for attractive coupled nonlinear  Schr&#xf6;dinger equations with fractional Laplacian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cheng%2C+Y">Yan Cheng</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+X">Xi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A linearly implicit conservative difference scheme is applied to discretize
the attractive coupled nonlinear Schr\"odinger equations with fractional
Laplacian. Complex symmetric linear systems can be obtained, and the system
matrices are indefinite and Toeplitz-plus-diagonal. Neither efficient
preconditioned iteration method nor fast direct method is available to deal
with these systems. In this paper, we propose a novel matrix splitting
iteration method based on a normal splitting of an equivalent real block form
of the complex linear systems. This new iteration method converges
unconditionally, and the quasi-optimal iteration parameter is deducted. The
corresponding new preconditioner is obtained naturally, which can be
constructed easily and implemented efficiently by fast Fourier transform.
Theoretical analysis indicates that the eigenvalues of the preconditioned
system matrix are tightly clustered. Numerical experiments show that the new
preconditioner can significantly accelerate the convergence rate of the Krylov
subspace iteration methods. Specifically, the convergence behavior of the
related preconditioned GMRES iteration method is spacial mesh-size-independent,
and almost fractional order insensitive. Moreover, the linearly implicit
conservative difference scheme in conjunction with the preconditioned GMRES
iteration method conserves the discrete mass and energy in terms of a given
precision.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11742" title="Abstract">arXiv:2310.11742</a> [<a href="/pdf/2310.11742" title="Download PDF">pdf</a>, <a href="/format/2310.11742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaVis: Adaptive and Explainable Visualization Recommendation for  Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haotian Li</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Huamin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Automated visualization recommendation facilitates the rapid creation of
effective visualizations, which is especially beneficial for users with limited
time and limited knowledge of data visualization. There is an increasing trend
in leveraging machine learning (ML) techniques to achieve an end-to-end
visualization recommendation. However, existing ML-based approaches implicitly
assume that there is only one appropriate visualization for a specific dataset,
which is often not true for real applications. Also, they often work like a
black box, and are difficult for users to understand the reasons for
recommending specific visualizations. To fill the research gap, we propose
AdaVis, an adaptive and explainable approach to recommend one or multiple
appropriate visualizations for a tabular dataset. It leverages a box
embedding-based knowledge graph to well model the possible one-to-many mapping
relations among different entities (i.e., data features, dataset columns,
datasets, and visualization choices). The embeddings of the entities and
relations can be learned from dataset-visualization pairs. Also, AdaVis
incorporates the attention mechanism into the inference framework. Attention
can indicate the relative importance of data features for a dataset and provide
fine-grained explainability. Our extensive evaluations through quantitative
metric evaluations, case studies, and user interviews demonstrate the
effectiveness of AdaVis.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11747" title="Abstract">arXiv:2310.11747</a> [<a href="/pdf/2310.11747" title="Download PDF">pdf</a>, <a href="/format/2310.11747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coded Kalman Filtering Over Gaussian Channels with Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Barron Han</a>, 
<a href="/search/cs?searchtype=author&query=Sabag%2C+O">Oron Sabag</a>, 
<a href="/search/cs?searchtype=author&query=Kostina%2C+V">Victoria Kostina</a>, 
<a href="/search/cs?searchtype=author&query=Hassibi%2C+B">Babak Hassibi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at 59th Allerton Conference on Communication, Control, and Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper investigates the problem of zero-delay joint source-channel coding
of a vector Gauss-Markov source over a multiple-input multiple-output (MIMO)
additive white Gaussian noise (AWGN) channel with feedback. In contrast to the
classical problem of causal estimation using noisy observations, we examine a
system where the source can be encoded before transmission. An encoder,
equipped with feedback of past channel outputs, observes the source state and
encodes the information in a causal manner as inputs to the channel while
adhering to a power constraint. The objective of the code is to estimate the
source state with minimum mean square error at the infinite horizon. This work
shows a fundamental theorem for two scenarios: for the transmission of an
unstable vector Gauss-Markov source over either a multiple-input single-output
(MISO) or a single-input multiple-output (SIMO) AWGN channel, finite estimation
error is achievable if and only if the sum of logs of the unstable eigenvalues
of the state gain matrix is less than the Shannon channel capacity. We prove
these results by showing an optimal linear innovations encoder that can be
applied to sources and channels of any dimension and analyzing it together with
the corresponding Kalman filter decoder.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11748" title="Abstract">arXiv:2310.11748</a> [<a href="/pdf/2310.11748" title="Download PDF">pdf</a>, <a href="/format/2310.11748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BanglaAbuseMeme: A Dataset for Bengali Abusive Meme Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+M">Mithun Das</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (main conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The dramatic increase in the use of social media platforms for information
sharing has also fueled a steep growth in online abuse. A simple yet effective
way of abusing individuals or communities is by creating memes, which often
integrate an image with a short piece of text layered on top of it. Such
harmful elements are in rampant use and are a threat to online safety. Hence it
is necessary to develop efficient models to detect and flag abusive memes. The
problem becomes more challenging in a low-resource setting (e.g., Bengali
memes, i.e., images with Bengali text embedded on it) because of the absence of
benchmark datasets on which AI models could be trained. In this paper we bridge
this gap by building a Bengali meme dataset. To setup an effective benchmark we
implement several baseline models for classifying abusive memes using this
dataset. We observe that multimodal models that use both textual and visual
information outperform unimodal models. Our best-performing model achieves a
macro F1 score of 70.51. Finally, we perform a qualitative error analysis of
the misclassified memes of the best-performing text-based, image-based and
multimodal models.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11749" title="Abstract">arXiv:2310.11749</a> [<a href="/pdf/2310.11749" title="Download PDF">pdf</a>, <a href="/format/2310.11749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Material Properties of Interacting Objects Using Sum-GP-UCB
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seker%2C+M+Y">M. Yunus Seker</a>, 
<a href="/search/cs?searchtype=author&query=Kroemer%2C+O">Oliver Kroemer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Robots need to estimate the material and dynamic properties of objects from
observations in order to simulate them accurately. We present a Bayesian
optimization approach to identifying the material property parameters of
objects based on a set of observations. Our focus is on estimating these
properties based on observations of scenes with different sets of interacting
objects. We propose an approach that exploits the structure of the reward
function by modeling the reward for each observation separately and using only
the parameters of the objects in that scene as inputs. The resulting
lower-dimensional models generalize better over the parameter space, which in
turn results in a faster optimization. To speed up the optimization process
further, and reduce the number of simulation runs needed to find good parameter
values, we also propose partial evaluations of the reward function, wherein the
selected parameters are only evaluated on a subset of real world evaluations.
The approach was successfully evaluated on a set of scenes with a wide range of
object interactions, and we showed that our method can effectively perform
incremental learning without resetting the rewards of the gathered
observations.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11750" title="Abstract">arXiv:2310.11750</a> [<a href="/pdf/2310.11750" title="Download PDF">pdf</a>, <a href="/ps/2310.11750" title="Download PostScript">ps</a>, <a href="/format/2310.11750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Min-max Decoding Error Probability Optimization in RIS-Aided Hybrid  TDMA-NOMA Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+T+H+T">Tra Huong Thi Le</a>, 
<a href="/search/cs?searchtype=author&query=Tun%2C+Y+K">Yan Kyaw Tun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">One of the primary objectives for future wireless communication networks is
to facilitate the provision of ultra-reliable and low-latency communication
services while simultaneously ensuring the capability for vast connection. In
order to achieve this objective, we examine a hybrid multi-access scheme inside
the finite blocklength (FBL) regime. This system combines the benefits of
non-orthogonal multiple access (NOMA) and time-division multiple access (TDMA)
schemes with the aim of fulfilling the objectives of future wireless
communication networks. In addition, a reconfigurable intelligent surface (RIS)
is utilized to facilitate the establishment of the uplink transmission between
the base station and mobile devices in situations when impediments impede their
direct communication linkages. This paper aims to minimize the worst-case
decoding-error probability for all mobile users by jointly optimizing power
allocation, receiving beamforming, blocklength, RIS reflection, and user
pairing. To deal with the coupled variables in the formulated mixed-integer
non-convex optimization problem, we decompose it into three sub-problems,
namely, 1) decoding order determination problem, 2) joint power allocation,
receiving beamforming, RIS reflection, and blocklength optimization problem,
and 3) optimal user pairing problem. Then, we provide the sequential convex
approximation (SCA) and semidefinite relaxation (SDR)-based algorithms as
potential solutions for iteratively addressing the deconstructed first two
sub-problems at a fixed random user pairing. In addition, the Hungarian
matching approach is employed to address the challenge of optimizing user
pairing. In conclusion, we undertake a comprehensive simulation, which reveals
the advantageous qualities of the proposed algorithm and its superior
performance compared to existing benchmark methods.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11751" title="Abstract">arXiv:2310.11751</a> [<a href="/pdf/2310.11751" title="Download PDF">pdf</a>, <a href="/format/2310.11751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Elicitation from Decentralized Crowd Without Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kexin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jianwei Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Information Elicitation Without Verification (IEWV) refers to the problem of
eliciting high-accuracy solutions from crowd members when the ground truth is
unverifiable. A high-accuracy team solution (aggregated from members'
solutions) requires members' effort exertion, which should be incentivized
properly. Previous research on IEWV mainly focused on scenarios where a central
entity (e.g., the crowdsourcing platform) provides incentives to motivate crowd
members. Still, the proposed designs do not apply to practical situations where
no central entity exists. This paper studies the overlooked decentralized IEWV
scenario, where crowd members act as both incentive contributors and task
solvers. We model the interactions among members with heterogeneous team
solution accuracy valuations as a two-stage game, where each member decides her
incentive contribution strategy in Stage 1 and her effort exertion strategy in
Stage 2. We analyze members' equilibrium behaviors under three incentive
allocation mechanisms: Equal Allocation (EA), Output Agreement (OA), and
Shapley Value (SV). We show that at an equilibrium under any allocation
mechanism, a low-valuation member exerts no more effort than a high-valuation
member. Counter-intuitively, a low-valuation member provides incentives to the
collaboration while a high-valuation member does not at an equilibrium under
SV. This is because a high-valuation member who values the aggregated team
solution more needs fewer incentives to exert effort. In addition, when
members' valuations are sufficiently heterogeneous, SV leads to team solution
accuracy and social welfare no smaller than EA and OA.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11753" title="Abstract">arXiv:2310.11753</a> [<a href="/pdf/2310.11753" title="Download PDF">pdf</a>, <a href="/format/2310.11753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bias in Emotion Recognition with ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wake%2C+N">Naoki Wake</a>, 
<a href="/search/cs?searchtype=author&query=Kanehira%2C+A">Atsushi Kanehira</a>, 
<a href="/search/cs?searchtype=author&query=Sasabuchi%2C+K">Kazuhiro Sasabuchi</a>, 
<a href="/search/cs?searchtype=author&query=Takamatsu%2C+J">Jun Takamatsu</a>, 
<a href="/search/cs?searchtype=author&query=Ikeuchi%2C+K">Katsushi Ikeuchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">This technical report explores the ability of ChatGPT in recognizing emotions
from text, which can be the basis of various applications like interactive
chatbots, data annotation, and mental health analysis. While prior research has
shown ChatGPT's basic ability in sentiment analysis, its performance in more
nuanced emotion recognition is not yet explored. Here, we conducted experiments
to evaluate its performance of emotion recognition across different datasets
and emotion labels. Our findings indicate a reasonable level of reproducibility
in its performance, with noticeable improvement through fine-tuning. However,
the performance varies with different emotion labels and datasets, highlighting
an inherent instability and possible bias. The choice of dataset and emotion
labels significantly impacts ChatGPT's emotion recognition performance. This
paper sheds light on the importance of dataset and label selection, and the
potential of fine-tuning in enhancing ChatGPT's emotion recognition
capabilities, providing a groundwork for better integration of emotion analysis
in applications using ChatGPT.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11755" title="Abstract">arXiv:2310.11755</a> [<a href="/pdf/2310.11755" title="Download PDF">pdf</a>, <a href="/format/2310.11755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RGM: A Robust Generalist Matching Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xinyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages. Code is available at: <a href="https://github.com/aim-uofa/RGM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Finding corresponding pixels within a pair of images is a fundamental
computer vision task with various applications. Due to the specific
requirements of different tasks like optical flow estimation and local feature
matching, previous works are primarily categorized into dense matching and
sparse feature matching focusing on specialized architectures along with
task-specific datasets, which may somewhat hinder the generalization
performance of specialized models. In this paper, we propose a deep model for
sparse and dense matching, termed RGM (Robust Generalist Matching). In
particular, we elaborately design a cascaded GRU module for refinement by
exploring the geometric similarity iteratively at multiple scales following an
additional uncertainty estimation module for sparsification. To narrow the gap
between synthetic training samples and real-world scenarios, we build a new,
large-scale dataset with sparse correspondence ground truth by generating
optical flow supervision with greater intervals. As such, we are able to mix up
various dense and sparse matching datasets, significantly improving the
training diversity. The generalization capacity of our proposed RGM is greatly
improved by learning the matching and uncertainty estimation in a two-stage
manner on the large, mixed data. Superior performance is achieved for zero-shot
matching and downstream geometry estimation across multiple datasets,
outperforming the previous methods by a large margin.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11758" title="Abstract">arXiv:2310.11758</a> [<a href="/pdf/2310.11758" title="Download PDF">pdf</a>, <a href="/format/2310.11758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-Generalized Face Anti-Spoofing with Unknown Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Z">Zong-Wei Hong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yu-Chen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hsuan-Tung Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+Y">Yi-Ren Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chu-Song Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE International Conference on Image Processing (ICIP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Although face anti-spoofing (FAS) methods have achieved remarkable
performance on specific domains or attack types, few studies have focused on
the simultaneous presence of domain changes and unknown attacks, which is
closer to real application scenarios. To handle domain-generalized unknown
attacks, we introduce a new method, DGUA-FAS, which consists of a
Transformer-based feature extractor and a synthetic unknown attack sample
generator (SUASG). The SUASG network simulates unknown attack samples to assist
the training of the feature extractor. Experimental results show that our
method achieves superior performance on domain generalization FAS with known or
unknown attacks.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11760" title="Abstract">arXiv:2310.11760</a> [<a href="/pdf/2310.11760" title="Download PDF">pdf</a>, <a href="/ps/2310.11760" title="Download PostScript">ps</a>, <a href="/format/2310.11760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Security-Constrained Optimal Power Management Algorithm for Shipboard  Microgrids with Battery Energy Storage System and Fuel Cell
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=D%27Agostino%2C+F">Fabio D&#x27;Agostino</a>, 
<a href="/search/eess?searchtype=author&query=Gallo%2C+M">Marco Gallo</a>, 
<a href="/search/eess?searchtype=author&query=Saviozzi%2C+M">Matteo Saviozzi</a>, 
<a href="/search/eess?searchtype=author&query=Silvestro%2C+F">Federico Silvestro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2304.03621">arXiv:2304.03621</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This work proposes an optimal power management strategy for shipboard
microgrids equipped with diesel generators, a fuel cell and a battery energy
storage system. The optimization aims to determine both the unit commitment and
the optimal power dispatch for all resources to ensure a reliable power supply
at minimum cost and with minimal environmental impact. This strategy takes into
account the zero-emission capability of the ship and incorporates a soft
constraint related to the ship's speed. The optimization is performed solving a
mixed integer linear programming problem, where the constraints are defined
according to the operational limits of the resources when a contingency occurs.
The algorithm is tested on a notional all-electric ship where the electrical
load is generated through a Markov chain, modelled on real measurement data.
The results show that the proposed power management strategy successfully
maximizes fuel and emission savings while ensuring blackout prevention
capability.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11761" title="Abstract">arXiv:2310.11761</a> [<a href="/pdf/2310.11761" title="Download PDF">pdf</a>, <a href="/format/2310.11761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Evaluation of Large Language Models on Legal Judgment  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shui%2C+R">Ruihao Shui</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated great potential for
domain-specific applications, such as the law domain. However, recent disputes
over GPT-4's law evaluation raise questions concerning their performance in
real-world legal tasks. To systematically investigate their competency in the
law, we design practical baseline solutions based on LLMs and test on the task
of legal judgment prediction. In our solutions, LLMs can work alone to answer
open questions or coordinate with an information retrieval (IR) system to learn
from similar cases or solve simplified multi-choice questions. We show that
similar cases and multi-choice options, namely label candidates, included in
prompts can help LLMs recall domain knowledge that is critical for expertise
legal reasoning. We additionally present an intriguing paradox wherein an IR
system surpasses the performance of LLM+IR due to limited gains acquired by
weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes
redundant. Our evaluation pipeline can be easily extended into other tasks to
facilitate evaluations in other domains. Code is available at
https://github.com/srhthu/LM-CompEval-Legal
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11762" title="Abstract">arXiv:2310.11762</a> [<a href="/pdf/2310.11762" title="Download PDF">pdf</a>, <a href="/format/2310.11762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quasi-Wasserstein Loss for Learning Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minjie Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongteng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">When learning graph neural networks (GNNs) in node-level prediction tasks,
most existing loss functions are applied for each node independently, even if
node embeddings and their labels are non-i.i.d. because of their graph
structures. To eliminate such inconsistency, in this study we propose a novel
Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on
graphs, leading to new learning and prediction paradigms of GNNs. In
particular, we design a "Quasi-Wasserstein" distance between the observed
multi-dimensional node labels and their estimations, optimizing the label
transport defined on graph edges. The estimations are parameterized by a GNN in
which the optimal label transport may determine the graph edge weights
optionally. By reformulating the strict constraint of the label transport to a
Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein
loss associated with two efficient solvers learning the GNN together with
optimal label transport. When predicting node labels, our model combines the
output of the GNN with the residual component provided by the optimal label
transport, leading to a new transductive prediction paradigm. Experiments show
that the proposed QW loss applies to various GNNs and helps to improve their
performance in node-level classification and regression tasks.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11763" title="Abstract">arXiv:2310.11763</a> [<a href="/pdf/2310.11763" title="Download PDF">pdf</a>, <a href="/format/2310.11763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PhishReplicant: A Language Model-based Approach to Detect Generated  Squatting Domain Names
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koide%2C+T">Takashi Koide</a>, 
<a href="/search/cs?searchtype=author&query=Fukushi%2C+N">Naoki Fukushi</a>, 
<a href="/search/cs?searchtype=author&query=Nakano%2C+H">Hiroki Nakano</a>, 
<a href="/search/cs?searchtype=author&query=Chiba%2C+D">Daiki Chiba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACSAC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Domain squatting is a technique used by attackers to create domain names for
phishing sites. In recent phishing attempts, we have observed many domain names
that use multiple techniques to evade existing methods for domain squatting.
These domain names, which we call generated squatting domains (GSDs), are quite
different in appearance from legitimate domain names and do not contain brand
names, making them difficult to associate with phishing. In this paper, we
propose a system called PhishReplicant that detects GSDs by focusing on the
linguistic similarity of domain names. We analyzed newly registered and
observed domain names extracted from certificate transparency logs, passive
DNS, and DNS zone files. We detected 3,498 domain names acquired by attackers
in a four-week experiment, of which 2,821 were used for phishing sites within a
month of detection. We also confirmed that our proposed system outperformed
existing systems in both detection accuracy and number of domain names
detected. As an in-depth analysis, we examined 205k GSDs collected over 150
days and found that phishing using GSDs was distributed globally. However,
attackers intensively targeted brands in specific regions and industries. By
analyzing GSDs in real time, we can block phishing sites before or immediately
after they appear.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11766" title="Abstract">arXiv:2310.11766</a> [<a href="/pdf/2310.11766" title="Download PDF">pdf</a>, <a href="/format/2310.11766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi Task Consistency Guided Source-Free Test-Time Domain Adaptation  Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yanyu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenxi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+C">Chunna Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages,7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Source-free test-time adaptation for medical image segmentation aims to
enhance the adaptability of segmentation models to diverse and previously
unseen test sets of the target domain, which contributes to the
generalizability and robustness of medical image segmentation models without
access to the source domain. Ensuring consistency between target edges and
paired inputs is crucial for test-time adaptation. To improve the performance
of test-time domain adaptation, we propose a multi task consistency guided
source-free test-time domain adaptation medical image segmentation method which
ensures the consistency of the local boundary predictions and the global
prototype representation. Specifically, we introduce a local boundary
consistency constraint method that explores the relationship between tissue
region segmentation and tissue boundary localization tasks. Additionally, we
propose a global feature consistency constraint toto enhance the intra-class
compactness. We conduct extensive experiments on the segmentation of benchmark
fundus images. Compared to prediction directly by the source domain model, the
segmentation Dice score is improved by 6.27\% and 0.96\% in RIM-ONE-r3 and
Drishti GS datasets, respectively. Additionally, the results of experiments
demonstrate that our proposed method outperforms existing competitive domain
adaptation segmentation algorithms.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11768" title="Abstract">arXiv:2310.11768</a> [<a href="/pdf/2310.11768" title="Download PDF">pdf</a>, <a href="/format/2310.11768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Classification of Weierstrass Elliptic Curves over $\mathbb{Z}_n$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parekh%2C+P">Param Parekh</a>, 
<a href="/search/cs?searchtype=author&query=Parekh%2C+P">Paavan Parekh</a>, 
<a href="/search/cs?searchtype=author&query=Deb%2C+S">Sourav Deb</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M+K">Manish K Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 figures, draft
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT); Algebraic Geometry (math.AG); Number Theory (math.NT)

</div>
<p class="mathjax">The development of secure cryptographic protocols and the subsequent attack
mechanisms have been placed in the literature with the utmost curiosity.
<br />While sophisticated quantum attacks bring a concern to the classical
cryptographic protocols present in the applications used in everyday life, the
necessity of developing post-quantum protocols is felt primarily.
<br />In post-quantum cryptography, elliptic curve-base protocols are exciting to
the researchers.
<br />While the comprehensive study of elliptic curves over finite fields is well
known, the extended study over finite rings is still missing.
<br />In this work, we generalize the study of Weierstrass elliptic curves over
finite ring $\mathbb{Z}_n$ through classification.
<br />Several expressions to compute critical factors in studying elliptic curves
are conferred.
<br />An all-around computational classification on the Weierstrass elliptic curves
over $\mathbb{Z}_n$ for rigorous understanding is also attached to this work.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11769" title="Abstract">arXiv:2310.11769</a> [<a href="/pdf/2310.11769" title="Download PDF">pdf</a>, <a href="/format/2310.11769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Annotated Job Ads with Named Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stollenwerk%2C+F">Felix Stollenwerk</a>, 
<a href="/search/cs?searchtype=author&query=Fastlund%2C+N">Niklas Fastlund</a>, 
<a href="/search/cs?searchtype=author&query=Nyqvist%2C+A">Anna Nyqvist</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96hman%2C+J">Joey &#xd6;hman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SLTC 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We have trained a named entity recognition (NER) model that screens Swedish
job ads for different kinds of useful information (e.g. skills required from a
job seeker). It was obtained by fine-tuning KB-BERT. The biggest challenge we
faced was the creation of a labelled dataset, which required manual annotation.
This paper gives an overview of the methods we employed to make the annotation
process more efficient and to ensure high quality data. We also report on the
performance of the resulting model.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11770" title="Abstract">arXiv:2310.11770</a> [<a href="/pdf/2310.11770" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Telecom AI Native Systems in the Age of Generative AI -- An Engineering  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Britto%2C+R">Ricardo Britto</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+T">Timothy Murphy</a>, 
<a href="/search/cs?searchtype=author&query=Iovene%2C+M">Massimo Iovene</a>, 
<a href="/search/cs?searchtype=author&query=Jonsson%2C+L">Leif Jonsson</a>, 
<a href="/search/cs?searchtype=author&query=Erol-Kantarci%2C+M">Melike Erol-Kantarci</a>, 
<a href="/search/cs?searchtype=author&query=Kov%C3%A1cs%2C+B">Benedek Kov&#xe1;cs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid advancements in Artificial Intelligence (AI), particularly in
generative AI and foundational models (FMs), have ushered in transformative
changes across various industries. Large language models (LLMs), a type of FM,
have demonstrated their prowess in natural language processing tasks and
content generation, revolutionizing how we interact with software products and
services. This article explores the integration of FMs in the
telecommunications industry, shedding light on the concept of AI native telco,
where AI is seamlessly woven into the fabric of telecom products. It delves
into the engineering considerations and unique challenges associated with
implementing FMs into the software life cycle, emphasizing the need for AI
native-first approaches. Despite the enormous potential of FMs, ethical,
regulatory, and operational challenges require careful consideration,
especially in mission-critical telecom contexts. As the telecom industry seeks
to harness the power of AI, a comprehensive understanding of these challenges
is vital to thrive in a fiercely competitive market.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11772" title="Abstract">arXiv:2310.11772</a> [<a href="/pdf/2310.11772" title="Download PDF">pdf</a>, <a href="/format/2310.11772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Long Document Topic Segmentation Models With Enhanced  Coherence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023. Codes is available at <a href="https://github.com/alibaba-damo-academy/SpokenNLP/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Topic segmentation is critical for obtaining structured long documents and
improving downstream tasks like information retrieval. Due to its ability of
automatically exploring clues of topic shift from a large amount of labeled
data, recent supervised neural models have greatly promoted the development of
long document topic segmentation, but leaving the deeper relationship of
semantic coherence and topic segmentation underexplored. Therefore, this paper
enhances the supervised model's ability to capture coherence from both
structure and similarity perspectives to further improve the topic segmentation
performance, including the Topic-aware Sentence Structure Prediction (TSSP) and
Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is
proposed to force the model to comprehend structural information by learning
the original relations of adjacent sentences in a disarrayed document, which is
constructed by jointly disrupting the original document at the topic and
sentence levels. In addition, we utilize inter- and intra-topic information to
construct contrastive samples and design the CSSL objective to ensure that the
sentences representations in the same topic have higher semantic similarity,
while those in different topics are less similar. Extensive experiments show
that the Longformer with our approach significantly outperforms old
state-of-the-art (SOTA) methods. Our approach improves $F_{1}$ of old SOTA by
3.42 (73.74 -&gt; 77.16) and reduces $P_{k}$ by 1.11 points (15.0 -&gt; 13.89) on
WIKI-727K and achieves an average reduction of 0.83 points on $P_{k}$ on
WikiSection. The average $P_{k}$ drop of 2.82 points on the two out-of-domain
datasets also illustrates the robustness of our approach
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11777" title="Abstract">arXiv:2310.11777</a> [<a href="/pdf/2310.11777" title="Download PDF">pdf</a>, <a href="/format/2310.11777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DCRNN: A Deep Cross approach based on RNN for Partial Parameter Sharing  in Multi-task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qian Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work done while the first author was an algorithm engineer at Xiaomi Inc
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">In recent years, DL has developed rapidly, and personalized services are
exploring using DL algorithms to improve the performance of the recommendation
system. For personalized services, a successful recommendation consists of two
parts: attracting users to click the item and users being willing to consume
the item. If both tasks need to be predicted at the same time, traditional
recommendation systems generally train two independent models. This approach is
cumbersome and does not effectively model the relationship between the two
subtasks of "click-consumption". Therefore, in order to improve the success
rate of recommendation and reduce computational costs, researchers are trying
to model multi-task learning.
<br />At present, existing multi-task learning models generally adopt hard
parameter sharing or soft parameter sharing architecture, but these two
architectures each have certain problems. Therefore, in this work, we propose a
novel recommendation model based on real recommendation scenarios, Deep Cross
network based on RNN for partial parameter sharing (DCRNN). The model has three
innovations: 1) It adopts the idea of cross network and uses RNN network to
cross-process the features, thereby effectively improves the expressive ability
of the model; 2) It innovatively proposes the structure of partial parameter
sharing; 3) It can effectively capture the potential correlation between
different tasks to optimize the efficiency and methods for learning different
tasks.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11778" title="Abstract">arXiv:2310.11778</a> [<a href="/pdf/2310.11778" title="Download PDF">pdf</a>, <a href="/format/2310.11778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Agents for Detecting Implicit Stereotypes in Text-to-image  Models at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+T">Tian Bian</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yian Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tingyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H+M">Helen M. Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bingzhe Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The recent surge in the research of diffusion models has accelerated the
adoption of text-to-image models in various Artificial Intelligence Generated
Content (AIGC) commercial products. While these exceptional AIGC products are
gaining increasing recognition and sparking enthusiasm among consumers, the
questions regarding whether, when, and how these models might unintentionally
reinforce existing societal stereotypes remain largely unaddressed. Motivated
by recent advancements in language agents, here we introduce a novel agent
architecture tailored for stereotype detection in text-to-image models. This
versatile agent architecture is capable of accommodating free-form detection
tasks and can autonomously invoke various tools to facilitate the entire
process, from generating corresponding instructions and images, to detecting
stereotypes. We build the stereotype-relevant benchmark based on multiple
open-text datasets, and apply this architecture to commercial products and
popular open source text-to-image models. We find that these models often
display serious stereotypes when it comes to certain prompts about personal
characteristics, social cultural context and crime-related aspects. In summary,
these empirical findings underscore the pervasive existence of stereotypes
across social dimensions, including gender, race, and religion, which not only
validate the effectiveness of our proposed approach, but also emphasize the
critical necessity of addressing potential ethical risks in the burgeoning
realm of AIGC. As AIGC continues its rapid expansion trajectory, with new
models and plugins emerging daily in staggering numbers, the challenge lies in
the timely detection and mitigation of potential biases within these models.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11780" title="Abstract">arXiv:2310.11780</a> [<a href="/pdf/2310.11780" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Annotation Handbook: A Practical Guide for Machine Learning  Projects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stollenwerk%2C+F">Felix Stollenwerk</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96hman%2C+J">Joey &#xd6;hman</a>, 
<a href="/search/cs?searchtype=author&query=Petrelli%2C+D">Danila Petrelli</a>, 
<a href="/search/cs?searchtype=author&query=Waller%C3%B6%2C+E">Emma Waller&#xf6;</a>, 
<a href="/search/cs?searchtype=author&query=Olsson%2C+F">Fredrik Olsson</a>, 
<a href="/search/cs?searchtype=author&query=Bengtsson%2C+C">Camilla Bengtsson</a>, 
<a href="/search/cs?searchtype=author&query=Horndahl%2C+A">Andreas Horndahl</a>, 
<a href="/search/cs?searchtype=author&query=Gandler%2C+G+Z">Gabriela Zarzar Gandler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, white paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This handbook is a hands-on guide on how to approach text annotation tasks.
It provides a gentle introduction to the topic, an overview of theoretical
concepts as well as practical advice. The topics covered are mostly technical,
but business, ethical and regulatory issues are also touched upon. The focus
lies on readability and conciseness rather than completeness and scientific
rigor. Experience with annotation and knowledge of machine learning are useful
but not required. The document may serve as a primer or reference book for a
wide range of professions such as team leaders, project managers, IT
architects, software developers and machine learning engineers.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11781" title="Abstract">arXiv:2310.11781</a> [<a href="/pdf/2310.11781" title="Download PDF">pdf</a>, <a href="/format/2310.11781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blind estimation of audio effects using an auto-encoder approach and  differentiable signal processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peladeau%2C+C">C&#xf4;me Peladeau</a>, 
<a href="/search/cs?searchtype=author&query=Peeters%2C+G">Geoffroy Peeters</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Blind Estimation of Audio Effects (BE-AFX) aims at estimating the Audio
Effects (AFXs) applied to an original, unprocessed audio sample solely based on
the processed audio sample. To train such a system traditional approaches
optimize a loss between ground truth and estimated AFX parameters. This
involves knowing the exact implementation of the AFXs used for the process. In
this work, we propose an alternative solution that eliminates the requirement
for knowing this implementation. Instead, we introduce an auto-encoder
approach, which optimizes an audio quality metric. We explore, suggest, and
compare various implementations of commonly used mastering AFXs, using
differential signal processing or neural approximations. Our findings
demonstrate that our auto-encoder approach yields superior estimates of the
audio quality produced by a chain of AFXs, compared to the traditional
parameter-based approach, even if the latter provides a more accurate parameter
estimation.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11784" title="Abstract">arXiv:2310.11784</a> [<a href="/pdf/2310.11784" title="Download PDF">pdf</a>, <a href="/format/2310.11784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive3D: Progressively Local Editing for Text-to-3D Content  Creation with Complex Semantic Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xinhua Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://cxh0519.github.io/projects/Progressive3D/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent text-to-3D generation methods achieve impressive 3D content creation
capacity thanks to the advances in image diffusion models and optimizing
strategies. However, current methods struggle to generate correct 3D content
for a complex prompt in semantics, i.e., a prompt describing multiple
interacted objects binding with different attributes. In this work, we propose
a general framework named Progressive3D, which decomposes the entire generation
into a series of locally progressive editing steps to create precise 3D content
for complex prompts, and we constrain the content change to only occur in
regions determined by user-defined region prompts in each editing step.
Furthermore, we propose an overlapped semantic component suppression technique
to encourage the optimization process to focus more on the semantic differences
between prompts. Extensive experiments demonstrate that the proposed
Progressive3D framework generates precise 3D content for prompts with complex
semantics and is general for various text-to-3D methods driven by different 3D
representations.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11787" title="Abstract">arXiv:2310.11787</a> [<a href="/pdf/2310.11787" title="Download PDF">pdf</a>, <a href="/format/2310.11787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuroCUT: A Neural Approach for Robust Graph Partitioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+R">Rishi Shah</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+K">Krishnanshu Jain</a>, 
<a href="/search/cs?searchtype=author&query=Manchanda%2C+S">Sahil Manchanda</a>, 
<a href="/search/cs?searchtype=author&query=Medya%2C+S">Sourav Medya</a>, 
<a href="/search/cs?searchtype=author&query=Ranu%2C+S">Sayan Ranu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph partitioning aims to divide a graph into $k$ disjoint subsets while
optimizing a specific partitioning objective. The majority of formulations
related to graph partitioning exhibit NP-hardness due to their combinatorial
nature. As a result, conventional approximation algorithms rely on heuristic
methods, sometimes with approximation guarantees and sometimes without.
Unfortunately, traditional approaches are tailored for specific partitioning
objectives and do not generalize well across other known partitioning
objectives from the literature. To overcome this limitation, and learn
heuristics from the data directly, neural approaches have emerged,
demonstrating promising outcomes. In this study, we extend this line of work
through a novel framework, NeuroCut. NeuroCut introduces two key innovations
over prevailing methodologies. First, it is inductive to both graph topology
and the partition count, which is provided at query time. Second, by leveraging
a reinforcement learning based framework over node representations derived from
a graph neural network, NeuroCut can accommodate any optimization objective,
even those encompassing non-differentiable functions. Through empirical
evaluation, we demonstrate that NeuroCut excels in identifying high-quality
partitions, showcases strong generalization across a wide spectrum of
partitioning objectives, and exhibits resilience to topological modifications.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11789" title="Abstract">arXiv:2310.11789</a> [<a href="/pdf/2310.11789" title="Download PDF">pdf</a>, <a href="/format/2310.11789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Training for Physics-Informed Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yao Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shengzhu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhichang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Boying Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Physics-informed neural networks have shown great promise in solving partial
differential equations. However, due to insufficient robustness, vanilla PINNs
often face challenges when solving complex PDEs, especially those involving
multi-scale behaviors or solutions with sharp or oscillatory characteristics.
To address these issues, based on the projected gradient descent adversarial
attack, we proposed an adversarial training strategy for PINNs termed by
AT-PINNs. AT-PINNs enhance the robustness of PINNs by fine-tuning the model
with adversarial samples, which can accurately identify model failure locations
and drive the model to focus on those regions during training. AT-PINNs can
also perform inference with temporal causality by selecting the initial
collocation points around temporal initial values. We implement AT-PINNs to the
elliptic equation with multi-scale coefficients, Poisson equation with
multi-peak solutions, Burgers equation with sharp solutions and the Allen-Cahn
equation. The results demonstrate that AT-PINNs can effectively locate and
reduce failure regions. Moreover, AT-PINNs are suitable for solving complex
PDEs, since locating failure regions through adversarial attacks is independent
of the size of failure regions or the complexity of the distribution.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11790" title="Abstract">arXiv:2310.11790</a> [<a href="/pdf/2310.11790" title="Download PDF">pdf</a>, <a href="/format/2310.11790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite Time Performance Analysis of MIMO Systems Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+S">Shuai Sun</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jiayun Li</a>, 
<a href="/search/eess?searchtype=author&query=Mo%2C+Y">Yilin Mo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper is concerned with the finite time identification performance of an
n dimensional discrete-time Multiple-Input Multiple-Output (MIMO) Linear
Time-Invariant system, with p inputs and m outputs. We prove that the
widely-used Ho-Kalman algorithm and Multivariable Output Error State Space
(MOESP) algorithm are ill-conditioned for MIMO system when n/m or n/p is large.
Moreover, by analyzing the Cramer-Rao bound, we derive a fundamental limit for
identifying the real and stable (or marginally stable) poles of MIMO system and
prove that the sample complexity for any unbiased pole estimation algorithm to
reach a certain level of accuracy explodes superpolynomially with respect to
n/(pm). Numerical results are provided to illustrate the ill-conditionedness of
Ho-Kalman algorithm and MOESP algorithm as well as the fundamental limit on
identification.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11792" title="Abstract">arXiv:2310.11792</a> [<a href="/pdf/2310.11792" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Perceptive Motion Control using Control Barrier Functions with  Analytical Smoothing for Six-Wheeled-Telescopic-Legged Robot Tachyon 3
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takasugi%2C+N">Noriaki Takasugi</a>, 
<a href="/search/cs?searchtype=author&query=Kinoshita%2C+M">Masaya Kinoshita</a>, 
<a href="/search/cs?searchtype=author&query=Kamikawa%2C+Y">Yasuhisa Kamikawa</a>, 
<a href="/search/cs?searchtype=author&query=Tsuzaki%2C+R">Ryoichi Tsuzaki</a>, 
<a href="/search/cs?searchtype=author&query=Sakamoto%2C+A">Atsushi Sakamoto</a>, 
<a href="/search/cs?searchtype=author&query=Kai%2C+T">Toshimitsu Kai</a>, 
<a href="/search/cs?searchtype=author&query=Kawanami%2C+Y">Yasunori Kawanami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures, This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">To achieve safe legged locomotion, it is important to generate motion in
real-time considering various constraints in robots and environments. In this
study, we propose a lightweight real-time perspective motion control system for
the newly developed six-wheeled-telescopic-legged robot, Tachyon 3. In the
proposed method, analytically smoothed constraints including Smooth Separating
Axis Theorem (Smooth SAT) as a novel higher order differentiable collision
detection for 3D shapes is applied to the Control Barrier Function (CBF). The
proposed system integrating the CBF achieves online motion generation in a
short control cycle of 1 ms that satisfies joint limitations, environmental
collision avoidance and safe convex foothold constraints. The efficiency of
Smooth SAT is shown from the collision detection time of 1 us or less and the
CBF constraint computation time for Tachyon3 of several us. Furthermore, the
effectiveness of the proposed system is verified through the stair-climbing
motion, integrating online recognition in a simulation and a real machine.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11797" title="Abstract">arXiv:2310.11797</a> [<a href="/pdf/2310.11797" title="Download PDF">pdf</a>, <a href="/format/2310.11797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Panoptic Out-of-Distribution Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohan%2C+R">Rohit Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraswamy%2C+K">Kiran Kumaraswamy</a>, 
<a href="/search/cs?searchtype=author&query=Hurtado%2C+J+V">Juana Valeria Hurtado</a>, 
<a href="/search/cs?searchtype=author&query=Petek%2C+K">K&#xfc;rsat Petek</a>, 
<a href="/search/cs?searchtype=author&query=Valada%2C+A">Abhinav Valada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep learning has led to remarkable strides in scene understanding with
panoptic segmentation emerging as a key holistic scene interpretation task.
However, the performance of panoptic segmentation is severely impacted in the
presence of out-of-distribution (OOD) objects i.e. categories of objects that
deviate from the training distribution. To overcome this limitation, we propose
Panoptic Out-of Distribution Segmentation for joint pixel-level semantic
in-distribution and out-of-distribution classification with instance
prediction. We extend two established panoptic segmentation benchmarks,
Cityscapes and BDD100K, with out-of-distribution instance segmentation
annotations, propose suitable evaluation metrics, and present multiple strong
baselines. Importantly, we propose the novel PoDS architecture with a shared
backbone, an OOD contextual module for learning global and local OOD object
cues, and dual symmetrical decoders with task-specific heads that employ our
alignment-mismatch strategy for better OOD generalization. Combined with our
data augmentation strategy, this approach facilitates progressive learning of
out-of-distribution objects while maintaining in-distribution performance. We
perform extensive evaluations that demonstrate that our proposed PoDS network
effectively addresses the main challenges and substantially outperforms the
baselines. We make the dataset, code, and trained models publicly available at
<a href="http://pods.cs.uni-freiburg.de.">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11798" title="Abstract">arXiv:2310.11798</a> [<a href="/pdf/2310.11798" title="Download PDF">pdf</a>, <a href="/ps/2310.11798" title="Download PostScript">ps</a>, <a href="/format/2310.11798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auction-Based Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Avni%2C+G">Guy Avni</a>, 
<a href="/search/cs?searchtype=author&query=Mallik%2C+K">Kaushik Mallik</a>, 
<a href="/search/cs?searchtype=author&query=Sadhukhan%2C+S">Suman Sadhukhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Many sequential decision-making tasks require satisfaction of multiple,
partially contradictory objectives. Existing approaches are monolithic, namely
all objectives are fulfilled using a single policy, which is a function that
selects a sequence of actions. We present auction-based scheduling, a modular
framework for multi-objective decision-making problems. Each objective is
fulfilled using a separate policy, and the policies can be independently
created, modified, and replaced. Understandably, different policies with
conflicting goals may choose conflicting actions at a given time. In order to
resolve conflicts, and compose policies, we employ a novel auction-based
mechanism. We allocate a bounded budget to each policy, and at each step, the
policies simultaneously bid from their available budgets for the privilege of
being scheduled and choosing an action. Policies express their scheduling
urgency using their bids and the bounded budgets ensure long-run scheduling
fairness. We lay the foundations of auction-based scheduling using path
planning problems on finite graphs with two temporal objectives. We present
decentralized algorithms to synthesize a pair of policies, their initially
allocated budgets, and bidding strategies. We consider three categories of
decentralized synthesis problems, parameterized by the assumptions that the
policies make on each other: (a) strong synthesis, with no assumptions and
strongest guarantees, (b) assume-admissible synthesis, with weakest rationality
assumptions, and (c) assume-guarantee synthesis, with explicit contract-based
assumptions. For reachability objectives, we show that, surprisingly,
decentralized assume-admissible synthesis is always possible when the
out-degrees of all vertices are at most two.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11802" title="Abstract">arXiv:2310.11802</a> [<a href="/pdf/2310.11802" title="Download PDF">pdf</a>, <a href="/format/2310.11802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> De novo protein design using geometric vector field networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+W">Weian Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Muzhi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zheng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shuaike Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L+Y">Lin Yuanbo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Machine Learning (cs.LG); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Innovations like protein diffusion have enabled significant progress in de
novo protein design, which is a vital topic in life science. These methods
typically depend on protein structure encoders to model residue backbone
frames, where atoms do not exist. Most prior encoders rely on atom-wise
features, such as angles and distances between atoms, which are not available
in this context. Thus far, only several simple encoders, such as IPA, have been
proposed for this scenario, exposing the frame modeling as a bottleneck. In
this work, we proffer the Vector Field Network (VFN), which enables network
layers to perform learnable vector computations between coordinates of
frame-anchored virtual atoms, thus achieving a higher capability for modeling
frames. The vector computation operates in a manner similar to a linear layer,
with each input channel receiving 3D virtual atom coordinates instead of scalar
values. The multiple feature vectors output by the vector computation are then
used to update the residue representations and virtual atom coordinates via
attention aggregation. Remarkably, VFN also excels in modeling both frames and
atoms, as the real atoms can be treated as the virtual atoms for modeling,
positioning VFN as a potential universal encoder. In protein diffusion (frame
modeling), VFN exhibits an impressive performance advantage over IPA, excelling
in terms of both designability (67.04% vs. 53.58%) and diversity (66.54% vs.
51.98%). In inverse folding (frame and atom modeling), VFN outperforms the
previous SoTA model, PiFold (54.7% vs. 51.66%), on sequence recovery rate. We
also propose a method of equipping VFN with the ESM model, which significantly
surpasses the previous ESM-based SoTA (62.67% vs. 55.65%), LM-Design, by a
substantial margin.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11804" title="Abstract">arXiv:2310.11804</a> [<a href="/pdf/2310.11804" title="Download PDF">pdf</a>, <a href="/format/2310.11804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed Neural Network for Acoustic Resonance Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yokota%2C+K">Kazuya Yokota</a>, 
<a href="/search/cs?searchtype=author&query=Kurahashi%2C+T">Takahiko Kurahashi</a>, 
<a href="/search/cs?searchtype=author&query=Abe%2C+M">Masajiro Abe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 14 figures. The following article has been submitted to the Journal of the Acoustical Society of America. After it is published, it will be found at <a href="https://pubs.aip.org/asa/jasa">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This study proposes the physics-informed neural network (PINN) framework to
solve the wave equation for acoustic resonance analysis. ResoNet, the
analytical model proposed in this study, minimizes the loss function for
periodic solutions, in addition to conventional PINN loss functions, thereby
effectively using the function approximation capability of neural networks,
while performing resonance analysis. Additionally, it can be easily applied to
inverse problems. Herein, the resonance in a one-dimensional acoustic tube was
analyzed. The effectiveness of the proposed method was validated through the
forward and inverse analyses of the wave equation with energy-loss terms. In
the forward analysis, the applicability of PINN to the resonance problem was
evaluated by comparison with the finite-difference method. The inverse
analysis, which included the identification of the energy loss term in the wave
equation and design optimization of the acoustic tube, was performed with good
accuracy.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11805" title="Abstract">arXiv:2310.11805</a> [<a href="/pdf/2310.11805" title="Download PDF">pdf</a>, <a href="/format/2310.11805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GMC-Pos: Graph-Based Multi-Robot Coverage Positioning Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pongsirijinda%2C+K">Khattiya Pongsirijinda</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiqiang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Shalihan%2C+M">Muhammad Shalihan</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+B+K+K">Benny Kai Kiat Ng</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+B+P+L">Billy Pik Lik Lau</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+U">U-Xuan Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by the 2023 IEEE International Conference on Robotics and Biomimetics (IEEE ROBIO 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Nowadays, several real-world tasks require adequate environment coverage for
maintaining communication between multiple robots, for example, target search
tasks, environmental monitoring, and post-disaster rescues. In this study, we
look into a situation where there are a human operator and multiple robots, and
we assume that each human or robot covers a certain range of areas. We want
them to maximize their area of coverage collectively. Therefore, in this paper,
we propose the Graph-Based Multi-Robot Coverage Positioning Method (GMC-Pos) to
find strategic positions for robots that maximize the area coverage. Our novel
approach consists of two main modules: graph generation and node selection.
Firstly, graph generation represents the environment using a weighted connected
graph. Then, we present a novel generalized graph-based distance and utilize it
together with the graph degrees to be the conditions for node selection in a
recursive manner. Our method is deployed in three environments with different
settings. The results show that it outperforms the benchmark method by 15.13%
to 24.88% regarding the area coverage percentage.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11806" title="Abstract">arXiv:2310.11806</a> [<a href="/pdf/2310.11806" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical accompanying and inhibiting patterns on the spatial  arrangement of taxis&#x27; local hotspots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiao-Jian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Changjiang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Huanga%2C+Z">Zhou Huanga</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Keli Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Due to the large volume of recording, the complete spontaneity, and the
flexible pick-up and drop-off locations, taxi data portrays a realistic and
detailed picture of urban space use to a certain extent. The spatial
arrangement of pick-up and drop-off hotspots reflects the organizational space,
which has received attention in urban structure studies. Previous studies
mainly explore the hotspots at a large scale by visual analysis or some simple
indexes, where the hotspots usually cover the entire central business district,
train stations, or dense residential areas, reaching a radius of hundreds or
even thousands of meters. However, the spatial arrangement patterns of
small-scale hotspots, reflecting the specific popular pick-up and drop-off
locations, have not received much attention. Using two taxi trajectory datasets
in Wuhan and Beijing, China, this study quantitatively explores the spatial
arrangement of fine-grained pick-up and drop-off local hotspots with different
levels of popularity, where the sizes are adaptively set as 90m*90m in Wuhan
and 105m*105m in Beijing according to the local hotspot identification method.
Results show that popular hotspots tend to be surrounded by less popular
hotspots, but the existence of less popular hotspots is inhibited in regions
with a large number of popular hotspots. We use the terms hierarchical
accompany and inhibiting patterns for these two spatial configurations.
Finally, to uncover the underlying mechanism, a KNN-based model is proposed to
reproduce the spatial distribution of other less popular hotspots according to
the most popular ones. These findings help decision-makers construct reasonable
urban minimum units for precise traffic and disease control, as well as plan a
more humane spatial arrangement of points of interest.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11811" title="Abstract">arXiv:2310.11811</a> [<a href="/pdf/2310.11811" title="Download PDF">pdf</a>, <a href="/format/2310.11811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShapeGraFormer: GraFormer-Based Network for Hand-Object Reconstruction  from a Single Depth Map
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aboukhadra%2C+A+T">Ahmed Tawfik Aboukhadra</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+J">Jameel Malik</a>, 
<a href="/search/cs?searchtype=author&query=Robertini%2C+N">Nadia Robertini</a>, 
<a href="/search/cs?searchtype=author&query=Elhayek%2C+A">Ahmed Elhayek</a>, 
<a href="/search/cs?searchtype=author&query=Stricker%2C+D">Didier Stricker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">3D reconstruction of hand-object manipulations is important for emulating
human actions. Most methods dealing with challenging object manipulation
scenarios, focus on hands reconstruction in isolation, ignoring physical and
kinematic constraints due to object contact. Some approaches produce more
realistic results by jointly reconstructing 3D hand-object interactions.
However, they focus on coarse pose estimation or rely upon known hand and
object shapes. We propose the first approach for realistic 3D hand-object shape
and pose reconstruction from a single depth map. Unlike previous work, our
voxel-based reconstruction network regresses the vertex coordinates of a hand
and an object and reconstructs more realistic interaction. Our pipeline
additionally predicts voxelized hand-object shapes, having a one-to-one mapping
to the input voxelized depth. Thereafter, we exploit the graph nature of the
hand and object shapes, by utilizing the recent GraFormer network with
positional embedding to reconstruct shapes from template meshes. In addition,
we show the impact of adding another GraFormer component that refines the
reconstructed shapes based on the hand-object interactions and its ability to
reconstruct more accurate object shapes. We perform an extensive evaluation on
the HO-3D and DexYCB datasets and show that our method outperforms existing
approaches in hand reconstruction and produces plausible reconstructions for
the objects
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11812" title="Abstract">arXiv:2310.11812</a> [<a href="/pdf/2310.11812" title="Download PDF">pdf</a>, <a href="/format/2310.11812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Problems in (Hyper)Graph Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ajwani%2C+D">Deepak Ajwani</a>, 
<a href="/search/cs?searchtype=author&query=Bisseling%2C+R+H">Rob H. Bisseling</a>, 
<a href="/search/cs?searchtype=author&query=Casel%2C+K">Katrin Casel</a>, 
<a href="/search/cs?searchtype=author&query=%C3%87ataly%C3%BCrek%2C+%C3%9C+V">&#xdc;mit V. &#xc7;ataly&#xfc;rek</a>, 
<a href="/search/cs?searchtype=author&query=Chevalier%2C+C">C&#xe9;dric Chevalier</a>, 
<a href="/search/cs?searchtype=author&query=Chudigiewitsch%2C+F">Florian Chudigiewitsch</a>, 
<a href="/search/cs?searchtype=author&query=Faraj%2C+M+F">Marcelo Fonseca Faraj</a>, 
<a href="/search/cs?searchtype=author&query=Fellows%2C+M">Michael Fellows</a>, 
<a href="/search/cs?searchtype=author&query=Gottesb%C3%BCren%2C+L">Lars Gottesb&#xfc;ren</a>, 
<a href="/search/cs?searchtype=author&query=Heuer%2C+T">Tobias Heuer</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>, 
<a href="/search/cs?searchtype=author&query=Kaya%2C+K">Kamer Kaya</a>, 
<a href="/search/cs?searchtype=author&query=Lacki%2C+J">Jakub Lacki</a>, 
<a href="/search/cs?searchtype=author&query=Langguth%2C+J">Johannes Langguth</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X+S">Xiaoye Sherry Li</a>, 
<a href="/search/cs?searchtype=author&query=Mayer%2C+R">Ruben Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Meintrup%2C+J">Johannes Meintrup</a>, 
<a href="/search/cs?searchtype=author&query=Mizutani%2C+Y">Yosuke Mizutani</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrini%2C+F">Fran&#xe7;ois Pellegrini</a>, 
<a href="/search/cs?searchtype=author&query=Petrini%2C+F">Fabrizio Petrini</a>, 
<a href="/search/cs?searchtype=author&query=Rosamond%2C+F">Frances Rosamond</a>, 
<a href="/search/cs?searchtype=author&query=Safro%2C+I">Ilya Safro</a>, 
<a href="/search/cs?searchtype=author&query=Schlag%2C+S">Sebastian Schlag</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+C">Christian Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Roohani Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Strash%2C+D">Darren Strash</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+B+D">Blair D. Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=U%C3%A7ar%2C+B">Bora U&#xe7;ar</a>, 
<a href="/search/cs?searchtype=author&query=Yzelman%2C+A">Albert-Jan Yzelman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Large networks are useful in a wide range of applications. Sometimes problem
instances are composed of billions of entities. Decomposing and analyzing these
structures helps us gain new insights about our surroundings. Even if the final
application concerns a different problem (such as traversal, finding paths,
trees, and flows), decomposing large graphs is often an important subproblem
for complexity reduction or parallelization. This report is a summary of
discussions that happened at Dagstuhl seminar 23331 on "Recent Trends in Graph
Decomposition" and presents currently open problems and future directions in
the area of (hyper)graph decomposition.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11815" title="Abstract">arXiv:2310.11815</a> [<a href="/pdf/2310.11815" title="Download PDF">pdf</a>, <a href="/format/2310.11815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conservative Predictions on Noisy Financial Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nabar%2C+O">Omkar Nabar</a>, 
<a href="/search/cs?searchtype=author&query=Shroff%2C+G">Gautam Shroff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACM ICAIF 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Price movements in financial markets are well known to be very noisy. As a
result, even if there are, on occasion, exploitable patterns that could be
picked up by machine-learning algorithms, these are obscured by feature and
label noise rendering the predictions less useful, and risky in practice.
Traditional rule-learning techniques developed for noisy data, such as CN2,
would seek only high precision rules and refrain from making predictions where
their antecedents did not apply. We apply a similar approach, where a model
abstains from making a prediction on data points that it is uncertain on.
During training, a cascade of such models are learned in sequence, similar to
rule lists, with each model being trained only on data on which the previous
model(s) were uncertain. Similar pruning of data takes place at test-time, with
(higher accuracy) predictions being made albeit only on a fraction (support) of
test-time data. In a financial prediction setting, such an approach allows
decisions to be taken only when the ensemble model is confident, thereby
reducing risk. We present results using traditional MLPs as well as
differentiable decision trees, on synthetic data as well as real financial
market data, to predict fixed-term returns using commonly used features. We
submit that our approach is likely to result in better overall returns at a
lower level of risk. In this context we introduce an utility metric to measure
the average gain per trade, as well as the return adjusted for downside risk,
both of which are improved significantly by our approach.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11818" title="Abstract">arXiv:2310.11818</a> [<a href="/pdf/2310.11818" title="Download PDF">pdf</a>, <a href="/format/2310.11818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IntentDial: An Intent Graph based Multi-Turn Dialogue System with  Reasoning Path Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zengguang Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Binxia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yafang Wang</a>, 
<a href="/search/cs?searchtype=author&query=de+Melo%2C+G">Gerard de Melo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaolong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Intent detection and identification from multi-turn dialogue has become a
widely explored technique in conversational agents, for example, voice
assistants and intelligent customer services. The conventional approaches
typically cast the intent mining process as a classification task. Although
neural classifiers have proven adept at such classification tasks, the issue of
neural network models often impedes their practical deployment in real-world
settings. We present a novel graph-based multi-turn dialogue system called ,
which identifies a user's intent by identifying intent elements and a standard
query from a dynamically constructed and extensible intent graph using
reinforcement learning. In addition, we provide visualization components to
monitor the immediate reasoning path for each turn of a dialogue, which greatly
facilitates further improvement of the system.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11825" title="Abstract">arXiv:2310.11825</a> [<a href="/pdf/2310.11825" title="Download PDF">pdf</a>, <a href="/format/2310.11825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduced-Complexity Verification for K-Step and Infinite-Step Opacity in  Discrete Event Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hadjicostis%2C+C+N">Christoforos N. Hadjicostis</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiwu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">Opacity is a property that captures security concerns in cyber-physical
systems and its verification plays a significant role. This paper investigates
the verifications of K-step and infinite-step weak and strong opacity for
partially observed nondeterministic finite state automata. K-step weak opacity
is checked by constructing, for some states in the observer, appropriate
state-trees, to propose a necessary and sufficient condition. Based on the
relation between K-step weak and infinite-step weak opacity, a condition that
determines when a system is not infinite-step weak opaque is presented.
Regarding K-step and infinite-step strong opacity, we develop a secret-involved
projected automaton, based on which we construct secret-unvisited state trees
to derive a necessary and sufficient condition for K-step strong opacity.
Furthermore, an algorithm is reported to compute a verifier that can be used to
obtain a necessary and sufficient condition for infinite-step strong opacity.
It is argued that, in some particular cases, the proposed methods achieve
reduced complexity compared with the state of the art.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11829" title="Abstract">arXiv:2310.11829</a> [<a href="/pdf/2310.11829" title="Download PDF">pdf</a>, <a href="/format/2310.11829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Graph Foundation Models: A Survey and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junze Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yibo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengmei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+T">Ting Bai</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Emerging as fundamental building blocks for diverse artificial intelligence
applications, foundation models have achieved notable success across natural
language processing and many other domains. Parallelly, graph machine learning
has witnessed a transformative shift, with shallow methods giving way to deep
learning approaches. The emergence and homogenization capabilities of
foundation models have piqued the interest of graph machine learning
researchers, sparking discussions about developing the next graph learning
paradigm that is pre-trained on broad graph data and can be adapted to a wide
range of downstream graph tasks. However, there is currently no clear
definition and systematic analysis for this type of work. In this article, we
propose the concept of graph foundation models (GFMs), and provide the first
comprehensive elucidation on their key characteristics and technologies.
Following that, we categorize existing works towards GFMs into three categories
based on their reliance on graph neural networks and large language models.
Beyond providing a comprehensive overview of the current landscape of graph
foundation models, this article also discusses potential research directions
for this evolving field.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11830" title="Abstract">arXiv:2310.11830</a> [<a href="/pdf/2310.11830" title="Download PDF">pdf</a>, <a href="/format/2310.11830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLARA: Multilingual Contrastive Learning for Audio Representation  Acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noriy%2C+K+A">Kari A Noriy</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaosong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Budka%2C+M">Marcin Budka</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+J">Jian Jun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper proposes a novel framework for multilingual speech and sound
representation learning using contrastive learning. The lack of sizeable
labelled datasets hinders speech-processing research across languages. Recent
advances in contrastive learning provide self-supervised techniques to learn
from unlabelled data. Motivated by reducing data dependence and improving
generalisation across diverse languages and conditions, we develop a
multilingual contrastive framework. This framework enables models to acquire
shared representations across languages, facilitating cross-lingual transfer
with limited target language data.
<br />Additionally, capturing emotional cues within speech is challenging due to
subjective perceptual assessments. By learning expressive representations from
diverse, multilingual data in a self-supervised manner, our approach aims to
develop speech representations that encode emotive dimensions.
<br />Our method trains encoders on a large corpus of multi-lingual audio data.
Data augmentation techniques are employed to expand the dataset. The
contrastive learning approach trains the model to maximise agreement between
positive pairs and minimise agreement between negative pairs. Extensive
experiments demonstrate state-of-the-art performance of the proposed model on
emotion recognition, audio classification, and retrieval benchmarks under
zero-shot and few-shot conditions. This provides an effective approach for
acquiring shared and generalised speech representations across languages and
acoustic conditions while encoding latent emotional dimensions.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11834" title="Abstract">arXiv:2310.11834</a> [<a href="/pdf/2310.11834" title="Download PDF">pdf</a>, <a href="/format/2310.11834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HB-net: Holistic bursting cell cluster integrated network for occluded  multi-objects recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xudong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X+G">Xiao Guang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+J">Jia Rong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaowei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+X">Xiang Liao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Within the realm of image recognition, a specific category of multi-label
classification (MLC) challenges arises when objects within the visual field may
occlude one another, demanding simultaneous identification of both occluded and
occluding objects. Traditional convolutional neural networks (CNNs) can tackle
these challenges; however, those models tend to be bulky and can only attain
modest levels of accuracy. Leveraging insights from cutting-edge neural science
research, specifically the Holistic Bursting (HB) cell, this paper introduces a
pioneering integrated network framework named HB-net. Built upon the foundation
of HB cell clusters, HB-net is designed to address the intricate task of
simultaneously recognizing multiple occluded objects within images. Various
Bursting cell cluster structures are introduced, complemented by an evidence
accumulation mechanism. Testing is conducted on multiple datasets comprising
digits and letters. The results demonstrate that models incorporating the HB
framework exhibit a significant $2.98\%$ enhancement in recognition accuracy
compared to models without the HB framework ($1.0298$ times, $p=0.0499$).
Although in high-noise settings, standard CNNs exhibit slightly greater
robustness when compared to HB-net models, the models that combine the HB
framework and EA mechanism achieve a comparable level of accuracy and
resilience to ResNet50, despite having only three convolutional layers and
approximately $1/30$ of the parameters. The findings of this study offer
valuable insights for improving computer vision algorithms. The essential code
is provided at https://github.com/d-lab438/hb-net.git.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11835" title="Abstract">arXiv:2310.11835</a> [<a href="/pdf/2310.11835" title="Download PDF">pdf</a>, <a href="/format/2310.11835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> T3P: Demystifying Low-Earth Orbit Satellite Broadband
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+S">Shubham Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Bhushan%2C+S">Saksham Bhushan</a>, 
<a href="/search/cs?searchtype=author&query=Taneja%2C+A">Aryan Taneja</a>, 
<a href="/search/cs?searchtype=author&query=Kassem%2C+M">Mohamed Kassem</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Cheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Cong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhiyuan He</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+A">Aravindh Raman</a>, 
<a href="/search/cs?searchtype=author&query=Sastry%2C+N">Nishanth Sastry</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Lili Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacherjee%2C+D">Debopam Bhattacherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The Internet is going through a massive infrastructural revolution with the
advent of low-flying satellite networks, 5/6G, WiFi7, and hollow-core fiber
deployments. While these networks could unleash enhanced connectivity and new
capabilities, it is critical to understand the performance characteristics to
efficiently drive applications over them. Low-Earth orbit (LEO) satellite
mega-constellations like SpaceX Starlink aim to offer broad coverage and low
latencies at the expense of high orbital dynamics leading to continuous latency
changes and frequent satellite hand-offs. This paper aims to quantify
Starlink's latency and its variations and components using a real testbed
spanning multiple latitudes from the North to the South of Europe. We identify
tail latencies as a problem. We develop predictors for latency and throughput
and show their utility in improving application performance by up to 25%. We
also explore how transport protocols can be optimized for LEO networks and show
that this can improve throughput by up to 115% (with only a 5% increase in
latency). Also, our measurement testbed with a footprint across multiple
locations offers unique trigger-based scheduling capabilities that are
necessary to quantify the impact of LEO dynamics.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11840" title="Abstract">arXiv:2310.11840</a> [<a href="/pdf/2310.11840" title="Download PDF">pdf</a>, <a href="/format/2310.11840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On The Expressivity of Objective-Specification Formalisms in  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramani%2C+R">Rohan Subramani</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+M">Marcus Williams</a>, 
<a href="/search/cs?searchtype=author&query=Heitmann%2C+M">Max Heitmann</a>, 
<a href="/search/cs?searchtype=author&query=Holm%2C+H">Halfdan Holm</a>, 
<a href="/search/cs?searchtype=author&query=Griffin%2C+C">Charlie Griffin</a>, 
<a href="/search/cs?searchtype=author&query=Skalse%2C+J">Joar Skalse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">To solve a task with reinforcement learning (RL), it is necessary to formally
specify the goal of that task. Although most RL algorithms require that the
goal is formalised as a Markovian reward function, alternatives have been
developed (such as Linear Temporal Logic and Multi-Objective Reinforcement
Learning). Moreover, it is well known that some of these formalisms are able to
express certain tasks that other formalisms cannot express. However, there has
not yet been any thorough analysis of how these formalisms relate to each other
in terms of expressivity. In this work, we fill this gap in the existing
literature by providing a comprehensive comparison of the expressivities of 17
objective-specification formalisms in RL. We place these formalisms in a
preorder based on their expressive power, and present this preorder as a Hasse
diagram. We find a variety of limitations for the different formalisms, and
that no formalism is both dominantly expressive and straightforward to optimise
with current techniques. For example, we prove that each of Regularised RL,
Outer Nonlinear Markov Rewards, Reward Machines, Linear Temporal Logic, and
Limit Average Rewards can express an objective that the others cannot. Our
findings have implications for both policy optimisation and reward learning.
Firstly, we identify expressivity limitations which are important to consider
when specifying objectives in practice. Secondly, our results highlight the
need for future research which adapts reward learning to work with a variety of
formalisms, since many existing reward learning methods implicitly assume that
desired objectives can be expressed with Markovian rewards. Our work
contributes towards a more cohesive understanding of the costs and benefits of
different RL objective-specification formalisms.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11841" title="Abstract">arXiv:2310.11841</a> [<a href="/pdf/2310.11841" title="Download PDF">pdf</a>, <a href="/ps/2310.11841" title="Download PostScript">ps</a>, <a href="/format/2310.11841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification Aggregation without Unanimity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cailloux%2C+O">Olivier Cailloux</a>, 
<a href="/search/cs?searchtype=author&query=Hervouin%2C+M">Matthieu Hervouin</a>, 
<a href="/search/cs?searchtype=author&query=Ozkes%2C+A+I">Ali I. Ozkes</a>, 
<a href="/search/cs?searchtype=author&query=Sanver%2C+M+R">M. Remzi Sanver</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">A classification is a surjective mapping from a set of objects to a set of
categories. A classification aggregation function aggregates every vector of
classifications into a single one. We show that every citizen sovereign and
independent classification aggregation function is essentially a dictatorship.
This impossibility implies an earlier result of Maniquet and Mongin (2016), who
show that every unanimous and independent classification aggregation function
is a dictatorship. The relationship between the two impossibilities is
reminiscent to the relationship between Wilson's and Arrow's impossibilities in
preference aggregation. Moreover, while the Maniquet-Mongin impossibility rests
on the existence of at least three categories, we propose an alternative proof
technique that covers the case of two categories, except when the number of
objects is also two. We also identify all independent and unanimous
classification aggregation functions for the case of two categories and two
objects.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11843" title="Abstract">arXiv:2310.11843</a> [<a href="/pdf/2310.11843" title="Download PDF">pdf</a>, <a href="/format/2310.11843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do We Run Large-scale Multi-Robot Systems on the Edge? More Evidence for  Two-Phase Performance in System Size Scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuckling%2C+J">Jonas Kuckling</a>, 
<a href="/search/cs?searchtype=author&query=Luckey%2C+R">Robin Luckey</a>, 
<a href="/search/cs?searchtype=author&query=Avrutin%2C+V">Viktor Avrutin</a>, 
<a href="/search/cs?searchtype=author&query=Vardy%2C+A">Andrew Vardy</a>, 
<a href="/search/cs?searchtype=author&query=Reina%2C+A">Andreagiovanni Reina</a>, 
<a href="/search/cs?searchtype=author&query=Hamann%2C+H">Heiko Hamann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">With increasing numbers of mobile robots arriving in real-world applications,
more robots coexist in the same space, interact, and possibly collaborate.
Methods to provide such systems with system size scalability are known, for
example, from swarm robotics. Example strategies are self-organizing behavior,
a strict decentralized approach, and limiting the robot-robot communication.
Despite applying such strategies, any multi-robot system breaks above a certain
critical system size (i.e., number of robots) as too many robots share a
resource (e.g., space, communication channel). We provide additional evidence
based on simulations, that at these critical system sizes, the system
performance separates into two phases: nearly optimal and minimal performance.
We speculate that in real-world applications that are configured for optimal
system size, the supposedly high-performing system may actually live on
borrowed time as it is on a transient to breakdown. We provide two modeling
options (based on queueing theory and a population model) that may help to
support this reasoning.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11845" title="Abstract">arXiv:2310.11845</a> [<a href="/pdf/2310.11845" title="Download PDF">pdf</a>, <a href="/format/2310.11845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerate Presolve in Large-Scale Linear Programming via Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Y">Yufei Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fangzhou Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+M">Meng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhihai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+J">Jia Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Houqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Feng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large-scale LP problems from industry usually contain much redundancy that
severely hurts the efficiency and reliability of solving LPs, making presolve
(i.e., the problem simplification module) one of the most critical components
in modern LP solvers. However, how to design high-quality presolve routines --
that is, the program determining (P1) which presolvers to select, (P2) in what
order to execute, and (P3) when to stop -- remains a highly challenging task
due to the extensive requirements on expert knowledge and the large search
space. Due to the sequential decision property of the task and the lack of
expert demonstrations, we propose a simple and efficient reinforcement learning
(RL) framework -- namely, reinforcement learning for presolve (RL4Presolve) --
to tackle (P1)-(P3) simultaneously. Specifically, we formulate the routine
design task as a Markov decision process and propose an RL framework with
adaptive action sequences to generate high-quality presolve routines
efficiently. Note that adaptive action sequences help learn complex behaviors
efficiently and adapt to various benchmarks. Experiments on two solvers
(open-source and commercial) and eight benchmarks (real-world and synthetic)
demonstrate that RL4Presolve significantly and consistently improves the
efficiency of solving large-scale LPs, especially on benchmarks from industry.
Furthermore, we optimize the hard-coded presolve routines in LP solvers by
extracting rules from learned policies for simple and efficient deployment to
Huawei's supply chain. The results show encouraging economic and academic
potential for incorporating machine learning to modern solvers.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11846" title="Abstract">arXiv:2310.11846</a> [<a href="/pdf/2310.11846" title="Download PDF">pdf</a>, <a href="/format/2310.11846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Pretraining for Multi-Agent Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinmin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chuming Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Building a single generalist agent with zero-shot capability has recently
sparked significant advancements in decision-making. However, extending this
capability to multi-agent scenarios presents challenges. Most current works
struggle with zero-shot capabilities, due to two challenges particular to the
multi-agent settings: a mismatch between centralized pretraining and
decentralized execution, and varying agent numbers and action spaces, making it
difficult to create generalizable representations across diverse downstream
tasks. To overcome these challenges, we propose a \textbf{Mask}ed pretraining
framework for \textbf{M}ulti-\textbf{a}gent decision making (MaskMA). This
model, based on transformer architecture, employs a mask-based collaborative
learning strategy suited for decentralized execution with partial observation.
Moreover, MaskMA integrates a generalizable action representation by dividing
the action space into actions toward self-information and actions related to
other entities. This flexibility allows MaskMA to tackle tasks with varying
agent numbers and thus different action spaces. Extensive experiments in SMAC
reveal MaskMA, with a single model pretrained on 11 training maps, can achieve
an impressive 77.8% zero-shot win rate on 60 unseen test maps by decentralized
execution, while also performing effectively on other types of downstream tasks
(\textit{e.g.,} varied policies collaboration and ad hoc team play).
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11848" title="Abstract">arXiv:2310.11848</a> [<a href="/pdf/2310.11848" title="Download PDF">pdf</a>, <a href="/format/2310.11848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Value-Sensitive Conversational Agent Co-Design Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadek%2C+M">Malak Sadek</a>, 
<a href="/search/cs?searchtype=author&query=Calvo%2C+R+A">Rafael A. Calvo</a>, 
<a href="/search/cs?searchtype=author&query=Mougenot%2C+C">Celine Mougenot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Conversational agents (CAs) are gaining traction in both industry and
academia, especially with the advent of generative AI and large language
models. As these agents are used more broadly by members of the general public
and take on a number of critical use cases and social roles, it becomes
important to consider the values embedded in these systems. This consideration
includes answering questions such as 'whose values get embedded in these
agents?' and 'how do those values manifest in the agents being designed?'
Accordingly, the aim of this paper is to present the Value-Sensitive
Conversational Agent (VSCA) Framework for enabling the collaborative design
(co-design) of value-sensitive CAs with relevant stakeholders. Firstly,
requirements for co-designing value-sensitive CAs which were identified in
previous works are summarised here. Secondly, the practical framework is
presented and discussed, including its operationalisation into a design
toolkit. The framework facilitates the co-design of three artefacts that elicit
stakeholder values and have a technical utility to CA teams to guide CA
implementation, enabling the creation of value-embodied CA prototypes. Finally,
an evaluation protocol for the framework is proposed where the effects of the
framework and toolkit are explored in a design workshop setting to evaluate
both the process followed and the outcomes produced.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11850" title="Abstract">arXiv:2310.11850</a> [<a href="/pdf/2310.11850" title="Download PDF">pdf</a>, <a href="/format/2310.11850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Transferable Adversarial Image Examples: Attack  Categorization, Evaluation Guidelines, and New Insights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhengyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Renjue Li</a>, 
<a href="/search/cs?searchtype=author&query=Sicre%2C+R">Ronan Sicre</a>, 
<a href="/search/cs?searchtype=author&query=Amsaleg%2C+L">Laurent Amsaleg</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/ZhengyuZhao/TransferAttackEval">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Transferable adversarial examples raise critical security concerns in
real-world, black-box attack scenarios. However, in this work, we identify two
main problems in common evaluation practices: (1) For attack transferability,
lack of systematic, one-to-one attack comparison and fair hyperparameter
settings. (2) For attack stealthiness, simply no comparisons. To address these
problems, we establish new evaluation guidelines by (1) proposing a novel
attack categorization strategy and conducting systematic and fair
intra-category analyses on transferability, and (2) considering diverse
imperceptibility metrics and finer-grained stealthiness characteristics from
the perspective of attack traceback. To this end, we provide the first
large-scale evaluation of transferable adversarial examples on ImageNet,
involving 23 representative attacks against 9 representative defenses. Our
evaluation leads to a number of new insights, including consensus-challenging
ones: (1) Under a fair attack hyperparameter setting, one early attack method,
DI, actually outperforms all the follow-up methods. (2) A state-of-the-art
defense, DiffPure, actually gives a false sense of (white-box) security since
it is indeed largely bypassed by our (black-box) transferable attacks. (3) Even
when all attacks are bounded by the same $L_p$ norm, they lead to dramatically
different stealthiness performance, which negatively correlates with their
transferability performance. Overall, our work demonstrates that existing
problematic evaluations have indeed caused misleading conclusions and missing
points, and as a result, hindered the assessment of the actual progress in this
field.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11852" title="Abstract">arXiv:2310.11852</a> [<a href="/pdf/2310.11852" title="Download PDF">pdf</a>, <a href="/format/2310.11852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CIR at the NTCIR-17 ULTRE-2 Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lulu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+K">Keping Bi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiafeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure, NTCIR-17
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The Chinese academy of sciences Information Retrieval team (CIR) has
participated in the NTCIR-17 ULTRE-2 task. This paper describes our approaches
and reports our results on the ULTRE-2 task. We recognize the issue of false
negatives in the Baidu search data in this competition is very severe, much
more severe than position bias. Hence, we adopt the Dual Learning Algorithm
(DLA) to address the position bias and use it as an auxiliary model to study
how to alleviate the false negative issue. We approach the problem from two
perspectives: 1) correcting the labels for non-clicked items by a relevance
judgment model trained from DLA, and learn a new ranker that is initialized
from DLA; 2) including random documents as true negatives and documents that
have partial matching as hard negatives. Both methods can enhance the model
performance and our best method has achieved nDCG@10 of 0.5355, which is 2.66%
better than the best score from the organizer.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11853" title="Abstract">arXiv:2310.11853</a> [<a href="/pdf/2310.11853" title="Download PDF">pdf</a>, <a href="/format/2310.11853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation of Distribution Grid Expansion Costs in Power System  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=B%C3%B6ttcher%2C+L">Luis B&#xf6;ttcher</a>, 
<a href="/search/eess?searchtype=author&query=Fr%C3%B6hlich%2C+C">Christian Fr&#xf6;hlich</a>, 
<a href="/search/eess?searchtype=author&query=Kortmann%2C+S">Steffen Kortmann</a>, 
<a href="/search/eess?searchtype=author&query=Braun%2C+S">Simon Braun</a>, 
<a href="/search/eess?searchtype=author&query=Saat%2C+J">Julian Saat</a>, 
<a href="/search/eess?searchtype=author&query=Ulbig%2C+A">Andreas Ulbig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The shift towards clean energy brings about notable transformations to the
energy system. In order to optimally plan a future energy system, it is
necessary to consider the influence of several sectors as well as the
interaction of the transmission grid and distribution grid. The concept of
Feasible Operation Region (FOR) is a detailed approach to representing the
operational dependencies between the transmission and distribution grid.
However, in previous planning procedures, only a simplified expansion of the
distribution grids can be taken into account. With the method presented in this
paper, a Feasible Planning Region (FPR) is developed, which represents the
operational boundaries of the distribution grids for several expansion stages
and thus represents an admissible solution space for the planning of
distribution grids in systemic planning approaches. It hence enables a more
detailed representation of the necessary distribution grid expansion for the
integration of distributed technologies in an optimized energy system of the
future. In this paper, we present the method by which the FPR is formed and its
integration into an energy system planning formulation. In the results, the FPR
is presented for different voltage levels, and its use in power system planning
is demonstrated.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11857" title="Abstract">arXiv:2310.11857</a> [<a href="/pdf/2310.11857" title="Download PDF">pdf</a>, <a href="/format/2310.11857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multistable Perception, False Consensus, and Information Complements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+Y">Yuqing Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">This paper presents a distributed communication model to investigate
multistable perception, where a stimulus gives rise to multiple competing
perceptual interpretations. We formalize stable perception as consensus
achieved through components exchanging information. Our key finding is that
relationships between components influence monostable versus multistable
perceptions. When components contain substitute information about the
prediction target, stimuli display monostability. With complementary
information, multistability arises. We then analyze phenomena like order
effects and switching costs. Finally, we provide two additional perspectives.
An optimization perspective balances accuracy and communication costs, relating
stability to local optima. A Prediction market perspective highlights the
strategic behaviors of neural coordination and provides insights into phenomena
like rivalry, inhibition, and mental disorders. The two perspectives
demonstrate how relationships among components influence perception costs, and
impact competition and coordination behaviors in neural dynamics.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11862" title="Abstract">arXiv:2310.11862</a> [<a href="/pdf/2310.11862" title="Download PDF">pdf</a>, <a href="/format/2310.11862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Generate Parameters of ConvNets for Unseen Image Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+K">Kaituo Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoren Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Typical Convolutional Neural Networks (ConvNets) depend heavily on large
amounts of image data and resort to an iterative optimization algorithm (e.g.,
SGD or Adam) to learn network parameters, which makes training very time- and
resource-intensive. In this paper, we propose a new training paradigm and
formulate the parameter learning of ConvNets into a prediction task: given a
ConvNet architecture, we observe there exists correlations between image
datasets and their corresponding optimal network parameters, and explore if we
can learn a hyper-mapping between them to capture the relations, such that we
can directly predict the parameters of the network for an image dataset never
seen during the training phase. To do this, we put forward a new hypernetwork
based model, called PudNet, which intends to learn a mapping between datasets
and their corresponding network parameters, and then predicts parameters for
unseen data with only a single forward propagation. Moreover, our model
benefits from a series of adaptive hyper recurrent units sharing weights to
capture the dependencies of parameters among different network layers.
Extensive experiments demonstrate that our proposed method achieves good
efficacy for unseen image datasets on two kinds of settings: Intra-dataset
prediction and Inter-dataset prediction. Our PudNet can also well scale up to
large-scale datasets, e.g., ImageNet-1K. It takes 8967 GPU seconds to train
ResNet-18 on the ImageNet-1K using GC from scratch and obtain a top-5 accuracy
of 44.65 %. However, our PudNet costs only 3.89 GPU seconds to predict the
network parameters of ResNet-18 achieving comparable performance (44.92 %),
more than 2,300 times faster than the traditional training paradigm.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11864" title="Abstract">arXiv:2310.11864</a> [<a href="/pdf/2310.11864" title="Download PDF">pdf</a>, <a href="/format/2310.11864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector  Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Hongliang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jing Liao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review. Project Page: <a href="https://jtbzhl.github.io/VQ-NeRF.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose VQ-NeRF, a two-branch neural network model that incorporates
Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes.
Conventional neural reflectance fields use only continuous representations to
model 3D scenes, despite the fact that objects are typically composed of
discrete materials in reality. This lack of discretization can result in noisy
material decomposition and complicated material editing. To address these
limitations, our model consists of a continuous branch and a discrete branch.
The continuous branch follows the conventional pipeline to predict decomposed
materials, while the discrete branch uses the VQ mechanism to quantize
continuous materials into individual ones. By discretizing the materials, our
model can reduce noise in the decomposition process and generate a segmentation
map of discrete materials. Specific materials can be easily selected for
further editing by clicking on the corresponding area of the segmentation
outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy
to predict the number of materials in a scene, which reduces redundancy in the
material segmentation process. To improve usability, we also develop an
interactive interface to further assist material editing. We evaluate our model
on both computer-generated and real-world scenes, demonstrating its superior
performance. To the best of our knowledge, our model is the first to enable
discrete material editing in 3D scenes.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11865" title="Abstract">arXiv:2310.11865</a> [<a href="/pdf/2310.11865" title="Download PDF">pdf</a>, <a href="/format/2310.11865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective and Efficient Federated Tree Learning on Hybrid Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qinbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chulin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaojun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Ce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bingsheng He</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning has emerged as a promising distributed learning paradigm
that facilitates collaborative learning among multiple parties without
transferring raw data. However, most existing federated learning studies focus
on either horizontal or vertical data settings, where the data of different
parties are assumed to be from the same feature or sample space. In practice, a
common scenario is the hybrid data setting, where data from different parties
may differ both in the features and samples. To address this, we propose
HybridTree, a novel federated learning approach that enables federated tree
learning on hybrid data. We observe the existence of consistent split rules in
trees. With the help of these split rules, we theoretically show that the
knowledge of parties can be incorporated into the lower layers of a tree. Based
on our theoretical analysis, we propose a layer-level solution that does not
need frequent communication traffic to train a tree. Our experiments
demonstrate that HybridTree can achieve comparable accuracy to the centralized
setting with low computational and communication overhead. HybridTree can
achieve up to 8 times speedup compared with the other baselines.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11866" title="Abstract">arXiv:2310.11866</a> [<a href="/pdf/2310.11866" title="Download PDF">pdf</a>, <a href="/ps/2310.11866" title="Download PostScript">ps</a>, <a href="/format/2310.11866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Optimization for Non-convex Problem with Inexact Hessian  Matrix, Gradient, and Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuanqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1809.09853">arXiv:1809.09853</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Trust-region (TR) and adaptive regularization using cubics (ARC) have proven
to have some very appealing theoretical properties for non-convex optimization
by concurrently computing function value, gradient, and Hessian matrix to
obtain the next search direction and the adjusted parameters. Although
stochastic approximations help largely reduce the computational cost, it is
challenging to theoretically guarantee the convergence rate. In this paper, we
explore a family of stochastic TR and ARC methods that can simultaneously
provide inexact computations of the Hessian matrix, gradient, and function
values. Our algorithms require much fewer propagations overhead per iteration
than TR and ARC. We prove that the iteration complexity to achieve
$\epsilon$-approximate second-order optimality is of the same order as the
exact computations demonstrated in previous studies. Additionally, the mild
conditions on inexactness can be met by leveraging a random sampling technology
in the finite-sum minimization problem. Numerical experiments with a non-convex
problem support these findings and demonstrate that, with the same or a similar
number of iterations, our algorithms require less computational overhead per
iteration than current second-order methods.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11867" title="Abstract">arXiv:2310.11867</a> [<a href="/pdf/2310.11867" title="Download PDF">pdf</a>, <a href="/format/2310.11867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Fairness of Discriminative Foundation Models in Computer  Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+J">Junaid Ali</a>, 
<a href="/search/cs?searchtype=author&query=Kleindessner%2C+M">Matthaeus Kleindessner</a>, 
<a href="/search/cs?searchtype=author&query=Wenzel%2C+F">Florian Wenzel</a>, 
<a href="/search/cs?searchtype=author&query=Budhathoki%2C+K">Kailash Budhathoki</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>, 
<a href="/search/cs?searchtype=author&query=Russell%2C+C">Chris Russell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AIES'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a novel taxonomy for bias evaluation of discriminative foundation
models, such as Contrastive Language-Pretraining (CLIP), that are used for
labeling tasks. We then systematically evaluate existing methods for mitigating
bias in these models with respect to our taxonomy. Specifically, we evaluate
OpenAI's CLIP and OpenCLIP models for key applications, such as zero-shot
classification, image retrieval and image captioning. We categorize desired
behaviors based around three axes: (i) if the task concerns humans; (ii) how
subjective the task is (i.e., how likely it is that people from a diverse range
of backgrounds would agree on a labeling); and (iii) the intended purpose of
the task and if fairness is better served by impartiality (i.e., making
decisions independent of the protected attributes) or representation (i.e.,
making decisions to maximize diversity). Finally, we provide quantitative
fairness evaluations for both binary-valued and multi-valued protected
attributes over ten diverse datasets. We find that fair PCA, a post-processing
method for fair representations, works very well for debiasing in most of the
aforementioned tasks while incurring only minor loss of performance. However,
different debiasing approaches vary in their effectiveness depending on the
task. Hence, one should choose the debiasing approach depending on the specific
use case.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11868" title="Abstract">arXiv:2310.11868</a> [<a href="/pdf/2310.11868" title="Download PDF">pdf</a>, <a href="/format/2310.11868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still  Easy To Generate Unsafe Images ... For Now
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yimeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinghan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Aochuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiancheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Ke Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Codes are available at <a href="https://github.com/OPTML-Group/Diffusion-MU-Attack">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The recent advances in diffusion models (DMs) have revolutionized the
generation of complex and diverse images. However, these models also introduce
potential safety hazards, such as the production of harmful content and
infringement of data copyrights. Although there have been efforts to create
safety-driven unlearning methods to counteract these challenges, doubts remain
about their capabilities. To bridge this uncertainty, we propose an evaluation
framework built upon adversarial attacks (also referred to as adversarial
prompts), in order to discern the trustworthiness of these safety-driven
unlearned DMs. Specifically, our research explores the (worst-case) robustness
of unlearned DMs in eradicating unwanted concepts, styles, and objects,
assessed by the generation of adversarial prompts. We develop a novel
adversarial learning approach called UnlearnDiff that leverages the inherent
classification capabilities of DMs to streamline the generation of adversarial
prompts, making it as simple for DMs as it is for image classification attacks.
This technique streamlines the creation of adversarial prompts, making the
process as intuitive for generative modeling as it is for image classification
assaults. Through comprehensive benchmarking, we assess the unlearning
robustness of five prevalent unlearned DMs across multiple tasks. Our results
underscore the effectiveness and efficiency of UnlearnDiff when compared to
state-of-the-art adversarial prompting methods. Codes are available at
https://github.com/OPTML-Group/Diffusion-MU-Attack. WARNING: This paper
contains model outputs that may be offensive in nature.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11870" title="Abstract">arXiv:2310.11870</a> [<a href="/pdf/2310.11870" title="Download PDF">pdf</a>, <a href="/format/2310.11870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Nushu: An Exploration of Language Emergence in Sisterhood -Through  the Lens of Computational Linguistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuqian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yuying Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Ze Gao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zhijun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chuyan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yurou Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kejiang Qian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhigang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Braud%2C+T">Tristan Braud</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C+H">Chang Hee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Asadipour%2C+A">Ali Asadipour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at SIGGRAPH Asia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents "AI Nushu," an emerging language system inspired by Nushu
(women's scripts), the unique language created and used exclusively by ancient
Chinese women who were thought to be illiterate under a patriarchal society. In
this interactive installation, two artificial intelligence (AI) agents are
trained in the Chinese dictionary and the Nushu corpus. By continually
observing their environment and communicating, these agents collaborate towards
creating a standard writing system to encode Chinese. It offers an artistic
interpretation of the creation of a non-western script from a computational
linguistics perspective, integrating AI technology with Chinese cultural
heritage and a feminist viewpoint.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11873" title="Abstract">arXiv:2310.11873</a> [<a href="/pdf/2310.11873" title="Download PDF">pdf</a>, <a href="/ps/2310.11873" title="Download PostScript">ps</a>, <a href="/format/2310.11873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Weight Hierarchies of Linear Codes from Simplicial Complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Dabin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoqiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The study of the generalized Hamming weight of linear codes is a significant
research topic in coding theory as it conveys the structural information of the
codes and determines their performance in various applications. However,
determining the generalized Hamming weights of linear codes, especially the
weight hierarchy, is generally challenging. In this paper, we investigate the
generalized Hamming weights of a class of linear code $\C$ over $\bF_q$, which
is constructed from defining sets. These defining sets are either special
simplicial complexes or their complements in $\bF_q^m$. We determine the
complete weight hierarchies of these codes by analyzing the maximum or minimum
intersection of certain simplicial complexes and all $r$-dimensional subspaces
of $\bF_q^m$, where $1\leq r\leq {\rm dim}_{\bF_q}(\C)$.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11874" title="Abstract">arXiv:2310.11874</a> [<a href="/pdf/2310.11874" title="Download PDF">pdf</a>, <a href="/ps/2310.11874" title="Download PostScript">ps</a>, <a href="/format/2310.11874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some derivations among Logarithmic Space Bounded Counting Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Janaki%2C+V">V. Janaki</a>, 
<a href="/search/cs?searchtype=author&query=Madhan%2C+S">S. Madhan</a>, 
<a href="/search/cs?searchtype=author&query=Vijayaraghavan%2C+T+C">T. C. Vijayaraghavan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Logic (math.LO)

</div>
<p class="mathjax">In this paper we show derivations among logarithmic space bounded counting
classes based on closure properties of $#L$ that leads us to the result that
$NL=PL=C_=L$.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11875" title="Abstract">arXiv:2310.11875</a> [<a href="/pdf/2310.11875" title="Download PDF">pdf</a>, <a href="/format/2310.11875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fractional Concepts in Neural Networks: Enhancing Activation and Loss  Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alijani%2C+Z">Zahra Alijani</a>, 
<a href="/search/cs?searchtype=author&query=Molek%2C+V">Vojtech Molek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures, submitted to Neurocomputing journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The paper presents a method for using fractional concepts in a neural network
to modify the activation and loss functions. The methodology allows the neural
network to define and optimize its activation functions by determining the
fractional derivative order of the training process as an additional
hyperparameter. This will enable neurons in the network to adjust their
activation functions to match input data better and reduce output errors,
potentially improving the network's overall performance.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11876" title="Abstract">arXiv:2310.11876</a> [<a href="/pdf/2310.11876" title="Download PDF">pdf</a>, <a href="/ps/2310.11876" title="Download PostScript">ps</a>, <a href="/format/2310.11876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SQ Lower Bounds for Learning Mixtures of Linear Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diakonikolas%2C+I">Ilias Diakonikolas</a>, 
<a href="/search/cs?searchtype=author&query=Kane%2C+D+M">Daniel M. Kane</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuxin Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problem of learning mixtures of linear classifiers under
Gaussian covariates. Given sample access to a mixture of $r$ distributions on
$\mathbb{R}^n$ of the form $(\mathbf{x},y_{\ell})$, $\ell\in [r]$, where
$\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n)$ and
$y_\ell=\mathrm{sign}(\langle\mathbf{v}_\ell,\mathbf{x}\rangle)$ for an unknown
unit vector $\mathbf{v}_\ell$, the goal is to learn the underlying distribution
in total variation distance. Our main result is a Statistical Query (SQ) lower
bound suggesting that known algorithms for this problem are essentially best
possible, even for the special case of uniform mixtures. In particular, we show
that the complexity of any SQ algorithm for the problem is
$n^{\mathrm{poly}(1/\Delta) \log(r)}$, where $\Delta$ is a lower bound on the
pairwise $\ell_2$-separation between the $\mathbf{v}_\ell$'s. The key technical
ingredient underlying our result is a new construction of spherical designs
that may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11877" title="Abstract">arXiv:2310.11877</a> [<a href="/pdf/2310.11877" title="Download PDF">pdf</a>, <a href="/format/2310.11877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the  Hidden States of Over-Confident Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Slobodkin%2C+A">Aviv Slobodkin</a>, 
<a href="/search/cs?searchtype=author&query=Goldman%2C+O">Omer Goldman</a>, 
<a href="/search/cs?searchtype=author&query=Caciularu%2C+A">Avi Caciularu</a>, 
<a href="/search/cs?searchtype=author&query=Dagan%2C+I">Ido Dagan</a>, 
<a href="/search/cs?searchtype=author&query=Ravfogel%2C+S">Shauli Ravfogel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have been shown to possess impressive
capabilities, while also raising crucial concerns about the faithfulness of
their responses. A primary issue arising in this context is the management of
unanswerable queries by LLMs, which often results in hallucinatory behavior,
due to overconfidence. In this paper, we explore the behavior of LLMs when
presented with unanswerable queries. We ask: do models \textbf{represent} the
fact that the question is unanswerable when generating a hallucinatory answer?
Our results show strong indications that such models encode the answerability
of an input query, with the representation of the first decoded token often
being a strong indicator. These findings shed new light on the spatial
organization within the latent representations of LLMs, unveiling previously
unexplored facets of these models. Moreover, they pave the way for the
development of improved decoding techniques with better adherence to factual
generation, particularly in scenarios where query unanswerability is a concern.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11878" title="Abstract">arXiv:2310.11878</a> [<a href="/pdf/2310.11878" title="Download PDF">pdf</a>, <a href="/format/2310.11878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Dissonance to Insights: Dissecting Disagreements in Rationale  Dataset Construction for Case Outcome Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shanshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+S+T+Y+S">Santosh T.Y.S.S</a>, 
<a href="/search/cs?searchtype=author&query=Ichim%2C+O">Oana Ichim</a>, 
<a href="/search/cs?searchtype=author&query=Risini%2C+I">Isabella Risini</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In legal NLP, Case Outcome Classification (COC) must not only be accurate but
also trustworthy and explainable. Existing work in explainable COC has been
limited to annotations by a single expert. However, it is well-known that
lawyers may disagree in their assessment of case facts. We hence collect a
novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two
experts in the domain of international human rights law, for whom we observe
weak agreement. We study their disagreements and build a two-level
task-independent taxonomy, supplemented with COC-specific subcategories. To our
knowledge, this is the first work in the legal NLP that focuses on human label
variation. We quantitatively assess different taxonomy categories and find that
disagreements mainly stem from underspecification of the legal context, which
poses challenges given the typically limited granularity and noise in COC
metadata. We further assess the explainablility of SOTA COC models on RAVE and
observe limited agreement between models and experts. Overall, our case study
reveals hitherto underappreciated complexities in creating benchmark datasets
in legal NLP that revolve around identifying aspects of a case's facts
supposedly relevant to its outcome.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11880" title="Abstract">arXiv:2310.11880</a> [<a href="/pdf/2310.11880" title="Download PDF">pdf</a>, <a href="/ps/2310.11880" title="Download PostScript">ps</a>, <a href="/format/2310.11880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Convex Optimization with Switching Cost and Delayed Gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Senapati%2C+S">Spandan Senapati</a>, 
<a href="/search/cs?searchtype=author&query=Vaze%2C+R">Rahul Vaze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, accepted at IFIP Performance'23, appears in Elsevier Performance Evaluation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider the online convex optimization (OCO) problem with quadratic and
linear switching cost in the limited information setting, where an online
algorithm can choose its action using only gradient information about the
previous objective function. For $L$-smooth and $\mu$-strongly convex objective
functions, we propose an online multiple gradient descent (OMGD) algorithm and
show that its competitive ratio for the OCO problem with quadratic switching
cost is at most $4(L + 5) + \frac{16(L + 5)}{\mu}$. The competitive ratio upper
bound for OMGD is also shown to be order-wise tight in terms of $L,\mu$. In
addition, we show that the competitive ratio of any online algorithm is
$\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu}})\}$ in the limited information
setting when the switching cost is quadratic. We also show that the OMGD
algorithm achieves the optimal (order-wise) dynamic regret in the limited
information setting. For the linear switching cost, the competitive ratio upper
bound of the OMGD algorithm is shown to depend on both the path length and the
squared path length of the problem instance, in addition to $L, \mu$, and is
shown to be order-wise, the best competitive ratio any online algorithm can
achieve. Consequently, we conclude that the optimal competitive ratio for the
quadratic and linear switching costs are fundamentally different in the limited
information setting.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11881" title="Abstract">arXiv:2310.11881</a> [<a href="/pdf/2310.11881" title="Download PDF">pdf</a>, <a href="/format/2310.11881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Study of Image Restoration Networks for General Backbone  Network Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+Y">Yuandong Pu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiantao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chao Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite the significant progress made by deep models in various image
restoration tasks, existing image restoration networks still face challenges in
terms of task generality. An intuitive manifestation is that networks which
excel in certain tasks often fail to deliver satisfactory results in others. To
illustrate this point, we select five representative image restoration networks
and conduct a comparative study on five classic image restoration tasks. First,
we provide a detailed explanation of the characteristics of different image
restoration tasks and backbone networks. Following this, we present the
benchmark results and analyze the reasons behind the performance disparity of
different models across various tasks. Drawing from this comparative study, we
propose that a general image restoration backbone network needs to meet the
functional requirements of diverse tasks. Based on this principle, we design a
new general image restoration backbone network, X-Restormer. Extensive
experiments demonstrate that X-Restormer possesses good task generality and
achieves state-of-the-art performance across a variety of tasks.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11882" title="Abstract">arXiv:2310.11882</a> [<a href="/pdf/2310.11882" title="Download PDF">pdf</a>, <a href="/ps/2310.11882" title="Download PostScript">ps</a>, <a href="/format/2310.11882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Sample-Driven Solving Procedure for the Repeated Reachability of  Quantum CTMCs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jianling Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Ming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuxin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhi-Bin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Reachability analysis plays a central role in system design and verification.
The reachability problem, denoted $\Diamond^J\,\Phi$, asks whether the system
will meet the property $\Phi$ after some time in a given time interval $J$.
Recently, it has been considered on a novel kind of real-time systems --
quantum continuous-time Markov chains (QCTMCs), and embedded into the
model-checking algorithm. In this paper, we further study the repeated
reachability problem in QCTMCs, denoted $\Box^I\,\Diamond^J\,\Phi$, which
concerns whether the system starting from each \emph{absolute} time in $I$ will
meet the property $\Phi$ after some coming \emph{relative} time in $J$. First
of all, we reduce it to the real root isolation of a class of real-valued
functions (exponential polynomials), whose solvability is conditional to
Schanuel's conjecture being true. To speed up the procedure, we employ the
strategy of sampling. The original problem is shown to be equivalent to the
existence of a finite collection of satisfying samples. We then present a
sample-driven procedure, which can effectively refine the sample space after
each time of sampling, no matter whether the sample itself is successful or
conflicting. The improvement on efficiency is validated by randomly generated
instances. Hence the proposed method would be promising to attack the repeated
reachability problems together with checking other $\omega$-regular properties
in a wide scope of real-time systems.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11884" title="Abstract">arXiv:2310.11884</a> [<a href="/pdf/2310.11884" title="Download PDF">pdf</a>, <a href="/format/2310.11884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Neural Activations to Concepts: A Survey on Explaining Concepts in  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+H">Jae Hee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lanza%2C+S">Sergio Lanza</a>, 
<a href="/search/cs?searchtype=author&query=Wermter%2C+S">Stefan Wermter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Neurosymbolic Artificial Intelligence (<a href="https://neurosymbolic-ai-journal.com/paper/neural-activations-concepts-survey-explaining-concepts-neural-networks">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In this paper, we review recent approaches for explaining concepts in neural
networks. Concepts can act as a natural link between learning and reasoning:
once the concepts are identified that a neural learning system uses, one can
integrate those concepts with a reasoning system for inference or use a
reasoning system to act upon them to improve or enhance the learning system. On
the other hand, knowledge can not only be extracted from neural networks but
concept knowledge can also be inserted into neural network architectures. Since
integrating learning and reasoning is at the core of neuro-symbolic AI, the
insights gained from this survey can serve as an important step towards
realizing neuro-symbolic AI based on explainable concepts.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11886" title="Abstract">arXiv:2310.11886</a> [<a href="/pdf/2310.11886" title="Download PDF">pdf</a>, <a href="/format/2310.11886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling Algorithms for Butterfly Counting on Temporal Bipartite Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pu%2C+J">Jiaxi Pu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 10 figures; under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Databases (cs.DB); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Temporal bipartite graphs are widely used to denote time-evolving
relationships between two disjoint sets of nodes, such as customer-product
interactions in E-commerce and user-group memberships in social networks.
Temporal butterflies, $(2,2)$-bicliques that occur within a short period and in
a prescribed order, are essential in modeling the structural and sequential
patterns of such graphs. Counting the number of temporal butterflies is thus a
fundamental task in analyzing temporal bipartite graphs. However, existing
algorithms for butterfly counting on static bipartite graphs and motif counting
on temporal unipartite graphs are inefficient for this purpose. In this paper,
we present a general framework with three sampling strategies for temporal
butterfly counting. Since exact counting can be time-consuming on large graphs,
our approach alternatively computes approximate estimates accurately and
efficiently. We also provide analytical bounds on the number of samples each
strategy requires to obtain estimates with small relative errors and high
probability. We finally evaluate our framework on six real-world datasets and
demonstrate its superior accuracy and efficiency compared to several baselines.
Overall, our proposed framework and sampling strategies provide efficient and
accurate approaches to approximating temporal butterfly counts on large-scale
temporal bipartite graphs.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11889" title="Abstract">arXiv:2310.11889</a> [<a href="/pdf/2310.11889" title="Download PDF">pdf</a>, <a href="/format/2310.11889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building a Graph-based Deep Learning network model from captured traffic  traces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%BCemes-Palau%2C+C">Carlos G&#xfc;emes-Palau</a>, 
<a href="/search/cs?searchtype=author&query=Galm%C3%A9s%2C+M+F">Miquel Ferriol Galm&#xe9;s</a>, 
<a href="/search/cs?searchtype=author&query=Cabellos-Aparicio%2C+A">Albert Cabellos-Aparicio</a>, 
<a href="/search/cs?searchtype=author&query=Barlet-Ros%2C+P">Pere Barlet-Ros</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Currently the state of the art network models are based or depend on Discrete
Event Simulation (DES). While DES is highly accurate, it is also
computationally costly and cumbersome to parallelize, making it unpractical to
simulate high performance networks. Additionally, simulated scenarios fail to
capture all of the complexities present in real network scenarios. While there
exists network models based on Machine Learning (ML) techniques to minimize
these issues, these models are also trained with simulated data and hence
vulnerable to the same pitfalls. Consequently, the Graph Neural Networking
Challenge 2023 introduces a dataset of captured traffic traces that can be used
to build a ML-based network model without these limitations. In this paper we
propose a Graph Neural Network (GNN)-based solution specifically designed to
better capture the complexities of real network scenarios. This is done through
a novel encoding method to capture information from the sequence of captured
packets, and an improved message passing algorithm to better represent the
dependencies present in physical networks. We show that the proposed solution
it is able to learn and generalize to unseen captured network scenarios.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11890" title="Abstract">arXiv:2310.11890</a> [<a href="/pdf/2310.11890" title="Download PDF">pdf</a>, <a href="/format/2310.11890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IRAD: Implicit Representation-driven Image Resampling against  Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yue Cao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaofeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+I">Ivor Tsang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qing Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce a novel approach to counter adversarial attacks, namely, image
resampling. Image resampling transforms a discrete image into a new one,
simulating the process of scene recapturing or rerendering as specified by a
geometrical transformation. The underlying rationale behind our idea is that
image resampling can alleviate the influence of adversarial perturbations while
preserving essential semantic information, thereby conferring an inherent
advantage in defending against adversarial attacks. To validate this concept,
we present a comprehensive study on leveraging image resampling to defend
against adversarial attacks. We have developed basic resampling methods that
employ interpolation strategies and coordinate shifting magnitudes. Our
analysis reveals that these basic methods can partially mitigate adversarial
attacks. However, they come with apparent limitations: the accuracy of clean
images noticeably decreases, while the improvement in accuracy on adversarial
examples is not substantial. We propose implicit representation-driven image
resampling (IRAD) to overcome these limitations. First, we construct an
implicit continuous representation that enables us to represent any input image
within a continuous coordinate space. Second, we introduce SampleNet, which
automatically generates pixel-wise shifts for resampling in response to
different inputs. Furthermore, we can extend our approach to the
state-of-the-art diffusion-based method, accelerating it with fewer time steps
while preserving its defense capability. Extensive experiments demonstrate that
our method significantly enhances the adversarial robustness of diverse deep
models against various attacks while maintaining high accuracy on clean images.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11892" title="Abstract">arXiv:2310.11892</a> [<a href="/pdf/2310.11892" title="Download PDF">pdf</a>, <a href="/format/2310.11892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Distributed Stochastic Optimization with  Time-Varying Sample Sizes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jimin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Ji-Feng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Differentially private distributed stochastic optimization has become a hot
topic due to the urgent need of privacy protection in distributed stochastic
optimization. In this paper, two-time scale stochastic approximation-type
algorithms for differentially private distributed stochastic optimization with
time-varying sample sizes are proposed using gradient- and output-perturbation
methods. For both gradient- and output-perturbation cases, the convergence of
the algorithm and differential privacy with a finite cumulative privacy budget
$\varepsilon$ for an infinite number of iterations are simultaneously
established, which is substantially different from the existing works. By a
time-varying sample sizes method, the privacy level is enhanced, and
differential privacy with a finite cumulative privacy budget $\varepsilon$ for
an infinite number of iterations is established. By properly choosing a
Lyapunov function, the algorithm achieves almost-sure and mean-square
convergence even when the added privacy noises have an increasing variance.
Furthermore, we rigorously provide the mean-square convergence rates of the
algorithm and show how the added privacy noise affects the convergence rate of
the algorithm. Finally, numerical examples including distributed training on a
benchmark machine learning dataset are presented to demonstrate the efficiency
and advantages of the algorithms.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11895" title="Abstract">arXiv:2310.11895</a> [<a href="/pdf/2310.11895" title="Download PDF">pdf</a>, <a href="/format/2310.11895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible Computation Offloading at the Edge for Autonomous Drones with  Uncertain Flight Times
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Polychronis%2C+G">Giorgos Polychronis</a>, 
<a href="/search/cs?searchtype=author&query=Lalis%2C+S">Spyros Lalis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures, preprint accepted in the 19th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 19th International Conference on Distributed Computing in
  Smart Systems and the Internet of Things (DCOSS-IoT), Pafos, Cyprus, 2023,
  pp. 201-208
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">An ever increasing number of applications can employ aerial unmanned
vehicles, or so-called drones, to perform different sensing and possibly also
actuation tasks from the air. In some cases, the data that is captured at a
given point has to be processed before moving to the next one. Drones can
exploit nearby edge servers to offload the computation instead of performing it
locally. However, doing this in a naive way can be suboptimal if servers have
limited computing resources and drones have limited energy resources. In this
paper, we propose a protocol and resource reservation scheme for each drone and
edge server to decide, in a dynamic and fully decentralized way, whether to
offload the computation and respectively whether to accept such an offloading
requests, with the objective to evenly reduce the drones' mission times. We
evaluate our approach through extensive simulation experiments, showing that it
can significantly reduce the mission times compared to a no-offloading scenario
by up to 26.2%, while outperforming an offloading schedule that has been
computed offline by up to 7.4% as well as a purely opportunistic approach by up
to 23.9%.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11897" title="Abstract">arXiv:2310.11897</a> [<a href="/pdf/2310.11897" title="Download PDF">pdf</a>, <a href="/format/2310.11897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yen-Ju Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+N">Nai-Chieh Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+P">Ping-Chun Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Policy gradient methods have recently been shown to enjoy global convergence
at a $\Theta(1/t)$ rate in the non-regularized tabular softmax setting.
Accordingly, one important research question is whether this convergence rate
can be further improved, with only first-order updates. In this paper, we
answer the above question from the perspective of momentum by adapting the
celebrated Nesterov's accelerated gradient (NAG) method to reinforcement
learning (RL), termed \textit{Accelerated Policy Gradient} (APG). To
demonstrate the potential of APG in achieving faster global convergence, we
formally show that with the true gradient, APG with softmax policy
parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. To
the best of our knowledge, this is the first characterization of the global
convergence rate of NAG in the context of RL. Notably, our analysis relies on
one interesting finding: Regardless of the initialization, APG could end up
reaching a locally nearly-concave regime, where APG could benefit significantly
from the momentum, within finite iterations. By means of numerical validation,
we confirm that APG exhibits $\tilde{O}(1/t^2)$ rate as well as show that APG
could significantly improve the convergence behavior over the standard policy
gradient.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11901" title="Abstract">arXiv:2310.11901</a> [<a href="/pdf/2310.11901" title="Download PDF">pdf</a>, <a href="/format/2310.11901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Malicious Agent Detection for Robust Multi-Agent Collaborative  Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yangheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Z">Zhen Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+S">Sheng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+X">Xianghe Pang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Recently, multi-agent collaborative (MAC) perception has been proposed and
outperformed the traditional single-agent perception in many applications, such
as autonomous driving. However, MAC perception is more vulnerable to
adversarial attacks than single-agent perception due to the information
exchange. The attacker can easily degrade the performance of a victim agent by
sending harmful information from a malicious agent nearby. In this paper, we
extend adversarial attacks to an important perception task -- MAC object
detection, where generic defenses such as adversarial training are no longer
effective against these attacks. More importantly, we propose Malicious Agent
Detection (MADE), a reactive defense specific to MAC perception that can be
deployed by each agent to accurately detect and then remove any potential
malicious agent in its local collaboration network. In particular, MADE
inspects each agent in the network independently using a semi-supervised
anomaly detector based on a double-hypothesis test with the Benjamini-Hochberg
procedure to control the false positive rate of the inference. For the two
hypothesis tests, we propose a match loss statistic and a collaborative
reconstruction loss statistic, respectively, both based on the consistency
between the agent to be inspected and the ego agent where our detector is
deployed. We conduct comprehensive evaluations on a benchmark 3D dataset
V2X-sim and a real-road dataset DAIR-V2X and show that with the protection of
MADE, the drops in the average precision compared with the best-case "oracle"
defender against our attack are merely 1.28% and 0.34%, respectively, much
lower than 8.92% and 10.00% for adversarial training, respectively.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11906" title="Abstract">arXiv:2310.11906</a> [<a href="/pdf/2310.11906" title="Download PDF">pdf</a>, <a href="/format/2310.11906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rather a Nurse than a Physician -- Contrastive Explanations under  Investigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eberle%2C+O">Oliver Eberle</a>, 
<a href="/search/cs?searchtype=author&query=Chalkidis%2C+I">Ilias Chalkidis</a>, 
<a href="/search/cs?searchtype=author&query=Cabello%2C+L">Laura Cabello</a>, 
<a href="/search/cs?searchtype=author&query=Brandl%2C+S">Stephanie Brandl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, long paper at EMNLP 2023 proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Contrastive explanations, where one decision is explained in contrast to
another, are supposed to be closer to how humans explain a decision than
non-contrastive explanations, where the decision is not necessarily referenced
to an alternative. This claim has never been empirically validated. We analyze
four English text-classification datasets (SST2, DynaSent, BIOS and
DBpedia-Animals). We fine-tune and extract explanations from three different
models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three
post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore
collect and release human rationale annotations for a subset of 100 samples
from the BIOS dataset for contrastive and non-contrastive settings. A
cross-comparison between model-based rationales and human annotations, both in
contrastive and non-contrastive settings, yields a high agreement between the
two settings for models as well as for humans. Moreover, model-based
explanations computed in both settings align equally well with human
rationales. Thus, we empirically find that humans do not necessarily explain in
a contrastive manner.9 pages, long paper at ACL 2022 proceedings.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11908" title="Abstract">arXiv:2310.11908</a> [<a href="/pdf/2310.11908" title="Download PDF">pdf</a>, <a href="/format/2310.11908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge Manipulations for the Maximum Vertex-Weighted Bipartite b-matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Auricchio%2C+G">Gennaro Auricchio</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 6 figures. arXiv admin note: substantial text overlap with <a href="/abs/2307.12305">arXiv:2307.12305</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In this paper, we explore the Mechanism Design aspects of the Maximum
Vertex-weighted $b$-Matching (MVbM) problem on bipartite graphs $(A\cup T, E)$.
The set $A$ comprises agents, while $T$ represents tasks. The set $E$ is the
private information of either agents or tasks. In this framework, we
investigate three mechanisms - $\MB$, $\MD$, and $\MG$ - that, given an MVbM
problem as input, return a $b$-matching. We examine scenarios in which either
agents or tasks are strategic and report their adjacent edges to one of the
three mechanisms. In both cases, we assume that the strategic entities are
bounded by their statements: they can hide edges, but they cannot report edges
that do not exist. First, we consider the case in which agents can manipulate.
In this framework, $\MB$ and $\MD$ are optimal but not truthful. By
characterizing the Nash Equilibria induced by $\MB$ and $\MD$, we reveal that
both mechanisms have a Price of Anarchy ($PoA$) and Price of Stability ($PoS$)
of $2$. These efficiency guarantees are tight; no deterministic mechanism can
achieve a lower $PoA$ or $PoS$. In contrast, the third mechanism, $\MG$, is not
optimal, but truthful and its approximation ratio is $2$. We demonstrate that
this ratio is optimal; no deterministic and truthful mechanism can outperform
it. We then shift our focus to scenarios where tasks can exhibit strategic
behaviour. In this case, $\MB$, $\MD$, and $\MG$ all maintain truthfulness,
making $\MB$ and $\MD$ truthful and optimal mechanisms. In conclusion, we
investigate the manipulability of $\MB$ and $\MD$ through experiments on
randomly generated graphs. We observe that (1) $\MB$ is less prone to be
manipulated by the first agent than $\MD$ (2) $\MB$ is more manipulable on
instances in which the total capacity of the agents is equal to the number of
tasks (3) randomizing the agents' order reduces the agents' ability to
manipulate $\MB$.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11911" title="Abstract">arXiv:2310.11911</a> [<a href="/pdf/2310.11911" title="Download PDF">pdf</a>, <a href="/format/2310.11911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolving Bitcoin Custody
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Swambo%2C+J+T+G">Jacob Tyge Goker Swambo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The broad topic of this thesis is the design and analysis of Bitcoin custody
systems. Both the technology and threat landscape are evolving constantly.
Therefore, custody systems, defence strategies, and risk models should be
adaptive too.
<br />We introduce Bitcoin custody by describing the different types, design
principles, phases and functions of custody systems. We review the technology
stack of these systems and focus on the fundamentals; key-management and
privacy. We present a perspective we call the systems view. It is an attempt to
capture the full complexity of a custody system, including technology, people,
and processes. We review existing custody systems and standards.
<br />We explore Bitcoin covenants. This is a mechanism to enforce constraints on
transaction sequences. Although previous work has proposed how to construct and
apply Bitcoin covenants, these require modifying the consensus rules of
Bitcoin, a notoriously difficult task. We introduce the first detailed
exposition and security analysis of a deleted-key covenant protocol, which is
compatible with current consensus rules. We demonstrate a range of security
models for deleted-key covenants which seem practical, in particular, when
applied in autonomous (user-controlled) custody systems. We conclude with a
comparative analysis with previous proposals.
<br />Covenants are often proclaimed to be an important primitive for custody
systems, but no complete design has been proposed to validate that claim. To
address this, we propose an autonomous custody system called Ajolote which uses
deleted-key covenants to enforce a vault sequence. We evaluate Ajolote with; a
model of its state dynamics, a privacy analysis, and a risk model. We propose a
threat model for custody systems which captures a realistic attacker for a
system with offline devices and user-verification. We perform ceremony analysis
to construct the risk model.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11913" title="Abstract">arXiv:2310.11913</a> [<a href="/pdf/2310.11913" title="Download PDF">pdf</a>, <a href="/format/2310.11913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic Sparse Pattern Matching via the Baur-Strassen Theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fischer%2C+N">Nick Fischer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Abstract shortened to fit arxiv requirements
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">How fast can you test whether a constellation of stars appears in the night
sky? This question can be modeled as the computational problem of testing
whether a set of points $P$ can be moved into (or close to) another set $Q$
under some prescribed group of transformations.
<br />Consider, as a simple representative, the following problem: Given two sets
of at most $n$ integers $P,Q\subseteq[N]$, determine whether there is some
shift $s$ such that $P$ shifted by $s$ is a subset of $Q$, i.e.,
$P+s=\{p+s:p\in P\}\subseteq Q$. This problem, to which we refer as the
Constellation problem, can be solved in near-linear time $O(n\log n)$ by a
Monte Carlo randomized algorithm [Cardoze, Schulman; FOCS'98] and time
$O(n\log^2 N)$ by a Las Vegas randomized algorithm [Cole, Hariharan; STOC'02].
Moreover, there is a deterministic algorithm running in time
$n\cdot2^{O(\sqrt{\log n\log\log N})}$ [Chan, Lewenstein; STOC'15]. An
interesting question left open by these previous works is whether Constellation
is in deterministic near-linear time (i.e., with only polylogarithmic
overhead).
<br />We answer this question positively by giving an $n\cdot(\log N)^{O(1)}$-time
deterministic algorithm for the Constellation problem. Our algorithm extends to
various more complex Point Pattern Matching problems in higher dimensions,
under translations and rigid motions, and possibly with mismatches, and also to
a near-linear-time derandomization of the Sparse Wildcard Matching problem on
strings.
<br />We find it particularly interesting how we obtain our deterministic
algorithm. All previous algorithms are based on the same baseline idea, using
additive hashing and the Fast Fourier Transform. In contrast, our algorithms
are based on new ideas, involving a surprising blend of combinatorial and
algebraic techniques. At the heart lies an innovative application of the
Baur-Strassen theorem from algebraic complexity theory.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11915" title="Abstract">arXiv:2310.11915</a> [<a href="/pdf/2310.11915" title="Download PDF">pdf</a>, <a href="/format/2310.11915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SHA-SCP: A UI Element Spatial Hierarchy Aware Smartphone User Click  Behavior Prediction Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Ling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yiyi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kai Qian</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hongyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaofan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Predicting user click behavior and making relevant recommendations based on
the user's historical click behavior are critical to simplifying operations and
improving user experience. Modeling UI elements is essential to user click
behavior prediction, while the complexity and variety of the UI make it
difficult to adequately capture the information of different scales. In
addition, the lack of relevant datasets also presents difficulties for such
studies. In response to these challenges, we construct a fine-grained
smartphone usage behavior dataset containing 3,664,325 clicks of 100 users and
propose a UI element spatial hierarchy aware smartphone user click behavior
prediction method (SHA-SCP). SHA-SCP builds element groups by clustering the
elements according to their spatial positions and uses attention mechanisms to
perceive the UI at the element level and the element group level to fully
capture the information of different scales. Experiments are conducted on the
fine-grained smartphone usage behavior dataset, and the results show that our
method outperforms the best baseline by an average of 10.52%, 11.34%, and
10.42% in Top-1 Accuracy, Top-3 Accuracy, and Top-5 Accuracy, respectively.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11917" title="Abstract">arXiv:2310.11917</a> [<a href="/pdf/2310.11917" title="Download PDF">pdf</a>, <a href="/format/2310.11917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kochsiek%2C+A">Adrian Kochsiek</a>, 
<a href="/search/cs?searchtype=author&query=Gemulla%2C+R">Rainer Gemulla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of
predicting facts for new, previously unseen entities based on context
information. Although new entities can be integrated by retraining the model
from scratch in principle, such an approach is infeasible for large-scale KGs,
where retraining is expensive and new entities may arise frequently. In this
paper, we propose and describe a large-scale benchmark to evaluate
semi-inductive LP models. The benchmark is based on and extends Wikidata5M: It
provides transductive, k-shot, and 0-shot LP tasks, each varying the available
information from (i) only KG structure, to (ii) including textual mentions, and
(iii) detailed descriptions of the entities. We report on a small study of
recent approaches and found that semi-inductive LP performance is far from
transductive performance on long-tail entities throughout all experiments. The
benchmark provides a test bed for further research into integrating context and
textual information in semi-inductive LP models.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11921" title="Abstract">arXiv:2310.11921</a> [<a href="/pdf/2310.11921" title="Download PDF">pdf</a>, <a href="/format/2310.11921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BUT CHiME-7 system description
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karafi%C3%A1t%2C+M">Martin Karafi&#xe1;t</a>, 
<a href="/search/cs?searchtype=author&query=Vesel%C3%BD%2C+K">Karel Vesel&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=Sz%C3%B6ke%2C+I">Igor Sz&#xf6;ke</a>, 
<a href="/search/cs?searchtype=author&query=Mo%C5%A1ner%2C+L">Ladislav Mo&#x161;ner</a>, 
<a href="/search/cs?searchtype=author&query=Bene%C5%A1%2C+K">Karel Bene&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Witkowski%2C+M">Marcin Witkowski</a>, 
<a href="/search/cs?searchtype=author&query=Barchi%2C+G">Germ&#xe1;n Barchi</a>, 
<a href="/search/cs?searchtype=author&query=Pepino%2C+L">Leonardo Pepino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, Chime-7 challenge 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper describes the joint effort of Brno University of Technology (BUT),
AGH University of Krakow and University of Buenos Aires on the development of
Automatic Speech Recognition systems for the CHiME-7 Challenge. We train and
evaluate various end-to-end models with several toolkits. We heavily relied on
Guided Source Separation (GSS) to convert multi-channel audio to single
channel. The ASR is leveraging speech representations from models pre-trained
by self-supervised learning, and we do a fusion of several ASR systems. In
addition, we modified external data from the LibriSpeech corpus to become a
close domain and added it to the training. Our efforts were focused on the
far-field acoustic robustness sub-track of Task 1 - Distant Automatic Speech
Recognition (DASR), our systems use oracle segmentation.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11923" title="Abstract">arXiv:2310.11923</a> [<a href="/pdf/2310.11923" title="Download PDF">pdf</a>, <a href="/format/2310.11923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating semantic subspaces of Transformer sentence embeddings  through linear structural probing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikolaev%2C+D">Dmitry Nikolaev</a>, 
<a href="/search/cs?searchtype=author&query=Pad%C3%B3%2C+S">Sebastian Pad&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to BlackboxNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The question of what kinds of linguistic information are encoded in different
layers of Transformer-based language models is of considerable interest for the
NLP community. Existing work, however, has overwhelmingly focused on word-level
representations and encoder-only language models with the masked-token training
objective. In this paper, we present experiments with semantic structural
probing, a method for studying sentence-level representations via finding a
subspace of the embedding space that provides suitable task-specific pairwise
distances between data-points. We apply our method to language models from
different families (encoder-only, decoder-only, encoder-decoder) and of
different sizes in the context of two tasks, semantic textual similarity and
natural-language inference. We find that model families differ substantially in
their performance and layer dynamics, but that the results are largely
model-size invariant.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11926" title="Abstract">arXiv:2310.11926</a> [<a href="/pdf/2310.11926" title="Download PDF">pdf</a>, <a href="/ps/2310.11926" title="Download PostScript">ps</a>, <a href="/format/2310.11926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Asymptotic Rank Conjecture and the Set Cover Conjecture are not Both  True
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rklund%2C+A">Andreas Bj&#xf6;rklund</a>, 
<a href="/search/cs?searchtype=author&query=Kaski%2C+P">Petteri Kaski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Strassen's asymptotic rank conjecture [Progr. Math. 120 (1994)] claims a
strong submultiplicative upper bound on the rank of a three-tensor obtained as
an iterated Kronecker product of a constant-size base tensor. The conjecture,
if true, most notably would put square matrix multiplication in quadratic time.
We note here that some more-or-less unexpected algorithmic results in the area
of exponential-time algorithms would also follow. Specifically, we study the
so-called set cover conjecture, which states that for any $\epsilon&gt;0$ there
exists a positive integer constant $k$ such that no algorithm solves the
$k$-Set Cover problem in worst-case time $\mathcal{O}((2-\epsilon)^n|\mathcal
F|\operatorname{poly}(n))$. The $k$-Set Cover problem asks, given as input an
$n$-element universe $U$, a family $\mathcal F$ of size-at-most-$k$ subsets of
$U$, and a positive integer $t$, whether there is a subfamily of at most $t$
sets in $\mathcal F$ whose union is $U$. The conjecture was formulated by Cygan
et al. in the monograph Parameterized Algorithms [Springer, 2015] but was
implicit as a hypothesis already in Cygan et al. [CCC 2012, ACM Trans.
Algorithms 2016], there conjectured to follow from the Strong Exponential Time
Hypothesis. We prove that if the asymptotic rank conjecture is true, then the
set cover conjecture is false. Using a reduction by Krauthgamer and Trabelsi
[STACS 2019], in this scenario we would also get a
$\mathcal{O}((2-\delta)^n)$-time randomized algorithm for some constant
$\delta&gt;0$ for another well-studied problem for which no such algorithm is
known, namely that of deciding whether a given $n$-vertex directed graph has a
Hamiltonian cycle.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11927" title="Abstract">arXiv:2310.11927</a> [<a href="/pdf/2310.11927" title="Download PDF">pdf</a>, <a href="/format/2310.11927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UNav-Sim: A Visually Realistic Underwater Robotics Simulator and  Synthetic Data-generation Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amer%2C+A">Abdelhakim Amer</a>, 
<a href="/search/cs?searchtype=author&query=%C3%81lvarez-Tu%C3%B1%C3%B3n%2C+O">Olaya &#xc1;lvarez-Tu&#xf1;&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Ugurlu%2C+H+I">Halil Ibrahim Ugurlu</a>, 
<a href="/search/cs?searchtype=author&query=Sejersen%2C+J+l+F">Jonas le Fevre Sejersen</a>, 
<a href="/search/cs?searchtype=author&query=Brodskiy%2C+Y">Yury Brodskiy</a>, 
<a href="/search/cs?searchtype=author&query=Kayacan%2C+E">Erdal Kayacan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Underwater robotic surveys can be costly due to the complex working
environment and the need for various sensor modalities. While underwater
simulators are essential, many existing simulators lack sufficient rendering
quality, restricting their ability to transfer algorithms from simulation to
real-world applications. To address this limitation, we introduce UNav-Sim,
which, to the best of our knowledge, is the first simulator to incorporate the
efficient, high-detail rendering of Unreal Engine 5 (UE5). UNav-Sim is
open-source and includes an autonomous vision-based navigation stack. By
supporting standard robotics tools like ROS, UNav-Sim enables researchers to
develop and test algorithms for underwater environments efficiently.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11931" title="Abstract">arXiv:2310.11931</a> [<a href="/pdf/2310.11931" title="Download PDF">pdf</a>, <a href="/format/2310.11931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulating Users in Interactive Web Table Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Engelmann%2C+B">Bj&#xf6;rn Engelmann</a>, 
<a href="/search/cs?searchtype=author&query=Breuer%2C+T">Timo Breuer</a>, 
<a href="/search/cs?searchtype=author&query=Schaer%2C+P">Philipp Schaer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages + references; accepted at CIKM'23
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Considering the multimodal signals of search items is beneficial for
retrieval effectiveness. Especially in web table retrieval (WTR) experiments,
accounting for multimodal properties of tables boosts effectiveness. However,
it still remains an open question how the single modalities affect user
experience in particular. Previous work analyzed WTR performance in ad-hoc
retrieval benchmarks, which neglects interactive search behavior and limits the
conclusion about the implications for real-world user environments.
<br />To this end, this work presents an in-depth evaluation of simulated
interactive WTR search sessions as a more cost-efficient and reproducible
alternative to real user studies. As a first of its kind, we introduce
interactive query reformulation strategies based on Doc2Query, incorporating
cognitive states of simulated user knowledge. Our evaluations include two
perspectives on user effectiveness by considering different cost paradigms,
namely query-wise and time-oriented measures of effort. Our multi-perspective
evaluation scheme reveals new insights about query strategies, the impact of
modalities, and different user types in simulated WTR search sessions.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11935" title="Abstract">arXiv:2310.11935</a> [<a href="/pdf/2310.11935" title="Download PDF">pdf</a>, <a href="/format/2310.11935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An EigenValue Stabilization Technique for Immersed Boundary Finite  Element Methods in Explicit Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eisentr%C3%A4ger%2C+S">Sascha Eisentr&#xe4;ger</a>, 
<a href="/search/math?searchtype=author&query=Radtke%2C+L">Lars Radtke</a>, 
<a href="/search/math?searchtype=author&query=Garhuom%2C+W">Wadhah Garhuom</a>, 
<a href="/search/math?searchtype=author&query=L%C3%B6hnert%2C+S">Stefan L&#xf6;hnert</a>, 
<a href="/search/math?searchtype=author&query=D%C3%BCster%2C+A">Alexander D&#xfc;ster</a>, 
<a href="/search/math?searchtype=author&query=Juhre%2C+D">Daniel Juhre</a>, 
<a href="/search/math?searchtype=author&query=Schillinger%2C+D">Dominik Schillinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages, 25 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">The application of immersed boundary methods in static analyses is often
impeded by poorly cut elements (small cut elements problem), leading to
ill-conditioned linear systems of equations and stability problems. While these
concerns may not be paramount in explicit dynamics, a substantial reduction in
the critical time step size based on the smallest volume fraction $\chi$ of a
cut element is observed. This reduction can be so drastic that it renders
explicit time integration schemes impractical. To tackle this challenge, we
propose the use of a dedicated eigenvalue stabilization (EVS) technique.
<br />The EVS-technique serves a dual purpose. Beyond merely improving the
condition number of system matrices, it plays a pivotal role in extending the
critical time increment, effectively broadening the stability region in
explicit dynamics. As a result, our approach enables robust and efficient
analyses of high-frequency transient problems using immersed boundary methods.
A key advantage of the stabilization method lies in the fact that only
element-level operations are required.
<br />This is accomplished by computing all eigenvalues of the element matrices and
subsequently introducing a stabilization term that mitigates the adverse
effects of cutting. Notably, the stabilization of the mass matrix
$\mathbf{M}_\mathrm{c}$ of cut elements -- especially for high polynomial
orders $p$ of the shape functions -- leads to a significant raise in the
critical time step size $\Delta t_\mathrm{cr}$.
<br />To demonstrate the efficacy of our technique, we present two specifically
selected dynamic benchmark examples related to wave propagation analysis, where
an explicit time integration scheme must be employed to leverage the increase
in the critical time step size.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11938" title="Abstract">arXiv:2310.11938</a> [<a href="/pdf/2310.11938" title="Download PDF">pdf</a>, <a href="/format/2310.11938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grounded and Well-rounded: A Methodological Approach to the Study of  Cross-modal and Cross-lingual Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mickus%2C+T">Timothee Mickus</a>, 
<a href="/search/cs?searchtype=author&query=Zosa%2C+E">Elaine Zosa</a>, 
<a href="/search/cs?searchtype=author&query=Paperno%2C+D">Denis Paperno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Grounding has been argued to be a crucial component towards the development
of more complete and truly semantically competent artificial intelligence
systems. Literature has divided into two camps: While some argue that grounding
allows for qualitatively different generalizations, others believe it can be
compensated by mono-modal data quantity. Limited empirical evidence has emerged
for or against either position, which we argue is due to the methodological
challenges that come with studying grounding and its effects on NLP systems.
<br />In this paper, we establish a methodological framework for studying what the
effects are - if any - of providing models with richer input sources than
text-only. The crux of it lies in the construction of comparable samples of
populations of models trained on different input modalities, so that we can
tease apart the qualitative effects of different input sources from
quantifiable model performances. Experiments using this framework reveal
qualitative differences in model behavior between cross-modally grounded,
cross-lingually grounded, and ungrounded models, which we measure both at a
global dataset level as well as for specific word representations, depending on
how concrete their semantics is.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11950" title="Abstract">arXiv:2310.11950</a> [<a href="/pdf/2310.11950" title="Download PDF">pdf</a>, <a href="/format/2310.11950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Too Good To Be True: performance overestimation in (re)current practices  for Human Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tello%2C+A">Andr&#xe9;s Tello</a>, 
<a href="/search/cs?searchtype=author&query=Degeler%2C+V">Victoria Degeler</a>, 
<a href="/search/cs?searchtype=author&query=Lazovik%2C+A">Alexander Lazovik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Today, there are standard and well established procedures within the Human
Activity Recognition (HAR) pipeline. However, some of these conventional
approaches lead to accuracy overestimation. In particular, sliding windows for
data segmentation followed by standard random k-fold cross validation, produce
biased results. An analysis of previous literature and present-day studies,
surprisingly, shows that these are common approaches in state-of-the-art
studies on HAR. It is important to raise awareness in the scientific community
about this problem, whose negative effects are being overlooked. Otherwise,
publications of biased results lead to papers that report lower accuracies,
with correct unbiased methods, harder to publish. Several experiments with
different types of datasets and different types of classification models allow
us to exhibit the problem and show it persists independently of the method or
dataset.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11952" title="Abstract">arXiv:2310.11952</a> [<a href="/pdf/2310.11952" title="Download PDF">pdf</a>, <a href="/format/2310.11952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recasting Continual Learning as Sequence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Soochan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Son%2C+J">Jaehyeon Son</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gunhee Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we aim to establish a strong connection between two significant
bodies of machine learning research: continual learning and sequence modeling.
That is, we propose to formulate continual learning as a sequence modeling
problem, allowing advanced sequence models to be utilized for continual
learning. Under this formulation, the continual learning process becomes the
forward pass of a sequence model. By adopting the meta-continual learning (MCL)
framework, we can train the sequence model at the meta-level, on multiple
continual learning episodes. As a specific example of our new formulation, we
demonstrate the application of Transformers and their efficient variants as MCL
methods. Our experiments on seven benchmarks, covering both classification and
regression, show that sequence models can be an attractive solution for general
MCL.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11954" title="Abstract">arXiv:2310.11954</a> [<a href="/pdf/2310.11954" title="Download PDF">pdf</a>, <a href="/format/2310.11954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MusicAgent: An AI Agent for Music Understanding and Generation with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dingyao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kaitao Song</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Peiling Lu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianyu He</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">AI-empowered music processing is a diverse field that encompasses dozens of
tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension
tasks (e.g., music classification). For developers and amateurs, it is very
difficult to grasp all of these task to satisfy their requirements in music
processing, especially considering the huge differences in the representations
of music data and the model applicability across platforms among various tasks.
Consequently, it is necessary to build a system to organize and integrate these
tasks, and thus help practitioners to automatically analyze their demand and
call suitable tools as solutions to fulfill their requirements. Inspired by the
recent success of large language models (LLMs) in task automation, we develop a
system, named MusicAgent, which integrates numerous music-related tools and an
autonomous workflow to address user requirements. More specifically, we build
1) toolset that collects tools from diverse sources, including Hugging Face,
GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g.,
ChatGPT) to organize these tools and automatically decompose user requests into
multiple sub-tasks and invoke corresponding music tools. The primary goal of
this system is to free users from the intricacies of AI-music tools, enabling
them to concentrate on the creative aspect. By granting users the freedom to
effortlessly combine tools, the system offers a seamless and enriching music
experience.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11956" title="Abstract">arXiv:2310.11956</a> [<a href="/pdf/2310.11956" title="Download PDF">pdf</a>, <a href="/format/2310.11956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acoustic shape optimization using energy stable curvilinear finite  differences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eriksson%2C+G">Gustav Eriksson</a>, 
<a href="/search/math?searchtype=author&query=Stiernstr%C3%B6m%2C+V">Vidar Stiernstr&#xf6;m</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A gradient-based method for shape optimization problems constrained by the
acoustic wave equation is presented. The method makes use of high-order
accurate finite differences with summation-by-parts properties on multiblock
curvilinear grids to discretize in space. Representing the design domain
through a coordinate mapping from a reference domain, the design shape is
obtained by inversion of the discretized coordinate map. The adjoint state
framework is employed to efficiently compute the gradient of the loss
functional. Using the summation-by-parts properties of the finite difference
discretization, we prove stability and dual consistency for the semi-discrete
forward and adjoint problems. Numerical experiments verify the accuracy of the
finite difference scheme and demonstrate the capabilities of the shape
optimization method on two model problems with real-world relevance.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11957" title="Abstract">arXiv:2310.11957</a> [<a href="/pdf/2310.11957" title="Download PDF">pdf</a>, <a href="/format/2310.11957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supporting UAVs with Edge Computing: A Review of Opportunities and  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jan%C3%9Fen%2C+M">Malte Jan&#xdf;en</a>, 
<a href="/search/cs?searchtype=author&query=Pfandzelter%2C+T">Tobias Pfandzelter</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Minghe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bermbach%2C+D">David Bermbach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Over the last years, Unmanned Aerial Vehicles (UAVs) have seen significant
advancements in sensor capabilities and computational abilities, allowing for
efficient autonomous navigation and visual tracking applications. However, the
demand for computationally complex tasks has increased faster than advances in
battery technology. This opens up possibilities for improvements using edge
computing. In edge computing, edge servers can achieve lower latency responses
compared to traditional cloud servers through strategic geographic deployments.
Furthermore, these servers can maintain superior computational performance
compared to UAVs, as they are not limited by battery constraints. Combining
these technologies by aiding UAVs with edge servers, research finds measurable
improvements in task completion speed, energy efficiency, and reliability
across multiple applications and industries. This systematic literature review
aims to analyze the current state of research and collect, select, and extract
the key areas where UAV activities can be supported and improved through edge
computing.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11958" title="Abstract">arXiv:2310.11958</a> [<a href="/pdf/2310.11958" title="Download PDF">pdf</a>, <a href="/format/2310.11958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emptying the Ocean with a Spoon: Should We Edit Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pinter%2C+Y">Yuval Pinter</a>, 
<a href="/search/cs?searchtype=author&query=Elhadad%2C+M">Michael Elhadad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of ACL: EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We call into question the recently popularized method of direct model editing
as a means of correcting factual errors in LLM generations. We contrast model
editing with three similar but distinct approaches that pursue better defined
objectives: (1) retrieval-based architectures, which decouple factual memory
from inference and linguistic capabilities embodied in LLMs; (2) concept
erasure methods, which aim at preventing systemic bias in generated text; and
(3) attribution methods, which aim at grounding generations into identified
textual sources. We argue that direct model editing cannot be trusted as a
systematic remedy for the disadvantages inherent to LLMs, and while it has
proven potential in improving model explainability, it opens risks by
reinforcing the notion that models can be trusted for factuality. We call for
cautious promotion and application of model editing as part of the LLM
deployment process, and for responsibly limiting the use cases of LLMs to those
not relying on editing as a critical component.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11959" title="Abstract">arXiv:2310.11959</a> [<a href="/pdf/2310.11959" title="Download PDF">pdf</a>, <a href="/format/2310.11959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Shuhan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Sizhe Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+W">Weipeng Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+S+-+G">S.-H. Gary Chan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Time series data, often characterized by unique composition and complex
multi-scale temporal variations, requires special consideration of
decomposition and multi-scale modeling in its analysis. Existing deep learning
methods on this best fit to only univariate time series, and have not
sufficiently accounted for sub-series level modeling and decomposition
completeness. To address this, we propose MSD-Mixer, a Multi-Scale
Decomposition MLP-Mixer which learns to explicitly decompose the input time
series into different components, and represents the components in different
layers. To handle multi-scale temporal patterns and inter-channel dependencies,
we propose a novel temporal patching approach to model the time series as
multi-scale sub-series, i.e., patches, and employ MLPs to mix intra- and
inter-patch variations and channel-wise correlations. In addition, we propose a
loss function to constrain both the magnitude and autocorrelation of the
decomposition residual for decomposition completeness. Through extensive
experiments on various real-world datasets for five common time series analysis
tasks (long- and short-term forecasting, imputation, anomaly detection, and
classification), we demonstrate that MSD-Mixer consistently achieves
significantly better performance in comparison with other state-of-the-art
task-general and task-specific approaches.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11960" title="Abstract">arXiv:2310.11960</a> [<a href="/pdf/2310.11960" title="Download PDF">pdf</a>, <a href="/format/2310.11960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for  Long Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yanming Kang</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+G">Giang Tran</a>, 
<a href="/search/cs?searchtype=author&query=De+Sterck%2C+H">Hans De Sterck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Transformer-based models have achieved state-of-the-art performance in many
areas. However, the quadratic complexity of self-attention with respect to the
input length hinders the applicability of Transformer-based models to long
sequences. To address this, we present Fast Multipole Attention, a new
attention mechanism that uses a divide-and-conquer strategy to reduce the time
and memory complexity of attention for sequences of length $n$ from
$\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a
global receptive field. The hierarchical approach groups queries, keys, and
values into $\mathcal{O}( \log n)$ levels of resolution, where groups at
greater distances are increasingly larger in size and the weights to compute
group quantities are learned. As such, the interaction between tokens far from
each other is considered in lower resolution in an efficient hierarchical
manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$
or $\mathcal{O}(n \log n)$, depending on whether the queries are down-sampled
or not. This multi-level divide-and-conquer strategy is inspired by fast
summation methods from $n$-body physics and the Fast Multipole Method. We
perform evaluation on autoregressive and bidirectional language modeling tasks
and compare our Fast Multipole Attention model with other efficient attention
variants on medium-size datasets. We find empirically that the Fast Multipole
Transformer performs much better than other efficient transformers in terms of
memory size and accuracy. The Fast Multipole Attention mechanism has the
potential to empower large language models with much greater sequence lengths,
taking the full context into account in an efficient, naturally hierarchical
manner during training and when generating long sequences.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11964" title="Abstract">arXiv:2310.11964</a> [<a href="/pdf/2310.11964" title="Download PDF">pdf</a>, <a href="/format/2310.11964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMR Parsing with Causal Hierarchical Attention and Pointers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+C">Chao Lou</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+K">Kewei Tu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Translation-based AMR parsers have recently gained popularity due to their
simplicity and effectiveness. They predict linearized graphs as free texts,
avoiding explicit structure modeling. However, this simplicity neglects
structural locality in AMR graphs and introduces unnecessary tokens to
represent coreferences. In this paper, we introduce new target forms of AMR
parsing and a novel model, CHAP, which is equipped with causal hierarchical
attention and the pointer mechanism, enabling the integration of structures
into the Transformer decoder. We empirically explore various alternative
modeling options. Experiments show that our model outperforms baseline models
on four out of five benchmarks in the setting of no additional data.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11965" title="Abstract">arXiv:2310.11965</a> [<a href="/pdf/2310.11965" title="Download PDF">pdf</a>, <a href="/format/2310.11965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Filling in the Gaps: Efficient Event Coreference Resolution using Graph  Autoencoder Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Langhe%2C+L">Loic De Langhe</a>, 
<a href="/search/cs?searchtype=author&query=De+Clercq%2C+O">Orph&#xe9;e De Clercq</a>, 
<a href="/search/cs?searchtype=author&query=Hoste%2C+V">Veronique Hoste</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce a novel and efficient method for Event Coreference Resolution
(ECR) applied to a lower-resourced language domain. By framing ECR as a graph
reconstruction task, we are able to combine deep semantic embeddings with
structural coreference chain knowledge to create a parameter-efficient family
of Graph Autoencoder models (GAE). Our method significantly outperforms
classical mention-pair methods on a large Dutch event coreference corpus in
terms of overall score, efficiency and training speed. Additionally, we show
that our models are consistently able to classify more difficult coreference
links and are far more robust in low-data settings when compared to
transformer-based mention-pair coreference algorithms.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11966" title="Abstract">arXiv:2310.11966</a> [<a href="/pdf/2310.11966" title="Download PDF">pdf</a>, <a href="/ps/2310.11966" title="Download PostScript">ps</a>, <a href="/format/2310.11966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible Payload Configuration for Satellites using Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mendonca%2C+M+O+K">Marcele O. K. Mendonca</a>, 
<a href="/search/cs?searchtype=author&query=Ortiz-Gomez%2C+F+G">Flor G. Ortiz-Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Querol%2C+J">Jorge Querol</a>, 
<a href="/search/cs?searchtype=author&query=Lagunas%2C+E">Eva Lagunas</a>, 
<a href="/search/cs?searchtype=author&query=Peralvo%2C+J+A+V">Juan A. V&#xe1;squez Peralvo</a>, 
<a href="/search/cs?searchtype=author&query=Baeza%2C+V+M">Victor Monzon Baeza</a>, 
<a href="/search/cs?searchtype=author&query=Chatzinotas%2C+S">Symeon Chatzinotas</a>, 
<a href="/search/cs?searchtype=author&query=Ottersten%2C+B">Bjorn Ottersten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in review for conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Satellite communications, essential for modern connectivity, extend access to
maritime, aeronautical, and remote areas where terrestrial networks are
unfeasible. Current GEO systems distribute power and bandwidth uniformly across
beams using multi-beam footprints with fractional frequency reuse. However,
recent research reveals the limitations of this approach in heterogeneous
traffic scenarios, leading to inefficiencies. To address this, this paper
presents a machine learning (ML)-based approach to Radio Resource Management
(RRM).
<br />We treat the RRM task as a regression ML problem, integrating RRM objectives
and constraints into the loss function that the ML algorithm aims at
minimizing. Moreover, we introduce a context-aware ML metric that evaluates the
ML model's performance but also considers the impact of its resource allocation
decisions on the overall performance of the communication system.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11967" title="Abstract">arXiv:2310.11967</a> [<a href="/pdf/2310.11967" title="Download PDF">pdf</a>, <a href="/format/2310.11967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Take the aTrain. Introducing an Interface for the Accessible  Transcription of Interviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haberl%2C+A">Armin Haberl</a>, 
<a href="/search/cs?searchtype=author&query=Flei%C3%9F%2C+J">J&#xfc;rgen Flei&#xdf;</a>, 
<a href="/search/cs?searchtype=author&query=Kowald%2C+D">Dominik Kowald</a>, 
<a href="/search/cs?searchtype=author&query=Thalmann%2C+S">Stefan Thalmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Install via Microsoft store: apps.microsoft.com/store/detail/atrain/9N15Q44SZNS2. Github: github.com/BANDAS-Center/aTrain
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">aTrain is an open-source and offline tool for transcribing audio data in
multiple languages with CPU and NVIDIA GPU support. It is specifically designed
for researchers using qualitative data generated from various forms of speech
interactions with research participants. aTrain requires no programming skills,
runs on most computers, does not require an internet connection, and was
verified not to upload data to any server. aTrain combines OpenAI's Whisper
model with speaker recognition to provide output that integrates with the
popular qualitative data analysis software tools MAXQDA and ATLAS.ti. It has an
easy-to-use graphical interface and is provided as a Windows-App through the
Microsoft Store allowing for simple installation by researchers. The source
code is freely available on GitHub. Having developed aTrain with a focus on
speed on local computers, we show that the transcription time on current mobile
CPUs is around 2 to 3 times the duration of the audio file using the
highest-accuracy transcription models. If an entry-level graphics card is
available, the transcription speed increases to 20% of the audio duration.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11970" title="Abstract">arXiv:2310.11970</a> [<a href="/pdf/2310.11970" title="Download PDF">pdf</a>, <a href="/format/2310.11970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Privacy Risks of Prompts in Visual Prompt Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yixin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+R">Rui Wen</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Berrang%2C+P">Pascal Berrang</a>, 
<a href="/search/cs?searchtype=author&query=Humbert%2C+M">Mathias Humbert</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the 33rd USENIX Security Symposium, August 14-16, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Large-scale pre-trained models are increasingly adapted to downstream tasks
through a new paradigm called prompt learning. In contrast to fine-tuning,
prompt learning does not update the pre-trained model's parameters. Instead, it
only learns an input perturbation, namely prompt, to be added to the downstream
task data for predictions. Given the fast development of prompt learning, a
well-generalized prompt inevitably becomes a valuable asset as significant
effort and proprietary data are used to create it. This naturally raises the
question of whether a prompt may leak the proprietary information of its
training data. In this paper, we perform the first comprehensive privacy
assessment of prompts learned by visual prompt learning through the lens of
property inference and membership inference attacks. Our empirical evaluation
shows that the prompts are vulnerable to both attacks. We also demonstrate that
the adversary can mount a successful property inference attack with limited
cost. Moreover, we show that membership inference attacks against prompts can
be successful with relaxed adversarial assumptions. We further make some
initial investigations on the defenses and observe that our method can mitigate
the membership inference attacks with a decent utility-defense trade-off but
fails to defend against property inference attacks. We hope our results can
shed light on the privacy risks of the popular prompt learning paradigm. To
facilitate the research in this direction, we will share our code and models
with the community.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11971" title="Abstract">arXiv:2310.11971</a> [<a href="/pdf/2310.11971" title="Download PDF">pdf</a>, <a href="/format/2310.11971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Generalization of Alignment with Human Preferences through  Group Invariant Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yuan Hua</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+W">Wenbin Lai</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Z">Zhiheng Xi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haoran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The success of AI assistants based on language models (LLMs) hinges crucially
on Reinforcement Learning from Human Feedback (RLHF), which enables the
generation of responses more aligned with human preferences. As universal AI
assistants, there's a growing expectation for them to perform consistently
across various domains. However, previous work shows that Reinforcement
Learning (RL) often exploits shortcuts to attain high rewards and overlooks
challenging samples. This focus on quick reward gains undermines both the
stability in training and the model's ability to generalize to new, unseen
data. In this work, we propose a novel approach that can learn a consistent
policy via RL across various data groups or domains. Given the challenges
associated with acquiring group annotations, our method automatically
classifies data into different groups, deliberately maximizing performance
variance. Then, we optimize the policy to perform well on challenging groups.
Lastly, leveraging the established groups, our approach adaptively adjusts the
exploration space, allocating more learning capacity to more challenging data
and preventing the model from over-optimizing on simpler data. Experimental
results indicate that our approach significantly enhances training stability
and model generalization.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11976" title="Abstract">arXiv:2310.11976</a> [<a href="/pdf/2310.11976" title="Download PDF">pdf</a>, <a href="/format/2310.11976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfoDiffusion: Information Entropy Aware Diffusion Process for  Non-Autoregressive Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Renzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Piji Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Diffusion models have garnered considerable interest in the field of text
generation. Several studies have explored text diffusion models with different
structures and applied them to various tasks, including named entity
recognition and summarization. However, there exists a notable disparity
between the "easy-first" text generation process of current diffusion models
and the "keyword-first" natural text generation process of humans, which has
received limited attention. To bridge this gap, we propose InfoDiffusion, a
non-autoregressive text diffusion model. Our approach introduces a
"keyinfo-first" generation strategy and incorporates a noise schedule based on
the amount of text information. In addition, InfoDiffusion combines
self-conditioning with a newly proposed partially noising model structure.
Experimental results show that InfoDiffusion outperforms the baseline model in
terms of generation quality and diversity, as well as exhibiting higher
sampling efficiency.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11983" title="Abstract">arXiv:2310.11983</a> [<a href="/pdf/2310.11983" title="Download PDF">pdf</a>, <a href="/format/2310.11983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Consensus-Based Generalized Multi-Population Aggregative Game with  Application to Charging Coordination of Electric Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ghavami%2C+M">Mahsa Ghavami</a>, 
<a href="/search/eess?searchtype=author&query=Bakhshayesh%2C+B+G">Babak Ghaffarzadeh Bakhshayesh</a>, 
<a href="/search/eess?searchtype=author&query=Haeri%2C+M">Mohammad Haeri</a>, 
<a href="/search/eess?searchtype=author&query=Como%2C+G">Giacomo Como</a>, 
<a href="/search/eess?searchtype=author&query=Kebriaei%2C+H">Hamed Kebriaei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper introduces a consensus-based generalized multi-population
aggregative game coordination approach with application to electric vehicles
charging under transmission line constraints. The algorithm enables agents to
seek an equilibrium solution while considering the limited infrastructure
capacities that impose coupling constraints among the users. The Nash-seeking
algorithm consists of two interrelated iterations. In the upper layer,
population coordinators collaborate for a distributed estimation of the
coupling aggregate term in the agents' cost function and the associated
Lagrange multiplier of the coupling constraint, transmitting the latest updated
values to their population's agents. In the lower layer, each agent updates its
best response based on the most recent information received and communicates it
back to its population coordinator. For the case when the agents' best response
mappings are non-expansive, we prove the algorithm's convergence to the
generalized Nash equilibrium point of the game. Simulation results demonstrate
the algorithm's effectiveness in achieving equilibrium in the presence of a
coupling constraint.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11984" title="Abstract">arXiv:2310.11984</a> [<a href="/pdf/2310.11984" title="Download PDF">pdf</a>, <a href="/format/2310.11984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Interpolation to Extrapolation: Complete Length Generalization for  Arithmetic Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+S">Shaoxiong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yining Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Since its introduction, the transformer model has demonstrated outstanding
performance across various tasks. However, there are still unresolved issues
regarding length generalization, particularly in algorithmic tasks. In this
paper, we investigate the inherent capabilities of transformer models in
learning arithmetic algorithms, such as addition and multiplication. Through
experiments and attention analysis, we identify a number of crucial factors for
achieving optimal length generalization. We show that transformer models are
able to generalize to long lengths with the help of targeted attention biasing.
We then introduce Attention Bias Calibration (ABC), a calibration stage that
enables the model to automatically learn the proper attention biases, which we
link to mechanisms in relative position encoding. We demonstrate that using
ABC, the transformer model can achieve unprecedented perfect length
generalization on certain arithmetic tasks.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11985" title="Abstract">arXiv:2310.11985</a> [<a href="/pdf/2310.11985" title="Download PDF">pdf</a>, <a href="/format/2310.11985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Finite-Horizon Approach to Active Level Set Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kearns%2C+P">Phillip Kearns</a>, 
<a href="/search/cs?searchtype=author&query=Jedynak%2C+B">Bruno Jedynak</a>, 
<a href="/search/cs?searchtype=author&query=Lipor%2C+J">John Lipor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider the problem of active learning in the context of spatial sampling
for level set estimation (LSE), where the goal is to localize all regions where
a function of interest lies above/below a given threshold as quickly as
possible. We present a finite-horizon search procedure to perform LSE in one
dimension while optimally balancing both the final estimation error and the
distance traveled for a fixed number of samples. A tuning parameter is used to
trade off between the estimation accuracy and distance traveled. We show that
the resulting optimization problem can be solved in closed form and that the
resulting policy generalizes existing approaches to this problem. We then show
how this approach can be used to perform level set estimation in higher
dimensions under the popular Gaussian process model. Empirical results on
synthetic data indicate that as the cost of travel increases, our method's
ability to treat distance nonmyopically allows it to significantly improve on
the state of the art. On real air quality data, our approach achieves roughly
one fifth the estimation error at less than half the cost of competing
algorithms.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11986" title="Abstract">arXiv:2310.11986</a> [<a href="/pdf/2310.11986" title="Download PDF">pdf</a>, <a href="/format/2310.11986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sociotechnical Safety Evaluation of Generative AI Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weidinger%2C+L">Laura Weidinger</a>, 
<a href="/search/cs?searchtype=author&query=Rauh%2C+M">Maribeth Rauh</a>, 
<a href="/search/cs?searchtype=author&query=Marchal%2C+N">Nahema Marchal</a>, 
<a href="/search/cs?searchtype=author&query=Manzini%2C+A">Arianna Manzini</a>, 
<a href="/search/cs?searchtype=author&query=Hendricks%2C+L+A">Lisa Anne Hendricks</a>, 
<a href="/search/cs?searchtype=author&query=Mateos-Garcia%2C+J">Juan Mateos-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Bergman%2C+S">Stevie Bergman</a>, 
<a href="/search/cs?searchtype=author&query=Kay%2C+J">Jackie Kay</a>, 
<a href="/search/cs?searchtype=author&query=Griffin%2C+C">Conor Griffin</a>, 
<a href="/search/cs?searchtype=author&query=Bariach%2C+B">Ben Bariach</a>, 
<a href="/search/cs?searchtype=author&query=Gabriel%2C+I">Iason Gabriel</a>, 
<a href="/search/cs?searchtype=author&query=Rieser%2C+V">Verena Rieser</a>, 
<a href="/search/cs?searchtype=author&query=Isaac%2C+W">William Isaac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> main paper p.1-29, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
<p class="mathjax">Generative AI systems produce a range of risks. To ensure the safety of
generative AI systems, these risks must be evaluated. In this paper, we make
two main contributions toward establishing such evaluations. First, we propose
a three-layered framework that takes a structured, sociotechnical approach to
evaluating these risks. This framework encompasses capability evaluations,
which are the main current approach to safety evaluation. It then reaches
further by building on system safety principles, particularly the insight that
context determines whether a given capability may cause harm. To account for
relevant context, our framework adds human interaction and systemic impacts as
additional layers of evaluation. Second, we survey the current state of safety
evaluation of generative AI systems and create a repository of existing
evaluations. Three salient evaluation gaps emerge from this analysis. We
propose ways forward to closing these gaps, outlining practical steps as well
as roles and responsibilities for different actors. Sociotechnical safety
evaluation is a tractable approach to the robust and comprehensive safety
evaluation of generative AI systems.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11989" title="Abstract">arXiv:2310.11989</a> [<a href="/pdf/2310.11989" title="Download PDF">pdf</a>, <a href="/format/2310.11989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Clustering with External Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunfan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+P">Peng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Dezhong Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+J">Jiancheng Lv</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jianping Fan</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xi Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The core of clustering is incorporating prior knowledge to construct
supervision signals. From classic k-means based on data compactness to recent
contrastive clustering guided by self-supervision, the evolution of clustering
methods intrinsically corresponds to the progression of supervision signals. At
present, substantial efforts have been devoted to mining internal supervision
signals from data. Nevertheless, the abundant external knowledge such as
semantic descriptions, which naturally conduces to clustering, is regrettably
overlooked. In this work, we propose leveraging external knowledge as a new
supervision signal to guide clustering, even though it seems irrelevant to the
given data. To implement and validate our idea, we design an externally guided
clustering method (Text-Aided Clustering, TAC), which leverages the textual
semantics of WordNet to facilitate image clustering. Specifically, TAC first
selects and retrieves WordNet nouns that best distinguish images to enhance the
feature discriminability. Then, to improve image clustering performance, TAC
collaborates text and image modalities by mutually distilling cross-modal
neighborhood information. Experiments demonstrate that TAC achieves
state-of-the-art performance on five widely used and three more challenging
image clustering benchmarks, including the full ImageNet-1K dataset.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11991" title="Abstract">arXiv:2310.11991</a> [<a href="/pdf/2310.11991" title="Download PDF">pdf</a>, <a href="/format/2310.11991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Removing Spurious Concepts from Neural Network Representations via Joint  Subspace Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holstege%2C+F">Floris Holstege</a>, 
<a href="/search/cs?searchtype=author&query=Wouters%2C+B">Bram Wouters</a>, 
<a href="/search/cs?searchtype=author&query=van+Giersbergen%2C+N">Noud van Giersbergen</a>, 
<a href="/search/cs?searchtype=author&query=Diks%2C+C">Cees Diks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under Review. 33 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Out-of-distribution generalization in neural networks is often hampered by
spurious correlations. A common strategy is to mitigate this by removing
spurious concepts from the neural network representation of the data. Existing
concept-removal methods tend to be overzealous by inadvertently eliminating
features associated with the main task of the model, thereby harming model
performance. We propose an iterative algorithm that separates spurious from
main-task concepts by jointly identifying two low-dimensional orthogonal
subspaces in the neural network representation. We evaluate the algorithm on
benchmark datasets for computer vision (Waterbirds, CelebA) and natural
language processing (MultiNLI), and show that it outperforms existing concept
removal methods
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11994" title="Abstract">arXiv:2310.11994</a> [<a href="/pdf/2310.11994" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel Log Spectra index (PaLOSi): a quality metric in large scale  resting EEG preprocessing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+J">Jie Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Langer%2C+N">Nicolas Langer</a>, 
<a href="/search/cs?searchtype=author&query=Bosch-Bayard%2C+J">Jorge Bosch-Bayard</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Z">Zhao Lv</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+D">Dezhong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Valdes-Sosa%2C+P+A">Pedro Antonio Valdes-Sosa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Signal Processing (eess.SP); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Toward large scale electrophysiology data analysis, many preprocessing
pipelines are developed to reject artifacts as the prerequisite step before the
downstream analysis. A mainstay of these pipelines is based on the data driven
approach -- Independent Component Analysis (ICA). Nevertheless, there is little
effort put to the preprocessing quality control. In this paper, attentions to
this issue were carefully paid by our observation that after running ICA based
preprocessing pipeline: some subjects showed approximately Parallel
multichannel Log power Spectra (PaLOS), namely, multichannel power spectra are
proportional to each other. Firstly, the presence of PaLOS and its implications
to connectivity analysis were described by real instance and simulation;
secondly, we built its mathematical model and proposed the PaLOS index (PaLOSi)
based on the common principal component analysis to detect its presence;
thirdly, the performance of PaLOSi was tested on 30094 cases of EEG from 5
databases. The results showed that 1) the PaLOS implies a sole source which is
physiologically implausible. 2) PaLOSi can detect the excessive elimination of
brain components and is robust in terms of channel number, electrode layout,
reference, and the other factors. 3) PaLOSi can output the channel and
frequency wise index to help for in-depth check. This paper presented the PaLOS
issue in the quality control step after running the preprocessing pipeline and
the proposed PaLOSi may serve as a novel data quality metric in the large-scale
automatic preprocessing.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12001" title="Abstract">arXiv:2310.12001</a> [<a href="/pdf/2310.12001" title="Download PDF">pdf</a>, <a href="/format/2310.12001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Flow Networks in Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pyla%2C+M">Mateusz Pyla</a>, 
<a href="/search/cs?searchtype=author&query=Deja%2C+K">Kamil Deja</a>, 
<a href="/search/cs?searchtype=author&query=Twardowski%2C+B">Bart&#x142;omiej Twardowski</a>, 
<a href="/search/cs?searchtype=author&query=Trzci%C5%84ski%2C+T">Tomasz Trzci&#x144;ski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to NeurIPS 2023 Workshop on Diffusion Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Bayesian Flow Networks (BFNs) has been recently proposed as one of the most
promising direction to universal generative modelling, having ability to learn
any of the data type. Their power comes from the expressiveness of neural
networks and Bayesian inference which make them suitable in the context of
continual learning. We delve into the mechanics behind BFNs and conduct the
experiments to empirically verify the generative capabilities on non-stationary
data.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12004" title="Abstract">arXiv:2310.12004</a> [<a href="/pdf/2310.12004" title="Download PDF">pdf</a>, <a href="/format/2310.12004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Super-resolution Via Latent Diffusion: A Sampling-space Mixture Of  Experts And Frequency-augmented Decoder Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Feng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jinxi Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wei Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The recent use of diffusion prior, enhanced by pre-trained text-image models,
has markedly elevated the performance of image super-resolution (SR). To
alleviate the huge computational cost required by pixel-based diffusion SR,
latent-based methods utilize a feature encoder to transform the image and then
implement the SR image generation in a compact latent space. Nevertheless,
there are two major issues that limit the performance of latent-based
diffusion. First, the compression of latent space usually causes reconstruction
distortion. Second, huge computational cost constrains the parameter scale of
the diffusion model. To counteract these issues, we first propose a frequency
compensation module that enhances the frequency components from latent space to
pixel space. The reconstruction distortion (especially for high-frequency
information) can be significantly decreased. Then, we propose to use
Sample-Space Mixture of Experts (SS-MoE) to achieve more powerful latent-based
SR, which steadily improves the capacity of the model without a significant
increase in inference costs. These carefully crafted designs contribute to
performance improvements in largely explored 4x blind super-resolution
benchmarks and extend to large magnification factors, i.e., 8x image SR
benchmarks. The code is available at https://github.com/amandaluof/moe_sr.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12006" title="Abstract">arXiv:2310.12006</a> [<a href="/pdf/2310.12006" title="Download PDF">pdf</a>, <a href="/format/2310.12006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guaranteed, Predictable, Polynomial AGV Time-Pathing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Forster%2C+J">James Forster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Data Structures and Algorithms (cs.DS); Information Theory (cs.IT); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper we present a framework of key algorithms and data-structures
for efficiently generating timetables for any number of AGVs from any given
positioning on any given graph to accomplish any given demands as long as a few
easily satisfiable assumptions are met. Our proposed algorithms provide
guaranteed solutions in predictable polynomial running-times, which is
fundamental to any real-time application. We also develop an improved
geographic reservation algorithm that provides a substantial run-time
improvement of the previously best-known algorithm from $O(nm)$ to $O(n)$.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12007" title="Abstract">arXiv:2310.12007</a> [<a href="/pdf/2310.12007" title="Download PDF">pdf</a>, <a href="/format/2310.12007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KI-PMF: Knowledge Integrated Plausible Motion Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vivekanandan%2C+A">Abhishek Vivekanandan</a>, 
<a href="/search/cs?searchtype=author&query=Abouelazm%2C+A">Ahmed Abouelazm</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6rner%2C+P">Philip Sch&#xf6;rner</a>, 
<a href="/search/cs?searchtype=author&query=Z%C3%B6llner%2C+J+M">J. Marius Z&#xf6;llner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Accurately forecasting the motion of traffic actors is crucial for the
deployment of autonomous vehicles at a large scale. Current trajectory
forecasting approaches primarily concentrate on optimizing a loss function with
a specific metric, which can result in predictions that do not adhere to
physical laws or violate external constraints. Our objective is to incorporate
explicit knowledge priors that allow a network to forecast future trajectories
in compliance with both the kinematic constraints of a vehicle and the geometry
of the driving environment. To achieve this, we introduce a non-parametric
pruning layer and attention layers to integrate the defined knowledge priors.
Our proposed method is designed to ensure reachability guarantees for traffic
actors in both complex and dynamic situations. By conditioning the network to
follow physical laws, we can obtain accurate and safe predictions, essential
for maintaining autonomous vehicles' safety and efficiency in real-world
settings.In summary, this paper presents concepts that prevent off-road
predictions for safe and reliable motion forecasting by incorporating knowledge
priors into the training process.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12008" title="Abstract">arXiv:2310.12008</a> [<a href="/pdf/2310.12008" title="Download PDF">pdf</a>, <a href="/format/2310.12008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiwei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Guti%C3%A9rrez-Basulto%2C+V">V&#xed;ctor Guti&#xe9;rrez-Basulto</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Z">Zhiliang Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ru Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Knowledge graph entity typing (KGET) aims at inferring plausible types of
entities in knowledge graphs. Existing approaches to KGET focus on how to
better encode the knowledge provided by the neighbors and types of an entity
into its representation. However, they ignore the semantic knowledge provided
by the way in which types can be clustered together. In this paper, we propose
a novel method called Multi-view Contrastive Learning for knowledge graph
Entity Typing (MCLET), which effectively encodes the coarse-grained knowledge
provided by clusters into entity and type embeddings. MCLET is composed of
three modules: i) Multi-view Generation and Encoder module, which encodes
structured information from entity-type, entity-cluster and cluster-type views;
ii) Cross-view Contrastive Learning module, which encourages different views to
collaboratively improve view-specific representations of entities and types;
iii) Entity Typing Prediction module, which integrates multi-head attention and
a Mixture-of-Experts strategy to infer missing entity types. Extensive
experiments show the strong performance of MCLET compared to the
state-of-the-art
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12011" title="Abstract">arXiv:2310.12011</a> [<a href="/pdf/2310.12011" title="Download PDF">pdf</a>, <a href="/format/2310.12011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gold: A Global and Local-aware Denoising Framework for Commonsense  Knowledge Graph Noise Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zheye Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning,
yet constructing them through human annotations can be costly. As a result,
various automatic methods have been proposed to construct CSKG with larger
semantic coverage. However, these unsupervised approaches introduce spurious
noise that can lower the quality of the resulting CSKG, which cannot be tackled
easily by existing denoising algorithms due to the unique characteristics of
nodes and structures in CSKGs. To address this issue, we propose Gold (Global
and Local-aware Denoising), a denoising framework for CSKGs that incorporates
entity semantic information, global rules, and local structural information
from the CSKG. Experiment results demonstrate that Gold outperforms all
baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks.
Furthermore, we show that denoising a real-world CSKG is effective and even
benefits the downstream zero-shot commonsense question-answering task.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12017" title="Abstract">arXiv:2310.12017</a> [<a href="/pdf/2310.12017" title="Download PDF">pdf</a>, <a href="/format/2310.12017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Decision-based Black-box Attacks on Face Forgery Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaoyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kaixun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shouhong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqiang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Face forgery generation technologies generate vivid faces, which have raised
public concerns about security and privacy. Many intelligent systems, such as
electronic payment and identity verification, rely on face forgery detection.
Although face forgery detection has successfully distinguished fake faces,
recent studies have demonstrated that face forgery detectors are very
vulnerable to adversarial examples. Meanwhile, existing attacks rely on network
architectures or training datasets instead of the predicted labels, which leads
to a gap in attacking deployed applications. To narrow this gap, we first
explore the decision-based attacks on face forgery detection. However, applying
existing decision-based attacks directly suffers from perturbation
initialization failure and low image quality. First, we propose cross-task
perturbation to handle initialization failures by utilizing the high
correlation of face features on different tasks. Then, inspired by using
frequency cues by face forgery detection, we propose the frequency
decision-based attack. We add perturbations in the frequency domain and then
constrain the visual quality in the spatial domain. Finally, extensive
experiments demonstrate that our method achieves state-of-the-art attack
performance on FaceForensics++, CelebDF, and industrial APIs, with high query
efficiency and guaranteed image quality. Further, the fake faces by our method
can pass face forgery detection and face recognition, which exposes the
security problems of face forgery detectors.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12019" title="Abstract">arXiv:2310.12019</a> [<a href="/pdf/2310.12019" title="Download PDF">pdf</a>, <a href="/format/2310.12019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DesignQuizzer: A Community-Powered Conversational Agent for Learning  Visual Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Zhenhui Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiaoyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Oulasvirta%2C+A">Antti Oulasvirta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Online design communities, where members exchange free-form views on others'
designs, offer a space for beginners to learn visual design. However, the
content of these communities is often unorganized for learners, containing many
redundancies and irrelevant comments. In this paper, we propose a computational
approach for leveraging online design communities to run a conversational agent
that assists informal learning of visual elements (e.g., color and space). Our
method extracts critiques, suggestions, and rationales on visual elements from
comments. We present DesignQuizzer, which asks questions about visual design in
UI examples and provides structured comment summaries. Two user studies
demonstrate the engagement and usefulness of DesignQuizzer compared with the
baseline (reading reddit.com/r/UI_design). We also showcase how effectively
novices can apply what they learn with DesignQuizzer in a design critique task
and a visual design task. We discuss how to use our approach with other
communities and offer design considerations for community-powered learning
support tools.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12020" title="Abstract">arXiv:2310.12020</a> [<a href="/pdf/2310.12020" title="Download PDF">pdf</a>, <a href="/format/2310.12020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic  Tabletop Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengqiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wicke%2C+P">Philipp Wicke</a>, 
<a href="/search/cs?searchtype=author&query=%C5%9Eenel%2C+L+K">L&#xfc;tfi Kerem &#x15e;enel</a>, 
<a href="/search/cs?searchtype=author&query=Figueredo%2C+L">Luis Figueredo</a>, 
<a href="/search/cs?searchtype=author&query=Naceri%2C+A">Abdeldjallil Naceri</a>, 
<a href="/search/cs?searchtype=author&query=Haddadin%2C+S">Sami Haddadin</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures. The video and code of LoHoRavens are available at <a href="https://shengqiang-zhang.github.io/lohoravens-webpage/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The convergence of embodied agents and large language models (LLMs) has
brought significant advancements to embodied instruction following.
Particularly, the strong reasoning capabilities of LLMs make it possible for
robots to perform long-horizon tasks without expensive annotated
demonstrations. However, public benchmarks for testing the long-horizon
reasoning capabilities of language-conditioned robots in various scenarios are
still missing. To fill this gap, this work focuses on the tabletop manipulation
task and releases a simulation benchmark, \textit{LoHoRavens}, which covers
various long-horizon reasoning aspects spanning color, size, space, arithmetics
and reference. Furthermore, there is a key modality bridging problem for
long-horizon manipulation tasks with LLMs: how to incorporate the observation
feedback during robot execution for the LLM's closed-loop planning, which is
however less studied by prior work. We investigate two methods of bridging the
modality gap: caption generation and learnable interface for incorporating
explicit and implicit observation feedback to the LLM, respectively. These
methods serve as the two baselines for our proposed benchmark. Experiments show
that both methods struggle to solve some tasks, indicating long-horizon
manipulation tasks are still challenging for current popular models. We expect
the proposed public benchmark and baselines can help the community develop
better models for long-horizon tabletop manipulation tasks.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12024" title="Abstract">arXiv:2310.12024</a> [<a href="/pdf/2310.12024" title="Download PDF">pdf</a>, <a href="/format/2310.12024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CORE: A Few-Shot Company Relation Classification Dataset for Robust  Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borchert%2C+P">Philipp Borchert</a>, 
<a href="/search/cs?searchtype=author&query=De+Weerdt%2C+J">Jochen De Weerdt</a>, 
<a href="/search/cs?searchtype=author&query=Coussement%2C+K">Kristof Coussement</a>, 
<a href="/search/cs?searchtype=author&query=De+Caigny%2C+A">Arno De Caigny</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce CORE, a dataset for few-shot relation classification (RC)
focused on company relations and business entities. CORE includes 4,708
instances of 12 relation types with corresponding textual evidence extracted
from company Wikipedia pages. Company names and business entities pose a
challenge for few-shot RC models due to the rich and diverse information
associated with them. For example, a company name may represent the legal
entity, products, people, or business divisions depending on the context.
Therefore, deriving the relation type between entities is highly dependent on
textual context. To evaluate the performance of state-of-the-art RC models on
the CORE dataset, we conduct experiments in the few-shot domain adaptation
setting. Our results reveal substantial performance gaps, confirming that
models trained on different domains struggle to adapt to CORE. Interestingly,
we find that models trained on CORE showcase improved out-of-domain
performance, which highlights the importance of high-quality data for robust
domain adaptation. Specifically, the information richness embedded in business
entities allows models to focus on contextual nuances, reducing their reliance
on superficial clues such as relation-specific verbs. In addition to the
dataset, we provide relevant code snippets to facilitate reproducibility and
encourage further research in the field.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12031" title="Abstract">arXiv:2310.12031</a> [<a href="/pdf/2310.12031" title="Download PDF">pdf</a>, <a href="/format/2310.12031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor  Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zemskova%2C+T">Tatiana Zemskova</a>, 
<a href="/search/cs?searchtype=author&query=Kichik%2C+M">Margarita Kichik</a>, 
<a href="/search/cs?searchtype=author&query=Yudin%2C+D">Dmitry Yudin</a>, 
<a href="/search/cs?searchtype=author&query=Staroverov%2C+A">Aleksei Staroverov</a>, 
<a href="/search/cs?searchtype=author&query=Panov%2C+A">Aleksandr Panov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents an adaptive transformer model named SegmATRon for
embodied image semantic segmentation. Its distinctive feature is the adaptation
of model weights during inference on several images using a hybrid
multicomponent loss function. We studied this model on datasets collected in
the photorealistic Habitat and the synthetic AI2-THOR Simulators. We showed
that obtaining additional images using the agent's actions in an indoor
environment can improve the quality of semantic segmentation. The code of the
proposed approach and datasets are publicly available at
https://github.com/wingrune/SegmATRon.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12032" title="Abstract">arXiv:2310.12032</a> [<a href="/pdf/2310.12032" title="Download PDF">pdf</a>, <a href="/format/2310.12032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact and efficient solutions of the LMC Multitask Gaussian Process  model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Truffinet%2C+O">Olivier Truffinet</a> (CEA Saclay), 
<a href="/search/cs?searchtype=author&query=Ammar%2C+K">Karim Ammar</a> (CEA Saclay), 
<a href="/search/cs?searchtype=author&query=Argaud%2C+J">Jean-Philippe Argaud</a> (EDF R&amp;D), 
<a href="/search/cs?searchtype=author&query=Bouriquet%2C+B">Bertrand Bouriquet</a> (EDF)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures, submitted to AISTATS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The Linear Model of Co-regionalization (LMC) is a very general model of
multitask gaussian process for regression or classification. While its
expressivity and conceptual simplicity are appealing, naive implementations
have cubic complexity in the number of datapoints and number of tasks, making
approximations mandatory for most applications. However, recent work has shown
that under some conditions the latent processes of the model can be decoupled,
leading to a complexity that is only linear in the number of said processes. We
here extend these results, showing from the most general assumptions that the
only condition necessary to an efficient exact computation of the LMC is a mild
hypothesis on the noise model. We introduce a full parametrization of the
resulting \emph{projected LMC} model, and an expression of the marginal
likelihood enabling efficient optimization. We perform a parametric study on
synthetic data to show the excellent performance of our approach, compared to
an unrestricted exact LMC and approximations of the latter. Overall, the
projected LMC appears as a credible and simpler alternative to state-of-the art
models, which greatly facilitates some computations such as leave-one-out
cross-validation and fantasization.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12033" title="Abstract">arXiv:2310.12033</a> [<a href="/pdf/2310.12033" title="Download PDF">pdf</a>, <a href="/format/2310.12033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Drug Property Prediction with Density Estimation under  Covariate Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laghuvarapu%2C+S">Siddhartha Laghuvarapu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In drug discovery, it is vital to confirm the predictions of pharmaceutical
properties from computational models using costly wet-lab experiments. Hence,
obtaining reliable uncertainty estimates is crucial for prioritizing drug
molecules for subsequent experimental validation. Conformal Prediction (CP) is
a promising tool for creating such prediction sets for molecular properties
with a coverage guarantee. However, the exchangeability assumption of CP is
often challenged with covariate shift in drug discovery tasks: Most datasets
contain limited labeled data, which may not be representative of the vast
chemical space from which molecules are drawn. To address this limitation, we
propose a method called CoDrug that employs an energy-based model leveraging
both training data and unlabelled data, and Kernel Density Estimation (KDE) to
assess the densities of a molecule set. The estimated densities are then used
to weigh the molecule samples while building prediction sets and rectifying for
distribution shift. In extensive experiments involving realistic distribution
drifts in various small-molecule drug discovery tasks, we demonstrate the
ability of CoDrug to provide valid prediction sets and its utility in
addressing the distribution shift arising from de novo drug design models. On
average, using CoDrug can reduce the coverage gap by over 35% when compared to
conformal prediction sets not adjusted for covariate shift.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12035" title="Abstract">arXiv:2310.12035</a> [<a href="/pdf/2310.12035" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking flow: Decoding dynamic flow experience on a sub-minute  timescale through performance in fine fingertip force control task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+B">Bohao Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sirui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Kaiping Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dangxiao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Flow, an optimal mental state merging action and awareness, significantly
impacts performance, emotion and wellbeing in real-world contexts. However,
capturing its fluctuations on a sub-minute timescale is challenging due to the
sparsity of the existing flow measuring tools. Here we present a virtual
reality fine fingertip force control (F3C) task to induce flow, wherein the
task challenge is set at a compatible level with personal skill, and to track
the flow fluctuations from the synchronous force control performance. We
extract eight performance metrics from the fingertip force sequence and reveal
their significant differences under distinct flow states. Further, we built a
flow decoder and demonstrated that the flow variations can be decoded using
selected metrics. The predicted values reach significant correlation with the
self-reported flow intensity (r=0.81). This study showcases the feasibility of
tracking intrinsic flow variations with high temporal resolution using task
performance measures.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12036" title="Abstract">arXiv:2310.12036</a> [<a href="/pdf/2310.12036" title="Download PDF">pdf</a>, <a href="/format/2310.12036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Theoretical Paradigm to Understand Learning from Human  Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azar%2C+M+G">Mohammad Gheshlaghi Azar</a>, 
<a href="/search/cs?searchtype=author&query=Rowland%2C+M">Mark Rowland</a>, 
<a href="/search/cs?searchtype=author&query=Piot%2C+B">Bilal Piot</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Daniel Guo</a>, 
<a href="/search/cs?searchtype=author&query=Calandriello%2C+D">Daniele Calandriello</a>, 
<a href="/search/cs?searchtype=author&query=Valko%2C+M">Michal Valko</a>, 
<a href="/search/cs?searchtype=author&query=Munos%2C+R">R&#xe9;mi Munos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">The prevalent deployment of learning from human preferences through
reinforcement learning (RLHF) relies on two important approximations: the first
assumes that pairwise preferences can be substituted with pointwise rewards.
The second assumes that a reward model trained on these pointwise rewards can
generalize from collected data to out-of-distribution data sampled by the
policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an
approach that bypasses the second approximation and learn directly a policy
from collected data without the reward modelling stage. However, this method
still heavily relies on the first approximation.
<br />In this paper we try to gain a deeper theoretical understanding of these
practical algorithms. In particular we derive a new general objective called
$\Psi$PO for learning from human preferences that is expressed in terms of
pairwise preferences and therefore bypasses both approximations. This new
general objective allows us to perform an in-depth analysis of the behavior of
RLHF and DPO (as special cases of $\Psi$PO) and to identify their potential
pitfalls. We then consider another special case for $\Psi$PO by setting $\Psi$
simply to Identity, for which we can derive an efficient optimisation
procedure, prove performance guarantees and demonstrate its empirical
superiority to DPO on some illustrative examples.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12037" title="Abstract">arXiv:2310.12037</a> [<a href="/pdf/2310.12037" title="Download PDF">pdf</a>, <a href="/format/2310.12037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Envisioning the Future of Cyber Security in Post-Quantum Era: A Survey  on PQ Standardization, Applications, Challenges and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darzi%2C+S">Saleh Darzi</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+K">Kasra Ahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Aghapour%2C+S">Saeed Aghapour</a>, 
<a href="/search/cs?searchtype=author&query=Yavuz%2C+A+A">Attila Altay Yavuz</a>, 
<a href="/search/cs?searchtype=author&query=Kermani%2C+M+M">Mehran Mozaffari Kermani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The rise of quantum computers exposes vulnerabilities in current public key
cryptographic protocols, necessitating the development of secure post-quantum
(PQ) schemes. Hence, we conduct a comprehensive study on various PQ approaches,
covering the constructional design, structural vulnerabilities, and offer
security assessments, implementation evaluations, and a particular focus on
side-channel attacks. We analyze global standardization processes, evaluate
their metrics in relation to real-world applications, and primarily focus on
standardized PQ schemes, selected additional signature competition candidates,
and PQ-secure cutting-edge schemes beyond standardization. Finally, we present
visions and potential future directions for a seamless transition to the PQ
era.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12039" title="Abstract">arXiv:2310.12039</a> [<a href="/pdf/2310.12039" title="Download PDF">pdf</a>, <a href="/format/2310.12039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ordered Reliability Direct Error Pattern Testing Decoding Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hadavian%2C+R">Reza Hadavian</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaoting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Truhachev%2C+D">Dmitri Truhachev</a>, 
<a href="/search/cs?searchtype=author&query=El-Sankary%2C+K">Kamal El-Sankary</a>, 
<a href="/search/cs?searchtype=author&query=Ebrahimzad%2C+H">Hamid Ebrahimzad</a>, 
<a href="/search/cs?searchtype=author&query=Najafi%2C+H">Hossein Najafi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We introduce a novel universal soft-decision decoding algorithm for binary
block codes called ordered reliability direct error pattern testing (ORDEPT).
Our results, obtained for a variety of popular short high-rate codes,
demonstrate that ORDEPT outperforms state-of-the-art decoding algorithms of
comparable complexity such as ordered reliability bits guessing random additive
noise decoding (ORBGRAND) in terms of the decoding error probability and
latency. The improvements carry on to the iterative decoding of product codes
and convolutional product-like codes, where we present a new adaptive decoding
algorithm and demonstrate the ability of ORDEPT to efficiently find multiple
candidate codewords to produce soft output.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12044" title="Abstract">arXiv:2310.12044</a> [<a href="/pdf/2310.12044" title="Download PDF">pdf</a>, <a href="/format/2310.12044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TeslaCharge: Smart Robotic Charger Driven by Impedance Control and Human  Haptic Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alyounes%2C+O">Oussama Alyounes</a>, 
<a href="/search/cs?searchtype=author&query=Cabrera%2C+M+A">Miguel Altamirano Cabrera</a>, 
<a href="/search/cs?searchtype=author&query=Tsetserukou%2C+D">Dzmitry Tsetserukou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 21st IEEE International Conference on Advanced Robotics (ICAR 2023). IEEE copyright
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The growing demand for electric vehicles requires the development of
automated car charging methods. At the moment, the process of charging an
electric car is completely manual, and that requires physical effort to
accomplish the task, which is not suitable for people with disabilities.
Typically, the effort in the research is focused on detecting the position and
orientation of the socket, which resulted in a relatively high accuracy, $\pm 5
\: mm $ and $\pm 10^o$. However, this accuracy is not enough to complete the
charging process. In this work, we focus on designing a novel methodology for
robust robotic plug-in and plug-out based on human haptics, to overcome the
error in the position and orientation of the socket. Participants were invited
to perform the charging task, and their cognitive capabilities were recognized
by measuring the applied forces along with the movement of the charger. Three
controllers were designed based on impedance control to mimic the human
patterns of charging an electric car. The recorded data from humans were used
to calibrate the parameters of the impedance controllers: inertia $M_d$,
damping $D_d$, and stiffness $K_d$. A robotic validation was performed, where
the designed controllers were applied to the robot UR10. Using the proposed
controllers and the human kinesthetic data, it was possible to successfully
automate the operation of charging an electric car.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12049" title="Abstract">arXiv:2310.12049</a> [<a href="/pdf/2310.12049" title="Download PDF">pdf</a>, <a href="/format/2310.12049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison  Scaling of Texts with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+P+Y">Patrick Y. Wu</a>, 
<a href="/search/cs?searchtype=author&query=Nagler%2C+J">Jonathan Nagler</a>, 
<a href="/search/cs?searchtype=author&query=Tucker%2C+J+A">Joshua A. Tucker</a>, 
<a href="/search/cs?searchtype=author&query=Messing%2C+S">Solomon Messing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Existing text scaling methods often require a large corpus, struggle with
short texts, or require labeled data. We develop a text scaling method that
leverages the pattern recognition capabilities of generative large language
models (LLMs). Specifically, we propose concept-guided chain-of-thought
(CGCoT), which uses prompts designed to summarize ideas and identify target
parties in texts to generate concept-specific breakdowns, in many ways similar
to guidance for human coder content analysis. CGCoT effectively shifts pairwise
text comparisons from a reasoning problem to a pattern recognition problem. We
then pairwise compare concept-specific breakdowns using an LLM. We use the
results of these pairwise comparisons to estimate a scale using the
Bradley-Terry model. We use this approach to scale affective speech on Twitter.
Our measures correlate more strongly with human judgments than alternative
approaches like Wordfish. Besides a small set of pilot data to develop the
CGCoT prompts, our measures require no additional labeled data and produce
binary predictions comparable to a RoBERTa-Large model fine-tuned on thousands
of human-labeled tweets. We demonstrate how combining substantive knowledge
with LLMs can create state-of-the-art measures of abstract concepts.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12051" title="Abstract">arXiv:2310.12051</a> [<a href="/pdf/2310.12051" title="Download PDF">pdf</a>, <a href="/ps/2310.12051" title="Download PostScript">ps</a>, <a href="/format/2310.12051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simpler and Higher Lower Bounds for Shortcut Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Williams%2C+V+V">Virginia Vassilevska Williams</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yinzhan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zixuan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in SODA 2024. Abstract shortened to fit arXiv requirements
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We provide a variety of lower bounds for the well-known shortcut set problem:
how much can one decrease the diameter of a directed graph on $n$ vertices and
$m$ edges by adding $O(n)$ or $O(m)$ of shortcuts from the transitive closure
of the graph. Our results are based on a vast simplification of the recent
construction of Bodwin and Hoppenworth [FOCS 2023] which was used to show an
$\widetilde{\Omega}(n^{1/4})$ lower bound for the $O(n)$-sized shortcut set
problem. We highlight that our simplification completely removes the use of the
convex sets by B\'ar\'any and Larman [Math. Ann. 1998] used in all previous
lower bound constructions. Our simplification also removes the need for
randomness and further removes some log factors. This allows us to generalize
the construction to higher dimensions, which in turn can be used to show the
following results. For $O(m)$-sized shortcut sets, we show an $\Omega(n^{1/5})$
lower bound, improving on the previous best $\Omega(n^{1/8})$ lower bound. For
all $\varepsilon &gt; 0$, we show that there exists a $\delta &gt; 0$ such that there
are $n$-vertex $O(n)$-edge graphs $G$ where adding any shortcut set of size
$O(n^{2-\varepsilon})$ keeps the diameter of $G$ at $\Omega(n^\delta)$. This
improves the sparsity of the constructed graph compared to a known similar
result by Hesse [SODA 2003].
<br />We also consider the sourcewise setting for shortcut sets: given a graph
$G=(V,E)$, a set $S\subseteq V$, how much can we decrease the sourcewise
diameter of $G$, $\max_{(s, v) \in S \times V, \text{dist}(s, v) &lt; \infty}
\text{dist}(s,v)$ by adding a set of edges $H$ from the transitive closure of
$G$? We show that for any integer $d \ge 2$, there exists a graph $G=(V, E)$ on
$n$ vertices and $S \subseteq V$ with $|S| = \widetilde{\Theta}(n^{3/(d+3)})$,
such that when adding $O(n)$ or $O(m)$ shortcuts, the sourcewise diameter is
$\widetilde{\Omega}(|S|^{1/3})$.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12052" title="Abstract">arXiv:2310.12052</a> [<a href="/pdf/2310.12052" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning-based Nutrient Application&#x27;s Timeline Recommendation  for Smart Agriculture: A Large-Scale Data Mining Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ikhlaq%2C+U">Usama Ikhlaq</a>, 
<a href="/search/cs?searchtype=author&query=Kechadi%2C+T">Tahar Kechadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Research articles have: 6 Pages, 6 Figures, and 3 Tables | ACKNOWLEDGMENT: CONSUS is funded under Science Foundation Ireland's Strategic Partnerships Programme (16/SPP/3296) and is co-funded by Origin Enterprises Plc
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study addresses the vital role of data analytics in monitoring
fertiliser applications in crop cultivation. Inaccurate fertiliser application
decisions can lead to costly consequences, hinder food production, and cause
environmental harm. We propose a solution to predict nutrient application by
determining required fertiliser quantities for an entire season. The proposed
solution recommends adjusting fertiliser amounts based on weather conditions
and soil characteristics to promote cost-effective and environmentally friendly
agriculture. The collected dataset is high-dimensional and heterogeneous. Our
research examines large-scale heterogeneous datasets in the context of the
decision-making process, encompassing data collection and analysis. We also
study the impact of fertiliser applications combined with weather data on crop
yield, using the winter wheat crop as a case study. By understanding local
contextual and geographic factors, we aspire to stabilise or even reduce the
demand for agricultural nutrients while enhancing crop development. The
proposed approach is proven to be efficient and scalable, as it is validated
using a real-world and large dataset.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12053" title="Abstract">arXiv:2310.12053</a> [<a href="/pdf/2310.12053" title="Download PDF">pdf</a>, <a href="/format/2310.12053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rational function approximation with normalized positive denominators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chok%2C+J">James Chok</a>, 
<a href="/search/math?searchtype=author&query=Vasil%2C+G+M">Geoffrey M. Vasil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Rational function approximations provide a simple but flexible alternative to
polynomial approximation, allowing one to capture complex non-linearities
without oscillatory artifacts. However, there have been few attempts to use
rational functions on noisy data due to the likelihood of creating spurious
singularities. To avoid the creation of singularities, we use Bernstein
polynomials and appropriate conditions on their coefficients to force the
denominator to be strictly positive. While this reduces the range of rational
polynomials that can be expressed, it keeps all the benefits of rational
functions while maintaining the robustness of polynomial approximation in noisy
data scenarios.
<br />Our numerical experiments on noisy data show that existing rational
approximation methods continually produce spurious poles inside the
approximation domain. This contrasts our method, which cannot create poles in
the approximation domain and provides better fits than a polynomial
approximation and even penalized splines on functions with multiple variables.
Moreover, guaranteeing pole-free in an interval is critical for estimating
non-constant coefficients when numerically solving differential equations using
spectral methods. This provides a compact representation of the original
differential equation, allowing numeric solvers to achieve high accuracy
quickly, as seen in our experiments.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12054" title="Abstract">arXiv:2310.12054</a> [<a href="/pdf/2310.12054" title="Download PDF">pdf</a>, <a href="/format/2310.12054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simultaneous Learning of Contact and Continuous Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bianchini%2C+B">Bibit Bianchini</a>, 
<a href="/search/cs?searchtype=author&query=Halm%2C+M">Mathew Halm</a>, 
<a href="/search/cs?searchtype=author&query=Posa%2C+M">Michael Posa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures. Accepted to Conference on Robot Learning (CoRL) 2023. Project webpage with code, datasets, media, and OpenReview link at <a href="https://sites.google.com/view/continuous-contact-nets/home">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robotic manipulation can greatly benefit from the data efficiency,
robustness, and predictability of model-based methods if robots can quickly
generate models of novel objects they encounter. This is especially difficult
when effects like complex joint friction lack clear first-principles models and
are usually ignored by physics simulators. Further, numerically-stiff contact
dynamics can make common model-building approaches struggle. We propose a
method to simultaneously learn contact and continuous dynamics of a novel,
possibly multi-link object by observing its motion through contact-rich
trajectories. We formulate a system identification process with a loss that
infers unmeasured contact forces, penalizing their violation of physical
constraints and laws of motion given current model parameters. Our loss is
unlike prediction-based losses used in differentiable simulation. Using a new
dataset of real articulated object trajectories and an existing cube toss
dataset, our method outperforms differentiable simulation and end-to-end
alternatives with more data efficiency. See our project page for code,
datasets, and media: https://sites.google.com/view/continuous-contact-nets/home
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12055" title="Abstract">arXiv:2310.12055</a> [<a href="/pdf/2310.12055" title="Download PDF">pdf</a>, <a href="/ps/2310.12055" title="Download PostScript">ps</a>, <a href="/format/2310.12055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Reward Ambiguity Through Optimal Transport Theory in  Inverse Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baheri%2C+A">Ali Baheri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">In inverse reinforcement learning (IRL), the central objective is to infer
underlying reward functions from observed expert behaviors in a way that not
only explains the given data but also generalizes to unseen scenarios. This
ensures robustness against reward ambiguity where multiple reward functions can
equally explain the same expert behaviors. While significant efforts have been
made in addressing this issue, current methods often face challenges with
high-dimensional problems and lack a geometric foundation. This paper harnesses
the optimal transport (OT) theory to provide a fresh perspective on these
challenges. By utilizing the Wasserstein distance from OT, we establish a
geometric framework that allows for quantifying reward ambiguity and
identifying a central representation or centroid of reward functions. These
insights pave the way for robust IRL methodologies anchored in geometric
interpretations, offering a structured approach to tackle reward ambiguity in
high-dimensional settings.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12058" title="Abstract">arXiv:2310.12058</a> [<a href="/pdf/2310.12058" title="Download PDF">pdf</a>, <a href="/ps/2310.12058" title="Download PostScript">ps</a>, <a href="/format/2310.12058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HIFuzz: Human Interaction Fuzzing for small Unmanned Aerial Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chambers%2C+T">Theodore Chambers</a>, 
<a href="/search/cs?searchtype=author&query=Vierhauser%2C+M">Michael Vierhauser</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Ankit Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+M">Michael Murphy</a>, 
<a href="/search/cs?searchtype=author&query=Brauer%2C+J+M">Jason Matthew Brauer</a>, 
<a href="/search/cs?searchtype=author&query=Purandare%2C+S">Salil Purandare</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+M+B">Myra B. Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Cleland-Huang%2C+J">Jane Cleland-Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Small Unmanned Aerial Systems (sUAS) must meet rigorous safety standards when
deployed in high-stress emergency response scenarios. However, tests that
execute perfectly in simulation can fail dramatically in real-world
environments. Fuzz testing can be used to increase system robustness by
providing malformed input data aimed at triggering failure cases. In this
paper, we apply fuzzing to support human interaction testing. Initial tests are
run in simulation to provide broad coverage of the input space in a safe
environment; however, they lack the fidelity of real-world tests. Field tests
provide higher fidelity but can result in costly or dangerous crashes. We,
therefore, propose and demonstrate HiFuzz, which executes large numbers of fuzz
tests in simulation and then down-selects tests for deployment in
human-in-the-loop simulations and safety-aware physical field tests. We apply
\hf to a multi-sUAS system and show that each test level serves a unique
purpose in identifying known and unknown failures associated with human
interactions.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12059" title="Abstract">arXiv:2310.12059</a> [<a href="/pdf/2310.12059" title="Download PDF">pdf</a>, <a href="/format/2310.12059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Symbol Binding Ability of Large Language Models for  Multiple-Choice Questions in Vietnamese General Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Duc-Vu Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q">Quoc-Nam Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we evaluate the ability of large language models (LLMs) to
perform multiple choice symbol binding (MCSB) for multiple choice question
answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus
on Vietnamese, with fewer challenging MCQA datasets than in English. The two
existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent
research in Vietnamese natural language processing (NLP) has focused on the
Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to
2023 to evaluate ChatGPT. However, these studies have mainly focused on how
ChatGPT solves the VNHSGE step by step. We aim to create a novel and
high-quality dataset by providing structured guidelines for typing LaTeX
formulas for mathematics, physics, chemistry, and biology. This dataset can be
used to evaluate the MCSB ability of LLMs and smaller language models (LMs)
because it is typed in a strict LaTeX style. We focus on predicting the
character (A, B, C, or D) that is the most likely answer to a question, given
the context of the question. Our evaluation of six well-known LLMs, namely
BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the
ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising
results on the MCSB ability of LLMs for Vietnamese. The dataset is available
for research purposes only.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12060" title="Abstract">arXiv:2310.12060</a> [<a href="/pdf/2310.12060" title="Download PDF">pdf</a>, <a href="/format/2310.12060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Class-Conditional Distribution Alignment for Partial Domain  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choudhuri%2C+S">Sandipan Choudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+A">Arunabha Sen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unwanted samples from private source categories in the learning objective of
a partial domain adaptation setup can lead to negative transfer and reduce
classification performance. Existing methods, such as re-weighting or
aggregating target predictions, are vulnerable to this issue, especially during
initial training stages, and do not adequately address overlapping categorical
distributions. We propose a solution to overcome these limitations by exploring
beyond the first-order moments for robust alignment of categorical
distributions. We employ objectives that optimize the intra and inter-class
distributions in a domain-invariant fashion and design a robust pseudo-labeling
for efficient target supervision. Our approach incorporates a complement
entropy objective module to reduce classification uncertainty and flatten
incorrect category predictions. The experimental findings and ablation analysis
of the proposed modules demonstrate the superior performance of our proposed
model compared to benchmarks.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12062" title="Abstract">arXiv:2310.12062</a> [<a href="/pdf/2310.12062" title="Download PDF">pdf</a>, <a href="/format/2310.12062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the use of Vision-Language models for Visual Sentiment Analysis: a  study on CLIP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bustos%2C+C">Cristina Bustos</a>, 
<a href="/search/cs?searchtype=author&query=Civit%2C+C">Carles Civit</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Brian Du</a>, 
<a href="/search/cs?searchtype=author&query=Sole-Ribalta%2C+A">Albert Sole-Ribalta</a>, 
<a href="/search/cs?searchtype=author&query=Lapedriza%2C+A">Agata Lapedriza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This work presents a study on how to exploit the CLIP embedding space to
perform Visual Sentiment Analysis. We experiment with two architectures built
on top of the CLIP embedding space, which we denote by CLIP-E. We train the
CLIP-E models with WEBEmo, the largest publicly available and manually labeled
benchmark for Visual Sentiment Analysis, and perform two sets of experiments.
First, we test on WEBEmo and compare the CLIP-E architectures with
state-of-the-art (SOTA) models and with CLIP Zero-Shot. Second, we perform
cross dataset evaluation, and test the CLIP-E architectures trained with WEBEmo
on other Visual Sentiment Analysis benchmarks. Our results show that the CLIP-E
approaches outperform SOTA models in WEBEmo fine grained categorization, and
they also generalize better when tested on datasets that have not been seen
during training. Interestingly, we observed that for the FI dataset, CLIP
Zero-Shot produces better accuracies than SOTA models and CLIP-E trained on
WEBEmo. These results motivate several questions that we discuss in this paper,
such as how we should design new benchmarks and evaluate Visual Sentiment
Analysis, and whether we should keep designing tailored Deep Learning models
for Visual Sentiment Analysis or focus our efforts on better using the
knowledge encoded in large vision-language models such as CLIP for this task.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12063" title="Abstract">arXiv:2310.12063</a> [<a href="/pdf/2310.12063" title="Download PDF">pdf</a>, <a href="/format/2310.12063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Black-Box Training Data Identification in GANs via Detector Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olagoke%2C+L">Lukman Olagoke</a>, 
<a href="/search/cs?searchtype=author&query=Vadhan%2C+S">Salil Vadhan</a>, 
<a href="/search/cs?searchtype=author&query=Neel%2C+S">Seth Neel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Since their inception Generative Adversarial Networks (GANs) have been
popular generative models across images, audio, video, and tabular data. In
this paper we study whether given access to a trained GAN, as well as fresh
samples from the underlying distribution, if it is possible for an attacker to
efficiently identify if a given point is a member of the GAN's training data.
This is of interest for both reasons related to copyright, where a user may
want to determine if their copyrighted data has been used to train a GAN, and
in the study of data privacy, where the ability to detect training set
membership is known as a membership inference attack. Unlike the majority of
prior work this paper investigates the privacy implications of using GANs in
black-box settings, where the attack only has access to samples from the
generator, rather than access to the discriminator as well. We introduce a
suite of membership inference attacks against GANs in the black-box setting and
evaluate our attacks on image GANs trained on the CIFAR10 dataset and tabular
GANs trained on genomic data. Our most successful attack, called The Detector,
involve training a second network to score samples based on their likelihood of
being generated by the GAN, as opposed to a fresh sample from the distribution.
We prove under a simple model of the generator that the detector is an
approximately optimal membership inference attack. Across a wide range of
tabular and image datasets, attacks, and GAN architectures, we find that
adversaries can orchestrate non-trivial privacy attacks when provided with
access to samples from the generator. At the same time, the attack success
achievable against GANs still appears to be lower compared to other generative
and discriminative models; this leaves the intriguing open question of whether
GANs are in fact more private, or if it is a matter of developing stronger
attacks.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12064" title="Abstract">arXiv:2310.12064</a> [<a href="/pdf/2310.12064" title="Download PDF">pdf</a>, <a href="/format/2310.12064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code Book for the Annotation of Diverse Cross-Document Coreference of  Entities in News Articles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vogel%2C+J">Jakob Vogel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper presents a scheme for annotating coreference across news articles,
extending beyond traditional identity relations by also considering
near-identity and bridging relations. It includes a precise description of how
to set up Inception, a respective annotation tool, how to annotate entities in
news articles, connect them with diverse coreferential relations, and link them
across documents to Wikidata's global knowledge graph. This multi-layered
annotation approach is discussed in the context of the problem of media bias.
Our main contribution lies in providing a methodology for creating a diverse
cross-document coreference corpus which can be applied to the analysis of media
bias by word-choice and labelling.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12065" title="Abstract">arXiv:2310.12065</a> [<a href="/pdf/2310.12065" title="Download PDF">pdf</a>, <a href="/ps/2310.12065" title="Download PostScript">ps</a>, <a href="/format/2310.12065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Persuasive Approach to Combating Misinformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+S">Safwan Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Mladenovic%2C+A">Andjela Mladenovic</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We propose using Bayesian Persuasion as a tool for social media platforms to
combat the spread of online misinformation. As platforms can predict the
popularity and misinformation features of to-be-shared posts, and users are
motivated to only share popular content, platforms can strategically reveal
this informational advantage to persuade users to not share misinformed
content. Our work mathematically characterizes the optimal information design
scheme and the resulting utility when observations are not perfectly observed
but arise from an imperfect classifier. Framing the optimization problem as a
linear program, we give sufficient and necessary conditions on the classifier
accuracy to ensure platform utility under optimal signaling is monotonically
increasing and continuous. We next consider this interaction under a
performative model, wherein platform intervention through signaling affects the
content distribution in the future. We fully characterize the convergence and
stability of optimal signaling under this performative process. Lastly, the
broader scope of using information design to combat misinformation is discussed
throughout.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12072" title="Abstract">arXiv:2310.12072</a> [<a href="/pdf/2310.12072" title="Download PDF">pdf</a>, <a href="/format/2310.12072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPEED: Speculative Pipelined Execution for Efficient Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hooper%2C+C">Coleman Hooper</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sehoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadzadeh%2C+H">Hiva Mohammadzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Genc%2C+H">Hasan Genc</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Gholami%2C+A">Amir Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+S">Sophia Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Generative Large Language Models (LLMs) based on the Transformer architecture
have recently emerged as a dominant foundation model for a wide range of
Natural Language Processing tasks. Nevertheless, their application in real-time
scenarios has been highly restricted due to the significant inference latency
associated with these models. This is particularly pronounced due to the
autoregressive nature of generative LLM inference, where tokens are generated
sequentially since each token depends on all previous output tokens. It is
therefore challenging to achieve any token-level parallelism, making inference
extremely memory-bound. In this work, we propose SPEED, which improves
inference efficiency by speculatively executing multiple future tokens in
parallel with the current token using predicted values based on early-layer
hidden states. For Transformer decoders that employ parameter sharing, the
memory operations for the tokens executing in parallel can be amortized, which
allows us to accelerate generative LLM inference. We demonstrate the efficiency
of our method in terms of latency reduction relative to model accuracy and
demonstrate how speculation allows for training deeper decoders with parameter
sharing with minimal runtime overhead.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12074" title="Abstract">arXiv:2310.12074</a> [<a href="/pdf/2310.12074" title="Download PDF">pdf</a>, <a href="/format/2310.12074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Safer Operations: An Expert-involved Dataset of High-Pressure  Gas Incidents for Preventing Future Failures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Inoue%2C+S">Shumpei Inoue</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M">Minh-Tien Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Mizokuchi%2C+H">Hiroki Mizokuchi</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+D">Tuan-Anh D. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Huu-Hiep Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+D+T">Dung Tien Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 (The industry track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper introduces a new IncidentAI dataset for safety prevention.
Different from prior corpora that usually contain a single task, our dataset
comprises three tasks: named entity recognition, cause-effect extraction, and
information retrieval. The dataset is annotated by domain experts who have at
least six years of practical experience as high-pressure gas conservation
managers. We validate the contribution of the dataset in the scenario of safety
prevention. Preliminary results on the three tasks show that NLP techniques are
beneficial for analyzing incident reports to prevent future failures. The
dataset facilitates future research in NLP and incident management communities.
The access to the dataset is also provided (the IncidentAI dataset is available
at: https://github.com/Cinnamon/incident-ai-dataset).
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12075" title="Abstract">arXiv:2310.12075</a> [<a href="/pdf/2310.12075" title="Download PDF">pdf</a>, <a href="/format/2310.12075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monte-Carlo Tree Search for Behavior Planning in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qianfeng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zhongyi Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lifeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongshun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The integration of autonomous vehicles into urban and highway environments
necessitates the development of robust and adaptable behavior planning systems.
This study presents an innovative approach to address this challenge by
utilizing a Monte-Carlo Tree Search (MCTS) based algorithm for autonomous
driving behavior planning. The core objective is to leverage the balance
between exploration and exploitation inherent in MCTS to facilitate intelligent
driving decisions in complex scenarios.
<br />We introduce an MCTS-based algorithm tailored to the specific demands of
autonomous driving. This involves the integration of carefully crafted cost
functions, encompassing safety, comfort, and passability metrics, into the MCTS
framework. The effectiveness of our approach is demonstrated by enabling
autonomous vehicles to navigate intricate scenarios, such as intersections,
unprotected left turns, cut-ins, and ramps, even under traffic congestion, in
real-time.
<br />Qualitative instances illustrate the integration of diverse driving
decisions, such as lane changes, acceleration, and deceleration, into the MCTS
framework. Moreover, quantitative results, derived from examining the impact of
iteration time and look-ahead steps on decision quality and real-time
applicability, substantiate the robustness of our approach. This robustness is
further underscored by the high success rate of the MCTS algorithm across
various scenarios.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12076" title="Abstract">arXiv:2310.12076</a> [<a href="/pdf/2310.12076" title="Download PDF">pdf</a>, <a href="/format/2310.12076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Fairness in Pre-trained Visual Transformer based Natural and  GAN Generated Image Detection Systems and Understanding the Impact of Image  Compression in Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gangan%2C+M+P">Manjary P. Gangan</a>, 
<a href="/search/cs?searchtype=author&query=Kadan%2C+A">Anoop Kadan</a>, 
<a href="/search/cs?searchtype=author&query=L%2C+L+V">Lajish V L</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">It is not only sufficient to construct computational models that can
accurately classify or detect fake images from real images taken from a camera,
but it is also important to ensure whether these computational models are fair
enough or produce biased outcomes that can eventually harm certain social
groups or cause serious security threats. Exploring fairness in forensic
algorithms is an initial step towards correcting these biases. Since visual
transformers are recently being widely used in most image classification based
tasks due to their capability to produce high accuracies, this study tries to
explore bias in the transformer based image forensic algorithms that classify
natural and GAN generated images. By procuring a bias evaluation corpora, this
study analyzes bias in gender, racial, affective, and intersectional domains
using a wide set of individual and pairwise bias evaluation measures. As the
generalizability of the algorithms against image compression is an important
factor to be considered in forensic tasks, this study also analyzes the role of
image compression on model bias. Hence to study the impact of image compression
on model bias, a two phase evaluation setting is followed, where a set of
experiments is carried out in the uncompressed evaluation setting and the other
in the compressed evaluation setting.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12077" title="Abstract">arXiv:2310.12077</a> [<a href="/pdf/2310.12077" title="Download PDF">pdf</a>, <a href="/format/2310.12077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Shot Imitation Learning: A Pose Estimation Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vitiello%2C+P">Pietro Vitiello</a>, 
<a href="/search/cs?searchtype=author&query=Dreczkowski%2C+K">Kamil Dreczkowski</a>, 
<a href="/search/cs?searchtype=author&query=Johns%2C+E">Edward Johns</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at the 7th Conference on Robot Learning (CoRL 2023). For more details please visit <a href="https://www.robot-learning.uk/pose-estimation-perspective">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we study imitation learning under the challenging setting of:
(1) only a single demonstration, (2) no further data collection, and (3) no
prior task or object knowledge. We show how, with these constraints, imitation
learning can be formulated as a combination of trajectory transfer and unseen
object pose estimation. To explore this idea, we provide an in-depth study on
how state-of-the-art unseen object pose estimators perform for one-shot
imitation learning on ten real-world tasks, and we take a deep dive into the
effects that camera calibration, pose estimation error, and spatial
generalisation have on task success rates. For videos, please visit
https://www.robot-learning.uk/pose-estimation-perspective.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12081" title="Abstract">arXiv:2310.12081</a> [<a href="/pdf/2310.12081" title="Download PDF">pdf</a>, <a href="/format/2310.12081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical  Optimal Transport Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Haoran Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Dixin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongteng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Graph matching is one of the most significant graph analytic tasks in
practice, which aims to find the node correspondence across different graphs.
Most existing approaches rely on adjacency matrices or node embeddings when
matching graphs, whose performances are often sub-optimal because of not fully
leveraging the multi-modal information hidden in graphs, such as node
attributes, subgraph structures, etc. In this study, we propose a novel and
effective graph matching method based on a differentiable hierarchical optimal
transport (HOT) framework, called DHOT-GM. Essentially, our method represents
each graph as a set of relational matrices corresponding to the information of
different modalities. Given two graphs, we enumerate all relational matrix
pairs and obtain their matching results, and accordingly, infer the node
correspondence by the weighted averaging of the matching results. This method
can be implemented as computing the HOT distance between the two graphs -- each
matching result is an optimal transport plan associated with the
Gromov-Wasserstein (GW) distance between two relational matrices, and the
weights of all matching results are the elements of an upper-level optimal
transport plan defined on the matrix sets. We propose a bi-level optimization
algorithm to compute the HOT distance in a differentiable way, making the
significance of the relational matrices adjustable. Experiments on various
graph matching tasks demonstrate the superiority and robustness of our method
compared to state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12083" title="Abstract">arXiv:2310.12083</a> [<a href="/pdf/2310.12083" title="Download PDF">pdf</a>, <a href="/format/2310.12083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contributing Components of Metabolic Energy Models to Metabolic Cost  Estimations in Gait
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gambietz%2C+M">Markus Gambietz</a>, 
<a href="/search/cs?searchtype=author&query=Nitschke%2C+M">Marlies Nitschke</a>, 
<a href="/search/cs?searchtype=author&query=Miehling%2C+J">J&#xf6;rg Miehling</a>, 
<a href="/search/cs?searchtype=author&query=Koelewijn%2C+A">Anne Koelewijn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Objective: As metabolic cost is a primary factor influencing humans' gait, we
want to deepen our understanding of metabolic energy expenditure models.
Therefore, this paper identifies the parameters and input variables, such as
muscle or joint states, that contribute to accurate metabolic cost estimations.
Methods: We explored the parameters of four metabolic energy expenditure models
in a Monte Carlo sensitivity analysis. Then, we analysed the model parameters
by their calculated sensitivity indices, physiological context, and the
resulting metabolic rates during the gait cycle. The parameter combination with
the highest accuracy in the Monte Carlo simulations represented a
quasi-optimized model. In the second step, we investigated the importance of
input parameters and variables by analysing the accuracy of neural networks
trained with different input features. Results: Power-related parameters were
most influential in the sensitivity analysis and the neural network-based
feature selection. We observed that the quasi-optimized models produced
negative metabolic rates, contradicting muscle physiology. Neural network-based
models showed promising abilities but have been unable to match the accuracy of
traditional metabolic energy expenditure models. Conclusion: We showed that
power-related metabolic energy expenditure model parameters and inputs are most
influential during gait. Furthermore, our results suggest that neural
network-based metabolic energy expenditure models are viable. However, bigger
datasets are required to achieve better accuracy. Significance: As there is a
need for more accurate metabolic energy expenditure models, we explored which
musculoskeletal parameters are essential when developing a model to estimate
metabolic energy.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12085" title="Abstract">arXiv:2310.12085</a> [<a href="/pdf/2310.12085" title="Download PDF">pdf</a>, <a href="/format/2310.12085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Benefit of Generative Foundation Models for Human Activity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leng%2C+Z">Zikang Leng</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+H">Hyeokhyen Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Pl%C3%B6tz%2C+T">Thomas Pl&#xf6;tz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Generative AI for Pervasive Computing (GenAI4PC) Symposium within UbiComp/ISWC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In human activity recognition (HAR), the limited availability of annotated
data presents a significant challenge. Drawing inspiration from the latest
advancements in generative AI, including Large Language Models (LLMs) and
motion synthesis models, we believe that generative AI can address this data
scarcity by autonomously generating virtual IMU data from text descriptions.
Beyond this, we spotlight several promising research pathways that could
benefit from generative AI for the community, including the generating
benchmark datasets, the development of foundational models specific to HAR, the
exploration of hierarchical structures within HAR, breaking down complex
activities, and applications in health sensing and activity summarization.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12086" title="Abstract">arXiv:2310.12086</a> [<a href="/pdf/2310.12086" title="Download PDF">pdf</a>, <a href="/format/2310.12086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Siren&#x27;s Song: Towards Reliable Fact-Conflicting  Hallucination Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Duanzheng Song</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+H">Honghao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengxi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Chengfei Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread
attention owing to their myriad of practical applications, yet their adoption
has been constrained by issues of fact-conflicting hallucinations across web
platforms. The assessment of factuality in text, produced by LLMs, remains
inadequately explored, extending not only to the judgment of vanilla facts but
also encompassing the evaluation of factual errors emerging in complex
inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a
fact-conflicting hallucination detection benchmark meticulously designed for
LLMs. Functioning as a pivotal tool in evaluating factuality within
"Query-Respons" contexts, our benchmark assimilates a large-scale dataset,
encapsulating a broad spectrum of factuality patterns, such as vanilla,
multi-hops, comparison, and set-operation patterns. A distinctive feature of
our benchmark is its incorporation of fact-based chains of evidence, thereby
facilitating comprehensive and conducive factual reasoning throughout the
assessment process. We evaluate multiple LLMs, demonstrating the effectiveness
of the benchmark and current methods fall short of faithfully detecting factual
errors. Furthermore, we present TRUTH-TRIANGULATOR that synthesizes reflective
considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming
to yield more credible detection through the amalgamation of predictive results
and evidence. The benchmark dataset and source code will be made available in
https://github.com/zjunlp/FactCHD.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12092" title="Abstract">arXiv:2310.12092</a> [<a href="/pdf/2310.12092" title="Download PDF">pdf</a>, <a href="/format/2310.12092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HSTR-Net: Reference Based Video Super-resolution for Aerial Surveillance  with Dual Cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suluhan%2C+H+U">H. Umut Suluhan</a>, 
<a href="/search/cs?searchtype=author&query=Ates%2C+H+F">Hasan F. Ates</a>, 
<a href="/search/cs?searchtype=author&query=Gunturk%2C+B+K">Bahadir K. Gunturk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Aerial surveillance requires high spatio-temporal resolution (HSTR) video for
more accurate detection and tracking of objects. This is especially true for
wide-area surveillance (WAS), where the surveyed region is large and the
objects of interest are small. This paper proposes a dual camera system for the
generation of HSTR video using reference-based super-resolution (RefSR). One
camera captures high spatial resolution low frame rate (HSLF) video while the
other captures low spatial resolution high frame rate (LSHF) video
simultaneously for the same scene. A novel deep learning architecture is
proposed to fuse HSLF and LSHF video feeds and synthesize HSTR video frames at
the output. The proposed model combines optical flow estimation and
(channel-wise and spatial) attention mechanisms to capture the fine motion and
intricate dependencies between frames of the two video feeds. Simulations show
that the proposed model provides significant improvement over existing
reference-based SR techniques in terms of PSNR and SSIM metrics. The method
also exhibits sufficient frames per second (FPS) for WAS when deployed on a
power-constrained drone equipped with dual cameras.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12095" title="Abstract">arXiv:2310.12095</a> [<a href="/pdf/2310.12095" title="Download PDF">pdf</a>, <a href="/format/2310.12095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the latent dimension of deep autoencoders for reduced order modeling  of PDEs parametrized by random fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Franco%2C+N+R">Nicola Rares Franco</a>, 
<a href="/search/cs?searchtype=author&query=Fraulin%2C+D">Daniel Fraulin</a>, 
<a href="/search/cs?searchtype=author&query=Manzoni%2C+A">Andrea Manzoni</a>, 
<a href="/search/cs?searchtype=author&query=Zunino%2C+P">Paolo Zunino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Deep Learning is having a remarkable impact on the design of Reduced Order
Models (ROMs) for Partial Differential Equations (PDEs), where it is exploited
as a powerful tool for tackling complex problems for which classical methods
might fail. In this respect, deep autoencoders play a fundamental role, as they
provide an extremely flexible tool for reducing the dimensionality of a given
problem by leveraging on the nonlinear capabilities of neural networks. Indeed,
starting from this paradigm, several successful approaches have already been
developed, which are here referred to as Deep Learning-based ROMs (DL-ROMs).
Nevertheless, when it comes to stochastic problems parameterized by random
fields, the current understanding of DL-ROMs is mostly based on empirical
evidence: in fact, their theoretical analysis is currently limited to the case
of PDEs depending on a finite number of (deterministic) parameters. The purpose
of this work is to extend the existing literature by providing some theoretical
insights about the use of DL-ROMs in the presence of stochasticity generated by
random fields. In particular, we derive explicit error bounds that can guide
domain practitioners when choosing the latent dimension of deep autoencoders.
We evaluate the practical usefulness of our theory by means of numerical
experiments, showing how our analysis can significantly impact the performance
of DL-ROMs.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12096" title="Abstract">arXiv:2310.12096</a> [<a href="/pdf/2310.12096" title="Download PDF">pdf</a>, <a href="/format/2310.12096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vital Edges for (s,t)-mincut: Efficient Algorithms, Compact Structures,  and Optimal Sensitivity Oracle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baswana%2C+S">Surender Baswana</a>, 
<a href="/search/cs?searchtype=author&query=Bhanja%2C+K">Koustav Bhanja</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 59 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Let G be a directed weighted graph (DiGraph) on n vertices and m edges with
source s and sink t. An edge in G is vital if its removal reduces the capacity
of (s,t)-mincut. Since the seminal work of Ford and Fulkerson, a long line of
work has been done on computing the most vital edge and all vital edges of G.
Unfortunately, after 60 years, the existing results are for undirected or
unweighted graphs. We present the following result for DiGraph, which solves an
open problem stated by Ausiello et al.
<br />1. There is an algorithm that computes all vital edges as well as the most
vital edge of G using O(n) maxflow computations.
<br />Vital edges play a crucial role in the design of Sensitivity Oracle (SO) for
(s,t)-mincut. For directed graphs, the only existing SO is for unweighted
graphs by Picard and Queyranne. We present the first and optimal SO for
DiGraph.
<br />2. (a) There is an O(n) space SO that can report in O(1) time the capacity of
(s,t)-mincut and (b) an O($n^2$) space SO that can report an (s,t)-mincut in
O(n) time after failure/insertion of an edge.
<br />For unweighted graphs, Picard and Queyranne designed an O(m) space DAG that
stores and characterizes all mincuts for all vital edges. Conversely, there is
a set containing at most n-1 (s,t)-cuts such that at least one mincut for every
vital edge belongs to the set. We generalize these results for DiGraph.
<br />3. (a) There is a set containing at most n-1 (s,t)-cuts such that at least
one mincut for every vital edge is present in the set. (b) We design two
compact structures for storing and characterizing all mincuts for all vital
edges, (i) O(m) space DAG for partial characterization and (ii) O(mn) space
structure for complete characterization.
<br />To arrive at our results, we develop new techniques, especially a
generalization of maxflow-mincut theorem by Ford and Fulkerson, which might be
of independent interest.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12100" title="Abstract">arXiv:2310.12100</a> [<a href="/pdf/2310.12100" title="Download PDF">pdf</a>, <a href="/format/2310.12100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning  for Versatile Multimodal Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jialin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dabral%2C+T">Tanmaya Dabral</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiageng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+G">Geoff Brown</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chun-Ta Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Frederick Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+B">Bo Pang</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>, 
<a href="/search/cs?searchtype=author&query=Soricut%2C+R">Radu Soricut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Large language models (LLMs) and vision language models (VLMs) demonstrate
excellent performance on a wide range of tasks by scaling up parameter counts
from O(10^9) to O(10^{12}) levels and further beyond. These large scales make
it impossible to adapt and deploy fully specialized models given a task of
interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising
direction to tackle the adaptation and serving challenges for such large
models. We categorize PEFT techniques into two types: intrusive and
non-intrusive. Intrusive PEFT techniques directly change a model's internal
architecture. Though more flexible, they introduce significant complexities for
training and serving. Non-intrusive PEFT techniques leave the internal
architecture unchanged and only adapt model-external parameters, such as
embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT
technique that achieves competitive performance compared to SoTA intrusive PEFT
(LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both
text-only and multimodal tasks, with experiments that account for both
parameter-count scaling and training regime (with and without instruction
tuning).
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12103" title="Abstract">arXiv:2310.12103</a> [<a href="/pdf/2310.12103" title="Download PDF">pdf</a>, <a href="/format/2310.12103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality Diversity through Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Li Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jenny Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Clune%2C+J">Jeff Clune</a>, 
<a href="/search/cs?searchtype=author&query=Spector%2C+L">Lee Spector</a>, 
<a href="/search/cs?searchtype=author&query=Lehman%2C+J">Joel Lehman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Reinforcement learning from human feedback (RLHF) has exhibited the potential
to enhance the performance of foundation models for qualitative tasks. Despite
its promise, its efficacy is often restricted when conceptualized merely as a
mechanism to maximize learned reward models of averaged human preferences,
especially in areas such as image generation which demand diverse model
responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking
diverse, high-quality solutions, are often constrained by the dependency on
manually defined diversity metrics. Interestingly, such limitations of RLHF and
QD can be overcome by blending insights from both. This paper introduces
Quality Diversity through Human Feedback (QDHF), which employs human feedback
for inferring diversity metrics, expanding the applicability of QD algorithms.
Empirical results reveal that QDHF outperforms existing QD methods regarding
automatic diversity discovery, and matches the search capabilities of QD with
human-constructed metrics. Notably, when deployed for a latent space
illumination task, QDHF markedly enhances the diversity of images generated by
a Diffusion model. The study concludes with an in-depth analysis of QDHF's
sample efficiency and the quality of its derived diversity metrics, emphasizing
its promise for enhancing exploration and diversity in optimization for
complex, open-ended tasks.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12107" title="Abstract">arXiv:2310.12107</a> [<a href="/pdf/2310.12107" title="Download PDF">pdf</a>, <a href="/ps/2310.12107" title="Download PostScript">ps</a>, <a href="/format/2310.12107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Online Learning Theory of Brokerage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boli%C4%87%2C+N">Nata&#x161;a Boli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Cesari%2C+T">Tommaso Cesari</a>, 
<a href="/search/cs?searchtype=author&query=Colomboni%2C+R">Roberto Colomboni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We investigate brokerage between traders from an online learning perspective.
At any round $t$, two traders arrive with their private valuations, and the
broker proposes a trading price. Unlike other bilateral trade problems already
studied in the online learning literature, we focus on the case where there are
no designated buyer and seller roles: each trader will attempt to either buy or
sell depending on the current price of the good.
<br />We assume the agents' valuations are drawn i.i.d. from a fixed but unknown
distribution. If the distribution admits a density bounded by some constant
$M$, then, for any time horizon $T$:
<br />$\bullet$ If the agents' valuations are revealed after each interaction, we
provide an algorithm achieving regret $M \log T$ and show this rate is optimal,
up to constant factors.
<br />$\bullet$ If only their willingness to sell or buy at the proposed price is
revealed after each interaction, we provide an algorithm achieving regret
$\sqrt{M T}$ and show this rate is optimal, up to constant factors.
<br />Finally, if we drop the bounded density assumption, we show that the optimal
rate degrades to $\sqrt{T}$ in the first case, and the problem becomes
unlearnable in the second.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12109" title="Abstract">arXiv:2310.12109</a> [<a href="/pdf/2310.12109" title="Download PDF">pdf</a>, <a href="/format/2310.12109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Simran Arora</a>, 
<a href="/search/cs?searchtype=author&query=Grogan%2C+J">Jessica Grogan</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+I">Isys Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Eyuboglu%2C+S">Sabri Eyuboglu</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+A+W">Armin W. Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Spector%2C+B">Benjamin Spector</a>, 
<a href="/search/cs?searchtype=author&query=Poli%2C+M">Michael Poli</a>, 
<a href="/search/cs?searchtype=author&query=Rudra%2C+A">Atri Rudra</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9%2C+C">Christopher R&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine learning models are increasingly being scaled in both sequence length
and model dimension to reach longer contexts and better performance. However,
existing architectures such as Transformers scale quadratically along both
these axes. We ask: are there performant architectures that can scale
sub-quadratically along sequence length and model dimension? We introduce
Monarch Mixer (M2), a new architecture that uses the same sub-quadratic
primitive along both sequence length and model dimension: Monarch matrices, a
simple class of expressive structured matrices that captures many linear
transforms, achieves high hardware efficiency on GPUs, and scales
sub-quadratically. As a proof of concept, we explore the performance of M2 in
three domains: non-causal BERT-style language modeling, ViT-style image
classification, and causal GPT-style language modeling. For non-causal
BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE
quality with up to 27% fewer parameters, and achieves up to 9.1$\times$ higher
throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in
accuracy, with only half the parameters. Causal GPT-style models introduce a
technical challenge: enforcing causality via masking introduces a quadratic
bottleneck. To alleviate this bottleneck, we develop a novel theoretical view
of Monarch matrices based on multivariate polynomial evaluation and
interpolation, which lets us parameterize M2 to be causal while remaining
sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers
at 360M parameters in pretraining perplexity on The PILE--showing for the first
time that it may be possible to match Transformer quality without attention or
MLPs.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12112" title="Abstract">arXiv:2310.12112</a> [<a href="/pdf/2310.12112" title="Download PDF">pdf</a>, <a href="/format/2310.12112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Cautionary Tale: On the Role of Reference Data in Empirical Privacy  Defenses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaplan%2C+C+G">Caelin G. Kaplan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Marfoq%2C+O">Othmane Marfoq</a>, 
<a href="/search/cs?searchtype=author&query=Neglia%2C+G">Giovanni Neglia</a>, 
<a href="/search/cs?searchtype=author&query=de+Oliveira%2C+A+S">Anderson Santana de Oliveira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Within the realm of privacy-preserving machine learning, empirical privacy
defenses have been proposed as a solution to achieve satisfactory levels of
training data privacy without a significant drop in model utility. Most
existing defenses against membership inference attacks assume access to
reference data, defined as an additional dataset coming from the same (or a
similar) underlying distribution as training data. Despite the common use of
reference data, previous works are notably reticent about defining and
evaluating reference data privacy. As gains in model utility and/or training
data privacy may come at the expense of reference data privacy, it is essential
that all three aspects are duly considered. In this paper, we first examine the
availability of reference data and its privacy treatment in previous works and
demonstrate its necessity for fairly comparing defenses. Second, we propose a
baseline defense that enables the utility-privacy tradeoff with respect to both
training and reference data to be easily understood. Our method is formulated
as an empirical risk minimization with a constraint on the generalization
error, which, in practice, can be evaluated as a weighted empirical risk
minimization (WERM) over the training and reference datasets. Although we
conceived of WERM as a simple baseline, our experiments show that,
surprisingly, it outperforms the most well-studied and current state-of-the-art
empirical privacy defenses using reference data for nearly all relative privacy
levels of reference and training data. Our investigation also reveals that
these existing methods are unable to effectively trade off reference data
privacy for model utility and/or training data privacy. Overall, our work
highlights the need for a proper evaluation of the triad model utility /
training data privacy / reference data privacy when comparing privacy defenses.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12113" title="Abstract">arXiv:2310.12113</a> [<a href="/pdf/2310.12113" title="Download PDF">pdf</a>, <a href="/format/2310.12113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAPGrasp: An $\mathbb{R}^3\times \text{SO(2)-equivariant}$ Continuous  Approach-Constrained Generative Grasp Sampler
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+Z">Zehang Weng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haofei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lundell%2C+J">Jens Lundell</a>, 
<a href="/search/cs?searchtype=author&query=Kragic%2C+D">Danica Kragic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We propose CAPGrasp, an $\mathbb{R}^3\times \text{SO(2)-equivariant}$ 6-DoF
continuous approach-constrained generative grasp sampler. It includes a novel
learning strategy for training CAPGrasp that eliminates the need to curate
massive conditionally labeled datasets and a constrained grasp refinement
technique that improves grasp poses while respecting the grasp approach
directional constraints. The experimental results demonstrate that CAPGrasp is
more than three times as sample efficient as unconstrained grasp samplers while
achieving up to 38% grasp success rate improvement. CAPGrasp also achieves
4-10% higher grasp success rates than constrained but noncontinuous grasp
samplers. Overall, CAPGrasp is a sample-efficient solution when grasps must
originate from specific directions, such as grasping in confined spaces.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12116" title="Abstract">arXiv:2310.12116</a> [<a href="/pdf/2310.12116" title="Download PDF">pdf</a>, <a href="/ps/2310.12116" title="Download PostScript">ps</a>, <a href="/format/2310.12116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Indexing Schemes for k-Dominant Skyline Analytics on  Uncertain Edge-IoT Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+C">Chuan-Chi Lai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hsuan-Yu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chuan-Ming Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures, 12 tables, to appear in IEEE Transactions on Emerging Topics in Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Skyline queries typically search a Pareto-optimal set from a given data set
to solve the corresponding multiobjective optimization problem. As the number
of criteria increases, the skyline presumes excessive data items, which yield a
meaningless result. To address this curse of dimensionality, we proposed a
k-dominant skyline in which the number of skyline members was reduced by
relaxing the restriction on the number of dimensions, considering the
uncertainty of data. Specifically, each data item was associated with a
probability of appearance, which represented the probability of becoming a
member of the k-dominant skyline. As data items appear continuously in data
streams, the corresponding k-dominant skyline may vary with time. Therefore, an
effective and rapid mechanism of updating the k-dominant skyline becomes
crucial. Herein, we proposed two time-efficient schemes, Middle Indexing (MI)
and All Indexing (AI), for k-dominant skyline in distributed edge-computing
environments, where irrelevant data items can be effectively excluded from the
compute to reduce the processing duration. Furthermore, the proposed schemes
were validated with extensive experimental simulations. The experimental
results demonstrated that the proposed MI and AI schemes reduced the
computation time by approximately 13% and 56%, respectively, compared with the
existing method.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12118" title="Abstract">arXiv:2310.12118</a> [<a href="/pdf/2310.12118" title="Download PDF">pdf</a>, <a href="/format/2310.12118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Dataset Cartography for Improved Compositional Generalization  in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C4%B0nce%2C+O+B">Osman Batur &#x130;nce</a>, 
<a href="/search/cs?searchtype=author&query=Zeraati%2C+T">Tanin Zeraati</a>, 
<a href="/search/cs?searchtype=author&query=Yagcioglu%2C+S">Semih Yagcioglu</a>, 
<a href="/search/cs?searchtype=author&query=Yaghoobzadeh%2C+Y">Yadollah Yaghoobzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Erdem%2C+E">Erkut Erdem</a>, 
<a href="/search/cs?searchtype=author&query=Erdem%2C+A">Aykut Erdem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Neural networks have revolutionized language modeling and excelled in various
downstream tasks. However, the extent to which these models achieve
compositional generalization comparable to human cognitive abilities remains a
topic of debate. While existing approaches in the field have mainly focused on
novel architectures and alternative learning paradigms, we introduce a
pioneering method harnessing the power of dataset cartography (Swayamdipta et
al., 2020). By strategically identifying a subset of compositional
generalization data using this approach, we achieve a remarkable improvement in
model accuracy, yielding enhancements of up to 10% on CFQ and COGS datasets.
Notably, our technique incorporates dataset cartography as a curriculum
learning criterion, eliminating the need for hyperparameter tuning while
consistently achieving superior performance. Our findings highlight the
untapped potential of dataset cartography in unleashing the full capabilities
of compositional generalization within Transformer models. Our code is
available at https://github.com/cyberiada/cartography-for-compositionality.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12121" title="Abstract">arXiv:2310.12121</a> [<a href="/pdf/2310.12121" title="Download PDF">pdf</a>, <a href="/format/2310.12121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic prediction of mortality in patients with mental illness using  electronic health records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sean Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Samuel Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Mental disorders impact the lives of millions of people globally, not only
impeding their day-to-day lives but also markedly reducing life expectancy.
This paper addresses the persistent challenge of predicting mortality in
patients with mental diagnoses using predictive machine-learning models with
electronic health records (EHR). Data from patients with mental disease
diagnoses were extracted from the well-known clinical MIMIC-III data set
utilizing demographic, prescription, and procedural information. Four machine
learning algorithms (Logistic Regression, Random Forest, Support Vector
Machine, and K-Nearest Neighbors) were used, with results indicating that
Random Forest and Support Vector Machine models outperformed others, with AUC
scores of 0.911. Feature importance analysis revealed that drug prescriptions,
particularly Morphine Sulfate, play a pivotal role in prediction. We applied a
variety of machine learning algorithms to predict 30-day mortality followed by
feature importance analysis. This study can be used to assist hospital workers
in identifying at-risk patients to reduce excess mortality.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12126" title="Abstract">arXiv:2310.12126</a> [<a href="/pdf/2310.12126" title="Download PDF">pdf</a>, <a href="/format/2310.12126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SHARCS: Efficient Transformers through Routing with Dynamic Width  Sub-networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salehi%2C+M">Mohammadreza Salehi</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S">Sachin Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Kusupati%2C+A">Aditya Kusupati</a>, 
<a href="/search/cs?searchtype=author&query=Farhadi%2C+A">Ali Farhadi</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We introduce SHARCS for adaptive inference that takes into account the
hardness of input samples. SHARCS can train a router on any transformer
network, enabling the model to direct different samples to sub-networks with
varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or
complements existing per-sample adaptive inference methods across various
classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes
across different architectures and can be even applied to compressed and
efficient transformer encoders to further improve their efficiency; (3) SHARCS
can provide a 2 times inference speed up at an insignificant drop in accuracy.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12127" title="Abstract">arXiv:2310.12127</a> [<a href="/pdf/2310.12127" title="Download PDF">pdf</a>, <a href="/format/2310.12127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for  Fairer Instruction-Tuned Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Attanasio%2C+G">Giuseppe Attanasio</a>, 
<a href="/search/cs?searchtype=author&query=Plaza-del-Arco%2C+F+M">Flor Miriam Plaza-del-Arco</a>, 
<a href="/search/cs?searchtype=author&query=Nozza%2C+D">Debora Nozza</a>, 
<a href="/search/cs?searchtype=author&query=Lauscher%2C+A">Anne Lauscher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023. Code and data at <a href="https://github.com/MilaNLProc/interpretability-mt-gender-bias">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent instruction fine-tuned models can solve multiple NLP tasks when
prompted to do so, with machine translation (MT) being a prominent use case.
However, current research often focuses on standard performance benchmarks,
leaving compelling fairness and ethical considerations behind. In MT, this
might lead to misgendered translations, resulting, among other harms, in the
perpetuation of stereotypes and prejudices. In this work, we address this gap
by investigating whether and to what extent such models exhibit gender bias in
machine translation and how we can mitigate it. Concretely, we compute
established gender bias metrics on the WinoMT corpus from English to German and
Spanish. We discover that IFT models default to male-inflected translations,
even disregarding female occupational stereotypes. Next, using interpretability
methods, we unveil that models systematically overlook the pronoun indicating
the gender of a target occupation in misgendered translations. Finally, based
on this finding, we propose an easy-to-implement and effective bias mitigation
solution based on few-shot learning that leads to significantly fairer
translations.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12128" title="Abstract">arXiv:2310.12128</a> [<a href="/pdf/2310.12128" title="Download PDF">pdf</a>, <a href="/format/2310.12128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zala%2C+A">Abhay Zala</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Han Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaemin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://diagrammerGPT.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-to-image (T2I) generation has seen significant growth over the past few
years. Despite this, there has been little work on generating diagrams with T2I
models. A diagram is a symbolic/schematic representation that explains
information using structurally rich and spatially complex visualizations (e.g.,
a dense combination of related objects, text labels, directional arrows,
connection lines, etc.). Existing state-of-the-art T2I models often fail at
diagram generation because they lack fine-grained object layout control when
many objects are densely connected via complex relations such as arrows/lines
and also often fail to render comprehensible text labels. To address this gap,
we present DiagrammerGPT, a novel two-stage text-to-diagram generation
framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4)
to generate more accurate open-domain, open-platform diagrams. In the first
stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a
planner-auditor feedback loop) which describe all the entities (objects and
text labels), their relationships (arrows or lines), and their bounding box
layouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a
text label rendering module to generate diagrams following the diagram plans.
To benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a
densely annotated diagram dataset built on top of the AI2D dataset. We show
quantitatively and qualitatively that our DiagrammerGPT framework produces more
accurate diagrams, outperforming existing T2I models. We also provide
comprehensive analysis including open-domain diagram generation, vector graphic
diagram generation in different platforms, human-in-the-loop diagram plan
editing, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our
work can inspire further research on diagram generation via T2I models and
LLMs.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12131" title="Abstract">arXiv:2310.12131</a> [<a href="/pdf/2310.12131" title="Download PDF">pdf</a>, <a href="/format/2310.12131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Attribute Extraction from Legal Proceedings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adhikary%2C+S">Subinay Adhikary</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sagnik Das</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Sagnik Saha</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+P">Procheta Sen</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+D">Dwaipayan Roy</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+K">Kripabandhu Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented in Mining and Learning in the Legal Domain (MLLD) workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The escalating number of pending cases is a growing concern world-wide.
Recent advancements in digitization have opened up possibilities for leveraging
artificial intelligence (AI) tools in the processing of legal documents.
Adopting a structured representation for legal documents, as opposed to a mere
bag-of-words flat text representation, can significantly enhance processing
capabilities. With the aim of achieving this objective, we put forward a set of
diverse attributes for criminal case proceedings. We use a state-of-the-art
sequence labeling framework to automatically extract attributes from the legal
documents. Moreover, we demonstrate the efficacy of the extracted attributes in
a downstream task, namely legal judgment prediction.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12132" title="Abstract">arXiv:2310.12132</a> [<a href="/pdf/2310.12132" title="Download PDF">pdf</a>, <a href="/format/2310.12132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Effects of Computational Resources on Flaky Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+D">Denini Silva</a>, 
<a href="/search/cs?searchtype=author&query=Gruber%2C+M">Martin Gruber</a>, 
<a href="/search/cs?searchtype=author&query=Gokhale%2C+S">Satyajit Gokhale</a>, 
<a href="/search/cs?searchtype=author&query=Arteca%2C+E">Ellen Arteca</a>, 
<a href="/search/cs?searchtype=author&query=Turcotte%2C+A">Alexi Turcotte</a>, 
<a href="/search/cs?searchtype=author&query=d%27Amorim%2C+M">Marcelo d&#x27;Amorim</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wing Lam</a>, 
<a href="/search/cs?searchtype=author&query=Winter%2C+S">Stefan Winter</a>, 
<a href="/search/cs?searchtype=author&query=Bell%2C+J">Jonathan Bell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Flaky tests are tests that nondeterministically pass and fail in unchanged
code. These tests can be detrimental to developers' productivity. Particularly
when tests run in continuous integration environments, the tests may be
competing for access to limited computational resources (CPUs, memory etc.),
and we hypothesize that resource (in)availability may be a significant factor
in the failure rate of flaky tests. We present the first assessment of the
impact that computational resources have on flaky tests, including a total of
52 projects written in Java, JavaScript and Python, and 27 different resource
configurations. Using a rigorous statistical methodology, we determine which
tests are RAFT (Resource-Affected Flaky Tests). We find that 46.5% of the flaky
tests in our dataset are RAFT, indicating that a substantial proportion of
flaky-test failures can be avoided by adjusting the resources available when
running tests. We report RAFTs and configurations to avoid them to developers,
and received interest to either fix the RAFTs or to improve the specifications
of the projects so that tests would be run only in configurations that are
unlikely to encounter RAFT failures. Our results also have implications for
researchers attempting to detect flaky tests, e.g., reducing the resources
available when running tests is a cost-effective approach to detect more flaky
failures.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12133" title="Abstract">arXiv:2310.12133</a> [<a href="/pdf/2310.12133" title="Download PDF">pdf</a>, <a href="/ps/2310.12133" title="Download PostScript">ps</a>, <a href="/format/2310.12133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A comprehensible analysis of the efficacy of Ensemble Models for Bug  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mar%C3%A7al%2C+I">Ingrid Mar&#xe7;al</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+R+E">Rog&#xe9;rio Eduardo Garcia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The correctness of software systems is vital for their effective operation.
It makes discovering and fixing software bugs an important development task.
The increasing use of Artificial Intelligence (AI) techniques in Software
Engineering led to the development of a number of techniques that can assist
software developers in identifying potential bugs in code. In this paper, we
present a comprehensible comparison and analysis of the efficacy of two
AI-based approaches, namely single AI models and ensemble AI models, for
predicting the probability of a Java class being buggy. We used two open-source
Apache Commons Project's Java components for training and evaluating the
models. Our experimental findings indicate that the ensemble of AI models can
outperform the results of applying individual AI models. We also offer insight
into the factors that contribute to the enhanced performance of the ensemble AI
model. The presented results demonstrate the potential of using ensemble AI
models to enhance bug prediction results, which could ultimately result in more
reliable software systems.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12135" title="Abstract">arXiv:2310.12135</a> [<a href="/pdf/2310.12135" title="Download PDF">pdf</a>, <a href="/format/2310.12135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pseudointelligence: A Unifying Framework for Language Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murty%2C+S">Shikhar Murty</a>, 
<a href="/search/cs?searchtype=author&query=Paradise%2C+O">Orr Paradise</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+P">Pratyusha Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With large language models surpassing human performance on an increasing
number of benchmarks, we must take a principled approach for targeted
evaluation of model capabilities. Inspired by pseudorandomness, we propose
pseudointelligence, which captures the maxim that "(perceived) intelligence
lies in the eye of the beholder". That is, that claims of intelligence are
meaningful only when their evaluator is taken into account. Concretely, we
propose a complexity-theoretic framework of model evaluation cast as a dynamic
interaction between a model and a learned evaluator. We demonstrate that this
framework can be used to reason about two case studies in language model
evaluation, as well as analyze existing evaluation methods.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12142" title="Abstract">arXiv:2310.12142</a> [<a href="/pdf/2310.12142" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electro-Mechanical system design:Self-balancing Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hayder%2C+M">Mustakim Hayder</a>, 
<a href="/search/eess?searchtype=author&query=Morshed%2C+M+A+A">Md Abid Al Morshed</a>, 
<a href="/search/eess?searchtype=author&query=Maruf%2C+T+R">Tayfur Rahman Maruf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Self-balancing robot is based on the principle of Inverted pendulum, which is
a two-wheel vehicle balances itself up in the vertical position with reference
to the ground. It consists of both hardware and software implementation.
Mechanical model based on the state space design of the cart, pendulum system.
To find its stable inverted position, we used a generic feedback controller
(i.e., PID controller). According to the situation we have to control both
angel of pendulum and position of cart. Mechanical design consists of two dc
gear motor with encoder, one Arduino microcontroller, IMU (inertial mass unit)
sensor and motor driver as a basic need. IMU sensor which consists of
accelerometer and gyroscope gives the reference acceleration and angle with
respect to ground (vertical), When encoder which is attached with the motor
gives the speed of the motor. These parameters are taken as the system
parameter and determine the external force needed to balance the robot up. It
will be prevented from falling by giving acceleration to the wheels according
to its inclination from the vertical. If the bot tilts by an angle, then in the
frame of the wheels; the center of mass of the bot will experience a pseudo
force which will apply a torque opposite to the direction of tilt. We used a
HC05 Bluetooth module to control the robot via our smartphone.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12143" title="Abstract">arXiv:2310.12143</a> [<a href="/pdf/2310.12143" title="Download PDF">pdf</a>, <a href="/format/2310.12143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Mechanisms for Representing, Indexing and Manipulating Concepts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Meka%2C+R">Raghu Meka</a>, 
<a href="/search/cs?searchtype=author&query=Panigrahy%2C+R">Rina Panigrahy</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+K">Kulin Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep networks typically learn concepts via classifiers, which involves
setting up a model and training it via gradient descent to fit the
concept-labeled data. We will argue instead that learning a concept could be
done by looking at its moment statistics matrix to generate a concrete
representation or signature of that concept. These signatures can be used to
discover structure across the set of concepts and could recursively produce
higher-level concepts by learning this structure from those signatures. When
the concepts are `intersected', signatures of the concepts can be used to find
a common theme across a number of related `intersected' concepts. This process
could be used to keep a dictionary of concepts so that inputs could correctly
identify and be routed to the set of concepts involved in the (latent)
generation of the input.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12144" title="Abstract">arXiv:2310.12144</a> [<a href="/pdf/2310.12144" title="Download PDF">pdf</a>, <a href="/format/2310.12144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic financial processes identification using sparse regressive  reservoir computers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Vides%2C+F">Fredy Vides</a>, 
<a href="/search/eess?searchtype=author&query=Nogueira%2C+I+B+R">Idelfonso B. R. Nogueira</a>, 
<a href="/search/eess?searchtype=author&query=Banegas%2C+L">Lendy Banegas</a>, 
<a href="/search/eess?searchtype=author&query=Flores%2C+E">Evelyn Flores</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The content of this publication represents the opinion of the researchers affiliated with the Department of Statistics and Research, but not the official opinion of the CNBS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this document, we present key findings in structured matrix approximation
theory, with applications to the regressive representation of dynamic financial
processes. Initially, we explore a comprehensive approach involving generic
nonlinear time delay embedding for time series data extracted from a financial
or economic system under examination. Subsequently, we employ sparse
least-squares and structured matrix approximation methods to discern
approximate representations of the output coupling matrices. These
representations play a pivotal role in establishing the regressive models
corresponding to the recursive structures inherent in a given financial system.
The document further introduces prototypical algorithms that leverage the
aforementioned techniques. These algorithms are demonstrated through
applications in approximate identification and predictive simulation of dynamic
financial and economic processes, encompassing scenarios that may or may not
exhibit chaotic behavior.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12145" title="Abstract">arXiv:2310.12145</a> [<a href="/pdf/2310.12145" title="Download PDF">pdf</a>, <a href="/format/2310.12145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairer and More Accurate Tabular Models Through NAS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+R">Richeek Das</a>, 
<a href="/search/cs?searchtype=author&query=Dooley%2C+S">Samuel Dooley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (stat.ML)

</div>
<p class="mathjax">Making models algorithmically fairer in tabular data has been long studied,
with techniques typically oriented towards fixes which usually take a neural
model with an undesirable outcome and make changes to how the data are
ingested, what the model weights are, or how outputs are processed. We employ
an emergent and different strategy where we consider updating the model's
architecture and training hyperparameters to find an entirely new model with
better outcomes from the beginning of the debiasing procedure. In this work, we
propose using multi-objective Neural Architecture Search (NAS) and
Hyperparameter Optimization (HPO) in the first application to the very
challenging domain of tabular data. We conduct extensive exploration of
architectural and hyperparameter spaces (MLP, ResNet, and FT-Transformer)
across diverse datasets, demonstrating the dependence of accuracy and fairness
metrics of model predictions on hyperparameter combinations. We show that
models optimized solely for accuracy with NAS often fail to inherently address
fairness concerns. We propose a novel approach that jointly optimizes
architectural and training hyperparameters in a multi-objective constraint of
both accuracy and fairness. We produce architectures that consistently Pareto
dominate state-of-the-art bias mitigation methods either in fairness, accuracy
or both, all of this while being Pareto-optimal over hyperparameters achieved
through single-objective (accuracy) optimization runs. This research
underscores the promise of automating fairness and accuracy optimization in
deep learning models.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12147" title="Abstract">arXiv:2310.12147</a> [<a href="/pdf/2310.12147" title="Download PDF">pdf</a>, <a href="/format/2310.12147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InViG: Benchmarking Interactive Visual Grounding with 500K Human-Robot  Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+Y">Yuchen Mo</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+T">Tao Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures, 3 tables, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Ambiguity is ubiquitous in human communication. Previous approaches in
Human-Robot Interaction (HRI) have often relied on predefined interaction
templates, leading to reduced performance in realistic and open-ended
scenarios. To address these issues, we present a large-scale dataset, \invig,
for interactive visual grounding under language ambiguity. Our dataset
comprises over 520K images accompanied by open-ended goal-oriented
disambiguation dialogues, encompassing millions of object instances and
corresponding question-answer pairs. Leveraging the \invig dataset, we conduct
extensive studies and propose a set of baseline solutions for end-to-end
interactive visual disambiguation and grounding, achieving a 45.6\% success
rate during validation. To the best of our knowledge, the \invig dataset is the
first large-scale dataset for resolving open-ended interactive visual
grounding, presenting a practical yet highly challenging benchmark for
ambiguity-aware HRI. Codes and datasets are available at:
\href{https://openivg.github.io}{https://openivg.github.io}.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12149" title="Abstract">arXiv:2310.12149</a> [<a href="/pdf/2310.12149" title="Download PDF">pdf</a>, <a href="/format/2310.12149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object-aware Inversion and Reassembly for Image Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+D">Dinggang Gui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://aim-uofa.github.io/OIR-Diffusion/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">By comparing the original and target prompts in editing task, we can obtain
numerous editing pairs, each comprising an object and its corresponding editing
target. To allow editability while maintaining fidelity to the input image,
existing editing methods typically involve a fixed number of inversion steps
that project the whole input image to its noisier latent representation,
followed by a denoising process guided by the target prompt. However, we find
that the optimal number of inversion steps for achieving ideal editing results
varies significantly among different editing pairs, owing to varying editing
difficulties. Therefore, the current literature, which relies on a fixed number
of inversion steps, produces sub-optimal generation quality, especially when
handling multiple editing pairs in a natural image. To this end, we propose a
new image editing paradigm, dubbed Object-aware Inversion and Reassembly (OIR),
to enable object-level fine-grained editing. Specifically, we design a new
search metric, which determines the optimal inversion steps for each editing
pair, by jointly considering the editability of the target and the fidelity of
the non-editing region. We use our search metric to find the optimal inversion
step for each editing pair when editing an image. We then edit these editing
pairs separately to avoid concept mismatch. Subsequently, we propose an
additional reassembly step to seamlessly integrate the respective editing
results and the non-editing region to obtain the final edited image. To
systematically evaluate the effectiveness of our method, we collect two
datasets for benchmarking single- and multi-object editing, respectively.
Experiments demonstrate that our method achieves superior performance in
editing object shapes, colors, materials, categories, etc., especially in
multi-object editing scenarios.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12150" title="Abstract">arXiv:2310.12150</a> [<a href="/pdf/2310.12150" title="Download PDF">pdf</a>, <a href="/format/2310.12150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Retrieval Augmentation for Long-Form Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hung-Ting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Fangyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S+A">Shane A. Arora</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Eunsol Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present a study of retrieval-augmented language models (LMs) on long-form
question answering. We analyze how retrieval augmentation impacts different
LMs, by comparing answers generated from models while using the same evidence
documents, and how differing quality of retrieval document set impacts the
answers generated from the same LM. We study various attributes of generated
answers (e.g., fluency, length, variance) with an emphasis on the attribution
of generated long-form answers to in-context evidence documents. We collect
human annotations of answer attribution and evaluate methods for automatically
judging attribution. Our study provides new insights on how retrieval
augmentation impacts long, knowledge-rich text generation of LMs. We further
identify attribution patterns for long text generation and analyze the main
culprits of attribution errors. Together, our analysis reveals how retrieval
augmentation impacts long knowledge-rich text generation and provide directions
for future work.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12152" title="Abstract">arXiv:2310.12152</a> [<a href="/pdf/2310.12152" title="Download PDF">pdf</a>, <a href="/format/2310.12152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Rich Semantics and Coarse Locations for Long-tailed Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Lingchen Meng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xiyang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongdong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yinpeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yi-Ling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zuxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu-Gang Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Long-tailed object detection (LTOD) aims to handle the extreme data imbalance
in real-world datasets, where many tail classes have scarce instances. One
popular strategy is to explore extra data with image-level labels, yet it
produces limited results due to (1) semantic ambiguity -- an image-level label
only captures a salient part of the image, ignoring the remaining rich
semantics within the image; and (2) location sensitivity -- the label highly
depends on the locations and crops of the original image, which may change
after data transformations like random cropping. To remedy this, we propose
RichSem, a simple but effective method, which is robust to learn rich semantics
from coarse locations without the need of accurate bounding boxes. RichSem
leverages rich semantics from images, which are then served as additional soft
supervision for training detectors. Specifically, we add a semantic branch to
our detector to learn these soft semantics and enhance feature representations
for long-tailed object detection. The semantic branch is only used for training
and is removed during inference. RichSem achieves consistent improvements on
both overall and rare-category of LVIS under different backbones and detectors.
Our method achieves state-of-the-art performance without requiring complex
training and testing procedures. Moreover, we show the effectiveness of our
method on other long-tailed datasets with additional experiments. Code is
available at \url{https://github.com/MengLcool/RichSem}.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12153" title="Abstract">arXiv:2310.12153</a> [<a href="/pdf/2310.12153" title="Download PDF">pdf</a>, <a href="/format/2310.12153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum  Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaech%2C+J">Jan-Nico Zaech</a>, 
<a href="/search/cs?searchtype=author&query=Danelljan%2C+M">Martin Danelljan</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Adiabatic quantum computing (AQC) is a promising quantum computing approach
for discrete and often NP-hard optimization problems. Current AQCs allow to
implement problems of research interest, which has sparked the development of
quantum representations for many machine learning and computer vision tasks.
Despite requiring multiple measurements from the noisy AQC, current approaches
only utilize the best measurement, discarding information contained in the
remaining ones. In this work, we explore the potential of using this
information for probabilistic balanced k-means clustering. Instead of
discarding non-optimal solutions, we propose to use them to compute calibrated
posterior probabilities with little additional compute cost. This allows us to
identify ambiguous solutions and data points, which we demonstrate on a D-Wave
AQC on synthetic and real data.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Thu, 19 Oct 23</h3>
<dl>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00532" title="Abstract">arXiv:2310.00532</a> (cross-list from math.ST) [<a href="/pdf/2310.00532" title="Download PDF">pdf</a>, <a href="/format/2310.00532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation  and Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lin%2C+L">Licong Lin</a>, 
<a href="/search/math?searchtype=author&query=Ying%2C+M">Mufang Ying</a>, 
<a href="/search/math?searchtype=author&query=Ghosh%2C+S">Suvrojit Ghosh</a>, 
<a href="/search/math?searchtype=author&query=Khamaru%2C+K">Koulik Khamaru</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Cun-Hui Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Estimation and inference in statistics pose significant challenges when data
are collected adaptively. Even in linear models, the Ordinary Least Squares
(OLS) estimator may fail to exhibit asymptotic normality for single coordinate
estimation and have inflated error. This issue is highlighted by a recent
minimax lower bound, which shows that the error of estimating a single
coordinate can be enlarged by a multiple of $\sqrt{d}$ when data are allowed to
be arbitrarily adaptive, compared with the case when they are i.i.d. Our work
explores this striking difference in estimation performance between utilizing
i.i.d. and adaptive data. We investigate how the degree of adaptivity in data
collection impacts the performance of estimating a low-dimensional parameter
component in high-dimensional linear models. We identify conditions on the data
collection mechanism under which the estimation error for a low-dimensional
parameter component matches its counterpart in the i.i.d. setting, up to a
factor that depends on the degree of adaptivity. We show that OLS or OLS on
centered data can achieve this matching error. In addition, we propose a novel
estimator for single coordinate inference via solving a Two-stage Adaptive
Linear Estimating equation (TALE). Under a weaker form of adaptivity in data
collection, we establish an asymptotic normality property of the proposed
estimator.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11458" title="Abstract">arXiv:2310.11458</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2310.11458" title="Download PDF">pdf</a>, <a href="/format/2310.11458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4 as an interface between researchers and computational software:  improving usability and reproducibility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Verduzco%2C+J+C">Juan C. Verduzco</a>, 
<a href="/search/cond-mat?searchtype=author&query=Holbrook%2C+E">Ethan Holbrook</a>, 
<a href="/search/cond-mat?searchtype=author&query=Strachan%2C+A">Alejandro Strachan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are playing an increasingly important role in
science and engineering. For example, their ability to parse and understand
human and computer languages makes them powerful interpreters and their use in
applications like code generation are well-documented. We explore the ability
of the GPT-4 LLM to ameliorate two major challenges in computational materials
science: i) the high barriers for adoption of scientific software associated
with the use of custom input languages, and ii) the poor reproducibility of
published results due to insufficient details in the description of simulation
methods. We focus on a widely used software for molecular dynamics simulations,
the Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS), and
quantify the usefulness of input files generated by GPT-4 from task
descriptions in English and its ability to generate detailed descriptions of
computational tasks from input files. We find that GPT-4 can generate correct
and ready-to-use input files for relatively simple tasks and useful starting
points for more complex, multi-step simulations. In addition, GPT-4's
description of computational tasks from input files can be tuned from a
detailed set of step-by-step instructions to a summary description appropriate
for publications. Our results show that GPT-4 can reduce the number of routine
tasks performed by researchers, accelerate the training of new users, and
enhance reproducibility.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11480" title="Abstract">arXiv:2310.11480</a> (cross-list from eess.IV) [<a href="/pdf/2310.11480" title="Download PDF">pdf</a>, <a href="/format/2310.11480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whole-brain radiomics for clustered federated personalization in brain  tumor segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Manthe%2C+M">Matthis Manthe</a> (MYRIAD, LIRIS), 
<a href="/search/eess?searchtype=author&query=Duffner%2C+S">Stefan Duffner</a> (LIRIS), 
<a href="/search/eess?searchtype=author&query=Lartizien%2C+C">Carole Lartizien</a> (MYRIAD)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Medical Imaging with Deep Learning (MiDL) 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated learning and its application to medical image segmentation have
recently become a popular research topic. This training paradigm suffers from
statistical heterogeneity between participating institutions' local datasets,
incurring convergence slowdown as well as potential accuracy loss compared to
classical training. To mitigate this effect, federated personalization emerged
as the federated optimization of one model per institution. We propose a novel
personalization algorithm tailored to the feature shift induced by the usage of
different scanners and acquisition parameters by different institutions. This
method is the first to account for both inter and intra-institution feature
shift (multiple scanners used in a single institution). It is based on the
computation, within each centre, of a series of radiomic features capturing the
global texture of each 3D image volume, followed by a clustering analysis
pooling all feature vectors transferred from the local institutions to the
central server. Each computed clustered decentralized dataset (potentially
including data from different institutions) then serves to finetune a global
model obtained through classical federated learning. We validate our approach
on the Federated Brain Tumor Segmentation 2022 Challenge dataset (FeTS2022).
Our code is available at (https://github.com/MatthisManthe/radiomics_CFFL).
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11486" title="Abstract">arXiv:2310.11486</a> (cross-list from eess.AS) [<a href="/pdf/2310.11486" title="Download PDF">pdf</a>, <a href="/format/2310.11486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End real time tracking of children&#x27;s reading with pointer network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sunder%2C+V">Vishal Sunder</a>, 
<a href="/search/eess?searchtype=author&query=Karrolla%2C+B">Beulah Karrolla</a>, 
<a href="/search/eess?searchtype=author&query=Fosler-Lussier%2C+E">Eric Fosler-Lussier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we explore how a real time reading tracker can be built
efficiently for children's voices. While previously proposed reading trackers
focused on ASR-based cascaded approaches, we propose a fully end-to-end model
making it less prone to lags in voice tracking. We employ a pointer network
that directly learns to predict positions in the ground truth text conditioned
on the streaming speech. To train this pointer network, we generate ground
truth training signals by using forced alignment between the read speech and
the text being read on the training set. Exploring different forced alignment
models, we find a neural attention based model is at least as close in
alignment accuracy to the Montreal Forced Aligner, but surprisingly is a better
training signal for the pointer network. Our results are reported on one adult
speech data (TIMIT) and two children's speech datasets (CMU Kids and Reading
Races). Our best model can accurately track adult speech with 87.8% accuracy
and the much harder and disfluent children's speech with 77.1% accuracy on CMU
Kids data and a 65.3% accuracy on the Reading Races dataset.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11526" title="Abstract">arXiv:2310.11526</a> (cross-list from quant-ph) [<a href="/pdf/2310.11526" title="Download PDF">pdf</a>, <a href="/ps/2310.11526" title="Download PostScript">ps</a>, <a href="/format/2310.11526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Commitments from Quantum One-Wayness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Khurana%2C+D">Dakshita Khurana</a> (UIUC), 
<a href="/search/quant-ph?searchtype=author&query=Tomer%2C+K">Kabir Tomer</a> (UIUC)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 68 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">One-way functions are central to classical cryptography. They are both
necessary for the existence of non-trivial classical cryptosystems, and
sufficient to realize meaningful primitives including commitments, pseudorandom
generators and digital signatures. At the same time, a mounting body of
evidence suggests that assumptions even weaker than one-way functions may
suffice for many cryptographic tasks of interest in a quantum world, including
bit commitments and secure multi-party computation. This work studies one-way
state generators, a natural quantum relaxation of one-way functions. Given a
secret key, a one-way state generator outputs a hard to invert quantum state. A
fundamental question is whether this type of quantum one-wayness suffices to
realize quantum cryptography. We obtain an affirmative answer to this question,
by proving that one-way state generators with pure state outputs imply quantum
bit commitments and secure multiparty computation. Along the way, we build an
intermediate primitive with classical outputs, which we call a (quantum)
one-way puzzle. Our main technical contribution is a proof that one-way puzzles
imply quantum bit commitments.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11527" title="Abstract">arXiv:2310.11527</a> (cross-list from stat.ML) [<a href="/pdf/2310.11527" title="Download PDF">pdf</a>, <a href="/format/2310.11527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thin and Deep Gaussian Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=de+Souza%2C+D+A">Daniel Augusto de Souza</a>, 
<a href="/search/stat?searchtype=author&query=Nikitin%2C+A">Alexander Nikitin</a>, 
<a href="/search/stat?searchtype=author&query=John%2C+S">ST John</a>, 
<a href="/search/stat?searchtype=author&query=Ross%2C+M">Magnus Ross</a>, 
<a href="/search/stat?searchtype=author&query=%C3%81lvarez%2C+M+A">Mauricio A. &#xc1;lvarez</a>, 
<a href="/search/stat?searchtype=author&query=Deisenroth%2C+M+P">Marc Peter Deisenroth</a>, 
<a href="/search/stat?searchtype=author&query=Gomes%2C+J+P+P">Jo&#xe3;o P. P. Gomes</a>, 
<a href="/search/stat?searchtype=author&query=Mesquita%2C+D">Diego Mesquita</a>, 
<a href="/search/stat?searchtype=author&query=Mattos%2C+C+L+C">C&#xe9;sar Lincoln C. Mattos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Conference on Neural Information Processing Systems (NeurIPS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Gaussian processes (GPs) can provide a principled approach to uncertainty
quantification with easy-to-interpret kernel hyperparameters, such as the
lengthscale, which controls the correlation distance of function values.
However, selecting an appropriate kernel can be challenging. Deep GPs avoid
manual kernel engineering by successively parameterizing kernels with GP
layers, allowing them to learn low-dimensional embeddings of the inputs that
explain the output data. Following the architecture of deep neural networks,
the most common deep GPs warp the input space layer-by-layer but lose all the
interpretability of shallow GPs. An alternative construction is to successively
parameterize the lengthscale of a kernel, improving the interpretability but
ultimately giving away the notion of learning lower-dimensional embeddings.
Unfortunately, both methods are susceptible to particular pathologies which may
hinder fitting and limit their interpretability. This work proposes a novel
synthesis of both previous approaches: Thin and Deep GP (TDGP). Each TDGP layer
defines locally linear transformations of the original input data maintaining
the concept of latent embeddings while also retaining the interpretation of
lengthscales of a kernel. Moreover, unlike the prior solutions, TDGP induces
non-pathological manifolds that admit learning lower-dimensional
representations. We show with theoretical and experimental results that i) TDGP
is, unlike previous models, tailored to specifically discover lower-dimensional
manifolds in the input data, ii) TDGP behaves well when increasing the number
of layers, and iii) TDGP performs well in standard benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11535" title="Abstract">arXiv:2310.11535</a> (cross-list from eess.IV) [<a href="/pdf/2310.11535" title="Download PDF">pdf</a>, <a href="/format/2310.11535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Lens Blur Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+E+Y+H">Esther Y. H. Lin</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhecheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+R">Rebecca Lin</a>, 
<a href="/search/eess?searchtype=author&query=Miau%2C+D">Daniel Miau</a>, 
<a href="/search/eess?searchtype=author&query=Kainz%2C+F">Florian Kainz</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jiawen Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X+C">Xuaner Cecilia Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Lindell%2C+D+B">David B. Lindell</a>, 
<a href="/search/eess?searchtype=author&query=Kutulakos%2C+K+N">Kiriakos N. Kutulakos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Optical blur is an inherent property of any lens system and is challenging to
model in modern cameras because of their complex optical elements. To tackle
this challenge, we introduce a high-dimensional neural representation of
blur$-$$\textit{the lens blur field}$$-$and a practical method for acquiring
it. The lens blur field is a multilayer perceptron (MLP) designed to (1)
accurately capture variations of the lens 2D point spread function over image
plane location, focus setting and, optionally, depth and (2) represent these
variations parametrically as a single, sensor-specific function. The
representation models the combined effects of defocus, diffraction, aberration,
and accounts for sensor features such as pixel color filters and pixel-specific
micro-lenses. To learn the real-world blur field of a given device, we
formulate a generalized non-blind deconvolution problem that directly optimizes
the MLP weights using a small set of focal stacks as the only input. We also
provide a first-of-its-kind dataset of 5D blur fields$-$for smartphone cameras,
camera bodies equipped with a variety of lenses, etc. Lastly, we show that
acquired 5D blur fields are expressive and accurate enough to reveal, for the
first time, differences in optical behavior of smartphone devices of the same
make and model.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11577" title="Abstract">arXiv:2310.11577</a> (cross-list from eess.IV) [<a href="/pdf/2310.11577" title="Download PDF">pdf</a>, <a href="/format/2310.11577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Studying the Effects of Sex-related Differences on Brain Age Prediction  using brain MR Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dibaji%2C+M">Mahsa Dibaji</a>, 
<a href="/search/eess?searchtype=author&query=Gianchandani%2C+N">Neha Gianchandani</a>, 
<a href="/search/eess?searchtype=author&query=Nair%2C+A">Akhil Nair</a>, 
<a href="/search/eess?searchtype=author&query=Singhal%2C+M">Mansi Singhal</a>, 
<a href="/search/eess?searchtype=author&query=Souza%2C+R">Roberto Souza</a>, 
<a href="/search/eess?searchtype=author&query=Bento%2C+M">Mariana Bento</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">While utilizing machine learning models, one of the most crucial aspects is
how bias and fairness affect model outcomes for diverse demographics. This
becomes especially relevant in the context of machine learning for medical
imaging applications as these models are increasingly being used for diagnosis
and treatment planning. In this paper, we study biases related to sex when
developing a machine learning model based on brain magnetic resonance images
(MRI). We investigate the effects of sex by performing brain age prediction
considering different experimental designs: model trained using only female
subjects, only male subjects and a balanced dataset. We also perform evaluation
on multiple MRI datasets (Calgary-Campinas(CC359) and CamCAN) to assess the
generalization capability of the proposed models. We found disparities in the
performance of brain age prediction models when trained on distinct sex
subgroups and datasets, in both final predictions and decision making (assessed
using interpretability models). Our results demonstrated variations in model
generalizability across sex-specific subgroups, suggesting potential biases in
models trained on unbalanced datasets. This underlines the critical role of
careful experimental design in generating fair and reliable outcomes.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11641" title="Abstract">arXiv:2310.11641</a> (cross-list from eess.IV) [<a href="/pdf/2310.11641" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cloud-Magnetic Resonance Imaging System: In the Era of 6G and Artificial  Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yirong Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Yanhuang Wu</a>, 
<a href="/search/eess?searchtype=author&query=Su%2C+Y">Yuhan Su</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/eess?searchtype=author&query=Cai%2C+J">Jianyun Cai</a>, 
<a href="/search/eess?searchtype=author&query=You%2C+Y">Yongfu You</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+D">Di Guo</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+X">Xiaobo Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4pages, 5figures, letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Magnetic Resonance Imaging (MRI) plays an important role in medical
diagnosis, generating petabytes of image data annually in large hospitals. This
voluminous data stream requires a significant amount of network bandwidth and
extensive storage infrastructure. Additionally, local data processing demands
substantial manpower and hardware investments. Data isolation across different
healthcare institutions hinders cross-institutional collaboration in clinics
and research. In this work, we anticipate an innovative MRI system and its four
generations that integrate emerging distributed cloud computing, 6G bandwidth,
edge computing, federated learning, and blockchain technology. This system is
called Cloud-MRI, aiming at solving the problems of MRI data storage security,
transmission speed, AI algorithm maintenance, hardware upgrading, and
collaborative work. The workflow commences with the transformation of k-space
raw data into the standardized Imaging Society for Magnetic Resonance in
Medicine Raw Data (ISMRMRD) format. Then, the data are uploaded to the cloud or
edge nodes for fast image reconstruction, neural network training, and
automatic analysis. Then, the outcomes are seamlessly transmitted to clinics or
research institutes for diagnosis and other services. The Cloud-MRI system will
save the raw imaging data, reduce the risk of data loss, facilitate
inter-institutional medical collaboration, and finally improve diagnostic
accuracy and work efficiency.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11708" title="Abstract">arXiv:2310.11708</a> (cross-list from eess.AS) [<a href="/pdf/2310.11708" title="Download PDF">pdf</a>, <a href="/format/2310.11708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental Results of Underwater Sound Speed Profile Inversion by  Few-shot Multi-task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+W">Wei Huang</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+F">Fan Gao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Junting Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Underwater Sound Speed Profile (SSP) distribution has great influence on the
propagation mode of acoustic signal, thus the fast and accurate estimation of
SSP is of great importance in building underwater observation systems. The
state-of-the-art SSP inversion methods include frameworks of matched field
processing (MFP), compressive sensing (CS), and feedforeward neural networks
(FNN), among which the FNN shows better real-time performance while maintain
the same level of accuracy. However, the training of FNN needs quite a lot
historical SSP samples, which is diffcult to be satisfied in many ocean areas.
This situation is called few-shot learning. To tackle this issue, we propose a
multi-task learning (MTL) model with partial parameter sharing among different
traning tasks. By MTL, common features could be extracted, thus accelerating
the learning process on given tasks, and reducing the demand for reference
samples, so as to enhance the generalization ability in few-shot learning. To
verify the feasibility and effectiveness of MTL, a deep-ocean experiment was
held in April 2023 at the South China Sea. Results shows that MTL outperforms
the state-of-the-art methods in terms of accuracy for SSP inversion, while
inherits the real-time advantage of FNN during the inversion stage.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11759" title="Abstract">arXiv:2310.11759</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.11759" title="Download PDF">pdf</a>, <a href="/format/2310.11759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual Measurements, Distances and Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Vacher%2C+J">Jonathan Vacher</a>, 
<a href="/search/q-bio?searchtype=author&query=Mamassian%2C+P">Pascal Mamassian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 6 figures, 5 appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT)

</div>
<p class="mathjax">Perception is often viewed as a process that transforms physical variables,
external to an observer, into internal psychological variables. Such a process
can be modeled by a function coined perceptual scale. The perceptual scale can
be deduced from psychophysical measurements that consist in comparing the
relative differences between stimuli (i.e. difference scaling experiments).
However, this approach is often overlooked by the modeling and experimentation
communities. Here, we demonstrate the value of measuring the perceptual scale
of classical (spatial frequency, orientation) and less classical physical
variables (interpolation between textures) by embedding it in recent
probabilistic modeling of perception. First, we show that the assumption that
an observer has an internal representation of univariate parameters such as
spatial frequency or orientation while stimuli are high-dimensional does not
lead to contradictory predictions when following the theoretical framework.
Second, we show that the measured perceptual scale corresponds to the
transduction function hypothesized in this framework. In particular, we
demonstrate that it is related to the Fisher information of the generative
model that underlies perception and we test the predictions given by the
generative model of different stimuli in a set a of difference scaling
experiments. Our main conclusion is that the perceptual scale is mostly driven
by the stimulus power spectrum. Finally, we propose that this measure of
perceptual scale is a way to push further the notion of perceptual distances by
estimating the perceptual geometry of images i.e. the path between images
instead of simply the distance between those.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11807" title="Abstract">arXiv:2310.11807</a> (cross-list from quant-ph) [<a href="/pdf/2310.11807" title="Download PDF">pdf</a>, <a href="/format/2310.11807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning and Discovering Quantum Properties with Multi-Task Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+Y">Ya-Dong Wu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhu%2C+Y">Yan Zhu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+Y">Yuexuan Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chiribella%2C+G">Giulio Chiribella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep neural networks are a powerful tool for predicting properties of quantum
states from limited measurement data. Here we develop a network model that can
simultaneously predict multiple quantum properties, including not only
expectation values of quantum observables, but also general nonlinear functions
of the quantum state, like entanglement entropies and many-body topological
invariants. Remarkably, we find that a model trained on a given set of
properties can also discover new properties outside that set. Multi-purpose
training also enables the model to infer global properties of many-body quantum
systems from local measurements, to classify symmetry protected topological
phases of matter, and to discover unknown boundaries between different phases.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11837" title="Abstract">arXiv:2310.11837</a> (cross-list from stat.ML) [<a href="/pdf/2310.11837" title="Download PDF">pdf</a>, <a href="/format/2310.11837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimising Distributions with Natural Gradient Surrogates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=So%2C+J">Jonathan So</a>, 
<a href="/search/stat?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Natural gradient methods have been used to optimise the parameters of
probability distributions in a variety of settings, often resulting in
fast-converging procedures. Unfortunately, for many distributions of interest,
computing the natural gradient has a number of challenges. In this work we
propose a novel technique for tackling such issues, which involves reframing
the optimisation as one with respect to the parameters of a surrogate
distribution, for which computing the natural gradient is easy. We give several
examples of existing methods that can be interpreted as applying this
technique, and propose a new method for applying it to a wide variety of
problems. Our method expands the set of distributions that can be efficiently
targeted with natural gradients. Furthermore, it is fast, easy to understand,
simple to implement using standard autodiff software, and does not require
lengthy model-specific derivations. We demonstrate our method on maximum
likelihood estimation and variational inference tasks.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11838" title="Abstract">arXiv:2310.11838</a> (cross-list from eess.IV) [<a href="/pdf/2310.11838" title="Download PDF">pdf</a>, <a href="/format/2310.11838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Bootstrapping for Uncertainty Quantification in Imaging  Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tachella%2C+J">Julian Tachella</a>, 
<a href="/search/eess?searchtype=author&query=Pereyra%2C+M">Marcelo Pereyra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Scientific imaging problems are often severely ill-posed, and hence have
significant intrinsic uncertainty. Accurately quantifying the uncertainty in
the solutions to such problems is therefore critical for the rigorous
interpretation of experimental results as well as for reliably using the
reconstructed images as scientific evidence. Unfortunately, existing imaging
methods are unable to quantify the uncertainty in the reconstructed images in a
manner that is robust to experiment replications. This paper presents a new
uncertainty quantification methodology based on an equivariant formulation of
the parametric bootstrap algorithm that leverages symmetries and invariance
properties commonly encountered in imaging problems. Additionally, the proposed
methodology is general and can be easily applied with any image reconstruction
technique, including unsupervised training strategies that can be trained from
observed data alone, thus enabling uncertainty quantification in situations
where there is no ground truth data available. We demonstrate the proposed
approach with a series of numerical experiments and through comparisons with
alternative uncertainty quantification strategies from the state-of-the-art,
such as Bayesian strategies involving score-based diffusion models and Langevin
samplers. In all our experiments, the proposed method delivers remarkably
accurate high-dimensional confidence regions and outperforms the competing
approaches in terms of estimation accuracy, uncertainty quantification
accuracy, and computing time.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11888" title="Abstract">arXiv:2310.11888</a> (cross-list from astro-ph.EP) [<a href="/pdf/2310.11888" title="Download PDF">pdf</a>, <a href="/format/2310.11888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyze Mass Spectrometry data with Artificial Intelligence to assist  the understanding of past habitability of Mars and provide insights for  future missions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Nasios%2C+I">Ioannis Nasios</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">This paper presents an application of artificial intelligence on mass
spectrometry data for detecting habitability potential of ancient Mars.
Although data was collected for planet Mars the same approach can be replicated
for any terrestrial object of our solar system. Furthermore, proposed
methodology can be adapted to any domain that uses mass spectrometry. This
research is focused in data analysis of two mass spectrometry techniques,
evolved gas analysis (EGA-MS) and gas chromatography (GC-MS), which are used to
identify specific chemical compounds in geological material samples. The study
demonstrates the applicability of EGA-MS and GC-MS data to extra-terrestrial
material analysis. Most important features of proposed methodology includes
square root transformation of mass spectrometry values, conversion of raw data
to 2D sprectrograms and utilization of specific machine learning models and
techniques to avoid overfitting on relative small datasets. Both EGA-MS and
GC-MS datasets come from NASA and two machine learning competitions that the
author participated and exploited. Complete running code for the GC-MS
dataset/competition is available at GitHub.1 Raw training mass spectrometry
data include [0, 1] labels of specific chemical compounds, selected to provide
valuable insights and contribute to our understanding of the potential past
habitability of Mars.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11891" title="Abstract">arXiv:2310.11891</a> (cross-list from quant-ph) [<a href="/pdf/2310.11891" title="Download PDF">pdf</a>, <a href="/format/2310.11891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hyperparameter Study for Quantum Kernel Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Egginger%2C+S">Sebastian Egginger</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sakhnenko%2C+A">Alona Sakhnenko</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lorenz%2C+J+M">Jeanette Miriam Lorenz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantum kernel methods are a promising method in quantum machine learning
thanks to the guarantees connected to them. Their accessibility for analytic
considerations also opens up the possibility of prescreening datasets based on
their potential for a quantum advantage. To do so, earlier works developed the
geometric difference, which can be understood as a closeness measure between
two kernel-based machine learning approaches, most importantly between a
quantum kernel and classical kernel. This metric links the quantum and
classical model complexities. Therefore, it raises the question of whether the
geometric difference, based on its relation to model complexity, can be a
useful tool in evaluations other than for the potential for quantum advantage.
In this work, we investigate the effects of hyperparameter choice on the model
performance and the generalization gap between classical and quantum kernels.
The importance of hyperparameter optimization is well known also for classical
machine learning. Especially for the quantum Hamiltonian evolution feature map,
the scaling of the input data has been shown to be crucial. However, there are
additional parameters left to be optimized, like the best number of qubits to
trace out before computing a projected quantum kernel. We investigate the
influence of these hyperparameters and compare the classically reliable method
of cross validation with the method of choosing based on the geometric
difference. Based on the thorough investigation of the hyperparameters across
11 datasets we identified commodities that can be exploited when examining a
new dataset. In addition, our findings contribute to better understanding of
the applicability of the geometric difference.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11896" title="Abstract">arXiv:2310.11896</a> (cross-list from eess.IV) [<a href="/pdf/2310.11896" title="Download PDF">pdf</a>, <a href="/format/2310.11896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Multimodal Medical Image Fusion based on Laplacian Autoencoder  with Channel Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wankhede%2C+P">Payal Wankhede</a>, 
<a href="/search/eess?searchtype=author&query=Das%2C+M">Manisha Das</a>, 
<a href="/search/eess?searchtype=author&query=Gupta%2C+D">Deep Gupta</a>, 
<a href="/search/eess?searchtype=author&query=Radeva%2C+P">Petia Radeva</a>, 
<a href="/search/eess?searchtype=author&query=Bakde%2C+A+M">Ashwini M Bakde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, % tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Medical image fusion combines the complementary information of multimodal
medical images to assist medical professionals in the clinical diagnosis of
patients' disorders and provide guidance during preoperative and
intra-operative procedures. Deep learning (DL) models have achieved end-to-end
image fusion with highly robust and accurate fusion performance. However, most
DL-based fusion models perform down-sampling on the input images to minimize
the number of learnable parameters and computations. During this process,
salient features of the source images become irretrievable leading to the loss
of crucial diagnostic edge details and contrast of various brain tissues. In
this paper, we propose a new multimodal medical image fusion model is proposed
that is based on integrated Laplacian-Gaussian concatenation with attention
pooling (LGCA). We prove that our model preserves effectively complementary
information and important tissue structures.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11910" title="Abstract">arXiv:2310.11910</a> (cross-list from eess.IV) [<a href="/pdf/2310.11910" title="Download PDF">pdf</a>, <a href="/format/2310.11910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-modal Medical Neurological Image Fusion using Wavelet Pooled Edge  Preserving Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Das%2C+M">Manisha Das</a>, 
<a href="/search/eess?searchtype=author&query=Gupta%2C+D">Deep Gupta</a>, 
<a href="/search/eess?searchtype=author&query=Radeva%2C+P">Petia Radeva</a>, 
<a href="/search/eess?searchtype=author&query=Bakde%2C+A+M">Ashwini M Bakde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Medical image fusion integrates the complementary diagnostic information of
the source image modalities for improved visualization and analysis of
underlying anomalies. Recently, deep learning-based models have excelled the
conventional fusion methods by executing feature extraction, feature selection,
and feature fusion tasks, simultaneously. However, most of the existing
convolutional neural network (CNN) architectures use conventional pooling or
strided convolutional strategies to downsample the feature maps. It causes the
blurring or loss of important diagnostic information and edge details available
in the source images and dilutes the efficacy of the feature extraction
process. Therefore, this paper presents an end-to-end unsupervised fusion model
for multimodal medical images based on an edge-preserving dense autoencoder
network. In the proposed model, feature extraction is improved by using wavelet
decomposition-based attention pooling of feature maps. This helps in preserving
the fine edge detail information present in both the source images and enhances
the visual perception of fused images. Further, the proposed model is trained
on a variety of medical image pairs which helps in capturing the intensity
distributions of the source images and preserves the diagnostic information
effectively. Substantial experiments are conducted which demonstrate that the
proposed method provides improved visual and quantitative results as compared
to the other state-of-the-art fusion methods.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11940" title="Abstract">arXiv:2310.11940</a> (cross-list from stat.ML) [<a href="/pdf/2310.11940" title="Download PDF">pdf</a>, <a href="/format/2310.11940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Spectral Variational AutoEncoder (ISVAE) for time series  clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rama%2C+%C3%93+J">&#xd3;scar Jim&#xe9;nez Rama</a>, 
<a href="/search/stat?searchtype=author&query=Moreno-Pino%2C+F">Fernando Moreno-Pino</a>, 
<a href="/search/stat?searchtype=author&query=Ram%C3%ADrez%2C+D">David Ram&#xed;rez</a>, 
<a href="/search/stat?searchtype=author&query=Olmos%2C+P+M">Pablo M. Olmos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The best encoding is the one that is interpretable in nature. In this work,
we introduce a novel model that incorporates an interpretable bottleneck-termed
the Filter Bank (FB)-at the outset of a Variational Autoencoder (VAE). This
arrangement compels the VAE to attend on the most informative segments of the
input signal, fostering the learning of a novel encoding ${f_0}$ which boasts
enhanced interpretability and clusterability over traditional latent spaces. By
deliberately constraining the VAE with this FB, we intentionally constrict its
capacity to access broad input domain information, promoting the development of
an encoding that is discernible, separable, and of reduced dimensionality. The
evolutionary learning trajectory of ${f_0}$ further manifests as a dynamic
hierarchical tree, offering profound insights into cluster similarities.
Additionally, for handling intricate data configurations, we propose a tailored
decoder structure that is symmetrically aligned with FB's architecture.
Empirical evaluations highlight the superior efficacy of ISVAE, which compares
favorably to state-of-the-art results in clustering metrics across real-world
datasets.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11945" title="Abstract">arXiv:2310.11945</a> (cross-list from math.DS) [<a href="/pdf/2310.11945" title="Download PDF">pdf</a>, <a href="/format/2310.11945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Downscaling Using CDAnet Under Observational and Model Noises: The  Rayleigh-Benard Convection Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hammoud%2C+M+A+E+R">Mohamad Abed El Rahman Hammoud</a>, 
<a href="/search/math?searchtype=author&query=Titi%2C+E+S">Edriss S. Titi</a>, 
<a href="/search/math?searchtype=author&query=Hoteit%2C+I">Ibrahim Hoteit</a>, 
<a href="/search/math?searchtype=author&query=Knio%2C+O">Omar Knio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Efficient downscaling of large ensembles of coarse-scale information is
crucial in several applications, such as oceanic and atmospheric modeling. The
determining form map is a theoretical lifting function from the low-resolution
solution trajectories of a dissipative dynamical system to their corresponding
fine-scale counterparts. Recently, a physics-informed deep neural network
("CDAnet") was introduced, providing a surrogate of the determining form map
for efficient downscaling. CDAnet was demonstrated to efficiently downscale
noise-free coarse-scale data in a deterministic setting. Herein, the
performance of well-trained CDAnet models is analyzed in a stochastic setting
involving (i) observational noise, (ii) model noise, and (iii) a combination of
observational and model noises. The analysis is performed employing the
Rayleigh-Benard convection paradigm, under three training conditions, namely,
training with perfect, noisy, or downscaled data. Furthermore, the effects of
noises, Rayleigh number, and spatial and temporal resolutions of the input
coarse-scale information on the downscaled fields are examined. The results
suggest that the expected l2-error of CDAnet behaves quadratically in terms of
the standard deviations of the observational and model noises. The results also
suggest that CDAnet responds to uncertainties similar to the theorized and
numerically-validated CDA behavior with an additional error overhead due to
CDAnet being a surrogate model of the determining form map.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11961" title="Abstract">arXiv:2310.11961</a> (cross-list from math.AP) [<a href="/pdf/2310.11961" title="Download PDF">pdf</a>, <a href="/ps/2310.11961" title="Download PostScript">ps</a>, <a href="/format/2310.11961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Kernel-Density-Estimator Minimizing Movement Scheme for Diffusion  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Flei%C3%9Fner%2C+F">Florentine Flei&#xdf;ner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The mathematical theory of a novel variational approximation scheme for
general second and fourth order partial differential equations
\begin{equation}\label{eq: A} \partial_t u -
\nabla\cdot\Big(u\nabla\frac{\delta\phi}{\delta
u}(u)\Big|\nabla\frac{\delta\phi}{\delta u}(u)\Big|^{q-2}\Big) \ = \ 0,
\quad\quad u\geq0, \end{equation} $q\in(1, +\infty)$, is developed.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11973" title="Abstract">arXiv:2310.11973</a> (cross-list from math.OC) [<a href="/pdf/2310.11973" title="Download PDF">pdf</a>, <a href="/format/2310.11973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Gradient-Free Methods for Stochastic Non-Smooth Non-Convex  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lin%2C+Z">Zhenwei Lin</a>, 
<a href="/search/math?searchtype=author&query=Xia%2C+J">Jingfan Xia</a>, 
<a href="/search/math?searchtype=author&query=Deng%2C+Q">Qi Deng</a>, 
<a href="/search/math?searchtype=author&query=Luo%2C+L">Luo Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We consider decentralized gradient-free optimization of minimizing Lipschitz
continuous functions that satisfy neither smoothness nor convexity assumption.
We propose two novel gradient-free algorithms, the Decentralized Gradient-Free
Method (DGFM) and its variant, the Decentralized Gradient-Free Method$^+$
(DGFM$^{+}$). Based on the techniques of randomized smoothing and gradient
tracking, DGFM requires the computation of the zeroth-order oracle of a single
sample in each iteration, making it less demanding in terms of computational
resources for individual computing nodes. Theoretically, DGFM achieves a
complexity of $\mathcal O(d^{3/2}\delta^{-1}\varepsilon ^{-4})$ for obtaining
an $(\delta,\varepsilon)$-Goldstein stationary point. DGFM$^{+}$, an advanced
version of DGFM, incorporates variance reduction to further improve the
convergence behavior. It samples a mini-batch at each iteration and
periodically draws a larger batch of data, which improves the complexity to
$\mathcal O(d^{3/2}\delta^{-1} \varepsilon^{-3})$. Moreover, experimental
results underscore the empirical advantages of our proposed algorithms when
applied to real-world datasets.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11978" title="Abstract">arXiv:2310.11978</a> (cross-list from stat.ML) [<a href="/pdf/2310.11978" title="Download PDF">pdf</a>, <a href="/format/2310.11978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can bin-wise scaling improve consistency and adaptivity of prediction  uncertainty for machine learning regression ?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Pernot%2C+P">Pascal Pernot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Chemical Physics (physics.chem-ph); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">Binwise Variance Scaling (BVS) has recently been proposed as a post hoc
recalibration method for prediction uncertainties of machine learning
regression problems that is able of more efficient corrections than uniform
variance (or temperature) scaling. The original version of BVS uses
uncertainty-based binning, which is aimed to improve calibration conditionally
on uncertainty, i.e. consistency. I explore here several adaptations of BVS, in
particular with alternative loss functions and a binning scheme based on an
input-feature (X) in order to improve adaptivity, i.e. calibration conditional
on X. The performances of BVS and its proposed variants are tested on a
benchmark dataset for the prediction of atomization energies and compared to
the results of isotonic regression.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12000" title="Abstract">arXiv:2310.12000</a> (cross-list from stat.ME) [<a href="/pdf/2310.12000" title="Download PDF">pdf</a>, <a href="/format/2310.12000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian  Process Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=K%C3%BCndig%2C+P">Pascal K&#xfc;ndig</a>, 
<a href="/search/stat?searchtype=author&query=Sigrist%2C+F">Fabio Sigrist</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Latent Gaussian process (GP) models are flexible probabilistic non-parametric
function models. Vecchia approximations are accurate approximations for GPs to
overcome computational bottlenecks for large data, and the Laplace
approximation is a fast method with asymptotic convergence guarantees to
approximate marginal likelihoods and posterior predictive distributions for
non-Gaussian likelihoods. Unfortunately, the computational complexity of
combined Vecchia-Laplace approximations grows faster than linearly in the
sample size when used in combination with direct solver methods such as the
Cholesky decomposition. Computations with Vecchia-Laplace approximations thus
become prohibitively slow precisely when the approximations are usually the
most accurate, i.e., on large data sets. In this article, we present several
iterative methods for inference with Vecchia-Laplace approximations which make
computations considerably faster compared to Cholesky-based calculations. We
analyze our proposed methods theoretically and in experiments with simulated
and real-world data. In particular, we obtain a speed-up of an order of
magnitude compared to Cholesky-based inference and a threefold increase in
prediction accuracy in terms of the continuous ranked probability score
compared to a state-of-the-art method on a large satellite data set. All
methods are implemented in a free C++ software library with high-level Python
and R packages.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12021" title="Abstract">arXiv:2310.12021</a> (cross-list from math.OC) [<a href="/pdf/2310.12021" title="Download PDF">pdf</a>, <a href="/format/2310.12021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Distributionally Robust Mitigation of Risk of Cascading  Failures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+G">Guangyi Liu</a>, 
<a href="/search/math?searchtype=author&query=Amini%2C+A">Arash Amini</a>, 
<a href="/search/math?searchtype=author&query=Pandey%2C+V">Vivek Pandey</a>, 
<a href="/search/math?searchtype=author&query=Motee%2C+N">Nader Motee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We introduce a novel data-driven method to mitigate the risk of cascading
failures in delayed discrete-time Linear Time-Invariant (LTI) systems. Our
approach involves formulating a distributionally robust finite-horizon optimal
control problem, where the objective is to minimize a given performance
function while satisfying a set of distributionally chances constraints on
cascading failures, which accounts for the impact of a known sequence of
failures that can be characterized using nested sets. The optimal control
problem becomes challenging as the risk of cascading failures and input
time-delay poses limitations on the set of feasible control inputs. However, by
solving the convex formulation of the distributionally robust model predictive
control (DRMPC) problem, the proposed approach is able to keep the system from
cascading failures while maintaining the system's performance with delayed
control input, which has important implications for designing and operating
complex engineering systems, where cascading failures can severely affect
system performance, safety, and reliability.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12026" title="Abstract">arXiv:2310.12026</a> (cross-list from stat.ML) [<a href="/pdf/2310.12026" title="Download PDF">pdf</a>, <a href="/format/2310.12026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonparametric Discrete Choice Experiments with Machine Learning Guided  Adaptive Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yin%2C+M">Mingzhang Yin</a>, 
<a href="/search/stat?searchtype=author&query=Gao%2C+R">Ruijiang Gao</a>, 
<a href="/search/stat?searchtype=author&query=Lin%2C+W">Weiran Lin</a>, 
<a href="/search/stat?searchtype=author&query=Shugan%2C+S+M">Steven M. Shugan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">Designing products to meet consumers' preferences is essential for a
business's success. We propose the Gradient-based Survey (GBS), a discrete
choice experiment for multiattribute product design. The experiment elicits
consumer preferences through a sequence of paired comparisons for partial
profiles. GBS adaptively constructs paired comparison questions based on the
respondents' previous choices. Unlike the traditional random utility
maximization paradigm, GBS is robust to model misspecification by not requiring
a parametric utility model. Cross-pollinating the machine learning and
experiment design, GBS is scalable to products with hundreds of attributes and
can design personalized products for heterogeneous consumers. We demonstrate
the advantage of GBS in accuracy and sample efficiency compared to the existing
parametric and nonparametric methods in simulations.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12029" title="Abstract">arXiv:2310.12029</a> (cross-list from math.OC) [<a href="/pdf/2310.12029" title="Download PDF">pdf</a>, <a href="/format/2310.12029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Total variation regularization for recovering the spatial source term in  a time-fractional diffusion equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fan%2C+B">Bin Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper, we consider an inverse space-dependent source problem for a
time-fractional diffusion equation. To deal with the ill-posedness of the
problem, we transform the problem into an optimal control problem with total
variational (TV) regularization. In contrast to the classical Tikhonov model
incorporating $L^2$ penalty terms, the inclusion of a TV term proves
advantageous in reconstructing solutions that exhibit discontinuities or
piecewise constancy. The control problem is approximated by a fully discrete
scheme, and convergence results are provided within this framework.
Furthermore, a lineraed primal-dual iterative algorithm is proposed to solve
the discrete control model based on an equivalent saddle-point reformulation,
and several numerical experiments are presented to demonstrate the efficiency
of the algorithm.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12046" title="Abstract">arXiv:2310.12046</a> (cross-list from stat.ML) [<a href="/pdf/2310.12046" title="Download PDF">pdf</a>, <a href="/format/2310.12046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applications of ML-Based Surrogates in Bayesian Approaches to Inverse  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ersin%2C+P">Pelin Ersin</a>, 
<a href="/search/stat?searchtype=author&query=Hayes%2C+E">Emma Hayes</a>, 
<a href="/search/stat?searchtype=author&query=Matthews%2C+P">Peter Matthews</a>, 
<a href="/search/stat?searchtype=author&query=Mohapatra%2C+P">Paramjyoti Mohapatra</a>, 
<a href="/search/stat?searchtype=author&query=Negrini%2C+E">Elisa Negrini</a>, 
<a href="/search/stat?searchtype=author&query=Schulz%2C+K">Karl Schulz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, submitted to NeurIPS Workshop on Machine Learning for Physical Sciences
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural networks have become a powerful tool as surrogate models to provide
numerical solutions for scientific problems with increased computational
efficiency. This efficiency can be advantageous for numerically challenging
problems where time to solution is important or when evaluation of many similar
analysis scenarios is required. One particular area of scientific interest is
the setting of inverse problems, where one knows the forward dynamics of a
system are described by a partial differential equation and the task is to
infer properties of the system given (potentially noisy) observations of these
dynamics. We consider the inverse problem of inferring the location of a wave
source on a square domain, given a noisy solution to the 2-D acoustic wave
equation. Under the assumption of Gaussian noise, a likelihood function for
source location can be formulated, which requires one forward simulation of the
system per evaluation. Using a standard neural network as a surrogate model
makes it computationally feasible to evaluate this likelihood several times,
and so Markov Chain Monte Carlo methods can be used to evaluate the posterior
distribution of the source location. We demonstrate that this method can
accurately infer source-locations from noisy data.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12069" title="Abstract">arXiv:2310.12069</a> (cross-list from astro-ph.IM) [<a href="/pdf/2310.12069" title="Download PDF">pdf</a>, <a href="/format/2310.12069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers for scientific data: a pedagogical review for astronomers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Tanoglidis%2C+D">Dimitrios Tanoglidis</a>, 
<a href="/search/astro-ph?searchtype=author&query=Jain%2C+B">Bhuvnesh Jain</a>, 
<a href="/search/astro-ph?searchtype=author&query=Qu%2C+H">Helen Qu</a> (University of Pennsylvania)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The deep learning architecture associated with ChatGPT and related generative
AI products is known as transformers. Initially applied to Natural Language
Processing, transformers and the self-attention mechanism they exploit have
gained widespread interest across the natural sciences. The goal of this
pedagogical and informal review is to introduce transformers to scientists. Our
pedagogical and informal review includes the mathematics underlying the
attention mechanism, a description of the original transformer architecture,
and a section on applications to time series and imaging data in astronomy. We
include with a Frequently Asked Questions section for readers who are curious
about generative AI and interested in getting started with transformers for
their research problem.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12079" title="Abstract">arXiv:2310.12079</a> (cross-list from stat.ML) [<a href="/pdf/2310.12079" title="Download PDF">pdf</a>, <a href="/format/2310.12079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differential Equation Scaling Limits of Shaped and Unshaped Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+M+B">Mufan Bill Li</a>, 
<a href="/search/stat?searchtype=author&query=Nica%2C+M">Mihai Nica</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent analyses of neural networks with shaped activations (i.e. the
activation function is scaled as the network size grows) have led to scaling
limits described by differential equations. However, these results do not a
priori tell us anything about "ordinary" unshaped networks, where the
activation is unchanged as the network size grows. In this article, we find
similar differential equation based asymptotic characterization for two types
of unshaped networks.
<br />Firstly, we show that the following two architectures converge to the same
infinite-depth-and-width limit at initialization: (i) a fully connected ResNet
with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network
depth. (ii) a multilayer perceptron (MLP) with depth $d \ll$ width $n$ and
shaped ReLU activation at rate $d^{-1/2}$.
<br />Secondly, for an unshaped MLP at initialization, we derive the first order
asymptotic correction to the layerwise correlation. In particular, if
$\rho_\ell$ is the correlation at layer $\ell$, then $q_t = \ell^2 (1 -
\rho_\ell)$ with $t = \frac{\ell}{n}$ converges to an SDE with a singularity at
$t=0$.
<br />These results together provide a connection between shaped and unshaped
network architectures, and opens up the possibility of studying the effect of
normalization methods and how it connects with shaping activation functions.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12111" title="Abstract">arXiv:2310.12111</a> (cross-list from eess.AS) [<a href="/pdf/2310.12111" title="Download PDF">pdf</a>, <a href="/format/2310.12111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DASA: Difficulty-Aware Semantic Augmentation for Speaker Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yuanyuan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Z">Zhihan Yang</a>, 
<a href="/search/eess?searchtype=author&query=Wei%2C+T">Tao Wei</a>, 
<a href="/search/eess?searchtype=author&query=Zou%2C+K">Kun Zou</a>, 
<a href="/search/eess?searchtype=author&query=Meng%2C+H">Helen Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICASSP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Data augmentation is vital to the generalization ability and robustness of
deep neural networks (DNNs) models. Existing augmentation methods for speaker
verification manipulate the raw signal, which are time-consuming and the
augmented samples lack diversity. In this paper, we present a novel
difficulty-aware semantic augmentation (DASA) approach for speaker
verification, which can generate diversified training samples in speaker
embedding space with negligible extra computing cost. Firstly, we augment
training samples by perturbing speaker embeddings along semantic directions,
which are obtained from speaker-wise covariance matrices. Secondly, accurate
covariance matrices are estimated from robust speaker embeddings during
training, so we introduce difficultyaware additive margin softmax
(DAAM-Softmax) to obtain optimal speaker embeddings. Finally, we assume the
number of augmented samples goes to infinity and derive a closed-form upper
bound of the expected loss with DASA, which achieves compatibility and
efficiency. Extensive experiments demonstrate the proposed approach can achieve
a remarkable performance improvement. The best result achieves a 14.6% relative
reduction in EER metric on CN-Celeb evaluation set.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12115" title="Abstract">arXiv:2310.12115</a> (cross-list from stat.ML) [<a href="/pdf/2310.12115" title="Download PDF">pdf</a>, <a href="/format/2310.12115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMD-based Variable Importance for Distributional Random Forest
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=B%C3%A9nard%2C+C">Cl&#xe9;ment B&#xe9;nard</a>, 
<a href="/search/stat?searchtype=author&query=N%C3%A4f%2C+J">Jeffrey N&#xe4;f</a>, 
<a href="/search/stat?searchtype=author&query=Josse%2C+J">Julie Josse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Distributional Random Forest (DRF) is a flexible forest-based method to
estimate the full conditional distribution of a multivariate output of interest
given input variables. In this article, we introduce a variable importance
algorithm for DRFs, based on the well-established drop and relearn principle
and MMD distance. While traditional importance measures only detect variables
with an influence on the output mean, our algorithm detects variables impacting
the output distribution more generally. We show that the introduced importance
measure is consistent, exhibits high empirical performance on both real and
simulated data, and outperforms competitors. In particular, our algorithm is
highly efficient to select variables through recursive feature elimination, and
can therefore provide small sets of variables to build accurate estimates of
conditional output distributions.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Thu, 19 Oct 23</h3>
<dl>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1510.03826" title="Abstract">arXiv:1510.03826</a> (replaced) [<a href="/pdf/1510.03826" title="Download PDF">pdf</a>, <a href="/format/1510.03826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adopting Robustness and Optimality in Fitting and Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Oates%2C+T">Tim Oates</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+J">James Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1506.02690">arXiv:1506.02690</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.03867" title="Abstract">arXiv:2003.03867</a> (replaced) [<a href="/pdf/2003.03867" title="Download PDF">pdf</a>, <a href="/ps/2003.03867" title="Download PostScript">ps</a>, <a href="/format/2003.03867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Abilities of Asynchronous Agents: Semantic Side Effects and  How to Tame Them
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Penczek%2C+W">Wojciech Penczek</a>, 
<a href="/search/cs?searchtype=author&query=Sidoruk%2C+T">Teofil Sidoruk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.12412" title="Abstract">arXiv:2007.12412</a> (replaced) [<a href="/pdf/2007.12412" title="Download PDF">pdf</a>, <a href="/format/2007.12412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Checkers Are Cool: How to Model Check Voting Protocols in Uppaal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>, 
<a href="/search/cs?searchtype=author&query=Ryan%2C+P+Y+A">Peter Y. A. Ryan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.12424" title="Abstract">arXiv:2007.12424</a> (replaced) [<a href="/pdf/2007.12424" title="Download PDF">pdf</a>, <a href="/ps/2007.12424" title="Download PostScript">ps</a>, <a href="/format/2007.12424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Strategic Abilities in Voting Protocols
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>, 
<a href="/search/cs?searchtype=author&query=Malvone%2C+V">Vadim Malvone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.02968" title="Abstract">arXiv:2010.02968</a> (replaced) [<a href="/pdf/2010.02968" title="Download PDF">pdf</a>, <a href="/format/2010.02968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modelling of functional profiles and explainable shape shifts detection:  An approach combining the notion of the Fr&#xe9;chet mean with the shape  invariant model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Papayiannis%2C+G+I">Georgios I. Papayiannis</a>, 
<a href="/search/stat?searchtype=author&query=Psarakis%2C+S">Stelios Psarakis</a>, 
<a href="/search/stat?searchtype=author&query=Yannacopoulos%2C+A+N">Athanasios N. Yannacopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.07887" title="Abstract">arXiv:2011.07887</a> (replaced) [<a href="/pdf/2011.07887" title="Download PDF">pdf</a>, <a href="/ps/2011.07887" title="Download PostScript">ps</a>, <a href="/format/2011.07887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Requirements for COVID-19 Mitigation Strategies. Part I:  Newspaper Clips
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Mestel%2C+D">David Mestel</a>, 
<a href="/search/cs?searchtype=author&query=Roenne%2C+P+B">Peter B. Roenne</a>, 
<a href="/search/cs?searchtype=author&query=Ryan%2C+P+Y+A">Peter Y. A. Ryan</a>, 
<a href="/search/cs?searchtype=author&query=Skrobot%2C+M">Marjan Skrobot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.03436" title="Abstract">arXiv:2012.03436</a> (replaced) [<a href="/pdf/2012.03436" title="Download PDF">pdf</a>, <a href="/ps/2012.03436" title="Download PostScript">ps</a>, <a href="/format/2012.03436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank  Tensor Completion and Tensor Robust Principal Component Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jicong Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Lijun Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chengrun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Udell%2C+M">Madeleine Udell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published by Transactions on Machine Learning Research, January 2023; <a href="https://openreview.net/forum?id=Grhi800jVz">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research, January 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.03223" title="Abstract">arXiv:2103.03223</a> (replaced) [<a href="/pdf/2103.03223" title="Download PDF">pdf</a>, <a href="/format/2103.03223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Evaluation of Quantification Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schumacher%2C+T">Tobias Schumacher</a>, 
<a href="/search/cs?searchtype=author&query=Strohmaier%2C+M">Markus Strohmaier</a>, 
<a href="/search/cs?searchtype=author&query=Lemmerich%2C+F">Florian Lemmerich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 18 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.11806" title="Abstract">arXiv:2107.11806</a> (replaced) [<a href="/pdf/2107.11806" title="Download PDF">pdf</a>, <a href="/format/2107.11806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Bounds for the Randomized and Quantum Communication Complexities  of Equality with Small Error
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Lalonde%2C+O">Olivier Lalonde</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mande%2C+N+S">Nikhil S. Mande</a>, 
<a href="/search/quant-ph?searchtype=author&query=de+Wolf%2C+R">Ronald de Wolf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: Added some results and an author. 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.14432" title="Abstract">arXiv:2107.14432</a> (replaced) [<a href="/pdf/2107.14432" title="Download PDF">pdf</a>, <a href="/format/2107.14432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yun Yue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yongchao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+S">Suo Tong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Chunyang Wen</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Huanjun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+L">Lihong Gu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yixiang Mu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages. Published as a conference paper at ECML PKDD 2021. This version includes Appendix which was not included in the published version because of page limit
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Machine Learning and Knowledge Discovery in Databases. Research
  Track - European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17,
  2021, Proceedings, Part III
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.08078" title="Abstract">arXiv:2201.08078</a> (replaced) [<a href="/pdf/2201.08078" title="Download PDF">pdf</a>, <a href="/format/2201.08078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Maximization Bias in Reinforcement Learning with Two-Sample  Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waltz%2C+M">Martin Waltz</a>, 
<a href="/search/cs?searchtype=author&query=Okhrin%2C+O">Ostap Okhrin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.12016" title="Abstract">arXiv:2202.12016</a> (replaced) [<a href="/pdf/2202.12016" title="Download PDF">pdf</a>, <a href="/format/2202.12016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Abstraction for Model Checking of Multi-Agent Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yan Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.03341" title="Abstract">arXiv:2203.03341</a> (replaced) [<a href="/pdf/2203.03341" title="Download PDF">pdf</a>, <a href="/format/2203.03341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recovering single precision accuracy from Tensor Cores while surpassing  the FP32 theoretical peak performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ootomo%2C+H">Hiroyuki Ootomo</a>, 
<a href="/search/cs?searchtype=author&query=Yokota%2C+R">Rio Yokota</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.13692" title="Abstract">arXiv:2203.13692</a> (replaced) [<a href="/pdf/2203.13692" title="Download PDF">pdf</a>, <a href="/ps/2203.13692" title="Download PostScript">ps</a>, <a href="/format/2203.13692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bisimulations for Verifying Strategic Abilities with an Application to  the ThreeBallot Voting Protocol
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belardinelli%2C+F">Francesco Belardinelli</a>, 
<a href="/search/cs?searchtype=author&query=Condurache%2C+R">Rodica Condurache</a>, 
<a href="/search/cs?searchtype=author&query=Dima%2C+C">Catalin Dima</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Knapik%2C+M">Michal Knapik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.17168" title="Abstract">arXiv:2203.17168</a> (replaced) [<a href="/pdf/2203.17168" title="Download PDF">pdf</a>, <a href="/ps/2203.17168" title="Download PostScript">ps</a>, <a href="/format/2203.17168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lower bounds for uniform read-once threshold formulae in the randomized  decision tree model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leonardos%2C+N">Nikos Leonardos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added a remark
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.11210" title="Abstract">arXiv:2204.11210</a> (replaced) [<a href="/pdf/2204.11210" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COVID-Net Biochem: An Explainability-driven Framework to Building  Machine Learning Models for Predicting Survival and Kidney Injury of COVID-19  Patients from Clinical and Biochemistry Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aboutalebi%2C+H">Hossein Aboutalebi</a>, 
<a href="/search/cs?searchtype=author&query=Pavlova%2C+M">Maya Pavlova</a>, 
<a href="/search/cs?searchtype=author&query=Shafiee%2C+M+J">Mohammad Javad Shafiee</a>, 
<a href="/search/cs?searchtype=author&query=Florea%2C+A">Adrian Florea</a>, 
<a href="/search/cs?searchtype=author&query=Hryniowski%2C+A">Andrew Hryniowski</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+A">Alexander Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.08005" title="Abstract">arXiv:2206.08005</a> (replaced) [<a href="/pdf/2206.08005" title="Download PDF">pdf</a>, <a href="/format/2206.08005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Self-Supervised Learning for Molecular Graph Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kaddour%2C+J">Jean Kaddour</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengchao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Lasenby%2C+J">Joan Lasenby</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready, NeurIPS Benchmark 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.10675" title="Abstract">arXiv:2206.10675</a> (replaced) [<a href="/pdf/2206.10675" title="Download PDF">pdf</a>, <a href="/format/2206.10675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orthogonal dissection into few rectangles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eppstein%2C+D">David Eppstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures. This version adds results on dissection with rotations and reflections
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.13181" title="Abstract">arXiv:2207.13181</a> (replaced) [<a href="/pdf/2207.13181" title="Download PDF">pdf</a>, <a href="/format/2207.13181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Planning and Learning: Path-Planning for Autonomous Vehicles, a Review  of the Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osanlou%2C+K">Kevin Osanlou</a>, 
<a href="/search/cs?searchtype=author&query=Guettier%2C+C">Christophe Guettier</a>, 
<a href="/search/cs?searchtype=author&query=Cazenave%2C+T">Tristan Cazenave</a>, 
<a href="/search/cs?searchtype=author&query=Jacopin%2C+E">Eric Jacopin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI-format &amp; updated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12550" title="Abstract">arXiv:2208.12550</a> (replaced) [<a href="/pdf/2208.12550" title="Download PDF">pdf</a>, <a href="/format/2208.12550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training and Tuning Generative Neural Radiance Fields for  Attribute-Conditional 3D-Aware Face Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jichao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Siarohin%2C+A">Aliaksandr Siarohin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yahui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Sebe%2C+N">Nicu Sebe</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12744" title="Abstract">arXiv:2208.12744</a> (replaced) [<a href="/pdf/2208.12744" title="Download PDF">pdf</a>, <a href="/format/2208.12744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implementing quantum dimensionality reduction for non-Markovian  stochastic simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+K">Kang-Da Wu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yang%2C+C">Chengran Yang</a>, 
<a href="/search/quant-ph?searchtype=author&query=He%2C+R">Ren-Dong He</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gu%2C+M">Mile Gu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Xiang%2C+G">Guo-Yong Xiang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+C">Chuan-Feng Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Guo%2C+G">Guang-Can Guo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Elliott%2C+T+J">Thomas J. Elliott</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11+2 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Nature Communications 14, 2624 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Information Theory (cs.IT); Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09035" title="Abstract">arXiv:2209.09035</a> (replaced) [<a href="/pdf/2209.09035" title="Download PDF">pdf</a>, <a href="/format/2209.09035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness in Face Presentation Attack Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meiling Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wufei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kuijper%2C+A">Arjan Kuijper</a>, 
<a href="/search/cs?searchtype=author&query=Struc%2C+V">Vitomir Struc</a>, 
<a href="/search/cs?searchtype=author&query=Damer%2C+N">Naser Damer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Pattern Recognition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.11882" title="Abstract">arXiv:2209.11882</a> (replaced) [<a href="/pdf/2209.11882" title="Download PDF">pdf</a>, <a href="/ps/2209.11882" title="Download PostScript">ps</a>, <a href="/format/2209.11882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logarithmically larger deletion codes of all distances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alon%2C+N">Noga Alon</a>, 
<a href="/search/math?searchtype=author&query=Bourla%2C+G">Gabriela Bourla</a>, 
<a href="/search/math?searchtype=author&query=Graham%2C+B">Ben Graham</a>, 
<a href="/search/math?searchtype=author&query=He%2C+X">Xiaoyu He</a>, 
<a href="/search/math?searchtype=author&query=Kravitz%2C+N">Noah Kravitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.13877" title="Abstract">arXiv:2209.13877</a> (replaced) [<a href="/pdf/2209.13877" title="Download PDF">pdf</a>, <a href="/format/2209.13877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YATO: Yet Another deep learning based Text analysis Open toolkit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zeqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiageng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Z">Zhiyang Teng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03350" title="Abstract">arXiv:2210.03350</a> (replaced) [<a href="/pdf/2210.03350" title="Download PDF">pdf</a>, <a href="/format/2210.03350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring and Narrowing the Compositionality Gap in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Press%2C+O">Ofir Press</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+S">Sewon Min</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+L">Ludwig Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Mike Lewis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.06624" title="Abstract">arXiv:2210.06624</a> (replaced) [<a href="/pdf/2210.06624" title="Download PDF">pdf</a>, <a href="/ps/2210.06624" title="Download PostScript">ps</a>, <a href="/format/2210.06624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Discrete Entropy Monotonicity for Log-Concave Sums
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gavalakis%2C+L">Lampros Gavalakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, no figures. A number of typos have been fixed and reviewers' comments have been incorporated. More details have been added in most of the proofs. To appear in Combinatorics, Probability and Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Information Theory (cs.IT); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.10487" title="Abstract">arXiv:2210.10487</a> (replaced) [<a href="/pdf/2210.10487" title="Download PDF">pdf</a>, <a href="/format/2210.10487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating the Contamination Factor&#x27;s Distribution in Unsupervised  Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perini%2C+L">Lorenzo Perini</a>, 
<a href="/search/cs?searchtype=author&query=Buerkner%2C+P">Paul Buerkner</a>, 
<a href="/search/cs?searchtype=author&query=Klami%2C+A">Arto Klami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.10694" title="Abstract">arXiv:2210.10694</a> (replaced) [<a href="/pdf/2210.10694" title="Download PDF">pdf</a>, <a href="/format/2210.10694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verification of the Socio-Technical Aspects of Voting: The Case of the  Polish Postal Vote 2020
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Ryan%2C+P+Y+A">Peter Y.A. Ryan</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yan Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15237" title="Abstract">arXiv:2210.15237</a> (replaced) [<a href="/pdf/2210.15237" title="Download PDF">pdf</a>, <a href="/format/2210.15237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lee%2C+J">Ju-Hyung Lee</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+D">Dong-Ho Lee</a>, 
<a href="/search/eess?searchtype=author&query=Sheen%2C+E">Eunsoo Sheen</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+T">Thomas Choi</a>, 
<a href="/search/eess?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ASILOMAR 2023 conference, 4 pages, 2 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.02176" title="Abstract">arXiv:2211.02176</a> (replaced) [<a href="/pdf/2211.02176" title="Download PDF">pdf</a>, <a href="/format/2211.02176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connected k-Center and k-Diameter Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Drexler%2C+L">Lukas Drexler</a>, 
<a href="/search/cs?searchtype=author&query=Eube%2C+J">Jan Eube</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kelin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Reineccius%2C+D">Dorian Reineccius</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6glin%2C+H">Heiko R&#xf6;glin</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+M">Melanie Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Wargalla%2C+J">Julian Wargalla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.02681" title="Abstract">arXiv:2211.02681</a> (replaced) [<a href="/pdf/2211.02681" title="Download PDF">pdf</a>, <a href="/ps/2211.02681" title="Download PostScript">ps</a>, <a href="/format/2211.02681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Distance Sensitivity Oracles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+D">Davin Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Gunby-Mann%2C+A">Allison Gunby-Mann</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S">Sarel Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Katzmann%2C+M">Maximilian Katzmann</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+C">Chau Pham</a>, 
<a href="/search/cs?searchtype=author&query=Bhakta%2C+A">Arnav Bhakta</a>, 
<a href="/search/cs?searchtype=author&query=Friedrich%2C+T">Tobias Friedrich</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+S">Sang Chin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2007.11495">arXiv:2007.11495</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06206" title="Abstract">arXiv:2211.06206</a> (replaced) [<a href="/pdf/2211.06206" title="Download PDF">pdf</a>, <a href="/format/2211.06206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical error estimates for computing the matrix logarithm by  Pad&#xe9;-type approximants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aceto%2C+L">Lidia Aceto</a>, 
<a href="/search/math?searchtype=author&query=Durastante%2C+F">Fabio Durastante</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09299" title="Abstract">arXiv:2211.09299</a> (replaced) [<a href="/pdf/2211.09299" title="Download PDF">pdf</a>, <a href="/format/2211.09299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedFA: Federated Learning with Feature Anchors to Align Features and  Classifiers for Heterogeneous Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tailin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+D+H+K">Danny H.K. Tsang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Mobile Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09925" title="Abstract">arXiv:2211.09925</a> (replaced) [<a href="/pdf/2211.09925" title="Download PDF">pdf</a>, <a href="/format/2211.09925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairMILE: Towards an Efficient Framework for Fair Graph Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuntian He</a>, 
<a href="/search/cs?searchtype=author&query=Gurukar%2C+S">Saket Gurukar</a>, 
<a href="/search/cs?searchtype=author&query=Parthasarathy%2C+S">Srinivasan Parthasarathy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11031" title="Abstract">arXiv:2211.11031</a> (replaced) [<a href="/pdf/2211.11031" title="Download PDF">pdf</a>, <a href="/format/2211.11031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aging with GRACE: Lifelong Model Editing with Discrete Key-Value  Adaptors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hartvigsen%2C+T">Thomas Hartvigsen</a>, 
<a href="/search/cs?searchtype=author&query=Sankaranarayanan%2C+S">Swami Sankaranarayanan</a>, 
<a href="/search/cs?searchtype=author&query=Palangi%2C+H">Hamid Palangi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ghassemi%2C+M">Marzyeh Ghassemi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13696" title="Abstract">arXiv:2211.13696</a> (replaced) [<a href="/pdf/2211.13696" title="Download PDF">pdf</a>, <a href="/format/2211.13696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPT: a Fixed-Point Accelerator for Torus Fully Homomorphic Encryption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Beirendonck%2C+M">Michiel Van Beirendonck</a>, 
<a href="/search/cs?searchtype=author&query=D%27Anvers%2C+J">Jan-Pieter D&#x27;Anvers</a>, 
<a href="/search/cs?searchtype=author&query=Turan%2C+F">Furkan Turan</a>, 
<a href="/search/cs?searchtype=author&query=Verbauwhede%2C+I">Ingrid Verbauwhede</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM CCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14603" title="Abstract">arXiv:2211.14603</a> (replaced) [<a href="/pdf/2211.14603" title="Download PDF">pdf</a>, <a href="/ps/2211.14603" title="Download PostScript">ps</a>, <a href="/format/2211.14603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Molecule Harvesting by Heterogeneous Receptors on MC  Transmitters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+M">Miaowen Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Schober%2C+R">Robert Schober</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures. This work has been accepted by IEEE GLOBECOM 2023. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10764" title="Abstract">arXiv:2212.10764</a> (replaced) [<a href="/pdf/2212.10764" title="Download PDF">pdf</a>, <a href="/format/2212.10764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning List-Level Domain-Invariant Representations for Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xian%2C+R">Ruicheng Xian</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+H">Honglei Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zamani%2C+H">Hamed Zamani</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Ji Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+K">Kai Hui</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuanhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.13643" title="Abstract">arXiv:2212.13643</a> (replaced) [<a href="/pdf/2212.13643" title="Download PDF">pdf</a>, <a href="/format/2212.13643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Business Users&#x27; Data-Driven Decision-Making: Practices,  Challenges, and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gathani%2C+S">Sneha Gathani</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Haas%2C+P+J">Peter J. Haas</a>, 
<a href="/search/cs?searchtype=author&query=Demiralp%2C+%C3%87">&#xc7;a&#x11f;atay Demiralp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE TVCG
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02432" title="Abstract">arXiv:2301.02432</a> (replaced) [<a href="/pdf/2301.02432" title="Download PDF">pdf</a>, <a href="/format/2301.02432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Myths and Legends in High-Performance Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matsuoka%2C+S">Satoshi Matsuoka</a>, 
<a href="/search/cs?searchtype=author&query=Domke%2C+J">Jens Domke</a>, 
<a href="/search/cs?searchtype=author&query=Wahib%2C+M">Mohamed Wahib</a>, 
<a href="/search/cs?searchtype=author&query=Drozd%2C+A">Aleksandr Drozd</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR); Computers and Society (cs.CY); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03560" title="Abstract">arXiv:2301.03560</a> (replaced) [<a href="/pdf/2301.03560" title="Download PDF">pdf</a>, <a href="/format/2301.03560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solo: Data Discovery Using Natural Language Questions Via A  Self-Supervised Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+R+C">Raul Castro Fernandez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at Sigmod 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11241" title="Abstract">arXiv:2301.11241</a> (replaced) [<a href="/pdf/2301.11241" title="Download PDF">pdf</a>, <a href="/format/2301.11241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Convergence of No-Regret Learning Dynamics in Time-Varying Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anagnostides%2C+I">Ioannis Anagnostides</a>, 
<a href="/search/cs?searchtype=author&query=Panageas%2C+I">Ioannis Panageas</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NeurIPS 2023; V3 incorporates reviewers' feedback and minor corrections
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12321" title="Abstract">arXiv:2301.12321</a> (replaced) [<a href="/pdf/2301.12321" title="Download PDF">pdf</a>, <a href="/format/2301.12321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Relation Graph: A Unified Framework for Identifying Label Noise  and Outlier Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jang-Hyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H+O">Hyun Oh Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00837" title="Abstract">arXiv:2302.00837</a> (replaced) [<a href="/pdf/2302.00837" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SHINE: Deep Learning-Based Accessible Parking Management System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neupane%2C+D">Dhiraj Neupane</a>, 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+A">Aashish Bhattarai</a>, 
<a href="/search/cs?searchtype=author&query=Aryal%2C+S">Sunil Aryal</a>, 
<a href="/search/cs?searchtype=author&query=Bouadjenek%2C+M+R">Mohamed Reda Bouadjenek</a>, 
<a href="/search/cs?searchtype=author&query=Seok%2C+U">Uk-Min Seok</a>, 
<a href="/search/cs?searchtype=author&query=Seok%2C+J">Jongwon Seok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02550" title="Abstract">arXiv:2302.02550</a> (replaced) [<a href="/pdf/2302.02550" title="Download PDF">pdf</a>, <a href="/format/2302.02550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Re-Modulation for Few-Shot Generative Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Heliang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shanshan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bin Li</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03121" title="Abstract">arXiv:2302.03121</a> (replaced) [<a href="/pdf/2302.03121" title="Download PDF">pdf</a>, <a href="/format/2302.03121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Value Distributions of Perfect Nonlinear Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=K%C3%B6lsch%2C+L">Lukas K&#xf6;lsch</a>, 
<a href="/search/math?searchtype=author&query=Polujan%2C+A">Alexandr Polujan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages. minor revisions of the previous version. The paper is now identical to the published version, outside of formatting
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> K\"olsch, L., Polujan, A. Value Distributions of Perfect Nonlinear
  Functions. Combinatorica (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03672" title="Abstract">arXiv:2302.03672</a> (replaced) [<a href="/pdf/2302.03672" title="Download PDF">pdf</a>, <a href="/ps/2302.03672" title="Download PostScript">ps</a>, <a href="/format/2302.03672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proportionality in Approval-Based Participatory Budgeting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brill%2C+M">Markus Brill</a>, 
<a href="/search/cs?searchtype=author&query=Forster%2C+S">Stefan Forster</a>, 
<a href="/search/cs?searchtype=author&query=Lackner%2C+M">Martin Lackner</a>, 
<a href="/search/cs?searchtype=author&query=Maly%2C+J">Jan Maly</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+J">Jannik Peters</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04813" title="Abstract">arXiv:2302.04813</a> (replaced) [<a href="/pdf/2302.04813" title="Download PDF">pdf</a>, <a href="/format/2302.04813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explanation Selection Using Unlabeled Data for Chain-of-Thought  Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Durrett%2C+G">Greg Durrett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05682" title="Abstract">arXiv:2302.05682</a> (replaced) [<a href="/pdf/2302.05682" title="Download PDF">pdf</a>, <a href="/format/2302.05682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maintaining Discrete Probability Distributions in Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allendorf%2C+D">Daniel Allendorf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ALENEX 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07684" title="Abstract">arXiv:2302.07684</a> (replaced) [<a href="/pdf/2302.07684" title="Download PDF">pdf</a>, <a href="/format/2302.07684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Federated Learning Benchmark for Drug-Target Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittone%2C+G">Gianluca Mittone</a>, 
<a href="/search/cs?searchtype=author&query=Svoboda%2C+F">Filip Svoboda</a>, 
<a href="/search/cs?searchtype=author&query=Aldinucci%2C+M">Marco Aldinucci</a>, 
<a href="/search/cs?searchtype=author&query=Lane%2C+N+D">Nicholas D. Lane</a>, 
<a href="/search/cs?searchtype=author&query=Lio%2C+P">Pietro Lio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is the accepted version of ACM copyrighted material published at the WWW'23 conference
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Companion Proceedings of the ACM Web Conference 2023 (pp.
  1177-1181)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07946" title="Abstract">arXiv:2302.07946</a> (replaced) [<a href="/pdf/2302.07946" title="Download PDF">pdf</a>, <a href="/format/2302.07946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimenting with Emerging RISC-V Systems for Decentralised Machine  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittone%2C+G">Gianluca Mittone</a>, 
<a href="/search/cs?searchtype=author&query=Tonci%2C+N">Nicol&#xf2; Tonci</a>, 
<a href="/search/cs?searchtype=author&query=Birke%2C+R">Robert Birke</a>, 
<a href="/search/cs?searchtype=author&query=Colonnelli%2C+I">Iacopo Colonnelli</a>, 
<a href="/search/cs?searchtype=author&query=Medi%C4%87%2C+D">Doriana Medi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Bartolini%2C+A">Andrea Bartolini</a>, 
<a href="/search/cs?searchtype=author&query=Esposito%2C+R">Roberto Esposito</a>, 
<a href="/search/cs?searchtype=author&query=Parisi%2C+E">Emanuele Parisi</a>, 
<a href="/search/cs?searchtype=author&query=Beneventi%2C+F">Francesco Beneventi</a>, 
<a href="/search/cs?searchtype=author&query=Polato%2C+M">Mirko Polato</a>, 
<a href="/search/cs?searchtype=author&query=Torquati%2C+M">Massimo Torquati</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>, 
<a href="/search/cs?searchtype=author&query=Aldinucci%2C+M">Marco Aldinucci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is the accepted version of ACM copyrighted material presented at the CF'23 conference in Bologna, Italy
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the 20th ACM International Conference on
  Computing Frontiers 2023 (CF '23), ACM, New York, NY, USA, 73-83
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08243" title="Abstract">arXiv:2302.08243</a> (replaced) [<a href="/pdf/2302.08243" title="Download PDF">pdf</a>, <a href="/format/2302.08243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Insights on Relieving Task-Recency Bias for Online Class Incremental  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+G">Guoqiang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaojie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaoqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shiyu Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanning Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages,16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08357" title="Abstract">arXiv:2302.08357</a> (replaced) [<a href="/pdf/2302.08357" title="Download PDF">pdf</a>, <a href="/format/2302.08357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boundary Guided Learning-Free Semantic Control with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Ye Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Russakovsky%2C+O">Olga Russakovsky</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yan Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. 27 pages including appendices, code at <a href="https://github.com/L-YeZhu/BoundaryDiffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08466" title="Abstract">arXiv:2302.08466</a> (replaced) [<a href="/pdf/2302.08466" title="Download PDF">pdf</a>, <a href="/format/2302.08466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Marich: A Query-efficient Distributionally Equivalent Model Extraction  Attack using Public Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karmakar%2C+P">Pratik Karmakar</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+D">Debabrota Basu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented in the Privacy-Preserving AI (PPAI) workshop at AAAI 2023 as a spotlight talk and published in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09473" title="Abstract">arXiv:2302.09473</a> (replaced) [<a href="/pdf/2302.09473" title="Download PDF">pdf</a>, <a href="/format/2302.09473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-Text Retrieval by Supervised Sparse Multi-Grained Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yimu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+P">Peng Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11649" title="Abstract">arXiv:2302.11649</a> (replaced) [<a href="/pdf/2302.11649" title="Download PDF">pdf</a>, <a href="/format/2302.11649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grounding Complex Natural Language Commands for Temporal Tasks in Unseen  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J+X">Jason Xinyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Idrees%2C+I">Ifrah Idrees</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Sam Liang</a>, 
<a href="/search/cs?searchtype=author&query=Schornstein%2C+B">Benjamin Schornstein</a>, 
<a href="/search/cs?searchtype=author&query=Tellex%2C+S">Stefanie Tellex</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Ankit Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Robot Learning 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14182" title="Abstract">arXiv:2302.14182</a> (replaced) [<a href="/pdf/2302.14182" title="Download PDF">pdf</a>, <a href="/format/2302.14182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taylor TD-learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garibbo%2C+M">Michele Garibbo</a>, 
<a href="/search/cs?searchtype=author&query=Robeyns%2C+M">Maxime Robeyns</a>, 
<a href="/search/cs?searchtype=author&query=Aitchison%2C+L">Laurence Aitchison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00067" title="Abstract">arXiv:2303.00067</a> (replaced) [<a href="/pdf/2303.00067" title="Download PDF">pdf</a>, <a href="/ps/2303.00067" title="Download PostScript">ps</a>, <a href="/format/2303.00067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Playing to Learn, or to Keep Secret: Alternating-Time Logic Meets  Information Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tabatabaei%2C+M">Masoud Tabatabaei</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00231" title="Abstract">arXiv:2303.00231</a> (replaced) [<a href="/pdf/2303.00231" title="Download PDF">pdf</a>, <a href="/ps/2303.00231" title="Download PostScript">ps</a>, <a href="/format/2303.00231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polyhedral Clinching Auctions for Indivisible Goods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hirai%2C+H">Hiroshi Hirai</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+R">Ryosuke Sato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01580" title="Abstract">arXiv:2303.01580</a> (replaced) [<a href="/pdf/2303.01580" title="Download PDF">pdf</a>, <a href="/format/2303.01580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture of Soft Prompts for Controllable Data Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Derek Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Celine Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yunan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Rosati%2C+D">Domenic Rosati</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 13 Tables, 2 Figures. Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04906" title="Abstract">arXiv:2303.04906</a> (replaced) [<a href="/pdf/2303.04906" title="Download PDF">pdf</a>, <a href="/format/2303.04906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Agnostic Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittone%2C+G">Gianluca Mittone</a>, 
<a href="/search/cs?searchtype=author&query=Riviera%2C+W">Walter Riviera</a>, 
<a href="/search/cs?searchtype=author&query=Colonnelli%2C+I">Iacopo Colonnelli</a>, 
<a href="/search/cs?searchtype=author&query=Birke%2C+R">Robert Birke</a>, 
<a href="/search/cs?searchtype=author&query=Aldinucci%2C+M">Marco Aldinucci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at the EuroPar'23 conference, Limassol, Cyprus
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Euro-Par 2023: Parallel Processing. Euro-Par 2023. Lecture
  Notes in Computer Science, vol 14100. Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05169" title="Abstract">arXiv:2303.05169</a> (replaced) [<a href="/pdf/2303.05169" title="Download PDF">pdf</a>, <a href="/format/2303.05169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing the Hubble parameter with future Gravitational Wave  missions using Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Mukherjee%2C+P">Purba Mukherjee</a>, 
<a href="/search/astro-ph?searchtype=author&query=Shah%2C+R">Rahul Shah</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bhaumik%2C+A">Arko Bhaumik</a>, 
<a href="/search/astro-ph?searchtype=author&query=Pal%2C+S">Supratik Pal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 sets of figures, Accepted in ApJ
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); General Relativity and Quantum Cosmology (gr-qc)

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06275" title="Abstract">arXiv:2303.06275</a> (replaced) [<a href="/pdf/2303.06275" title="Download PDF">pdf</a>, <a href="/format/2303.06275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Study of Joint Representation Learning on Protein Sequences  and Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Z">Zuobai Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+C">Chuanrui Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Xu%2C+M">Minghao Xu</a>, 
<a href="/search/q-bio?searchtype=author&query=Chenthamarakshan%2C+V">Vijil Chenthamarakshan</a>, 
<a href="/search/q-bio?searchtype=author&query=Lozano%2C+A">Aur&#xe9;lie Lozano</a>, 
<a href="/search/q-bio?searchtype=author&query=Das%2C+P">Payel Das</a>, 
<a href="/search/q-bio?searchtype=author&query=Tang%2C+J">Jian Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07759" title="Abstract">arXiv:2303.07759</a> (replaced) [<a href="/pdf/2303.07759" title="Download PDF">pdf</a>, <a href="/format/2303.07759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Baseline for Supervised Surround-view Depth Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xianda Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Wenjie Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunpeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10073" title="Abstract">arXiv:2303.10073</a> (replaced) [<a href="/pdf/2303.10073" title="Download PDF">pdf</a>, <a href="/format/2303.10073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DialogPaint: A Dialog-based Image Editing Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jingxuan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shiyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yequan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12237" title="Abstract">arXiv:2303.12237</a> (replaced) [<a href="/pdf/2303.12237" title="Download PDF">pdf</a>, <a href="/format/2303.12237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated deep learning segmentation of high-resolution 7 T postmortem  MRI for quantitative analysis of structure-pathology correlations in  neurodegenerative diseases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+P">Pulkit Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Duong%2C+M+T">Michael Tran Duong</a>, 
<a href="/search/cs?searchtype=author&query=Sadaghiani%2C+S">Shokufeh Sadaghiani</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+S">Sydney Lim</a>, 
<a href="/search/cs?searchtype=author&query=Denning%2C+A">Amanda Denning</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+E">Eunice Chung</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+S">Sadhana Ravikumar</a>, 
<a href="/search/cs?searchtype=author&query=Arezoumandan%2C+S">Sanaz Arezoumandan</a>, 
<a href="/search/cs?searchtype=author&query=Peterson%2C+C">Claire Peterson</a>, 
<a href="/search/cs?searchtype=author&query=Bedard%2C+M">Madigan Bedard</a>, 
<a href="/search/cs?searchtype=author&query=Capp%2C+N">Noah Capp</a>, 
<a href="/search/cs?searchtype=author&query=Ittyerah%2C+R">Ranjit Ittyerah</a>, 
<a href="/search/cs?searchtype=author&query=Migdal%2C+E">Elyse Migdal</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+G">Grace Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kopp%2C+E">Emily Kopp</a>, 
<a href="/search/cs?searchtype=author&query=Loja%2C+B">Bridget Loja</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+E">Eusha Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiacheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Bahena%2C+A">Alejandra Bahena</a>, 
<a href="/search/cs?searchtype=author&query=Prabhakaran%2C+K">Karthik Prabhakaran</a>, 
<a href="/search/cs?searchtype=author&query=Mizsei%2C+G">Gabor Mizsei</a>, 
<a href="/search/cs?searchtype=author&query=Gabrielyan%2C+M">Marianna Gabrielyan</a>, 
<a href="/search/cs?searchtype=author&query=Schuck%2C+T">Theresa Schuck</a>, 
<a href="/search/cs?searchtype=author&query=Trotman%2C+W">Winifred Trotman</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+J">John Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Ohm%2C+D">Daniel Ohm</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+E+B">Edward B. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Trojanowski%2C+J+Q">John Q. Trojanowski</a>, 
<a href="/search/cs?searchtype=author&query=McMillan%2C+C">Corey McMillan</a>, 
<a href="/search/cs?searchtype=author&query=Grossman%2C+M">Murray Grossman</a>, 
<a href="/search/cs?searchtype=author&query=Irwin%2C+D+J">David J. Irwin</a>, 
<a href="/search/cs?searchtype=author&query=Detre%2C+J">John Detre</a>, 
<a href="/search/cs?searchtype=author&query=Tisdall%2C+M+D">M. Dylan Tisdall</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S+R">Sandhitsu R. Das</a>, 
<a href="/search/cs?searchtype=author&query=Wisse%2C+L+E+M">Laura E.M. Wisse</a>, 
<a href="/search/cs?searchtype=author&query=Wolk%2C+D+A">David A. Wolk</a>, 
<a href="/search/cs?searchtype=author&query=Yushkevich%2C+P+A">Paul A. Yushkevich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to NeuroImage Project website: <a href="https://pulkit-khandelwal.github.io/exvivo-brain-upenn">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13408" title="Abstract">arXiv:2303.13408</a> (replaced) [<a href="/pdf/2303.13408" title="Download PDF">pdf</a>, <a href="/format/2303.13408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Paraphrasing evades detectors of AI-generated text, but retrieval is an  effective defense
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krishna%2C+K">Kalpesh Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yixiao Song</a>, 
<a href="/search/cs?searchtype=author&query=Karpinska%2C+M">Marzena Karpinska</a>, 
<a href="/search/cs?searchtype=author&query=Wieting%2C+J">John Wieting</a>, 
<a href="/search/cs?searchtype=author&query=Iyyer%2C+M">Mohit Iyyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera ready (32 pages). Code, models, data available in <a href="https://github.com/martiansideofthemoon/ai-detection-paraphrases">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13974" title="Abstract">arXiv:2303.13974</a> (replaced) [<a href="/e-print/2303.13974" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shukla%2C+N">Nitish Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+A">Anurima Dey</a>, 
<a href="/search/cs?searchtype=author&query=K%2C+S">Srivatsan K</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Study is not relevant
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16913" title="Abstract">arXiv:2303.16913</a> (replaced) [<a href="/pdf/2303.16913" title="Download PDF">pdf</a>, <a href="/format/2303.16913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Reconfigurable Intelligent Surfaces for Short Transmissions:  How Detailed Configurations can be Afforded?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Enqvist%2C+A">Anders Enqvist</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+%C3%96+T">&#xd6;zlem Tu&#x11f;fe Demir</a>, 
<a href="/search/cs?searchtype=author&query=Cavdar%2C+C">Cicek Cavdar</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rnson%2C+E">Emil Bj&#xf6;rnson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures. This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. arXiv admin note: text overlap with <a href="/abs/2303.16625">arXiv:2303.16625</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00188" title="Abstract">arXiv:2304.00188</a> (replaced) [<a href="/pdf/2304.00188" title="Download PDF">pdf</a>, <a href="/format/2304.00188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Influence of the Geometry of the world model on Curiosity Based  Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sergeant-Perthuis%2C+G">Gr&#xe9;goire Sergeant-Perthuis</a>, 
<a href="/search/cs?searchtype=author&query=Ruet%2C+N">Nils Ruet</a>, 
<a href="/search/cs?searchtype=author&query=Rudrauf%2C+D">David Rudrauf</a>, 
<a href="/search/cs?searchtype=author&query=Ognibene%2C+D">Dimitri Ognibene</a>, 
<a href="/search/cs?searchtype=author&query=Tisserand%2C+Y">Yvain Tisserand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Systems and Control (eess.SY); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05071" title="Abstract">arXiv:2304.05071</a> (replaced) [<a href="/pdf/2304.05071" title="Download PDF">pdf</a>, <a href="/format/2304.05071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fracture Detection in Pediatric Wrist Trauma X-ray Images Using YOLOv8  Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+R">Rui-Yang Ju</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+W">Weiming Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10532" title="Abstract">arXiv:2304.10532</a> (replaced) [<a href="/pdf/2304.10532" title="Download PDF">pdf</a>, <a href="/format/2304.10532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Warburg%2C+F">Frederik Warburg</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+E">Ethan Weber</a>, 
<a href="/search/cs?searchtype=author&query=Tancik%2C+M">Matthew Tancik</a>, 
<a href="/search/cs?searchtype=author&query=Holynski%2C+A">Aleksander Holynski</a>, 
<a href="/search/cs?searchtype=author&query=Kanazawa%2C+A">Angjoo Kanazawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023, project page: <a href="https://ethanweber.me/nerfbusters">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10737" title="Abstract">arXiv:2304.10737</a> (replaced) [<a href="/pdf/2304.10737" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Schooling to Exploit Foolish Contracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdelaziz%2C+T">Tamer Abdelaziz</a>, 
<a href="/search/cs?searchtype=author&query=Hobor%2C+A">Aquinas Hobor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11029" title="Abstract">arXiv:2304.11029</a> (replaced) [<a href="/pdf/2304.11029" title="Download PDF">pdf</a>, <a href="/format/2304.11029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic  Music Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shangda Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dingyao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures, 5 tables, accepted by ISMIR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Information Retrieval (cs.IR); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11060" title="Abstract">arXiv:2304.11060</a> (replaced) [<a href="/pdf/2304.11060" title="Download PDF">pdf</a>, <a href="/format/2304.11060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SkillGPT: a RESTful API service for skill extraction and standardization  using a Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nan Li</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+B">Bo Kang</a>, 
<a href="/search/cs?searchtype=author&query=De+Bie%2C+T">Tijl De Bie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13007" title="Abstract">arXiv:2304.13007</a> (replaced) [<a href="/pdf/2304.13007" title="Download PDF">pdf</a>, <a href="/format/2304.13007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Answering Questions by Meta-Reasoning over Multiple Chains of Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoran%2C+O">Ori Yoran</a>, 
<a href="/search/cs?searchtype=author&query=Wolfson%2C+T">Tomer Wolfson</a>, 
<a href="/search/cs?searchtype=author&query=Bogin%2C+B">Ben Bogin</a>, 
<a href="/search/cs?searchtype=author&query=Katz%2C+U">Uri Katz</a>, 
<a href="/search/cs?searchtype=author&query=Deutch%2C+D">Daniel Deutch</a>, 
<a href="/search/cs?searchtype=author&query=Berant%2C+J">Jonathan Berant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023). Author's final version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14133" title="Abstract">arXiv:2304.14133</a> (replaced) [<a href="/pdf/2304.14133" title="Download PDF">pdf</a>, <a href="/format/2304.14133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VERITE: A Robust Benchmark for Multimodal Misinformation Detection  Accounting for Unimodal Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papadopoulos%2C+S">Stefanos-Iordanis Papadopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Koutlis%2C+C">Christos Koutlis</a>, 
<a href="/search/cs?searchtype=author&query=Papadopoulos%2C+S">Symeon Papadopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Petrantonakis%2C+P+C">Panagiotis C. Petrantonakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14770" title="Abstract">arXiv:2304.14770</a> (replaced) [<a href="/pdf/2304.14770" title="Download PDF">pdf</a>, <a href="/format/2304.14770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RexUIE: A Recursive Method with Explicit Schema Instructor for Universal  Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fubang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yangyang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Changlong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+K">Kun Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01951" title="Abstract">arXiv:2305.01951</a> (replaced) [<a href="/pdf/2305.01951" title="Download PDF">pdf</a>, <a href="/format/2305.01951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LMs Generalize to Future Data? An Empirical Analysis on Text  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheang%2C+C+S">Chi Seng Cheang</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+H+P">Hou Pong Chan</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+D+F">Derek F. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuebo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaocong Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shudong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+L+S">Lidia S. Chao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02605" title="Abstract">arXiv:2305.02605</a> (replaced) [<a href="/pdf/2305.02605" title="Download PDF">pdf</a>, <a href="/format/2305.02605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IMAP: Intrinsically Motivated Adversarial Policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xingjun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02997" title="Abstract">arXiv:2305.02997</a> (replaced) [<a href="/pdf/2305.02997" title="Download PDF">pdf</a>, <a href="/format/2305.02997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Do Neural Nets Outperform Boosted Trees on Tabular Data?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McElfresh%2C+D">Duncan McElfresh</a>, 
<a href="/search/cs?searchtype=author&query=Khandagale%2C+S">Sujay Khandagale</a>, 
<a href="/search/cs?searchtype=author&query=Valverde%2C+J">Jonathan Valverde</a>, 
<a href="/search/cs?searchtype=author&query=C%2C+V+P">Vishak Prasad C</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Goldblum%2C+M">Micah Goldblum</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+C">Colin White</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS Datasets and Benchmarks Track 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03695" title="Abstract">arXiv:2305.03695</a> (replaced) [<a href="/pdf/2305.03695" title="Download PDF">pdf</a>, <a href="/format/2305.03695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vera: A General-Purpose Plausibility Estimation Model for Commonsense  Statements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dianzhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03807" title="Abstract">arXiv:2305.03807</a> (replaced) [<a href="/pdf/2305.03807" title="Download PDF">pdf</a>, <a href="/format/2305.03807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evading Watermark based Detection of AI-Generated Content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhengyuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in ACM Conference on Computer and Communications Security (CCS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04386" title="Abstract">arXiv:2305.04386</a> (replaced) [<a href="/pdf/2305.04386" title="Download PDF">pdf</a>, <a href="/format/2305.04386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring Local Structure from Pairwise Correlations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Rahman%2C+M">Mahajabin Rahman</a>, 
<a href="/search/physics?searchtype=author&query=Nemenman%2C+I">Ilya Nemenman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. E, 108 034410 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Analysis, Statistics and Probability (physics.data-an)</span>; Machine Learning (cs.LG); Other Statistics (stat.OT)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04449" title="Abstract">arXiv:2305.04449</a> (replaced) [<a href="/pdf/2305.04449" title="Download PDF">pdf</a>, <a href="/format/2305.04449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thach%2C+B">Bao Thach</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+B+Y">Brian Y. Cho</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+S">Shing-Hei Ho</a>, 
<a href="/search/cs?searchtype=author&query=Hermans%2C+T">Tucker Hermans</a>, 
<a href="/search/cs?searchtype=author&query=Kuntz%2C+A">Alan Kuntz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Robotics (T-RO). 18 pages, 25 figures. arXiv admin note: substantial text overlap with <a href="/abs/2110.04685">arXiv:2110.04685</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06026" title="Abstract">arXiv:2305.06026</a> (replaced) [<a href="/pdf/2305.06026" title="Download PDF">pdf</a>, <a href="/ps/2305.06026" title="Download PostScript">ps</a>, <a href="/format/2305.06026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty in GNN Learning Evaluations: The Importance of a Consistent  Benchmark for Community Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leeney%2C+W">William Leeney</a>, 
<a href="/search/cs?searchtype=author&query=McConville%2C+R">Ryan McConville</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Twelfth International Conference on Complex Networks &amp; Their Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10160" title="Abstract">arXiv:2305.10160</a> (replaced) [<a href="/pdf/2305.10160" title="Download PDF">pdf</a>, <a href="/format/2305.10160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stop Uploading Test Data in Plain Text: Practical Strategies for  Mitigating Data Contamination by Evaluation Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacovi%2C+A">Alon Jacovi</a>, 
<a href="/search/cs?searchtype=author&query=Caciularu%2C+A">Avi Caciularu</a>, 
<a href="/search/cs?searchtype=author&query=Goldman%2C+O">Omer Goldman</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10299" title="Abstract">arXiv:2305.10299</a> (replaced) [<a href="/pdf/2305.10299" title="Download PDF">pdf</a>, <a href="/format/2305.10299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Binarized Spectral Compressive Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yuanhao Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yuxin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jing Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoqian Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023; The first work to study binarized spectral compressive imaging reconstruction problem
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10744" title="Abstract">arXiv:2305.10744</a> (replaced) [<a href="/pdf/2305.10744" title="Download PDF">pdf</a>, <a href="/format/2305.10744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Resource Allocation in Episodic Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Duksang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dabeen Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10825" title="Abstract">arXiv:2305.10825</a> (replaced) [<a href="/pdf/2305.10825" title="Download PDF">pdf</a>, <a href="/format/2305.10825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffUTE: Universal Text Editing Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haoxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhuoer Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Z">Zhangxuan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+J">Jun Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaohui Li</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Changhua Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Huijia Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS'2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11096" title="Abstract">arXiv:2305.11096</a> (replaced) [<a href="/pdf/2305.11096" title="Download PDF">pdf</a>, <a href="/format/2305.11096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-modality Data Augmentation for End-to-End Sign Language  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jinhui Ye</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+W">Wenxiang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11141" title="Abstract">arXiv:2305.11141</a> (replaced) [<a href="/pdf/2305.11141" title="Download PDF">pdf</a>, <a href="/format/2305.11141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clifford Group Equivariant Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruhe%2C+D">David Ruhe</a>, 
<a href="/search/cs?searchtype=author&query=Brandstetter%2C+J">Johannes Brandstetter</a>, 
<a href="/search/cs?searchtype=author&query=Forr%C3%A9%2C+P">Patrick Forr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11508" title="Abstract">arXiv:2305.11508</a> (replaced) [<a href="/pdf/2305.11508" title="Download PDF">pdf</a>, <a href="/format/2305.11508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PlugMed: Improving Specificity in Patient-Centered Medical Dialogue  Generation using In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dou%2C+C">Chengfeng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+W">Wenping Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haiyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Z">Zhenwei Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yongqiang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11595" title="Abstract">arXiv:2305.11595</a> (replaced) [<a href="/pdf/2305.11595" title="Download PDF">pdf</a>, <a href="/format/2305.11595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining Inter-Consistency of Large Language Models Collaboration: An  In-depth Analysis via Debate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+K">Kai Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xiao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings Camera Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11828" title="Abstract">arXiv:2305.11828</a> (replaced) [<a href="/pdf/2305.11828" title="Download PDF">pdf</a>, <a href="/format/2305.11828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Appraising the Potential Uses and Harms of LLMs for Medical Systematic  Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yun%2C+H+S">Hye Sun Yun</a>, 
<a href="/search/cs?searchtype=author&query=Marshall%2C+I+J">Iain J. Marshall</a>, 
<a href="/search/cs?searchtype=author&query=Trikalinos%2C+T+A">Thomas A. Trikalinos</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B+C">Byron C. Wallace</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures, 8 tables. Accepted as an EMNLP 2023 main paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11913" title="Abstract">arXiv:2305.11913</a> (replaced) [<a href="/pdf/2305.11913" title="Download PDF">pdf</a>, <a href="/format/2305.11913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine learning for phase-resolved reconstruction of nonlinear ocean  wave surface elevations from sparse remote sensing data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ehlers%2C+S">Svenja Ehlers</a>, 
<a href="/search/physics?searchtype=author&query=Klein%2C+M">Marco Klein</a>, 
<a href="/search/physics?searchtype=author&query=Heinlein%2C+A">Alexander Heinlein</a>, 
<a href="/search/physics?searchtype=author&query=Wedler%2C+M">Mathies Wedler</a>, 
<a href="/search/physics?searchtype=author&query=Desmars%2C+N">Nicolas Desmars</a>, 
<a href="/search/physics?searchtype=author&query=Hoffmann%2C+N">Norbert Hoffmann</a>, 
<a href="/search/physics?searchtype=author&query=Stender%2C+M">Merten Stender</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 13 figures (without appendix), final version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ocean Engineering (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12084" title="Abstract">arXiv:2305.12084</a> (replaced) [<a href="/pdf/2305.12084" title="Download PDF">pdf</a>, <a href="/format/2305.12084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Entropy Rate Constancy in Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+V">Vivek Verma</a>, 
<a href="/search/cs?searchtype=author&query=Tomlin%2C+N">Nicholas Tomlin</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12169" title="Abstract">arXiv:2305.12169</a> (replaced) [<a href="/pdf/2305.12169" title="Download PDF">pdf</a>, <a href="/format/2305.12169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Compose Representations of Different Encoder Layers towards  Improving Compositional Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuangtao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yafang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+B">Biao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yidong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaodong Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12212" title="Abstract">arXiv:2305.12212</a> (replaced) [<a href="/pdf/2305.12212" title="Download PDF">pdf</a>, <a href="/format/2305.12212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition  with Auxiliary Refined Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zhuo Pan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Di Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenkun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gang Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12532" title="Abstract">arXiv:2305.12532</a> (replaced) [<a href="/pdf/2305.12532" title="Download PDF">pdf</a>, <a href="/format/2305.12532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Simplification of Medical Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joseph%2C+S">Sebastian Joseph</a>, 
<a href="/search/cs?searchtype=author&query=Kazanas%2C+K">Kathryn Kazanas</a>, 
<a href="/search/cs?searchtype=author&query=Reina%2C+K">Keziah Reina</a>, 
<a href="/search/cs?searchtype=author&query=Ramanathan%2C+V+J">Vishnesh J. Ramanathan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B+C">Byron C. Wallace</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+J">Junyi Jessy Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version will be in EMNLP 2023 main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12733" title="Abstract">arXiv:2305.12733</a> (replaced) [<a href="/pdf/2305.12733" title="Download PDF">pdf</a>, <a href="/format/2305.12733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MADNet: Maximizing Addressee Deduction Expectation for Multi-Party  Conversation Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jia-Chen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chao-Hong Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+C">Caiyuan Chu</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhen-Hua Ling</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chongyang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Quan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023. arXiv admin note: text overlap with <a href="/abs/2203.08500">arXiv:2203.08500</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13189" title="Abstract">arXiv:2305.13189</a> (replaced) [<a href="/pdf/2305.13189" title="Download PDF">pdf</a>, <a href="/format/2305.13189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Anomaly Detection with Rejection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perini%2C+L">Lorenzo Perini</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J">Jesse Davis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14124" title="Abstract">arXiv:2305.14124</a> (replaced) [<a href="/pdf/2305.14124" title="Download PDF">pdf</a>, <a href="/format/2305.14124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Does Monolingual Data Help Multilingual Translation: The Role of  Domain and Model Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baziotis%2C+C">Christos Baziotis</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Biao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Birch%2C+A">Alexandra Birch</a>, 
<a href="/search/cs?searchtype=author&query=Haddow%2C+B">Barry Haddow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14221" title="Abstract">arXiv:2305.14221</a> (replaced) [<a href="/pdf/2305.14221" title="Download PDF">pdf</a>, <a href="/format/2305.14221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Question Answering as Programming for Solving Time-Sensitive Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian-Guang Lou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14577" title="Abstract">arXiv:2305.14577</a> (replaced) [<a href="/pdf/2305.14577" title="Download PDF">pdf</a>, <a href="/format/2305.14577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Difference-Masking: Choosing What to Mask in Continued Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilf%2C+A">Alex Wilf</a>, 
<a href="/search/cs?searchtype=author&query=Akter%2C+S+N">Syeda Nahida Akter</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+L">Leena Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P+P">Paul Pu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Mathew%2C+S">Sheryl Mathew</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M">Mengrou Shou</a>, 
<a href="/search/cs?searchtype=author&query=Nyberg%2C+E">Eric Nyberg</a>, 
<a href="/search/cs?searchtype=author&query=Morency%2C+L">Louis-Philippe Morency</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14952" title="Abstract">arXiv:2305.14952</a> (replaced) [<a href="/pdf/2305.14952" title="Download PDF">pdf</a>, <a href="/format/2305.14952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Focus Your Attention (with Adaptive IIR Filters)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lutati%2C+S">Shahar Lutati</a>, 
<a href="/search/cs?searchtype=author&query=Zimerman%2C+I">Itamar Zimerman</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+L">Lior Wolf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16530" title="Abstract">arXiv:2305.16530</a> (replaced) [<a href="/pdf/2305.16530" title="Download PDF">pdf</a>, <a href="/format/2305.16530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bi-fidelity Variational Auto-encoder for Uncertainty Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cheng%2C+N">Nuojin Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Malik%2C+O+A">Osman Asif Malik</a>, 
<a href="/search/stat?searchtype=author&query=De%2C+S">Subhayan De</a>, 
<a href="/search/stat?searchtype=author&query=Becker%2C+S">Stephen Becker</a>, 
<a href="/search/stat?searchtype=author&query=Doostan%2C+A">Alireza Doostan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16580" title="Abstract">arXiv:2305.16580</a> (replaced) [<a href="/pdf/2305.16580" title="Download PDF">pdf</a>, <a href="/format/2305.16580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TFDet: Target-Aware Fusion for RGB-T Pedestrian Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao-Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+J">Jiacheng Ying</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Z">Zehua Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Heng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Hui-Liang Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18409" title="Abstract">arXiv:2305.18409</a> (replaced) [<a href="/pdf/2305.18409" title="Download PDF">pdf</a>, <a href="/format/2305.18409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direction-oriented Multi-objective Learning: Simple and Provable  Stochastic Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+P">Peiyao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ban%2C+H">Hao Ban</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Kaiyi Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18496" title="Abstract">arXiv:2305.18496</a> (replaced) [<a href="/pdf/2305.18496" title="Download PDF">pdf</a>, <a href="/format/2305.18496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized equivalences between subsampling and ridge regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Patil%2C+P">Pratik Patil</a>, 
<a href="/search/math?searchtype=author&query=Du%2C+J">Jin-Hong Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed typos; add an figure to illustrate the risk monotonicity of optimal ridge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18803" title="Abstract">arXiv:2305.18803</a> (replaced) [<a href="/pdf/2305.18803" title="Download PDF">pdf</a>, <a href="/format/2305.18803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Koopa: Learning Non-stationary Time Series Dynamics with Koopman  Predictors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18901" title="Abstract">arXiv:2305.18901</a> (replaced) [<a href="/pdf/2305.18901" title="Download PDF">pdf</a>, <a href="/format/2305.18901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Optimization for Continuous Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hanyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wenpin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+D+D">David D. Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19185" title="Abstract">arXiv:2305.19185</a> (replaced) [<a href="/pdf/2305.19185" title="Download PDF">pdf</a>, <a href="/format/2305.19185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compression with Bayesian Implicit Neural Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zongyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Flamich%2C+G">Gergely Flamich</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiajun He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhibo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a Spotlight paper in NeurIPS 2023. Camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00840" title="Abstract">arXiv:2306.00840</a> (replaced) [<a href="/pdf/2306.00840" title="Download PDF">pdf</a>, <a href="/format/2306.00840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What model does MuZero learn?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jinke He</a>, 
<a href="/search/cs?searchtype=author&query=Moerland%2C+T+M">Thomas M. Moerland</a>, 
<a href="/search/cs?searchtype=author&query=Oliehoek%2C+F+A">Frans A. Oliehoek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01487" title="Abstract">arXiv:2306.01487</a> (replaced) [<a href="/pdf/2306.01487" title="Download PDF">pdf</a>, <a href="/ps/2306.01487" title="Download PostScript">ps</a>, <a href="/format/2306.01487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantitative Graded Semantics and Spectra of Behavioural Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Forster%2C+J">Jonas Forster</a>, 
<a href="/search/cs?searchtype=author&query=Schr%C3%B6der%2C+L">Lutz Schr&#xf6;der</a>, 
<a href="/search/cs?searchtype=author&query=Wild%2C+P">Paul Wild</a>, 
<a href="/search/cs?searchtype=author&query=Beohar%2C+H">Harsh Beohar</a>, 
<a href="/search/cs?searchtype=author&query=Gurke%2C+S">Sebastian Gurke</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6nig%2C+B">Barbara K&#xf6;nig</a>, 
<a href="/search/cs?searchtype=author&query=Messing%2C+K">Karla Messing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02479" title="Abstract">arXiv:2306.02479</a> (replaced) [<a href="/pdf/2306.02479" title="Download PDF">pdf</a>, <a href="/format/2306.02479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contagion Effect Estimation Using Proximal Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fatemi%2C+Z">Zahra Fatemi</a>, 
<a href="/search/cs?searchtype=author&query=Zheleva%2C+E">Elena Zheleva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04528" title="Abstract">arXiv:2306.04528</a> (replaced) [<a href="/pdf/2306.04528" title="Download PDF">pdf</a>, <a href="/format/2306.04528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptBench: Towards Evaluating the Robustness of Large Language Models  on Adversarial Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kaijie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiaheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report; code is at: <a href="https://github.com/microsoft/promptbench">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05366" title="Abstract">arXiv:2306.05366</a> (replaced) [<a href="/pdf/2306.05366" title="Download PDF">pdf</a>, <a href="/format/2306.05366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ordinal Potential-based Player Rating
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vadori%2C+N">Nelson Vadori</a>, 
<a href="/search/cs?searchtype=author&query=Savani%2C+R">Rahul Savani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05374" title="Abstract">arXiv:2306.05374</a> (replaced) [<a href="/pdf/2306.05374" title="Download PDF">pdf</a>, <a href="/format/2306.05374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Ultrasound Tongue Image prediction from EEG during speech  production
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Csap%C3%B3%2C+T+G">Tam&#xe1;s G&#xe1;bor Csap&#xf3;</a>, 
<a href="/search/physics?searchtype=author&query=Arthur%2C+F+V">Frigyes Viktor Arthur</a>, 
<a href="/search/physics?searchtype=author&query=Nagy%2C+P">P&#xe9;ter Nagy</a>, 
<a href="/search/physics?searchtype=author&query=Boncz%2C+%C3%81">&#xc1;d&#xe1;m Boncz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at Interspeech 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of Interspeech 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05579" title="Abstract">arXiv:2306.05579</a> (replaced) [<a href="/pdf/2306.05579" title="Download PDF">pdf</a>, <a href="/format/2306.05579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with  Heterogeneous Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengfan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Klabjan%2C+D">Diego Klabjan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages, to appear at Advances in Neural Information Processing Systems (NeurIPS 2023 Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05809" title="Abstract">arXiv:2306.05809</a> (replaced) [<a href="/pdf/2306.05809" title="Download PDF">pdf</a>, <a href="/format/2306.05809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Explanation with Varying Level of Details in an Explainable  Scientific Literature Recommender System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guesmi%2C+M">Mouadh Guesmi</a>, 
<a href="/search/cs?searchtype=author&query=Chatti%2C+M+A">Mohamed Amine Chatti</a>, 
<a href="/search/cs?searchtype=author&query=Joarder%2C+S">Shoeb Joarder</a>, 
<a href="/search/cs?searchtype=author&query=Ain%2C+Q+U">Qurat Ul Ain</a>, 
<a href="/search/cs?searchtype=author&query=Alatrash%2C+R">Rawaa Alatrash</a>, 
<a href="/search/cs?searchtype=author&query=Siepmann%2C+C">Clara Siepmann</a>, 
<a href="/search/cs?searchtype=author&query=Vahidi%2C+T">Tannaz Vahidi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an original manuscript of an article published by Taylor &amp; Francis in the International Journal of Human-Computer Interaction on 15 Oct 2023, available online: <a href="https://doi.org/10.1080/10447318.2023.2262797">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08068" title="Abstract">arXiv:2306.08068</a> (replaced) [<a href="/pdf/2306.08068" title="Download PDF">pdf</a>, <a href="/format/2306.08068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DORSal: Diffusion for Object-centric Representations of Scenes et al
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jabri%2C+A">Allan Jabri</a>, 
<a href="/search/cs?searchtype=author&query=van+Steenkiste%2C+S">Sjoerd van Steenkiste</a>, 
<a href="/search/cs?searchtype=author&query=Hoogeboom%2C+E">Emiel Hoogeboom</a>, 
<a href="/search/cs?searchtype=author&query=Sajjadi%2C+M+S+M">Mehdi S. M. Sajjadi</a>, 
<a href="/search/cs?searchtype=author&query=Kipf%2C+T">Thomas Kipf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://www.sjoerdvansteenkiste.com/dorsal">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11249" title="Abstract">arXiv:2306.11249</a> (replaced) [<a href="/pdf/2306.11249" title="Download PDF">pdf</a>, <a href="/format/2306.11249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+W">Wenfei Guan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zedong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lirong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023. 33 pages, 17 figures, 19 tables. Under review. For more details, please refer to <a href="https://github.com/chengtan9907/OpenSTL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13192" title="Abstract">arXiv:2306.13192</a> (replaced) [<a href="/pdf/2306.13192" title="Download PDF">pdf</a>, <a href="/format/2306.13192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anytime, Anywhere: Human Arm Pose from Smartwatch Data for Ubiquitous  Robot Control and Teleoperation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weigend%2C+F+C">Fabian C Weigend</a>, 
<a href="/search/cs?searchtype=author&query=Sonawani%2C+S">Shubham Sonawani</a>, 
<a href="/search/cs?searchtype=author&query=Drolet%2C+M">Michael Drolet</a>, 
<a href="/search/cs?searchtype=author&query=Amor%2C+H+B">Heni Ben Amor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10, figures, 1 table, conference: IROS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14447" title="Abstract">arXiv:2306.14447</a> (replaced) [<a href="/pdf/2306.14447" title="Download PDF">pdf</a>, <a href="/format/2306.14447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse  Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haochen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+S">Samuel Clarke</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunzhu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://hshi74.github.io/robocook/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15444" title="Abstract">arXiv:2306.15444</a> (replaced) [<a href="/pdf/2306.15444" title="Download PDF">pdf</a>, <a href="/format/2306.15444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic  Superlinear Convergence Rate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gao%2C+Z">Zhan Gao</a>, 
<a href="/search/math?searchtype=author&query=Mokhtari%2C+A">Aryan Mokhtari</a>, 
<a href="/search/math?searchtype=author&query=Koppel%2C+A">Alec Koppel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15792" title="Abstract">arXiv:2306.15792</a> (replaced) [<a href="/pdf/2306.15792" title="Download PDF">pdf</a>, <a href="/format/2306.15792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sidecars on the Central Lane: Impact of Network Proxies on Microservices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahu%2C+P">Prateek Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lucy Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Bueso%2C+M">Marco Bueso</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Shijia Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yadwadkar%2C+N+J">Neeraja J. Yadwadkar</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+M">Mohit Tiwari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at HotInfra 2023 (co-located with ISCA 2023, Orlando, FL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15895" title="Abstract">arXiv:2306.15895</a> (replaced) [<a href="/pdf/2306.15895" title="Download PDF">pdf</a>, <a href="/format/2306.15895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model as Attributed Training Data Generator: A Tale of  Diversity and Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yuchen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Ratner%2C+A">Alexander Ratner</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiaming Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (Datasets and Benchmarks Track)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00521" title="Abstract">arXiv:2307.00521</a> (replaced) [<a href="/pdf/2307.00521" title="Download PDF">pdf</a>, <a href="/format/2307.00521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> zkFi: Privacy-Preserving and Regulation Compliant Transactions using  Zero Knowledge Proofs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahu%2C+N">Naveen Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Gajera%2C+M">Mitul Gajera</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+A">Amit Chaudhary</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01367" title="Abstract">arXiv:2307.01367</a> (replaced) [<a href="/pdf/2307.01367" title="Download PDF">pdf</a>, <a href="/ps/2307.01367" title="Download PostScript">ps</a>, <a href="/format/2307.01367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Geometric Constellation Shaping for Wiener Phase Noise  Channels with Viterbi-Viterbi Carrier Phase Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rode%2C+A">Andrej Rode</a>, 
<a href="/search/cs?searchtype=author&query=Gebrehiwot%2C+W+A">Wintana Araya Gebrehiwot</a>, 
<a href="/search/cs?searchtype=author&query=Chimmalgi%2C+S">Shrinivas Chimmalgi</a>, 
<a href="/search/cs?searchtype=author&query=Schmalen%2C+L">Laurent Schmalen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at European Conference on Optical Communications 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03258" title="Abstract">arXiv:2307.03258</a> (replaced) [<a href="/pdf/2307.03258" title="Download PDF">pdf</a>, <a href="/format/2307.03258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretty Good Strategies for Benaloh Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03517" title="Abstract">arXiv:2307.03517</a> (replaced) [<a href="/pdf/2307.03517" title="Download PDF">pdf</a>, <a href="/format/2307.03517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Maximum a Posteriori Carrier Phase Estimator for Wiener  Phase Noise Channels using Belief Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chimmalgi%2C+S">Shrinivas Chimmalgi</a>, 
<a href="/search/cs?searchtype=author&query=Rode%2C+A">Andrej Rode</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+L">Luca Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Schmalen%2C+L">Laurent Schmalen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at European Conference on Optical Communications 2023. Updated to include copyright notice
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05167" title="Abstract">arXiv:2307.05167</a> (replaced) [<a href="/pdf/2307.05167" title="Download PDF">pdf</a>, <a href="/format/2307.05167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Non-Custodial Wallet for CBDC: Design Challenges and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bowler%2C+R">Ryan Bowler</a>, 
<a href="/search/cs?searchtype=author&query=Goodell%2C+G">Geoffrey Goodell</a>, 
<a href="/search/cs?searchtype=author&query=Revans%2C+J">Joe Revans</a>, 
<a href="/search/cs?searchtype=author&query=Bizama%2C+G">Gabriel Bizama</a>, 
<a href="/search/cs?searchtype=author&query=Speed%2C+C">Chris Speed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07063" title="Abstract">arXiv:2307.07063</a> (replaced) [<a href="/pdf/2307.07063" title="Download PDF">pdf</a>, <a href="/format/2307.07063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bootstrapping Vision-Language Learning with Decoupled Language  Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jian%2C+Y">Yiren Jian</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chongyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+S">Soroush Vosoughi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07120" title="Abstract">arXiv:2307.07120</a> (replaced) [<a href="/pdf/2307.07120" title="Download PDF">pdf</a>, <a href="/format/2307.07120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Genetic Algorithm for the min-max Multiple Traveling Salesman  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahmoudinazlou%2C+S">Sasan Mahmoudinazlou</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+C">Changhyun Kwon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07280" title="Abstract">arXiv:2307.07280</a> (replaced) [<a href="/pdf/2307.07280" title="Download PDF">pdf</a>, <a href="/format/2307.07280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replay to Remember: Continual Layer-Specific Fine-tuning for German  Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosin%2C+T+P">Theresa Pekarek Rosin</a>, 
<a href="/search/cs?searchtype=author&query=Wermter%2C+S">Stefan Wermter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, accepted and presented at ICANN 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Artificial Neural Networks and Machine Learning - ICANN 2023,
  Lecture Notes in Computer Science, vol 14260, 489-500
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08813" title="Abstract">arXiv:2307.08813</a> (replaced) [<a href="/pdf/2307.08813" title="Download PDF">pdf</a>, <a href="/format/2307.08813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Performance Evaluation of Large Language Models for  Extracting Molecular Interactions and Pathway Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+G">Gilchan Park</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+B">Byung-Jun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xihaier Luo</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez-Marrero%2C+V">Vanessa L&#xf3;pez-Marrero</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Shinjae Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Shantenu Jha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10068" title="Abstract">arXiv:2307.10068</a> (replaced) [<a href="/pdf/2307.10068" title="Download PDF">pdf</a>, <a href="/format/2307.10068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Model Reductions for Verification of Multi-Agent Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yan Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11704" title="Abstract">arXiv:2307.11704</a> (replaced) [<a href="/pdf/2307.11704" title="Download PDF">pdf</a>, <a href="/format/2307.11704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JoinGym: An Efficient Query Optimization Environment for Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaiwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junxiong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yueying Li</a>, 
<a href="/search/cs?searchtype=author&query=Kallus%2C+N">Nathan Kallus</a>, 
<a href="/search/cs?searchtype=author&query=Trummer%2C+I">Immanuel Trummer</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> JoinGym is available at <a href="https://github.com/kaiwenw/JoinGym!">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12348" title="Abstract">arXiv:2307.12348</a> (replaced) [<a href="/pdf/2307.12348" title="Download PDF">pdf</a>, <a href="/format/2307.12348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResShift: Efficient Diffusion Model for Image Super-resolution by  Residual Shifting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+Z">Zongsheng Yue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Loy%2C+C+C">Chen Change Loy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Spotlight). Project page: <a href="https://zsyoaoa.github.io/projects/resshift/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12672" title="Abstract">arXiv:2307.12672</a> (replaced) [<a href="/pdf/2307.12672" title="Download PDF">pdf</a>, <a href="/format/2307.12672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked  Image Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pan%2C+J">Jiazhen Pan</a>, 
<a href="/search/eess?searchtype=author&query=Shit%2C+S">Suprosanna Shit</a>, 
<a href="/search/eess?searchtype=author&query=Turgut%2C+%C3%96">&#xd6;zg&#xfc;n Turgut</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+W">Wenqi Huang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H+B">Hongwei Bran Li</a>, 
<a href="/search/eess?searchtype=author&query=Stolt-Ans%C3%B3%2C+N">Nil Stolt-Ans&#xf3;</a>, 
<a href="/search/eess?searchtype=author&query=K%C3%BCstner%2C+T">Thomas K&#xfc;stner</a>, 
<a href="/search/eess?searchtype=author&query=Hammernik%2C+K">Kerstin Hammernik</a>, 
<a href="/search/eess?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00560" title="Abstract">arXiv:2308.00560</a> (replaced) [<a href="/pdf/2308.00560" title="Download PDF">pdf</a>, <a href="/format/2308.00560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning-based Non-Autoregressive Solver for Traveling  Salesman Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yubin Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huanhuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+W">Wei Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yanchun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">You Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01534" title="Abstract">arXiv:2308.01534</a> (replaced) [<a href="/pdf/2308.01534" title="Download PDF">pdf</a>, <a href="/format/2308.01534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Partition Approximating All $\ell_p$-norm Objectives in Correlation  Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davies%2C+S">Sami Davies</a>, 
<a href="/search/cs?searchtype=author&query=Moseley%2C+B">Benjamin Moseley</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+H">Heather Newman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02673" title="Abstract">arXiv:2308.02673</a> (replaced) [<a href="/pdf/2308.02673" title="Download PDF">pdf</a>, <a href="/ps/2308.02673" title="Download PostScript">ps</a>, <a href="/format/2308.02673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comments on &quot;A Linear Time Algorithm for the Optimal Discrete IRS  Beamforming&quot;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pekcan%2C+D+K">Dogan Kutay Pekcan</a>, 
<a href="/search/cs?searchtype=author&query=Ayanoglu%2C+E">Ender Ayanoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03666" title="Abstract">arXiv:2308.03666</a> (replaced) [<a href="/pdf/2308.03666" title="Download PDF">pdf</a>, <a href="/format/2308.03666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Trustworthiness and Open-World Learning: An Exploratory Neural  Approach for Enhancing Interpretability, Generalization, and Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Du%2C+S">Shide Du</a>, 
<a href="/search/stat?searchtype=author&query=Fang%2C+Z">Zihan Fang</a>, 
<a href="/search/stat?searchtype=author&query=Lan%2C+S">Shiyang Lan</a>, 
<a href="/search/stat?searchtype=author&query=Tan%2C+Y">Yanchao Tan</a>, 
<a href="/search/stat?searchtype=author&query=G%C3%BCnther%2C+M">Manuel G&#xfc;nther</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+S">Shiping Wang</a>, 
<a href="/search/stat?searchtype=author&query=Guo%2C+W">Wenzhong Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05314" title="Abstract">arXiv:2308.05314</a> (replaced) [<a href="/pdf/2308.05314" title="Download PDF">pdf</a>, <a href="/format/2308.05314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Semantic Graph Matching for Large-scale Outdoor Point Clouds  Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shaocong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Ruqin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+C">Chenguang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongsheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Longguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanyun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06444" title="Abstract">arXiv:2308.06444</a> (replaced) [<a href="/pdf/2308.06444" title="Download PDF">pdf</a>, <a href="/format/2308.06444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TongueSAM: An Universal Tongue Segmentation Model Based on SAM with  Zero-Shot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Shan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+Q">Qunsheng Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingfeng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07688" title="Abstract">arXiv:2308.07688</a> (replaced) [<a href="/pdf/2308.07688" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Network Initialization for Medical AI Models Using  Large-Scale, Unlabeled Natural Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Arasteh%2C+S+T">Soroosh Tayebi Arasteh</a>, 
<a href="/search/eess?searchtype=author&query=Misera%2C+L">Leo Misera</a>, 
<a href="/search/eess?searchtype=author&query=Kather%2C+J+N">Jakob Nikolas Kather</a>, 
<a href="/search/eess?searchtype=author&query=Truhn%2C+D">Daniel Truhn</a>, 
<a href="/search/eess?searchtype=author&query=Nebelung%2C+S">Sven Nebelung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11063" title="Abstract">arXiv:2308.11063</a> (replaced) [<a href="/pdf/2308.11063" title="Download PDF">pdf</a>, <a href="/format/2308.11063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaGCD: Learning to Continually Learn in Generalized Category Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yanan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Z">Zhixiang Chi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Songhe Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11594" title="Abstract">arXiv:2308.11594</a> (replaced) [<a href="/pdf/2308.11594" title="Download PDF">pdf</a>, <a href="/format/2308.11594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantization-based Optimization with Perspective of Quantum Mechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Seok%2C+J">Jinwuk Seok</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cho%2C+C">Changsik Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ICTC 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12544" title="Abstract">arXiv:2308.12544</a> (replaced) [<a href="/pdf/2308.12544" title="Download PDF">pdf</a>, <a href="/format/2308.12544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analog Multi-Party Computing: Locally Differential Private Protocols for  Collaborative Computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hsuan-Po Liu</a>, 
<a href="/search/cs?searchtype=author&query=Soleymani%2C+M">Mahdi Soleymani</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavifar%2C+H">Hessam Mahdavifar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14840" title="Abstract">arXiv:2308.14840</a> (replaced) [<a href="/pdf/2308.14840" title="Download PDF">pdf</a>, <a href="/format/2308.14840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying and Mitigating the Security Risks of Generative AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barrett%2C+C">Clark Barrett</a>, 
<a href="/search/cs?searchtype=author&query=Boyd%2C+B">Brad Boyd</a>, 
<a href="/search/cs?searchtype=author&query=Burzstein%2C+E">Elie Burzstein</a>, 
<a href="/search/cs?searchtype=author&query=Carlini%2C+N">Nicholas Carlini</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Brad Chen</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jihye Choi</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+A+R">Amrita Roy Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Christodorescu%2C+M">Mihai Christodorescu</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+A">Anupam Datta</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>, 
<a href="/search/cs?searchtype=author&query=Fisher%2C+K">Kathleen Fisher</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T">Tatsunori Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Hendrycks%2C+D">Dan Hendrycks</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Daniel Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kerschbaum%2C+F">Florian Kerschbaum</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+E">Eric Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+J">John Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Ramzan%2C+Z">Zulfikar Ramzan</a>, 
<a href="/search/cs?searchtype=author&query=Shams%2C+K">Khawaja Shams</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>, 
<a href="/search/cs?searchtype=author&query=Taly%2C+A">Ankur Taly</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15515" title="Abstract">arXiv:2308.15515</a> (replaced) [<a href="/pdf/2308.15515" title="Download PDF">pdf</a>, <a href="/format/2308.15515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Optimal 2-Packing Sets on Arbitrary Graphs at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borowitz%2C+J">Jannick Borowitz</a>, 
<a href="/search/cs?searchtype=author&query=Gro%C3%9Fmann%2C+E">Ernestine Gro&#xdf;mann</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+C">Christian Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Schweisgut%2C+D">Dominik Schweisgut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15613" title="Abstract">arXiv:2308.15613</a> (replaced) [<a href="/pdf/2308.15613" title="Download PDF">pdf</a>, <a href="/format/2308.15613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Variational Flows for Discrete Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Diluvi%2C+G+C">Gian Carlo Diluvi</a>, 
<a href="/search/stat?searchtype=author&query=Bloem-Reddy%2C+B">Benjamin Bloem-Reddy</a>, 
<a href="/search/stat?searchtype=author&query=Campbell%2C+T">Trevor Campbell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15684" title="Abstract">arXiv:2308.15684</a> (replaced) [<a href="/pdf/2308.15684" title="Download PDF">pdf</a>, <a href="/ps/2308.15684" title="Download PostScript">ps</a>, <a href="/format/2308.15684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactively Robot Action Planning with Uncertainty Analysis and Active  Questioning by Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hori%2C+K">Kazuki Hori</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+K">Kanata Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Ogata%2C+T">Tetsuya Ogata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, accepted at SII 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01406" title="Abstract">arXiv:2309.01406</a> (replaced) [<a href="/pdf/2309.01406" title="Download PDF">pdf</a>, <a href="/format/2309.01406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Residual Elastic Warps for Image Stitching under Dirichlet  Boundary Condition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yongjun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W+K">Woo Kyoung Han</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+K+H">Kyong Hwan Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01409" title="Abstract">arXiv:2309.01409</a> (replaced) [<a href="/pdf/2309.01409" title="Download PDF">pdf</a>, <a href="/format/2309.01409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Neural Image Stitching With Enhanced and Blended Feature  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaewon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byeonghun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Im%2C+S">Sunghoon Im</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+K+H">Kyong Hwan Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03247" title="Abstract">arXiv:2309.03247</a> (replaced) [<a href="/e-print/2309.03247" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Visual Tracking by Motion Analyzing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leo%2C+M">Mohammed Leo</a>, 
<a href="/search/cs?searchtype=author&query=Ubul%2C+K">Kurban Ubul</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">ShengJie Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Michael Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> found some key point that is missed,considering that it will take a lot of time to reproduce the results and revise our mistakes,we would like to withdraw the manuscript to avoid further mislead
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04837" title="Abstract">arXiv:2309.04837</a> (replaced) [<a href="/pdf/2309.04837" title="Download PDF">pdf</a>, <a href="/format/2309.04837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HAct: Out-of-Distribution Detection with Neural Net Activation  Histograms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+S">Sudeepta Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Sundaramoorthi%2C+G">Ganesh Sundaramoorthi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06800" title="Abstract">arXiv:2309.06800</a> (replaced) [<a href="/pdf/2309.06800" title="Download PDF">pdf</a>, <a href="/format/2309.06800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-aware Traffic Prediction under Missing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Hao Mei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junxian Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhiming Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guanjie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Bin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hua Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures, extended version of our short paper "Uncertainty-aware Traffic Prediction under Missing Data" submited to ICDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07891" title="Abstract">arXiv:2309.07891</a> (replaced) [<a href="/pdf/2309.07891" title="Download PDF">pdf</a>, <a href="/format/2309.07891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HandNeRF: Learning to Reconstruct Hand-Object Interaction Scene from a  Single RGB Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+H">Hongsuk Choi</a>, 
<a href="/search/cs?searchtype=author&query=Chavan-Dafle%2C+N">Nikhil Chavan-Dafle</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiacheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Isler%2C+V">Volkan Isler</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hyunsoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages including the supplementary material, 8 tables, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08523" title="Abstract">arXiv:2309.08523</a> (replaced) [<a href="/pdf/2309.08523" title="Download PDF">pdf</a>, <a href="/format/2309.08523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breathing New Life into 3D Assets with Generative Repainting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianfu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kanakis%2C+M">Menelaos Kanakis</a>, 
<a href="/search/cs?searchtype=author&query=Schindler%2C+K">Konrad Schindler</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>, 
<a href="/search/cs?searchtype=author&query=Obukhov%2C+A">Anton Obukhov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10065" title="Abstract">arXiv:2309.10065</a> (replaced) [<a href="/pdf/2309.10065" title="Download PDF">pdf</a>, <a href="/format/2309.10065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian longitudinal tensor response regression for modeling  neuroplasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Kundu%2C+S">Suprateek Kundu</a>, 
<a href="/search/q-bio?searchtype=author&query=Reinhardt%2C+A">Alec Reinhardt</a>, 
<a href="/search/q-bio?searchtype=author&query=Song%2C+S">Serena Song</a>, 
<a href="/search/q-bio?searchtype=author&query=Han%2C+J">Joo Han</a>, 
<a href="/search/q-bio?searchtype=author&query=Meadows%2C+M+L">M. Lawson Meadows</a>, 
<a href="/search/q-bio?searchtype=author&query=Crosson%2C+B">Bruce Crosson</a>, 
<a href="/search/q-bio?searchtype=author&query=Krishnamurthy%2C+V">Venkatagiri Krishnamurthy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 8 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10255" title="Abstract">arXiv:2309.10255</a> (replaced) [<a href="/pdf/2309.10255" title="Download PDF">pdf</a>, <a href="/format/2309.10255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RGB-based Category-level Object Pose Estimation via Decoupled Metric  Scale Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jiaxin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xibin Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weizhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kneip%2C+L">Laurent Kneip</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongdong Li</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+P">Pan Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11497" title="Abstract">arXiv:2309.11497</a> (replaced) [<a href="/pdf/2309.11497" title="Download PDF">pdf</a>, <a href="/format/2309.11497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FreeU: Free Lunch in Diffusion U-Net
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+C">Chenyang Si</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Ziqi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Method update: we proposed structure-based scaling to enhance the performance of FreeU. Project page: <a href="https://chenyangsi.top/FreeU/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12444" title="Abstract">arXiv:2309.12444</a> (replaced) [<a href="/pdf/2309.12444" title="Download PDF">pdf</a>, <a href="/format/2309.12444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Metrics: Quantifying Effectiveness of Healthcare  Conversations powered by Generative AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbasian%2C+M">Mahyar Abbasian</a>, 
<a href="/search/cs?searchtype=author&query=Khatibi%2C+E">Elahe Khatibi</a>, 
<a href="/search/cs?searchtype=author&query=Azimi%2C+I">Iman Azimi</a>, 
<a href="/search/cs?searchtype=author&query=Oniani%2C+D">David Oniani</a>, 
<a href="/search/cs?searchtype=author&query=Abad%2C+Z+S+H">Zahra Shakeri Hossein Abad</a>, 
<a href="/search/cs?searchtype=author&query=Thieme%2C+A">Alexander Thieme</a>, 
<a href="/search/cs?searchtype=author&query=Sriram%2C+R">Ram Sriram</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhongqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanshan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bryant Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gevaert%2C+O">Olivier Gevaert</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li-Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Ramesh Jain</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+A+M">Amir M. Rahmani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 2 tables, journal paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13365" title="Abstract">arXiv:2309.13365</a> (replaced) [<a href="/e-print/2309.13365" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in  IBMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kohler%2C+H">Hecotr Kohler</a>, 
<a href="/search/cs?searchtype=author&query=Akrour%2C+R">Riad Akrour</a>, 
<a href="/search/cs?searchtype=author&query=Preux%2C+P">Philippe Preux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be included in an other submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13609" title="Abstract">arXiv:2309.13609</a> (replaced) [<a href="/pdf/2309.13609" title="Download PDF">pdf</a>, <a href="/format/2309.13609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vulnerabilities in Video Quality Assessment Models: The Challenge of  Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Ao-Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+Y">Yu Ran</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Weixuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuan-Gen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15028" title="Abstract">arXiv:2309.15028</a> (replaced) [<a href="/pdf/2309.15028" title="Download PDF">pdf</a>, <a href="/format/2309.15028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t throw away your value model! Making PPO even better via  Value-Guided Monte-Carlo Tree Search decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A">Andrew Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Pasunuru%2C+R">Ramakanth Pasunuru</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15084" title="Abstract">arXiv:2309.15084</a> (replaced) [<a href="/pdf/2309.15084" title="Download PDF">pdf</a>, <a href="/format/2309.15084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Surveillance AI Pipeline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalluri%2C+P+R">Pratyusha Ria Kalluri</a>, 
<a href="/search/cs?searchtype=author&query=Agnew%2C+W">William Agnew</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Myra Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Owens%2C+K">Kentrell Owens</a>, 
<a href="/search/cs?searchtype=author&query=Soldaini%2C+L">Luca Soldaini</a>, 
<a href="/search/cs?searchtype=author&query=Birhane%2C+A">Abeba Birhane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15289" title="Abstract">arXiv:2309.15289</a> (replaced) [<a href="/e-print/2309.15289" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEPT: Towards Efficient Scene Representation Learning for Motion  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhiqian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuxuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yao Mu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+E">Shengbo Eben Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Keqiang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The content of the article needs to be further polished
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16661" title="Abstract">arXiv:2309.16661</a> (replaced) [<a href="/pdf/2309.16661" title="Download PDF">pdf</a>, <a href="/format/2309.16661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SA2-Net: Scale-aware Attention Network for Microscopic Image  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fiaz%2C+M">Mustansar Fiaz</a>, 
<a href="/search/cs?searchtype=author&query=Anwer%2C+R+M">Rao Muhammad Anwer</a>, 
<a href="/search/cs?searchtype=author&query=Cholakkal%2C+H">Hisham Cholakkal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BMVC 2023 accepted as oral
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16995" title="Abstract">arXiv:2309.16995</a> (replaced) [<a href="/pdf/2309.16995" title="Download PDF">pdf</a>, <a href="/ps/2309.16995" title="Download PostScript">ps</a>, <a href="/format/2309.16995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Max Weight Independent Set in sparse graphs with no long claws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abrishami%2C+T">Tara Abrishami</a>, 
<a href="/search/cs?searchtype=author&query=Chudnovsky%2C+M">Maria Chudnovsky</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Marcin Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Rz%C4%85%C5%BCewski%2C+P">Pawe&#x142; Rz&#x105;&#x17c;ewski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17264" title="Abstract">arXiv:2309.17264</a> (replaced) [<a href="/pdf/2309.17264" title="Download PDF">pdf</a>, <a href="/format/2309.17264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Foundation Model for General Moving Object Segmentation in Medical  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhongnuo Yan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tong Han</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Han Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiongquan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenlong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+D">Dong Ni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00313" title="Abstract">arXiv:2310.00313</a> (replaced) [<a href="/pdf/2310.00313" title="Download PDF">pdf</a>, <a href="/format/2310.00313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning in Large Language Models: A Neuroscience-inspired  Analysis of Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yousefi%2C+S">Safoora Yousefi</a>, 
<a href="/search/cs?searchtype=author&query=Betthauser%2C+L">Leo Betthauser</a>, 
<a href="/search/cs?searchtype=author&query=Hasanbeig%2C+H">Hosein Hasanbeig</a>, 
<a href="/search/cs?searchtype=author&query=Saran%2C+A">Akanksha Saran</a>, 
<a href="/search/cs?searchtype=author&query=Milli%C3%A8re%2C+R">Rapha&#xeb;l Milli&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Momennejad%2C+I">Ida Momennejad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added overview figures 1-3 in this version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00530" title="Abstract">arXiv:2310.00530</a> (replaced) [<a href="/pdf/2310.00530" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Neural Radiance Fields (NeRF) for Large-scale Aerial Images --  A Multi-tiling Approach and the Geometry Assessment of NeRF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Ningli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+R">Rongjun Qin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Debao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Remondino%2C+F">Fabio Remondino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00627" title="Abstract">arXiv:2310.00627</a> (replaced) [<a href="/pdf/2310.00627" title="Download PDF">pdf</a>, <a href="/format/2310.00627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Client Selection for Federated Learning using Cellular  Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pavlidis%2C+N">Nikolaos Pavlidis</a>, 
<a href="/search/cs?searchtype=author&query=Perifanis%2C+V">Vasileios Perifanis</a>, 
<a href="/search/cs?searchtype=author&query=Chatzinikolaou%2C+T+P">Theodoros Panagiotis Chatzinikolaou</a>, 
<a href="/search/cs?searchtype=author&query=Sirakoulis%2C+G+C">Georgios Ch. Sirakoulis</a>, 
<a href="/search/cs?searchtype=author&query=Efraimidis%2C+P+S">Pavlos S. Efraimidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18th IEEE International Workshop on Cellular Nanoscale Networks and their Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02523" title="Abstract">arXiv:2310.02523</a> (replaced) [<a href="/pdf/2310.02523" title="Download PDF">pdf</a>, <a href="/format/2310.02523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spatio-Temporal Attention-Based Method for Detecting Student Classroom  Behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02784" title="Abstract">arXiv:2310.02784</a> (replaced) [<a href="/pdf/2310.02784" title="Download PDF">pdf</a>, <a href="/format/2310.02784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAD Max Beyond Single-Node: Enabling Large Machine Learning Model  Acceleration on Distributed Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsia%2C+S">Samuel Hsia</a>, 
<a href="/search/cs?searchtype=author&query=Golden%2C+A">Alicia Golden</a>, 
<a href="/search/cs?searchtype=author&query=Acun%2C+B">Bilge Acun</a>, 
<a href="/search/cs?searchtype=author&query=Ardalani%2C+N">Newsha Ardalani</a>, 
<a href="/search/cs?searchtype=author&query=DeVito%2C+Z">Zachary DeVito</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Gu-Yeon Wei</a>, 
<a href="/search/cs?searchtype=author&query=Brooks%2C+D">David Brooks</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Carole-Jean Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02815" title="Abstract">arXiv:2310.02815</a> (replaced) [<a href="/pdf/2310.02815" title="Download PDF">pdf</a>, <a href="/format/2310.02815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoBEV: Elevating Roadside 3D Object Detection with Depth and Height  Complementarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+C">Chengshan Pang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+H">Huajian Ni</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yining Lin</a>, 
<a href="/search/cs?searchtype=author&query=Stiefelhagen%2C+R">Rainer Stiefelhagen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaiwei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The source code will be made publicly available at <a href="https://github.com/MasterHow/CoBEV">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04292" title="Abstract">arXiv:2310.04292</a> (replaced) [<a href="/pdf/2310.04292" title="Download PDF">pdf</a>, <a href="/format/2310.04292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Foundational Models for Molecular Learning on Large-Scale  Multi-Task Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beaini%2C+D">Dominique Beaini</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shenyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cunha%2C+J+A">Joao Alex Cunha</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Moisescu-Pareja%2C+G">Gabriela Moisescu-Pareja</a>, 
<a href="/search/cs?searchtype=author&query=Dymov%2C+O">Oleksandr Dymov</a>, 
<a href="/search/cs?searchtype=author&query=Maddrell-Mander%2C+S">Samuel Maddrell-Mander</a>, 
<a href="/search/cs?searchtype=author&query=McLean%2C+C">Callum McLean</a>, 
<a href="/search/cs?searchtype=author&query=Wenkel%2C+F">Frederik Wenkel</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+L">Luis M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Mohamud%2C+J+H">Jama Hussein Mohamud</a>, 
<a href="/search/cs?searchtype=author&query=Parviz%2C+A">Ali Parviz</a>, 
<a href="/search/cs?searchtype=author&query=Craig%2C+M">Michael Craig</a>, 
<a href="/search/cs?searchtype=author&query=Koziarski%2C+M">Micha&#x142; Koziarski</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiarui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhaocheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gabellini%2C+C">Cristian Gabellini</a>, 
<a href="/search/cs?searchtype=author&query=Klaser%2C+K">Kerstin Klaser</a>, 
<a href="/search/cs?searchtype=author&query=Dean%2C+J">Josef Dean</a>, 
<a href="/search/cs?searchtype=author&query=Wognum%2C+C">Cas Wognum</a>, 
<a href="/search/cs?searchtype=author&query=Sypetkowski%2C+M">Maciej Sypetkowski</a>, 
<a href="/search/cs?searchtype=author&query=Rabusseau%2C+G">Guillaume Rabusseau</a>, 
<a href="/search/cs?searchtype=author&query=Rabbany%2C+R">Reihaneh Rabbany</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+C">Christopher Morris</a>, 
<a href="/search/cs?searchtype=author&query=Koutis%2C+I">Ioannis Koutis</a>, 
<a href="/search/cs?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+G">Guy Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Tossou%2C+P">Prudencio Tossou</a>, 
<a href="/search/cs?searchtype=author&query=Mary%2C+H">Hadrien Mary</a>, 
<a href="/search/cs?searchtype=author&query=Bois%2C+T">Therence Bois</a>, 
<a href="/search/cs?searchtype=author&query=Fitzgibbon%2C+A">Andrew Fitzgibbon</a>, 
<a href="/search/cs?searchtype=author&query=Banaszewski%2C+B">B&#x142;a&#x17c;ej Banaszewski</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+C">Chad Martin</a>, 
<a href="/search/cs?searchtype=author&query=Masters%2C+D">Dominic Masters</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04781" title="Abstract">arXiv:2310.04781</a> (replaced) [<a href="/pdf/2310.04781" title="Download PDF">pdf</a>, <a href="/format/2310.04781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying Foundation Models with Quadrotor Control for Visual Tracking  Beyond Object Categories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saviolo%2C+A">Alessandro Saviolo</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+P">Pratyaksh Rao</a>, 
<a href="/search/cs?searchtype=author&query=Radhakrishnan%2C+V">Vivek Radhakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jiuhong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Loianno%2C+G">Giuseppe Loianno</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04921" title="Abstract">arXiv:2310.04921</a> (replaced) [<a href="/pdf/2310.04921" title="Download PDF">pdf</a>, <a href="/format/2310.04921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crystal: Introspective Reasoners Reinforced with Self-Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pasunuru%2C+R">Ramakanth Pasunuru</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05012" title="Abstract">arXiv:2310.05012</a> (replaced) [<a href="/pdf/2310.05012" title="Download PDF">pdf</a>, <a href="/format/2310.05012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Abnormal Health Conditions in Smart Home Using a Drone
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barman%2C+P+K">Pronob Kumar Barman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05295" title="Abstract">arXiv:2310.05295</a> (replaced) [<a href="/pdf/2310.05295" title="Download PDF">pdf</a>, <a href="/format/2310.05295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Storytelling with Question-Answer Plans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Danyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lapata%2C+M">Mirella Lapata</a>, 
<a href="/search/cs?searchtype=author&query=Keller%2C+F">Frank Keller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07177" title="Abstract">arXiv:2310.07177</a> (replaced) [<a href="/pdf/2310.07177" title="Download PDF">pdf</a>, <a href="/format/2310.07177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Speculative Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lanxiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Bailis%2C+P">Peter Bailis</a>, 
<a href="/search/cs?searchtype=author&query=Stoica%2C+I">Ion Stoica</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhijie Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+A">Alvin Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07487" title="Abstract">arXiv:2310.07487</a> (replaced) [<a href="/pdf/2310.07487" title="Download PDF">pdf</a>, <a href="/format/2310.07487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cognate Transformer for Automated Phonological Reconstruction and  Cognate Reflex Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akavarapu%2C+V+S+D+S+M">V.S.D.S.Mahesh Akavarapu</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Arnab Bhattacharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP-2023 (Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07521" title="Abstract">arXiv:2310.07521</a> (replaced) [<a href="/pdf/2310.07521" title="Download PDF">pdf</a>, <a href="/format/2310.07521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survey on Factuality in Large Language Models: Knowledge, Retrieval and  Domain-Specificity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cunxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yuanhao Yue</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiayang%2C+C">Cheng Jiayang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yunzhi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wenyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zehan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages; 300+ references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07923" title="Abstract">arXiv:2310.07923</a> (replaced) [<a href="/pdf/2310.07923" title="Download PDF">pdf</a>, <a href="/ps/2310.07923" title="Download PostScript">ps</a>, <a href="/format/2310.07923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Expressive Power of Transformers with Chain of Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Merrill%2C+W">William Merrill</a>, 
<a href="/search/cs?searchtype=author&query=Sabharwal%2C+A">Ashish Sabharwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9-page preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Computation and Language (cs.CL); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08172" title="Abstract">arXiv:2310.08172</a> (replaced) [<a href="/pdf/2310.08172" title="Download PDF">pdf</a>, <a href="/format/2310.08172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Cognitive Knowledge Structure of Large Language Models: An  Educational Diagnostic Assessment Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 (Short Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08205" title="Abstract">arXiv:2310.08205</a> (replaced) [<a href="/pdf/2310.08205" title="Download PDF">pdf</a>, <a href="/format/2310.08205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiveVV: Human-Centered Live Volumetric Video Streaming System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kaiyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kaiying Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junhua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haowen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yili Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fangxin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08565" title="Abstract">arXiv:2310.08565</a> (replaced) [<a href="/pdf/2310.08565" title="Download PDF">pdf</a>, <a href="/format/2310.08565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security Considerations in AI-Robotics: A Survey of Current Methods,  Challenges, and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neupane%2C+S">Subash Neupane</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+S">Shaswata Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+I+A">Ivan A. Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Swayamjit Saha</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Sudip Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingdao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+N">Nisha Pillai</a>, 
<a href="/search/cs?searchtype=author&query=Rahimi%2C+S">Shahram Rahimi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08745" title="Abstract">arXiv:2310.08745</a> (replaced) [<a href="/pdf/2310.08745" title="Download PDF">pdf</a>, <a href="/format/2310.08745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AcTExplore: Active Tactile Exploration on Unknown Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahidzadeh%2C+A">Amir-Hossein Shahidzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S+J">Seong Jong Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Mantripragada%2C+P">Pavan Mantripragada</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+C+D">Chahat Deep Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ferm%C3%BCller%2C+C">Cornelia Ferm&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Aloimonos%2C+Y">Yiannis Aloimonos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08754" title="Abstract">arXiv:2310.08754</a> (replaced) [<a href="/pdf/2310.08754" title="Download PDF">pdf</a>, <a href="/format/2310.08754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tokenizer Choice For LLM Training: Negligible or Crucial?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+M">Mehdi Ali</a>, 
<a href="/search/cs?searchtype=author&query=Fromm%2C+M">Michael Fromm</a>, 
<a href="/search/cs?searchtype=author&query=Thellmann%2C+K">Klaudia Thellmann</a>, 
<a href="/search/cs?searchtype=author&query=Rutmann%2C+R">Richard Rutmann</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%BCbbering%2C+M">Max L&#xfc;bbering</a>, 
<a href="/search/cs?searchtype=author&query=Leveling%2C+J">Johannes Leveling</a>, 
<a href="/search/cs?searchtype=author&query=Klug%2C+K">Katrin Klug</a>, 
<a href="/search/cs?searchtype=author&query=Ebert%2C+J">Jan Ebert</a>, 
<a href="/search/cs?searchtype=author&query=Doll%2C+N">Niclas Doll</a>, 
<a href="/search/cs?searchtype=author&query=Buschhoff%2C+J+S">Jasper Schulze Buschhoff</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+C">Charvi Jain</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+A+A">Alexander Arno Weber</a>, 
<a href="/search/cs?searchtype=author&query=Jurkschat%2C+L">Lena Jurkschat</a>, 
<a href="/search/cs?searchtype=author&query=Abdelwahab%2C+H">Hammam Abdelwahab</a>, 
<a href="/search/cs?searchtype=author&query=John%2C+C">Chelsea John</a>, 
<a href="/search/cs?searchtype=author&query=Suarez%2C+P+O">Pedro Ortiz Suarez</a>, 
<a href="/search/cs?searchtype=author&query=Ostendorff%2C+M">Malte Ostendorff</a>, 
<a href="/search/cs?searchtype=author&query=Weinbach%2C+S">Samuel Weinbach</a>, 
<a href="/search/cs?searchtype=author&query=Sifa%2C+R">Rafet Sifa</a>, 
<a href="/search/cs?searchtype=author&query=Kesselheim%2C+S">Stefan Kesselheim</a>, 
<a href="/search/cs?searchtype=author&query=Flores-Herr%2C+N">Nicolas Flores-Herr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08808" title="Abstract">arXiv:2310.08808</a> (replaced) [<a href="/pdf/2310.08808" title="Download PDF">pdf</a>, <a href="/format/2310.08808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attacks Meet Interpretability (AmI) Evaluation and Findings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Ziping Ye</a>, 
<a href="/search/cs?searchtype=author&query=Mehnaz%2C+S">Shagufta Mehnaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08825" title="Abstract">arXiv:2310.08825</a> (replaced) [<a href="/e-print/2310.08825" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Dongsheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songlin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hongkai Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qi Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is under the corporation's legal review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08868" title="Abstract">arXiv:2310.08868</a> (replaced) [<a href="/pdf/2310.08868" title="Download PDF">pdf</a>, <a href="/format/2310.08868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeCoNet: A Heterosexual Contact Network Growth Model for Human  Papillomavirus Disease Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Piraveenan%2C+M">Mahendra Piraveenan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09166" title="Abstract">arXiv:2310.09166</a> (replaced) [<a href="/pdf/2310.09166" title="Download PDF">pdf</a>, <a href="/format/2310.09166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing a Natural Language Understanding Model to Characterize Cable  News Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benson%2C+S+P">Seth P. Benson</a>, 
<a href="/search/cs?searchtype=author&query=Cruickshank%2C+I+J">Iain J. Cruickshank</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09199" title="Abstract">arXiv:2310.09199</a> (replaced) [<a href="/pdf/2310.09199" title="Download PDF">pdf</a>, <a href="/format/2310.09199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PaLI-3 Vision Language Models: Smaller, Faster, Stronger
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Beyer%2C+L">Lucas Beyer</a>, 
<a href="/search/cs?searchtype=author&query=Kolesnikov%2C+A">Alexander Kolesnikov</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jialin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Voigtlaender%2C+P">Paul Voigtlaender</a>, 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+B">Basil Mustafa</a>, 
<a href="/search/cs?searchtype=author&query=Goodman%2C+S">Sebastian Goodman</a>, 
<a href="/search/cs?searchtype=author&query=Alabdulmohsin%2C+I">Ibrahim Alabdulmohsin</a>, 
<a href="/search/cs?searchtype=author&query=Padlewski%2C+P">Piotr Padlewski</a>, 
<a href="/search/cs?searchtype=author&query=Salz%2C+D">Daniel Salz</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+X">Xi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Vlasic%2C+D">Daniel Vlasic</a>, 
<a href="/search/cs?searchtype=author&query=Pavetic%2C+F">Filip Pavetic</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+K">Keran Rong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianli Yu</a>, 
<a href="/search/cs?searchtype=author&query=Keysers%2C+D">Daniel Keysers</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+X">Xiaohua Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Soricut%2C+R">Radu Soricut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09232" title="Abstract">arXiv:2310.09232</a> (replaced) [<a href="/pdf/2310.09232" title="Download PDF">pdf</a>, <a href="/ps/2310.09232" title="Download PostScript">ps</a>, <a href="/format/2310.09232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounds on Guessing Numbers and Secret Sharing Combining Information  Theory Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%BCrp%C4%B1nar%2C+E">Emirhan G&#xfc;rp&#x131;nar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preliminary version of the results presented in section 4 (bounds on the information ratio of access structures for secret sharing schemes) was published in proceedings of IEEE ISIT, the text of which is available as <a href="/abs/2201.11656">arXiv:2201.11656</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09238" title="Abstract">arXiv:2310.09238</a> (replaced) [<a href="/pdf/2310.09238" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models  for Sentiment Analysis of Bangla Social Media Posts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Saumajit Saha</a>, 
<a href="/search/cs?searchtype=author&query=Nanda%2C+A">Albert Nanda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures, workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09619" title="Abstract">arXiv:2310.09619</a> (replaced) [<a href="/pdf/2310.09619" title="Download PDF">pdf</a>, <a href="/format/2310.09619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Expression Tree Decoding Strategy for Mathematical Equation  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yongliang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Nong%2C+Q">Qingpeng Nong</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zeqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yanna Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Weiming Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP-2023, camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09667" title="Abstract">arXiv:2310.09667</a> (replaced) [<a href="/pdf/2310.09667" title="Download PDF">pdf</a>, <a href="/ps/2310.09667" title="Download PostScript">ps</a>, <a href="/format/2310.09667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge-InversionNet: Enabling Efficient Inference of InversionNet on Edge  Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhepeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Putla%2C+I">Isaacshubhanand Putla</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weiwen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Youzuo Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP); Geophysics (physics.geo-ph)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09716" title="Abstract">arXiv:2310.09716</a> (replaced) [<a href="/pdf/2310.09716" title="Download PDF">pdf</a>, <a href="/format/2310.09716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Conversational Search: Large Language Model-Aided Informative  Query Rewriting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+F">Fanghua Ye</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shenghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Yilmaz%2C+E">Emine Yilmaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, accepted to EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09903" title="Abstract">arXiv:2310.09903</a> (replaced) [<a href="/pdf/2310.09903" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of feature selection performance for identification of best  effective technical indicators on stock market price prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Moodi%2C+F">Fatemeh Moodi</a>, 
<a href="/search/q-fin?searchtype=author&query=Jahangard-Rafsanjani%2C+A">Amir Jahangard-Rafsanjani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures,4 tables,45 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09978" title="Abstract">arXiv:2310.09978</a> (replaced) [<a href="/e-print/2310.09978" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chinese Painting Style Transfer Using Deep Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weijian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Y">Yanyang Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper is too old (written in 2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09982" title="Abstract">arXiv:2310.09982</a> (replaced) [<a href="/pdf/2310.09982" title="Download PDF">pdf</a>, <a href="/format/2310.09982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AP$n$P: A Less-constrained P$n$P Solver for Pose Estimation with Unknown  Anisotropic Scaling or Focal Lengths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jiaxin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Leutenegger%2C+S">Stefan Leutenegger</a>, 
<a href="/search/cs?searchtype=author&query=Kneip%2C+L">Laurent Kneip</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10046" title="Abstract">arXiv:2310.10046</a> (replaced) [<a href="/pdf/2310.10046" title="Download PDF">pdf</a>, <a href="/format/2310.10046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRANSOM: An Efficient Fault-Tolerant System for Training LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Baodong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingping Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kangyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yongqiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tieyao Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shigang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10378" title="Abstract">arXiv:2310.10378</a> (replaced) [<a href="/pdf/2310.10378" title="Download PDF">pdf</a>, <a href="/format/2310.10378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Lingual Consistency of Factual Knowledge in Multilingual Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Jirui Qi</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+R">Raquel Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Bisazza%2C+A">Arianna Bisazza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP2023 main conference. All code and data are released at <a href="https://github.com/Betswish/Cross-Lingual-Consistency">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10404" title="Abstract">arXiv:2310.10404</a> (replaced) [<a href="/pdf/2310.10404" title="Download PDF">pdf</a>, <a href="/format/2310.10404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Fine-grained Scene Graph Generation via Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kibum Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+K">Kanghoon Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+J">Jaehyeong Jeon</a>, 
<a href="/search/cs?searchtype=author&query=In%2C+Y">Yeonjun In</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+J">Jinyoung Moon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Donghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10449" title="Abstract">arXiv:2310.10449</a> (replaced) [<a href="/pdf/2310.10449" title="Download PDF">pdf</a>, <a href="/ps/2310.10449" title="Download PostScript">ps</a>, <a href="/format/2310.10449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Summarization Using Large Language Models: A Comparative Study of  MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basyal%2C+L">Lochan Basyal</a>, 
<a href="/search/cs?searchtype=author&query=Sanghvi%2C+M">Mihir Sanghvi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10520" title="Abstract">arXiv:2310.10520</a> (replaced) [<a href="/pdf/2310.10520" title="Download PDF">pdf</a>, <a href="/format/2310.10520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Parsing by Large Language Models for Intricate Updating  Strategies of Zero-Shot Dialogue State Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuxiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023 (Short Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10537" title="Abstract">arXiv:2310.10537</a> (replaced) [<a href="/pdf/2310.10537" title="Download PDF">pdf</a>, <a href="/format/2310.10537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Microscaling Data Formats for Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rouhani%2C+B+D">Bita Darvish Rouhani</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ritchie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=More%2C+A">Ankit More</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+M">Mathew Hall</a>, 
<a href="/search/cs?searchtype=author&query=Khodamoradi%2C+A">Alireza Khodamoradi</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Summer Deng</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+D">Dhruv Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Cornea%2C+M">Marius Cornea</a>, 
<a href="/search/cs?searchtype=author&query=Dellinger%2C+E">Eric Dellinger</a>, 
<a href="/search/cs?searchtype=author&query=Denolf%2C+K">Kristof Denolf</a>, 
<a href="/search/cs?searchtype=author&query=Dusan%2C+S">Stosic Dusan</a>, 
<a href="/search/cs?searchtype=author&query=Elango%2C+V">Venmugil Elango</a>, 
<a href="/search/cs?searchtype=author&query=Golub%2C+M">Maximilian Golub</a>, 
<a href="/search/cs?searchtype=author&query=Heinecke%2C+A">Alexander Heinecke</a>, 
<a href="/search/cs?searchtype=author&query=James-Roxby%2C+P">Phil James-Roxby</a>, 
<a href="/search/cs?searchtype=author&query=Jani%2C+D">Dharmesh Jani</a>, 
<a href="/search/cs?searchtype=author&query=Kolhe%2C+G">Gaurav Kolhe</a>, 
<a href="/search/cs?searchtype=author&query=Langhammer%2C+M">Martin Langhammer</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ada Li</a>, 
<a href="/search/cs?searchtype=author&query=Melnick%2C+L">Levi Melnick</a>, 
<a href="/search/cs?searchtype=author&query=Mesmakhosroshahi%2C+M">Maral Mesmakhosroshahi</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+A">Andres Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Schulte%2C+M">Michael Schulte</a>, 
<a href="/search/cs?searchtype=author&query=Shafipour%2C+R">Rasoul Shafipour</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+L">Lei Shao</a>, 
<a href="/search/cs?searchtype=author&query=Siu%2C+M">Michael Siu</a>, 
<a href="/search/cs?searchtype=author&query=Dubey%2C+P">Pradeep Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Micikevicius%2C+P">Paulius Micikevicius</a>, 
<a href="/search/cs?searchtype=author&query=Naumov%2C+M">Maxim Naumov</a>, 
<a href="/search/cs?searchtype=author&query=Verrilli%2C+C">Colin Verrilli</a>, 
<a href="/search/cs?searchtype=author&query=Wittig%2C+R">Ralph Wittig</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+E">Eric Chung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10608" title="Abstract">arXiv:2310.10608</a> (replaced) [<a href="/pdf/2310.10608" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality control using convolutional neural networks applied to samples  of very small size
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatzimichail%2C+R+A">Rallou A. Chatzimichail</a> (1), 
<a href="/search/cs?searchtype=author&query=Hatjimihail%2C+A+T">Aristides T. Hatjimihail</a> (1) ((1) Hellenic Complex Systems Laboratory, Drama, Greece)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Article: 21 pages, 5 figures, 8 tables. Appendix: 166 pages, 178 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10618" title="Abstract">arXiv:2310.10618</a> (replaced) [<a href="/pdf/2310.10618" title="Download PDF">pdf</a>, <a href="/format/2310.10618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpolatory $\mathcal{H}_2$-optimality Conditions for Structured  Linear Time-invariant Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mlinari%C4%87%2C+P">Petar Mlinari&#x107;</a>, 
<a href="/search/math?searchtype=author&query=Benner%2C+P">Peter Benner</a>, 
<a href="/search/math?searchtype=author&query=Gugercin%2C+S">Serkan Gugercin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10669" title="Abstract">arXiv:2310.10669</a> (replaced) [<a href="/pdf/2310.10669" title="Download PDF">pdf</a>, <a href="/format/2310.10669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbiased Watermark for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhengmian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xidong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yihan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10706" title="Abstract">arXiv:2310.10706</a> (replaced) [<a href="/pdf/2310.10706" title="Download PDF">pdf</a>, <a href="/format/2310.10706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation  through the Lens of News Headline Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zijian Ding</a>, 
<a href="/search/cs?searchtype=author&query=Smith-Renner%2C+A">Alison Smith-Renner</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tetreault%2C+J+R">Joel R. Tetreault</a>, 
<a href="/search/cs?searchtype=author&query=Jaimes%2C+A">Alejandro Jaimes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10780" title="Abstract">arXiv:2310.10780</a> (replaced) [<a href="/pdf/2310.10780" title="Download PDF">pdf</a>, <a href="/format/2310.10780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Poisoning Backdoor Attacks from a Statistical Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Ganghua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+X">Xun Xian</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasa%2C+J">Jayanth Srinivasa</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+A">Ashish Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+X">Xuan Bi</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+M">Mingyi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jie Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10907" title="Abstract">arXiv:2310.10907</a> (replaced) [<a href="/pdf/2310.10907" title="Download PDF">pdf</a>, <a href="/format/2310.10907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surrogate Active Subspaces for Jump-Discontinuous Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wycoff%2C+N">Nathan Wycoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> comments very welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10943" title="Abstract">arXiv:2310.10943</a> (replaced) [<a href="/pdf/2310.10943" title="Download PDF">pdf</a>, <a href="/format/2310.10943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reaching the Limit in Autonomous Racing: Optimal Control versus  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yunlong Song</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+A">Angel Romero</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+M">Matthias Mueller</a>, 
<a href="/search/cs?searchtype=author&query=Koltun%2C+V">Vladlen Koltun</a>, 
<a href="/search/cs?searchtype=author&query=Scaramuzza%2C+D">Davide Scaramuzza</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Science Robotics, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11029" title="Abstract">arXiv:2310.11029</a> (replaced) [<a href="/pdf/2310.11029" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Core Building Blocks: Next Gen Geo Spatial GPT Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+A">Ashley Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Dube%2C+S">Swaraj Dube</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11037" title="Abstract">arXiv:2310.11037</a> (replaced) [<a href="/pdf/2310.11037" title="Download PDF">pdf</a>, <a href="/ps/2310.11037" title="Download PostScript">ps</a>, <a href="/format/2310.11037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling for Remote Estimation of the Wiener Process over an Unreliable  Channel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jiayu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shroff%2C+N+B">Ness B. Shroff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Sigmetrics, will appear in ACM POMACS journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11102" title="Abstract">arXiv:2310.11102</a> (replaced) [<a href="/pdf/2310.11102" title="Download PDF">pdf</a>, <a href="/format/2310.11102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HGCVAE: Integrating Generative and Contrastive Learning for  Heterogeneous Graph Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yulan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhirui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Sheng Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Junchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fuzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11147" title="Abstract">arXiv:2310.11147</a> (replaced) [<a href="/pdf/2310.11147" title="Download PDF">pdf</a>, <a href="/format/2310.11147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering wall-shear stress dynamics from neural-network enhanced fluid  flow measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lagemann%2C+E">Esther Lagemann</a>, 
<a href="/search/physics?searchtype=author&query=Brunton%2C+S+L">Steven L. Brunton</a>, 
<a href="/search/physics?searchtype=author&query=Lagemann%2C+C">Christian Lagemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11211" title="Abstract">arXiv:2310.11211</a> (replaced) [<a href="/pdf/2310.11211" title="Download PDF">pdf</a>, <a href="/format/2310.11211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Fairness Surrogate Functions in Algorithmic Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanke Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhicong Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11270" title="Abstract">arXiv:2310.11270</a> (replaced) [<a href="/pdf/2310.11270" title="Download PDF">pdf</a>, <a href="/format/2310.11270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks for Recommendation: Reproducibility, Graph  Topology, and Node Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malitesta%2C+D">Daniele Malitesta</a>, 
<a href="/search/cs?searchtype=author&query=Pomo%2C+C">Claudio Pomo</a>, 
<a href="/search/cs?searchtype=author&query=Di+Noia%2C+T">Tommaso Di Noia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11349" title="Abstract">arXiv:2310.11349</a> (replaced) [<a href="/pdf/2310.11349" title="Download PDF">pdf</a>, <a href="/ps/2310.11349" title="Download PostScript">ps</a>, <a href="/format/2310.11349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A robust and high precision algorithm for elastic scattering problems  from cornered domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yao%2C+J">Jianan Yao</a>, 
<a href="/search/math?searchtype=author&query=Xie%2C+B">Baoling Xie</a>, 
<a href="/search/math?searchtype=author&query=Lai%2C+J">Jun Lai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11404" title="Abstract">arXiv:2310.11404</a> (replaced) [<a href="/pdf/2310.11404" title="Download PDF">pdf</a>, <a href="/format/2310.11404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesis of constrained robust feedback policies and model predictive  control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gramlich%2C+D">Dennis Gramlich</a>, 
<a href="/search/math?searchtype=author&query=Scherer%2C+C+W">Carsten W. Scherer</a>, 
<a href="/search/math?searchtype=author&query=H%C3%A4ring%2C+H">Hannah H&#xe4;ring</a>, 
<a href="/search/math?searchtype=author&query=Ebenbauer%2C+C">Christian Ebenbauer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of a contribution to be submitted to the European Control Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11431" title="Abstract">arXiv:2310.11431</a> (replaced) [<a href="/pdf/2310.11431" title="Download PDF">pdf</a>, <a href="/format/2310.11431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Interpretable Visual Features in Artificial and Biological  Neural Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Klindt%2C+D">David Klindt</a>, 
<a href="/search/stat?searchtype=author&query=Sanborn%2C+S">Sophia Sanborn</a>, 
<a href="/search/stat?searchtype=author&query=Acosta%2C+F">Francisco Acosta</a>, 
<a href="/search/stat?searchtype=author&query=Poitevin%2C+F">Fr&#xe9;d&#xe9;ric Poitevin</a>, 
<a href="/search/stat?searchtype=author&query=Miolane%2C+N">Nina Miolane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11440" title="Abstract">arXiv:2310.11440</a> (replaced) [<a href="/pdf/2310.11440" title="Download PDF">pdf</a>, <a href="/format/2310.11440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvalCrafter: Benchmarking and Evaluating Large Video Generation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaofang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cun%2C+X">Xiaodong Cun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuebo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haoxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+T">Tieyong Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+R">Raymond Chan</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report, Project page: <a href="https://evalcrafter.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11448" title="Abstract">arXiv:2310.11448</a> (replaced) [<a href="/pdf/2310.11448" title="Download PDF">pdf</a>, <a href="/format/2310.11448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4K4D: Real-Time 4D View Synthesis at 4K Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Sida Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haotong Lin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guangzhao He</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiaming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yujun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Hujun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaowei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://zju3dv.github.io/4k4d">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item305">Cross-lists</a></li>
<li><a href="#item337">Replacements</a></li>
</ul>
<small>[ total of 567 entries:  <b>1-567</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
