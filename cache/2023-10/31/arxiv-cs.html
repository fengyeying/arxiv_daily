<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Fri 27 Oct 23  to  Mon 30 Oct 23, announced Tue, 31 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item684">Cross-lists</a></li>
<li><a href="#item762">Replacements</a></li>
</ul>
<small>[ total of 1394 entries:  <b>1-1394</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Tue, 31 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18315" title="Abstract">arXiv:2310.18315</a> [<a href="/pdf/2310.18315" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Analysis of COVID-19 Ontologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bain%2C+D">Debanjali Bain</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+B">Biswanath Dutta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, accepted for publication in 17th International Conference on Metadata and Semantics Research (MTSR2023), University of Milano-Bicocca, Milan, Italy, October 23-27, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">This comprehensive study conducts an in-depth analysis of existing COVID-19
ontologies, scrutinizing their objectives, classifications, design
methodologies, and domain focal points. The study is conducted through a
dual-stage approach, commencing with a systematic review of relevant literature
and followed by an ontological assessment utilizing a parametric methodology.
Through this meticulous process, twenty-four COVID-19 Ontologies (CovOs) are
selected and examined. The findings highlight the scope, intended purpose,
granularity of ontology, modularity, formalism, vocabulary reuse, and extent of
domain coverage. The analysis reveals varying levels of formality in ontology
development, a prevalent preference for utilizing OWL as the representational
language, and diverse approaches to constructing class hierarchies within the
models. Noteworthy is the recurrent reuse of ontologies like OBO models (CIDO,
GO, etc.) alongside CODO. The METHONTOLOGY approach emerges as a favored design
methodology, often coupled with application-based or data-centric evaluation
methods. Our study provides valuable insights for the scientific community and
COVID-19 ontology developers, supplemented by comprehensive ontology metrics.
By meticulously evaluating and documenting COVID-19 information-driven
ontological models, this research offers a comparative cross-domain
perspective, shedding light on knowledge representation variations. The present
study significantly enhances understanding of CovOs, serving as a consolidated
resource for comparative analysis and future development, while also
pinpointing research gaps and domain emphases, thereby guiding the trajectory
of future ontological advancements.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18316" title="Abstract">arXiv:2310.18316</a> [<a href="/pdf/2310.18316" title="Download PDF">pdf</a>, <a href="/format/2310.18316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cognitive modeling and learning with sparse binary hypervectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhonghao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Following the general theoretical framework of VSA (Vector Symbolic
Architecture), a cognitive model with the use of sparse binary hypervectors is
proposed. In addition, learning algorithms are introduced to bootstrap the
model from incoming data stream, with much improved transparency and
efficiency. Mimicking human cognitive process, the training can be performed
online while inference is in session. Word-level embedding is re-visited with
such hypervectors, and further applications in the field of NLP (Natural
Language Processing) are explored.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18318" title="Abstract">arXiv:2310.18318</a> [<a href="/pdf/2310.18318" title="Download PDF">pdf</a>, <a href="/format/2310.18318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenCog Hyperon: A Framework for AGI at the Human Level and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goertzel%2C+B">Ben Goertzel</a>, 
<a href="/search/cs?searchtype=author&query=Bogdanov%2C+V">Vitaly Bogdanov</a>, 
<a href="/search/cs?searchtype=author&query=Duncan%2C+M">Michael Duncan</a>, 
<a href="/search/cs?searchtype=author&query=Duong%2C+D">Deborah Duong</a>, 
<a href="/search/cs?searchtype=author&query=Goertzel%2C+Z">Zarathustra Goertzel</a>, 
<a href="/search/cs?searchtype=author&query=Horlings%2C+J">Jan Horlings</a>, 
<a href="/search/cs?searchtype=author&query=Ikle%27%2C+M">Matthew Ikle&#x27;</a>, 
<a href="/search/cs?searchtype=author&query=Meredith%2C+L+G">Lucius Greg Meredith</a>, 
<a href="/search/cs?searchtype=author&query=Potapov%2C+A">Alexey Potapov</a>, 
<a href="/search/cs?searchtype=author&query=de+Senna%2C+A+L">Andre&#x27; Luiz de Senna</a>, 
<a href="/search/cs?searchtype=author&query=Suarez%2C+H+S+A">Hedra Seid Andres Suarez</a>, 
<a href="/search/cs?searchtype=author&query=Vandervorst%2C+A">Adam Vandervorst</a>, 
<a href="/search/cs?searchtype=author&query=Werko%2C+R">Robert Werko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">An introduction to the OpenCog Hyperon framework for Artificiai General
Intelligence is presented. Hyperon is a new, mostly from-the-ground-up
rewrite/redesign of the OpenCog AGI framework, based on similar conceptual and
cognitive principles to the previous OpenCog version, but incorporating a
variety of new ideas at the mathematical, software architecture and
AI-algorithm level. This review lightly summarizes: 1) some of the history
behind OpenCog and Hyperon, 2) the core structures and processes underlying
Hyperon as a software system, 3) the integration of this software system with
the SingularityNET ecosystem's decentralized infrastructure, 4) the cognitive
model(s) being experimentally pursued within Hyperon on the hopeful path to
advanced AGI, 5) the prospects seen for advanced aspects like reflective
self-modification and self-improvement of the codebase, 6) the tentative
development roadmap and various challenges expected to be faced, 7) the
thinking of the Hyperon team regarding how to guide this sort of work in a
beneficial direction ... and gives links and references for readers who wish to
delve further into any of these aspects.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18320" title="Abstract">arXiv:2310.18320</a> [<a href="/pdf/2310.18320" title="Download PDF">pdf</a>, <a href="/ps/2310.18320" title="Download PostScript">ps</a>, <a href="/format/2310.18320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI (r)evolution -- where are we heading? Thoughts about the future of  music and sound technologies in the era of deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bindi%2C+G">Giovanni Bindi</a>, 
<a href="/search/cs?searchtype=author&query=Demerl%C3%A9%2C+N">Nils Demerl&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Diaz%2C+R">Rodrigo Diaz</a>, 
<a href="/search/cs?searchtype=author&query=Genova%2C+D">David Genova</a>, 
<a href="/search/cs?searchtype=author&query=Golvet%2C+A">Ali&#xe9;nor Golvet</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+B">Ben Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiawen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lele Liu</a>, 
<a href="/search/cs?searchtype=author&query=Martos%2C+V">Vincent Martos</a>, 
<a href="/search/cs?searchtype=author&query=Nabi%2C+S">Sarah Nabi</a>, 
<a href="/search/cs?searchtype=author&query=Pelinski%2C+T">Teresa Pelinski</a>, 
<a href="/search/cs?searchtype=author&query=Renault%2C+L">Lenny Renault</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Saurjya Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Sarmento%2C+P">Pedro Sarmento</a>, 
<a href="/search/cs?searchtype=author&query=Vahidi%2C+C">Cyrus Vahidi</a>, 
<a href="/search/cs?searchtype=author&query=Wolstanholme%2C+L">Lewis Wolstanholme</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Roebel%2C+A">Axel Roebel</a>, 
<a href="/search/cs?searchtype=author&query=Bryan-Kinns%2C+N">Nick Bryan-Kinns</a>, 
<a href="/search/cs?searchtype=author&query=Giavitto%2C+J">Jean-Louis Giavitto</a>, 
<a href="/search/cs?searchtype=author&query=Barthet%2C+M">Mathieu Barthet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Artificial Intelligence (AI) technologies such as deep learning are evolving
very quickly bringing many changes to our everyday lives. To explore the future
impact and potential of AI in the field of music and sound technologies a
doctoral day was held between Queen Mary University of London (QMUL, UK) and
Sciences et Technologies de la Musique et du Son (STMS, France). Prompt
questions about current trends in AI and music were generated by academics from
QMUL and STMS. Students from the two institutions then debated these questions.
This report presents a summary of the student debates on the topics of: Data,
Impact, and the Environment; Responsible Innovation and Creative Practice;
Creativity and Bias; and From Tools to the Singularity. The students represent
the future generation of AI and music researchers. The academics represent the
incumbent establishment. The student debates reported here capture visions,
dreams, concerns, uncertainties, and contentious issues for the future of AI
and music as the establishment is rightfully challenged by the next generation.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18322" title="Abstract">arXiv:2310.18322</a> [<a href="/pdf/2310.18322" title="Download PDF">pdf</a>, <a href="/format/2310.18322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What&#x27;s Next in Affective Modeling? Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yongsatianchot%2C+N">Nutchanon Yongsatianchot</a>, 
<a href="/search/cs?searchtype=author&query=Thejll-Madsen%2C+T">Tobias Thejll-Madsen</a>, 
<a href="/search/cs?searchtype=author&query=Marsella%2C+S">Stacy Marsella</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 11th International Conference on Affective Computing and
  Intelligent Interaction Workshop and Demo (ACIIW) 2023 1-7
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLM) have recently been shown to perform well at
various tasks from language understanding, reasoning, storytelling, and
information search to theory of mind. In an extension of this work, we explore
the ability of GPT-4 to solve tasks related to emotion prediction. GPT-4
performs well across multiple emotion tasks; it can distinguish emotion
theories and come up with emotional stories. We show that by prompting GPT-4 to
identify key factors of an emotional experience, it is able to manipulate the
emotional intensity of its own stories. Furthermore, we explore GPT-4's ability
on reverse appraisals by asking it to predict either the goal, belief, or
emotion of a person using the other two. In general, GPT-4 can make the correct
inferences. We suggest that LLMs could play an important role in affective
modeling; however, they will not fully replace works that attempt to model the
mechanisms underlying emotion-related processes.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18323" title="Abstract">arXiv:2310.18323</a> [<a href="/pdf/2310.18323" title="Download PDF">pdf</a>, <a href="/format/2310.18323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of AdaBoost : Reconciling its views to better understand its  dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beja-Battais%2C+P">Perceval Beja-Battais</a> (CB)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Boosting methods have been introduced in the late 1980's. They were born
following the theoritical aspect of PAC learning. The main idea of boosting
methods is to combine weak learners to obtain a strong learner. The weak
learners are obtained iteratively by an heuristic which tries to correct the
mistakes of the previous weak learner. In 1995, Freund and Schapire [18]
introduced AdaBoost, a boosting algorithm that is still widely used today.
Since then, many views of the algorithm have been proposed to properly tame its
dynamics. In this paper, we will try to cover all the views that one can have
on AdaBoost. We will start with the original view of Freund and Schapire before
covering the different views and unify them with the same formalism. We hope
this paper will help the non-expert reader to better understand the dynamics of
AdaBoost and how the different views are equivalent and related to each other.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18324" title="Abstract">arXiv:2310.18324</a> [<a href="/pdf/2310.18324" title="Download PDF">pdf</a>, <a href="/ps/2310.18324" title="Download PostScript">ps</a>, <a href="/format/2310.18324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;A Nova Eletricidade: Aplica&#xe7;&#xf5;es, Riscos e Tend&#xea;ncias da IA  Moderna -- &quot;The New Electricity&quot;: Applications, Risks, and Trends in Current  AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bazzan%2C+A+L+C">Ana L.C. Bazzan</a>, 
<a href="/search/cs?searchtype=author&query=Tavares%2C+A+R">Anderson R. Tavares</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+A+G">Andr&#xe9; G. Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+C+R">Cl&#xe1;udio R. Jung</a>, 
<a href="/search/cs?searchtype=author&query=Scharcanski%2C+J">Jacob Scharcanski</a>, 
<a href="/search/cs?searchtype=author&query=Carbonera%2C+J+L">Joel Luis Carbonera</a>, 
<a href="/search/cs?searchtype=author&query=Lamb%2C+L+C">Lu&#xed;s C. Lamb</a>, 
<a href="/search/cs?searchtype=author&query=Recamonde-Mendoza%2C+M">Mariana Recamonde-Mendoza</a>, 
<a href="/search/cs?searchtype=author&query=da+Silveira%2C+T+L+T">Thiago L.T. da Silveira</a>, 
<a href="/search/cs?searchtype=author&query=Moreira%2C+V">Viviane Moreira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Portuguese
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">The thought-provoking analogy between AI and electricity, made by computer
scientist and entrepreneur Andrew Ng, summarizes the deep transformation that
recent advances in Artificial Intelligence (AI) have triggered in the world.
This chapter presents an overview of the ever-evolving landscape of AI, written
in Portuguese. With no intent to exhaust the subject, we explore the AI
applications that are redefining sectors of the economy, impacting society and
humanity. We analyze the risks that may come along with rapid technological
progress and future trends in AI, an area that is on the path to becoming a
general-purpose technology, just like electricity, which revolutionized society
in the 19th and 20th centuries.
<br />A provocativa compara\c{c}\~ao entre IA e eletricidade, feita pelo cientista
da computa\c{c}\~ao e empreendedor Andrew Ng, resume a profunda
transforma\c{c}\~ao que os recentes avan\c{c}os em Intelig\^encia Artificial
(IA) t\^em desencadeado no mundo. Este cap\'itulo apresenta uma vis\~ao geral
pela paisagem em constante evolu\c{c}\~ao da IA. Sem pretens\~oes de exaurir o
assunto, exploramos as aplica\c{c}\~oes que est\~ao redefinindo setores da
economia, impactando a sociedade e a humanidade. Analisamos os riscos que
acompanham o r\'apido progresso tecnol\'ogico e as tend\^encias futuras da IA,
\'area que trilha o caminho para se tornar uma tecnologia de prop\'osito geral,
assim como a eletricidade, que revolucionou a sociedade dos s\'eculos XIX e XX.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18326" title="Abstract">arXiv:2310.18326</a> [<a href="/pdf/2310.18326" title="Download PDF">pdf</a>, <a href="/format/2310.18326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Adaptive Bandit Experiments to Increase and Investigate Engagement  in Mental Health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+H">Harsh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tong Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiakai Shi</a>, 
<a href="/search/cs?searchtype=author&query=Musabirov%2C+I">Ilya Musabirov</a>, 
<a href="/search/cs?searchtype=author&query=Kornfield%2C+R">Rachel Kornfield</a>, 
<a href="/search/cs?searchtype=author&query=Meyerhoff%2C+J">Jonah Meyerhoff</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+A">Ananya Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Karr%2C+C">Chris Karr</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Theresa Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Mohr%2C+D">David Mohr</a>, 
<a href="/search/cs?searchtype=author&query=Rafferty%2C+A">Anna Rafferty</a>, 
<a href="/search/cs?searchtype=author&query=Villar%2C+S">Sofia Villar</a>, 
<a href="/search/cs?searchtype=author&query=Deliu%2C+N">Nina Deliu</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J+J">Joseph Jay Williams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Digital mental health (DMH) interventions, such as text-message-based lessons
and activities, offer immense potential for accessible mental health support.
While these interventions can be effective, real-world experimental testing can
further enhance their design and impact. Adaptive experimentation, utilizing
algorithms like Thompson Sampling for (contextual) multi-armed bandit (MAB)
problems, can lead to continuous improvement and personalization. However, it
remains unclear when these algorithms can simultaneously increase user
experience rewards and facilitate appropriate data collection for
social-behavioral scientists to analyze with sufficient statistical confidence.
Although a growing body of research addresses the practical and statistical
aspects of MAB and other adaptive algorithms, further exploration is needed to
assess their impact across diverse real-world contexts. This paper presents a
software system developed over two years that allows text-messaging
intervention components to be adapted using bandit and other algorithms while
collecting data for side-by-side comparison with traditional uniform random
non-adaptive experiments. We evaluate the system by deploying a
text-message-based DMH intervention to 1100 users, recruited through a large
mental health non-profit organization, and share the path forward for deploying
this system at scale. This system not only enables applications in mental
health but could also serve as a model testbed for adaptive experimentation
algorithms in other domains.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18329" title="Abstract">arXiv:2310.18329</a> [<a href="/pdf/2310.18329" title="Download PDF">pdf</a>, <a href="/format/2310.18329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Energy Efficiency in Deep Learning: Measurement, Prediction,  and Scoring across Edge Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+X">Xiaolong Tu</a>, 
<a href="/search/cs?searchtype=author&query=Mallik%2C+A">Anik Mallik</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dawei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kyungtae Han</a>, 
<a href="/search/cs?searchtype=author&query=Altintas%2C+O">Onur Altintas</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jiang Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by ACM/IEEE Symposium on Edge Computing (SEC '23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Performance (cs.PF)

</div>
<p class="mathjax">Today, deep learning optimization is primarily driven by research focused on
achieving high inference accuracy and reducing latency. However, the energy
efficiency aspect is often overlooked, possibly due to a lack of sustainability
mindset in the field and the absence of a holistic energy dataset. In this
paper, we conduct a threefold study, including energy measurement, prediction,
and efficiency scoring, with an objective to foster transparency in power and
energy consumption within deep learning across various edge devices. Firstly,
we present a detailed, first-of-its-kind measurement study that uncovers the
energy consumption characteristics of on-device deep learning. This study
results in the creation of three extensive energy datasets for edge devices,
covering a wide range of kernels, state-of-the-art DNN models, and popular AI
applications. Secondly, we design and implement the first kernel-level energy
predictors for edge devices based on our kernel-level energy dataset.
Evaluation results demonstrate the ability of our predictors to provide
consistent and accurate energy estimations on unseen DNN models. Lastly, we
introduce two scoring metrics, PCS and IECS, developed to convert complex power
and energy consumption data of an edge device into an easily understandable
manner for edge device end-users. We hope our work can help shift the mindset
of both end-users and the research community towards sustainability in edge
computing, a principle that drives our research. Find data, code, and more
up-to-date information at https://amai-gsu.github.io/DeepEn2023.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18330" title="Abstract">arXiv:2310.18330</a> [<a href="/pdf/2310.18330" title="Download PDF">pdf</a>, <a href="/format/2310.18330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Detecting Contextual Real-Time Toxicity for In-Game Chat
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zachary Yang</a>, 
<a href="/search/cs?searchtype=author&query=Grenan-Godbout%2C+N">Nicolas Grenan-Godbout</a>, 
<a href="/search/cs?searchtype=author&query=Rabbany%2C+R">Reihaneh Rabbany</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, 13 tables. arXiv admin note: text overlap with <a href="/abs/2305.12542">arXiv:2305.12542</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Real-time toxicity detection in online environments poses a significant
challenge, due to the increasing prevalence of social media and gaming
platforms. We introduce ToxBuster, a simple and scalable model that reliably
detects toxic content in real-time for a line of chat by including chat history
and metadata. ToxBuster consistently outperforms conventional toxicity models
across popular multiplayer games, including Rainbow Six Siege, For Honor, and
DOTA 2. We conduct an ablation study to assess the importance of each model
component and explore ToxBuster's transferability across the datasets.
Furthermore, we showcase ToxBuster's efficacy in post-game moderation,
successfully flagging 82.1% of chat-reported players at a precision level of
90.0%. Additionally, we show how an additional 6% of unreported toxic players
can be proactively moderated.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18331" title="Abstract">arXiv:2310.18331</a> [<a href="/pdf/2310.18331" title="Download PDF">pdf</a>, <a href="/format/2310.18331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AllTogether: Investigating the Efficacy of Spliced Prompt for Web  Navigation using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiarun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wentao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chunhong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, published at COLING2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have emerged as promising agents for web
navigation tasks, interpreting objectives and interacting with web pages.
However, the efficiency of spliced prompts for such tasks remains
underexplored. We introduces AllTogether, a standardized prompt template that
enhances task context representation, thereby improving LLMs' performance in
HTML-based web navigation. We evaluate the efficacy of this approach through
prompt learning and instruction finetuning based on open-source Llama-2 and
API-accessible GPT models. Our results reveal that models like GPT-4 outperform
smaller models in web navigation tasks. Additionally, we find that the length
of HTML snippet and history trajectory significantly influence performance, and
prior step-by-step instructions prove less effective than real-time
environmental feedback. Overall, we believe our work provides valuable insights
for future research in LLM-driven web agents.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18332" title="Abstract">arXiv:2310.18332</a> [<a href="/pdf/2310.18332" title="Download PDF">pdf</a>, <a href="/format/2310.18332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WordArt Designer: User-Driven Artistic Typography Synthesis using Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jun-Yan He</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingdong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+W">Wangmeng Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xianhui Lin</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+X">Xiaoyang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zengke Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yusen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yifeng Geng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xuansong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023, 10 pages, 11 figures, 1 table, the system is at <a href="https://www.modelscope.cn/studios/WordArt/WordArt">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)

</div>
<p class="mathjax">This paper introduces "WordArt Designer", a user-driven framework for
artistic typography synthesis, relying on Large Language Models (LLM). The
system incorporates four key modules: the "LLM Engine", "SemTypo", "StyTypo",
and "TexTypo" modules. 1) The "LLM Engine", empowered by LLM (e.g.,
GPT-3.5-turbo), interprets user inputs and generates actionable prompts for the
other modules, thereby transforming abstract concepts into tangible designs. 2)
The "SemTypo module" optimizes font designs using semantic concepts, striking a
balance between artistic transformation and readability. 3) Building on the
semantic layout provided by the "SemTypo module", the "StyTypo module" creates
smooth, refined images. 4) The "TexTypo module" further enhances the design's
aesthetics through texture rendering, enabling the generation of inventive
textured fonts. Notably, "WordArt Designer" highlights the fusion of generative
AI with artistic typography. Experience its capabilities on ModelScope:
https://www.modelscope.cn/studios/WordArt/WordArt.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18333" title="Abstract">arXiv:2310.18333</a> [<a href="/pdf/2310.18333" title="Download PDF">pdf</a>, <a href="/format/2310.18333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design-Inclusive Language Models for Responsible Information Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatrath%2C+V">Veronica Chatrath</a>, 
<a href="/search/cs?searchtype=author&query=Bamgbose%2C+O">Oluwanifemi Bamgbose</a>, 
<a href="/search/cs?searchtype=author&query=Raza%2C+S">Shaina Raza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As the use of large language models (LLMs) increases for everyday tasks,
appropriate safeguards must be in place to ensure unbiased and safe output.
Recent events highlight ethical concerns around conventionally trained LLMs,
leading to overall unsafe user experiences. This motivates the need for
responsible LLMs that are trained fairly, transparent to the public, and
regularly monitored after deployment. In this work, we introduce the
"Responsible Development of Language Models (ReDev)" framework to foster the
development of fair, safe, and robust LLMs for all users. We also present a
test suite of unique prompt types to assess LLMs on the aforementioned
elements, ensuring all generated responses are non-harmful and free from biased
content. Outputs from four state-of-the-art LLMs, OPT, GPT-3.5, GPT-4, and
LLaMA-2, are evaluated by our test suite, highlighting the importance of
considering fairness, safety, and robustness at every stage of the machine
learning pipeline, including data curation, training, and post-deployment.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18334" title="Abstract">arXiv:2310.18334</a> [<a href="/pdf/2310.18334" title="Download PDF">pdf</a>, <a href="/format/2310.18334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypersparse Traffic Matrix Construction using GraphBLAS on a DPU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bergeron%2C+W">William Bergeron</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+M">Michael Jones</a>, 
<a href="/search/cs?searchtype=author&query=Barber%2C+C">Chase Barber</a>, 
<a href="/search/cs?searchtype=author&query=DeYoung%2C+K">Kale DeYoung</a>, 
<a href="/search/cs?searchtype=author&query=Amariucai%2C+G">George Amariucai</a>, 
<a href="/search/cs?searchtype=author&query=Ernst%2C+K">Kaleb Ernst</a>, 
<a href="/search/cs?searchtype=author&query=Fleming%2C+N">Nathan Fleming</a>, 
<a href="/search/cs?searchtype=author&query=Michaleas%2C+P">Peter Michaleas</a>, 
<a href="/search/cs?searchtype=author&query=Pisharody%2C+S">Sandeep Pisharody</a>, 
<a href="/search/cs?searchtype=author&query=Wells%2C+N">Nathan Wells</a>, 
<a href="/search/cs?searchtype=author&query=Rosa%2C+A">Antonio Rosa</a>, 
<a href="/search/cs?searchtype=author&query=Vasserman%2C+E">Eugene Vasserman</a>, 
<a href="/search/cs?searchtype=author&query=Kepner%2C+J">Jeremy Kepner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Low-power small form factor data processing units (DPUs) enable offloading
and acceleration of a broad range of networking and security services. DPUs
have accelerated the transition to programmable networking by enabling the
replacement of FPGAs/ASICs in a wide range of network oriented devices. The
GraphBLAS sparse matrix graph open standard math library is well-suited for
constructing anonymized hypersparse traffic matrices of network traffic which
can enable a wide range of network analytics. This paper measures the
performance of the GraphBLAS on an ARM based NVIDIA DPU (BlueField 2) and, to
the best of our knowledge, represents the first reported GraphBLAS results on a
DPU and/or ARM based system. Anonymized hypersparse traffic matrices were
constructed at a rate of over 18 million packets per second.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18335" title="Abstract">arXiv:2310.18335</a> [<a href="/pdf/2310.18335" title="Download PDF">pdf</a>, <a href="/format/2310.18335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The hardware is the software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laydevant%2C+J">Jeremie Laydevant</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+L+G">Logan G. Wright</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=McMahon%2C+P+L">Peter L. McMahon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Human brains and bodies are not hardware running software: the hardware is
the software. We reason that because the microscopic physics of
artificial-intelligence hardware and of human biological "hardware" is
distinct, neuromorphic engineers need to be cautious (and yet also creative) in
how we take inspiration from biological intelligence. We should focus primarily
on principles and design ideas that respect -- and embrace -- the underlying
hardware physics of non-biological intelligent systems, rather than abstracting
it away. We see a major role for neuroscience in neuromorphic computing as
identifying the physics-agnostic principles of biological intelligence -- that
is the principles of biological intelligence that can be gainfully adapted and
applied to any physical hardware.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18336" title="Abstract">arXiv:2310.18336</a> [<a href="/pdf/2310.18336" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AITA Generating Moral Judgements of the Crowd with Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bsher%2C+O">Osama Bsher</a>, 
<a href="/search/cs?searchtype=author&query=Sabri%2C+A">Ameer Sabri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Morality is a fundamental aspect of human behavior and ethics, influencing
how we interact with each other and the world around us. When faced with a
moral dilemma, a person's ability to make clear moral judgments can be clouded.
Due to many factors such as personal biases, emotions and situational factors
people can find it difficult to decide their best course of action. The
AmITheAsshole (AITA) subreddit is a forum on the social media platform Reddit
that helps people get clarity and objectivity on their predicaments. In the
forum people post anecdotes about moral dilemmas they are facing in their
lives, seeking validation for their actions or advice on how to navigate the
situation from the community. The morality of the actions in each post is
classified based on the collective opinion of the community into mainly two
labels, "Not The Asshole" (NTA) and "You Are The Asshole" (YTA). This project
aims to generate comments with moral reasoning for stories with moral dilemmas
using the AITA subreddit as a dataset. While past literature has explored the
classification of posts into labels (Alhassan et al., 2022), the generation of
comments remains a novel and challenging task. It involves understanding the
complex social and ethical considerations in each situation. To address this
challenge, we will leverage the vast amount of data on the forum with the goal
of generating coherent comments that align with the norms and values of the
AITA community. In this endeavor, we aim to evaluate state-of-the-art seq2seq
text generation models for their ability to make moral judgments similarly to
humans, ultimately producing concise comments providing clear moral stances and
advice for the poster.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18337" title="Abstract">arXiv:2310.18337</a> [<a href="/pdf/2310.18337" title="Download PDF">pdf</a>, <a href="/format/2310.18337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing tensor product B&#xe9;zier surfaces for coincidence: A  comprehensive solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vlachkova%2C+K">Krassimira Vlachkova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">It is known that B\'{e}zier curves and surfaces may have multiple
representations by different control polygons. The polygons may have different
number of control points and may even be disjoint. Up to our knowledge,
Pekerman et al. (2005) were the first to address the problem of testing two
parametric polynomial curves for coincidence. Their approach is based on
reduction of the input curves into canonical irreducible form. They claimed
that their approach can be extended for testing tensor product surfaces but
gave no further detail. In this paper we develop a new technique and provide a
comprehensive solution to the problem of testing tensor product B\'ezier
surfaces for coincidence. In (Vlachkova, 2017) an algorithm for testing
B\'ezier curves was proposed based on subdivision. There a partial solution to
the problem of testing tensor product B\'ezier surfaces was presented. Namely,
the case where the irreducible surfaces are of same degree $(n,m)$, $n,m \in
\mathbb{N}$, was resolved under certain additional condition. The other cases
where one of the surfaces is of degree $(n,m)$ and the other is of degree
either $(n,n+m)$, or $(n+m,m)$, or $(n+m,n+m)$ remained open. We have
implemented our algorithm for testing tensor product B\'ezier surfaces for
coincidence using Mathematica package. Experimental results and their analysis
are presented.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18338" title="Abstract">arXiv:2310.18338</a> [<a href="/pdf/2310.18338" title="Download PDF">pdf</a>, <a href="/format/2310.18338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small Language Models Fine-tuned to Coordinate Larger Language Models  improve Complex Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Juneja%2C+G">Gurusha Juneja</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Subhabrata Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarti%2C+S">Soumen Chakrabarti</a>, 
<a href="/search/cs?searchtype=author&query=Manchanda%2C+S">Sunny Manchanda</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) prompted to generate chain-of-thought (CoT)
exhibit impressive reasoning capabilities. Recent attempts at prompt
decomposition toward solving complex, multi-step reasoning problems depend on
the ability of the LLM to simultaneously decompose and solve the problem. A
significant disadvantage is that foundational LLMs are typically not available
for fine-tuning, making adaptation computationally prohibitive. We believe (and
demonstrate) that problem decomposition and solution generation are distinct
capabilites, better addressed in separate modules, than by one monolithic LLM.
We introduce DaSLaM, which uses a decomposition generator to decompose complex
problems into subproblems that require fewer reasoning steps. These subproblems
are answered by a solver. We use a relatively small (13B parameters) LM as the
decomposition generator, which we train using policy gradient optimization to
interact with a solver LM (regarded as black-box) and guide it through
subproblems, thereby rendering our method solver-agnostic. Evaluation on
multiple different reasoning datasets reveal that with our method, a 175
billion parameter LM (text-davinci-003) can produce competitive or even better
performance, compared to its orders-of-magnitude larger successor, GPT-4.
Additionally, we show that DaSLaM is not limited by the solver's capabilities
as a function of scale; e.g., solver LMs with diverse sizes give significant
performance improvement with our solver-agnostic decomposition technique.
Exhaustive ablation studies evince the superiority of our modular finetuning
technique over exorbitantly large decomposer LLMs, based on prompting alone.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18339" title="Abstract">arXiv:2310.18339</a> [<a href="/pdf/2310.18339" title="Download PDF">pdf</a>, <a href="/format/2310.18339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for  Multi-task Medical Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qidong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuanshao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Derong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+F">Feng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yefeng Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The recent surge in the field of Large Language Models (LLMs) has gained
significant attention in numerous domains. In order to tailor an LLM to a
specific domain such as a web-based healthcare system, fine-tuning with domain
knowledge is necessary. However, two issues arise during fine-tuning LLMs for
medical applications. The first is the problem of task variety, where there are
numerous distinct tasks in real-world medical scenarios. This diversity often
results in suboptimal fine-tuning due to data imbalance and seesawing problems.
Additionally, the high cost of fine-tuning can be prohibitive, impeding the
application of LLMs. The large number of parameters in LLMs results in enormous
time and computational consumption during fine-tuning, which is difficult to
justify. To address these two issues simultaneously, we propose a novel
parameter-efficient fine-tuning framework for multi-task medical applications
called MOELoRA. The framework aims to capitalize on the benefits of both MOE
for multi-task learning and LoRA for parameter-efficient fine-tuning. To unify
MOE and LoRA, we devise multiple experts as the trainable parameters, where
each expert consists of a pair of low-rank matrices to maintain a small number
of trainable parameters. Additionally, we propose a task-motivated gate
function for all MOELoRA layers that can regulate the contributions of each
expert and generate distinct parameters for various tasks. To validate the
effectiveness and practicality of the proposed method, we conducted
comprehensive experiments on a public multi-task Chinese medical dataset. The
experimental results demonstrate that MOELoRA outperforms existing
parameter-efficient fine-tuning methods. The implementation is available online
for convenient reproduction of our experiments.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18340" title="Abstract">arXiv:2310.18340</a> [<a href="/pdf/2310.18340" title="Download PDF">pdf</a>, <a href="/format/2310.18340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Urban Region Profiling Meets Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yibo Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Haomin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Siru Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haodong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+R">Roger Zimmermann</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Urban region profiling from web-sourced data is of utmost importance for
urban planning and sustainable development. We are witnessing a rising trend of
LLMs for various fields, especially dealing with multi-modal data research such
as vision-language learning, where the text modality serves as a supplement
information for the image. Since textual modality has never been introduced
into modality combinations in urban region profiling, we aim to answer two
fundamental questions in this paper: i) Can textual modality enhance urban
region profiling? ii) and if so, in what ways and with regard to which aspects?
To answer the questions, we leverage the power of Large Language Models (LLMs)
and introduce the first-ever LLM-enhanced framework that integrates the
knowledge of textual modality into urban imagery profiling, named LLM-enhanced
Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP).
Specifically, it first generates a detailed textual description for each
satellite image by an open-source Image-to-Text LLM. Then, the model is trained
on the image-text pairs, seamlessly unifying natural language supervision for
urban visual representation learning, jointly with contrastive loss and
language modeling loss. Results on predicting three urban indicators in four
major Chinese metropolises demonstrate its superior performance, with an
average improvement of 6.1% on R^2 compared to the state-of-the-art methods.
Our code and the image-language dataset will be released upon paper
notification.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18341" title="Abstract">arXiv:2310.18341</a> [<a href="/pdf/2310.18341" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seowoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=D.%2C+M">M.D.</a>, 
<a href="/search/cs?searchtype=author&query=Youn%2C+J">Jiwon Youn</a>, 
<a href="/search/cs?searchtype=author&query=D.%2C+M+K+P">Mansu Kim Ph.D.</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S+H">Soon Ho Yoon</a>, 
<a href="/search/cs?searchtype=author&query=D%2C+M+D+P">M.D. Ph.D</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Purpose: Recent advancements in large language models (LLMs) have expanded
their capabilities in a multimodal fashion, potentially replicating the image
interpretation of human radiologists. This study aimed to develop open-source
multimodal large language model for interpreting chest X-ray images
(CXR-LLaVA). We also examined the effect of prompt engineering and model
parameters such as temperature and nucleus sampling.
<br />Materials and Methods: For training, we collected 659,287 publicly available
CXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset
1); 241,951 CXRs provided free-text radiology reports (dataset 2). After
pre-training the Resnet50 as an image encoder, the contrastive language-image
pre-training was used to align CXRs and corresponding radiographic
abnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using
dataset 2, which were refined using GPT-4, with generating various question
answering scenarios. The code can be found at
https://github.com/ECOFRI/CXR_LLaVA.
<br />Results: In the test set, we observed that the model's performance fluctuated
based on its parameters. On average, it achieved F1 score of 0.34 for five
pathologic findings (atelectasis, cardiomegaly, consolidation, edema, and
pleural effusion), which was improved to 0.46 through prompt engineering. In
the independent set, the model achieved an average F1 score of 0.30 for the
same pathologic findings. Notably, for the pediatric chest radiograph dataset,
which was unseen during training, the model differentiated abnormal radiographs
with an F1 score ranging from 0.84 to 0.85.
<br />Conclusion: CXR-LLaVA demonstrates promising potential in CXR interpretation.
Both prompt engineering and model parameter adjustments can play pivotal roles
in interpreting CXRs.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18342" title="Abstract">arXiv:2310.18342</a> [<a href="/pdf/2310.18342" title="Download PDF">pdf</a>, <a href="/format/2310.18342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIRACLE: Towards Personalized Dialogue Generation with Latent-Space  Multiple Personal Attribute Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhenyi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xiaoye Qu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+X">XianLing Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dangyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jixiong Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023 findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Personalized dialogue systems aim to endow the chatbot agent with more
anthropomorphic traits for human-like interactions. Previous approaches have
explored explicitly user profile modeling using text descriptions, implicit
derivation of user embeddings, or utilizing handicraft prompts for ChatGPT-like
models. However, textual personas are limited in describing multi-faceted
attributes (\emph{e.g.}, \emph{language style, inner character nuances}),
implicit embedding suffers from personality sparsity, and handicraft prompts
lack fine-grained and stable controllability. Hence, these approaches may
struggle with complex personalized dialogue generation tasks that require
generating controllable responses with multiple personal attributes. To this
end, we propose \textbf{\textsc{Miracle}}, a novel personalized dialogue
generation method through \textbf{M}ult\textbf{I}ple Pe\textbf{R}sonal
\textbf{A}ttributes \textbf{C}ontrol within \textbf{L}atent-Space
\textbf{E}nergy-based Models. ttributes \textbf{C}ontrol within
\textbf{L}atent-Space \textbf{E}nergy-based Models. Specifically, our approach
first disentangles complex personality into multi-faceted attributes.
Subsequently, we employ a conditional variational auto-encoder to align with
the dense personalized responses within a latent joint attribute space. We have
also tailored a dedicated energy function and customized the ordinary
differential equations sampling method to offer flexible attribute composition
and precise attribute control. Extensive experiments demonstrate that
\textsc{Miracle} outperforms several strong baselines in terms of personality
controllability and response generation quality. Our dataset and code are
available at \url{https://github.com/LZY-the-boys/MIRACLE}
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18343" title="Abstract">arXiv:2310.18343</a> [<a href="/pdf/2310.18343" title="Download PDF">pdf</a>, <a href="/format/2310.18343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PHD: Pixel-Based Language Modeling of Historical Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borenstein%2C+N">Nadav Borenstein</a>, 
<a href="/search/cs?searchtype=author&query=Rust%2C+P">Phillip Rust</a>, 
<a href="/search/cs?searchtype=author&query=Elliott%2C+D">Desmond Elliott</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the main conference of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The digitisation of historical documents has provided historians with
unprecedented research opportunities. Yet, the conventional approach to
analysing historical documents involves converting them from images to text
using OCR, a process that overlooks the potential benefits of treating them as
images and introduces high levels of noise. To bridge this gap, we take
advantage of recent advancements in pixel-based language models trained to
reconstruct masked patches of pixels instead of predicting token distributions.
Due to the scarcity of real historical scans, we propose a novel method for
generating synthetic scans to resemble real historical documents. We then
pre-train our model, PHD, on a combination of synthetic scans and real
historical newspapers from the 1700-1900 period. Through our experiments, we
demonstrate that PHD exhibits high proficiency in reconstructing masked image
patches and provide evidence of our model's noteworthy language understanding
capabilities. Notably, we successfully apply our model to a historical QA task,
highlighting its usefulness in this domain.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18344" title="Abstract">arXiv:2310.18344</a> [<a href="/pdf/2310.18344" title="Download PDF">pdf</a>, <a href="/format/2310.18344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chainpoll: A high efficacy method for LLM hallucination detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Friel%2C+R">Robert Friel</a>, 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+A">Atindriyo Sanyal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have experienced notable advancements in
generating coherent and contextually relevant responses. However,
hallucinations - incorrect or unfounded claims - are still prevalent, prompting
the creation of automated metrics to detect these in LLM outputs. Our
contributions include: introducing ChainPoll, an innovative hallucination
detection method that excels compared to its counterparts, and unveiling
RealHall, a refined collection of benchmark datasets to assess hallucination
detection metrics from recent studies. While creating RealHall, we assessed
tasks and datasets from previous hallucination detection studies and observed
that many are not suitable for the potent LLMs currently in use. Overcoming
this, we opted for four datasets challenging for modern LLMs and pertinent to
real-world scenarios. Using RealHall, we conducted a comprehensive comparison
of ChainPoll with numerous hallucination metrics from recent studies. Our
findings indicate that ChainPoll outperforms in all RealHall benchmarks,
achieving an overall AUROC of 0.781. This surpasses the next best theoretical
method by 11% and exceeds industry standards by over 23%. Additionally,
ChainPoll is cost-effective and offers greater transparency than other metrics.
We introduce two novel metrics to assess LLM hallucinations: Adherence and
Correctness. Adherence is relevant to Retrieval Augmented Generation workflows,
evaluating an LLM's analytical capabilities within given documents and
contexts. In contrast, Correctness identifies logical and reasoning errors.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18345" title="Abstract">arXiv:2310.18345</a> [<a href="/pdf/2310.18345" title="Download PDF">pdf</a>, <a href="/format/2310.18345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Semantic Processing Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+R">Rui Mao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kai He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xulang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jinjie Ni</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonglin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at Information Fusion, Volume 101, 2024, 101988, ISSN 1566-2535. The equal contribution mark is missed in the published version due to the publication policies. Please contact Prof. Erik Cambria for details
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Semantic processing is a fundamental research domain in computational
linguistics. In the era of powerful pre-trained language models and large
language models, the advancement of research in this domain appears to be
decelerating. However, the study of semantics is multi-dimensional in
linguistics. The research depth and breadth of computational semantic
processing can be largely improved with new technologies. In this survey, we
analyzed five semantic processing tasks, e.g., word sense disambiguation,
anaphora resolution, named entity recognition, concept extraction, and
subjectivity detection. We study relevant theoretical research in these fields,
advanced methods, and downstream applications. We connect the surveyed tasks
with downstream applications because this may inspire future scholars to fuse
these low-level semantic processing tasks with high-level natural language
processing tasks. The review of theoretical research may also inspire new tasks
and technologies in the semantic processing domain. Finally, we compare the
different semantic processing techniques and summarize their technical trends,
application trends, and future directions.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18347" title="Abstract">arXiv:2310.18347</a> [<a href="/pdf/2310.18347" title="Download PDF">pdf</a>, <a href="/format/2310.18347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRCA: Fitting Black-Box Large Language Models for Retrieval Question  Answering via Pluggable Reward-Driven Contextual Adapter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haoyan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhitao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianzong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Ning Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. (EMNLP2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The Retrieval Question Answering (ReQA) task employs the retrieval-augmented
framework, composed of a retriever and generator. The generator formulates the
answer based on the documents retrieved by the retriever. Incorporating Large
Language Models (LLMs) as generators is beneficial due to their advanced QA
capabilities, but they are typically too large to be fine-tuned with budget
constraints while some of them are only accessible via APIs. To tackle this
issue and further improve ReQA performance, we propose a trainable Pluggable
Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box.
Positioned between the retriever and generator in a Pluggable manner, PRCA
refines the retrieved information by operating in a token-autoregressive
strategy via maximizing rewards of the reinforcement learning phase. Our
experiments validate PRCA's effectiveness in enhancing ReQA performance on
three datasets by up to 20% improvement to fit black-box LLMs into existing
frameworks, demonstrating its considerable potential in the LLMs era.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18348" title="Abstract">arXiv:2310.18348</a> [<a href="/pdf/2310.18348" title="Download PDF">pdf</a>, <a href="/format/2310.18348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meaning Representations from Trajectories in Autoregressive Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T+Y">Tian Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Trager%2C+M">Matthew Trager</a>, 
<a href="/search/cs?searchtype=author&query=Achille%2C+A">Alessandro Achille</a>, 
<a href="/search/cs?searchtype=author&query=Perera%2C+P">Pramuditha Perera</a>, 
<a href="/search/cs?searchtype=author&query=Zancato%2C+L">Luca Zancato</a>, 
<a href="/search/cs?searchtype=author&query=Soatto%2C+S">Stefano Soatto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose to extract meaning representations from autoregressive language
models by considering the distribution of all possible trajectories extending
an input text. This strategy is prompt-free, does not require fine-tuning, and
is applicable to any pre-trained autoregressive model. Moreover, unlike
vector-based representations, distribution-based representations can also model
asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym
relations) by using algebraic operations between likelihood functions. These
ideas are grounded in distributional perspectives on semantics and are
connected to standard constructions in automata theory, but to our knowledge
they have not been applied to modern language models. We empirically show that
the representations obtained from large models align well with human
annotations, outperform other zero-shot and prompt-free methods on semantic
similarity tasks, and can be used to solve more complex entailment and
containment tasks that standard embeddings cannot handle. Finally, we extend
our method to represent data from different modalities (e.g., image and text)
using multimodal autoregressive models.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18349" title="Abstract">arXiv:2310.18349</a> [<a href="/pdf/2310.18349" title="Download PDF">pdf</a>, <a href="/format/2310.18349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Boundary Offset Prediction Network for Named Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+M">Minghao Tang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yongquan He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yongxiu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongbo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP 2023, 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Named entity recognition (NER) is a fundamental task in natural language
processing that aims to identify and classify named entities in text. However,
span-based methods for NER typically assign entity types to text spans,
resulting in an imbalanced sample space and neglecting the connections between
non-entity and entity spans. To address these issues, we propose a novel
approach for NER, named the Boundary Offset Prediction Network (BOPN), which
predicts the boundary offsets between candidate spans and their nearest entity
spans. By leveraging the guiding semantics of boundary offsets, BOPN
establishes connections between non-entity and entity spans, enabling
non-entity spans to function as additional positive samples for entity
detection. Furthermore, our method integrates entity type and span
representations to generate type-aware boundary offsets instead of using entity
types as detection targets. We conduct experiments on eight widely-used NER
datasets, and the results demonstrate that our proposed BOPN outperforms
previous state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18351" title="Abstract">arXiv:2310.18351</a> [<a href="/pdf/2310.18351" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioImage.IO Chatbot: A Personalized Assistant for BioImage Analysis  Augmented by Community Knowledge Base
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+W">Wanlu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Fuster-Barcel%C3%B3%2C+C">Caterina Fuster-Barcel&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1oz-Barrutia%2C+A">Arrate Mu&#xf1;oz-Barrutia</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wei Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">The rapidly expanding landscape of bioimage analysis tools presents a
navigational challenge for both experts and newcomers. Traditional search
methods often fall short in assisting users in this complex environment. To
address this, we introduce the BioImage.IO Chatbot, an AI-driven conversational
assistant tailored for the bioimage community. Built upon large language
models, this chatbot provides personalized, context-aware answers by
aggregating and interpreting information from diverse databases, tool-specific
documentation, and structured data sources. Enhanced by a community-contributed
knowledge base and fine-tuned retrieval methods, the BioImage.IO Chatbot offers
not just a personalized interaction but also a knowledge-enriched,
context-aware experience. It fundamentally transforms the way biologists,
bioimage analysts, and developers navigate and utilize advanced bioimage
analysis tools, setting a new standard for community-driven, accessible
scientific research.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18353" title="Abstract">arXiv:2310.18353</a> [<a href="/pdf/2310.18353" title="Download PDF">pdf</a>, <a href="/format/2310.18353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supporting Custom Instructions with the LLVM Compiler for RISC-V  Processor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%9Cnay%2C+E">Eymen &#xdc;nay</a>, 
<a href="/search/cs?searchtype=author&query=%C4%B0nan%2C+B">Bora &#x130;nan</a>, 
<a href="/search/cs?searchtype=author&query=Yi%C4%9Fit%2C+E">Emrecan Yi&#x11f;it</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Electronics and Communication Engineering B.Sc. Graduation Project. Source can be found in <a href="https://github.com/eymay/Senior-Design-Project">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">The rise of hardware accelerators with custom instructions necessitates
custom compiler backends supporting these accelerators. This study provides
detailed analyses of LLVM and its RISC-V backend, supplemented with case
studies providing end-to-end overview of the mentioned transformations.
<br />We discuss that instruction design should consider both hardware and software
design space. The necessary compiler modifications may mean that the
instruction is not well designed and need to be reconsidered. We discuss that
RISC-V standard extensions provide exemplary instructions that can guide
instruction designers.
<br />In this study, the process of adding a custom instruction to compiler is
split into two parts as Assembler support and pattern matching support. Without
pattern matching support, conventional software requires manual entries of
inline Assembly for the accelerator which is not scalable. While it is trivial
to add Assembler support regardless of the instruction semantics, pattern
matching support is on the contrary. Pattern matching support and choosing the
right stage for the modification, requires the knowledge of the internal
transformations in the compiler. This study delves deep into pattern matching
and presents multiple ways to approach the problem of pattern matching support.
It is discussed that depending on the pattern's complexity, higher level
transformations, e.g. IR level, can be more maintainable compared to
Instruction Selection phase.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18354" title="Abstract">arXiv:2310.18354</a> [<a href="/pdf/2310.18354" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Reinforcement Learning for Natural Language Processing, and  Applications in Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haozhu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huixue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yu Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sicheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hoetzlein%2C+R">Rama Hoetzlein</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Reinforcement learning (RL) has emerged as a powerful approach for tackling
complex medical decision-making problems such as treatment planning,
personalized medicine, and optimizing the scheduling of surgeries and
appointments. It has gained significant attention in the field of Natural
Language Processing (NLP) due to its ability to learn optimal strategies for
tasks such as dialogue systems, machine translation, and question-answering.
This paper presents a review of the RL techniques in NLP, highlighting key
advancements, challenges, and applications in healthcare. The review begins by
visualizing a roadmap of machine learning and its applications in healthcare.
And then it explores the integration of RL with NLP tasks. We examined dialogue
systems where RL enables the learning of conversational strategies, RL-based
machine translation models, question-answering systems, text summarization, and
information extraction. Additionally, ethical considerations and biases in
RL-NLP systems are addressed.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18355" title="Abstract">arXiv:2310.18355</a> [<a href="/pdf/2310.18355" title="Download PDF">pdf</a>, <a href="/format/2310.18355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Health Disparities through Generative AI Models: A Comparison Study  Using A Domain Specific large language model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bautista%2C+Y+J+P">Yohn Jairo Parra Bautista</a>, 
<a href="/search/cs?searchtype=author&query=Lima%2C+V">Vinicious Lima</a>, 
<a href="/search/cs?searchtype=author&query=Theran%2C+C">Carlos Theran</a>, 
<a href="/search/cs?searchtype=author&query=Alo%2C+R">Richard Alo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Health disparities are differences in health outcomes and access to
healthcare between different groups, including racial and ethnic minorities,
low-income people, and rural residents. An artificial intelligence (AI) program
called large language models (LLMs) can understand and generate human language,
improving health communication and reducing health disparities. There are many
challenges in using LLMs in human-doctor interaction, including the need for
diverse and representative data, privacy concerns, and collaboration between
healthcare providers and technology experts. We introduce the comparative
investigation of domain-specific large language models such as SciBERT with a
multi-purpose LLMs BERT. We used cosine similarity to analyze text queries
about health disparities in exam rooms when factors such as race are used
alone. Using text queries, SciBERT fails when it doesn't differentiate between
queries text: "race" alone and "perpetuates health disparities." We believe
clinicians can use generative AI to create a draft response when communicating
asynchronously with patients. However, careful attention must be paid to ensure
they are developed and implemented ethically and equitably.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18356" title="Abstract">arXiv:2310.18356</a> [<a href="/pdf/2310.18356" title="Download PDF">pdf</a>, <a href="/format/2310.18356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRAShear: Efficient Large Language Model Structured Pruning and  Knowledge Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+T">Tianyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+B">Badal Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Zharkov%2C+I">Ilya Zharkov</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Luming Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have transformed the landscape of artificial
intelligence, while their enormous size presents significant challenges in
terms of computational costs. We introduce LoRAShear, a novel efficient
approach to structurally prune LLMs and recover knowledge. Given general LLMs,
LoRAShear first creates the dependency graphs to discover minimally removal
structures and analyze the knowledge distribution. It then proceeds progressive
structured pruning on LoRA adaptors and enables inherent knowledge transfer to
better preserve the information in the redundant structures. To recover the
lost knowledge during pruning, LoRAShear meticulously studies and proposes a
dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow
down the performance gap to the full models. Numerical results demonstrate that
by only using one GPU within a couple of GPU days, LoRAShear effectively
reduced footprint of LLMs by 20% with only 1.0% performance degradation and
significantly outperforms state-of-the-arts. The source code will be available
at https://github.com/microsoft/lorashear.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18357" title="Abstract">arXiv:2310.18357</a> [<a href="/pdf/2310.18357" title="Download PDF">pdf</a>, <a href="/format/2310.18357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Large Language Models for Enhanced Product Descriptions in  eCommerce
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jianghong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J+N+A+Y">Jhalak Nilesh Acharya Yao Hong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kuang-chih Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+M">Musen Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, EMNLP2023 workshop, The 2023 Conference on Empirical Methods in Natural Language Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the dynamic field of eCommerce, the quality and comprehensiveness of
product descriptions are pivotal for enhancing search visibility and customer
engagement. Effective product descriptions can address the 'cold start'
problem, align with market trends, and ultimately lead to increased
click-through rates. Traditional methods for crafting these descriptions often
involve significant human effort and may lack both consistency and scalability.
This paper introduces a novel methodology for automating product description
generation using the LLAMA 2.0 7B language model. We train the model on a
dataset of authentic product descriptions from Walmart, one of the largest
eCommerce platforms. The model is then fine-tuned for domain-specific language
features and eCommerce nuances to enhance its utility in sales and user
engagement. We employ multiple evaluation metrics, including NDCG, customer
click-through rates, and human assessments, to validate the effectiveness of
our approach. Our findings reveal that the system is not only scalable but also
significantly reduces the human workload involved in creating product
descriptions. This study underscores the considerable potential of large
language models like LLAMA 2.0 7B in automating and optimizing various facets
of eCommerce platforms, offering significant business impact, including
improved search functionality and increased sales.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18358" title="Abstract">arXiv:2310.18358</a> [<a href="/pdf/2310.18358" title="Download PDF">pdf</a>, <a href="/format/2310.18358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Communication Theory Perspective on Prompting Engineering Methods for  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yuanfeng Song</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuanqin He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuefang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+H">Hanlin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Di Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haijun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lixin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qiang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The springing up of Large Language Models (LLMs) has shifted the community
from single-task-orientated natural language processing (NLP) research to a
holistic end-to-end multi-task learning paradigm. Along this line of research
endeavors in the area, LLM-based prompting methods have attracted much
attention, partially due to the technological advantages brought by prompt
engineering (PE) as well as the underlying NLP principles disclosed by various
prompting methods. Traditional supervised learning usually requires training a
model based on labeled data and then making predictions. In contrast, PE
methods directly use the powerful capabilities of existing LLMs (i.e., GPT-3
and GPT-4) via composing appropriate prompts, especially under few-shot or
zero-shot scenarios. Facing the abundance of studies related to the prompting
and the ever-evolving nature of this field, this article aims to (i) illustrate
a novel perspective to review existing PE methods, within the well-established
communication theory framework; (ii) facilitate a better/deeper understanding
of developing trends of existing PE methods used in four typical tasks; (iii)
shed light on promising research directions for future PE methods.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18359" title="Abstract">arXiv:2310.18359</a> [<a href="/pdf/2310.18359" title="Download PDF">pdf</a>, <a href="/format/2310.18359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeSIQ: Towards an Unbiased, Challenging Benchmark for Social  Intelligence Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xiao-Yu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan-Fang Li</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, EMNLP 2023 Long Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Social intelligence is essential for understanding and reasoning about human
expressions, intents and interactions. One representative benchmark for its
study is Social Intelligence Queries (Social-IQ), a dataset of multiple-choice
questions on videos of complex social interactions. We define a comprehensive
methodology to study the soundness of Social-IQ, as the soundness of such
benchmark datasets is crucial to the investigation of the underlying research
problem. Our analysis reveals that Social-IQ contains substantial biases, which
can be exploited by a moderately strong language model to learn spurious
correlations to achieve perfect performance without being given the context or
even the question. We introduce DeSIQ, a new challenging dataset, constructed
by applying simple perturbations to Social-IQ. Our empirical analysis shows
DeSIQ significantly reduces the biases in the original Social-IQ dataset.
Furthermore, we examine and shed light on the effect of model size, model
style, learning settings, commonsense knowledge, and multi-modality on the new
benchmark performance. Our new dataset, observations and findings open up
important research questions for the study of social intelligence.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18360" title="Abstract">arXiv:2310.18360</a> [<a href="/pdf/2310.18360" title="Download PDF">pdf</a>, <a href="/format/2310.18360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading  Comprehension Shortcut Triggers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levy%2C+M">Mosh Levy</a>, 
<a href="/search/cs?searchtype=author&query=Ravfogel%2C+S">Shauli Ravfogel</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent applications of LLMs in Machine Reading Comprehension (MRC) systems
have shown impressive results, but the use of shortcuts, mechanisms triggered
by features spuriously correlated to the true label, has emerged as a potential
threat to their reliability. We analyze the problem from two angles: LLMs as
editors, guided to edit text to mislead LLMs; and LLMs as readers, who answer
questions based on the edited text. We introduce a framework that guides an
editor to add potential shortcuts-triggers to samples. Using GPT4 as the
editor, we find it can successfully edit trigger shortcut in samples that fool
LLMs. Analysing LLMs as readers, we observe that even capable LLMs can be
deceived using shortcut knowledge. Strikingly, we discover that GPT4 can be
deceived by its own edits (15% drop in F1). Our findings highlight inherent
vulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a
curated dataset generated by our framework for future research.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18361" title="Abstract">arXiv:2310.18361</a> [<a href="/pdf/2310.18361" title="Download PDF">pdf</a>, <a href="/format/2310.18361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clinical Decision Support System for Unani Medicine Practitioners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sultan%2C+H">Haider Sultan</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+H+F">Hafiza Farwa Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Fatima%2C+N">Noor Fatima</a>, 
<a href="/search/cs?searchtype=author&query=Nadeem%2C+M">Marriyam Nadeem</a>, 
<a href="/search/cs?searchtype=author&query=Waheed%2C+T">Talha Waheed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 59 pages, 11 figures, Computer Science Bachelor's Thesis on use of Artificial Intelligence in Clinical Decision Support System for Unani Medicines
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Like other fields of Traditional Medicines, Unani Medicines have been found
as an effective medical practice for ages. It is still widely used in the
subcontinent, particularly in Pakistan and India. However, Unani Medicines
Practitioners are lacking modern IT applications in their everyday clinical
practices. An Online Clinical Decision Support System may address this
challenge to assist apprentice Unani Medicines practitioners in their
diagnostic processes. The proposed system provides a web-based interface to
enter the patient's symptoms, which are then automatically analyzed by our
system to generate a list of probable diseases. The system allows practitioners
to choose the most likely disease and inform patients about the associated
treatment options remotely. The system consists of three modules: an Online
Clinical Decision Support System, an Artificial Intelligence Inference Engine,
and a comprehensive Unani Medicines Database. The system employs advanced AI
techniques such as Decision Trees, Deep Learning, and Natural Language
Processing. For system development, the project team used a technology stack
that includes React, FastAPI, and MySQL. Data and functionality of the
application is exposed using APIs for integration and extension with similar
domain applications. The novelty of the project is that it addresses the
challenge of diagnosing diseases accurately and efficiently in the context of
Unani Medicines principles. By leveraging the power of technology, the proposed
Clinical Decision Support System has the potential to ease access to healthcare
services and information, reduce cost, boost practitioner and patient
satisfaction, improve speed and accuracy of the diagnostic process, and provide
effective treatments remotely. The application will be useful for Unani
Medicines Practitioners, Patients, Government Drug Regulators, Software
Developers, and Medical Researchers.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18362" title="Abstract">arXiv:2310.18362</a> [<a href="/pdf/2310.18362" title="Download PDF">pdf</a>, <a href="/ps/2310.18362" title="Download PostScript">ps</a>, <a href="/format/2310.18362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoK: Memorization in General-Purpose Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hartmann%2C+V">Valentin Hartmann</a>, 
<a href="/search/cs?searchtype=author&query=Suri%2C+A">Anshuman Suri</a>, 
<a href="/search/cs?searchtype=author&query=Bindschaedler%2C+V">Vincent Bindschaedler</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+D">David Evans</a>, 
<a href="/search/cs?searchtype=author&query=Tople%2C+S">Shruti Tople</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) are advancing at a remarkable pace, with myriad
applications under development. Unlike most earlier machine learning models,
they are no longer built for one specific application but are designed to excel
in a wide range of tasks. A major part of this success is due to their huge
training datasets and the unprecedented number of model parameters, which allow
them to memorize large amounts of information contained in the training data.
This memorization goes beyond mere language, and encompasses information only
present in a few documents. This is often desirable since it is necessary for
performing tasks such as question answering, and therefore an important part of
learning, but also brings a whole array of issues, from privacy and security to
copyright and beyond. LLMs can memorize short secrets in the training data, but
can also memorize concepts like facts or writing styles that can be expressed
in text in many different ways. We propose a taxonomy for memorization in LLMs
that covers verbatim text, facts, ideas and algorithms, writing styles,
distributional properties, and alignment goals. We describe the implications of
each type of memorization - both positive and negative - for model performance,
privacy, security and confidentiality, copyright, and auditing, and ways to
detect and prevent memorization. We further highlight the challenges that arise
from the predominant way of defining memorization with respect to model
behavior instead of model weights, due to LLM-specific phenomena such as
reasoning capabilities or differences between decoding algorithms. Throughout
the paper, we describe potential risks and opportunities arising from
memorization in LLMs that we hope will motivate new research directions.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18363" title="Abstract">arXiv:2310.18363</a> [<a href="/pdf/2310.18363" title="Download PDF">pdf</a>, <a href="/format/2310.18363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Contextualized Real-Time Multimodal Emotion Recognition for  Conversational Agents using Graph Convolutional Networks in Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+F+A">Fathima Abdul Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+G">Guang Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages (4 main + 1 reference), 2 figures. Submitted to IEEE FG2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Owing to the recent developments in Generative Artificial Intelligence
(GenAI) and Large Language Models (LLM), conversational agents are becoming
increasingly popular and accepted. They provide a human touch by interacting in
ways familiar to us and by providing support as virtual companions. Therefore,
it is important to understand the user's emotions in order to respond
considerately. Compared to the standard problem of emotion recognition,
conversational agents face an additional constraint in that recognition must be
real-time. Studies on model architectures using audio, visual, and textual
modalities have mainly focused on emotion classification using full video
sequences that do not provide online features. In this work, we present a novel
paradigm for contextualized Emotion Recognition using Graph Convolutional
Network with Reinforcement Learning (conER-GRL). Conversations are partitioned
into smaller groups of utterances for effective extraction of contextual
information. The system uses Gated Recurrent Units (GRU) to extract multimodal
features from these groups of utterances. More importantly, Graph Convolutional
Networks (GCN) and Reinforcement Learning (RL) agents are cascade trained to
capture the complex dependencies of emotion features in interactive scenarios.
Comparing the results of the conER-GRL model with other state-of-the-art models
on the benchmark dataset IEMOCAP demonstrates the advantageous capabilities of
the conER-GRL architecture in recognizing emotions in real-time from multimodal
conversational signals.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18364" title="Abstract">arXiv:2310.18364</a> [<a href="/pdf/2310.18364" title="Download PDF">pdf</a>, <a href="/format/2310.18364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Heuristic to Analytic: Cognitively Motivated Strategies for  Coherent Physical Commonsense Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Storks%2C+S">Shane Storks</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+F">Fengyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+S">Sungryull Sohn</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Moontae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Honglak Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+J">Joyce Chai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pre-trained language models (PLMs) have shown impressive performance in
various language tasks. However, they are prone to spurious correlations, and
often generate illusory information. In real-world applications, PLMs should
justify decisions with formalized, coherent reasoning chains, but this
challenge remains under-explored. Cognitive psychology theorizes that humans
are capable of utilizing fast and intuitive heuristic thinking to make
decisions based on past experience, then rationalizing the decisions through
slower and deliberative analytic reasoning. We incorporate these interlinked
dual processes in fine-tuning and in-context learning with PLMs, applying them
to two language understanding tasks that require coherent physical commonsense
reasoning. We show that our proposed Heuristic-Analytic Reasoning (HAR)
strategies drastically improve the coherence of rationalizations for model
decisions, yielding state-of-the-art results on Tiered Reasoning for Intuitive
Physics (TRIP). We also find that this improved coherence is a direct result of
more faithful attention to relevant language context in each step of reasoning.
Our findings suggest that human-like reasoning strategies can effectively
improve the coherence and reliability of PLM reasoning.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18365" title="Abstract">arXiv:2310.18365</a> [<a href="/pdf/2310.18365" title="Download PDF">pdf</a>, <a href="/format/2310.18365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using GPT-4 to Augment Unbalanced Data for Automatic Scoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+L">Luyang Fang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Gyeong-Geon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+X">Xiaoming Zhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Machine learning-based automatic scoring can be challenging if students'
responses are unbalanced across scoring categories, as it introduces
uncertainty in the machine training process. To meet this challenge, we
introduce a novel text data augmentation framework leveraging GPT-4, a
generative large language model, specifically tailored for unbalanced datasets
in automatic scoring. Our experimental dataset comprised student written
responses to two science items. We crafted prompts for GPT-4 to generate
responses resembling student written answers, particularly for the minority
scoring classes, to augment the data. We then finetuned DistillBERT for
automatic scoring based on the augmented and original datasets. Model
performance was assessed using accuracy, precision, recall, and F1 metrics. Our
findings revealed that incorporating GPT-4-augmented data remarkedly improved
model performance, particularly for precision, recall, and F1 scores.
Interestingly, the extent of improvement varied depending on the specific
dataset and the proportion of augmented data used. Notably, we found that a
varying amount of augmented data (5\%-40\%) was needed to obtain stable
improvement for automatic scoring. We also compared the accuracies of models
trained with GPT-4 augmented data to those trained with additional
student-written responses. Results suggest that the GPT-4 augmented scoring
models outperform or match the models trained with student-written augmented
data. This research underscores the potential and effectiveness of data
augmentation techniques utilizing generative large language models--GPT-4 in
addressing unbalanced datasets within automated assessment.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18366" title="Abstract">arXiv:2310.18366</a> [<a href="/pdf/2310.18366" title="Download PDF">pdf</a>, <a href="/format/2310.18366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multilingual Virtual Guide for Self-Attachment Technique
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Law%2C+A+J">Alicia Jiayun Law</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Ruoyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Alazraki%2C+L">Lisa Alazraki</a>, 
<a href="/search/cs?searchtype=author&query=Gopalan%2C+A">Anandha Gopalan</a>, 
<a href="/search/cs?searchtype=author&query=Polydorou%2C+N">Neophytos Polydorou</a>, 
<a href="/search/cs?searchtype=author&query=Edalat%2C+A">Abbas Edalat</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE 4th International Conference on Cognitive Machine
  Intelligence (CogMI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we propose a computational framework that leverages existing
out-of-language data to create a conversational agent for the delivery of
Self-Attachment Technique (SAT) in Mandarin. Our framework does not require
large-scale human translations, yet it achieves a comparable performance whilst
also maintaining safety and reliability. We propose two different methods of
augmenting available response data through empathetic rewriting. We evaluate
our chatbot against a previous, English-only SAT chatbot through non-clinical
human trials (N=42), each lasting five days, and quantitatively show that we
are able to attain a comparable level of performance to the English SAT
chatbot. We provide qualitative analysis on the limitations of our study and
suggestions with the aim of guiding future improvements.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18368" title="Abstract">arXiv:2310.18368</a> [<a href="/pdf/2310.18368" title="Download PDF">pdf</a>, <a href="/format/2310.18368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Muslim-Violence Bias Persists in Debiased GPT Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hemmatian%2C+B">Babak Hemmatian</a>, 
<a href="/search/cs?searchtype=author&query=Baltaji%2C+R">Razan Baltaji</a>, 
<a href="/search/cs?searchtype=author&query=Varshney%2C+L+R">Lav R. Varshney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages, 2 figures. This work will be presented at MusIML neurips workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Abid et al. (2021) showed a tendency in GPT-3 to generate violent completions
when prompted about Muslims, compared with other religions. Two pre-registered
replication attempts found few violent completions and only the weakest
anti-Muslim bias in the Instruct version, fine-tuned to eliminate biased and
toxic outputs. However, more pre-registered experiments showed that using
common names associated with the religions in prompts increases several-fold
the rate of violent completions, revealing a highly significant second-order
bias against Muslims. Our content analysis revealed religion-specific violent
themes containing highly offensive ideas regardless of prompt format.
Replications with ChatGPT suggest that any effects of GPT-3's de-biasing have
disappeared with continued model development, as this newer model showed both a
strong Muslim-violence bias and rates of violent completions closer to Abid et
al. (2021). Our results show the need for continual de-biasing of models in
ways that address higher-order associations.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18369" title="Abstract">arXiv:2310.18369</a> [<a href="/pdf/2310.18369" title="Download PDF">pdf</a>, <a href="/format/2310.18369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Apollo: Zero-shot MultiModal Reasoning with Multiple Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben-David%2C+D">Daniela Ben-David</a>, 
<a href="/search/cs?searchtype=author&query=Paz-Argaman%2C+T">Tzuf Paz-Argaman</a>, 
<a href="/search/cs?searchtype=author&query=Tsarfaty%2C+R">Reut Tsarfaty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> GitHub: <a href="https://github.com/danielabd/Apollo-Cap">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose a modular framework that leverages the expertise of different
foundation models over different modalities and domains in order to perform a
single, complex, multi-modal task, without relying on prompt engineering or
otherwise tailor-made multi-modal training. Our approach enables decentralized
command execution and allows each model to both contribute and benefit from the
expertise of the other models. Our method can be extended to a variety of
foundation models (including audio and vision), above and beyond only language
models, as it does not depend on prompts. We demonstrate our approach on two
tasks. On the well-known task of stylized image captioning, our experiments
show that our approach outperforms semi-supervised state-of-the-art models,
while being zero-shot and avoiding costly training, data collection, and prompt
engineering. We further demonstrate this method on a novel task, audio-aware
image captioning, in which an image and audio are given and the task is to
generate text that describes the image within the context of the provided
audio. Our code is available on GitHub.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18370" title="Abstract">arXiv:2310.18370</a> [<a href="/pdf/2310.18370" title="Download PDF">pdf</a>, <a href="/format/2310.18370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Boolean satisfiability problem heuristic strategy: Minimal Positive  Negative Product Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Menghui Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This study presents a novel heuristic algorithm called the "Minimal Positive
Negative Product Strategy" to guide the CDCL algorithm in solving the Boolean
satisfiability problem. It provides a mathematical explanation for the
superiority of this algorithm over widely used heuristics such as the Dynamic
Largest Individual Sum (DLIS) and the Variable State Independent Decaying Sum
(VSIDS). Experimental results further confirm the effectiveness of this
heuristic strategy in problem-solving.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18371" title="Abstract">arXiv:2310.18371</a> [<a href="/pdf/2310.18371" title="Download PDF">pdf</a>, <a href="/ps/2310.18371" title="Download PostScript">ps</a>, <a href="/format/2310.18371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Ability Transfer for Question Decomposition in Complex QA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=V%2C+V">Venktesh V</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Sourangshu Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+A">Avishek Anand</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Answering complex questions is a challenging task that requires question
decomposition and multistep reasoning for arriving at the solution. While
existing supervised and unsupervised approaches are specialized to a certain
task and involve training, recently proposed prompt-based approaches offer
generalizable solutions to tackle a wide variety of complex question-answering
(QA) tasks. However, existing prompt-based approaches that are effective for
complex QA tasks involve expensive hand annotations from experts in the form of
rationales and are not generalizable to newer complex QA scenarios and tasks.
We propose, icat (In-Context Ability Transfer) which induces reasoning
capabilities in LLMs without any LLM fine-tuning or manual annotation of
in-context samples. We transfer the ability to decompose complex questions to
simpler questions or generate step-by-step rationales to LLMs, by careful
selection from available data sources of related tasks. We also propose an
automated uncertainty-aware exemplar selection approach for selecting examples
from transfer data sources. Finally, we conduct large-scale experiments on a
variety of complex QA tasks involving numerical reasoning, compositional
complex QA, and heterogeneous complex QA which require decomposed reasoning. We
show that ICAT convincingly outperforms existing prompt-based solutions without
involving any model training, showcasing the benefits of re-using existing
abilities.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18373" title="Abstract">arXiv:2310.18373</a> [<a href="/pdf/2310.18373" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Grade Short-answer Reading Comprehension Questions :  Foundational Literacy Assessment in LMICs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henkel%2C+O">Owen Henkel</a>, 
<a href="/search/cs?searchtype=author&query=Hills%2C+L">Libby Hills</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+B">Bill Roberts</a>, 
<a href="/search/cs?searchtype=author&query=McGrane%2C+J">Joshua McGrane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents emerging evidence of using generative large language
models (i.e., GPT-4) to reliably evaluate short-answer reading comprehension
questions. Specifically, we explore how various configurations of generative
(LLMs) are able to evaluate student responses from a new dataset, drawn from a
battery of reading assessments conducted with over 150 students in Ghana. As
this dataset is novel and hence not used in training runs of GPT, it offers an
opportunity to test for domain shift and evaluate the generalizability of
generative LLMs, which are predominantly designed and trained on data from
high-income North American countries. We found that GPT-4, with minimal prompt
engineering performed extremely well on evaluating the novel dataset (Quadratic
Weighted Kappa 0.923, F1 0.88), substantially outperforming transfer-learning
based approaches, and even exceeding expert human raters (Quadratic Weighted
Kappa 0.915, F1 0.87). To the best of our knowledge, our work is the first to
empirically evaluate the performance of generative LLMs on short-answer reading
comprehension questions, using real student data, and suggests that generative
LLMs have the potential to reliably evaluate foundational literacy. Currently
the assessment of formative literacy and numeracy is infrequent in many low and
middle-income countries (LMICs) due to the cost and operational complexities of
conducting them at scale. Automating the grading process for reading assessment
could enable wider usage, and in turn improve decision-making regarding
curricula, school management, and teaching practice at the classroom level.
Importantly, in contrast transfer learning based approaches, generative LLMs
generalize well and the technical barriers to their use are low, making them
more feasible to implement and scale in lower resource educational contexts.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18375" title="Abstract">arXiv:2310.18375</a> [<a href="/pdf/2310.18375" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMOS-based Single-Cycle In-Memory XOR/XNOR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+S">Shamiul Alam</a>, 
<a href="/search/cs?searchtype=author&query=Hutchins%2C+J">Jack Hutchins</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+N">Nikhil Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Asifuzzaman%2C+K">Kazi Asifuzzaman</a>, 
<a href="/search/cs?searchtype=author&query=Aziz%2C+A">Ahmedullah Aziz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Big data applications are on the rise, and so is the number of data centers.
The ever-increasing massive data pool needs to be periodically backed up in a
secure environment. Moreover, a massive amount of securely backed-up data is
required for training binary convolutional neural networks for image
classification. XOR and XNOR operations are essential for large-scale data copy
verification, encryption, and classification algorithms. The disproportionate
speed of existing compute and memory units makes the von Neumann architecture
inefficient to perform these Boolean operations. Compute-in-memory (CiM) has
proved to be an optimum approach for such bulk computations. The existing
CiM-based XOR/XNOR techniques either require multiple cycles for computing or
add to the complexity of the fabrication process. Here, we propose a CMOS-based
hardware topology for single-cycle in-memory XOR/XNOR operations. Our design
provides at least 2 times improvement in the latency compared with other
existing CMOS-compatible solutions. We verify the proposed system through
circuit/system-level simulations and evaluate its robustness using a 5000-point
Monte Carlo variation analysis. This all-CMOS design paves the way for
practical implementation of CiM XOR/XNOR at scaled technology nodes.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18376" title="Abstract">arXiv:2310.18376</a> [<a href="/pdf/2310.18376" title="Download PDF">pdf</a>, <a href="/format/2310.18376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bazaga%2C+A">Adri&#xe1;n Bazaga</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Micklem%2C+G">Gos Micklem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, there has been growing interest in text-to-SQL translation,
which is the task of converting natural language questions into executable SQL
queries. This technology is important for its potential to democratize data
extraction from databases. However, some of its key hurdles include domain
generalisation, which is the ability to adapt to previously unseen databases,
and alignment of natural language questions with the corresponding SQL queries.
To overcome these challenges, we introduce SQLformer, a novel Transformer
architecture specifically crafted to perform text-to-SQL translation tasks. Our
model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive
way, incorporating structural inductive bias in the encoder and decoder layers.
This bias, guided by database table and column selection, aids the decoder in
generating SQL query ASTs represented as graphs in a Breadth-First Search
canonical order. Comprehensive experiments illustrate the state-of-the-art
performance of SQLformer in the challenging text-to-SQL Spider benchmark. Our
implementation is available at https://github.com/AdrianBZG/SQLformer
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18378" title="Abstract">arXiv:2310.18378</a> [<a href="/pdf/2310.18378" title="Download PDF">pdf</a>, <a href="/format/2310.18378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ontology Revision based on Pre-trained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Q">Qiu Ji</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yuxin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaye Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Site Li</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jianjie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Songtao Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Ontology revision aims to seamlessly incorporate new information into an
existing ontology and plays a crucial role in tasks such as ontology evolution,
ontology maintenance, and ontology alignment. Similar to repair single
ontologies, resolving logical incoherence in the task of ontology revision is
also important and meaningful since incoherence is a main potential factor to
cause inconsistency and reasoning with an inconsistent ontology will obtain
meaningless answers. To deal with this problem, various ontology revision
methods have been proposed to define revision operators and design ranking
strategies for axioms in an ontology. However, they rarely consider axiom
semantics which provides important information to differentiate axioms. On the
other hand, pre-trained models can be utilized to encode axiom semantics, and
have been widely applied in many natural language processing tasks and
ontology-related ones in recent years. Therefore, in this paper, we define four
scoring functions to rank axioms based on a pre-trained model by considering
various information from a rebuttal ontology and its corresponding reliable
ontology. Based on such a scoring function, we propose an ontology revision
algorithm to deal with unsatisfiable concepts at once. If it is hard to resolve
all unsatisfiable concepts in a rebuttal ontology together, an adapted revision
algorithm is designed to deal with them group by group. We conduct experiments
over 19 ontology pairs and compare our algorithms and scoring functions with
existing ones. According to the experiments, it shows that our algorithms could
achieve promising performance. The adapted revision algorithm could improve the
efficiency largely, and at most 96% time could be saved for some ontology
pairs. Some of our scoring functions help a revision algorithm obtain better
results in many cases, especially for the challenging pairs.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18382" title="Abstract">arXiv:2310.18382</a> [<a href="/pdf/2310.18382" title="Download PDF">pdf</a>, <a href="/format/2310.18382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Generative AI to Generative Internet of Things: Fundamentals,  Framework, and Outlooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jinbo Wen</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+J">Jiangtian Nie</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiawen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guizani%2C+M">Mohsen Guizani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Generative Artificial Intelligence (GAI) possesses the capabilities of
generating realistic data and facilitating advanced decision-making. By
integrating GAI into modern Internet of Things (IoT), Generative Internet of
Things (GIoT) is emerging and holds immense potential to revolutionize various
aspects of society, enabling more efficient and intelligent IoT applications,
such as smart surveillance and voice assistants. In this article, we present
the concept of GIoT and conduct an exploration of its potential prospects.
Specifically, we first overview four GAI techniques and investigate promising
GIoT applications. Then, we elaborate on the main challenges in enabling GIoT
and propose a general GAI-based secure incentive mechanism framework to address
them, in which we adopt Generative Diffusion Models (GDMs) for incentive
mechanism designs and apply blockchain technologies for secure GIoT management.
Moreover, we conduct a case study on modern Internet of Vehicle traffic
monitoring, which utilizes GDMs to generate effective contracts for
incentivizing users to contribute sensing data with high quality. Finally, we
suggest several open directions worth investigating for the future popularity
of GIoT.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18384" title="Abstract">arXiv:2310.18384</a> [<a href="/pdf/2310.18384" title="Download PDF">pdf</a>, <a href="/format/2310.18384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MicroNAS: Memory and Latency Constrained Hardware-Aware Neural  Architecture Search for Time Series Classification on Microcontrollers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=King%2C+T">Tobias King</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yexu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6ddiger%2C+T">Tobias R&#xf6;ddiger</a>, 
<a href="/search/cs?searchtype=author&query=Beigl%2C+M">Michael Beigl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper presents MicroNAS, a system designed to automatically search and
generate neural network architectures capable of classifying time series data
on resource-constrained microcontrollers (MCUs) and generating standard tf-lite
ML models. MicroNAS takes into account user-defined constraints on execution
latency and peak memory consumption on a target MCU. This approach ensures that
the resulting neural network architectures are optimised for the specific
constraints and requirements of the MCU on which they are implemented. To
achieve this, MicroNAS uses a look-up table estimation approach for accurate
execution latency calculations, with a minimum error of only 1.02ms. This
accurate latency estimation on MCUs sets it apart from other hardware-aware
neural architecture search (HW-NAS) methods that use less accurate estimation
techniques. Finally, MicroNAS delivers performance close to that of
state-of-the-art models running on desktop computers, achieving high
classification accuracies on recognised datasets (93.93% on UCI-HAR and 96.33%
on SkodaR) while running on a Cortex-M4 MCU.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18385" title="Abstract">arXiv:2310.18385</a> [<a href="/pdf/2310.18385" title="Download PDF">pdf</a>, <a href="/format/2310.18385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matching of Descriptive Labels to Glossary Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+T">Toshihiro Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Tateishi%2C+T">Takaaki Tateishi</a>, 
<a href="/search/cs?searchtype=author&query=Tatsubori%2C+M">Michiaki Tatsubori</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
<p class="mathjax">Semantic text similarity plays an important role in software engineering
tasks in which engineers are requested to clarify the semantics of descriptive
labels (e.g., business terms, table column names) that are often consists of
too short or too generic words and appears in their IT systems. We formulate
this type of problem as a task of matching descriptive labels to glossary
descriptions. We then propose a framework to leverage an existing semantic text
similarity measurement (STS) and augment it using semantic label enrichment and
set-based collective contextualization where the former is a method to retrieve
sentences relevant to a given label and the latter is a method to compute
similarity between two contexts each of which is derived from a set of texts
(e.g., column names in the same table). We performed an experiment on two
datasets derived from publicly available data sources. The result indicated
that the proposed methods helped the underlying STS correctly match more
descriptive labels with the descriptions.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18387" title="Abstract">arXiv:2310.18387</a> [<a href="/pdf/2310.18387" title="Download PDF">pdf</a>, <a href="/format/2310.18387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OffMix-3L: A Novel Code-Mixed Dataset in Bangla-English-Hindi for  Offensive Language Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goswami%2C+D">Dhiman Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Raihan%2C+M+N">Md Nishat Raihan</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+A">Antara Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Anstasopoulos%2C+A">Antonios Anstasopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Zampieri%2C+M">Marcos Zampieri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2310.18023">arXiv:2310.18023</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Code-mixing is a well-studied linguistic phenomenon when two or more
languages are mixed in text or speech. Several works have been conducted on
building datasets and performing downstream NLP tasks on code-mixed data.
Although it is not uncommon to observe code-mixing of three or more languages,
most available datasets in this domain contain code-mixed data from only two
languages. In this paper, we introduce OffMix-3L, a novel offensive language
identification dataset containing code-mixed data from three different
languages. We experiment with several models on this dataset and observe that
BanglishBERT outperforms other transformer-based models and GPT-3.5.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18388" title="Abstract">arXiv:2310.18388</a> [<a href="/pdf/2310.18388" title="Download PDF">pdf</a>, <a href="/format/2310.18388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Middle-mile optimization for next-day delivery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benidis%2C+K">Konstantinos Benidis</a>, 
<a href="/search/cs?searchtype=author&query=Paschos%2C+G">Georgios Paschos</a>, 
<a href="/search/cs?searchtype=author&query=Gross%2C+M">Martin Gross</a>, 
<a href="/search/cs?searchtype=author&query=Iosifidis%2C+G">George Iosifidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We consider an e-commerce retailer operating a supply chain that consists of
middle- and last-mile transportation, and study its ability to deliver products
stored in warehouses within a day from customer's order time. Successful
next-day delivery requires inventory availability and timely truck schedules in
the middle-mile and in this paper we assume a fixed inventory position and
focus on optimizing the middle-mile. We formulate a novel optimization problem
which decides the departure of the last middle-mile truck at each (potential)
network connection in order to maximize the number of next-day deliveries. We
show that the respective \emph{next-day delivery optimization} is a
combinatorial problem that is $NP$-hard to approximate within
$(1-1/e)\cdot\texttt{opt}\approx 0.632\cdot\texttt{opt}$, hence every retailer
that offers one-day deliveries has to deal with this complexity barrier. We
study three variants of the problem motivated by operational constraints that
different retailers encounter, and propose solutions schemes tailored to each
problem's properties. To that end, we rely on greedy submodular maximization,
pipage rounding techniques, and Lagrangian heuristics. The algorithms are
scalable, offer optimality gap guarantees, and evaluated in realistic datasets
and network scenarios were found to achieve near-optimal results.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18390" title="Abstract">arXiv:2310.18390</a> [<a href="/pdf/2310.18390" title="Download PDF">pdf</a>, <a href="/format/2310.18390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Unlu%2C+E">Eren Unlu</a>, 
<a href="/search/cs?searchtype=author&query=Ciftci%2C+U">Unver Ciftci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) are evolving to integrate multiple modalities,
such as text, image, and audio into a unified linguistic space. We envision a
future direction based on this framework where conceptual entities defined in
sequences of text can also be imagined as modalities. Such a formulation has
the potential to overcome the cognitive and computational limitations of
current models. Several illustrative examples of such potential implicit
modalities are given. Along with vast promises of the hypothesized structure,
expected challenges are discussed as well.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18409" title="Abstract">arXiv:2310.18409</a> [<a href="/pdf/2310.18409" title="Download PDF">pdf</a>, <a href="/format/2310.18409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State-Action Similarity-Based Representations for Off-Policy Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pavse%2C+B+S">Brahma S. Pavse</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+J+P">Josiah P. Hanna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Neural Information Processing Systems (NeurIPS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In reinforcement learning, off-policy evaluation (OPE) is the problem of
estimating the expected return of an evaluation policy given a fixed dataset
that was collected by running one or more different policies. One of the more
empirically successful algorithms for OPE has been the fitted q-evaluation
(FQE) algorithm that uses temporal difference updates to learn an action-value
function, which is then used to estimate the expected return of the evaluation
policy. Typically, the original fixed dataset is fed directly into FQE to learn
the action-value function of the evaluation policy. Instead, in this paper, we
seek to enhance the data-efficiency of FQE by first transforming the fixed
dataset using a learned encoder, and then feeding the transformed dataset into
FQE. To learn such an encoder, we introduce an OPE-tailored state-action
behavioral similarity metric, and use this metric and the fixed dataset to
learn an encoder that models this metric. Theoretically, we show that this
metric allows us to bound the error in the resulting OPE estimate. Empirically,
we show that other state-action similarity metrics lead to representations that
cannot represent the action-value function of the evaluation policy, and that
our state-action representation method boosts the data-efficiency of FQE and
lowers OPE error relative to other OPE-based representation learning methods on
challenging OPE tasks. We also empirically show that the learned
representations significantly mitigate divergence of FQE under varying
distribution shifts. Our code is available here:
https://github.com/Badger-RL/ROPE.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18411" title="Abstract">arXiv:2310.18411</a> [<a href="/pdf/2310.18411" title="Download PDF">pdf</a>, <a href="/format/2310.18411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A general learning scheme for classical and quantum Ising machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmid%2C+L">Ludwig Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Zardini%2C+E">Enrico Zardini</a>, 
<a href="/search/cs?searchtype=author&query=Pastorello%2C+D">Davide Pastorello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">An Ising machine is any hardware specifically designed for finding the ground
state of the Ising model. Relevant examples are coherent Ising machines and
quantum annealers. In this paper, we propose a new machine learning model that
is based on the Ising structure and can be efficiently trained using gradient
descent. We provide a mathematical characterization of the training process,
which is based upon optimizing a loss function whose partial derivatives are
not explicitly calculated but estimated by the Ising machine itself. Moreover,
we present some experimental results on the training and execution of the
proposed learning model. These results point out new possibilities offered by
Ising machines for different learning tasks. In particular, in the quantum
realm, the quantum resources are used for both the execution and the training
of the model, providing a promising perspective in quantum machine learning.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18413" title="Abstract">arXiv:2310.18413</a> [<a href="/pdf/2310.18413" title="Download PDF">pdf</a>, <a href="/format/2310.18413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Fairness ROAD: Robust Optimization for Adversarial Debiasing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grari%2C+V">Vincent Grari</a>, 
<a href="/search/cs?searchtype=author&query=Laugel%2C+T">Thibault Laugel</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T">Tatsunori Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Lamprier%2C+S">Sylvain Lamprier</a>, 
<a href="/search/cs?searchtype=author&query=Detyniecki%2C+M">Marcin Detyniecki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">In the field of algorithmic fairness, significant attention has been put on
group fairness criteria, such as Demographic Parity and Equalized Odds.
Nevertheless, these objectives, measured as global averages, have raised
concerns about persistent local disparities between sensitive groups. In this
work, we address the problem of local fairness, which ensures that the
predictor is unbiased not only in terms of expectations over the whole
population, but also within any subregion of the feature space, unknown at
training time. To enforce this objective, we introduce ROAD, a novel approach
that leverages the Distributionally Robust Optimization (DRO) framework within
a fair adversarial learning objective, where an adversary tries to infer the
sensitive attribute from the predictions. Using an instance-level re-weighting
strategy, ROAD is designed to prioritize inputs that are likely to be locally
unfair, i.e. where the adversary faces the least difficulty in reconstructing
the sensitive attribute. Numerical experiments demonstrate the effectiveness of
our method: it achieves Pareto dominance with respect to local fairness and
accuracy for a given global fairness level across three standard datasets, and
also enhances fairness generalization under distribution shift.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18416" title="Abstract">arXiv:2310.18416</a> [<a href="/pdf/2310.18416" title="Download PDF">pdf</a>, <a href="/format/2310.18416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolyMerge: A Novel Technique aimed at Dynamic HD Map Updates Leveraging  Polylines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayed%2C+M">Mohamed Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Perminov%2C+S">Stepan Perminov</a>, 
<a href="/search/cs?searchtype=author&query=Tsetserukou%2C+D">Dzmitry Tsetserukou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Currently, High-Definition (HD) maps are a prerequisite for the stable
operation of autonomous vehicles. Such maps contain information about all
static road objects for the vehicle to consider during navigation, such as road
edges, road lanes, crosswalks, and etc. To generate such an HD map, current
approaches need to process pre-recorded environment data obtained from onboard
sensors. However, recording such a dataset often requires a lot of time and
effort. In addition, every time actual road environments are changed, a new
dataset should be recorded to generate a relevant HD map.
<br />This paper addresses a novel approach that allows to continuously generate or
update the HD map using onboard sensor data. When there is no need to
pre-record the dataset, updating the HD map can be run in parallel with the
main autonomous vehicle navigation pipeline.
<br />The proposed approach utilizes the VectorMapNet framework to generate vector
road object instances from a sensor data scan. The PolyMerge technique is aimed
to merge new instances into previous ones, mitigating detection errors and,
therefore, generating or updating the HD map.
<br />The performance of the algorithm was confirmed by comparison with ground
truth on the NuScenes dataset. Experimental results showed that the mean error
for different levels of environment complexity was comparable to the
VectorMapNet single instance error.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18417" title="Abstract">arXiv:2310.18417</a> [<a href="/pdf/2310.18417" title="Download PDF">pdf</a>, <a href="/format/2310.18417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teacher Perception of Automatically Extracted Grammar Concepts for L2  Language Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+A">Aditi Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Sampath%2C+A">Arun Sampath</a>, 
<a href="/search/cs?searchtype=author&query=Sheshadri%2C+A">Ashwin Sheshadri</a>, 
<a href="/search/cs?searchtype=author&query=Anastasopoulos%2C+A">Antonios Anastasopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP Findings 2023. arXiv admin note: substantial text overlap with <a href="/abs/2206.05154">arXiv:2206.05154</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">One of the challenges in language teaching is how best to organize rules
regarding syntax, semantics, or phonology in a meaningful manner. This not only
requires content creators to have pedagogical skills, but also have that
language's deep understanding. While comprehensive materials to develop such
curricula are available in English and some broadly spoken languages, for many
other languages, teachers need to manually create them in response to their
students' needs. This is challenging because i) it requires that such experts
be accessible and have the necessary resources, and ii) describing all the
intricacies of a language is time-consuming and prone to omission. In this
work, we aim to facilitate this process by automatically discovering and
visualizing grammar descriptions. We extract descriptions from a natural text
corpus that answer questions about morphosyntax (learning of word order,
agreement, case marking, or word formation) and semantics (learning of
vocabulary). We apply this method for teaching two Indian languages, Kannada
and Marathi, which, unlike English, do not have well-developed resources for
second language learning. To assess the perceived utility of the extracted
material, we enlist the help of language educators from schools in North
America to perform a manual evaluation, who find the materials have potential
to be used for their lesson preparation and learner evaluation.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18418" title="Abstract">arXiv:2310.18418</a> [<a href="/pdf/2310.18418" title="Download PDF">pdf</a>, <a href="/format/2310.18418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STV+Reductions: Towards Practical Verification of Strategic Ability  Using Model Reductions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>, 
<a href="/search/cs?searchtype=author&query=Pazderski%2C+W">Witold Pazderski</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yan Kim</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AAMAS'21: 20th International Conference on Autonomous Agents and
  Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021, 1770--1772
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We present a substantially expanded version of our tool STV for strategy
synthesis and verification of strategic abilities. The new version adds
user-definable models and support for model reduction through partial order
reduction and checking for bisimulation.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18419" title="Abstract">arXiv:2310.18419</a> [<a href="/pdf/2310.18419" title="Download PDF">pdf</a>, <a href="/ps/2310.18419" title="Download PostScript">ps</a>, <a href="/format/2310.18419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On nonlinear compression costs: when Shannon meets R&#xe9;nyi
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Somazzi%2C+A">Andrea Somazzi</a>, 
<a href="/search/cs?searchtype=author&query=Ferragina%2C+P">Paolo Ferragina</a>, 
<a href="/search/cs?searchtype=author&query=Garlaschelli%2C+D">Diego Garlaschelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Data Structures and Algorithms (cs.DS); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">Shannon entropy is the shortest average codeword length a lossless compressor
can achieve by encoding i.i.d. symbols. However, there are cases in which the
objective is to minimize the \textit{exponential} average codeword length, i.e.
when the cost of encoding/decoding scales exponentially with the length of
codewords. The optimum is reached by all strategies that map each symbol $x_i$
generated with probability $p_i$ into a codeword of length
$\ell^{(q)}_D(i)=-\log_D\frac{p_i^q}{\sum_{j=1}^Np_j^q}$. This leads to the
minimum exponential average codeword length, which equals the R\'enyi, rather
than Shannon, entropy of the source distribution. We generalize the established
Arithmetic Coding (AC) compressor to this framework. We analytically show that
our generalized algorithm provides an exponential average length which is
arbitrarily close to the R\'enyi entropy, if the symbols to encode are i.i.d..
We then apply our algorithm to both simulated (i.i.d. generated) and real (a
piece of Wikipedia text) datasets. While, as expected, we find that the
application to i.i.d. data confirms our analytical results, we also find that,
when applied to the real dataset (composed by highly correlated symbols), our
algorithm is still able to significantly reduce the exponential average
codeword length with respect to the classical `Shannonian' one. Moreover, we
provide another justification of the use of the exponential average: namely, we
show that by minimizing the exponential average length it is possible to
minimize the probability that codewords exceed a certain threshold length. This
relation relies on the connection between the exponential average and the
cumulant generating function of the source distribution, which is in turn
related to the probability of large deviations. We test and confirm our results
again on both simulated and real datasets.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18424" title="Abstract">arXiv:2310.18424</a> [<a href="/pdf/2310.18424" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Machine Learning Method with Vector Embedding on Orthonormal Basis  and Spectral Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+L+Y">Louis Yu Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This paper presents a novel fast machine learning method that leverages two
techniques: Vector Embedding on Orthonormal Basis (VEOB) and Spectral Transform
(ST). The VEOB converts the original data encoding into a vector embedding with
coordinates projected onto orthonormal bases. The Singular Value Decomposition
(SVD) technique is used to calculate the vector basis and projection
coordinates, leading to an enhanced distance measurement in the embedding space
and facilitating data compression by preserving the projection vectors
associated with the largest singular values. On the other hand, ST transforms
sequence of vector data into spectral space. By applying the Discrete Cosine
Transform (DCT) and selecting the most significant components, it streamlines
the handling of lengthy vector sequences. The paper provides examples of word
embedding, text chunk embedding, and image embedding, implemented in Julia
language with a vector database. It also investigates unsupervised learning and
supervised learning using this method, along with strategies for handling large
data volumes.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18425" title="Abstract">arXiv:2310.18425</a> [<a href="/pdf/2310.18425" title="Download PDF">pdf</a>, <a href="/format/2310.18425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel-Jaw Gripper and Grasp Co-Optimization for Sets of Planar  Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R+H">Rebecca H. Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Doshi%2C+N">Neel Doshi</a>, 
<a href="/search/cs?searchtype=author&query=Gondhalekar%2C+R">Ravi Gondhalekar</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+A">Alberto Rodriguez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE IROS conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We propose a framework for optimizing a planar parallel-jaw gripper for use
with multiple objects. While optimizing general-purpose grippers and contact
locations for grasps are both well studied, co-optimizing grasps and the
gripper geometry to execute them receives less attention. As such, our
framework synthesizes grippers optimized to stably grasp sets of polygonal
objects. Given a fixed number of contacts and their assignments to object faces
and gripper jaws, our framework optimizes contact locations along these faces,
gripper pose for each grasp, and gripper shape. Our key insights are to pose
shape and contact constraints in frames fixed to the gripper jaws, and to
leverage the linearity of constraints in our grasp stability and gripper shape
models via an augmented Lagrangian formulation. Together, these enable a
tractable nonlinear program implementation. We apply our method to several
examples. The first illustrative problem shows the discovery of a geometrically
simple solution where possible. In another, space is constrained, forcing
multiple objects to be contacted by the same features as each other. Finally a
toolset-grasping example shows that our framework applies to complex,
real-world objects. We provide a physical experiment of the toolset grasps.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18427" title="Abstract">arXiv:2310.18427</a> [<a href="/pdf/2310.18427" title="Download PDF">pdf</a>, <a href="/ps/2310.18427" title="Download PostScript">ps</a>, <a href="/format/2310.18427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximizing Equitable Reach and Accessibility of ETDs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ingram%2C+W+A">William A. Ingram</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+E+A">Edward A. Fox</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL), Santa
  Fe, NM, USA, 2023, pp. 256-257
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">This poster addresses accessibility issues of electronic theses and
dissertations (ETDs) in digital libraries (DLs). ETDs are available primarily
as PDF files, which present barriers to equitable access, especially for users
with visual impairments, cognitive or learning disabilities, or for anyone
needing more efficient and effective ways of finding relevant information
within these long documents. We propose using AI techniques, including natural
language processing (NLP), computer vision, and text analysis, to convert PDFs
into machine-readable HTML documents with semantic tags and structure,
extracting figures and tables, and generating summaries and keywords. Our goal
is to increase the accessibility of ETDs and to make this important scholarship
available to a wider audience.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18428" title="Abstract">arXiv:2310.18428</a> [<a href="/pdf/2310.18428" title="Download PDF">pdf</a>, <a href="/ps/2310.18428" title="Download PostScript">ps</a>, <a href="/format/2310.18428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Bayesian Stability Zoo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moran%2C+S">Shay Moran</a>, 
<a href="/search/cs?searchtype=author&query=Schefler%2C+H">Hilla Schefler</a>, 
<a href="/search/cs?searchtype=author&query=Shafer%2C+J">Jonathan Shafer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We show that many definitions of stability found in the learning theory
literature are equivalent to one another. We distinguish between two families
of definitions of stability: distribution-dependent and
distribution-independent Bayesian stability. Within each family, we establish
equivalences between various definitions, encompassing approximate differential
privacy, pure differential privacy, replicability, global stability, perfect
generalization, TV stability, mutual information stability, KL-divergence
stability, and R\'enyi-divergence stability. Along the way, we prove boosting
results that enable the amplification of the stability of a learning rule. This
work is a step towards a more systematic taxonomy of stability notions in
learning theory, which can promote clarity and an improved understanding of an
array of stability concepts that have emerged in recent years.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18431" title="Abstract">arXiv:2310.18431</a> [<a href="/pdf/2310.18431" title="Download PDF">pdf</a>, <a href="/format/2310.18431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDOH-NLI: a Dataset for Inferring Social Determinants of Health from  Clinical Notes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lelkes%2C+A+D">Adam D. Lelkes</a>, 
<a href="/search/cs?searchtype=author&query=Loreaux%2C+E">Eric Loreaux</a>, 
<a href="/search/cs?searchtype=author&query=Schuster%2C+T">Tal Schuster</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Ming-Jun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rajkomar%2C+A">Alvin Rajkomar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Social and behavioral determinants of health (SDOH) play a significant role
in shaping health outcomes, and extracting these determinants from clinical
notes is a first step to help healthcare providers systematically identify
opportunities to provide appropriate care and address disparities. Progress on
using NLP methods for this task has been hindered by the lack of high-quality
publicly available labeled data, largely due to the privacy and regulatory
constraints on the use of real patients' information. This paper introduces a
new dataset, SDOH-NLI, that is based on publicly available notes and which we
release publicly. We formulate SDOH extraction as a natural language inference
(NLI) task, and provide binary textual entailment labels obtained from human
raters for a cross product of a set of social history snippets as premises and
SDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks in
that our premises and hypotheses are obtained independently. We evaluate both
"off-the-shelf" entailment models as well as models fine-tuned on our data, and
highlight the ways in which our dataset appears more challenging than commonly
used NLI datasets.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18434" title="Abstract">arXiv:2310.18434</a> [<a href="/pdf/2310.18434" title="Download PDF">pdf</a>, <a href="/format/2310.18434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Distributionally Robust Learning and Offline RL: An Approach to  Mitigate Distribution Shift and Partial Data Coverage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panaganti%2C+K">Kishan Panaganti</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zaiyan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kalathil%2C+D">Dileep Kalathil</a>, 
<a href="/search/cs?searchtype=author&query=Ghavamzadeh%2C+M">Mohammad Ghavamzadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The goal of an offline reinforcement learning (RL) algorithm is to learn
optimal polices using historical (offline) data, without access to the
environment for online exploration. One of the main challenges in offline RL is
the distribution shift which refers to the difference between the state-action
visitation distribution of the data generating policy and the learning policy.
Many recent works have used the idea of pessimism for developing offline RL
algorithms and characterizing their sample complexity under a relatively weak
assumption of single policy concentrability. Different from the offline RL
literature, the area of distributionally robust learning (DRL) offers a
principled framework that uses a minimax formulation to tackle model mismatch
between training and testing environments. In this work, we aim to bridge these
two areas by showing that the DRL approach can be used to tackle the
distributional shift problem in offline RL. In particular, we propose two
offline RL algorithms using the DRL framework, for the tabular and linear
function approximation settings, and characterize their sample complexity under
the single policy concentrability assumption. We also demonstrate the superior
performance our proposed algorithm through simulation experiments.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18435" title="Abstract">arXiv:2310.18435</a> [<a href="/pdf/2310.18435" title="Download PDF">pdf</a>, <a href="/ps/2310.18435" title="Download PostScript">ps</a>, <a href="/format/2310.18435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expanding the Set of Pragmatic Considerations in Conversational AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seals%2C+S+M">S.M. Seals</a>, 
<a href="/search/cs?searchtype=author&query=Shalin%2C+V+L">Valerie L. Shalin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print version of paper that appeared at Multidisciplinary Perspectives on COntext-aware embodied Spoken Interactions (MP-COSIN) workshop at IEEE RO-MAN 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite considerable performance improvements, current conversational AI
systems often fail to meet user expectations. We discuss several pragmatic
limitations of current conversational AI systems. We illustrate pragmatic
limitations with examples that are syntactically appropriate, but have clear
pragmatic deficiencies. We label our complaints as "Turing Test Triggers"
(TTTs) as they indicate where current conversational AI systems fall short
compared to human behavior. We develop a taxonomy of pragmatic considerations
intended to identify what pragmatic competencies a conversational AI system
requires and discuss implications for the design and evaluation of
conversational AI systems.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18438" title="Abstract">arXiv:2310.18438</a> [<a href="/pdf/2310.18438" title="Download PDF">pdf</a>, <a href="/format/2310.18438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Shape Embedding for Cloth-Changing Person Re-Identification  via 2D-3D Correspondences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yubin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Huimin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuyi Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Biyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yichong Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Cloth-Changing Person Re-Identification (CC-ReID) is a common and realistic
problem since fashion constantly changes over time and people's aesthetic
preferences are not set in stone. While most existing cloth-changing ReID
methods focus on learning cloth-agnostic identity representations from coarse
semantic cues (e.g. silhouettes and part segmentation maps), they neglect the
continuous shape distributions at the pixel level. In this paper, we propose
Continuous Surface Correspondence Learning (CSCL), a new shape embedding
paradigm for cloth-changing ReID. CSCL establishes continuous correspondences
between a 2D image plane and a canonical 3D body surface via pixel-to-vertex
classification, which naturally aligns a person image to the surface of a 3D
human model and simultaneously obtains pixel-wise surface embeddings. We
further extract fine-grained shape features from the learned surface embeddings
and then integrate them with global RGB features via a carefully designed
cross-modality fusion module. The shape embedding paradigm based on 2D-3D
correspondences remarkably enhances the model's global understanding of human
body shape. To promote the study of ReID under clothing change, we construct 3D
Dense Persons (DP3D), which is the first large-scale cloth-changing ReID
dataset that provides densely annotated 2D-3D correspondences and a precise 3D
mesh for each person image, while containing diverse cloth-changing cases over
all four seasons. Experiments on both cloth-changing and cloth-consistent ReID
benchmarks validate the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18440" title="Abstract">arXiv:2310.18440</a> [<a href="/pdf/2310.18440" title="Download PDF">pdf</a>, <a href="/format/2310.18440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thalken%2C+R">Rosamond Thalken</a>, 
<a href="/search/cs?searchtype=author&query=Stiglitz%2C+E+H">Edward H. Stiglitz</a>, 
<a href="/search/cs?searchtype=author&query=Mimno%2C+D">David Mimno</a>, 
<a href="/search/cs?searchtype=author&query=Wilkens%2C+M">Matthew Wilkens</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Generative language models (LMs) are increasingly used for document
class-prediction tasks and promise enormous improvements in cost and
efficiency. Existing research often examines simple classification tasks, but
the capability of LMs to classify on complex or specialized tasks is less well
understood. We consider a highly complex task that is challenging even for
humans: the classification of legal reasoning according to jurisprudential
philosophy. Using a novel dataset of historical United States Supreme Court
opinions annotated by a team of domain experts, we systematically test the
performance of a variety of LMs. We find that generative models perform poorly
when given instructions (i.e. prompts) equal to the instructions presented to
human annotators through our codebook. Our strongest results derive from
fine-tuning models on the annotated dataset; the best performing model is an
in-domain model, LEGAL-BERT. We apply predictions from this fine-tuned model to
study historical trends in jurisprudence, an exercise that both aligns with
prominent qualitative historical accounts and points to areas of possible
refinement in those accounts. Our findings generally sound a note of caution in
the use of generative LMs on complex tasks without fine-tuning and point to the
continued relevance of human annotation-intensive classification methods.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18442" title="Abstract">arXiv:2310.18442</a> [<a href="/pdf/2310.18442" title="Download PDF">pdf</a>, <a href="/format/2310.18442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Recursive Update for Ensemble Kalman Filters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Michaelson%2C+K">Kristen Michaelson</a>, 
<a href="/search/math?searchtype=author&query=Popov%2C+A+A">Andrey A. Popov</a>, 
<a href="/search/math?searchtype=author&query=Zanetti%2C+R">Renato Zanetti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Engineering, Finance, and Science (cs.CE); Optimization and Control (math.OC)

</div>
<p class="mathjax">Few real-world systems are amenable to truly Bayesian filtering;
nonlinearities and non-Gaussian noises can wreak havoc on filters that rely on
linearization and Gaussian uncertainty approximations. This article presents
the Bayesian Recursive Update Filter (BRUF), a Kalman filter that uses a
recursive approach to incorporate information from nonlinear measurements. The
BRUF relaxes the measurement linearity assumption of the Extended Kalman Filter
(EKF) by dividing the measurement update into a user-defined number of steps.
The proposed technique is extended for ensemble filters in the Bayesian
Recursive Update Ensemble Kalman Filter (BRUEnKF). The performance of both
filters is demonstrated in numerical examples, and new filters are introduced
which exploit the theoretical foundation of the BRUF in different ways. A
comparison between the BRUEnKF and Gromov flow, a popular particle flow
algorithm, is presented in detail. Finally, the BRUEnKF is shown to outperform
the EnKF for a very high-dimensional system.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18443" title="Abstract">arXiv:2310.18443</a> [<a href="/pdf/2310.18443" title="Download PDF">pdf</a>, <a href="/format/2310.18443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a fuller understanding of neurons with Clustered Compositional  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=La+Rosa%2C+B">Biagio La Rosa</a>, 
<a href="/search/cs?searchtype=author&query=Gilpin%2C+L+H">Leilani H. Gilpin</a>, 
<a href="/search/cs?searchtype=author&query=Capobianco%2C+R">Roberto Capobianco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Compositional Explanations is a method for identifying logical formulas of
concepts that approximate the neurons' behavior. However, these explanations
are linked to the small spectrum of neuron activations (i.e., the highest ones)
used to check the alignment, thus lacking completeness. In this paper, we
propose a generalization, called Clustered Compositional Explanations, that
combines Compositional Explanations with clustering and a novel search
heuristic to approximate a broader spectrum of the neurons' behavior. We define
and address the problems connected to the application of these methods to
multiple ranges of activations, analyze the insights retrievable by using our
algorithm, and propose desiderata qualities that can be used to study the
explanations returned by different algorithms.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18444" title="Abstract">arXiv:2310.18444</a> [<a href="/pdf/2310.18444" title="Download PDF">pdf</a>, <a href="/format/2310.18444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning  of Mixture Graph Matching and Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiaxin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zetian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianzhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Existing graph matching methods typically assume that there are similar
structures between graphs and they are matchable. However, these assumptions do
not align with real-world applications. This work addresses a more realistic
scenario where graphs exhibit diverse modes, requiring graph grouping before or
along with matching, a task termed mixture graph matching and clustering. We
introduce Minorize-Maximization Matching and Clustering (M3C), a learning-free
algorithm that guarantees theoretical convergence through the
Minorize-Maximization framework and offers enhanced flexibility via relaxed
clustering. Building on M3C, we develop UM3C, an unsupervised model that
incorporates novel edge-wise affinity learning and pseudo label selection.
Extensive experimental results on public benchmarks demonstrate that our method
outperforms state-of-the-art graph matching and mixture graph matching and
clustering approaches in both accuracy and efficiency. Source code will be made
publicly available.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18446" title="Abstract">arXiv:2310.18446</a> [<a href="/pdf/2310.18446" title="Download PDF">pdf</a>, <a href="/format/2310.18446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaoyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Hu Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Artificial Intelligence (cs.AI); Computational Geometry (cs.CG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Optimal transportation is a fundamental topic that has attracted a great
amount of attention from machine learning community in the past decades. In
this paper, we consider an interesting discrete dynamic optimal transport
problem: can we efficiently update the optimal transport plan when the weights
or the locations of the data points change? This problem is naturally motivated
by several applications in machine learning. For example, we often need to
compute the optimal transportation cost between two different data sets; if
some change happens to a few data points, should we re-compute the high
complexity cost function or update the cost by some efficient dynamic data
structure? We are aware that several dynamic maximum flow algorithms have been
proposed before, however, the research on dynamic minimum cost flow problem is
still quite limited, to the best of our knowledge. We propose a novel 2D Skip
Orthogonal List together with some dynamic tree techniques. Although our
algorithm is based on the conventional simplex method, it can efficiently
complete each pivoting operation within $O(|V|)$ time with high probability
where $V$ is the set of all supply and demand nodes. Since dynamic
modifications typically do not introduce significant changes, our algorithm
requires only a few simplex iterations in practice. So our algorithm is more
efficient than re-computing the optimal transportation cost that needs at least
one traversal over all the $O(|E|) = O(|V|^2)$ variables in general cases. Our
experiments demonstrate that our algorithm significantly outperforms existing
algorithms in the dynamic scenarios.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18448" title="Abstract">arXiv:2310.18448</a> [<a href="/pdf/2310.18448" title="Download PDF">pdf</a>, <a href="/ps/2310.18448" title="Download PostScript">ps</a>, <a href="/format/2310.18448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability and Convergence of HDG Schemes under Minimal Regularity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jiang%2C+J">Jiannan Jiang</a>, 
<a href="/search/math?searchtype=author&query=Walkington%2C+N+J">Noel J. Walkington</a>, 
<a href="/search/math?searchtype=author&query=Yue%2C+Y">Yukun Yue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Convergence and compactness properties of approximate solutions to elliptic
partial differential computed with the hybridized discontinuous Galerkin (HDG)
are established. While it is known that solutions computed using the HDG scheme
converge at optimal rates to smooth solutions, this does not establish the
stability of the method or convergence to solutions with minimal regularity.
The compactness and convergence results show that the HDG scheme can be
utilized for the solution of nonlinear problems and linear problems with
non-smooth coefficients on domains with reentrant corners.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18451" title="Abstract">arXiv:2310.18451</a> [<a href="/pdf/2310.18451" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusion of the Power from Citations: Enhance your Influence by  Integrating Information from References
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+C">Cong Qi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Influence prediction plays a crucial role in the academic community. The
amount of scholars' influence determines whether their work will be accepted by
others. Most existing research focuses on predicting one paper's citation count
after a period or identifying the most influential papers among the massive
candidates, without concentrating on an individual paper's negative or positive
impact on its authors. Thus, this study aims to formulate the prediction
problem to identify whether one paper can increase scholars' influence or not,
which can provide feedback to the authors before they publish their papers.
First, we presented the self-adapted ACC (Average Annual Citation Counts)
metric to measure authors' impact yearly based on their annual published
papers, paper citation counts, and contributions in each paper. Then, we
proposed the RD-GAT (Reference-Depth Graph Attention Network) model to
integrate heterogeneous graph information from different depth of references by
assigning attention coefficients on them. Experiments on AMiner dataset
demonstrated that the proposed ACC metrics could represent the authors
influence effectively, and the RD-GAT model is more efficiently on the academic
citation network, and have stronger robustness against the overfitting problem
compared with the baseline models. By applying the framework in this work,
scholars can identify whether their papers can improve their influence in the
future.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18454" title="Abstract">arXiv:2310.18454</a> [<a href="/pdf/2310.18454" title="Download PDF">pdf</a>, <a href="/format/2310.18454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> T5 meets Tybalt: Author Attribution in Early Modern English Drama Using  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hicke%2C+R+M+M">Rebecca M. M. Hicke</a>, 
<a href="/search/cs?searchtype=author&query=Mimno%2C+D">David Mimno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in CHR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models have shown breakthrough potential in many NLP domains.
Here we consider their use for stylometry, specifically authorship
identification in Early Modern English drama. We find both promising and
concerning results; LLMs are able to accurately predict the author of
surprisingly short passages but are also prone to confidently misattribute
texts to specific authors. A fine-tuned t5-large model outperforms all tested
baselines, including logistic regression, SVM with a linear kernel, and cosine
delta, at attributing small passages. However, we see indications that the
presence of certain authors in the model's pre-training data affects predictive
results in ways that are difficult to assess.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18455" title="Abstract">arXiv:2310.18455</a> [<a href="/pdf/2310.18455" title="Download PDF">pdf</a>, <a href="/format/2310.18455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient  Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pavasovic%2C+K+L">Krunoslav Lehman Pavasovic</a>, 
<a href="/search/cs?searchtype=author&query=Durmus%2C+A">Alain Durmus</a>, 
<a href="/search/cs?searchtype=author&query=Simsekli%2C+U">Umut Simsekli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Neural Information Processing Systems (NeurIPS), Spotlight Presentation, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">A recent line of empirical studies has demonstrated that SGD might exhibit a
heavy-tailed behavior in practical settings, and the heaviness of the tails
might correlate with the overall performance. In this paper, we investigate the
emergence of such heavy tails. Previous works on this problem only considered,
up to our knowledge, online (also called single-pass) SGD, in which the
emergence of heavy tails in theoretical findings is contingent upon access to
an infinite amount of data. Hence, the underlying mechanism generating the
reported heavy-tailed behavior in practical settings, where the amount of
training data is finite, is still not well-understood. Our contribution aims to
fill this gap. In particular, we show that the stationary distribution of
offline (also called multi-pass) SGD exhibits 'approximate' power-law tails and
the approximation error is controlled by how fast the empirical distribution of
the training data converges to the true underlying data distribution in the
Wasserstein metric. Our main takeaway is that, as the number of data points
increases, offline SGD will behave increasingly 'power-law-like'. To achieve
this result, we first prove nonasymptotic Wasserstein convergence bounds for
offline SGD to online SGD as the number of data points increases, which can be
interesting on their own. Finally, we illustrate our theory on various
experiments conducted on synthetic data and neural networks.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18457" title="Abstract">arXiv:2310.18457</a> [<a href="/pdf/2310.18457" title="Download PDF">pdf</a>, <a href="/format/2310.18457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMSTEP: LLM proofstep suggestions in Lean
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Welleck%2C+S">Sean Welleck</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+R">Rahul Saha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present LLMSTEP, a tool for integrating a language model into the Lean
proof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to
a server hosting a language model. The language model generates suggestions,
which are checked in Lean and displayed to a user in their development
environment. We provide a baseline language model, along with code for
fine-tuning and evaluation to support further development. We provide server
implementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a
step towards fast, effective language model suggestions for any user.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18458" title="Abstract">arXiv:2310.18458</a> [<a href="/pdf/2310.18458" title="Download PDF">pdf</a>, <a href="/format/2310.18458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Not Harm Protected Groups in Debiasing Language Representation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C+Q">Chloe Qinyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Stureborg%2C+R">Rickard Stureborg</a>, 
<a href="/search/cs?searchtype=author&query=Fain%2C+B">Brandon Fain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Language Representation Models (LRMs) trained with real-world data may
capture and exacerbate undesired bias and cause unfair treatment of people in
various demographic groups. Several techniques have been investigated for
applying interventions to LRMs to remove bias in benchmark evaluations on, for
example, word embeddings. However, the negative side effects of debiasing
interventions are usually not revealed in the downstream tasks. We propose
xGAP-DEBIAS, a set of evaluations on assessing the fairness of debiasing. In
this work, We examine four debiasing techniques on a real-world text
classification task and show that reducing biasing is at the cost of degrading
performance for all demographic groups, including those the debiasing
techniques aim to protect. We advocate that a debiasing technique should have
good downstream performance with the constraint of ensuring no harm to the
protected group.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18459" title="Abstract">arXiv:2310.18459</a> [<a href="/pdf/2310.18459" title="Download PDF">pdf</a>, <a href="/format/2310.18459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VFAS-Grasp: Closed Loop Grasping with Visual Feedback and Adaptive  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piacenza%2C+P">Pedro Piacenza</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiacheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Huh%2C+J">Jinwook Huh</a>, 
<a href="/search/cs?searchtype=author&query=Isler%2C+V">Volkan Isler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We consider the problem of closed-loop robotic grasping and present a novel
planner which uses Visual Feedback and an uncertainty-aware Adaptive Sampling
strategy (VFAS) to close the loop. At each iteration, our method VFAS-Grasp
builds a set of candidate grasps by generating random perturbations of a seed
grasp. The candidates are then scored using a novel metric which combines a
learned grasp-quality estimator, the uncertainty in the estimate and the
distance from the seed proposal to promote temporal consistency. Additionally,
we present two mechanisms to improve the efficiency of our sampling strategy:
We dynamically scale the sampling region size and number of samples in it based
on past grasp scores. We also leverage a motion vector field estimator to shift
the center of our sampling region. We demonstrate that our algorithm can run in
real time (20 Hz) and is capable of improving grasp performance for static
scenes by refining the initial grasp proposal. We also show that it can enable
grasping of slow moving objects, such as those encountered during human to
robot handover.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18460" title="Abstract">arXiv:2310.18460</a> [<a href="/pdf/2310.18460" title="Download PDF">pdf</a>, <a href="/ps/2310.18460" title="Download PostScript">ps</a>, <a href="/format/2310.18460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Firefly Algorithm for Optimal Transmit Beamforming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+T+A">Tuan Anh Le</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin-She Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper proposes a generalized Firefly Algorithm (FA) to solve an
optimization framework having objective function and constraints as
multivariate functions of independent optimization variables. Four
representative examples of how the proposed generalized FA can be adopted to
solve downlink beamforming problems are shown for a classic transmit
beamforming, cognitive beamforming, reconfigurable-intelligent-surfaces-aided
(RIS-aided) transmit beamforming, and RIS-aided wireless power transfer (WPT).
Complexity analyzes indicate that in large-antenna regimes the proposed FA
approaches require less computational complexity than their corresponding
interior point methods (IPMs) do, yet demand a higher complexity than the
iterative and the successive convex approximation (SCA) approaches do.
Simulation results reveal that the proposed FA attains the same global optimal
solution as that of the IPM for an optimization problem in cognitive
beamforming. On the other hand, the proposed FA approaches outperform the
iterative, IPM and SCA in terms of obtaining better solution for optimization
problems, respectively, for a classic transmit beamforming, RIS-aided transmit
beamforming and RIS-aided WPT.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18463" title="Abstract">arXiv:2310.18463</a> [<a href="/pdf/2310.18463" title="Download PDF">pdf</a>, <a href="/format/2310.18463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PeTailor: Improving Large Language Model by Tailored Chunk Scorer in  Biomedical Triple Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">M.Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huixue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> this is the first preprint version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The automatic extraction of biomedical entities and their interaction from
unstructured data remains a challenging task due to the limited availability of
expert-labeled standard datasets. In this paper, we introduce PETAI-LOR, a
retrieval-based language framework that is augmented by tailored chunk scorer.
Unlike previous retrieval-augmented language models (LM) that retrieve relevant
documents by calculating the similarity between the input sentence and the
candidate document set, PETAILOR segments the sentence into chunks and
retrieves the relevant chunk from our pre-computed chunk-based relational
key-value memory. Moreover, in order to comprehend the specific requirements of
the LM, PETAI-LOR adapt the tailored chunk scorer to the LM. We also introduce
GM-CIHT, an expert annotated biomedical triple extraction dataset with more
relation types. This dataset is centered on the non-drug treatment and general
biomedical domain. Additionally, we investigate the efficacy of triple
extraction models trained on general domains when applied to the biomedical
domain. Our experiments reveal that PETAI-LOR achieves state-of-the-art
performance on GM-CIHT
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18465" title="Abstract">arXiv:2310.18465</a> [<a href="/pdf/2310.18465" title="Download PDF">pdf</a>, <a href="/format/2310.18465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimax Optimal Submodular Optimization with Bandit Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tajdini%2C+A">Artin Tajdini</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+L">Lalit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+K">Kevin Jamieson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider maximizing a monotonic, submodular set function $f: 2^{[n]}
\rightarrow [0,1]$ under stochastic bandit feedback. Specifically, $f$ is
unknown to the learner but at each time $t=1,\dots,T$ the learner chooses a set
$S_t \subset [n]$ with $|S_t| \leq k$ and receives reward $f(S_t) + \eta_t$
where $\eta_t$ is mean-zero sub-Gaussian noise. The objective is to minimize
the learner's regret over $T$ times with respect to ($1-e^{-1}$)-approximation
of maximum $f(S_*)$ with $|S_*| = k$, obtained through greedy maximization of
$f$. To date, the best regret bound in the literature scales as $k n^{1/3}
T^{2/3}$. And by trivially treating every set as a unique arm one deduces that
$\sqrt{ {n \choose k} T }$ is also achievable. In this work, we establish the
first minimax lower bound for this setting that scales like
$\mathcal{O}(\min_{i \le k}(in^{1/3}T^{2/3} + \sqrt{n^{k-i}T}))$. Moreover, we
propose an algorithm that is capable of matching the lower bound regret.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18467" title="Abstract">arXiv:2310.18467</a> [<a href="/pdf/2310.18467" title="Download PDF">pdf</a>, <a href="/format/2310.18467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure preserving numerical methods for the ideal compressible MHD  system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dao%2C+T+A">Tuan Anh Dao</a>, 
<a href="/search/math?searchtype=author&query=Nazarov%2C+M">Murtazo Nazarov</a>, 
<a href="/search/math?searchtype=author&query=Tomas%2C+I">Ignacio Tomas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">We introduce a novel structure-preserving method in order to approximate the
compressible ideal Magnetohydrodynamics (MHD) equations. This technique
addresses the MHD equations using a non-divergence formulation, where the
contributions of the magnetic field to the momentum and total mechanical energy
are treated as source terms. Our approach uses the Marchuk-Strang splitting
technique and involves three distinct components: a compressible Euler solver,
a source-system solver, and an update procedure for the total mechanical
energy. The scheme allows for significant freedom on the choice of Euler's
equation solver, while the magnetic field is discretized using a
curl-conforming finite element space, yielding exact preservation of the
involution constraints. We prove that the method preserves invariant domain
properties, including positivity of density, positivity of internal energy, and
the minimum principle of the specific entropy. If the scheme used to solve
Euler's equation conserves total energy, then the resulting MHD scheme can be
proven to preserve total energy. Similarly, if the scheme used to solve Euler's
equation is entropy-stable, then the resulting MHD scheme is entropy stable as
well. In our approach, the CFL condition does not depend on magnetosonic
wave-speeds, but only on the usual maximum wave speed from Euler's system. To
validate the effectiveness of our method, we solve a variety of ideal MHD
problems, showing that the method is capable of delivering high-order accuracy
in space for smooth problems, while also offering unconditional robustness in
the shock hydrodynamics regime as well.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18469" title="Abstract">arXiv:2310.18469</a> [<a href="/pdf/2310.18469" title="Download PDF">pdf</a>, <a href="/format/2310.18469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Synthetic Dataset Augmentation for Application-Specific Gaze  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leblond-Menard%2C+C">Cedric Leblond-Menard</a>, 
<a href="/search/cs?searchtype=author&query=Picard-Krashevski%2C+G">Gabriel Picard-Krashevski</a>, 
<a href="/search/cs?searchtype=author&query=Achiche%2C+S">Sofiane Achiche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although the number of gaze estimation datasets is growing, the application
of appearance-based gaze estimation methods is mostly limited to estimating the
point of gaze on a screen. This is in part because most datasets are generated
in a similar fashion, where the gaze target is on a screen close to camera's
origin. In other applications such as assistive robotics or marketing research,
the 3D point of gaze might not be close to the camera's origin, meaning models
trained on current datasets do not generalize well to these tasks. We therefore
suggest generating a textured tridimensional mesh of the face and rendering the
training images from a virtual camera at a specific position and orientation
related to the application as a mean of augmenting the existing datasets. In
our tests, this lead to an average 47% decrease in gaze estimation angular
error.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18471" title="Abstract">arXiv:2310.18471</a> [<a href="/pdf/2310.18471" title="Download PDF">pdf</a>, <a href="/format/2310.18471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal disentanglement of multimodal data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Walker%2C+E">Elise Walker</a>, 
<a href="/search/cs?searchtype=author&query=Actor%2C+J+A">Jonas A. Actor</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+C">Carianne Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Trask%2C+N">Nathaniel Trask</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Causal representation learning algorithms discover lower-dimensional
representations of data that admit a decipherable interpretation of cause and
effect; as achieving such interpretable representations is challenging, many
causal learning algorithms utilize elements indicating prior information, such
as (linear) structural causal models, interventional data, or weak supervision.
Unfortunately, in exploratory causal representation learning, such elements and
prior information may not be available or warranted. Alternatively, scientific
datasets often have multiple modalities or physics-based constraints, and the
use of such scientific, multimodal data has been shown to improve
disentanglement in fully unsupervised settings. Consequently, we introduce a
causal representation learning algorithm (causalPIMA) that can use multimodal
data and known physics to discover important features with causal
relationships. Our innovative algorithm utilizes a new differentiable
parametrization to learn a directed acyclic graph (DAG) together with a latent
space of a variational autoencoder in an end-to-end differentiable framework
via a single, tractable evidence lower bound loss function. We place a Gaussian
mixture prior on the latent space and identify each of the mixtures with an
outcome of the DAG nodes; this novel identification enables feature discovery
with causal relationships. Tested against a synthetic and a scientific dataset,
our results demonstrate the capability of learning an interpretable causal
structure while simultaneously discovering key features in a fully unsupervised
setting.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18472" title="Abstract">arXiv:2310.18472</a> [<a href="/pdf/2310.18472" title="Download PDF">pdf</a>, <a href="/format/2310.18472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Efficient Methods for Metastases Detection from Clinical Notes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barabadi%2C+M+A">Maede Ashofteh Barabadi</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaodan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+W+Y">Wai Yip Chan</a>, 
<a href="/search/cs?searchtype=author&query=Simpson%2C+A+L">Amber L. Simpson</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+R+K+G">Richard K.G. Do</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure, The 36th Canadian Conference on Artificial Intelligence
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Barabadi, M. A., Zhu, X., Chan, W. Y., Simpson, A. L., &amp; Do, R. K.
  G. (2023). Parameter-Efficient Methods for Metastases Detection fromClinical
  Notes. Proceedings of the Canadian Conference on Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Understanding the progression of cancer is crucial for defining treatments
for patients. The objective of this study is to automate the detection of
metastatic liver disease from free-style computed tomography (CT) radiology
reports. Our research demonstrates that transferring knowledge using three
approaches can improve model performance. First, we utilize generic language
models (LMs), pretrained in a self-supervised manner. Second, we use a
semi-supervised approach to train our model by automatically annotating a large
unlabeled dataset; this approach substantially enhances the model's
performance. Finally, we transfer knowledge from related tasks by designing a
multi-task transfer learning methodology. We leverage the recent advancement of
parameter-efficient LM adaptation strategies to improve performance and
training efficiency. Our dataset consists of CT reports collected at Memorial
Sloan Kettering Cancer Center (MSKCC) over the course of 12 years. 2,641
reports were manually annotated by domain experts; among them, 841 reports have
been annotated for the presence of liver metastases. Our best model achieved an
F1-score of 73.8%, a precision of 84%, and a recall of 65.8%.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18473" title="Abstract">arXiv:2310.18473</a> [<a href="/pdf/2310.18473" title="Download PDF">pdf</a>, <a href="/format/2310.18473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pouring by Feel: An Analysis of Tactile and Proprioceptive Sensing for  Accurate Pouring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piacenza%2C+P">Pedro Piacenza</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Daewon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Isler%2C+V">Volkan Isler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">As service robots begin to be deployed to assist humans, it is important for
them to be able to perform a skill as ubiquitous as pouring. Specifically, we
focus on the task of pouring an exact amount of water without any environmental
instrumentation, that is, using only the robot's own sensors to perform this
task in a general way robustly. In our approach we use a simple PID controller
which uses the measured change in weight of the held container to supervise the
pour. Unlike previous methods which use specialized force-torque sensors at the
robot wrist, we use our robot joint torque sensors and investigate the added
benefit of tactile sensors at the fingertips. We train three estimators from
data which regress the poured weight out of the source container and show that
we can accurately pour within 10 ml of the target on average while being robust
enough to pour at novel locations and with different grasps on the source
container.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18477" title="Abstract">arXiv:2310.18477</a> [<a href="/pdf/2310.18477" title="Download PDF">pdf</a>, <a href="/ps/2310.18477" title="Download PostScript">ps</a>, <a href="/format/2310.18477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Improving Ensemble Adversarial Defense
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yian Deng</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+T">Tingting Mu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The strategy of ensemble has become popular in adversarial defense, which
trains multiple base classifiers to defend against adversarial attacks in a
cooperative manner. Despite the empirical success, theoretical explanations on
why an ensemble of adversarially trained classifiers is more robust than single
ones remain unclear. To fill in this gap, we develop a new error theory
dedicated to understanding ensemble adversarial defense, demonstrating a
provable 0-1 loss reduction on challenging sample sets in an adversarial
defense scenario. Guided by this theory, we propose an effective approach to
improve ensemble adversarial defense, named interactive global adversarial
training (iGAT). The proposal includes (1) a probabilistic distributing rule
that selectively allocates to different base classifiers adversarial examples
that are globally challenging to the ensemble, and (2) a regularization term to
rescue the severest weaknesses of the base classifiers. Being tested over
various existing ensemble adversarial defense techniques, iGAT is capable of
boosting their performance by increases up to 17% evaluated using CIFAR10 and
CIFAR100 datasets under both white-box and black-box attacks.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18479" title="Abstract">arXiv:2310.18479</a> [<a href="/pdf/2310.18479" title="Download PDF">pdf</a>, <a href="/format/2310.18479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted Sampled Split Learning (WSSL): Balancing Privacy, Robustness,  and Fairness in Distributed Learning Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osti%2C+M">Manish Osti</a>, 
<a href="/search/cs?searchtype=author&query=Thakuri%2C+A">Aashray Thakuri</a>, 
<a href="/search/cs?searchtype=author&query=Qolomany%2C+B">Basheer Qolomany</a>, 
<a href="/search/cs?searchtype=author&query=Mulahuwaish%2C+A">Aos Mulahuwaish</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">This study presents Weighted Sampled Split Learning (WSSL), an innovative
framework tailored to bolster privacy, robustness, and fairness in distributed
machine learning systems. Unlike traditional approaches, WSSL disperses the
learning process among multiple clients, thereby safeguarding data
confidentiality. Central to WSSL's efficacy is its utilization of weighted
sampling. This approach ensures equitable learning by tactically selecting
influential clients based on their contributions. Our evaluation of WSSL
spanned various client configurations and employed two distinct datasets: Human
Gait Sensor and CIFAR-10. We observed three primary benefits: heightened model
accuracy, enhanced robustness, and maintained fairness across diverse client
compositions. Notably, our distributed frameworks consistently surpassed
centralized counterparts, registering accuracy peaks of 82.63% and 75.51% for
the Human Gait Sensor and CIFAR-10 datasets, respectively. These figures
contrast with the top accuracies of 81.12% and 58.60% achieved by centralized
systems. Collectively, our findings champion WSSL as a potent and scalable
successor to conventional centralized learning, marking it as a pivotal stride
forward in privacy-focused, resilient, and impartial distributed machine
learning.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18480" title="Abstract">arXiv:2310.18480</a> [<a href="/pdf/2310.18480" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capacity, Collision Avoidance and Shopping Rate under a Social  Distancing Regime
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Haitian Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Sankoff%2C+D">David Sankoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 22 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Capacity restrictions in stores, maintained by mechanisms like spacing
customer intake, became familiar features of retailing in the time of the
pandemic. Shopping rates in a crowded store under a social distance regime is
prone to considerable slowdown. Inspired by the random particle collision
concepts of statistical mechanics, we introduce a dynamical model of the
evolution of shopping rate as a function of a given customer intake rate. The
slowdown of each individual customer is incorporated as an additive term to a
baseline value shopping time, proportional to the number of other customers in
the store. We determine analytically and by simulation the trajectory of the
model as it approaches a Little's Law equilibrium, and identify the point
beyond which equilibrium cannot be achieved. By relating customer shopping rate
to the slowdown compared to the baseline, we can calculate the optimal intake
rate leading to maximum equilibrium spending. This turns out to be the maximum
rate compatible with equilibrium. The slowdown due to the largest possible
number of shoppers is more than compensated for by the increased volume of
shopping. This macroscopic model is validated by simulation experiments in
which avoidance interactions between pairs of shoppers are responsible for
shopping delays.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18481" title="Abstract">arXiv:2310.18481</a> [<a href="/pdf/2310.18481" title="Download PDF">pdf</a>, <a href="/format/2310.18481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOSEL: Inference Serving Using Dynamic Modality Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bodun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Le Xu</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+J">Jeongyoon Moon</a>, 
<a href="/search/cs?searchtype=author&query=Yadwadkar%2C+N+J">Neeraja J. Yadwadkar</a>, 
<a href="/search/cs?searchtype=author&query=Akella%2C+A">Aditya Akella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Operating Systems (cs.OS)

</div>
<p class="mathjax">Rapid advancements over the years have helped machine learning models reach
previously hard-to-achieve goals, sometimes even exceeding human capabilities.
However, to attain the desired accuracy, the model sizes and in turn their
computational requirements have increased drastically. Thus, serving
predictions from these models to meet any target latency and cost requirements
of applications remains a key challenge, despite recent work in building
inference-serving systems as well as algorithmic approaches that dynamically
adapt models based on inputs. In this paper, we introduce a form of dynamism,
modality selection, where we adaptively choose modalities from inference inputs
while maintaining the model quality. We introduce MOSEL, an automated inference
serving system for multi-modal ML models that carefully picks input modalities
per request based on user-defined performance and accuracy requirements. MOSEL
exploits modality configurations extensively, improving system throughput by
3.6$\times$ with an accuracy guarantee and shortening job completion times by
11$\times$.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18491" title="Abstract">arXiv:2310.18491</a> [<a href="/pdf/2310.18491" title="Download PDF">pdf</a>, <a href="/format/2310.18491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Publicly Detectable Watermarking for Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fairoze%2C+J">Jaiden Fairoze</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Sanjam Garg</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>, 
<a href="/search/cs?searchtype=author&query=Mahloujifar%2C+S">Saeed Mahloujifar</a>, 
<a href="/search/cs?searchtype=author&query=Mahmoody%2C+M">Mohammad Mahmoody</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingyuan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We construct the first provable watermarking scheme for language models with
public detectability or verifiability: we use a private key for watermarking
and a public key for watermark detection. Our protocol is the first
watermarking scheme that does not embed a statistical signal in generated text.
Rather, we directly embed a publicly-verifiable cryptographic signature using a
form of rejection sampling. We show that our construction meets strong formal
security guarantees and preserves many desirable properties found in schemes in
the private-key watermarking setting. In particular, our watermarking scheme
retains distortion-freeness and model agnosticity. We implement our scheme and
make empirical measurements over open models in the 7B parameter range. Our
experiments suggest that our watermarking scheme meets our formal claims while
preserving text quality.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18493" title="Abstract">arXiv:2310.18493</a> [<a href="/pdf/2310.18493" title="Download PDF">pdf</a>, <a href="/format/2310.18493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Kinetic Simulations of Electrostatic Plasmas with  Reduced-Order Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tsai%2C+P">Ping-Hsuan Tsai</a> (1), 
<a href="/search/math?searchtype=author&query=Chung%2C+S+W">Seung Whan Chung</a> (2), 
<a href="/search/math?searchtype=author&query=Ghosh%2C+D">Debojyoti Ghosh</a> (2), 
<a href="/search/math?searchtype=author&query=Loffeld%2C+J">John Loffeld</a> (2), 
<a href="/search/math?searchtype=author&query=Choi%2C+Y">Youngsoo Choi</a> (2), 
<a href="/search/math?searchtype=author&query=Belof%2C+J+L">Jonathan L. Belof</a> (2) ((1) University of Illinois Urbana--Champaign, (2) Lawrence Livermore National Laboratory)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Plasma Physics (physics.plasm-ph)

</div>
<p class="mathjax">Despite the advancements in high-performance computing and modern numerical
algorithms, the cost remains prohibitive for multi-query kinetic plasma
simulations. In this work, we develop data-driven reduced-order models (ROM)
for collisionless electrostatic plasma dynamics, based on the kinetic
Vlasov-Poisson equation. Our ROM approach projects the equation onto a linear
subspace defined by principal proper orthogonal decomposition (POD) modes. We
introduce an efficient tensorial method to update the nonlinear term using a
precomputed third-order tensor. We capture multiscale behavior with a minimal
number of POD modes by decomposing the solution into multiple time windows
using a physical-time indicator and creating a temporally-local ROM. Applied to
1D-1V simulations, specifically the benchmark two-stream instability case, our
time-windowed reduced-order model (TW-ROM) with the tensorial approach solves
the equation approximately 450 times faster than Eulerian simulations while
maintaining a maximum relative error of 3% for the training data and 12% for
the testing data.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18494" title="Abstract">arXiv:2310.18494</a> [<a href="/pdf/2310.18494" title="Download PDF">pdf</a>, <a href="/format/2310.18494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge-based in silico models and dataset for the comparative  evaluation of mammography AI for a range of breast characteristics, lesion  conspicuities and doses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sizikova%2C+E">Elena Sizikova</a>, 
<a href="/search/eess?searchtype=author&query=Saharkhiz%2C+N">Niloufar Saharkhiz</a>, 
<a href="/search/eess?searchtype=author&query=Sharma%2C+D">Diksha Sharma</a>, 
<a href="/search/eess?searchtype=author&query=Lago%2C+M">Miguel Lago</a>, 
<a href="/search/eess?searchtype=author&query=Sahiner%2C+B">Berkman Sahiner</a>, 
<a href="/search/eess?searchtype=author&query=Delfino%2C+J+G">Jana G. Delfino</a>, 
<a href="/search/eess?searchtype=author&query=Badano%2C+A">Aldo Badano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">To generate evidence regarding the safety and efficacy of artificial
intelligence (AI) enabled medical devices, AI models need to be evaluated on a
diverse population of patient cases, some of which may not be readily
available. We propose an evaluation approach for testing medical imaging AI
models that relies on in silico imaging pipelines in which stochastic digital
models of human anatomy (in object space) with and without pathology are imaged
using a digital replica imaging acquisition system to generate realistic
synthetic image datasets. Here, we release M-SYNTH, a dataset of cohorts with
four breast fibroglandular density distributions imaged at different exposure
levels using Monte Carlo x-ray simulations with the publicly available Virtual
Imaging Clinical Trial for Regulatory Evaluation (VICTRE) toolkit. We utilize
the synthetic dataset to analyze AI model performance and find that model
performance decreases with increasing breast density and increases with higher
mass density, as expected. As exposure levels decrease, AI model performance
drops with the highest performance achieved at exposure levels lower than the
nominal recommended dose for the breast type.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18496" title="Abstract">arXiv:2310.18496</a> [<a href="/pdf/2310.18496" title="Download PDF">pdf</a>, <a href="/format/2310.18496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Well Do Feature-Additive Explainers Explain Feature-Additive  Predictors?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carmichael%2C+Z">Zachariah Carmichael</a>, 
<a href="/search/cs?searchtype=author&query=Scheirer%2C+W+J">Walter J. Scheirer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS Workshop XAI in Action: Past, Present, and Future Applications. arXiv admin note: text overlap with <a href="/abs/2106.08376">arXiv:2106.08376</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Surging interest in deep learning from high-stakes domains has precipitated
concern over the inscrutable nature of black box neural networks. Explainable
AI (XAI) research has led to an abundance of explanation algorithms for these
black boxes. Such post hoc explainers produce human-comprehensible
explanations, however, their fidelity with respect to the model is not well
understood - explanation evaluation remains one of the most challenging issues
in XAI. In this paper, we ask a targeted but important question: can popular
feature-additive explainers (e.g., LIME, SHAP, SHAPR, MAPLE, and PDP) explain
feature-additive predictors? Herein, we evaluate such explainers on ground
truth that is analytically derived from the additive structure of a model. We
demonstrate the efficacy of our approach in understanding these explainers
applied to symbolic expressions, neural networks, and generalized additive
models on thousands of synthetic and several real-world tasks. Our results
suggest that all explainers eventually fail to correctly attribute the
importance of features, especially when a decision-making process involves
feature interactions.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18498" title="Abstract">arXiv:2310.18498</a> [<a href="/pdf/2310.18498" title="Download PDF">pdf</a>, <a href="/ps/2310.18498" title="Download PostScript">ps</a>, <a href="/format/2310.18498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4 Vision on Medical Image Classification -- A Case Study on COVID-19  Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruibo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+T">Tianyi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yihan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guodong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhengmian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanshuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This technical report delves into the application of GPT-4 Vision (GPT-4V) in
the nuanced realm of COVID-19 image classification, leveraging the
transformative potential of in-context learning to enhance diagnostic
processes.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18502" title="Abstract">arXiv:2310.18502</a> [<a href="/pdf/2310.18502" title="Download PDF">pdf</a>, <a href="/format/2310.18502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Automatic Generation and Simplification of Children&#x27;s Stories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valentini%2C+M">Maria Valentini</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+J">Jennifer Weber</a>, 
<a href="/search/cs?searchtype=author&query=Salcido%2C+J">Jesus Salcido</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+T">T&#xe9;a Wright</a>, 
<a href="/search/cs?searchtype=author&query=Colunga%2C+E">Eliana Colunga</a>, 
<a href="/search/cs?searchtype=author&query=Kann%2C+K">Katharina Kann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (main conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With recent advances in large language models (LLMs), the concept of
automatically generating children's educational materials has become
increasingly realistic. Working toward the goal of age-appropriate simplicity
in generated educational texts, we first examine the ability of several popular
LLMs to generate stories with properly adjusted lexical and readability levels.
We find that, in spite of the growing capabilities of LLMs, they do not yet
possess the ability to limit their vocabulary to levels appropriate for younger
age groups. As a second experiment, we explore the ability of state-of-the-art
lexical simplification models to generalize to the domain of children's stories
and, thus, create an efficient pipeline for their automatic generation. In
order to test these models, we develop a dataset of child-directed lexical
simplification instances, with examples taken from the LLM-generated stories in
our first experiment. We find that, while the strongest-performing current
lexical simplification models do not perform as well on material designed for
children due to their reliance on large language models behind the scenes, some
models that still achieve fairly strong results on general data can mimic or
even improve their performance on children-directed data with proper
fine-tuning, which we conduct using our newly created child-directed
simplification dataset.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18505" title="Abstract">arXiv:2310.18505</a> [<a href="/pdf/2310.18505" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-fidelity Design of Porous Microstructures for Thermofluidic  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eweis-LaBolle%2C+J+T">Jonathan Tammer Eweis-LaBolle</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chuanning Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+Y">Yoonjin Won</a>, 
<a href="/search/cs?searchtype=author&query=Bostanabad%2C+R">Ramin Bostanabad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">As modern electronic devices are increasingly miniaturized and integrated,
their performance relies more heavily on effective thermal management.
Two-phase cooling methods enhanced by porous surfaces, which capitalize on
thin-film evaporation atop structured porous surfaces, are emerging as
potential solutions. In such porous structures, the optimum heat dissipation
capacity relies on two competing objectives that depend on mass and heat
transfer. The computational costs of evaluating these objectives, the high
dimensionality of the design space which a voxelated microstructure
representation, and the manufacturability constraints hinder the optimization
process for thermal management. We address these challenges by developing a
data-driven framework for designing optimal porous microstructures for cooling
applications. In our framework we leverage spectral density functions (SDFs) to
encode the design space via a handful of interpretable variables and, in turn,
efficiently search it. We develop physics-based formulas to quantify the
thermofluidic properties and feasibility of candidate designs via offline
simulations. To decrease the reliance on expensive simulations, we generate
multi-fidelity data and build emulators to find Pareto-optimal designs. We
apply our approach to a canonical problem on evaporator wick design and obtain
fin-like topologies in the optimal microstructures which are also
characteristics often observed in industrial applications.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18509" title="Abstract">arXiv:2310.18509</a> [<a href="/pdf/2310.18509" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning for Weapons to Targets Assignment in a  Hypersonic strike
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaudet%2C+B">Brian Gaudet</a>, 
<a href="/search/cs?searchtype=author&query=Drozd%2C+K">Kris Drozd</a>, 
<a href="/search/cs?searchtype=author&query=Furfaro%2C+R">Roberto Furfaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We use deep reinforcement learning (RL) to optimize a weapons to target
assignment (WTA) policy for multi-vehicle hypersonic strike against multiple
targets. The objective is to maximize the total value of destroyed targets in
each episode. Each randomly generated episode varies the number and initial
conditions of the hypersonic strike weapons (HSW) and targets, the value
distribution of the targets, and the probability of a HSW being intercepted. We
compare the performance of this WTA policy to that of a benchmark WTA policy
derived using non-linear integer programming (NLIP), and find that the RL WTA
policy gives near optimal performance with a 1000X speedup in computation time,
allowing real time operation that facilitates autonomous decision making in the
mission end game.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18511" title="Abstract">arXiv:2310.18511</a> [<a href="/pdf/2310.18511" title="Download PDF">pdf</a>, <a href="/format/2310.18511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for  Compositional Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Slim%2C+H">Habib Slim</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+M">Mahmoud Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Ayman%2C+M">Mohamed Ayman</a>, 
<a href="/search/cs?searchtype=author&query=Upadhyay%2C+U">Ujjwal Upadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Abdelreheem%2C+A">Ahmed Abdelreheem</a>, 
<a href="/search/cs?searchtype=author&query=Prajapati%2C+A">Arpit Prajapati</a>, 
<a href="/search/cs?searchtype=author&query=Pothigara%2C+S">Suhail Pothigara</a>, 
<a href="/search/cs?searchtype=author&query=Wonka%2C+P">Peter Wonka</a>, 
<a href="/search/cs?searchtype=author&query=Elhoseiny%2C+M">Mohamed Elhoseiny</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://3dcompat-dataset.org/v2/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160
million rendered views of more than 10 million stylized 3D shapes carefully
annotated at the part-instance level, alongside matching RGB point clouds, 3D
textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41
shape categories, 275 fine-grained part categories, and 293 fine-grained
material classes that can be compositionally applied to parts of 3D objects. We
render a subset of one million stylized shapes from four equally spaced views
as well as four randomized views, leading to a total of 160 million renderings.
Parts are segmented at the instance level, with coarse-grained and fine-grained
semantic levels. We introduce a new task, called Grounded CoMPaT Recognition
(GCR), to collectively recognize and ground compositions of materials on parts
of 3D objects. Additionally, we report the outcomes of a data challenge
organized at CVPR2023, showcasing the winning method's utilization of a
modified PointNet$^{++}$ model trained on 6D inputs, and exploring alternative
techniques for GCR enhancement. We hope our work will help ease future research
on compositional 3D Vision.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18512" title="Abstract">arXiv:2310.18512</a> [<a href="/pdf/2310.18512" title="Download PDF">pdf</a>, <a href="/format/2310.18512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preventing Language Models From Hiding Their Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roger%2C+F">Fabien Roger</a>, 
<a href="/search/cs?searchtype=author&query=Greenblatt%2C+R">Ryan Greenblatt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large language models (LLMs) often benefit from intermediate steps of
reasoning to generate answers to complex problems. When these intermediate
steps of reasoning are used to monitor the activity of the model, it is
essential that this explicit reasoning is faithful, i.e. that it reflects what
the model is actually reasoning about. In this work, we focus on one potential
way intermediate steps of reasoning could be unfaithful: encoded reasoning,
where an LLM could encode intermediate steps of reasoning in the generated text
in a way that is not understandable to human readers. We show that language
models can be trained to make use of encoded reasoning to get higher
performance without the user understanding the intermediate steps of reasoning.
We argue that, as language models get stronger, this behavior becomes more
likely to appear naturally. Finally, we describe a methodology that enables the
evaluation of defenses against encoded reasoning, and show that, under the
right conditions, paraphrasing successfully prevents even the best encoding
schemes we built from encoding more than 3 bits of information per KB of text.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18515" title="Abstract">arXiv:2310.18515</a> [<a href="/pdf/2310.18515" title="Download PDF">pdf</a>, <a href="/format/2310.18515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to design protein-protein interactions with enhanced  generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bushuiev%2C+A">Anton Bushuiev</a>, 
<a href="/search/cs?searchtype=author&query=Bushuiev%2C+R">Roman Bushuiev</a>, 
<a href="/search/cs?searchtype=author&query=Filkin%2C+A">Anatolii Filkin</a>, 
<a href="/search/cs?searchtype=author&query=Kouba%2C+P">Petr Kouba</a>, 
<a href="/search/cs?searchtype=author&query=Gabrielova%2C+M">Marketa Gabrielova</a>, 
<a href="/search/cs?searchtype=author&query=Gabriel%2C+M">Michal Gabriel</a>, 
<a href="/search/cs?searchtype=author&query=Sedlar%2C+J">Jiri Sedlar</a>, 
<a href="/search/cs?searchtype=author&query=Pluskal%2C+T">Tomas Pluskal</a>, 
<a href="/search/cs?searchtype=author&query=Damborsky%2C+J">Jiri Damborsky</a>, 
<a href="/search/cs?searchtype=author&query=Mazurenko%2C+S">Stanislav Mazurenko</a>, 
<a href="/search/cs?searchtype=author&query=Sivic%2C+J">Josef Sivic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Discovering mutations enhancing protein-protein interactions (PPIs) is
critical for advancing biomedical research and developing improved
therapeutics. While machine learning approaches have substantially advanced the
field, they often struggle to generalize beyond training data in practical
scenarios. The contributions of this work are three-fold. First, we construct
PPIRef, the largest and non-redundant dataset of 3D protein-protein
interactions, enabling effective large-scale learning. Second, we leverage the
PPIRef dataset to pre-train PPIformer, a new SE(3)-equivariant model
generalizing across diverse protein-binder variants. We fine-tune PPIformer to
predict effects of mutations on protein-protein interactions via a
thermodynamically motivated adjustment of the pre-training loss function.
Finally, we demonstrate the enhanced generalization of our new PPIformer
approach by outperforming other state-of-the-art methods on new, non-leaking
splits of standard labeled PPI mutational data and independent case studies
optimizing a human antibody against SARS-CoV-2 and increasing the thrombolytic
activity of staphylokinase.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18517" title="Abstract">arXiv:2310.18517</a> [<a href="/pdf/2310.18517" title="Download PDF">pdf</a>, <a href="/format/2310.18517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to recognize occluded and small objects with partial inputs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zunair%2C+H">Hasib Zunair</a>, 
<a href="/search/cs?searchtype=author&query=Hamza%2C+A+B">A. Ben Hamza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recognizing multiple objects in an image is challenging due to occlusions,
and becomes even more so when the objects are small. While promising, existing
multi-label image recognition models do not explicitly learn context-based
representations, and hence struggle to correctly recognize small and occluded
objects. Intuitively, recognizing occluded objects requires knowledge of
partial input, and hence context. Motivated by this intuition, we propose
Masked Supervised Learning (MSL), a single-stage, model-agnostic learning
paradigm for multi-label image recognition. The key idea is to learn
context-based representations using a masked branch and to model label
co-occurrence using label consistency. Experimental results demonstrate the
simplicity, applicability and more importantly the competitive performance of
MSL against previous state-of-the-art methods on standard multi-label image
recognition benchmarks. In addition, we show that MSL is robust to random
masking and demonstrate its effectiveness in recognizing non-masked objects.
Code and pretrained models are available on GitHub.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18518" title="Abstract">arXiv:2310.18518</a> [<a href="/pdf/2310.18518" title="Download PDF">pdf</a>, <a href="/ps/2310.18518" title="Download PostScript">ps</a>, <a href="/format/2310.18518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfiguration of plane trees in convex geometric graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bousquet%2C+N">Nicolas Bousquet</a>, 
<a href="/search/cs?searchtype=author&query=De+Meyer%2C+L">Lucas De Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Pierron%2C+T">Th&#xe9;o Pierron</a>, 
<a href="/search/cs?searchtype=author&query=Wesolek%2C+A">Alexandra Wesolek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">A non-crossing spanning tree of a set of points in the plane is a spanning
tree whose edges pairwise do not cross. Avis and Fukuda in 1996 proved that
there always exists a flip sequence of length at most $2n-4$ between any pair
of non-crossing spanning trees (where $n$ denotes the number of points).
Hernando et al. proved that the length of a minimal flip sequence can be of
length at least $\frac 32 n$. Two recent results of Aichholzer et al. and
Bousquet et al. improved the Avis and Fukuda upper bound by proving that there
always exists a flip sequence of length respectively at most $2n - \log n$ and
$2n - \sqrt{n}$. We improve the upper bound by a linear factor for the first
time in 25 years by proving that there always exists a flip sequence between
any pair of non-crossing spanning trees $T_1,T_2$ of length at most $c n$ where
$c \approx 1.95$. Our result is actually stronger since we prove that, for any
two trees $T_1,T_2$, there exists a flip sequence from $T_1$ to $T_2$ of length
at most $c |T_1 \setminus T_2|$. We also improve the best lower bound in terms
of the symmetric difference by proving that there exists a pair of trees
$T_1,T_2$ such that a minimal flip sequence has length $\frac 53 |T_1 \setminus
T_2|$, improving the lower bound of Hernando et al. by considering the
symmetric difference instead of the number of vertices. We generalize this
lower bound construction to non-crossing flips (where we close the gap between
upper and lower bounds) and rotations.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18523" title="Abstract">arXiv:2310.18523</a> [<a href="/pdf/2310.18523" title="Download PDF">pdf</a>, <a href="/format/2310.18523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using convolutional neural networks for stereological characterization  of 3D hetero-aggregates based on synthetic STEM data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+L">Lukas Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Kirstein%2C+T">Tom Kirstein</a>, 
<a href="/search/cs?searchtype=author&query=Mahr%2C+C">Christoph Mahr</a>, 
<a href="/search/cs?searchtype=author&query=Furat%2C+O">Orkun Furat</a>, 
<a href="/search/cs?searchtype=author&query=Baric%2C+V">Valentin Baric</a>, 
<a href="/search/cs?searchtype=author&query=Rosenauer%2C+A">Andreas Rosenauer</a>, 
<a href="/search/cs?searchtype=author&query=Maedler%2C+L">Lutz Maedler</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+V">Volker Schmidt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 16 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The structural characterization of hetero-aggregates in 3D is of great
interest, e.g., for deriving process-structure or structure-property
relationships. However, since 3D imaging techniques are often difficult to
perform as well as time and cost intensive, a characterization of
hetero-aggregates based on 2D image data is desirable, but often non-trivial.
To overcome the issues of characterizing 3D structures from 2D measurements, a
method is presented that relies on machine learning combined with methods of
spatial stochastic modeling, where the latter are utilized for the generation
of synthetic training data. This kind of training data has the advantage that
time-consuming experiments for the synthesis of differently structured
materials followed by their 3D imaging can be avoided. More precisely, a
parametric stochastic 3D model is presented, from which a wide spectrum of
virtual hetero-aggregates can be generated. Additionally, the virtual
structures are passed to a physics-based simulation tool in order to generate
virtual scanning transmission electron microscopy (STEM) images. The preset
parameters of the 3D model together with the simulated STEM images serve as a
database for the training of convolutional neural networks, which can be used
to determine the parameters of the underlying 3D model and, consequently, to
predict 3D structures of hetero-aggregates from 2D STEM images. Furthermore, an
error analysis is performed to evaluate the prediction power of the trained
neural networks with respect to structural descriptors, e.g. the
hetero-coordination number.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18526" title="Abstract">arXiv:2310.18526</a> [<a href="/pdf/2310.18526" title="Download PDF">pdf</a>, <a href="/format/2310.18526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample based Explanations via Generalized Representers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsai%2C+C">Che-Ping Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C">Chih-Kuan Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+P">Pradeep Ravikumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose a general class of sample based explanations of machine learning
models, which we term generalized representers. To measure the effect of a
training sample on a model's test prediction, generalized representers use two
components: a global sample importance that quantifies the importance of the
training point to the model and is invariant to test samples, and a local
sample importance that measures similarity between the training sample and the
test point with a kernel. A key contribution of the paper is to show that
generalized representers are the only class of sample based explanations
satisfying a natural set of axiomatic properties. We discuss approaches to
extract global importances given a kernel, and also natural choices of kernels
given modern non-linear models. As we show, many popular existing sample based
explanations could be cast as generalized representers with particular choices
of kernels and approaches to extract global importances. Additionally, we
conduct empirical comparisons of different generalized representers on two
image and two text classification datasets.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18531" title="Abstract">arXiv:2310.18531</a> [<a href="/pdf/2310.18531" title="Download PDF">pdf</a>, <a href="/format/2310.18531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Selection in the Contrastive Analysis Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+E">Ethan Weinberger</a>, 
<a href="/search/cs?searchtype=author&query=Covert%2C+I">Ian Covert</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Su-In Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Contrastive analysis (CA) refers to the exploration of variations uniquely
enriched in a target dataset as compared to a corresponding background dataset
generated from sources of variation that are irrelevant to a given task. For
example, a biomedical data analyst may wish to find a small set of genes to use
as a proxy for variations in genomic data only present among patients with a
given disease (target) as opposed to healthy control subjects (background).
However, as of yet the problem of feature selection in the CA setting has
received little attention from the machine learning community. In this work we
present contrastive feature selection (CFS), a method for performing feature
selection in the CA setting. We motivate our approach with a novel
information-theoretic analysis of representation learning in the CA setting,
and we empirically validate CFS on a semi-synthetic dataset and four real-world
biomedical datasets. We find that our method consistently outperforms
previously proposed state-of-the-art supervised and fully unsupervised feature
selection methods not designed for the CA setting. An open-source
implementation of our method is available at https://github.com/suinleelab/CFS.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18532" title="Abstract">arXiv:2310.18532</a> [<a href="/pdf/2310.18532" title="Download PDF">pdf</a>, <a href="/format/2310.18532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SkipAnalyzer: An Embodied Agent for Code Analysis with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohajer%2C+M+M">Mohammad Mahdi Mohajer</a>, 
<a href="/search/cs?searchtype=author&query=Aleithan%2C+R">Reem Aleithan</a>, 
<a href="/search/cs?searchtype=author&query=Harzevili%2C+N+S">Nima Shiri Harzevili</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+M">Moshi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Belle%2C+A+B">Alvine Boaye Belle</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+H+V">Hung Viet Pham</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">We introduce SkipAnalyzer, the first large language model (LLM)-powered
embodied agent for static code analysis. It can detect bugs, filter false
positive warnings, and patch the detected bugs without human intervention.
SkipAnalyzer consists of three components, 1) an LLM-based static bug detector
that scans source code and reports specific types of bugs, 2) an LLM-based
false-positive filter that can identify false-positive bugs in the results of
static bug detectors to improve detection accuracy, and 3) an LLM-based patch
generator that can generate patches for the detected bugs above. As a
proof-of-concept, SkipAnalyzer is built on ChatGPT, which has exhibited
outstanding performance in various software engineering tasks. To evaluate
SkipAnalyzer, we focus on two types of typical and critical bugs that are
targeted by static bug detection, i.e., Null Dereference and Resource Leak as
subjects. We employ Infer to aid the gathering of these two bug types from 10
open-source projects. Consequently, our experiment dataset contains 222
instances of Null Dereference bugs and 46 instances of Resource Leak bugs. Our
study demonstrates that SkipAnalyzer achieves remarkable performance in the
mentioned static analysis tasks, including bug detection, false-positive
warning removal, and bug repair. In static bug detection, SkipAnalyzer achieves
accuracy values of up to 68.37% for detecting Null Dereference bugs and 76.95%
for detecting Resource Leak bugs, outperforming the current leading bug
detector, Infer. For removing false-positive warnings, SkipAnalyzer can reach a
precision of up to 93.88% for Null Dereference bugs and 63.33% for Resource
Leak bugs. Additionally, SkipAnalyzer surpasses state-of-the-art false-positive
warning removal tools. Furthermore, in bug repair, SkipAnalyzer can generate
syntactically correct patches to fix its detected bugs with a success rate of
up to 97.30%.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18534" title="Abstract">arXiv:2310.18534</a> [<a href="/pdf/2310.18534" title="Download PDF">pdf</a>, <a href="/format/2310.18534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi Time Scale World Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaj%2C+V">Vaisakh Shaj</a>, 
<a href="/search/cs?searchtype=author&query=Zadeh%2C+S+G">Saleh Gholam Zadeh</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+O">Ozan Demir</a>, 
<a href="/search/cs?searchtype=author&query=Douat%2C+L+R">Luiz Ricardo Douat</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+G">Gerhard Neumann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as spotlight at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Intelligent agents use internal world models to reason and make predictions
about different courses of their actions at many scales. Devising learning
paradigms and architectures that allow machines to learn world models that
operate at multiple levels of temporal abstractions while dealing with complex
uncertainty predictions is a major technical hurdle. In this work, we propose a
probabilistic formalism to learn multi-time scale world models which we call
the Multi Time Scale State Space (MTS3) model. Our model uses a computationally
efficient inference scheme on multiple time scales for highly accurate
long-horizon predictions and uncertainty estimates over several seconds into
the future. Our experiments, which focus on action conditional long horizon
future predictions, show that MTS3 outperforms recent methods on several system
identification benchmarks including complex simulated and real-world dynamical
systems.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18537" title="Abstract">arXiv:2310.18537</a> [<a href="/pdf/2310.18537" title="Download PDF">pdf</a>, <a href="/format/2310.18537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heuristics for Inequality minimization in PageRank values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahu%2C+S">Subhajit Sahu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">This research study investigates the minimization of inequality in the ranks
of vertices obtained using the PageRank algorithm. PageRank is a widely used
algorithm for ranking webpages and plays a significant role in determining web
traffic. This study employs the Gini coefficient, a measure of income/wealth
inequality, to assess the inequality in PageRank distributions on various types
of graphs. The investigation involves two experiments: one that modifies
strategies for handling dead-end nodes and another that explores six
deterministic methods for reducing inequality. Our findings indicate that a
combination of two distinct heuristics may present an effective strategy for
minimizing inequality.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18538" title="Abstract">arXiv:2310.18538</a> [<a href="/pdf/2310.18538" title="Download PDF">pdf</a>, <a href="/format/2310.18538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Cross-Domain Text-to-SQL Models and Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pourreza%2C+M">Mohammadreza Pourreza</a>, 
<a href="/search/cs?searchtype=author&query=Rafiei%2C+D">Davood Rafiei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Databases (cs.DB); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-to-SQL benchmarks play a crucial role in evaluating the progress made in
the field and the ranking of different models. However, accurately matching a
model-generated SQL query to a reference SQL query in a benchmark fails for
various reasons, such as underspecified natural language queries, inherent
assumptions in both model-generated and reference queries, and the
non-deterministic nature of SQL output under certain conditions. In this paper,
we conduct an extensive study of several prominent cross-domain text-to-SQL
benchmarks and re-evaluate some of the top-performing models within these
benchmarks, by both manually evaluating the SQL queries and rewriting them in
equivalent expressions. Our evaluation reveals that attaining a perfect
performance on these benchmarks is unfeasible due to the multiple
interpretations that can be derived from the provided samples. Furthermore, we
find that the true performance of the models is underestimated and their
relative performance changes after a re-evaluation. Most notably, our
evaluation reveals a surprising discovery: a recent GPT4-based model surpasses
the gold standard reference queries in the Spider benchmark in our human
evaluation. This finding highlights the importance of interpreting benchmark
evaluations cautiously, while also acknowledging the critical role of
additional independent evaluations in driving advancements in the field.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18541" title="Abstract">arXiv:2310.18541</a> [<a href="/pdf/2310.18541" title="Download PDF">pdf</a>, <a href="/format/2310.18541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReConTab: Regularized Contrastive Representation Learning for Tabular  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Suiyao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hovakimyan%2C+N">Naira Hovakimyan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Handong Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Representation learning stands as one of the critical machine learning
techniques across various domains. Through the acquisition of high-quality
features, pre-trained embeddings significantly reduce input space redundancy,
benefiting downstream pattern recognition tasks such as classification,
regression, or detection. Nonetheless, in the domain of tabular data, feature
engineering and selection still heavily rely on manual intervention, leading to
time-consuming processes and necessitating domain expertise. In response to
this challenge, we introduce ReConTab, a deep automatic representation learning
framework with regularized contrastive learning. Agnostic to any type of
modeling task, ReConTab constructs an asymmetric autoencoder based on the same
raw features from model inputs, producing low-dimensional representative
embeddings. Specifically, regularization techniques are applied for raw feature
selection. Meanwhile, ReConTab leverages contrastive learning to distill the
most pertinent information for downstream tasks. Experiments conducted on
extensive real-world datasets substantiate the framework's capacity to yield
substantial and robust performance improvements. Furthermore, we empirically
demonstrate that pre-trained embeddings can seamlessly integrate as easily
adaptable features, enhancing the performance of various traditional methods
such as XGBoost and Random Forest.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18542" title="Abstract">arXiv:2310.18542</a> [<a href="/pdf/2310.18542" title="Download PDF">pdf</a>, <a href="/format/2310.18542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Feature Selection Approach for Learning Skinny Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+S">Shibal Ibrahim</a>, 
<a href="/search/cs?searchtype=author&query=Behdin%2C+K">Kayhan Behdin</a>, 
<a href="/search/cs?searchtype=author&query=Mazumder%2C+R">Rahul Mazumder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Joint feature selection and tree ensemble learning is a challenging task.
Popular tree ensemble toolkits e.g., Gradient Boosted Trees and Random Forests
support feature selection post-training based on feature importances, which are
known to be misleading, and can significantly hurt performance. We propose
Skinny Trees: a toolkit for feature selection in tree ensembles, such that
feature selection and tree ensemble learning occurs simultaneously. It is based
on an end-to-end optimization approach that considers feature selection in
differentiable trees with Group $\ell_0 - \ell_2$ regularization. We optimize
with a first-order proximal method and present convergence guarantees for a
non-convex and non-smooth objective. Interestingly, dense-to-sparse
regularization scheduling can lead to more expressive and sparser tree
ensembles than vanilla proximal method. On 15 synthetic and real-world
datasets, Skinny Trees can achieve $1.5\times$ - $620\times$ feature
compression rates, leading up to $10\times$ faster inference over dense trees,
without any loss in performance. Skinny Trees lead to superior feature
selection than many existing toolkits e.g., in terms of AUC performance for
$25\%$ feature budget, Skinny Trees outperforms LightGBM by $10.2\%$ (up to
$37.7\%$), and Random Forests by $3\%$ (up to $12.5\%$).
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18544" title="Abstract">arXiv:2310.18544</a> [<a href="/pdf/2310.18544" title="Download PDF">pdf</a>, <a href="/format/2310.18544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discourse Structures Guided Fine-grained Propaganda Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yuanyuan Lei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruihong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Propaganda is a form of deceptive narratives that instigate or mislead the
public, usually with a political purpose. In this paper, we aim to identify
propaganda in political news at two fine-grained levels: sentence-level and
token-level. We observe that propaganda content is more likely to be embedded
in sentences that attribute causality or assert contrast to nearby sentences,
as well as seen in opinionated evaluation, speculation and discussions of
future expectation. Hence, we propose to incorporate both local and global
discourse structures for propaganda discovery and construct two teacher models
for identifying PDTB-style discourse relations between nearby sentences and
common discourse roles of sentences in a news article respectively. We further
devise two methods to incorporate the two types of discourse structures for
propaganda identification by either using teacher predicted probabilities as
additional features or soliciting guidance in a knowledge distillation
framework. Experiments on the benchmark dataset demonstrate that leveraging
guidance from discourse structures can significantly improve both precision and
recall of propaganda content identification.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18545" title="Abstract">arXiv:2310.18545</a> [<a href="/pdf/2310.18545" title="Download PDF">pdf</a>, <a href="/format/2310.18545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Conspiracy Theories News based on Event Relation Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yuanyuan Lei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruihong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Conspiracy theories, as a type of misinformation, are narratives that
explains an event or situation in an irrational or malicious manner. While most
previous work examined conspiracy theory in social media short texts, limited
attention was put on such misinformation in long news documents. In this paper,
we aim to identify whether a news article contains conspiracy theories. We
observe that a conspiracy story can be made up by mixing uncorrelated events
together, or by presenting an unusual distribution of relations between events.
Achieving a contextualized understanding of events in a story is essential for
detecting conspiracy theories. Thus, we propose to incorporate an event
relation graph for each article, in which events are nodes, and four common
types of event relations, coreference, temporal, causal, and subevent
relations, are considered as edges. Then, we integrate the event relation graph
into conspiracy theory identification in two ways: an event-aware language
model is developed to augment the basic language model with the knowledge of
events and event relations via soft labels; further, a heterogeneous graph
attention network is designed to derive a graph embedding based on hard labels.
Experiments on a large benchmark dataset show that our approach based on event
relation graph improves both precision and recall of conspiracy theory
identification, and generalizes well for new unseen media sources.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18547" title="Abstract">arXiv:2310.18547</a> [<a href="/pdf/2310.18547" title="Download PDF">pdf</a>, <a href="/format/2310.18547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Punica: Multi-Tenant LoRA Serving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lequn Chen</a> (1), 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zihao Ye</a> (1), 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yongji Wu</a> (2), 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+D">Danyang Zhuo</a> (2), 
<a href="/search/cs?searchtype=author&query=Ceze%2C+L">Luis Ceze</a> (1), 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Arvind Krishnamurthy</a> (1) ((1) University of Washington, (2) Duke University)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Low-rank adaptation (LoRA) has become an important and popular method to
adapt pre-trained models to specific domains. We present Punica, a system to
serve multiple LoRA models in a shared GPU cluster. Punica contains a new CUDA
kernel design that allows batching of GPU operations for different LoRA models.
This allows a GPU to hold only a single copy of the underlying pre-trained
model when serving multiple, different LoRA models, significantly enhancing GPU
efficiency in terms of both memory and computation. Our scheduler consolidates
multi-tenant LoRA serving workloads in a shared GPU cluster. With a fixed-sized
GPU cluster, our evaluations show that Punica achieves 12x higher throughput in
serving multiple LoRA models compared to state-of-the-art LLM serving systems
while only adding 2ms latency per token. Punica is open source at
https://github.com/punica-ai/punica .
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18548" title="Abstract">arXiv:2310.18548</a> [<a href="/pdf/2310.18548" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEDAVET: Traffic Vehicle Anomaly Detection Mechanism based on spatial  and temporal structures in vehicle traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reyna%2C+A+R+H">Ana Rosal&#xed;a Huam&#xe1;n Reyna</a>, 
<a href="/search/cs?searchtype=author&query=Farf%C3%A1n%2C+A+J+F">Alex Josu&#xe9; Fl&#xf3;rez Farf&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Filho%2C+G+P+R">Geraldo Pereira Rocha Filho</a>, 
<a href="/search/cs?searchtype=author&query=Sampaio%2C+S">Sandra Sampaio</a>, 
<a href="/search/cs?searchtype=author&query=de+Grande%2C+R">Robson de Grande</a>, 
<a href="/search/cs?searchtype=author&query=Hideo%2C+L">Luis Hideo</a>, 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+V">Vasconcelos Nakamura</a>, 
<a href="/search/cs?searchtype=author&query=Meneguette%2C+R+I">Rodolfo Ipolito Meneguette</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures, submitted to Journal of Internet Services and Applications - JISA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Currently, there are computer vision systems that help us with tasks that
would be dull for humans, such as surveillance and vehicle tracking. An
important part of this analysis is to identify traffic anomalies. An anomaly
tells us that something unusual has happened, in this case on the highway. This
paper aims to model vehicle tracking using computer vision to detect traffic
anomalies on a highway. We develop the steps of detection, tracking, and
analysis of traffic: the detection of vehicles from video of urban traffic, the
tracking of vehicles using a bipartite graph and the Convex Hull algorithm to
delimit moving areas. Finally for anomaly detection we use two data structures
to detect the beginning and end of the anomaly. The first is the QuadTree that
groups vehicles that are stopped for a long time on the road and the second
that approaches vehicles that are occluded. Experimental results show that our
method is acceptable on the Track4 test set, with an F1 score of 85.7% and a
mean squared error of 25.432.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18549" title="Abstract">arXiv:2310.18549</a> [<a href="/pdf/2310.18549" title="Download PDF">pdf</a>, <a href="/format/2310.18549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Intrinsic Decomposition with Adversarial Learning for Hyperspectral  Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zhiqiang Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wen Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE TIP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Convolutional neural networks (CNNs) have been demonstrated their powerful
ability to extract discriminative features for hyperspectral image
classification. However, general deep learning methods for CNNs ignore the
influence of complex environmental factor which enlarges the intra-class
variance and decreases the inter-class variance. This multiplies the difficulty
to extract discriminative features. To overcome this problem, this work
develops a novel deep intrinsic decomposition with adversarial learning, namely
AdverDecom, for hyperspectral image classification to mitigate the negative
impact of environmental factors on classification performance. First, we
develop a generative network for hyperspectral image (HyperNet) to extract the
environmental-related feature and category-related feature from the image.
Then, a discriminative network is constructed to distinguish different
environmental categories. Finally, a environmental and category joint learning
loss is developed for adversarial learning to make the deep model learn
discriminative features. Experiments are conducted over three commonly used
real-world datasets and the comparison results show the superiority of the
proposed method. The implementation of the proposed method and other compared
methods could be accessed at https://github.com/shendu-sw/Adversarial Learning
Intrinsic Decomposition for the sake of reproducibility.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18550" title="Abstract">arXiv:2310.18550</a> [<a href="/pdf/2310.18550" title="Download PDF">pdf</a>, <a href="/format/2310.18550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiScale Spectral-Spatial Convolutional Transformer for Hyperspectral  Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zhiqiang Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wen Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE GRSL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Due to the powerful ability in capturing the global information, Transformer
has become an alternative architecture of CNNs for hyperspectral image
classification. However, general Transformer mainly considers the global
spectral information while ignores the multiscale spatial information of the
hyperspectral image. In this paper, we propose a multiscale spectral-spatial
convolutional Transformer (MultiscaleFormer) for hyperspectral image
classification. First, the developed method utilizes multiscale spatial patches
as tokens to formulate the spatial Transformer and generates multiscale spatial
representation of each band in each pixel. Second, the spatial representation
of all the bands in a given pixel are utilized as tokens to formulate the
spectral Transformer and generate the multiscale spectral-spatial
representation of each pixel. Besides, a modified spectral-spatial CAF module
is constructed in the MultiFormer to fuse cross-layer spectral and spatial
information. Therefore, the proposed MultiFormer can capture the multiscale
spectral-spatial information and provide better performance than most of other
architectures for hyperspectral image classification. Experiments are conducted
over commonly used real-world datasets and the comparison results show the
superiority of the proposed method.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18553" title="Abstract">arXiv:2310.18553</a> [<a href="/pdf/2310.18553" title="Download PDF">pdf</a>, <a href="/format/2310.18553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Affective Polarization in Social Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldman%2C+D">Dan Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+A">Ashwin Rao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zihao He</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+K">Kristina Lerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Affective polarization has grown dramatically in recent years, with surveys
showing that liberals and conservatives not only disagree on policy issues but
also dislike and distrust each other. While studies have implicated social
media in amplifying polarization, there is a lack of agreement on the
mechanisms driving affective polarization and methods to measure it. Our paper
addresses these gaps. First, we directly measure affective polarization on
social media by quantifying the emotional tone of reply interactions between
users. As predicted by affective polarization, in-group interactions between
same-partisanship users tend to be positive, while out-group interactions
between opposite-partisanship users are characterized by negativity and
toxicity. Second, we show that affective polarization generalizes beyond the
in-group/out-group dichotomy and can be considered a structural property of
social networks. Specifically, we show that emotions vary with network distance
between users, with closer interactions eliciting positive emotions and more
distant interactions leading to anger, disgust, and toxicity. These findings
are consistent across diverse datasets and languages, spanning discussions on
topics such as the Covid-19 pandemic, abortion, and the 2017 French Election.
Our research provides new insights into the complex social dynamics of
affective polarization in the digital age and its implications for political
discourse.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18555" title="Abstract">arXiv:2310.18555</a> [<a href="/pdf/2310.18555" title="Download PDF">pdf</a>, <a href="/format/2310.18555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group Robust Classification Without Any Group Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsirigotis%2C+C">Christos Tsirigotis</a>, 
<a href="/search/cs?searchtype=author&query=Monteiro%2C+J">Joao Monteiro</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+P">Pau Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Vazquez%2C+D">David Vazquez</a>, 
<a href="/search/cs?searchtype=author&query=Courville%2C+A">Aaron Courville</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Code is available at <a href="https://github.com/tsirif/uLA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Empirical risk minimization (ERM) is sensitive to spurious correlations in
the training data, which poses a significant risk when deploying systems
trained under this paradigm in high-stake applications. While the existing
literature focuses on maximizing group-balanced or worst-group accuracy,
estimating these accuracies is hindered by costly bias annotations. This study
contends that current bias-unsupervised approaches to group robustness continue
to rely on group information to achieve optimal performance. Firstly, these
methods implicitly assume that all group combinations are represented during
training. To illustrate this, we introduce a systematic generalization task on
the MPI3D dataset and discover that current algorithms fail to improve the ERM
baseline when combinations of observed attribute values are missing. Secondly,
bias labels are still crucial for effective model selection, restricting the
practicality of these methods in real-world scenarios. To address these
limitations, we propose a revised methodology for training and validating
debiased models in an entirely bias-unsupervised manner. We achieve this by
employing pretrained self-supervised models to reliably extract bias
information, which enables the integration of a logit adjustment training loss
with our validation criterion. Our empirical analysis on synthetic and
real-world tasks provides evidence that our approach overcomes the identified
challenges and consistently enhances robust accuracy, attaining performance
which is competitive with or outperforms that of state-of-the-art methods,
which, conversely, rely on bias labels for validation.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18562" title="Abstract">arXiv:2310.18562</a> [<a href="/pdf/2310.18562" title="Download PDF">pdf</a>, <a href="/format/2310.18562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization-Free Test-Time Adaptation for Cross-Person Activity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuoyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+H">HuaJun Xi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bob Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented at UbiComp 2024; Accepted by Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Human Activity Recognition (HAR) models often suffer from performance
degradation in real-world applications due to distribution shifts in activity
patterns across individuals. Test-Time Adaptation (TTA) is an emerging learning
paradigm that aims to utilize the test stream to adjust predictions in
real-time inference, which has not been explored in HAR before. However, the
high computational cost of optimization-based TTA algorithms makes it
intractable to run on resource-constrained edge devices. In this paper, we
propose an Optimization-Free Test-Time Adaptation (OFTTA) framework for
sensor-based HAR. OFTTA adjusts the feature extractor and linear classifier
simultaneously in an optimization-free manner. For the feature extractor, we
propose Exponential DecayTest-time Normalization (EDTN) to replace the
conventional batch normalization (CBN) layers. EDTN combines CBN and Test-time
batch Normalization (TBN) to extract reliable features against domain shifts
with TBN's influence decreasing exponentially in deeper layers. For the
classifier, we adjust the prediction by computing the distance between the
feature and the prototype, which is calculated by a maintained support set. In
addition, the update of the support set is based on the pseudo label, which can
benefit from reliable features extracted by EDTN. Extensive experiments on
three public cross-person HAR datasets and two different TTA settings
demonstrate that OFTTA outperforms the state-of-the-art TTA approaches in both
classification performance and computational efficiency. Finally, we verify the
superiority of our proposed OFTTA on edge devices, indicating possible
deployment in real applications. Our code is available at
\href{https://github.com/Claydon-Wang/OFTTA}{this https URL}.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18564" title="Abstract">arXiv:2310.18564</a> [<a href="/pdf/2310.18564" title="Download PDF">pdf</a>, <a href="/format/2310.18564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Framework for Robust G-Invariance in G-Equivariant Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanborn%2C+S">Sophia Sanborn</a>, 
<a href="/search/cs?searchtype=author&query=Miolane%2C+N">Nina Miolane</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 37th Conference on Neural Information
  Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We introduce a general method for achieving robust group-invariance in
group-equivariant convolutional neural networks ($G$-CNNs), which we call the
$G$-triple-correlation ($G$-TC) layer. The approach leverages the theory of the
triple-correlation on groups, which is the unique, lowest-degree polynomial
invariant map that is also complete. Many commonly used invariant maps - such
as the max - are incomplete: they remove both group and signal structure. A
complete invariant, by contrast, removes only the variation due to the actions
of the group, while preserving all information about the structure of the
signal. The completeness of the triple correlation endows the $G$-TC layer with
strong robustness, which can be observed in its resistance to invariance-based
adversarial attacks. In addition, we observe that it yields measurable
improvements in classification accuracy over standard Max $G$-Pooling in
$G$-CNN architectures. We provide a general and efficient implementation of the
method for any discretized group, which requires only a table defining the
group's product structure. We demonstrate the benefits of this method for
$G$-CNNs defined on both commutative and non-commutative groups - $SO(2)$,
$O(2)$, $SO(3)$, and $O(3)$ (discretized as the cyclic $C8$, dihedral $D16$,
chiral octahedral $O$ and full octahedral $O_h$ groups) - acting on
$\mathbb{R}^2$ and $\mathbb{R}^3$ on both $G$-MNIST and $G$-ModelNet10
datasets.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18568" title="Abstract">arXiv:2310.18568</a> [<a href="/pdf/2310.18568" title="Download PDF">pdf</a>, <a href="/ps/2310.18568" title="Download PostScript">ps</a>, <a href="/format/2310.18568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the second-order zero differential spectra of some power functions  over finite fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Man%2C+Y">Yuying Man</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nian Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Z">Zejun Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xiangyong Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Boukerrou et al. (IACR Trans. Symmetric Cryptol. 2020(1), 331-362) introduced
the notion of Feistel Boomerang Connectivity Table (FBCT), the Feistel
counterpart of the Boomerang Connectivity Table (BCT), and the Feistel
boomerang uniformity (which is the same as the second-order zero differential
uniformity in even characteristic). FBCT is a crucial table for the analysis of
the resistance of block ciphers to power attacks such as differential and
boomerang attacks. It is worth noting that the coefficients of FBCT are related
to the second-order zero differential spectra of functions. In this paper, by
carrying out certain finer manipulations of solving specific equations over the
finite field $\mathbb{F}_{p^n}$, we explicitly determine the second-order zero
differential spectra of some power functions with low differential uniformity,
and show that our considered functions also have low second-order zero
differential uniformity. Our study pushes further former investigations on
second-order zero differential uniformity and Feistel boomerang differential
uniformity for a power function $F$.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18569" title="Abstract">arXiv:2310.18569</a> [<a href="/pdf/2310.18569" title="Download PDF">pdf</a>, <a href="/format/2310.18569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Grasping Performance of Novel Objects through an Improved  Fine-Tuning Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangsheng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Grasping algorithms have evolved from planar depth grasping to utilizing
point cloud information, allowing for application in a wider range of
scenarios. However, data-driven grasps based on models trained on basic
open-source datasets may not perform well on novel objects, which are often
required in different scenarios, necessitating fine-tuning using new objects.
The data driving these algorithms essentially corresponds to the closing region
of the hand in 6D pose, and due to the uniqueness of 6D pose, synthetic
annotation or real-machine annotation methods are typically employed. Acquiring
large amounts of data with real-machine annotation is challenging, making
synthetic annotation a common practice. However, obtaining annotated 6D pose
data using conventional methods is extremely time-consuming. Therefore, we
propose a method to quickly acquire data for novel objects, enabling more
efficient fine-tuning. Our method primarily samples grasp orientations to
generate and annotate grasps. Experimental results demonstrate that our
fine-tuning process for a new object is 400 \% faster than other methods.
Furthermore, we propose an optimized grasp annotation framework that accounts
for the effects of the gripper closing, making the annotations more reasonable.
Upon acceptance of this paper, we will release our algorithm as open-source.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18573" title="Abstract">arXiv:2310.18573</a> [<a href="/pdf/2310.18573" title="Download PDF">pdf</a>, <a href="/format/2310.18573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Channel Estimation Performance for Uplink OTFS Transmissions:  Pilot Design based on A Posteriori Cramer-Rao Bound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+M">Mingcheng Nie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuangyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+D">Deepak Mishra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Orthogonal time frequency space (OTFS) has been widely acknowledged as a
promising wireless technology for challenging transmission scenarios, including
high-mobility channels. In this paper, we investigate the pilot design for the
multi-user OTFS system based on the a priori statistical channel state
information (CSI), where the practical threshold-based estimation scheme is
adopted. Specifically, we first derive the a posteriori Cramer-Rao bound (PCRB)
based on a priori channel information for each user. According to our
derivation, the PCRB only relates to the user's pilot signal-to-noise ratio
(SNR) and the range of delay and Doppler shifts under the practical power-delay
and power-Doppler profiles. Then, a pilot scheme is proposed to minimize the
average PCRB of different users, where a closed-form global optimal pilot power
allocation is derived. Our numerical results verify the multi-user PCRB
analysis. Also, we demonstrate an around 3 dB improvement in the average
normalized-mean-square error (NMSE) by using the proposed pilot design in
comparison to the conventional embedded pilot design under the same total pilot
power.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18574" title="Abstract">arXiv:2310.18574</a> [<a href="/pdf/2310.18574" title="Download PDF">pdf</a>, <a href="/format/2310.18574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Trilemma of Privacy, Utility, Efficiency via Controllable  Machine Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+G">Guangyao Dou</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yijun Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chunhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chien%2C+E">Eli Chien</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Ziwei Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine Unlearning (MU) algorithms have become increasingly critical due to
the imperative adherence to data privacy regulations. The primary objective of
MU is to erase the influence of specific data samples on a given model without
the need to retrain it from scratch. Accordingly, existing methods focus on
maximizing user privacy protection. However, there are different degrees of
privacy regulations for each real-world web-based application. Exploring the
full spectrum of trade-offs between privacy, model utility, and runtime
efficiency is critical for practical unlearning scenarios. Furthermore,
designing the MU algorithm with simple control of the aforementioned trade-off
is desirable but challenging due to the inherent complex interaction. To
address the challenges, we present Controllable Machine Unlearning (ConMU), a
novel framework designed to facilitate the calibration of MU. The ConMU
framework contains three integral modules: an important data selection module
that reconciles the runtime efficiency and model generalization, a progressive
Gaussian mechanism module that balances privacy and model generalization, and
an unlearning proxy that controls the trade-offs between privacy and runtime
efficiency. Comprehensive experiments on various benchmark datasets have
demonstrated the robust adaptability of our control mechanism and its
superiority over established unlearning methods. ConMU explores the full
spectrum of the Privacy-Utility-Efficiency trade-off and allows practitioners
to account for different real-world regulations. Source code available at:
https://github.com/guangyaodou/ConMU.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18577" title="Abstract">arXiv:2310.18577</a> [<a href="/pdf/2310.18577" title="Download PDF">pdf</a>, <a href="/format/2310.18577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QoS Aware Transmit Beamforming for Secure Backscattering in Symbiotic  Radio Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+M">Mingcheng Nie</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+D">Deepak Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Al-nahari%2C+A">Azzam Al-nahari</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jinhong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Jantti%2C+R">Riku Jantti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper focuses on secure backscatter transmission in the presence of a
passive multi-antenna eavesdropper through a symbiotic radio (SR) network.
Specifically, a single-antenna backscatter device (BD) aims to transmit
confidential information to a primary receiver (PR) by using a multi-antenna
primary transmitter's (PT) signal, where the received symbols are jointly
decoded at the PR. Our objective is to achieve confidential communications for
BD while ensuring that the primary system's quality of service (QoS)
requirements are met. We propose an alternating optimisation algorithm that
maximises the achievable secrecy rate of BD by jointly optimising primary
transmit beamforming and power sharing between information and artificial noise
(AN) signals. Numerical results verify our analytical claims on the optimality
of the proposed solution and the proposed methodology's underlying low
complexity. Additionally, our simulations provide nontrivial design insights
into the critical system parameters and quantify the achievable gains over the
relevant benchmark schemes.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18581" title="Abstract">arXiv:2310.18581</a> [<a href="/pdf/2310.18581" title="Download PDF">pdf</a>, <a href="/format/2310.18581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating LLM Inference by Enabling Intermediate Layer Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Varshney%2C+N">Neeraj Varshney</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+A">Agneet Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Parmar%2C+M">Mihir Parmar</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+C">Chitta Baral</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved remarkable performance across a
wide variety of natural language tasks; however, their large size makes their
inference slow and computationally expensive which poses a practical challenge
for resource constrained real-world applications. Focusing on this problem, we
propose to instruction tune LLMs in a way that enables intermediate layer
decoding for efficiently generating text, but importantly without compromising
the quality of the generation. Specifically, we instruction tune LLMs with
additional explicit Losses from the InTermediate layErs (LITE) and show that it
enables these layers to acquire 'good' generation ability without affecting the
generation ability of the final layer. We perform 'dynamic confidence-based
early exiting' at token level from the intermediate layers which improves the
efficiency of inference while maintaining the generation quality. We conduct
comprehensive experiments by instruction tuning LLaMA-2 models on the widely
used Alpaca dataset and holistically evaluate on four different
human-instruction test sets: Vicuna, WizardLM, Koala, and Self-Instruct. We
show that 'dynamic early exiting' achieves consistent and considerable cost
improvements (37.86% on average) while maintaining the generation quality of
the responses. We further conduct a thorough analysis of the results over
several important aspects, such as comparing the semantic similarity of the
outputs and dissecting the efficiency improvements by comparing the number of
tokens generated in the output. In summary, our work contributes to improving
the efficiency of LLM inference while maintaining the generation quality, a
crucial step en route to enabling their widespread adoption.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18583" title="Abstract">arXiv:2310.18583</a> [<a href="/pdf/2310.18583" title="Download PDF">pdf</a>, <a href="/ps/2310.18583" title="Download PostScript">ps</a>, <a href="/format/2310.18583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Multi-Modality Learning for Multi-Label Skin Lesion  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+E">Euijoon Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+L">Lei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinman Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The clinical diagnosis of skin lesion involves the analysis of dermoscopic
and clinical modalities. Dermoscopic images provide a detailed view of the
surface structures whereas clinical images offer a complementary macroscopic
information. The visual diagnosis of melanoma is also based on seven-point
checklist which involves identifying different visual attributes. Recently,
supervised learning approaches such as convolutional neural networks (CNNs)
have shown great performances using both dermoscopic and clinical modalities
(Multi-modality). The seven different visual attributes in the checklist are
also used to further improve the the diagnosis. The performances of these
approaches, however, are still reliant on the availability of large-scaled
labeled data. The acquisition of annotated dataset is an expensive and
time-consuming task, more so with annotating multi-attributes. To overcome this
limitation, we propose a self-supervised learning (SSL) algorithm for
multi-modality skin lesion classification. Our algorithm enables the
multi-modality learning by maximizing the similarities between paired
dermoscopic and clinical images from different views. In addition, we generate
surrogate pseudo-multi-labels that represent seven attributes via clustering
analysis. We also propose a label-relation-aware module to refine each
pseudo-label embedding and capture the interrelationships between
pseudo-multi-labels. We validated the effectiveness of our algorithm using
well-benchmarked seven-point skin lesion dataset. Our results show that our
algorithm achieved better performances than other state-of-the-art SSL
counterparts.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18585" title="Abstract">arXiv:2310.18585</a> [<a href="/pdf/2310.18585" title="Download PDF">pdf</a>, <a href="/format/2310.18585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Explanations via Iterated Integrated Attributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barkan%2C+O">Oren Barkan</a>, 
<a href="/search/cs?searchtype=author&query=Elisha%2C+Y">Yehonatan Elisha</a>, 
<a href="/search/cs?searchtype=author&query=Asher%2C+Y">Yuval Asher</a>, 
<a href="/search/cs?searchtype=author&query=Eshel%2C+A">Amit Eshel</a>, 
<a href="/search/cs?searchtype=author&query=Koenigstein%2C+N">Noam Koenigstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce Iterated Integrated Attributions (IIA) - a generic method for
explaining the predictions of vision models. IIA employs iterative integration
across the input image, the internal representations generated by the model,
and their gradients, yielding precise and focused explanation maps. We
demonstrate the effectiveness of IIA through comprehensive evaluations across
various tasks, datasets, and network architectures. Our results showcase that
IIA produces accurate explanation maps, outperforming other state-of-the-art
explanation techniques.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18586" title="Abstract">arXiv:2310.18586</a> [<a href="/pdf/2310.18586" title="Download PDF">pdf</a>, <a href="/format/2310.18586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport for Kernel Gaussian Mixture Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+J+H">Jung Hun Oh</a>, 
<a href="/search/cs?searchtype=author&query=Elkin%2C+R">Rena Elkin</a>, 
<a href="/search/cs?searchtype=author&query=Simhal%2C+A+K">Anish Kumar Simhal</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiening Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Deasy%2C+J+O">Joseph O Deasy</a>, 
<a href="/search/cs?searchtype=author&query=Tannenbaum%2C+A">Allen Tannenbaum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The Wasserstein distance from optimal mass transport (OMT) is a powerful
mathematical tool with numerous applications that provides a natural measure of
the distance between two probability distributions. Several methods to
incorporate OMT into widely used probabilistic models, such as Gaussian or
Gaussian mixture, have been developed to enhance the capability of modeling
complex multimodal densities of real datasets. However, very few studies have
explored the OMT problems in a reproducing kernel Hilbert space (RKHS), wherein
the kernel trick is utilized to avoid the need to explicitly map input data
into a high-dimensional feature space. In the current study, we propose a
Wasserstein-type metric to compute the distance between two Gaussian mixtures
in a RKHS via the kernel trick, i.e., kernel Gaussian mixture models.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18587" title="Abstract">arXiv:2310.18587</a> [<a href="/pdf/2310.18587" title="Download PDF">pdf</a>, <a href="/format/2310.18587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing and Improving Syntactic Adversarial Robustness of Pre-trained  Models for Code Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tingting Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Taolue Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context: Pre-trained models (PTMs) have demonstrated significant potential in
automatic code translation. However, the vulnerability of these models in
translation tasks, particularly in terms of syntax, has not been extensively
investigated. Objective: To fill this gap, our study aims to propose a novel
approach CoTR to assess and improve the syntactic adversarial robustness of
PTMs in code translation. Method: CoTR consists of two components: CoTR-A and
CoTR-D. CoTR-A generates adversarial examples by transforming programs, while
CoTR-D proposes a semantic distance-based sampling data augmentation method and
adversarial training method to improve the model's robustness and
generalization capabilities. The Pass@1 metric is used by CoTR to assess the
performance of PTMs, which is more suitable for code translation tasks and
offers a more precise evaluation in real world scenarios. Results: The
effectiveness of CoTR is evaluated through experiments on real world Java to
Python datasets. The results demonstrate that CoTR-A can significantly reduce
the performance of existing PTMs, while CoTR-D effectively improves the
robustness of PTMs. Conclusion: Our study identifies the limitations of current
PTMs, including large language models, in code translation tasks. It highlights
the potential of CoTR as an effective solution to enhance the robustness of
PTMs for code translation tasks.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18589" title="Abstract">arXiv:2310.18589</a> [<a href="/pdf/2310.18589" title="Download PDF">pdf</a>, <a href="/format/2310.18589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> This Looks Like Those: Illuminating Prototypical Concepts Using Multiple  Visualizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chiyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Brandon Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chaofan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rudin%2C+C">Cynthia Rudin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present ProtoConcepts, a method for interpretable image classification
combining deep learning and case-based reasoning using prototypical parts.
Existing work in prototype-based image classification uses a ``this looks like
that'' reasoning process, which dissects a test image by finding prototypical
parts and combining evidence from these prototypes to make a final
classification. However, all of the existing prototypical part-based image
classifiers provide only one-to-one comparisons, where a single training image
patch serves as a prototype to compare with a part of our test image. With
these single-image comparisons, it can often be difficult to identify the
underlying concept being compared (e.g., ``is it comparing the color or the
shape?''). Our proposed method modifies the architecture of prototype-based
networks to instead learn prototypical concepts which are visualized using
multiple image patches. Having multiple visualizations of the same prototype
allows us to more easily identify the concept captured by that prototype (e.g.,
``the test image and the related training patches are all the same shade of
blue''), and allows our model to create richer, more interpretable visual
explanations. Our experiments show that our ``this looks like those'' reasoning
process can be applied as a modification to a wide range of existing
prototypical image classification networks while achieving comparable accuracy
on benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18590" title="Abstract">arXiv:2310.18590</a> [<a href="/pdf/2310.18590" title="Download PDF">pdf</a>, <a href="/format/2310.18590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Early Readouts to Mediate Featural Bias in Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+R">Rishabh Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Sivasubramanian%2C+D">Durga Sivasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Mekala%2C+A">Anmol Mekala</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Shenoy%2C+P">Pradeep Shenoy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep networks tend to learn spurious feature-label correlations in real-world
supervised learning tasks. This vulnerability is aggravated in distillation,
where a student model may have lesser representational capacity than the
corresponding teacher model. Often, knowledge of specific spurious correlations
is used to reweight instances &amp; rebalance the learning process. We propose a
novel early readout mechanism whereby we attempt to predict the label using
representations from earlier network layers. We show that these early readouts
automatically identify problem instances or groups in the form of confident,
incorrect predictions. Leveraging these signals to modulate the distillation
loss on an instance level allows us to substantially improve not only group
fairness measures across benchmark datasets, but also overall accuracy of the
student model. We also provide secondary analyses that bring insight into the
role of feature learning in supervision and distillation.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18596" title="Abstract">arXiv:2310.18596</a> [<a href="/pdf/2310.18596" title="Download PDF">pdf</a>, <a href="/format/2310.18596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Hard is Takeover in DPoS Blockchains? Understanding the Security of  Coin-based Voting Governance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chao Li</a>, 
<a href="/search/cs?searchtype=author&query=Palanisamy%2C+B">Balaji Palanisamy</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Runhua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+L">Li Duan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiqiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted by ACM CCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Delegated-Proof-of-Stake (DPoS) blockchains, such as EOSIO, Steem and TRON,
are governed by a committee of block producers elected via a coin-based voting
system. We recently witnessed the first de facto blockchain takeover that
happened between Steem and TRON. Within one hour of this incident, TRON founder
took over the entire Steem committee, forcing the original Steem community to
leave the blockchain that they maintained for years. This is a historical event
in the evolution of blockchains and Web 3.0. Despite its significant disruptive
impact, little is known about how vulnerable DPoS blockchains are in general to
takeovers and the ways in which we can improve their resistance to takeovers.
<br />In this paper, we demonstrate that the resistance of a DPoS blockchain to
takeovers is governed by both the theoretical design and the actual use of its
underlying coin-based voting governance system. When voters actively cooperate
to resist potential takeovers, our theoretical analysis reveals that the
current active resistance of DPoS blockchains is far below the theoretical
upper bound. However in practice, voter preferences could be significantly
different. This paper presents the first large-scale empirical study of the
passive takeover resistance of EOSIO, Steem and TRON. Our study identifies the
diversity in voter preferences and characterizes the impact of this diversity
on takeover resistance. Through both theoretical and empirical analyses, our
study provides novel insights into the security of coin-based voting governance
and suggests potential ways to improve the takeover resistance of any
blockchain that implements this governance model.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18598" title="Abstract">arXiv:2310.18598</a> [<a href="/pdf/2310.18598" title="Download PDF">pdf</a>, <a href="/format/2310.18598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Generalisation via Risk Distribution Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Toan Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+K">Kien Do</a>, 
<a href="/search/cs?searchtype=author&query=Duong%2C+B">Bao Duong</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thin Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose a novel approach for domain generalisation (DG) leveraging risk
distributions to characterise domains, thereby achieving domain invariance. In
our findings, risk distributions effectively highlight differences between
training domains and reveal their inherent complexities. In testing, we may
observe similar, or potentially intensifying in magnitude, divergences between
risk distributions. Hence, we propose a compelling proposition: Minimising the
divergences between risk distributions across training domains leads to robust
invariance for DG. The key rationale behind this concept is that a model,
trained on domain-invariant or stable features, may consistently produce
similar risk distributions across various domains. Building upon this idea, we
propose Risk Distribution Matching (RDM). Using the maximum mean discrepancy
(MMD) distance, RDM aims to minimise the variance of risk distributions across
training domains. However, when the number of domains increases, the direct
optimisation of variance leads to linear growth in MMD computations, resulting
in inefficiency. Instead, we propose an approximation that requires only one
MMD computation, by aligning just two distributions: that of the worst-case
domain and the aggregated distribution from all domains. Notably, this method
empirically outperforms optimising distributional variance while being
computationally more efficient. Unlike conventional DG matching algorithms, RDM
stands out for its enhanced efficacy by concentrating on scalar risk
distributions, sidestepping the pitfalls of high-dimensional challenges seen in
feature or gradient matching. Our extensive experiments on standard benchmark
datasets demonstrate that RDM shows superior generalisation capability over
state-of-the-art DG methods.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18600" title="Abstract">arXiv:2310.18600</a> [<a href="/pdf/2310.18600" title="Download PDF">pdf</a>, <a href="/format/2310.18600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of  Indian Legal Case Judgments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Datta%2C+D">Debtanu Datta</a>, 
<a href="/search/cs?searchtype=author&query=Soni%2C+S">Shubham Soni</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+R">Rajdeep Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Saptarshi Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automatic summarization of legal case judgments is a practically important
problem that has attracted substantial research efforts in many countries. In
the context of the Indian judiciary, there is an additional complexity --
Indian legal case judgments are mostly written in complex English, but a
significant portion of India's population lacks command of the English
language. Hence, it is crucial to summarize the legal documents in Indian
languages to ensure equitable access to justice. While prior research primarily
focuses on summarizing legal case judgments in their source languages, this
study presents a pioneering effort toward cross-lingual summarization of
English legal documents into Hindi, the most frequently spoken Indian language.
We construct the first high-quality legal corpus comprising of 3,122 case
judgments from prominent Indian courts in English, along with their summaries
in both English and Hindi, drafted by legal practitioners. We benchmark the
performance of several diverse summarization approaches on our corpus and
demonstrate the need for further research in cross-lingual summarization in the
legal domain.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18602" title="Abstract">arXiv:2310.18602</a> [<a href="/pdf/2310.18602" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Device-Edge Cooperative Fine-Tuning of Foundation Models as a 6G Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaibin Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Foundation models (FoMos), referring to large-scale AI models, possess
human-like capabilities and are able to perform competitively in the domain of
human intelligence. The breakthrough in FoMos has inspired researchers to
deploy such models in the sixth-generation (6G) mobile networks for automating
a broad range of tasks in next-generation mobile applications. While the sizes
of FoMos are reaching their peaks, their next phase is expected to focus on
fine-tuning the models to specific downstream tasks. This inspires us to
propose the vision of FoMo fine-tuning as a 6G service. Its key feature is the
exploitation of existing parameter-efficient fine-tuning (PEFT) techniques to
tweak only a small fraction of model weights for a FoMo to become customized
for a specific task. To materialize the said vision, we survey the
state-of-the-art PEFT and then present a novel device-edge fine-tuning (DEFT)
framework for providing efficient and privacy-preserving fine-tuning services
at the 6G network edge. The framework consists of the following comprehensive
set of techniques: 1) Control of fine-tuning parameter sizes in different
transformer blocks of a FoMo; 2) Over-the-air computation for realizing neural
connections in DEFT; 3) Federated DEFT in a multi-device system by downloading
a FoMo emulator or gradients; 4) On-the-fly prompt-ensemble tuning; 5)
Device-to-device prompt transfer among devices. Experiments are conducted using
pre-trained FoMos with up to 11 billion parameters to demonstrate the
effectiveness of DEFT techniques. The article is concluded by presenting future
research opportunities.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18603" title="Abstract">arXiv:2310.18603</a> [<a href="/pdf/2310.18603" title="Download PDF">pdf</a>, <a href="/format/2310.18603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Are Better Adversaries: Exploring Generative  Clean-Label Backdoor Attacks Against Text Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+W">Wencong You</a>, 
<a href="/search/cs?searchtype=author&query=Hammoudeh%2C+Z">Zayd Hammoudeh</a>, 
<a href="/search/cs?searchtype=author&query=Lowd%2C+D">Daniel Lowd</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Backdoor attacks manipulate model predictions by inserting innocuous triggers
into training and test data. We focus on more realistic and more challenging
clean-label attacks where the adversarial training examples are correctly
labeled. Our attack, LLMBkd, leverages language models to automatically insert
diverse style-based triggers into texts. We also propose a poison selection
technique to improve the effectiveness of both LLMBkd as well as existing
textual backdoor attacks. Lastly, we describe REACT, a baseline defense to
mitigate backdoor attacks via antidote training examples. Our evaluations
demonstrate LLMBkd's effectiveness and efficiency, where we consistently
achieve high attack success rates across a wide range of styles with little
effort and no model training.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18604" title="Abstract">arXiv:2310.18604</a> [<a href="/pdf/2310.18604" title="Download PDF">pdf</a>, <a href="/format/2310.18604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anaphor Assisted Document-Level Relation Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chonggang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Richong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Kai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaein Kim</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cunwang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yongyi Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Document-level relation extraction (DocRE) involves identifying relations
between entities distributed in multiple sentences within a document. Existing
methods focus on building a heterogeneous document graph to model the internal
structure of an entity and the external interaction between entities. However,
there are two drawbacks in existing methods. On one hand, anaphor plays an
important role in reasoning to identify relations between entities but is
ignored by these methods. On the other hand, these methods achieve
cross-sentence entity interactions implicitly by utilizing a document or
sentences as intermediate nodes. Such an approach has difficulties in learning
fine-grained interactions between entities across different sentences,
resulting in sub-optimal performance. To address these issues, we propose an
Anaphor-Assisted (AA) framework for DocRE tasks. Experimental results on the
widely-used datasets demonstrate that our model achieves a new state-of-the-art
performance.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18605" title="Abstract">arXiv:2310.18605</a> [<a href="/pdf/2310.18605" title="Download PDF">pdf</a>, <a href="/format/2310.18605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TorchDEQ: A Library for Deep Equilibrium Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+Z">Zhengyang Geng</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+J+Z">J. Zico Kolter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep Equilibrium (DEQ) Models, an emerging class of implicit models that maps
inputs to fixed points of neural networks, are of growing interest in the deep
learning community. However, training and applying DEQ models is currently done
in an ad-hoc fashion, with various techniques spread across the literature. In
this work, we systematically revisit DEQs and present TorchDEQ, an
out-of-the-box PyTorch-based library that allows users to define, train, and
infer using DEQs over multiple domains with minimal code and best practices.
Using TorchDEQ, we build a ``DEQ Zoo'' that supports six published implicit
models across different domains. By developing a joint framework that
incorporates the best practices across all models, we have substantially
improved the performance, training stability, and efficiency of DEQs on ten
datasets across all six projects in the DEQ Zoo. TorchDEQ and DEQ Zoo are
released as \href{https://github.com/locuslab/torchdeq}{open source}.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18606" title="Abstract">arXiv:2310.18606</a> [<a href="/pdf/2310.18606" title="Download PDF">pdf</a>, <a href="/format/2310.18606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where have you been? A Study of Privacy Risk for Point-of-Interest  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+K">Kunlin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shand%2C+W">Will Shand</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Z">Zhiqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Desheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+J">Jianfeng Chi</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuan Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">As location-based services (LBS) have grown in popularity, the collection of
human mobility data has become increasingly extensive to build machine learning
(ML) models offering enhanced convenience to LBS users. However, the
convenience comes with the risk of privacy leakage since this type of data
might contain sensitive information related to user identities, such as
home/work locations. Prior work focuses on protecting mobility data privacy
during transmission or prior to release, lacking the privacy risk evaluation of
mobility data-based ML models. To better understand and quantify the privacy
leakage in mobility data-based ML models, we design a privacy attack suite
containing data extraction and membership inference attacks tailored for
point-of-interest (POI) recommendation models, one of the most widely used
mobility data-based ML models. These attacks in our attack suite assume
different adversary knowledge and aim to extract different types of sensitive
information from mobility data, providing a holistic privacy risk assessment
for POI recommendation models. Our experimental evaluation using two real-world
mobility datasets demonstrates that current POI recommendation models are
vulnerable to our attacks. We also present unique findings to understand what
types of mobility data are more susceptible to privacy attacks. Finally, we
evaluate defenses against these attacks and highlight future directions and
challenges.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18608" title="Abstract">arXiv:2310.18608</a> [<a href="/pdf/2310.18608" title="Download PDF">pdf</a>, <a href="/format/2310.18608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding in Recommender Systems: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Maolin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xinjian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiansheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shucheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Ruocheng Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recommender systems have become an essential component of many online
platforms, providing personalized recommendations to users. A crucial aspect is
embedding techniques that coverts the high-dimensional discrete features, such
as user and item IDs, into low-dimensional continuous vectors and can enhance
the recommendation performance. Applying embedding techniques captures complex
entity relationships and has spurred substantial research. In this survey, we
provide an overview of the recent literature on embedding techniques in
recommender systems. This survey covers embedding methods like collaborative
filtering, self-supervised learning, and graph-based techniques. Collaborative
filtering generates embeddings capturing user-item preferences, excelling in
sparse data. Self-supervised methods leverage contrastive or generative
learning for various tasks. Graph-based techniques like node2vec exploit
complex relationships in network-rich environments. Addressing the scalability
challenges inherent to embedding methods, our survey delves into innovative
directions within the field of recommendation systems. These directions aim to
enhance performance and reduce computational complexity, paving the way for
improved recommender systems. Among these innovative approaches, we will
introduce Auto Machine Learning (AutoML), hash techniques, and quantization
techniques in this survey. We discuss various architectures and techniques and
highlight the challenges and future directions in these aspects. This survey
aims to provide a comprehensive overview of the state-of-the-art in this
rapidly evolving field and serve as a useful resource for researchers and
practitioners working in the area of recommender systems.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18609" title="Abstract">arXiv:2310.18609</a> [<a href="/pdf/2310.18609" title="Download PDF">pdf</a>, <a href="/format/2310.18609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep3DSketch+: Obtaining Customized 3D Model by Single Free-Hand Sketch  through Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zang%2C+Y">Ying Zang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chenglong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianrun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuanqi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingshan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenjun Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">As 3D models become critical in today's manufacturing and product design,
conventional 3D modeling approaches based on Computer-Aided Design (CAD) are
labor-intensive, time-consuming, and have high demands on the creators. This
work aims to introduce an alternative approach to 3D modeling by utilizing
free-hand sketches to obtain desired 3D models. We introduce Deep3DSketch+,
which is a deep-learning algorithm that takes the input of a single free-hand
sketch and produces a complete and high-fidelity model that matches the sketch
input. The neural network has view- and structural-awareness enabled by a Shape
Discriminator (SD) and a Stroke Enhancement Module (SEM), which overcomes the
limitations of sparsity and ambiguity of the sketches. The network design also
brings high robustness to partial sketch input in industrial applications.Our
approach has undergone extensive experiments, demonstrating its
state-of-the-art (SOTA) performance on both synthetic and real-world datasets.
These results validate the effectiveness and superiority of our method compared
to existing techniques. We have demonstrated the conversion of free-hand
sketches into physical 3D objects using additive manufacturing. We believe that
our approach has the potential to accelerate product design and democratize
customized manufacturing.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18612" title="Abstract">arXiv:2310.18612</a> [<a href="/pdf/2310.18612" title="Download PDF">pdf</a>, <a href="/format/2310.18612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient kernel surrogates for neural network-based regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qadeer%2C+S">Saad Qadeer</a>, 
<a href="/search/cs?searchtype=author&query=Engel%2C+A">Andrew Engel</a>, 
<a href="/search/cs?searchtype=author&query=Tsou%2C+A">Adam Tsou</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+M">Max Vargas</a>, 
<a href="/search/cs?searchtype=author&query=Stinis%2C+P">Panos Stinis</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Tony Chiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages. software used to reach results available upon request, approved for release by Pacific Northwest National Laboratory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Despite their immense promise in performing a variety of learning tasks, a
theoretical understanding of the effectiveness and limitations of Deep Neural
Networks (DNNs) has so far eluded practitioners. This is partly due to the
inability to determine the closed forms of the learned functions, making it
harder to assess their precise dependence on the training data and to study
their generalization properties on unseen datasets. Recent work has shown that
randomly initialized DNNs in the infinite width limit converge to kernel
machines relying on a Neural Tangent Kernel (NTK) with known closed form. These
results suggest, and experimental evidence corroborates, that empirical kernel
machines can also act as surrogates for finite width DNNs. The high
computational cost of assembling the full NTK, however, makes this approach
infeasible in practice, motivating the need for low-cost approximations. In the
current work, we study the performance of the Conjugate Kernel (CK), an
efficient approximation to the NTK that has been observed to yield fairly
similar results. For the regression problem of smooth functions and
classification using logistic regression, we show that the CK performance is
only marginally worse than that of the NTK and, in certain cases, is shown to
be superior. In particular, we establish bounds for the relative test losses,
verify them with numerical tests, and identify the regularity of the kernel as
the key determinant of performance. In addition to providing a theoretical
grounding for using CKs instead of NTKs, our framework provides insights into
understanding the robustness of the various approximants and suggests a recipe
for improving DNN accuracy inexpensively. We present a demonstration of this on
the foundation model GPT-2 by comparing its performance on a classification
task using a conventional approach and our prescription.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18614" title="Abstract">arXiv:2310.18614</a> [<a href="/pdf/2310.18614" title="Download PDF">pdf</a>, <a href="/format/2310.18614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Mutual Information Analysis: Towards Multi-view Clustering  in The Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiatai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xuewen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multi-view clustering (MVC) can explore common semantics from unsupervised
views generated by different sources, and thus has been extensively used in
applications of practical computer vision. Due to the spatio-temporal
asynchronism, multi-view data often suffer from view missing and are unaligned
in real-world applications, which makes it difficult to learn consistent
representations. To address the above issues, this work proposes a deep MVC
framework where data recovery and alignment are fused in a hierarchically
consistent way to maximize the mutual information among different views and
ensure the consistency of their latent spaces. More specifically, we first
leverage dual prediction to fill in missing views while achieving the
instance-level alignment, and then take the contrastive reconstruction to
achieve the class-level alignment. To the best of our knowledge, this could be
the first successful attempt to handle the missing and unaligned data problem
separately with different learning paradigms. Extensive experiments on public
datasets demonstrate that our method significantly outperforms state-of-the-art
methods on multi-view clustering even in the cases of view missing and
unalignment.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18615" title="Abstract">arXiv:2310.18615</a> [<a href="/pdf/2310.18615" title="Download PDF">pdf</a>, <a href="/format/2310.18615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporally Disentangled Representation Learning under Unknown  Nonstationarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiangchen Song</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Weiran Yao</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yewen Fan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xinshuai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Niebles%2C+J+C">Juan Carlos Niebles</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In unsupervised causal representation learning for sequential data with
time-delayed latent causal influences, strong identifiability results for the
disentanglement of causally-related latent variables have been established in
stationary settings by leveraging temporal structure. However, in nonstationary
setting, existing work only partially addressed the problem by either utilizing
observed auxiliary variables (e.g., class labels and/or domain indexes) as side
information or assuming simplified latent causal dynamics. Both constrain the
method to a limited range of scenarios. In this study, we further explored the
Markov Assumption under time-delayed causally related process in nonstationary
setting and showed that under mild conditions, the independent latent
components can be recovered from their nonlinear mixture up to a permutation
and a component-wise transformation, without the observation of auxiliary
variables. We then introduce NCTRL, a principled estimation framework, to
reconstruct time-delayed latent causal variables and identify their relations
from measured sequential data only. Empirical evaluations demonstrated the
reliable identification of time-delayed latent causal influences, with our
methodology substantially outperforming existing baselines that fail to exploit
the nonstationarity adequately and then, consequently, cannot distinguish
distribution shifts.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18617" title="Abstract">arXiv:2310.18617</a> [<a href="/pdf/2310.18617" title="Download PDF">pdf</a>, <a href="/format/2310.18617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pessimistic Off-Policy Multi-Objective Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+S">Shima Alizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Bhargava%2C+A">Aniruddha Bhargava</a>, 
<a href="/search/cs?searchtype=author&query=Gopalswamy%2C+K">Karthick Gopalswamy</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+L">Lalit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Kveton%2C+B">Branislav Kveton</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Ge Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Multi-objective optimization is a type of decision making problems where
multiple conflicting objectives are optimized. We study offline optimization of
multi-objective policies from data collected by an existing policy. We propose
a pessimistic estimator for the multi-objective policy values that can be
easily plugged into existing formulas for hypervolume computation and
optimized. The estimator is based on inverse propensity scores (IPS), and
improves upon a naive IPS estimator in both theory and experiments. Our
analysis is general, and applies beyond our IPS estimators and methods for
optimizing them. The pessimistic estimator can be optimized by policy gradients
and performs well in all of our experiments.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18618" title="Abstract">arXiv:2310.18618</a> [<a href="/pdf/2310.18618" title="Download PDF">pdf</a>, <a href="/format/2310.18618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subspace projection regularization for large-scale Bayesian linear  inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+H">Haibo Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The Bayesian statistical framework provides a systematic approach to enhance
the regularization model by incorporating prior information about the desired
solution. For the Bayesian linear inverse problems with Gaussian noise and
Gaussian prior, we propose a new iterative regularization algorithm that
belongs to subspace projection regularization (SPR) methods. By treating the
forward model matrix as a linear operator between the two underlying finite
dimensional Hilbert spaces with new introduced inner products, we first
introduce an iterative process that can generate a series of valid solution
subspaces. The SPR method then projects the original problem onto these
solution subspaces to get a series of low dimensional linear least squares
problems, where an efficient procedure is developed to update the solutions of
them to approximate the desired solution of the original problem. With the new
designed early stopping rules, this iterative algorithm can obtain a
regularized solution with a satisfied accuracy. Several theoretical results
about the algorithm are established to reveal the regularization properties of
it. We use both small-scale and large-scale inverse problems to test the
proposed algorithm and demonstrate its robustness and efficiency. The most
computationally intensive operations in the proposed algorithm only involve
matrix-vector products, making it highly efficient for large-scale problems.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18619" title="Abstract">arXiv:2310.18619</a> [<a href="/pdf/2310.18619" title="Download PDF">pdf</a>, <a href="/format/2310.18619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dense Retrieval as Indirect Supervision for Large-space Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+M">Mingtao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Many discriminative natural language understanding (NLU) tasks have large
label spaces. Learning such a process of large-space decision making is
particularly challenging due to the lack of training instances per label and
the difficulty of selection among many fine-grained labels. Inspired by dense
retrieval methods for passage finding in open-domain QA, we propose a
reformulation of large-space discriminative NLU tasks as a learning-to-retrieve
task, leading to a novel solution named Dense Decision Retrieval (DDR ).
Instead of predicting fine-grained decisions as logits, DDR adopts a
dual-encoder architecture that learns to predict by retrieving from a decision
thesaurus. This approach not only leverages rich indirect supervision signals
from easy-to-consume learning resources for dense retrieval, it also leads to
enhanced prediction generalizability with a semantically meaningful
representation of the large decision space. When evaluated on tasks with
decision spaces ranging from hundreds to hundred-thousand scales, DDR
outperforms strong baselines greatly by 27.54% in P@1 on two extreme
multi-label classification tasks, 1.17% in F1 score ultra-fine entity typing,
and 1.26% in accuracy on three few-shot intent classification tasks on average.
Code and resources are available at https://github.com/luka-group/DDR
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18620" title="Abstract">arXiv:2310.18620</a> [<a href="/pdf/2310.18620" title="Download PDF">pdf</a>, <a href="/format/2310.18620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ODM3D: Alleviating Foreground Sparsity for Enhanced Semi-Supervised  Monocular 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weijia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dongnan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+W">Weidong Cai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Monocular 3D object detection (M3OD) is a significant yet inherently
challenging task in autonomous driving due to absence of implicit depth cues in
a single RGB image. In this paper, we strive to boost currently underperforming
monocular 3D object detectors by leveraging an abundance of unlabelled data via
semi-supervised learning. Our proposed ODM3D framework entails cross-modal
knowledge distillation at various levels to inject LiDAR-domain knowledge into
a monocular detector during training. By identifying foreground sparsity as the
main culprit behind existing methods' suboptimal training, we exploit the
precise localisation information embedded in LiDAR points to enable more
foreground-attentive and efficient distillation via the proposed BEV occupancy
guidance mask, leading to notably improved knowledge transfer and M3OD
performance. Besides, motivated by insights into why existing cross-modal
GT-sampling techniques fail on our task at hand, we further design a novel
cross-modal object-wise data augmentation strategy for effective RGB-LiDAR
joint learning. Our method ranks 1st in both KITTI validation and test
benchmarks, significantly surpassing all existing monocular methods, supervised
or semi-supervised, on both BEV and 3D detection metrics.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18622" title="Abstract">arXiv:2310.18622</a> [<a href="/pdf/2310.18622" title="Download PDF">pdf</a>, <a href="/format/2310.18622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arbitrarily Scalable Environment Generators via Neural Cellular Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fontaine%2C+M+C">Matthew C. Fontaine</a>, 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+V">Varun Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Nikolaidis%2C+S">Stefanos Nikolaidis</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaoyang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Advances in Neural Information Processing Systems (NeurIPS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">We study the problem of generating arbitrarily large environments to improve
the throughput of multi-robot systems. Prior work proposes Quality Diversity
(QD) algorithms as an effective method for optimizing the environments of
automated warehouses. However, these approaches optimize only relatively small
environments, falling short when it comes to replicating real-world warehouse
sizes. The challenge arises from the exponential increase in the search space
as the environment size increases. Additionally, the previous methods have only
been tested with up to 350 robots in simulations, while practical warehouses
could host thousands of robots. In this paper, instead of optimizing
environments, we propose to optimize Neural Cellular Automata (NCA) environment
generators via QD algorithms. We train a collection of NCA generators with QD
algorithms in small environments and then generate arbitrarily large
environments from the generators at test time. We show that NCA environment
generators maintain consistent, regularized patterns regardless of environment
size, significantly enhancing the scalability of multi-robot systems in two
different domains with up to 2,350 robots. Additionally, we demonstrate that
our method scales a single-agent reinforcement learning policy to arbitrarily
large environments with similar patterns. We include the source code at
\url{https://github.com/lunjohnzhang/warehouse_env_gen_nca_public}.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18626" title="Abstract">arXiv:2310.18626</a> [<a href="/pdf/2310.18626" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmark Generation Framework with Customizable Distortions for Image  Classifier Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Soumyendu Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Babu%2C+A+R">Ashwin Ramesh Babu</a>, 
<a href="/search/cs?searchtype=author&query=Mousavi%2C+S">Sajad Mousavi</a>, 
<a href="/search/cs?searchtype=author&query=Carmichael%2C+Z">Zachariah Carmichael</a>, 
<a href="/search/cs?searchtype=author&query=Gundecha%2C+V">Vineet Gundecha</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbanpour%2C+S">Sahand Ghorbanpour</a>, 
<a href="/search/cs?searchtype=author&query=Luna%2C+R">Ricardo Luna</a>, 
<a href="/search/cs?searchtype=author&query=Guillen%2C+G+A">Gutierrez Antonio Guillen</a>, 
<a href="/search/cs?searchtype=author&query=Naug%2C+A">Avisek Naug</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a novel framework for generating adversarial benchmarks to
evaluate the robustness of image classification models. Our framework allows
users to customize the types of distortions to be optimally applied to images,
which helps address the specific distortions relevant to their deployment. The
benchmark can generate datasets at various distortion levels to assess the
robustness of different image classifiers. Our results show that the
adversarial samples generated by our framework with any of the image
classification models, like ResNet-50, Inception-V3, and VGG-16, are effective
and transferable to other models causing them to fail. These failures happen
even when these models are adversarially retrained using state-of-the-art
techniques, demonstrating the generalizability of our adversarial samples. We
achieve competitive performance in terms of net $L_2$ distortion compared to
state-of-the-art benchmark techniques on CIFAR-10 and ImageNet; however, we
demonstrate our framework achieves such results with simple distortions like
Gaussian noise without introducing unnatural artifacts or color bleeds. This is
made possible by a model-based reinforcement learning (RL) agent and a
technique that reduces a deep tree search of the image for model sensitivity to
perturbations, to a one-level analysis and action. The flexibility of choosing
distortions and setting classification probability thresholds for multiple
classes makes our framework suitable for algorithmic audits.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18628" title="Abstract">arXiv:2310.18628</a> [<a href="/pdf/2310.18628" title="Download PDF">pdf</a>, <a href="/format/2310.18628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive  Learning for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hailin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Amrita Saha</a>, 
<a href="/search/cs?searchtype=author&query=Hoi%2C+S">Steven Hoi</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are
increasing interests in distilling the capabilies of close-sourced LLMs to
smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT
to generate a set of instructions and answers, for the student model to learn.
However, such standard distillation approach neglects the merits and conditions
of the student model. Inspired by modern teaching principles, we design a
personalised distillation process, in which the student attempts to solve a
task first, then the teacher provides an adaptive refinement for the student to
improve. Instead of feeding the student with teacher's prior, personalised
distillation enables personalised learning for the student model, as it only
learns on examples it makes mistakes upon and learns to improve its own
solution. On code generation, personalised distillation consistently
outperforms standard distillation with only one third of the data. With only
2.5-3K personalised examples that incur a data-collection cost of 4-6$, we
boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to
achieve 45.8% pass@1 on HumanEval.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18629" title="Abstract">arXiv:2310.18629</a> [<a href="/pdf/2310.18629" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Modeling for Wind Power Forecasting: A Glass-Box Approach  with Exceptional Accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Wenlong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Port%C3%A9-Agel%2C+F">Fernando Port&#xe9;-Agel</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiannong Fang</a>, 
<a href="/search/cs?searchtype=author&query=Bak-Jensen%2C+B">Birgitte Bak-Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+G">Guangchun Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhe Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It was submitted to the 23rd Power Systems Computation Conference (PSCC 2024) in September 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Machine learning models (e.g., neural networks) achieve high accuracy in wind
power forecasting, but they are usually regarded as black boxes that lack
interpretability. To address this issue, the paper proposes a glass-box
approach that combines exceptional accuracy with transparency for wind power
forecasting. Specifically, advanced artificial intelligence methods (e.g.,
gradient boosting) are innovatively employed to create shape functions within
the forecasting model. These functions effectively map the intricate non-linear
relationships between wind power output and input features. Furthermore, the
forecasting model is enriched by incorporating interaction terms that adeptly
capture interdependencies and synergies among the input features. Simulation
results show that the proposed glass-box approach effectively interprets the
results of wind power forecasting from both global and instance perspectives.
Besides, it outperforms most benchmark models and exhibits comparable
performance to the best-performing neural networks. This dual strength of
transparency and high accuracy positions the proposed glass-box approach as a
compelling choice for reliable wind power forecasting.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18630" title="Abstract">arXiv:2310.18630</a> [<a href="/pdf/2310.18630" title="Download PDF">pdf</a>, <a href="/format/2310.18630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Localization and Communication Enhancement in Uplink Integrated  Sensing and Communications System with Clock Asynchronism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">XinXin He</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhiyong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qixun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Ping Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 11 figures, submitted to JSAC special issue "Positioning and Sensing Over Wireless Networks"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we propose a joint single-base localization and communication
enhancement scheme for the uplink (UL) integrated sensing and communications
(ISAC) system with asynchronism, which can achieve accurate single-base
localization of user equipment (UE) and significantly improve the communication
reliability despite the existence of timing offset (TO) due to the clock
asynchronism between UE and base station (BS). Our proposed scheme integrates
the CSI enhancement into the multiple signal classification (MUSIC)-based AoA
estimation and thus imposes no extra complexity on the ISAC system. We further
exploit a MUSIC-based range estimation method and prove that it can suppress
the time-varying TO-related phase terms. Exploiting the AoA and range
estimation of UE, we can estimate the location of UE. Finally, we propose a
joint CSI and data signals-based localization scheme that can coherently
exploit the data and the CSI signals to improve the AoA and range estimation,
which further enhances the single-base localization of UE. The extensive
simulation results show that the enhanced CSI can achieve equivalent bit error
rate performance to the minimum mean square error (MMSE) CSI estimator. The
proposed joint CSI and data signals-based localization scheme can achieve
decimeter-level localization accuracy despite the existing clock asynchronism
and improve the localization mean square error (MSE) by about 8 dB compared
with the maximum likelihood (ML)-based benchmark method.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18633" title="Abstract">arXiv:2310.18633</a> [<a href="/pdf/2310.18633" title="Download PDF">pdf</a>, <a href="/format/2310.18633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Setting the Trap: Capturing and Defeating Backdoors in Pretrained  Language Models through Honeypots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruixiang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiayi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiming Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zirui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In the field of natural language processing, the prevalent approach involves
fine-tuning pretrained language models (PLMs) using local samples. Recent
research has exposed the susceptibility of PLMs to backdoor attacks, wherein
the adversaries can embed malicious prediction behaviors by manipulating a few
training samples. In this study, our objective is to develop a
backdoor-resistant tuning procedure that yields a backdoor-free model, no
matter whether the fine-tuning dataset contains poisoned samples. To this end,
we propose and integrate a honeypot module into the original PLM, specifically
designed to absorb backdoor information exclusively. Our design is motivated by
the observation that lower-layer representations in PLMs carry sufficient
backdoor features while carrying minimal information about the original tasks.
Consequently, we can impose penalties on the information acquired by the
honeypot module to inhibit backdoor creation during the fine-tuning process of
the stem network. Comprehensive experiments conducted on benchmark datasets
substantiate the effectiveness and robustness of our defensive strategy.
Notably, these results indicate a substantial reduction in the attack success
rate ranging from 10\% to 40\% when compared to prior state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18634" title="Abstract">arXiv:2310.18634</a> [<a href="/pdf/2310.18634" title="Download PDF">pdf</a>, <a href="/format/2310.18634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSL Framework for Causal Inconsistency between Structures and  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+K">Keqing Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The cross-pollination of deep learning and causal discovery has catalyzed a
burgeoning field of research seeking to elucidate causal relationships within
non-statistical data forms like images, videos, and text. Such data, often
being named `indefinite data', exhibit unique challenges-inconsistency between
causal structure and representation, which are not common in conventional data
forms. To tackle this issue, we theoretically develop intervention strategies
suitable for indefinite data and derive causal consistency condition (CCC).
Moreover, we design a self-supervised learning (SSL) framework that considers
interventions as `views' and CCC as a `philosophy' with two implement examples
on Supervised Specialized Models (SSMs) and Large Language Models (LLMs),
respectively. To evaluate pure inconsistency manifestations, we have prepared
the first high-quality causal dialogue dataset-Causalogue. Evaluations are also
performed on three other downstream tasks. Extensive experimentation has
substantiated the efficacy of our methodology, illuminating how CCC could
potentially play an influential role in various fields.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18635" title="Abstract">arXiv:2310.18635</a> [<a href="/pdf/2310.18635" title="Download PDF">pdf</a>, <a href="/format/2310.18635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> T-PickSeer: Visual Analysis of Taxi Pick-up Point Selection Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Shuxian Gu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yemo Dai</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zezheng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H">Haipeng Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 10 figures; The 10th China Visualization and Visual Analytics Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Taxi drivers often take much time to navigate the streets to look for
passengers, which leads to high vacancy rates and wasted resources. Empty taxi
cruising remains a big concern for taxi companies. Analyzing the pick-up point
selection behavior can solve this problem effectively, providing suggestions
for taxi management and dispatch. Many studies have been devoted to analyzing
and recommending hot-spot regions of pick-up points, which can make it easier
for drivers to pick up passengers. However, the selection of pick-up points is
complex and affected by multiple factors, such as convenience and traffic
management. Most existing approaches cannot produce satisfactory results in
real-world applications because of the changing travel demands and the lack of
interpretability. In this paper, we introduce a visual analytics system,
T-PickSeer, for taxi company analysts to better explore and understand the
pick-up point selection behavior of passengers. We explore massive taxi GPS
data and employ an overview-to-detail approach to enable effective analysis of
pick-up point selection. Our system provides coordinated views to compare
different regularities and characteristics in different regions. Also, our
system assists in identifying potential pick-up points and checking the
performance of each pick-up point. Three case studies based on a real-world
dataset and interviews with experts have demonstrated the effectiveness of our
system.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18636" title="Abstract">arXiv:2310.18636</a> [<a href="/pdf/2310.18636" title="Download PDF">pdf</a>, <a href="/format/2310.18636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electrical Impedance Tomography: A Fair Comparative Study on Deep  Learning and Analytic-based Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tanyu%2C+D+N">Derick Nganyu Tanyu</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+J">Jianfeng Ning</a>, 
<a href="/search/cs?searchtype=author&query=Hauptmann%2C+A">Andreas Hauptmann</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bangti Jin</a>, 
<a href="/search/cs?searchtype=author&query=Maass%2C+P">Peter Maass</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Computer Vision and Pattern Recognition (cs.CV); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Electrical Impedance Tomography (EIT) is a powerful imaging technique with
diverse applications, e.g., medical diagnosis, industrial monitoring, and
environmental studies. The EIT inverse problem is about inferring the internal
conductivity distribution of an object from measurements taken on its boundary.
It is severely ill-posed, necessitating advanced computational methods for
accurate image reconstructions. Recent years have witnessed significant
progress, driven by innovations in analytic-based approaches and deep learning.
This review explores techniques for solving the EIT inverse problem, focusing
on the interplay between contemporary deep learning-based strategies and
classical analytic-based methods. Four state-of-the-art deep learning
algorithms are rigorously examined, harnessing the representational
capabilities of deep neural networks to reconstruct intricate conductivity
distributions. In parallel, two analytic-based methods, rooted in mathematical
formulations and regularisation techniques, are dissected for their strengths
and limitations. These methodologies are evaluated through various numerical
experiments, encompassing diverse scenarios that reflect real-world
complexities. A suite of performance metrics is employed to assess the efficacy
of these methods. These metrics collectively provide a nuanced understanding of
the methods' ability to capture essential features and delineate complex
conductivity patterns. One novel feature of the study is the incorporation of
variable conductivity scenarios, introducing a level of heterogeneity that
mimics textured inclusions. This departure from uniform conductivity
assumptions mimics realistic scenarios where tissues or materials exhibit
spatially varying electrical properties. Exploring how each method responds to
such variable conductivity scenarios opens avenues for understanding their
robustness and adaptability.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18639" title="Abstract">arXiv:2310.18639</a> [<a href="/pdf/2310.18639" title="Download PDF">pdf</a>, <a href="/format/2310.18639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Plastic and Stable Exemplar-Free Incremental Learning: A  Dual-Learner Framework with Cumulative Parameter Averaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wenju Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingyong Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yangli-ao Geng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The dilemma between plasticity and stability presents a significant challenge
in Incremental Learning (IL), especially in the exemplar-free scenario where
accessing old-task samples is strictly prohibited during the learning of a new
task. A straightforward solution to this issue is learning and storing an
independent model for each task, known as Single Task Learning (STL). Despite
the linear growth in model storage with the number of tasks in STL, we
empirically discover that averaging these model parameters can potentially
preserve knowledge across all tasks. Inspired by this observation, we propose a
Dual-Learner framework with Cumulative Parameter Averaging (DLCPA). DLCPA
employs a dual-learner design: a plastic learner focused on acquiring new-task
knowledge and a stable learner responsible for accumulating all learned
knowledge. The knowledge from the plastic learner is transferred to the stable
learner via cumulative parameter averaging. Additionally, several task-specific
classifiers work in cooperation with the stable learner to yield the final
prediction. Specifically, when learning a new task, these modules are updated
in a cyclic manner: i) the plastic learner is initially optimized using a
self-supervised loss besides the supervised loss to enhance the feature
extraction robustness; ii) the stable learner is then updated with respect to
the plastic learner in a cumulative parameter averaging manner to maintain its
task-wise generalization; iii) the task-specific classifier is accordingly
optimized to align with the stable learner. Experimental results on CIFAR-100
and Tiny-ImageNet show that DLCPA outperforms several state-of-the-art
exemplar-free baselines in both Task-IL and Class-IL settings.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18640" title="Abstract">arXiv:2310.18640</a> [<a href="/pdf/2310.18640" title="Download PDF">pdf</a>, <a href="/format/2310.18640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Switching Temporary Teachers for Semi-Supervised Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Na%2C+J">Jaemin Na</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+J">Jung-Woo Ha</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H+J">Hyung Jin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dongyoon Han</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+W">Wonjun Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS-2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The teacher-student framework, prevalent in semi-supervised semantic
segmentation, mainly employs the exponential moving average (EMA) to update a
single teacher's weights based on the student's. However, EMA updates raise a
problem in that the weights of the teacher and student are getting coupled,
causing a potential performance bottleneck. Furthermore, this problem may
become more severe when training with more complicated labels such as
segmentation masks but with few annotated data. This paper introduces Dual
Teacher, a simple yet effective approach that employs dual temporary teachers
aiming to alleviate the coupling problem for the student. The temporary
teachers work in shifts and are progressively improved, so consistently prevent
the teacher and student from becoming excessively close. Specifically, the
temporary teachers periodically take turns generating pseudo-labels to train a
student model and maintain the distinct characteristics of the student model
for each epoch. Consequently, Dual Teacher achieves competitive performance on
the PASCAL VOC, Cityscapes, and ADE20K benchmarks with remarkably shorter
training times than state-of-the-art methods. Moreover, we demonstrate that our
approach is model-agnostic and compatible with both CNN- and Transformer-based
models. Code is available at \url{https://github.com/naver-ai/dual-teacher}.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18642" title="Abstract">arXiv:2310.18642</a> [<a href="/pdf/2310.18642" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-shot Localization and Segmentation of Medical Images with Foundation  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anand%2C+D">Deepa Anand</a>, 
<a href="/search/cs?searchtype=author&query=M%2C+G+R">Gurunath Reddy M</a>, 
<a href="/search/cs?searchtype=author&query=Singhal%2C+V">Vanika Singhal</a>, 
<a href="/search/cs?searchtype=author&query=Shanbhag%2C+D+D">Dattesh D. Shanbhag</a>, 
<a href="/search/cs?searchtype=author&query=KS%2C+S">Shriram KS</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+U">Uday Patil</a>, 
<a href="/search/cs?searchtype=author&query=Bhushan%2C+C">Chitresh Bhushan</a>, 
<a href="/search/cs?searchtype=author&query=Manickam%2C+K">Kavitha Manickam</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+D">Dawei Gui</a>, 
<a href="/search/cs?searchtype=author&query=Mullick%2C+R">Rakesh Mullick</a>, 
<a href="/search/cs?searchtype=author&query=Gopal%2C+A">Avinash Gopal</a>, 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+P">Parminder Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Kass-Hout%2C+T">Taha Kass-Hout</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 R0-FoMo Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advances in Vision Transformers (ViT) and Stable Diffusion (SD) models
with their ability to capture rich semantic features of the image have been
used for image correspondence tasks on natural images. In this paper, we
examine the ability of a variety of pre-trained ViT (DINO, DINOv2, SAM, CLIP)
and SD models, trained exclusively on natural images, for solving the
correspondence problems on medical images. While many works have made a case
for in-domain training, we show that the models trained on natural images can
offer good performance on medical images across different modalities
(CT,MR,Ultrasound) sourced from various manufacturers, over multiple anatomical
regions (brain, thorax, abdomen, extremities), and on wide variety of tasks.
Further, we leverage the correspondence with respect to a template image to
prompt a Segment Anything (SAM) model to arrive at single shot segmentation,
achieving dice range of 62%-90% across tasks, using just one image as
reference. We also show that our single-shot method outperforms the recently
proposed few-shot segmentation method - UniverSeg (Dice range 47%-80%) on most
of the semantic segmentation tasks(six out of seven) across medical imaging
modalities.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18646" title="Abstract">arXiv:2310.18646</a> [<a href="/pdf/2310.18646" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Agricultural Commodities Prices with Machine Learning: A  Review of Current Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+N">Nhat-Quang Tran</a>, 
<a href="/search/cs?searchtype=author&query=Felipe%2C+A">Anna Felipe</a>, 
<a href="/search/cs?searchtype=author&query=Ngoc%2C+T+N">Thanh Nguyen Ngoc</a>, 
<a href="/search/cs?searchtype=author&query=Huynh%2C+T">Tom Huynh</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+Q">Quang Tran</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+A">Arthur Tang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thuy Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Agricultural price prediction is crucial for farmers, policymakers, and other
stakeholders in the agricultural sector. However, it is a challenging task due
to the complex and dynamic nature of agricultural markets. Machine learning
algorithms have the potential to revolutionize agricultural price prediction by
improving accuracy, real-time prediction, customization, and integration. This
paper reviews recent research on machine learning algorithms for agricultural
price prediction. We discuss the importance of agriculture in developing
countries and the problems associated with crop price falls. We then identify
the challenges of predicting agricultural prices and highlight how machine
learning algorithms can support better prediction. Next, we present a
comprehensive analysis of recent research, discussing the strengths and
weaknesses of various machine learning techniques. We conclude that machine
learning has the potential to revolutionize agricultural price prediction, but
further research is essential to address the limitations and challenges
associated with this approach.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18647" title="Abstract">arXiv:2310.18647</a> [<a href="/pdf/2310.18647" title="Download PDF">pdf</a>, <a href="/format/2310.18647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sleep Deprivation in the Forward-Forward Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lic%C4%83%2C+M">Mircea-Tudor Lic&#x103;</a>, 
<a href="/search/cs?searchtype=author&query=Dinucu-Jianu%2C+D">David Dinucu-Jianu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, published in ICLR 2023 TinyPapers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper aims to explore the separation of the two forward passes in the
Forward-Forward algorithm from a biological perspective in the context of
sleep. We show the size of the gap between the sleep and awake phase influences
the learning capabilities of the algorithm and highlight the importance of
negative data in diminishing the devastating effects of sleep deprivation.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18648" title="Abstract">arXiv:2310.18648</a> [<a href="/pdf/2310.18648" title="Download PDF">pdf</a>, <a href="/format/2310.18648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Artificial Intelligence for Software Engineering -- A  Research Agenda
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen-Duc%2C+A">Anh Nguyen-Duc</a>, 
<a href="/search/cs?searchtype=author&query=Cabrero-Daniel%2C+B">Beatriz Cabrero-Daniel</a>, 
<a href="/search/cs?searchtype=author&query=Przybylek%2C+A">Adam Przybylek</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+C">Chetan Arora</a>, 
<a href="/search/cs?searchtype=author&query=Khanna%2C+D">Dron Khanna</a>, 
<a href="/search/cs?searchtype=author&query=Herda%2C+T">Tomas Herda</a>, 
<a href="/search/cs?searchtype=author&query=Rafiq%2C+U">Usman Rafiq</a>, 
<a href="/search/cs?searchtype=author&query=Melegati%2C+J">Jorge Melegati</a>, 
<a href="/search/cs?searchtype=author&query=Guerra%2C+E">Eduardo Guerra</a>, 
<a href="/search/cs?searchtype=author&query=Kemell%2C+K">Kai-Kristian Kemell</a>, 
<a href="/search/cs?searchtype=author&query=Saari%2C+M">Mika Saari</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Huy Le</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+T">Tho Quan</a>, 
<a href="/search/cs?searchtype=author&query=Abrahamsson%2C+P">Pekka Abrahamsson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Generative Artificial Intelligence (GenAI) tools have become increasingly
prevalent in software development, offering assistance to various managerial
and technical project activities. Notable examples of these tools include
OpenAIs ChatGPT, GitHub Copilot, and Amazon CodeWhisperer. Although many recent
publications have explored and evaluated the application of GenAI, a
comprehensive understanding of the current development, applications,
limitations, and open challenges remains unclear to many. Particularly, we do
not have an overall picture of the current state of GenAI technology in
practical software engineering usage scenarios. We conducted a literature
review and focus groups for a duration of five months to develop a research
agenda on GenAI for Software Engineering. We identified 78 open Research
Questions (RQs) in 11 areas of Software Engineering. Our results show that it
is possible to explore the adoption of GenAI in partial automation and support
decision-making in all software development activities. While the current
literature is skewed toward software implementation, quality assurance and
software maintenance, other areas, such as requirements engineering, software
design, and software engineering education, would need further research
attention. Common considerations when implementing GenAI include industry-level
assessment, dependability and accuracy, data accessibility, transparency, and
sustainability aspects associated with the technology. GenAI is bringing
significant changes to the field of software engineering. Nevertheless, the
state of research on the topic still remains immature. We believe that this
research agenda holds significance and practical value for informing both
researchers and practitioners about current applications and guiding future
research.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18650" title="Abstract">arXiv:2310.18650</a> [<a href="/pdf/2310.18650" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing a Kinetic Fa&#xe7;ade Using BB-BC Algorithm with a Focus on  Enhancing Building Energy Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Soltani%2C+M">Marzieh Soltani</a>, 
<a href="/search/eess?searchtype=author&query=Atashi%2C+A">Arash Atashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8th International conference on Civil Engineering, Architecture &amp; Urban Development,Tehran, Iran
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In order to increase energy efficiency in buildings, optimizing the
parameters of the facade form can be challenging due to the dynamic nature of
solar radiation. One effective solution is the use of kinetic facades as a
second skin, which can control energy consumption. This study proposes a
parametric kinetic facade to increase building energy efficiency, along with a
framework to optimize its form using the Bang-Big Crunch (BB-BC) optimization
algorithm. The study involved modeling a two-story office building in Shiraz
city and calculating the energy consumption resulting from building operation
over a three-day period without considering the second skin of the facade. In
the second stage of the study, the second skin was optimized for the same
three-day interval and calculated as a parametric, static facade. In the last
step, the parameters of the second skin were optimized for three one-day
intervals, assuming the possibility of kinematic changes each day. The total
energy consumption of building operation for the three days was then calculated
and analyzed.The Python programming language was used to develop the
optimization algorithm, while Rhino software, and Grasshopper, Ladybug, and
Honeybee plugins were used for building modeling and simulation of light,
energy, and weather parameters.The results of the study demonstrate the
effectiveness of the proposed kinetic facade and the proper performance of the
proposed algorithm for solving similar problems. The study found that the use
of the second skin with kinetic function reduced energy consumption by 28%.
Additionally, the results from the second and third stages of the study showed
that the use of the second facade shell with kinematic function, compared to
its function in static mode, reduced energy consumption by 4%.Overall...
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18651" title="Abstract">arXiv:2310.18651</a> [<a href="/pdf/2310.18651" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local-Global Self-Supervised Visual Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javidani%2C+A">Ali Javidani</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+M+A">Mohammad Amin Sadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Araabi%2C+B+N">Babak Nadjar Araabi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised representation learning methods mainly focus on image-level
instance discrimination. This study explores the potential benefits of
incorporating patch-level discrimination into existing methods to enhance the
quality of learned representations by simultaneously looking at local and
global visual features. Towards this idea, we present a straightforward yet
effective patch-matching algorithm that can find the corresponding patches
across the augmented views of an image. The augmented views are subsequently
fed into a self-supervised learning framework employing Vision Transformer
(ViT) as its backbone. The result is the generation of both image-level and
patch-level representations. Leveraging the proposed patch-matching algorithm,
the model minimizes the representation distance between not only the CLS tokens
but also the corresponding patches. As a result, the model gains a more
comprehensive understanding of both the entirety of the image as well as its
finer details. We pretrain the proposed method on small, medium, and
large-scale datasets. It is shown that our approach could outperform
state-of-the-art image-level representation learning methods on both image
classification and downstream tasks. Keywords: Self-Supervised Learning; Visual
Representations; Local-Global Representation Learning; Patch-Wise
Representation Learning; Vision Transformer (ViT)
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18652" title="Abstract">arXiv:2310.18652</a> [<a href="/pdf/2310.18652" title="Download PDF">pdf</a>, <a href="/format/2310.18652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health  Records with Chest X-ray Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bae%2C+S">Seongsu Bae</a>, 
<a href="/search/cs?searchtype=author&query=Kyung%2C+D">Daeun Kyung</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+J">Jaehee Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+E">Eunbyeol Cho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Gyubok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kweon%2C+S">Sunjun Kweon</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Jungwoo Oh</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+L">Lei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E+I">Eric I-Chao Chang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Tackeun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Edward Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 Datasets and Benchmarks Track (10 pages for main text, 4 pages for references, 28 pages for supplementary materials)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Electronic Health Records (EHRs), which contain patients' medical histories
in various multi-modal formats, often overlook the potential for joint
reasoning across imaging and table modalities underexplored in current EHR
Question Answering (QA) systems. In this paper, we introduce EHRXQA, a novel
multi-modal question answering dataset combining structured EHRs and chest
X-ray images. To develop our dataset, we first construct two uni-modal
resources: 1) The MIMIC- CXR-VQA dataset, our newly created medical visual
question answering (VQA) benchmark, specifically designed to augment the
imaging modality in EHR QA, and 2) EHRSQL (MIMIC-IV), a refashioned version of
a previously established table-based EHR QA dataset. By integrating these two
uni-modal resources, we successfully construct a multi-modal EHR QA dataset
that necessitates both uni-modal and cross-modal reasoning. To address the
unique challenges of multi-modal questions within EHRs, we propose a
NeuralSQL-based strategy equipped with an external VQA API. This pioneering
endeavor enhances engagement with multi-modal EHR sources and we believe that
our dataset can catalyze advances in real-world medical scenarios such as
clinical decision-making and research. EHRXQA is available at
https://github.com/baeseongsu/ehrxqa.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18653" title="Abstract">arXiv:2310.18653</a> [<a href="/pdf/2310.18653" title="Download PDF">pdf</a>, <a href="/format/2310.18653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Guided Masked Autoencoder for Self-supervised Learning in Remote  Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez%2C+H+H">Hugo Hern&#xe1;ndez Hern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+C+M">Conrad M Albrecht</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X+X">Xiao Xiang Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised learning guided by masked image modelling, such as Masked
AutoEncoder (MAE), has attracted wide attention for pretraining vision
transformers in remote sensing. However, MAE tends to excessively focus on
pixel details, thereby limiting the model's capacity for semantic
understanding, in particular for noisy SAR images. In this paper, we explore
spectral and spatial remote sensing image features as improved
MAE-reconstruction targets. We first conduct a study on reconstructing various
image features, all performing comparably well or better than raw pixels. Based
on such observations, we propose Feature Guided Masked Autoencoder (FG-MAE):
reconstructing a combination of Histograms of Oriented Graidents (HOG) and
Normalized Difference Indices (NDI) for multispectral images, and
reconstructing HOG for SAR images. Experimental results on three downstream
tasks illustrate the effectiveness of FG-MAE with a particular boost for SAR
imagery. Furthermore, we demonstrate the well-inherited scalability of FG-MAE
and release a first series of pretrained vision transformers for medium
resolution SAR and multispectral images.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18659" title="Abstract">arXiv:2310.18659</a> [<a href="/pdf/2310.18659" title="Download PDF">pdf</a>, <a href="/format/2310.18659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Indeterminacy to Determinacy: Augmenting Logical Reasoning  Capabilities with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hongda Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weikai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luan%2C+J">Jian Luan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+S">Shuo Shang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code repo: <a href="https://github.com/XiaoMi/DetermLR">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent advances in LLMs have revolutionized the landscape of reasoning tasks.
To enhance the capabilities of LLMs to emulate human reasoning, prior works
focus on modeling reasoning steps using specific thought structures like
chains, trees, or graphs. However, LLM-based reasoning continues to encounter
three challenges: 1) Selecting appropriate reasoning structures for various
tasks; 2) Exploiting known conditions sufficiently and efficiently to deduce
new insights; 3) Considering the impact of historical reasoning experience. To
address these challenges, we propose DetermLR, a novel reasoning framework that
formulates the reasoning process as a transformational journey from
indeterminate premises to determinate ones. This process is marked by the
incremental accumulation of determinate premises, making the conclusion
progressively closer to clarity. DetermLR includes three essential components:
1) Premise identification: We categorize premises into two distinct types:
determinate and indeterminate. This empowers LLMs to customize reasoning
structures to match the specific task complexities. 2) Premise prioritization
and exploration: We leverage quantitative measurements to assess the relevance
of each premise to the target, prioritizing more relevant premises for
exploring new insights. 3) Iterative process with reasoning memory: We
introduce a reasoning memory module to automate storage and extraction of
available premises and reasoning paths, preserving historical reasoning details
for more accurate premise prioritization. Comprehensive experimental results
show that DetermLR outperforms all baselines on four challenging logical
reasoning tasks: LogiQA, ProofWriter, FOLIO, and LogicalDeduction. DetermLR can
achieve better reasoning performance while requiring fewer visited states,
highlighting its superior efficiency and effectiveness in tackling logical
reasoning tasks.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18660" title="Abstract">arXiv:2310.18660</a> [<a href="/pdf/2310.18660" title="Download PDF">pdf</a>, <a href="/format/2310.18660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Models for Generalist Geospatial Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jakubik%2C+J">Johannes Jakubik</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Sujit Roy</a>, 
<a href="/search/cs?searchtype=author&query=Phillips%2C+C+E">C. E. Phillips</a>, 
<a href="/search/cs?searchtype=author&query=Fraccaro%2C+P">Paolo Fraccaro</a>, 
<a href="/search/cs?searchtype=author&query=Godwin%2C+D">Denys Godwin</a>, 
<a href="/search/cs?searchtype=author&query=Zadrozny%2C+B">Bianca Zadrozny</a>, 
<a href="/search/cs?searchtype=author&query=Szwarcman%2C+D">Daniela Szwarcman</a>, 
<a href="/search/cs?searchtype=author&query=Gomes%2C+C">Carlos Gomes</a>, 
<a href="/search/cs?searchtype=author&query=Nyirjesy%2C+G">Gabby Nyirjesy</a>, 
<a href="/search/cs?searchtype=author&query=Edwards%2C+B">Blair Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Kimura%2C+D">Daiki Kimura</a>, 
<a href="/search/cs?searchtype=author&query=Simumba%2C+N">Naomi Simumba</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+L">Linsong Chu</a>, 
<a href="/search/cs?searchtype=author&query=Mukkavilli%2C+S+K">S. Karthik Mukkavilli</a>, 
<a href="/search/cs?searchtype=author&query=Lambhate%2C+D">Devyani Lambhate</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+K">Kamal Das</a>, 
<a href="/search/cs?searchtype=author&query=Bangalore%2C+R">Ranjini Bangalore</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+D">Dario Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Muszynski%2C+M">Michal Muszynski</a>, 
<a href="/search/cs?searchtype=author&query=Ankur%2C+K">Kumar Ankur</a>, 
<a href="/search/cs?searchtype=author&query=Ramasubramanian%2C+M">Muthukumaran Ramasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Gurung%2C+I">Iksha Gurung</a>, 
<a href="/search/cs?searchtype=author&query=Khallaghi%2C+S">Sam Khallaghi</a>, 
<a href="/search/cs?searchtype=author&query=Hanxi">Hanxi</a> (Steve)Li, 
<a href="/search/cs?searchtype=author&query=Cecil%2C+M">Michael Cecil</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+M">Maryam Ahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Kordi%2C+F">Fatemeh Kordi</a>, 
<a href="/search/cs?searchtype=author&query=Alemohammad%2C+H">Hamed Alemohammad</a>, 
<a href="/search/cs?searchtype=author&query=Maskey%2C+M">Manil Maskey</a>, 
<a href="/search/cs?searchtype=author&query=Ganti%2C+R">Raghu Ganti</a>, 
<a href="/search/cs?searchtype=author&query=Weldemariam%2C+K">Kommy Weldemariam</a>, 
<a href="/search/cs?searchtype=author&query=Ramachandran%2C+R">Rahul Ramachandran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Significant progress in the development of highly adaptable and reusable
Artificial Intelligence (AI) models is expected to have a significant impact on
Earth science and remote sensing. Foundation models are pre-trained on large
unlabeled datasets through self-supervision, and then fine-tuned for various
downstream tasks with small labeled datasets. This paper introduces a
first-of-a-kind framework for the efficient pre-training and fine-tuning of
foundational models on extensive geospatial data. We have utilized this
framework to create Prithvi, a transformer-based geospatial foundational model
pre-trained on more than 1TB of multispectral satellite imagery from the
Harmonized Landsat-Sentinel 2 (HLS) dataset. Our study demonstrates the
efficacy of our framework in successfully fine-tuning Prithvi to a range of
Earth observation tasks that have not been tackled by previous work on
foundation models involving multi-temporal cloud gap imputation, flood mapping,
wildfire scar segmentation, and multi-temporal crop segmentation. Our
experiments show that the pre-trained model accelerates the fine-tuning process
compared to leveraging randomly initialized weights. In addition, pre-trained
Prithvi compares well against the state-of-the-art, e.g., outperforming a
conditional GAN model in multi-temporal cloud imputation by up to 5pp (or 5.7%)
in the structural similarity index. Finally, due to the limited availability of
labeled data in the field of Earth observation, we gradually reduce the
quantity of available labeled data for refining the model to evaluate data
efficiency and demonstrate that data can be decreased significantly without
affecting the model's accuracy. The pre-trained 100 million parameter model and
corresponding fine-tuning workflows have been released publicly as open source
contributions to the global Earth sciences community through Hugging Face.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18662" title="Abstract">arXiv:2310.18662</a> [<a href="/pdf/2310.18662" title="Download PDF">pdf</a>, <a href="/format/2310.18662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASTormer: An AST Structure-aware Transformer Decoder for Text-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+R">Ruisheng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanchong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongshen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jieyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">Da Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text-to-SQL aims to generate an executable SQL program given the user
utterance and the corresponding database schema. To ensure the well-formedness
of output SQLs, one prominent approach adopts a grammar-based recurrent decoder
to produce the equivalent SQL abstract syntax tree (AST). However, previous
methods mainly utilize an RNN-series decoder, which 1) is time-consuming and
inefficient and 2) introduces very few structure priors. In this work, we
propose an AST structure-aware Transformer decoder (ASTormer) to replace
traditional RNN cells. The structural knowledge, such as node types and
positions in the tree, is seamlessly incorporated into the decoder via both
absolute and relative position embeddings. Besides, the proposed framework is
compatible with different traversing orders even considering adaptive node
selection. Extensive experiments on five text-to-SQL benchmarks demonstrate the
effectiveness and efficiency of our structured decoder compared to competitive
baselines.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18664" title="Abstract">arXiv:2310.18664</a> [<a href="/pdf/2310.18664" title="Download PDF">pdf</a>, <a href="/format/2310.18664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Node Cardinality Estimation in the Internet of Things Using Privileged  Feature Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Page%2C+P+S">Pranav S. Page</a>, 
<a href="/search/cs?searchtype=author&query=Siyote%2C+A+S">Anand S. Siyote</a>, 
<a href="/search/cs?searchtype=author&query=Borkar%2C+V+S">Vivek S. Borkar</a>, 
<a href="/search/cs?searchtype=author&query=Kasbekar%2C+G+S">Gaurav S. Kasbekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 17 figures, journal paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The Internet of Things (IoT) is emerging as a critical technology to connect
resource-constrained devices such as sensors and actuators as well as
appliances to the Internet. In this paper, we propose a novel methodology for
node cardinality estimation in wireless networks such as the IoT and
Radio-Frequency IDentification (RFID) systems, which uses the privileged
feature distillation (PFD) technique and works using a neural network with a
teacher-student model. The teacher is trained using both privileged and regular
features, and the student is trained with predictions from the teacher and
regular features. We propose node cardinality estimation algorithms based on
the PFD technique for homogeneous as well as heterogeneous wireless networks.
We show via extensive simulations that the proposed PFD based algorithms for
homogeneous as well as heterogeneous networks achieve much lower mean squared
errors in the computed node cardinality estimates than state-of-the-art
protocols proposed in prior work, while taking the same number of time slots
for executing the node cardinality estimation process as the latter protocols.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18666" title="Abstract">arXiv:2310.18666</a> [<a href="/pdf/2310.18666" title="Download PDF">pdf</a>, <a href="/ps/2310.18666" title="Download PostScript">ps</a>, <a href="/format/2310.18666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient stochastic particle method for high-dimensional nonlinear  PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lei%2C+Z">Zhengyang Lei</a>, 
<a href="/search/math?searchtype=author&query=Shao%2C+S">Sihong Shao</a>, 
<a href="/search/math?searchtype=author&query=Xiong%2C+Y">Yunfeng Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">Numerical resolution of high-dimensional nonlinear PDEs remains a huge
challenge due to the curse of dimensionality. Starting from the weak
formulation of the Lawson-Euler scheme, this paper proposes a stochastic
particle method (SPM) by tracking the deterministic motion, random jump,
resampling and reweighting of particles. Real-valued weighted particles are
adopted by SPM to approximate the high-dimensional solution, which
automatically adjusts the point distribution to intimate the relevant feature
of the solution. A piecewise constant reconstruction with virtual uniform grid
is employed to evaluate the nonlinear terms, which fully exploits the intrinsic
adaptive characteristic of SPM. Combining both can SPM achieve the goal of
adaptive sampling in time. Numerical experiments on the 6-D Allen-Cahn equation
and the 7-D Hamiltonian-Jacobi-Bellman equation demonstrate the potential of
SPM in solving high-dimensional nonlinear PDEs efficiently while maintaining an
acceptable accuracy.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18668" title="Abstract">arXiv:2310.18668</a> [<a href="/pdf/2310.18668" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FinBTech: Blockchain-Based Video and Voice Authentication System for  Enhanced Security in Financial Transactions Utilizing FaceNet512 and Gaussian  Mixture Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laila%2C+P+N+J">Prof N.Jeenath Laila</a>, 
<a href="/search/cs?searchtype=author&query=Tamilpavai%2C+D+G">Dr G.Tamilpavai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the digital age, it is crucial to make sure that financial transactions
are as secure and reliable as possible. This abstract offers a ground-breaking
method that combines smart contracts, blockchain technology, FaceNet512 for
improved face recognition, and Gaussian Mixture Models (GMM) for speech
authentication to create a system for video and audio verification that is
unmatched. Smart contracts and the immutable ledger of the blockchain are
combined to offer a safe and open environment for financial transactions.
FaceNet512 and GMM offer multi-factor biometric authentication simultaneously,
enhancing security to new heights. By combining cutting-edge technology, this
system offers a strong defense against identity theft and illegal access,
establishing a new benchmark for safe financial transactions.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18672" title="Abstract">arXiv:2310.18672</a> [<a href="/pdf/2310.18672" title="Download PDF">pdf</a>, <a href="/format/2310.18672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum Independent Set: Self-Training through Dynamic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brusca%2C+L">Lorenzo Brusca</a>, 
<a href="/search/cs?searchtype=author&query=Quaedvlieg%2C+L+C+P+M">Lars C.P.M. Quaedvlieg</a>, 
<a href="/search/cs?searchtype=author&query=Skoulakis%2C+S">Stratis Skoulakis</a>, 
<a href="/search/cs?searchtype=author&query=Chrysos%2C+G+G">Grigorios G Chrysos</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">This work presents a graph neural network (GNN) framework for solving the
maximum independent set (MIS) problem, inspired by dynamic programming (DP).
Specifically, given a graph, we propose a DP-like recursive algorithm based on
GNNs that firstly constructs two smaller sub-graphs, predicts the one with the
larger MIS, and then uses it in the next recursive call. To train our
algorithm, we require annotated comparisons of different graphs concerning
their MIS size. Annotating the comparisons with the output of our algorithm
leads to a self-training process that results in more accurate self-annotation
of the comparisons and vice versa. We provide numerical evidence showing the
superiority of our method vs prior methods in multiple synthetic and real-world
datasets.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18676" title="Abstract">arXiv:2310.18676</a> [<a href="/pdf/2310.18676" title="Download PDF">pdf</a>, <a href="/format/2310.18676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Object Detection in Optical Remote Sensing Imagery via  Attention-based Feature Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shamsolmoali%2C+P">Pourya Shamsolmoali</a>, 
<a href="/search/cs?searchtype=author&query=Chanussot%2C+J">Jocelyn Chanussot</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huiyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yue Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Efficient object detection methods have recently received great attention in
remote sensing. Although deep convolutional networks often have excellent
detection accuracy, their deployment on resource-limited edge devices is
difficult. Knowledge distillation (KD) is a strategy for addressing this issue
since it makes models lightweight while maintaining accuracy. However, existing
KD methods for object detection have encountered two constraints. First, they
discard potentially important background information and only distill nearby
foreground regions. Second, they only rely on the global context, which limits
the student detector's ability to acquire local information from the teacher
detector. To address the aforementioned challenges, we propose Attention-based
Feature Distillation (AFD), a new KD approach that distills both local and
global information from the teacher detector. To enhance local distillation, we
introduce a multi-instance attention mechanism that effectively distinguishes
between background and foreground elements. This approach prompts the student
detector to focus on the pertinent channels and pixels, as identified by the
teacher detector. Local distillation lacks global information, thus attention
global distillation is proposed to reconstruct the relationship between various
pixels and pass it from teacher to student detector. The performance of AFD is
evaluated on two public aerial image benchmarks, and the evaluation results
demonstrate that AFD in object detection can attain the performance of other
state-of-the-art models while being efficient.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18677" title="Abstract">arXiv:2310.18677</a> [<a href="/pdf/2310.18677" title="Download PDF">pdf</a>, <a href="/format/2310.18677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Based Models for Anomaly Detection: A Manifold Diffusion Recovery  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sangwoong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Young-Uk Jin</a>, 
<a href="/search/cs?searchtype=author&query=Noh%2C+Y">Yung-Kyun Noh</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+F+C">Frank C. Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present a new method of training energy-based models (EBMs) for anomaly
detection that leverages low-dimensional structures within data. The proposed
algorithm, Manifold Projection-Diffusion Recovery (MPDR), first perturbs a data
point along a low-dimensional manifold that approximates the training dataset.
Then, EBM is trained to maximize the probability of recovering the original
data. The training involves the generation of negative samples via MCMC, as in
conventional EBM training, but from a different distribution concentrated near
the manifold. The resulting near-manifold negative samples are highly
informative, reflecting relevant modes of variation in data. An energy function
of MPDR effectively learns accurate boundaries of the training data
distribution and excels at detecting out-of-distribution samples. Experimental
results show that MPDR exhibits strong performance across various anomaly
detection tasks involving diverse data types, such as images, vectors, and
acoustic signals.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18679" title="Abstract">arXiv:2310.18679</a> [<a href="/pdf/2310.18679" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> N-Critics: Self-Refinement of Large Language Models with Ensemble of  Critics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mousavi%2C+S">Sajad Mousavi</a>, 
<a href="/search/cs?searchtype=author&query=Guti%C3%A9rrez%2C+R+L">Ricardo Luna Guti&#xe9;rrez</a>, 
<a href="/search/cs?searchtype=author&query=Rengarajan%2C+D">Desik Rengarajan</a>, 
<a href="/search/cs?searchtype=author&query=Gundecha%2C+V">Vineet Gundecha</a>, 
<a href="/search/cs?searchtype=author&query=Babu%2C+A+R">Ashwin Ramesh Babu</a>, 
<a href="/search/cs?searchtype=author&query=Naug%2C+A">Avisek Naug</a>, 
<a href="/search/cs?searchtype=author&query=Guillen%2C+A">Antonio Guillen</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Soumyendu Sarkar</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023 Workshop on Robustness of Few-shot and Zero-shot
  Learning in Foundation Models 2023(NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a self-correction mechanism for Large Language Models (LLMs) to
mitigate issues such as toxicity and fact hallucination. This method involves
refining model outputs through an ensemble of critics and the model's own
feedback. Drawing inspiration from human behavior, we explore whether LLMs can
emulate the self-correction process observed in humans who often engage in
self-reflection and seek input from others to refine their understanding of
complex topics. Our approach is model-agnostic and can be applied across
various domains to enhance trustworthiness by addressing fairness, bias, and
robustness concerns. We consistently observe performance improvements in LLMs
for reducing toxicity and correcting factual errors.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18681" title="Abstract">arXiv:2310.18681</a> [<a href="/pdf/2310.18681" title="Download PDF">pdf</a>, <a href="/format/2310.18681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DySurv: Dynamic Deep Learning Model for Survival Prediction in the ICU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mesinovic%2C+M">Munib Mesinovic</a>, 
<a href="/search/cs?searchtype=author&query=Watkinson%2C+P">Peter Watkinson</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tingting Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Survival analysis helps approximate underlying distributions of
time-to-events which in the case of critical care like in the ICU can be a
powerful tool for dynamic mortality risk prediction. Extending beyond the
classical Cox model, deep learning techniques have been leveraged over the last
years relaxing the many constraints of their counterparts from statistical
methods. In this work, we propose a novel conditional variational
autoencoder-based method called DySurv which uses a combination of static and
time-series measurements from patient electronic health records in estimating
risk of death dynamically in the ICU. DySurv has been tested on standard
benchmarks where it outperforms most existing methods including other deep
learning methods and we evaluate it on a real-world patient database from
MIMIC-IV. The predictive capacity of DySurv is consistent and the survival
estimates remain disentangled across different datasets supporting the idea
that dynamic deep learning models based on conditional variational inference in
multi-task cases can be robust models for survival analysis.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18685" title="Abstract">arXiv:2310.18685</a> [<a href="/pdf/2310.18685" title="Download PDF">pdf</a>, <a href="/format/2310.18685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Reviewers Lock Horn: Finding Disagreement in Scientific Peer  Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sandeep Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Ghosal%2C+T">Tirthankar Ghosal</a>, 
<a href="/search/cs?searchtype=author&query=Ekbal%2C+A">Asif Ekbal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, EMNLP 2023 short
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">To this date, the efficacy of the scientific publishing enterprise
fundamentally rests on the strength of the peer review process. The journal
editor or the conference chair primarily relies on the expert reviewers'
assessment, identify points of agreement and disagreement and try to reach a
consensus to make a fair and informed decision on whether to accept or reject a
paper. However, with the escalating number of submissions requiring review,
especially in top-tier Artificial Intelligence (AI) conferences, the
editor/chair, among many other works, invests a significant, sometimes
stressful effort to mitigate reviewer disagreements. Here in this work, we
introduce a novel task of automatically identifying contradictions among
reviewers on a given article. To this end, we introduce ContraSciView, a
comprehensive review-pair contradiction dataset on around 8.5k papers (with
around 28k review pairs containing nearly 50k review pair comments) from the
open review-based ICLR and NeurIPS conferences. We further propose a baseline
model that detects contradictory statements from the review pairs. To the best
of our knowledge, we make the first attempt to identify disagreements among
peer reviewers automatically. We make our dataset and code public for further
investigations.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18687" title="Abstract">arXiv:2310.18687</a> [<a href="/pdf/2310.18687" title="Download PDF">pdf</a>, <a href="/format/2310.18687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Behavior Extraction via Random Intent Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiqin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jianing Ye</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+Z">Ziqing Mai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongjie Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Thirty-seventh Conference on Neural Information Processing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reward-free data is abundant and contains rich prior knowledge of human
behaviors, but it is not well exploited by offline reinforcement learning (RL)
algorithms. In this paper, we propose UBER, an unsupervised approach to extract
useful behaviors from offline reward-free datasets via diversified rewards.
UBER assigns different pseudo-rewards sampled from a given prior distribution
to different agents to extract a diverse set of behaviors, and reuse them as
candidate policies to facilitate the learning of new tasks. Perhaps
surprisingly, we show that rewards generated from random neural networks are
sufficient to extract diverse and useful behaviors, some even close to expert
ones. We provide both empirical and theoretical evidence to justify the use of
random priors for the reward function. Experiments on multiple benchmarks
showcase UBER's ability to learn effective and diverse behavior sets that
enhance sample efficiency for online RL, outperforming existing baselines. By
reducing reliance on human supervision, UBER broadens the applicability of RL
to real-world scenarios with abundant reward-free data.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18688" title="Abstract">arXiv:2310.18688</a> [<a href="/pdf/2310.18688" title="Download PDF">pdf</a>, <a href="/format/2310.18688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clairvoyance: A Pipeline Toolkit for Medical Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jarrett%2C+D">Daniel Jarrett</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jinsung Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Bica%2C+I">Ioana Bica</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhaozhi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Ercole%2C+A">Ari Ercole</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proc. 9th International Conference on Learning Representations
  (ICLR 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Time-series learning is the bread and butter of data-driven *clinical
decision support*, and the recent explosion in ML research has demonstrated
great potential in various healthcare settings. At the same time, medical
time-series problems in the wild are challenging due to their highly
*composite* nature: They entail design choices and interactions among
components that preprocess data, impute missing values, select features, issue
predictions, estimate uncertainty, and interpret models. Despite exponential
growth in electronic patient data, there is a remarkable gap between the
potential and realized utilization of ML for clinical research and decision
support. In particular, orchestrating a real-world project lifecycle poses
challenges in engineering (i.e. hard to build), evaluation (i.e. hard to
assess), and efficiency (i.e. hard to optimize). Designed to address these
issues simultaneously, Clairvoyance proposes a unified, end-to-end,
autoML-friendly pipeline that serves as a (i) software toolkit, (ii) empirical
standard, and (iii) interface for optimization. Our ultimate goal lies in
facilitating transparent and reproducible experimentation with complex
inference workflows, providing integrated pathways for (1) personalized
prediction, (2) treatment-effect estimation, and (3) information acquisition.
Through illustrative examples on real-world data in outpatient, general wards,
and intensive-care settings, we illustrate the applicability of the pipeline
paradigm on core tasks in the healthcare journey. To the best of our knowledge,
Clairvoyance is the first to demonstrate viability of a comprehensive and
automatable pipeline for clinical time-series ML.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18689" title="Abstract">arXiv:2310.18689</a> [<a href="/pdf/2310.18689" title="Download PDF">pdf</a>, <a href="/format/2310.18689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundational Models in Medical Imaging: A Comprehensive Survey and  Future Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azad%2C+B">Bobby Azad</a>, 
<a href="/search/cs?searchtype=author&query=Azad%2C+R">Reza Azad</a>, 
<a href="/search/cs?searchtype=author&query=Eskandari%2C+S">Sania Eskandari</a>, 
<a href="/search/cs?searchtype=author&query=Bozorgpour%2C+A">Afshin Bozorgpour</a>, 
<a href="/search/cs?searchtype=author&query=Kazerouni%2C+A">Amirhossein Kazerouni</a>, 
<a href="/search/cs?searchtype=author&query=Rekik%2C+I">Islem Rekik</a>, 
<a href="/search/cs?searchtype=author&query=Merhof%2C+D">Dorit Merhof</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is currently in the process of being prepared for submission to MIA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Foundation models, large-scale, pre-trained deep-learning models adapted to a
wide range of downstream tasks have gained significant interest lately in
various deep-learning problems undergoing a paradigm shift with the rise of
these models. Trained on large-scale dataset to bridge the gap between
different modalities, foundation models facilitate contextual reasoning,
generalization, and prompt capabilities at test time. The predictions of these
models can be adjusted for new tasks by augmenting the model input with
task-specific hints called prompts without requiring extensive labeled data and
retraining. Capitalizing on the advances in computer vision, medical imaging
has also marked a growing interest in these models. To assist researchers in
navigating this direction, this survey intends to provide a comprehensive
overview of foundation models in the domain of medical imaging. Specifically,
we initiate our exploration by providing an exposition of the fundamental
concepts forming the basis of foundation models. Subsequently, we offer a
methodical taxonomy of foundation models within the medical domain, proposing a
classification system primarily structured around training strategies, while
also incorporating additional facets such as application domains, imaging
modalities, specific organs of interest, and the algorithms integral to these
models. Furthermore, we emphasize the practical use case of some selected
approaches and then discuss the opportunities, applications, and future
directions of these large-scale pre-trained models, for analyzing medical
images. In the same vein, we address the prevailing challenges and research
pathways associated with foundational models in medical imaging. These
encompass the areas of interpretability, data management, computational
requirements, and the nuanced issue of contextual comprehension.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18696" title="Abstract">arXiv:2310.18696</a> [<a href="/pdf/2310.18696" title="Download PDF">pdf</a>, <a href="/format/2310.18696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing LLMs for Joint Encoding of Linguistic Categories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Starace%2C+G">Giulio Starace</a>, 
<a href="/search/cs?searchtype=author&query=Papakostas%2C+K">Konstantinos Papakostas</a>, 
<a href="/search/cs?searchtype=author&query=Choenni%2C+R">Rochelle Choenni</a>, 
<a href="/search/cs?searchtype=author&query=Panagiotopoulos%2C+A">Apostolos Panagiotopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Rosati%2C+M">Matteo Rosati</a>, 
<a href="/search/cs?searchtype=author&query=Leidinger%2C+A">Alina Leidinger</a>, 
<a href="/search/cs?searchtype=author&query=Shutova%2C+E">Ekaterina Shutova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) exhibit impressive performance on a range of NLP
tasks, due to the general-purpose linguistic knowledge acquired during
pretraining. Existing model interpretability research (Tenney et al., 2019)
suggests that a linguistic hierarchy emerges in the LLM layers, with lower
layers better suited to solving syntactic tasks and higher layers employed for
semantic processing. Yet, little is known about how encodings of different
linguistic phenomena interact within the models and to what extent processing
of linguistically-related categories relies on the same, shared model
representations. In this paper, we propose a framework for testing the joint
encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence
of joint encoding both at the same (related part-of-speech (POS) classes) and
different (POS classes and related syntactic dependency relations) levels of
linguistic hierarchy. Our cross-lingual experiments show that the same patterns
hold across languages in multilingual LLMs.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18697" title="Abstract">arXiv:2310.18697</a> [<a href="/pdf/2310.18697" title="Download PDF">pdf</a>, <a href="/format/2310.18697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KernelGPA: A Globally Optimal Solution to Deformable SLAM in Closed-form
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+F">Fang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kanzhi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bartoli%2C+A">Adrien Bartoli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in the International Journal of Robotics Research, 2023. <a href="https://doi.org/10.1177/02783649231195380">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Robotics Research, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We study the generalized Procrustes analysis (GPA), as a minimal formulation
to the simultaneous localization and mapping (SLAM) problem. We propose
KernelGPA, a novel global registration technique to solve SLAM in the
deformable environment. We propose the concept of deformable transformation
which encodes the entangled pose and deformation. We define deformable
transformations using a kernel method, and show that both the deformable
transformations and the environment map can be solved globally in closed-form,
up to global scale ambiguities. We solve the scale ambiguities by an
optimization formulation that maximizes rigidity. We demonstrate KernelGPA
using the Gaussian kernel, and validate the superiority of KernelGPA with
various datasets. Code and data are available at
\url{https://bitbucket.org/FangBai/deformableprocrustes}.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18698" title="Abstract">arXiv:2310.18698</a> [<a href="/pdf/2310.18698" title="Download PDF">pdf</a>, <a href="/format/2310.18698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Triplet Attention Transformer for Spatiotemporal Predictive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+X">Xuesong Nie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Haoyuan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhihang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yunfeng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+D">Donglian Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Spatiotemporal predictive learning offers a self-supervised learning paradigm
that enables models to learn both spatial and temporal patterns by predicting
future sequences based on historical sequences. Mainstream methods are
dominated by recurrent units, yet they are limited by their lack of
parallelization and often underperform in real-world scenarios. To improve
prediction quality while maintaining computational efficiency, we propose an
innovative triplet attention transformer designed to capture both inter-frame
dynamics and intra-frame static features. Specifically, the model incorporates
the Triplet Attention Module (TAM), which replaces traditional recurrent units
by exploring self-attention mechanisms in temporal, spatial, and channel
dimensions. In this configuration: (i) temporal tokens contain abstract
representations of inter-frame, facilitating the capture of inherent temporal
dependencies; (ii) spatial and channel attention combine to refine the
intra-frame representation by performing fine-grained interactions across
spatial and channel dimensions. Alternating temporal, spatial, and
channel-level attention allows our approach to learn more complex short- and
long-range spatiotemporal dependencies. Extensive experiments demonstrate
performance surpassing existing recurrent-based and recurrent-free methods,
achieving state-of-the-art under multi-scenario examination including moving
object trajectory prediction, traffic flow prediction, driving scene
prediction, and human motion capture.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18700" title="Abstract">arXiv:2310.18700</a> [<a href="/pdf/2310.18700" title="Download PDF">pdf</a>, <a href="/format/2310.18700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Collaborative Filtering with Principled Adversarial  Contrastive Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">An Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+L">Leheng Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhibo Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Contrastive Learning (CL) has achieved impressive performance in
self-supervised learning tasks, showing superior generalization ability.
Inspired by the success, adopting CL into collaborative filtering (CF) is
prevailing in semi-supervised top-K recommendations. The basic idea is to
routinely conduct heuristic-based data augmentation and apply contrastive
losses (e.g., InfoNCE) on the augmented views. Yet, some CF-tailored challenges
make this adoption suboptimal, such as the issue of out-of-distribution, the
risk of false negatives, and the nature of top-K evaluation. They necessitate
the CL-based CF scheme to focus more on mining hard negatives and
distinguishing false negatives from the vast unlabeled user-item interactions,
for informative contrast signals. Worse still, there is limited understanding
of contrastive loss in CF methods, especially w.r.t. its generalization
ability. To bridge the gap, we delve into the reasons underpinning the success
of contrastive loss in CF, and propose a principled Adversarial InfoNCE loss
(AdvInfoNCE), which is a variant of InfoNCE, specially tailored for CF methods.
AdvInfoNCE adaptively explores and assigns hardness to each negative instance
in an adversarial fashion and further utilizes a fine-grained hardness-aware
ranking criterion to empower the recommender's generalization ability. Training
CF models with AdvInfoNCE, we validate the effectiveness of AdvInfoNCE on both
synthetic and real-world benchmark datasets, thus showing its generalization
ability to mitigate out-of-distribution problems. Given the theoretical
guarantees and empirical superiority of AdvInfoNCE over most contrastive loss
functions, we advocate its adoption as a standard loss in recommender systems,
particularly for the out-of-distribution tasks. Codes are available at
https://github.com/LehengTHU/AdvInfoNCE.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18701" title="Abstract">arXiv:2310.18701</a> [<a href="/pdf/2310.18701" title="Download PDF">pdf</a>, <a href="/format/2310.18701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed  Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Bo Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yimu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yuanyu Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+J">Jinfeng Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper investigates the problem of generalized linear bandits with
heavy-tailed rewards, whose $(1+\epsilon)$-th moment is bounded for some
$\epsilon\in (0,1]$. Although there exist methods for generalized linear
bandits, most of them focus on bounded or sub-Gaussian rewards and are not
well-suited for many real-world scenarios, such as financial markets and
web-advertising. To address this issue, we propose two novel algorithms based
on truncation and mean of medians. These algorithms achieve an almost optimal
regret bound of $\widetilde{O}(dT^{\frac{1}{1+\epsilon}})$, where $d$ is the
dimension of contextual information and $T$ is the time horizon. Our
truncation-based algorithm supports online learning, distinguishing it from
existing truncation-based approaches. Additionally, our mean-of-medians-based
algorithm requires only $O(\log T)$ rewards and one estimator per epoch, making
it more practical. Moreover, our algorithms improve the regret bounds by a
logarithmic factor compared to existing algorithms when $\epsilon=1$. Numerical
experimental results confirm the merits of our algorithms.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18702" title="Abstract">arXiv:2310.18702</a> [<a href="/pdf/2310.18702" title="Download PDF">pdf</a>, <a href="/format/2310.18702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Combinatorial Generalization for Catalysts: A Kohn-Sham  Charge-Density Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pope%2C+P">Phillip Pope</a>, 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+D">David Jacobs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The Kohn-Sham equations underlie many important applications such as the
discovery of new catalysts. Recent machine learning work on catalyst modeling
has focused on prediction of the energy, but has so far not yet demonstrated
significant out-of-distribution generalization. Here we investigate another
approach based on the pointwise learning of the Kohn-Sham charge-density. On a
new dataset of bulk catalysts with charge densities, we show density models can
generalize to new structures with combinations of elements not seen at train
time, a form of combinatorial generalization. We show that over 80% of binary
and ternary test cases achieve faster convergence than standard baselines in
Density Functional Theory, amounting to an average reduction of 13% in the
number of iterations required to reach convergence, which may be of independent
interest. Our results suggest that density learning is a viable alternative,
trading greater inference costs for a step towards combinatorial
generalization, a key property for applications.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18706" title="Abstract">arXiv:2310.18706</a> [<a href="/pdf/2310.18706" title="Download PDF">pdf</a>, <a href="/format/2310.18706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALERTA-Net: A Temporal Distance-Aware Recurrent Networks for Stock  Movement and Volatility Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengkun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">YangXiao Bai</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+K">Kaiqun Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Linhan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chang-Tien Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Taoran Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">For both investors and policymakers, forecasting the stock market is
essential as it serves as an indicator of economic well-being. To this end, we
harness the power of social media data, a rich source of public sentiment, to
enhance the accuracy of stock market predictions. Diverging from conventional
methods, we pioneer an approach that integrates sentiment analysis,
macroeconomic indicators, search engine data, and historical prices within a
multi-attention deep learning model, masterfully decoding the complex patterns
inherent in the data. We showcase the state-of-the-art performance of our
proposed model using a dataset, specifically curated by us, for predicting
stock market movements and volatility.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18709" title="Abstract">arXiv:2310.18709</a> [<a href="/pdf/2310.18709" title="Download PDF">pdf</a>, <a href="/format/2310.18709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio-Visual Instance Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Ruohao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yaru Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yanyu Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+W">Wenzhen Yue</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+D">Dantong Niu</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+X">Xianghua Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we propose a new multi-modal task, namely audio-visual
instance segmentation (AVIS), in which the goal is to identify, segment, and
track individual sounding object instances in audible videos, simultaneously.
To our knowledge, it is the first time that instance segmentation has been
extended into the audio-visual domain. To better facilitate this research, we
construct the first audio-visual instance segmentation benchmark (AVISeg).
Specifically, AVISeg consists of 1,258 videos with an average duration of 62.6
seconds from YouTube and public audio-visual datasets, where 117 videos have
been annotated by using an interactive semi-automatic labeling tool based on
the Segment Anything Model (SAM). In addition, we present a simple baseline
model for the AVIS task. Our new model introduces an audio branch and a
cross-modal fusion module to Mask2Former to locate all sounding objects.
Finally, we evaluate the proposed method using two backbones on AVISeg. We
believe that AVIS will inspire the community towards a more comprehensive
multi-modal understanding.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18713" title="Abstract">arXiv:2310.18713</a> [<a href="/pdf/2310.18713" title="Download PDF">pdf</a>, <a href="/format/2310.18713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Episodic Multi-Task Learning with Heterogeneous Neural Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiayi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhen%2C+X">Xiantong Zhen</a>, Qi (Cheems)
<a href="/search/cs?searchtype=author&query=Wang">Wang</a>, 
<a href="/search/cs?searchtype=author&query=Worring%2C+M">Marcel Worring</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, spotlight of NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper focuses on the data-insufficiency problem in multi-task learning
within an episodic training setup. Specifically, we explore the potential of
heterogeneous information across tasks and meta-knowledge among episodes to
effectively tackle each task with limited data. Existing meta-learning methods
often fail to take advantage of crucial heterogeneous information in a single
episode, while multi-task learning models neglect reusing experience from
earlier episodes. To address the problem of insufficient data, we develop
Heterogeneous Neural Processes (HNPs) for the episodic multi-task setup. Within
the framework of hierarchical Bayes, HNPs effectively capitalize on prior
experiences as meta-knowledge and capture task-relatedness among heterogeneous
tasks, mitigating data-insufficiency. Meanwhile, transformer-structured
inference modules are designed to enable efficient inferences toward
meta-knowledge and task-relatedness. In this way, HNPs can learn more powerful
functional priors for adapting to novel heterogeneous tasks in each meta-test
episode. Experimental results show the superior performance of the proposed
HNPs over typical baselines, and ablation studies verify the effectiveness of
the designed inference modules.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18714" title="Abstract">arXiv:2310.18714</a> [<a href="/pdf/2310.18714" title="Download PDF">pdf</a>, <a href="/ps/2310.18714" title="Download PostScript">ps</a>, <a href="/format/2310.18714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Investigation of Darwiche and Pearl&#x27;s Postulates for Iterated Belief  Update
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+Q">Quanlong Guan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+L">Liangda Fang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+J">Junming Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zhao-Rong Lai</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Weiqi Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Belief revision and update, two significant types of belief change, both
focus on how an agent modify her beliefs in presence of new information. The
most striking difference between them is that the former studies the change of
beliefs in a static world while the latter concentrates on a
dynamically-changing world. The famous AGM and KM postulates were proposed to
capture rational belief revision and update, respectively. However, both of
them are too permissive to exclude some unreasonable changes in the iteration.
In response to this weakness, the DP postulates and its extensions for iterated
belief revision were presented. Furthermore, Rodrigues integrated these
postulates in belief update. Unfortunately, his approach does not meet the
basic requirement of iterated belief update. This paper is intended to solve
this problem of Rodrigues's approach. Firstly, we present a modification of the
original KM postulates based on belief states. Subsequently, we migrate several
well-known postulates for iterated belief revision to iterated belief update.
Moreover, we provide the exact semantic characterizations based on partial
preorders for each of the proposed postulates. Finally, we analyze the
compatibility between the above iterated postulates and the KM postulates for
belief update.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18715" title="Abstract">arXiv:2310.18715</a> [<a href="/pdf/2310.18715" title="Download PDF">pdf</a>, <a href="/format/2310.18715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Offline Policy Evaluation and Optimization with Heavy-Tailed  Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+R">Runzhe Wan</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zhengling Qi</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shikai Luo</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chengchun Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper endeavors to augment the robustness of offline reinforcement
learning (RL) in scenarios laden with heavy-tailed rewards, a prevalent
circumstance in real-world applications. We propose two algorithmic frameworks,
ROAM and ROOM, for robust off-policy evaluation (OPE) and offline policy
optimization (OPO), respectively. Central to our frameworks is the strategic
incorporation of the median-of-means method with offline RL, enabling
straightforward uncertainty estimation for the value function estimator. This
not only adheres to the principle of pessimism in OPO but also adeptly manages
heavy-tailed rewards. Theoretical results and extensive experiments demonstrate
that our two frameworks outperform existing methods on the logged dataset
exhibits heavy-tailed reward distributions.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18716" title="Abstract">arXiv:2310.18716</a> [<a href="/pdf/2310.18716" title="Download PDF">pdf</a>, <a href="/format/2310.18716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Laplacian Canonization: A Minimalist Approach to Sign and Basis  Invariant Spectral Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiangyan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yisen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Thirty-seventh Conference on Neural Information Processing Systems (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spectral embedding is a powerful graph embedding technique that has received
a lot of attention recently due to its effectiveness on Graph Transformers.
However, from a theoretical perspective, the universal expressive power of
spectral embedding comes at the price of losing two important invariance
properties of graphs, sign and basis invariance, which also limits its
effectiveness on graph data. To remedy this issue, many previous methods
developed costly approaches to learn new invariants and suffer from high
computation complexity. In this work, we explore a minimal approach that
resolves the ambiguity issues by directly finding canonical directions for the
eigenvectors, named Laplacian Canonization (LC). As a pure pre-processing
method, LC is light-weighted and can be applied to any existing GNNs. We
provide a thorough investigation, from theory to algorithm, on this approach,
and discover an efficient algorithm named Maximal Axis Projection (MAP) that
works for both sign and basis invariance and successfully canonizes more than
90% of all eigenvectors. Experiments on real-world benchmark datasets like
ZINC, MOLTOX21, and MOLPCBA show that MAP consistently outperforms existing
methods while bringing minimal computation overhead. Code is available at
https://github.com/PKU-ML/LaplacianCanonization.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18718" title="Abstract">arXiv:2310.18718</a> [<a href="/pdf/2310.18718" title="Download PDF">pdf</a>, <a href="/format/2310.18718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Carbon-Awareness in CI/CD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cla%C3%9Fen%2C+H">Henrik Cla&#xdf;en</a>, 
<a href="/search/cs?searchtype=author&query=Thierfeldt%2C+J">Jonas Thierfeldt</a>, 
<a href="/search/cs?searchtype=author&query=Tochman-Szewc%2C+J">Julian Tochman-Szewc</a>, 
<a href="/search/cs?searchtype=author&query=Wiesner%2C+P">Philipp Wiesner</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+O">Odej Kao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21st International Conference on Service-Oriented Computing (ICSOC '24) Workshops
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">While the environmental impact of digitalization is becoming more and more
evident, the climate crisis has become a major issue for society. For instance,
data centers alone account for 2.7% of Europe's energy consumption today. A
considerable part of this load is accounted for by cloud-based services for
automated software development, such as continuous integration and delivery
(CI/CD) workflows.
<br />In this paper, we discuss opportunities and challenges for greening CI/CD
services by better aligning their execution with the availability of low-carbon
energy. We propose a system architecture for carbon-aware CI/CD services, which
uses historical runtime information and, optionally, user-provided information.
We examined the potential effectiveness of different scheduling strategies
using real carbon intensity data and 7,392 workflow executions of Github
Actions, a popular CI/CD service. Our results show, that user-provided
information on workflow deadlines can effectively improve carbon-aware
scheduling.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18720" title="Abstract">arXiv:2310.18720</a> [<a href="/html/2310.18720" title="Download HTML">html</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proceedings of the First Workshop on Trends in Configurable Systems  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=ter+Beek%2C+M+H">Maurice H. ter Beek</a>, 
<a href="/search/cs?searchtype=author&query=Dubslaff%2C+C">Clemens Dubslaff</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 392, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Formal Languages and Automata Theory (cs.FL); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">The analysis of configurable systems, i.e., systems those behaviors depend on
parameters or support various features, is challenging due to the exponential
blowup arising in the number of configuration options. This volume contains the
post-proceedings of TiCSA 2023, the first workshop on Trends in Configurable
Systems Analysis, where current challenges and solutions in configurable
systems analysis were presented and discussed.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18722" title="Abstract">arXiv:2310.18722</a> [<a href="/pdf/2310.18722" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> B-splines and kernels of trigonometric interpolation splines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Denysiuk%2C+V">Volodymyr Denysiuk</a>, 
<a href="/search/math?searchtype=author&query=Hryshko%2C+O">Olena Hryshko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This article focuses on trigonometric Riemann B-splines and Riemann kernels
of trigonometric interpolation splines of arbitrary order; it is shown that
trigonometric interpolation splines are a convolution of trigonometric
B-splines with corresponding kernels. Theoretical statements are followed by
examples, and the results are applicable in many practical areas.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18724" title="Abstract">arXiv:2310.18724</a> [<a href="/pdf/2310.18724" title="Download PDF">pdf</a>, <a href="/format/2310.18724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WCLD: Curated Large Dataset of Criminal Cases from Wisconsin Circuit  Courts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ash%2C+E">Elliott Ash</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+N">Naman Goel</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nianyun Li</a>, 
<a href="/search/cs?searchtype=author&query=Marangon%2C+C">Claudia Marangon</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Peiyao Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> (Forthcoming) Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Machine learning based decision-support tools in criminal justice systems are
subjects of intense discussions and academic research. There are important open
questions about the utility and fairness of such tools. Academic researchers
often rely on a few small datasets that are not sufficient to empirically study
various real-world aspects of these questions. In this paper, we contribute
WCLD, a curated large dataset of 1.5 million criminal cases from circuit courts
in the U.S. state of Wisconsin. We used reliable public data from 1970 to 2020
to curate attributes like prior criminal counts and recidivism outcomes. The
dataset contains large number of samples from five racial groups, in addition
to information like sex and age (at judgment and first offense). Other
attributes in this dataset include neighborhood characteristics obtained from
census data, detailed types of offense, charge severity, case decisions,
sentence lengths, year of filing etc. We also provide pseudo-identifiers for
judge, county and zipcode. The dataset will not only enable researchers to more
rigorously study algorithmic fairness in the context of criminal justice, but
also relate algorithmic challenges with various systemic issues. We also
discuss in detail the process of constructing the dataset and provide a
datasheet. The WCLD dataset is available at
\url{https://clezdata.github.io/wcld/}.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18725" title="Abstract">arXiv:2310.18725</a> [<a href="/pdf/2310.18725" title="Download PDF">pdf</a>, <a href="/format/2310.18725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Evolution of the Interplay Between Input Distributions and Linear  Regions in Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xuan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yi Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">It is commonly recognized that the expressiveness of deep neural networks is
contingent upon a range of factors, encompassing their depth, width, and other
relevant considerations. Currently, the practical performance of the majority
of deep neural networks remains uncertain. For ReLU (Rectified Linear Unit)
networks with piecewise linear activations, the number of linear convex regions
serves as a natural metric to gauge the network's expressivity. In this paper,
we count the number of linear convex regions in deep neural networks based on
ReLU. In particular, we prove that for any one-dimensional input, there exists
a minimum threshold for the number of neurons required to express it. We also
empirically observe that for the same network, intricate inputs hinder its
capacity to express linear regions. Furthermore, we unveil the iterative
refinement process of decision boundaries in ReLU networks during training. We
aspire for our research to serve as an inspiration for network optimization
endeavors and aids in the exploration and analysis of the behaviors exhibited
by deep networks.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18727" title="Abstract">arXiv:2310.18727</a> [<a href="/pdf/2310.18727" title="Download PDF">pdf</a>, <a href="/ps/2310.18727" title="Download PostScript">ps</a>, <a href="/format/2310.18727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent class analysis by regularized spectral clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qing%2C+H">Huan Qing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The latent class model is a powerful tool for identifying latent classes
within populations that share common characteristics for categorical data in
social, psychological, and behavioral sciences. In this article, we propose two
new algorithms to estimate a latent class model for categorical data. Our
algorithms are developed by using a newly defined regularized Laplacian matrix
calculated from the response matrix. We provide theoretical convergence rates
of our algorithms by considering a sparsity parameter and show that our
algorithms stably yield consistent latent class analysis under mild conditions.
Additionally, we propose a metric to capture the strength of latent class
analysis and several procedures designed based on this metric to infer how many
latent classes one should use for real-world categorical data. The efficiency
and accuracy of our algorithms are verified by extensive simulated experiments,
and we further apply our algorithms to real-world categorical data with
promising results.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18728" title="Abstract">arXiv:2310.18728</a> [<a href="/pdf/2310.18728" title="Download PDF">pdf</a>, <a href="/format/2310.18728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Multi-view Anomaly Detection with Disentangled Product-of-Experts  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingdong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yan Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Multimedia 2023, 10 pages, 5 tables, and 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
<p class="mathjax">Multi-view or even multi-modal data is appealing yet challenging for
real-world applications. Detecting anomalies in multi-view data is a prominent
recent research topic. However, most of the existing methods 1) are only
suitable for two views or type-specific anomalies, 2) suffer from the issue of
fusion disentanglement, and 3) do not support online detection after model
deployment. To address these challenges, our main ideas in this paper are
three-fold: multi-view learning, disentangled representation learning, and
generative model. To this end, we propose dPoE, a novel multi-view variational
autoencoder model that involves (1) a Product-of-Experts (PoE) layer in
tackling multi-view data, (2) a Total Correction (TC) discriminator in
disentangling view-common and view-specific representations, and (3) a joint
loss function in wrapping up all components. In addition, we devise theoretical
information bounds to control both view-common and view-specific
representations. Extensive experiments on six real-world datasets demonstrate
that the proposed dPoE outperforms baselines markedly.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18729" title="Abstract">arXiv:2310.18729</a> [<a href="/pdf/2310.18729" title="Download PDF">pdf</a>, <a href="/format/2310.18729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Large Language Models to Support Thematic Analysis in Empirical  Legal Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dr%C3%A1pal%2C+J">Jakub Dr&#xe1;pal</a>, 
<a href="/search/cs?searchtype=author&query=Westermann%2C+H">Hannes Westermann</a>, 
<a href="/search/cs?searchtype=author&query=Savelka%2C+J">Jaromir Savelka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 3 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Thirty-sixth Annual Conference on Legal Knowledge and
  Information Systems (JURIX 2023), Maastricht, The Netherlands
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Thematic analysis and other variants of inductive coding are widely used
qualitative analytic methods within empirical legal studies (ELS). We propose a
novel framework facilitating effective collaboration of a legal expert with a
large language model (LLM) for generating initial codes (phase 2 of thematic
analysis), searching for themes (phase 3), and classifying the data in terms of
the themes (to kick-start phase 4). We employed the framework for an analysis
of a dataset (n=785) of facts descriptions from criminal court opinions
regarding thefts. The goal of the analysis was to discover classes of typical
thefts. Our results show that the LLM, namely OpenAI's GPT-4, generated
reasonable initial codes, and it was capable of improving the quality of the
codes based on expert feedback. They also suggest that the model performed well
in zero-shot classification of facts descriptions in terms of the themes.
Finally, the themes autonomously discovered by the LLM appear to map fairly
well to the themes arrived at by legal experts. These findings can be leveraged
by legal researchers to guide their decisions in integrating LLMs into their
thematic analyses, as well as other inductive coding projects.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18735" title="Abstract">arXiv:2310.18735</a> [<a href="/pdf/2310.18735" title="Download PDF">pdf</a>, <a href="/format/2310.18735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curriculum Learning for Graph Neural Networks: Which Edges Should We  Learn First
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have achieved great success in representing data
with dependencies by recursively propagating and aggregating messages along the
edges. However, edges in real-world graphs often have varying degrees of
difficulty, and some edges may even be noisy to the downstream tasks.
Therefore, existing GNNs may lead to suboptimal learned representations because
they usually treat every edge in the graph equally. On the other hand,
Curriculum Learning (CL), which mimics the human learning principle of learning
data samples in a meaningful order, has been shown to be effective in improving
the generalization ability and robustness of representation learners by
gradually proceeding from easy to more difficult samples during training.
Unfortunately, existing CL strategies are designed for independent data samples
and cannot trivially generalize to handle data dependencies. To address these
issues, we propose a novel CL strategy to gradually incorporate more edges into
training according to their difficulty from easy to hard, where the degree of
difficulty is measured by how well the edges are expected given the model
training status. We demonstrate the strength of our proposed method in
improving the generalization ability and robustness of learned representations
through extensive experiments on nine synthetic datasets and nine real-world
datasets. The code for our proposed method is available at
https://github.com/rollingstonezz/Curriculum_learning_for_GNNs.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18736" title="Abstract">arXiv:2310.18736</a> [<a href="/pdf/2310.18736" title="Download PDF">pdf</a>, <a href="/format/2310.18736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Gale-Shapley View of Unique Stable Marriages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gokhale%2C+K">Kartik Gokhale</a>, 
<a href="/search/cs?searchtype=author&query=Mallik%2C+A+K">Amit Kumar Mallik</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+A+K">Ankit Kumar Misra</a>, 
<a href="/search/cs?searchtype=author&query=Nath%2C+S">Swaprava Nath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
<p class="mathjax">Stable marriage of a two-sided market with unit demand is a classic problem
that arises in many real-world scenarios. In addition, a unique stable marriage
in this market simplifies a host of downstream desiderata. In this paper, we
explore a new set of sufficient conditions for unique stable matching (USM)
under this setup. Unlike other approaches that also address this question using
the structure of preference profiles, we use an algorithmic viewpoint and
investigate if this question can be answered using the lens of the deferred
acceptance (DA) algorithm (Gale and Shapley, 1962). Our results yield a set of
sufficient conditions for USM (viz., MaxProp and MaxRou) and show that these
are disjoint from the previously known sufficiency conditions like sequential
preference and no crossing. We also provide a characterization of MaxProp that
makes it efficiently verifiable, and shows the gap between MaxProp and the
entire USM class. These results give a more detailed view of the sub-structures
of the USM class.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18737" title="Abstract">arXiv:2310.18737</a> [<a href="/pdf/2310.18737" title="Download PDF">pdf</a>, <a href="/format/2310.18737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training with Random Orthogonal Projection Image Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haghighat%2C+M">Maryam Haghighat</a>, 
<a href="/search/cs?searchtype=author&query=Moghadam%2C+P">Peyman Moghadam</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+S">Shaheer Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Koniusz%2C+P">Piotr Koniusz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Masked Image Modeling (MIM) is a powerful self-supervised strategy for visual
pre-training without the use of labels. MIM applies random crops to input
images, processes them with an encoder, and then recovers the masked inputs
with a decoder, which encourages the network to capture and learn structural
information about objects and scenes. The intermediate feature representations
obtained from MIM are suitable for fine-tuning on downstream tasks. In this
paper, we propose an Image Modeling framework based on random orthogonal
projection instead of binary masking as in MIM. Our proposed Random Orthogonal
Projection Image Modeling (ROPIM) reduces spatially-wise token information
under guaranteed bound on the noise variance and can be considered as masking
entire spatial image area under locally varying masking degrees. Since ROPIM
uses a random subspace for the projection that realizes the masking step, the
readily available complement of the subspace can be used during unmasking to
promote recovery of removed information. In this paper, we show that using
random orthogonal projection leads to superior performance compared to
crop-based masking. We demonstrate state-of-the-art results on several popular
benchmarks.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18738" title="Abstract">arXiv:2310.18738</a> [<a href="/pdf/2310.18738" title="Download PDF">pdf</a>, <a href="/format/2310.18738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TLM: Token-Level Masking for Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yangjun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+K">Kebin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Gang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages. Accepted by EMNLP2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Structured dropout approaches, such as attention dropout and DropHead, have
been investigated to regularize the multi-head attention mechanism in
Transformers. In this paper, we propose a new regularization scheme based on
token-level rather than structure-level to reduce overfitting. Specifically, we
devise a novel Token-Level Masking (TLM) training strategy for Transformers to
regularize the connections of self-attention, which consists of two masking
techniques that are effective and easy to implement. The underlying idea is to
manipulate the connections between tokens in the multi-head attention via
masking, where the networks are forced to exploit partial neighbors'
information to produce a meaningful representation. The generality and
effectiveness of TLM are thoroughly evaluated via extensive experiments on 4
diversified NLP tasks across 18 datasets, including natural language
understanding benchmark GLUE, ChineseGLUE, Chinese Grammatical Error
Correction, and data-to-text generation. The results indicate that TLM can
consistently outperform attention dropout and DropHead, e.g., it increases by
0.5 points relative to DropHead with BERT-large on GLUE. Moreover, TLM can
establish a new record on the data-to-text benchmark Rotowire (18.93 BLEU). Our
code will be publicly available at https://github.com/Young1993/tlm.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18740" title="Abstract">arXiv:2310.18740</a> [<a href="/pdf/2310.18740" title="Download PDF">pdf</a>, <a href="/format/2310.18740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TraceDiag: Adaptive, Interpretable, and Efficient Root Cause Analysis on  Large-Scale Microservice Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+R">Ruomeng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Minghua Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaomin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Meng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingjun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xuedong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Hao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Rajmohan%2C+S">Saravan Rajmohan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qingwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Root Cause Analysis (RCA) is becoming increasingly crucial for ensuring the
reliability of microservice systems. However, performing RCA on modern
microservice systems can be challenging due to their large scale, as they
usually comprise hundreds of components, leading significant human effort. This
paper proposes TraceDiag, an end-to-end RCA framework that addresses the
challenges for large-scale microservice systems. It leverages reinforcement
learning to learn a pruning policy for the service dependency graph to
automatically eliminates redundant components, thereby significantly improving
the RCA efficiency. The learned pruning policy is interpretable and fully
adaptive to new RCA instances. With the pruned graph, a causal-based method can
be executed with high accuracy and efficiency. The proposed TraceDiag framework
is evaluated on real data traces collected from the Microsoft Exchange system,
and demonstrates superior performance compared to state-of-the-art RCA
approaches. Notably, TraceDiag has been integrated as a critical component in
the Microsoft M365 Exchange, resulting in a significant improvement in the
system's reliability and a considerable reduction in the human effort required
for RCA.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18741" title="Abstract">arXiv:2310.18741</a> [<a href="/pdf/2310.18741" title="Download PDF">pdf</a>, <a href="/format/2310.18741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Training Implicit Meta-Learning With Applications to Inductive  Weighing in Consistency Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rezk%2C+F">Fady Rezk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Meta-learning that uses implicit gradient have provided an exciting
alternative to standard techniques which depend on the trajectory of the inner
loop training. Implicit meta-learning (IML), however, require computing
$2^{nd}$ order gradients, particularly the Hessian which is impractical to
compute for modern deep learning models. Various approximations for the Hessian
were proposed but a systematic comparison of their compute cost, stability,
generalization of solution found and estimation accuracy were largely
overlooked. In this study, we start by conducting a systematic comparative
analysis of the various approximation methods and their effect when
incorporated into IML training routines. We establish situations where
catastrophic forgetting is exhibited in IML and explain their cause in terms of
the inability of the approximations to estimate the curvature at convergence
points. Sources of IML training instability are demonstrated and remedied. A
detailed analysis of the effeciency of various inverse Hessian-vector product
approximation methods is also provided. Subsequently, we use the insights
gained to propose and evaluate a novel semi-supervised learning algorithm that
learns to inductively weigh consistency regularization losses. We show how
training a "Confidence Network" to extract domain specific features can learn
to up-weigh useful images and down-weigh out-of-distribution samples. Results
outperform the baseline FixMatch performance.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18742" title="Abstract">arXiv:2310.18742</a> [<a href="/pdf/2310.18742" title="Download PDF">pdf</a>, <a href="/format/2310.18742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Ambiguity Strikes Back: How Documentation Improves GPT&#x27;s  Text-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zezhou Huang</a>, 
<a href="/search/cs?searchtype=author&query=Damalapati%2C+P+K">Pavan Kalyan Damalapati</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+E">Eugene Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Text-to-SQL allows experts to use databases without in-depth knowledge of
them. However, real-world tasks have both query and data ambiguities. Most
works on Text-to-SQL focused on query ambiguities and designed chat interfaces
for experts to provide clarifications. In contrast, the data management
community has long studied data ambiguities, but mainly addresses error
detection and correction, rather than documenting them for disambiguation in
data tasks. This work delves into these data ambiguities in real-world
datasets. We have identified prevalent data ambiguities of value consistency,
data coverage, and data granularity that affect tasks. We examine how
documentation, originally made to help humans to disambiguate data, can help
GPT-4 with Text-to-SQL tasks. By offering documentation on these, we found
GPT-4's performance improved by 28.9%.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18743" title="Abstract">arXiv:2310.18743</a> [<a href="/pdf/2310.18743" title="Download PDF">pdf</a>, <a href="/ps/2310.18743" title="Download PostScript">ps</a>, <a href="/format/2310.18743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization of utility-based shortfall risk: A non-asymptotic viewpoint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupte%2C+S">Sumedh Gupte</a>, 
<a href="/search/cs?searchtype=author&query=A.%2C+P+L">Prashanth L. A.</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+S+P">Sanjay P. Bhat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We consider the problems of estimation and optimization of utility-based
shortfall risk (UBSR), which is a popular risk measure in finance. In the
context of UBSR estimation, we derive a non-asymptotic bound on the
mean-squared error of the classical sample average approximation (SAA) of UBSR.
Next, in the context of UBSR optimization, we derive an expression for the UBSR
gradient under a smooth parameterization. This expression is a ratio of
expectations, both of which involve the UBSR. We use SAA for the numerator as
well as denominator in the UBSR gradient expression to arrive at a biased
gradient estimator. We derive non-asymptotic bounds on the estimation error,
which show that our gradient estimator is asymptotically unbiased. We
incorporate the aforementioned gradient estimator into a stochastic gradient
(SG) algorithm for UBSR optimization. Finally, we derive non-asymptotic bounds
that quantify the rate of convergence of our SG algorithm for UBSR
optimization.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18752" title="Abstract">arXiv:2310.18752</a> [<a href="/pdf/2310.18752" title="Download PDF">pdf</a>, <a href="/format/2310.18752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reboost Large Language Model-based Text-to-SQL, Text-to-Python, and  Text-to-Function -- with Real Applications in Traffic Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+G">Guanghu Sui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhishuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+J">Jingqing Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Hangyu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Previous state-of-the-art (SOTA) method achieved a remarkable execution
accuracy on the Spider dataset, which is one of the largest and most diverse
datasets in the Text-to-SQL domain. However, during our reproduce of the
business dataset, we observed a significant drop in performance. We examined
the differences in dataset complexity, as well as the clarity of questions'
intentions, and assessed how those differences could impact the performance of
prompting methods. Subsequently, We develop a more adaptable and more general
prompting method, involving mainly query rewriting and SQL boosting, which
respectively transform vague information into exact and precise information and
enhance the SQL itself by incorporating execution feedback and the query
results from the database content. In order to prevent information gaps, we
include the comments, value types, and value samples for columns as part of the
database description in the prompt. Our experiments with Large Language Models
(LLMs) illustrate the significant performance improvement on the business
dataset and prove the substantial potential of our method. In terms of
execution accuracy on the business dataset, the SOTA method scored 21.05, while
our approach scored 65.79. As a result, our approach achieved a notable
performance improvement even when using a less capable pre-trained language
model. Last but not the least, we also explore the Text-to-Python and
Text-to-Function options, and we deeply analyze the pros and cons among them,
offering valuable insights to the community.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18753" title="Abstract">arXiv:2310.18753</a> [<a href="/pdf/2310.18753" title="Download PDF">pdf</a>, <a href="/format/2310.18753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stochastic Nonlinear Model Predictive Control with an Uncertainty  Propagation Horizon for Autonomous Vehicle Motion Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zarrouki%2C+B">Baha Zarrouki</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Chenyang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Betz%2C+J">Johannes Betz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Employing Stochastic Nonlinear Model Predictive Control (SNMPC) for real-time
applications is challenging due to the complex task of propagating
uncertainties through nonlinear systems. This difficulty becomes more
pronounced in high-dimensional systems with extended prediction horizons, such
as autonomous vehicles. To enhance closed-loop performance in and feasibility
in SNMPCs, we introduce the concept of the Uncertainty Propagation Horizon
(UPH). The UPH limits the time for uncertainty propagation through system
dynamics, preventing trajectory divergence, optimizing feedback loop
advantages, and reducing computational overhead. Our SNMPC approach utilizes
Polynomial Chaos Expansion (PCE) to propagate uncertainties and incorporates
nonlinear hard constraints on state expectations and nonlinear probabilistic
constraints. We transform the probabilistic constraints into deterministic
constraints by estimating the nonlinear constraints' expectation and variance.
We then showcase our algorithm's effectiveness in real-time control of a
high-dimensional, highly nonlinear system-the trajectory following of an
autonomous passenger vehicle, modeled with a dynamic nonlinear single-track
model. Experimental results demonstrate our approach's robust capability to
follow an optimal racetrack trajectory at speeds of up to 37.5m/s while dealing
with state estimation disturbances, achieving a minimum solving frequency of
97Hz. Additionally, our experiments illustrate that limiting the UPH renders
previously infeasible SNMPC problems feasible, even when incorrect uncertainty
assumptions or strong disturbances are present.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18762" title="Abstract">arXiv:2310.18762</a> [<a href="/pdf/2310.18762" title="Download PDF">pdf</a>, <a href="/format/2310.18762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Purify++: Improving Diffusion-Purification with Advanced Diffusion  Models and Control of Randomness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Boya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Weijian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihua Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Adversarial attacks can mislead neural network classifiers. The defense
against adversarial attacks is important for AI safety. Adversarial
purification is a family of approaches that defend adversarial attacks with
suitable pre-processing. Diffusion models have been shown to be effective for
adversarial purification. Despite their success, many aspects of diffusion
purification still remain unexplored. In this paper, we investigate and improve
upon three limiting designs of diffusion purification: the use of an improved
diffusion model, advanced numerical simulation techniques, and optimal control
of randomness. Based on our findings, we propose Purify++, a new diffusion
purification algorithm that is now the state-of-the-art purification method
against several adversarial attacks. Our work presents a systematic exploration
of the limits of diffusion purification methods.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18765" title="Abstract">arXiv:2310.18765</a> [<a href="/pdf/2310.18765" title="Download PDF">pdf</a>, <a href="/format/2310.18765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Semi-Supervised Imbalanced Node Classification from  Bias-Variance Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Divin Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Gengchen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengzhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zengfeng Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper introduces a new approach to address the issue of class imbalance
in graph neural networks (GNNs) for learning on graph-structured data. Our
approach integrates imbalanced node classification and Bias-Variance
Decomposition, establishing a theoretical framework that closely relates data
imbalance to model variance. We also leverage graph augmentation technique to
estimate the variance, and design a regularization term to alleviate the impact
of imbalance. Exhaustive tests are conducted on multiple benchmarks, including
naturally imbalanced datasets and public-split class-imbalanced datasets,
demonstrating that our approach outperforms state-of-the-art methods in various
imbalanced scenarios. This work provides a novel theoretical perspective for
addressing the problem of imbalanced node classification in GNNs.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18768" title="Abstract">arXiv:2310.18768</a> [<a href="/pdf/2310.18768" title="Download PDF">pdf</a>, <a href="/format/2310.18768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crossing the Aisle: Unveiling Partisan and Counter-Partisan Events in  News Reporting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+K">Kaijian Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X+F">Xinliang Frederick Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Winston Wu</a>, 
<a href="/search/cs?searchtype=author&query=Beauchamp%2C+N">Nick Beauchamp</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP'23 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">News media is expected to uphold unbiased reporting. Yet they may still
affect public opinion by selectively including or omitting events that support
or contradict their ideological positions. Prior work in NLP has only studied
media bias via linguistic style and word usage. In this paper, we study to
which degree media balances news reporting and affects consumers through event
inclusion or omission. We first introduce the task of detecting both partisan
and counter-partisan events: events that support or oppose the author's
political ideology. To conduct our study, we annotate a high-quality dataset,
PAC, containing 8,511 (counter-)partisan event annotations in 304 news articles
from ideologically diverse media outlets. We benchmark PAC to highlight the
challenges of this task. Our findings highlight both the ways in which the news
subtly shapes opinion and the need for large language models that better
understand events within a broader context. Our dataset can be found at
https://github.com/launchnlp/Partisan-Event-Dataset.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18769" title="Abstract">arXiv:2310.18769</a> [<a href="/pdf/2310.18769" title="Download PDF">pdf</a>, <a href="/format/2310.18769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Mode Connectivity in Sparse Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McDermott%2C+L">Luke McDermott</a>, 
<a href="/search/cs?searchtype=author&query=Cummings%2C+D">Daniel Cummings</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in NeurIPS 2023 UniReps Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the rise in interest of sparse neural networks, we study how neural
network pruning with synthetic data leads to sparse networks with unique
training properties. We find that distilled data, a synthetic summarization of
the real data, paired with Iterative Magnitude Pruning (IMP) unveils a new
class of sparse networks that are more stable to SGD noise on the real data,
than either the dense model, or subnetworks found with real data in IMP. That
is, synthetically chosen subnetworks often train to the same minima, or exhibit
linear mode connectivity. We study this through linear interpolation, loss
landscape visualizations, and measuring the diagonal of the hessian. While
dataset distillation as a field is still young, we find that these properties
lead to synthetic subnetworks matching the performance of traditional IMP with
up to 150x less training points in settings where distilled data applies.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18770" title="Abstract">arXiv:2310.18770</a> [<a href="/pdf/2310.18770" title="Download PDF">pdf</a>, <a href="/format/2310.18770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Multimodal Features and Item-level User Feedback for Bundle  Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunshan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaohao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yinwei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Z">Zhulin Tao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Automatic bundle construction is a crucial prerequisite step in various
bundle-aware online services. Previous approaches are mostly designed to model
the bundling strategy of existing bundles. However, it is hard to acquire
large-scale well-curated bundle dataset, especially for those platforms that
have not offered bundle services before. Even for platforms with mature bundle
services, there are still many items that are included in few or even zero
bundles, which give rise to sparsity and cold-start challenges in the bundle
construction models. To tackle these issues, we target at leveraging multimodal
features, item-level user feedback signals, and the bundle composition
information, to achieve a comprehensive formulation of bundle construction.
Nevertheless, such formulation poses two new technical challenges: 1) how to
learn effective representations by optimally unifying multiple features, and 2)
how to address the problems of modality missing, noise, and sparsity problems
induced by the incomplete query bundles. In this work, to address these
technical challenges, we propose a Contrastive Learning-enhanced Hierarchical
Encoder method (CLHE). Specifically, we use self-attention modules to combine
the multimodal and multi-item features, and then leverage both item- and
bundle-level contrastive learning to enhance the representation learning, thus
to counter the modality missing, noise, and sparsity problems. Extensive
experiments on four datasets in two application domains demonstrate that our
method outperforms a list of SOTA methods. The code and dataset are available
at https://github.com/Xiaohao-Liu/CLHE.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18771" title="Abstract">arXiv:2310.18771</a> [<a href="/pdf/2310.18771" title="Download PDF">pdf</a>, <a href="/format/2310.18771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot Control based on Motor Primitives -- A Comparison of Two  Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nah%2C+M+C">Moses C. Nah</a>, 
<a href="/search/cs?searchtype=author&query=Lachner%2C+J">Johannes Lachner</a>, 
<a href="/search/cs?searchtype=author&query=Hogan%2C+N">Neville Hogan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Motor primitives are fundamental building blocks of a controller which enable
dynamic robot behavior with minimal high-level intervention. By treating motor
primitives as basic "modules," different modules can be sequenced or
superimposed to generate a rich repertoire of motor behavior. In robotics, two
distinct approaches have been proposed: Dynamic Movement Primitives (DMPs) and
Elementary Dynamic Actions (EDAs). While both approaches instantiate similar
ideas, significant differences also exist. This paper attempts to clarify the
distinction and provide a unifying view by delineating the similarities and
differences between DMPs and EDAs. We provide eight robot control examples,
including sequencing or superimposing movements, managing kinematic redundancy
and singularity, obstacle avoidance, and managing physical interaction. We show
that the two approaches clearly diverge in their implementation. We also
discuss how DMPs and EDAs might be combined to get the best of both approaches.
With this detailed comparison, we enable researchers to make informed decisions
to select the most suitable approach for specific robot tasks and applications.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18772" title="Abstract">arXiv:2310.18772</a> [<a href="/pdf/2310.18772" title="Download PDF">pdf</a>, <a href="/format/2310.18772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-driven Recommendation Framework for Optimal Walker Designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+A">Advaith Narayanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">The rapidly advancing fields of statistical modeling and machine learning
have significantly enhanced data-driven design and optimization. This paper
focuses on leveraging these design algorithms to optimize a medical walker, an
integral part of gait rehabilitation and physiological therapy of the lower
extremities. To achieve the desirable qualities of a walker, we train a
predictive machine-learning model to identify trade-offs between performance
objectives, thus enabling the use of efficient optimization algorithms. To do
this, we use an Automated Machine Learning model utilizing a stacked-ensemble
approach shown to outperform traditional ML models. However, training a
predictive model requires vast amounts of data for accuracy. Due to limited
publicly available walker designs, this paper presents a dataset of more than
5,000 parametric walker designs with performance values to assess mass,
structural integrity, and stability. These performance values include
displacement vectors for the given load case, stress coefficients, mass, and
other physical properties. We also introduce a novel method of systematically
calculating the stability index of a walker. We use MultiObjective
Counterfactuals for Design (MCD), a novel genetic-based optimization algorithm,
to explore the diverse 16-dimensional design space and search for
high-performing designs based on numerous objectives. This paper presents
potential walker designs that demonstrate up to a 30% mass reduction while
increasing structural stability and integrity. This work takes a step toward
the improved development of assistive mobility devices.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18773" title="Abstract">arXiv:2310.18773</a> [<a href="/pdf/2310.18773" title="Download PDF">pdf</a>, <a href="/format/2310.18773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale  Point Cloud Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miyanishi%2C+T">Taiki Miyanishi</a>, 
<a href="/search/cs?searchtype=author&query=Kitamori%2C+F">Fumiya Kitamori</a>, 
<a href="/search/cs?searchtype=author&query=Kurita%2C+S">Shuhei Kurita</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jungdae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kawanabe%2C+M">Motoaki Kawanabe</a>, 
<a href="/search/cs?searchtype=author&query=Inoue%2C+N">Nakamasa Inoue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS D&amp;B 2023. The first two authors are equally contributed
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">City-scale 3D point cloud is a promising way to express detailed and
complicated outdoor structures. It encompasses both the appearance and geometry
features of segmented city components, including cars, streets, and buildings,
that can be utilized for attractive applications such as user-interactive
navigation of autonomous vehicles and drones. However, compared to the
extensive text annotations available for images and indoor scenes, the scarcity
of text annotations for outdoor scenes poses a significant challenge for
achieving these applications. To tackle this problem, we introduce the
CityRefer dataset for city-level visual grounding. The dataset consists of 35k
natural language descriptions of 3D objects appearing in SensatUrban city
scenes and 5k landmarks labels synchronizing with OpenStreetMap. To ensure the
quality and accuracy of the dataset, all descriptions and labels in the
CityRefer dataset are manually verified. We also have developed a baseline
system that can learn encoded language descriptions, 3D object instances, and
geographical information about the city's landmarks to perform visual grounding
on the CityRefer dataset. To the best of our knowledge, the CityRefer dataset
is the largest city-level visual grounding dataset for localizing specific 3D
objects.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18776" title="Abstract">arXiv:2310.18776</a> [<a href="/pdf/2310.18776" title="Download PDF">pdf</a>, <a href="/format/2310.18776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Mixed Autonomy Traffic Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nice%2C+M">Matthew Nice</a>, 
<a href="/search/cs?searchtype=author&query=Bunting%2C+M">Matt Bunting</a>, 
<a href="/search/cs?searchtype=author&query=Richardson%2C+A">Alex Richardson</a>, 
<a href="/search/cs?searchtype=author&query=Zachar%2C+G">Gergely Zachar</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+W">Jonathan W. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Bayen%2C+A">Alexandre Bayen</a>, 
<a href="/search/cs?searchtype=author&query=Monache%2C+M+L+D">Maria Laura Delle Monache</a>, 
<a href="/search/cs?searchtype=author&query=Seibold%2C+B">Benjamin Seibold</a>, 
<a href="/search/cs?searchtype=author&query=Piccoli%2C+B">Benedetto Piccoli</a>, 
<a href="/search/cs?searchtype=author&query=Sprinkle%2C+J">Jonathan Sprinkle</a>, 
<a href="/search/cs?searchtype=author&query=Work%2C+D">Dan Work</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We demonstrate a new capability of automated vehicles: mixed autonomy traffic
control. With this new capability, automated vehicles can shape the traffic
flows composed of other non-automated vehicles, which has the promise to
improve safety, efficiency, and energy outcomes in transportation systems at a
societal scale. Investigating mixed autonomy mobile traffic control must be
done in situ given that the complex dynamics of other drivers and their
response to a team of automated vehicles cannot be effectively modeled. This
capability has been blocked because there is no existing scalable and
affordable platform for experimental control. This paper introduces an
extensible open-source hardware and software platform, enabling a team of 100
vehicles to execute several different vehicular control algorithms as a
collaborative fleet, composed of three different makes and models, which drove
22752 miles in a combined 1022 hours, over 5 days in Nashville, TN in November
2022.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18777" title="Abstract">arXiv:2310.18777</a> [<a href="/pdf/2310.18777" title="Download PDF">pdf</a>, <a href="/format/2310.18777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Compositional Generalization Using Iterated Learning and  Simplicial Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yi Ren</a>, 
<a href="/search/cs?searchtype=author&query=Lavoie%2C+S">Samuel Lavoie</a>, 
<a href="/search/cs?searchtype=author&query=Galkin%2C+M">Mikhail Galkin</a>, 
<a href="/search/cs?searchtype=author&query=Sutherland%2C+D+J">Danica J. Sutherland</a>, 
<a href="/search/cs?searchtype=author&query=Courville%2C+A">Aaron Courville</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Compositional generalization, the ability of an agent to generalize to unseen
combinations of latent factors, is easy for humans but hard for deep neural
networks. A line of research in cognitive science has hypothesized a process,
``iterated learning,'' to help explain how human language developed this
ability; the theory rests on simultaneous pressures towards compressibility
(when an ignorant agent learns from an informed one) and expressivity (when it
uses the representation for downstream tasks). Inspired by this process, we
propose to improve the compositional generalization of deep networks by using
iterated learning on models with simplicial embeddings, which can approximately
discretize representations. This approach is further motivated by an analysis
of compositionality based on Kolmogorov complexity. We show that this
combination of changes improves compositional generalization over other
approaches, demonstrating these improvements both on vision tasks with
well-understood latent factors and on real molecular graph prediction tasks
where the latent structure is unknown.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18778" title="Abstract">arXiv:2310.18778</a> [<a href="/pdf/2310.18778" title="Download PDF">pdf</a>, <a href="/format/2310.18778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProMap: Effective Bilingual Lexicon Induction via Language Model  Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mekki%2C+A+E">Abdellah El Mekki</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>, 
<a href="/search/cs?searchtype=author&query=Nagoudi%2C+E+B">ElMoatez Billah Nagoudi</a>, 
<a href="/search/cs?searchtype=author&query=Berrada%2C+I">Ismail Berrada</a>, 
<a href="/search/cs?searchtype=author&query=Khoumsi%2C+A">Ahmed Khoumsi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IJCNLP-AACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Bilingual Lexicon Induction (BLI), where words are translated between two
languages, is an important NLP task. While noticeable progress on BLI in rich
resource languages using static word embeddings has been achieved. The word
translation performance can be further improved by incorporating information
from contextualized word embeddings. In this paper, we introduce ProMap, a
novel approach for BLI that leverages the power of prompting pretrained
multilingual and multidialectal language models to address these challenges. To
overcome the employment of subword tokens in these models, ProMap relies on an
effective padded prompting of language models with a seed dictionary that
achieves good performance when used independently. We also demonstrate the
effectiveness of ProMap in re-ranking results from other BLI methods such as
with aligned static word embeddings. When evaluated on both rich-resource and
low-resource languages, ProMap consistently achieves state-of-the-art results.
Furthermore, ProMap enables strong performance in few-shot scenarios (even with
less than 10 training examples), making it a valuable tool for low-resource
language translation. Overall, we believe our method offers both exciting and
promising direction for BLI in general and low-resource languages in
particular. ProMap code and data are available at
\url{https://github.com/4mekki4/promap}.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18780" title="Abstract">arXiv:2310.18780</a> [<a href="/pdf/2310.18780" title="Download PDF">pdf</a>, <a href="/format/2310.18780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Laughing Hyena Distillery: Extracting Compact Recurrences From  Convolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Massaroli%2C+S">Stefano Massaroli</a>, 
<a href="/search/cs?searchtype=author&query=Poli%2C+M">Michael Poli</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
<a href="/search/cs?searchtype=author&query=Kumbong%2C+H">Hermann Kumbong</a>, 
<a href="/search/cs?searchtype=author&query=Parnichkun%2C+R+N">Rom N. Parnichkun</a>, 
<a href="/search/cs?searchtype=author&query=Timalsina%2C+A">Aman Timalsina</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+D+W">David W. Romero</a>, 
<a href="/search/cs?searchtype=author&query=McIntyre%2C+Q">Quinn McIntyre</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rudra%2C+A">Atri Rudra</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Ce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Re%2C+C">Christopher Re</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Recent advances in attention-free sequence models rely on convolutions as
alternatives to the attention operator at the core of Transformers. In
particular, long convolution sequence models have achieved state-of-the-art
performance in many domains, but incur a significant cost during
auto-regressive inference workloads -- naively requiring a full pass (or
caching of activations) over the input sequence for each generated token --
similarly to attention-based models. In this paper, we seek to enable $\mathcal
O(1)$ compute and memory cost per token in any pre-trained long convolution
architecture to reduce memory footprint and increase throughput during
generation. Concretely, our methods consist in extracting low-dimensional
linear state-space models from each convolution layer, building upon rational
interpolation and model-order reduction techniques. We further introduce
architectural improvements to convolution-based layers such as Hyena: by
weight-tying the filters across channels into heads, we achieve higher
pre-training quality and reduce the number of filters to be distilled. The
resulting model achieves 10x higher throughput than Transformers and 1.5x
higher than Hyena at 1.3B parameters, without any loss in quality after
distillation.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18783" title="Abstract">arXiv:2310.18783</a> [<a href="/pdf/2310.18783" title="Download PDF">pdf</a>, <a href="/format/2310.18783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are NLP Models Good at Tracing Thoughts: An Overview of Narrative  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lixing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Runcong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Narrative understanding involves capturing the author's cognitive processes,
providing insights into their knowledge, intentions, beliefs, and desires.
Although large language models (LLMs) excel in generating grammatically
coherent text, their ability to comprehend the author's thoughts remains
uncertain. This limitation hinders the practical applications of narrative
understanding. In this paper, we conduct a comprehensive survey of narrative
understanding tasks, thoroughly examining their key features, definitions,
taxonomy, associated datasets, training objectives, evaluation metrics, and
limitations. Furthermore, we explore the potential of expanding the
capabilities of modularized LLMs to address novel narrative understanding
tasks. By framing narrative understanding as the retrieval of the author's
imaginative cues that outline the narrative structure, our study introduces a
fresh perspective on enhancing narrative comprehension.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18784" title="Abstract">arXiv:2310.18784</a> [<a href="/pdf/2310.18784" title="Download PDF">pdf</a>, <a href="/ps/2310.18784" title="Download PostScript">ps</a>, <a href="/format/2310.18784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-probability Convergence Bounds for Nonlinear Stochastic Gradient  Descent Under Heavy-tailed Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Armacki%2C+A">Aleksandar Armacki</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+P">Pranay Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+G">Gauri Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Bajovic%2C+D">Dragana Bajovic</a>, 
<a href="/search/cs?searchtype=author&query=Jakovetic%2C+D">Dusan Jakovetic</a>, 
<a href="/search/cs?searchtype=author&query=Kar%2C+S">Soummya Kar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">Several recent works have studied the convergence \textit{in high
probability} of stochastic gradient descent (SGD) and its clipped variant.
Compared to vanilla SGD, clipped SGD is practically more stable and has the
additional theoretical benefit of logarithmic dependence on the failure
probability. However, the convergence of other practical nonlinear variants of
SGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved
communication efficiency or accelerated convergence is much less understood. In
this work, we study the convergence bounds \textit{in high probability} of a
broad class of nonlinear SGD methods. For strongly convex loss functions with
Lipschitz continuous gradients, we prove a logarithmic dependence on the
failure probability, even when the noise is heavy-tailed. Strictly more general
than the results for clipped SGD, our results hold for any nonlinearity with
bounded (component-wise or joint) outputs, such as clipping, normalization, and
quantization. Further, existing results with heavy-tailed noise assume bounded
$\eta$-th central moments, with $\eta \in (1,2]$. In contrast, our refined
analysis works even for $\eta=1$, strictly relaxing the noise moment
assumptions in the literature.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18786" title="Abstract">arXiv:2310.18786</a> [<a href="/pdf/2310.18786" title="Download PDF">pdf</a>, <a href="/format/2310.18786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Competitive Algorithm for Agnostic Active Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Price%2C+E">Eric Price</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yihan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">For some hypothesis classes and input distributions, active agnostic learning
needs exponentially fewer samples than passive learning; for other classes and
distributions, it offers little to no improvement. The most popular algorithms
for agnostic active learning express their performance in terms of a parameter
called the disagreement coefficient, but it is known that these algorithms are
inefficient on some inputs.
<br />We take a different approach to agnostic active learning, getting an
algorithm that is competitive with the optimal algorithm for any binary
hypothesis class $H$ and distribution $D_X$ over $X$. In particular, if any
algorithm can use $m^*$ queries to get $O(\eta)$ error, then our algorithm uses
$O(m^* \log |H|)$ queries to get $O(\eta)$ error. Our algorithm lies in the
vein of the splitting-based approach of Dasgupta [2004], which gets a similar
result for the realizable ($\eta = 0$) setting.
<br />We also show that it is NP-hard to do better than our algorithm's $O(\log
|H|)$ overhead in general.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18788" title="Abstract">arXiv:2310.18788</a> [<a href="/pdf/2310.18788" title="Download PDF">pdf</a>, <a href="/format/2310.18788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PrObeD: Proactive Object Detection Wrapper
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asnani%2C+V">Vishal Asnani</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Abhinav Kumar</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+S">Suya You</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoming Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Previous research in $2D$ object detection focuses on various tasks,
including detecting objects in generic and camouflaged images. These works are
regarded as passive works for object detection as they take the input image as
is. However, convergence to global minima is not guaranteed to be optimal in
neural networks; therefore, we argue that the trained weights in the object
detector are not optimal. To rectify this problem, we propose a wrapper based
on proactive schemes, PrObeD, which enhances the performance of these object
detectors by learning a signal. PrObeD consists of an encoder-decoder
architecture, where the encoder network generates an image-dependent signal
termed templates to encrypt the input images, and the decoder recovers this
template from the encrypted images. We propose that learning the optimum
template results in an object detector with an improved detection performance.
The template acts as a mask to the input images to highlight semantics useful
for the object detector. Finetuning the object detector with these encrypted
images enhances the detection performance for both generic and camouflaged. Our
experiments on MS-COCO, CAMO, COD$10$K, and NC$4$K datasets show improvement
over different detectors after applying PrObeD. Our models/codes are available
at https://github.com/vishal3477/Proactive-Object-Detection.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18789" title="Abstract">arXiv:2310.18789</a> [<a href="/pdf/2310.18789" title="Download PDF">pdf</a>, <a href="/format/2310.18789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Hybrid AC/DC Power Systems with the Complex Frequency Concept
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ponce%2C+I">Ignacio Ponce</a>, 
<a href="/search/eess?searchtype=author&query=Milano%2C+F">Federico Milano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The concept of complex frequency has been recently introduced on the IEEE
Transactions on Power Systems to study bus voltage variations in magnitude and
frequency and their link with complex power injections of a power system. In
this paper, the complex frequency is applied to time-varying series
connections, namely, RLC dynamic branches, regulating transformers and AC/DC
converters. The proposed modeling approach allows deriving an explicit
expression for the complex frequency of the voltage of a certain bus as a
linear combination of three elements: net current injected by the devices
connected to the bus, adjacent voltages, and time-varying series branches. The
proposed formulation unifies the link between voltage and frequency dynamics in
AC, DC, as well as hybrid AC/DC power systems. A variety of static and dynamic
examples are presented to show the potential of the proposed formulation.
Relevant applications of the proposed modeling approach are outlined.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18791" title="Abstract">arXiv:2310.18791</a> [<a href="/pdf/2310.18791" title="Download PDF">pdf</a>, <a href="/format/2310.18791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Do it my way!&quot;: Impact of Customizations on Trust perceptions in  Human-Robot Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+P">Parv Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+S">Simon Chu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angela Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages including references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Systems and Control (eess.SY)

</div>
<p class="mathjax">Trust has been shown to be a key factor in effective human-robot
collaboration. In the context of assistive robotics, the effect of trust
factors on human experience is further pronounced. Personalization of assistive
robots is an orthogonal factor positively correlated with robot adoption and
user perceptions. In this work, we investigate the relationship between these
factors through a within-subjects study (N=17). We provide different levels of
customization possibilities over baseline autonomous robot behavior and
investigate its impact on trust. Our findings indicate that increased levels of
customization was associated with higher trust and comfort perceptions. The
assistive robot design process can benefit significantly from our insights for
designing trustworthy and customized robots.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18792" title="Abstract">arXiv:2310.18792</a> [<a href="/pdf/2310.18792" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy as Contextual Integrity in Online Proctoring Systems in Higher  Education: A Scoping Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chantal%2C+M">Mutimukwe Chantal</a>, 
<a href="/search/cs?searchtype=author&query=Shengnan%2C+H">Han Shengnan</a>, 
<a href="/search/cs?searchtype=author&query=Olga%2C+V">Viberg Olga</a>, 
<a href="/search/cs?searchtype=author&query=Teresa%2C+C">Cerratto-Pargman Teresa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Privacy is one of the key challenges to the adoption and implementation of
online proctoring systems in higher education. To better understand this
challenge, we adopt privacy as contextual integrity theory to conduct a scoping
review of 17 papers. The results show different types of students' personal and
sensitive information are collected and disseminated; this raises considerable
privacy concerns. As well as the governing principles including transparency
and fairness, consent and choice, information minimization, accountability, and
information security and accuracy have been identified to address privacy
problems. This study notifies a need to clarify how these principles should be
implemented and sustained, and what privacy concerns and actors they relate to.
Further, it calls for the need to clarify the responsibility of key actors in
enacting and sustaining responsible adoption and use of OPS in higher
education.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18794" title="Abstract">arXiv:2310.18794</a> [<a href="/pdf/2310.18794" title="Download PDF">pdf</a>, <a href="/format/2310.18794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded  Dialogue Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yixin Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fanyou Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weijie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sengamedu%2C+S+H">Srinivasan H. Sengamedu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Model hallucination has been a crucial interest of research in Natural
Language Generation (NLG). In this work, we propose sequence-level certainty as
a common theme over hallucination in NLG, and explore the correlation between
sequence-level certainty and the level of hallucination in model responses. We
categorize sequence-level certainty into two aspects: probabilistic certainty
and semantic certainty, and reveal through experiments on Knowledge-Grounded
Dialogue Generation (KGDG) task that both a higher level of probabilistic
certainty and a higher level of semantic certainty in model responses are
significantly correlated with a lower level of hallucination. What's more, we
provide theoretical proof and analysis to show that semantic certainty is a
good estimator of probabilistic certainty, and therefore has the potential as
an alternative to probability-based certainty estimation in black-box
scenarios. Based on the observation on the relationship between certainty and
hallucination, we further propose Certainty-based Response Ranking (CRR), a
decoding-time method for mitigating hallucination in NLG. Based on our
categorization of sequence-level certainty, we propose 2 types of CRR approach:
Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR). P-CRR ranks individually
sampled model responses using their arithmetic mean log-probability of the
entire sequence. S-CRR approaches certainty estimation from meaning-space, and
ranks a number of model response candidates based on their semantic certainty
level, which is estimated by the entailment-based Agreement Score (AS). Through
extensive experiments across 3 KGDG datasets, 3 decoding methods, and on 4
different models, we validate the effectiveness of our 2 proposed CRR methods
to reduce model hallucination.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18795" title="Abstract">arXiv:2310.18795</a> [<a href="/pdf/2310.18795" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review on the Applications of Machine Learning for Tinnitus Diagnosis  Using EEG Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramezani%2C+F">Farzaneh Ramezani</a>, 
<a href="/search/cs?searchtype=author&query=Bolhasani%2C+H">Hamidreza Bolhasani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Tinnitus is a prevalent hearing disorder that can be caused by various
factors such as age, hearing loss, exposure to loud noises, ear infections or
tumors, certain medications, head or neck injuries, and psychological
conditions like anxiety and depression. While not every patient requires
medical attention, about 20% of sufferers seek clinical intervention. Early
diagnosis is crucial for effective treatment. New developments have been made
in tinnitus detection to aid in early detection of this illness. Over the past
few years, there has been a notable growth in the usage of
electroencephalography (EEG) to study variations in oscillatory brain activity
related to tinnitus. However, the results obtained from numerous studies vary
greatly, leading to conflicting conclusions. Currently, clinicians rely solely
on their expertise to identify individuals with tinnitus. Researchers in this
field have incorporated various data modalities and machine-learning techniques
to aid clinicians in identifying tinnitus characteristics and classifying
people with tinnitus. The purpose of writing this article is to review articles
that focus on using machine learning (ML) to identify or predict tinnitus
patients using EEG signals as input data. We have evaluated 11 articles
published between 2016 and 2023 using a systematic literature review (SLR)
method. This article arranges perfect summaries of all the research reviewed
and compares the significant aspects of each. Additionally, we performed
statistical analyses to gain a deeper comprehension of the most recent research
in this area. Almost all of the reviewed articles followed a five-step
procedure to achieve the goal of tinnitus. Disclosure. Finally, we discuss the
open affairs and challenges in this method of tinnitus recognition or
prediction and suggest future directions for research.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18801" title="Abstract">arXiv:2310.18801</a> [<a href="/pdf/2310.18801" title="Download PDF">pdf</a>, <a href="/format/2310.18801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrated Relative-Measurement-Based Network Localization and Formation  Maneuver Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fang%2C+X">Xu Fang</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+L">Lihua Xie</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xiaolei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper studies the problem of integrated distributed network localization
and formation maneuver control. We develop an integrated
relative-measurement-based scheme, which only uses relative positions,
distances, bearings, angles, ratio-of-distances, or their combination to
achieve distributed network localization and formation maneuver control in
$\mathbb{R}^d (d \ge 2)$. By exploring the localizability and invariance of the
target formation, the scale, rotation, and translation of the formation can be
controlled simultaneously by only tuning the leaders' positions, i.e., the
followers do not need to know parameters of the scale, rotation, and
translation of the target formation. The proposed method can globally drive the
formation errors to zero in finite time over multi-layer $d\!+\!1$-rooted
graphs. A simulation example is given to illustrate the theoretical results.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18802" title="Abstract">arXiv:2310.18802</a> [<a href="/pdf/2310.18802" title="Download PDF">pdf</a>, <a href="/ps/2310.18802" title="Download PostScript">ps</a>, <a href="/format/2310.18802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite element approximation of the Einstein tensor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gawlik%2C+E+S">Evan S. Gawlik</a>, 
<a href="/search/math?searchtype=author&query=Neunteufel%2C+M">Michael Neunteufel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2301.02159">arXiv:2301.02159</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Differential Geometry (math.DG)

</div>
<p class="mathjax">We construct and analyze finite element approximations of the Einstein tensor
in dimension $N \ge 3$. We focus on the setting where a smooth Riemannian
metric tensor $g$ on a polyhedral domain $\Omega \subset \mathbb{R}^N$ has been
approximated by a piecewise polynomial metric $g_h$ on a simplicial
triangulation $\mathcal{T}$ of $\Omega$ having maximum element diameter $h$. We
assume that $g_h$ possesses single-valued tangential-tangential components on
every codimension-1 simplex in $\mathcal{T}$. Such a metric is not classically
differentiable in general, but it turns out that one can still attribute
meaning to its Einstein curvature in a distributional sense. We study the
convergence of the distributional Einstein curvature of $g_h$ to the Einstein
curvature of $g$ under refinement of the triangulation. We show that in the
$H^{-2}(\Omega)$-norm, this convergence takes place at a rate of $O(h^{r+1})$
when $g_h$ is an optimal-order interpolant of $g$ that is piecewise polynomial
of degree $r \ge 1$. We provide numerical evidence to support this claim.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18803" title="Abstract">arXiv:2310.18803</a> [<a href="/pdf/2310.18803" title="Download PDF">pdf</a>, <a href="/format/2310.18803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Coupled Deep Q-Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shar%2C+I+E">Ibrahim El Shar</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D+R">Daniel R. Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We propose weakly coupled deep Q-networks (WCDQN), a novel deep reinforcement
learning algorithm that enhances performance in a class of structured problems
called weakly coupled Markov decision processes (WCMDP). WCMDPs consist of
multiple independent subproblems connected by an action space constraint, which
is a structural property that frequently emerges in practice. Despite this
appealing structure, WCMDPs quickly become intractable as the number of
subproblems grows. WCDQN employs a single network to train multiple DQN
"subagents", one for each subproblem, and then combine their solutions to
establish an upper bound on the optimal action value. This guides the main DQN
agent towards optimality. We show that the tabular version, weakly coupled
Q-learning (WCQL), converges almost surely to the optimal action value.
Numerical experiments show faster convergence compared to DQN and related
techniques in settings with as many as 10 subproblems, $3^{10}$ total actions,
and a continuous state space.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18804" title="Abstract">arXiv:2310.18804</a> [<a href="/pdf/2310.18804" title="Download PDF">pdf</a>, <a href="/format/2310.18804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Visual Knowledge Extraction via Relation-Oriented Multimodality  Model Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hejie Cui</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+X">Xinyu Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+X">Xuan Kan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Manling Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Carl Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Images contain rich relational knowledge that can help machines understand
the world. Existing methods on visual knowledge extraction often rely on the
pre-defined format (e.g., sub-verb-obj tuples) or vocabulary (e.g., relation
types), restricting the expressiveness of the extracted knowledge. In this
work, we take a first exploration to a new paradigm of open visual knowledge
extraction. To achieve this, we present OpenVik which consists of an open
relational region detector to detect regions potentially containing relational
knowledge and a visual knowledge generator that generates format-free knowledge
by prompting the large multimodality model with the detected region of
interest. We also explore two data enhancement techniques for diversifying the
generated format-free visual knowledge. Extensive knowledge quality evaluations
highlight the correctness and uniqueness of the extracted open visual knowledge
by OpenVik. Moreover, integrating our extracted knowledge across various visual
reasoning applications shows consistent improvements, indicating the real-world
applicability of OpenVik.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18805" title="Abstract">arXiv:2310.18805</a> [<a href="/pdf/2310.18805" title="Download PDF">pdf</a>, <a href="/format/2310.18805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse distance weighting attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCarter%2C+C">Calvin McCarter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Associative Memory &amp; Hopfield Networks Workshop at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We report the effects of replacing the scaled dot-product (within softmax)
attention with the negative-log of Euclidean distance. This form of attention
simplifies to inverse distance weighting interpolation. Used in simple one
hidden layer networks and trained with vanilla cross-entropy loss on
classification problems, it tends to produce a key matrix containing prototypes
and a value matrix with corresponding logits. We also show that the resulting
interpretable networks can be augmented with manually-constructed prototypes to
perform low-impact handling of special cases.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18807" title="Abstract">arXiv:2310.18807</a> [<a href="/pdf/2310.18807" title="Download PDF">pdf</a>, <a href="/format/2310.18807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OC-NMN: Object-centric Compositional Neural Module Network for  Generative Visual Analogical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Assouel%2C+R">Rim Assouel</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+P">Pau Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Taslakian%2C+P">Perouz Taslakian</a>, 
<a href="/search/cs?searchtype=author&query=Vazquez%2C+D">David Vazquez</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">A key aspect of human intelligence is the ability to imagine -- composing
learned concepts in novel ways -- to make sense of new scenarios. Such capacity
is not yet attained for machine learning systems. In this work, in the context
of visual reasoning, we show how modularity can be leveraged to derive a
compositional data augmentation framework inspired by imagination. Our method,
denoted Object-centric Compositional Neural Module Network (OC-NMN), decomposes
visual generative reasoning tasks into a series of primitives applied to
objects without using a domain-specific language. We show that our modular
architectural choices can be used to generate new training tasks that lead to
better out-of-distribution generalization. We compare our model to existing and
new baselines in proposed visual reasoning benchmark that consists of applying
arithmetic operations to MNIST digits.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18811" title="Abstract">arXiv:2310.18811</a> [<a href="/pdf/2310.18811" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Framework for Interpretable and Probabilistic Model-Based  Safe Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbas%2C+A+N">Ammar N. Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Chasparis%2C+G+C">Georgios C. Chasparis</a>, 
<a href="/search/cs?searchtype=author&query=Kelleher%2C+J+D">John D. Kelleher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2206.13433">arXiv:2206.13433</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Data &amp; Knowledge Engineering, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">The difficulty of identifying the physical model of complex systems has led
to exploring methods that do not rely on such complex modeling of the systems.
Deep reinforcement learning has been the pioneer for solving this problem
without the need for relying on the physical model of complex systems by just
interacting with it. However, it uses a black-box learning approach that makes
it difficult to be applied within real-world and safety-critical systems
without providing explanations of the actions derived by the model.
Furthermore, an open research question in deep reinforcement learning is how to
focus the policy learning of critical decisions within a sparse domain. This
paper proposes a novel approach for the use of deep reinforcement learning in
safety-critical systems. It combines the advantages of probabilistic modeling
and reinforcement learning with the added benefits of interpretability and
works in collaboration and synchronization with conventional decision-making
strategies. The BC-SRLA is activated in specific situations which are
identified autonomously through the fused information of probabilistic model
and reinforcement learning, such as abnormal conditions or when the system is
near-to-failure. Further, it is initialized with a baseline policy using policy
cloning to allow minimum interactions with the environment to address the
challenges associated with using RL in safety-critical industries. The
effectiveness of the BC-SRLA is demonstrated through a case study in
maintenance applied to turbofan engines, where it shows superior performance to
the prior art and other baselines.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18812" title="Abstract">arXiv:2310.18812</a> [<a href="/pdf/2310.18812" title="Download PDF">pdf</a>, <a href="/format/2310.18812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniCat: Crafting a Stronger Fusion Baseline for Multimodal  Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crawford%2C+J">Jennifer Crawford</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Haoli Yin</a>, 
<a href="/search/cs?searchtype=author&query=McDermott%2C+L">Luke McDermott</a>, 
<a href="/search/cs?searchtype=author&query=Cummings%2C+D">Daniel Cummings</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted NeurIPS 2023 UniReps, 9 pages, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multimodal Re-Identification (ReID) is a popular retrieval task that aims to
re-identify objects across diverse data streams, prompting many researchers to
integrate multiple modalities into a unified representation. While such fusion
promises a holistic view, our investigations shed light on potential pitfalls.
We uncover that prevailing late-fusion techniques often produce suboptimal
latent representations when compared to methods that train modalities in
isolation. We argue that this effect is largely due to the inadvertent
relaxation of the training objectives on individual modalities when using
fusion, what others have termed modality laziness. We present a nuanced
point-of-view that this relaxation can lead to certain modalities failing to
fully harness available task-relevant information, and yet, offers a protective
veil to noisy modalities, preventing them from overfitting to task-irrelevant
data. Our findings also show that unimodal concatenation (UniCat) and other
late-fusion ensembling of unimodal backbones, when paired with best-known
training techniques, exceed the current state-of-the-art performance across
several multimodal ReID benchmarks. By unveiling the double-edged sword of
"modality laziness", we motivate future research in balancing local modality
strengths with global representations.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18813" title="Abstract">arXiv:2310.18813</a> [<a href="/pdf/2310.18813" title="Download PDF">pdf</a>, <a href="/format/2310.18813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Synergy of Speculative Decoding and Batching in Serving Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Q">Qidong Su</a>, 
<a href="/search/cs?searchtype=author&query=Giannoula%2C+C">Christina Giannoula</a>, 
<a href="/search/cs?searchtype=author&query=Pekhimenko%2C+G">Gennady Pekhimenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Large Language Models (LLMs) like GPT are state-of-the-art text generation
models that provide significant assistance in daily routines. However, LLM
execution is inherently sequential, since they only produce one token at a
time, thus incurring low hardware utilization on modern GPUs. Batching and
speculative decoding are two techniques to improve GPU hardware utilization in
LLM inference. To study their synergy, we implement a prototype implementation
and perform an extensive characterization analysis on various LLM models and
GPU architectures. We observe that the optimal speculation length depends on
the batch size used. We analyze the key observation and build a quantitative
model to explain it. Based on our analysis, we propose a new adaptive
speculative decoding strategy that chooses the optimal speculation length for
different batch sizes. Our evaluations show that our proposed method can
achieve equal or better performance than the state-of-the-art speculation
decoding schemes with fixed speculation length.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18815" title="Abstract">arXiv:2310.18815</a> [<a href="/pdf/2310.18815" title="Download PDF">pdf</a>, <a href="/format/2310.18815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Semi-Supervised Federated Learning: How to co-train  fully-labeled and fully-unlabeled client imaging data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+P">Pramit Saha</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+D">Divyanshu Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Noble%2C+J+A">J. Alison Noble</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in MICCAI 2023 with early acceptance and selected as 1 of the top 20 poster highlights under the category: Which work has the potential to impact other applications of AI and CV
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The most challenging, yet practical, setting of semi-supervised federated
learning (SSFL) is where a few clients have fully labeled data whereas the
other clients have fully unlabeled data. This is particularly common in
healthcare settings where collaborating partners (typically hospitals) may have
images but not annotations. The bottleneck in this setting is the joint
training of labeled and unlabeled clients as the objective function for each
client varies based on the availability of labels. This paper investigates an
alternative way for effective training with labeled and unlabeled clients in a
federated setting. We propose a novel learning scheme specifically designed for
SSFL which we call Isolated Federated Learning (IsoFed) that circumvents the
problem by avoiding simple averaging of supervised and semi-supervised models
together. In particular, our training approach consists of two parts - (a)
isolated aggregation of labeled and unlabeled client models, and (b) local
self-supervised pretraining of isolated global models in all clients. We
evaluate our model performance on medical image datasets of four different
modalities publicly available within the biomedical image classification
benchmark MedMNIST. We further vary the proportion of labeled clients and the
degree of heterogeneity to demonstrate the effectiveness of the proposed method
under varied experimental settings.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18816" title="Abstract">arXiv:2310.18816</a> [<a href="/pdf/2310.18816" title="Download PDF">pdf</a>, <a href="/format/2310.18816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Test-Time Personalization for Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+W">Wenxuan Bao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianxin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haohan Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jingrui He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Personalized federated learning algorithms have shown promising results in
adapting models to various distribution shifts. However, most of these methods
require labeled data on testing clients for personalization, which is usually
unavailable in real-world scenarios. In this paper, we introduce a novel
setting called test-time personalized federated learning (TTPFL), where clients
locally adapt a global model in an unsupervised way without relying on any
labeled data during test-time. While traditional test-time adaptation (TTA) can
be used in this scenario, most of them inherently assume training data come
from a single domain, while they come from multiple clients (source domains)
with different distributions. Overlooking these domain interrelationships can
result in suboptimal generalization. Moreover, most TTA algorithms are designed
for a specific kind of distribution shift and lack the flexibility to handle
multiple kinds of distribution shifts in FL. In this paper, we find that this
lack of flexibility partially results from their pre-defining which modules to
adapt in the model. To tackle this challenge, we propose a novel algorithm
called ATP to adaptively learns the adaptation rates for each module in the
model from distribution shifts among source domains. Theoretical analysis
proves the strong generalization of ATP. Extensive experiments demonstrate its
superiority in handling various distribution shifts including label shift,
image corruptions, and domain shift, outperforming existing TTA methods across
multiple datasets and model architectures. Our code is available at
https://github.com/baowenxuan/ATP .
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18820" title="Abstract">arXiv:2310.18820</a> [<a href="/pdf/2310.18820" title="Download PDF">pdf</a>, <a href="/format/2310.18820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demand-Side Threats to Power Grid Operations from IoT-Enabled Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lakshminarayana%2C+S">Subhash Lakshminarayana</a>, 
<a href="/search/cs?searchtype=author&query=Maple%2C+C">Carsten Maple</a>, 
<a href="/search/cs?searchtype=author&query=Larkins%2C+A">Andrew Larkins</a>, 
<a href="/search/cs?searchtype=author&query=Flack%2C+D">Daryl Flack</a>, 
<a href="/search/cs?searchtype=author&query=Few%2C+C">Christopher Few</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A+K">Anurag. K. Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT); Systems and Control (eess.SY)

</div>
<p class="mathjax">The growing adoption of Internet-of-Things (IoT)-enabled energy smart
appliances (ESAs) at the consumer end, such as smart heat pumps, electric
vehicle chargers, etc., is seen as key to enabling demand-side response (DSR)
services. However, these smart appliances are often poorly engineered from a
security point of view and present a new threat to power grid operations. They
may become convenient entry points for malicious parties to gain access to the
system and disrupt important grid operations by abruptly changing the demand.
Unlike utility-side and SCADA assets, ESAs are not monitored continuously due
to their large numbers and the lack of extensive monitoring infrastructure at
consumer sites. This article presents an in-depth analysis of the demand side
threats to power grid operations including (i) an overview of the
vulnerabilities in ESAs and the wider risk from the DSR ecosystem and (ii) key
factors influencing the attack impact on power grid operations. Finally, it
presents measures to improve the cyber-physical resilience of power grids,
putting them in the context of ongoing efforts from the industry and regulatory
bodies worldwide.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18823" title="Abstract">arXiv:2310.18823</a> [<a href="/pdf/2310.18823" title="Download PDF">pdf</a>, <a href="/format/2310.18823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Successfully Applying Lottery Ticket Hypothesis to Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+B">Bo Hui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bohan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Da Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Despite the success of diffusion models, the training and inference of
diffusion models are notoriously expensive due to the long chain of the reverse
process. In parallel, the Lottery Ticket Hypothesis (LTH) claims that there
exists winning tickets (i.e., aproperly pruned sub-network together with
original weight initialization) that can achieve performance competitive to the
original dense neural network when trained in isolation. In this work, we for
the first time apply LTH to diffusion models. We empirically find subnetworks
at sparsity 90%-99% without compromising performance for denoising diffusion
probabilistic models on benchmarks (CIFAR-10, CIFAR-100, MNIST). Moreover,
existing LTH works identify the subnetworks with a unified sparsity along
different layers. We observe that the similarity between two winning tickets of
a model varies from block to block. Specifically, the upstream layers from two
winning tickets for a model tend to be more similar than the downstream layers.
Therefore, we propose to find the winning ticket with varying sparsity along
different layers in the model. Experimental results demonstrate that our method
can find sparser sub-models that require less memory for storage and reduce the
necessary number of FLOPs. Codes are available at
https://github.com/osier0524/Lottery-Ticket-to-DDPM.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18825" title="Abstract">arXiv:2310.18825</a> [<a href="/pdf/2310.18825" title="Download PDF">pdf</a>, <a href="/format/2310.18825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fuzzy Time Series-Based Model Using Particle Swarm Optimization and  Weighted Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ortiz-Arroyo%2C+D">Daniel Ortiz-Arroyo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">During the last decades, a myriad of fuzzy time series models have been
proposed in scientific literature. Among the most accurate models found in
fuzzy time series, the high-order ones are the most accurate. The research
described in this paper tackles three potential limitations associated with the
application of high-order fuzzy time series models. To begin with, the adequacy
of forecast rules lacks consistency. Secondly, as the model's order increases,
data utilization diminishes. Thirdly, the uniformity of forecast rules proves
to be highly contingent on the chosen interval partitions. To address these
likely drawbacks, we introduce a novel model based on fuzzy time series that
amalgamates the principles of particle swarm optimization (PSO) and weighted
summation. Our results show that our approach models accurately the time series
in comparison with previous methods.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18827" title="Abstract">arXiv:2310.18827</a> [<a href="/pdf/2310.18827" title="Download PDF">pdf</a>, <a href="/format/2310.18827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All Things Considered: Detecting Partisan Events from News Media with  Cross-Article Comparison
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yujian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X+F">Xinliang Frederick Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+K">Kaijian Zou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruihong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Beauchamp%2C+N">Nick Beauchamp</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP'23 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Public opinion is shaped by the information news media provide, and that
information in turn may be shaped by the ideological preferences of media
outlets. But while much attention has been devoted to media bias via overt
ideological language or topic selection, a more unobtrusive way in which the
media shape opinion is via the strategic inclusion or omission of partisan
events that may support one side or the other. We develop a latent
variable-based framework to predict the ideology of news articles by comparing
multiple articles on the same story and identifying partisan events whose
inclusion or omission reveals ideology. Our experiments first validate the
existence of partisan event selection, and then show that article alignment and
cross-document comparison detect partisan events and article ideology better
than competitive baselines. Our results reveal the high-level form of media
bias, which is present even among mainstream media with strong norms of
objectivity and nonpartisanship. Our codebase and dataset are available at
https://github.com/launchnlp/ATC.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18830" title="Abstract">arXiv:2310.18830</a> [<a href="/pdf/2310.18830" title="Download PDF">pdf</a>, <a href="/format/2310.18830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Translating away Translationese without Parallel Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jalota%2C+R">Rricha Jalota</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+K+D">Koel Dutta Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Espa%C3%B1a-Bonet%2C+C">Cristina Espa&#xf1;a-Bonet</a>, 
<a href="/search/cs?searchtype=author&query=van+Genabith%2C+J">Josef van Genabith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023, Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Translated texts exhibit systematic linguistic differences compared to
original texts in the same language, and these differences are referred to as
translationese. Translationese has effects on various cross-lingual natural
language processing tasks, potentially leading to biased results. In this
paper, we explore a novel approach to reduce translationese in translated
texts: translation-based style transfer. As there are no parallel
human-translated and original data in the same language, we use a
self-supervised approach that can learn from comparable (rather than parallel)
mono-lingual original and translated data. However, even this self-supervised
approach requires some parallel data for validation. We show how we can
eliminate the need for parallel validation data by combining the
self-supervised loss with an unsupervised loss. This unsupervised loss
leverages the original language model loss over the style-transferred output
and a semantic similarity loss between the input and style-transferred output.
We evaluate our approach in terms of original vs. translationese binary
classification in addition to measuring content preservation and target-style
fluency. The results show that our approach is able to reduce translationese
classifier accuracy to a level of a random classifier after style transfer
while adequately preserving the content and fluency in the target original
style.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18831" title="Abstract">arXiv:2310.18831</a> [<a href="/pdf/2310.18831" title="Download PDF">pdf</a>, <a href="/ps/2310.18831" title="Download PostScript">ps</a>, <a href="/format/2310.18831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Paired 2-disjoint path covers of burnt pancake graphs with faulty  elements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dvo%C5%99%C3%A1k%2C+T">Tom&#xe1;&#x161; Dvo&#x159;&#xe1;k</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+M">Mei-Mei Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">The burnt pancake graph $BP_n$ is the Cayley graph of the hyperoctahedral
group using prefix reversals as generators. Let $\{u,v\}$ and $\{x,y\}$ be any
two pairs of distinct vertices of $BP_n$ for $n\geq 4$. We show that there are
$u-v$ and $x-y$ paths whose vertices partition the vertex set of $BP_n$ even if
$BP_n$ has up to $n-4$ faulty elements. On the other hand, for every $n\ge3$
there is a set of $n-2$ faulty edges or faulty vertices for which such a
fault-free disjoint path cover does not exist.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18832" title="Abstract">arXiv:2310.18832</a> [<a href="/pdf/2310.18832" title="Download PDF">pdf</a>, <a href="/format/2310.18832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Responsible AI (RAI) Games and Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+Y">Yash Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+R">Runtian Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Suggala%2C+A">Arun Suggala</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+P">Pradeep Ravikumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Several recent works have studied the societal effects of AI; these include
issues such as fairness, robustness, and safety. In many of these objectives, a
learner seeks to minimize its worst-case loss over a set of predefined
distributions (known as uncertainty sets), with usual examples being perturbed
versions of the empirical distribution. In other words, aforementioned problems
can be written as min-max problems over these uncertainty sets. In this work,
we provide a general framework for studying these problems, which we refer to
as Responsible AI (RAI) games. We provide two classes of algorithms for solving
these games: (a) game-play based algorithms, and (b) greedy stagewise
estimation algorithms. The former class is motivated by online learning and
game theory, whereas the latter class is motivated by the classical statistical
literature on boosting, and regression. We empirically demonstrate the
applicability and competitive performance of our techniques for solving several
RAI problems, particularly around subpopulation shift.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18833" title="Abstract">arXiv:2310.18833</a> [<a href="/pdf/2310.18833" title="Download PDF">pdf</a>, <a href="/format/2310.18833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-based Control of the Scanning Tunneling Microscope: Enabling New  Modes of Imaging, Spectroscopy, and Lithography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alemansour%2C+H">Hamed Alemansour</a>, 
<a href="/search/eess?searchtype=author&query=Moheimani%2C+S+O+R">S. O. Reza Moheimani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The invention of scanning tunneling microscope (STM) dates back to the work
of Binnig and Rohrer in the early 1980s, whose seminal contribution was
rewarded by the 1986 Nobel Prize in Physics for the design of the scanning
tunneling microscope. Forty years later, the STM remains the best existing tool
for studying electronic, chemical, and physical properties of conducting and
semiconducting surfaces with atomic precision. It has opened entirely new
fields of research, enabling scientists to gain invaluable insight into
properties and structure of matter at the atomic scale. Recent breakthroughs in
STM-based automated hydrogen depassivation lithography (HDL) on silicon have
resulted in the STM being considered a viable tool for fabrication of
error-free silicon-based quantum-electronic devices. Despite the STM's unique
ability to interrogate and manipulate matter with atomic precision, it remains
a challenging tool to use. It turns out that many issues can be traced back to
the STM's feedback control system, which has remained essentially unchanged
since its invention about 40 years ago. This article explains the role of
feedback control system of the STM and reviews some of the recent progress made
possible in imaging, spectroscopy, and lithography by making appropriate
changes to the STM's feedback control loop. We believe that the full potential
of the STM is yet to be realized, and the key to new innovations will be the
application of advanced model-based control and estimation techniques to this
system.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18834" title="Abstract">arXiv:2310.18834</a> [<a href="/pdf/2310.18834" title="Download PDF">pdf</a>, <a href="/format/2310.18834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automating the Correctness Assessment of AI-generated Code for Security  Contexts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cotroneo%2C+D">Domenico Cotroneo</a>, 
<a href="/search/cs?searchtype=author&query=Foggia%2C+A">Alessio Foggia</a>, 
<a href="/search/cs?searchtype=author&query=Improta%2C+C">Cristina Improta</a>, 
<a href="/search/cs?searchtype=author&query=Liguori%2C+P">Pietro Liguori</a>, 
<a href="/search/cs?searchtype=author&query=Natella%2C+R">Roberto Natella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we propose a fully automated method, named ACCA, to evaluate
the correctness of AI-generated code for security purposes. The method uses
symbolic execution to assess whether the AI-generated code behaves as a
reference implementation. We use ACCA to assess four state-of-the-art models
trained to generate security-oriented assembly code and compare the results of
the evaluation with different baseline solutions, including output similarity
metrics, widely used in the field, and the well-known ChatGPT, the AI-powered
language model developed by OpenAI. Our experiments show that our method
outperforms the baseline solutions and assesses the correctness of the
AI-generated code similar to the human-based evaluation, which is considered
the ground truth for the assessment in the field. Moreover, ACCA has a very
strong correlation with human evaluation (Pearson's correlation coefficient
r=0.84 on average). Finally, since it is a fully automated solution that does
not require any human intervention, the proposed method performs the assessment
of every code snippet in ~0.17s on average, which is definitely lower than the
average time required by human analysts to manually inspect the code, based on
our experience.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18839" title="Abstract">arXiv:2310.18839</a> [<a href="/pdf/2310.18839" title="Download PDF">pdf</a>, <a href="/format/2310.18839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Telehealth Chain: a protocol for secure and transparent telemedicine  transactions on the blockchain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahdi%2C+S+S">Syed Sarosh Mahdi</a>, 
<a href="/search/cs?searchtype=author&query=Ullah%2C+Z">Zaib Ullah</a>, 
<a href="/search/cs?searchtype=author&query=Battineni%2C+G">Gopi Battineni</a>, 
<a href="/search/cs?searchtype=author&query=Babar%2C+M+G">Muneer Gohar Babar</a>, 
<a href="/search/cs?searchtype=author&query=Daood%2C+U">Umer Daood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Blockchain technology provides a secure and decentralized platform for
storing and transferring sensitive medical data, which can be utilized to
enable remote medical consultations. This paper proposes a theoretical
framework for creating a blockchain-based digital entity to facilitate
telemedicine services. The proposed framework utilizes blockchain technology to
provide a secure and reliable platform for medical practitioners to remotely
interact with patient transactions. The blockchain will serve as a one-stop
digital service to secure patient data, ensure privacy, and facilitate
payments. The proposed framework leverages the existing Hyperledger Fabric
platform to build a secure blockchain-assisted telemedicine platform.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18840" title="Abstract">arXiv:2310.18840</a> [<a href="/pdf/2310.18840" title="Download PDF">pdf</a>, <a href="/format/2310.18840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Customizing 360-Degree Panoramas through Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+X">Xiaoyu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yuchen Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jing-Hao Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV 2024, Project Page: <a href="https://littlewhitesea.github.io/stitchdiffusion.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Personalized text-to-image (T2I) synthesis based on diffusion models has
attracted significant attention in recent research. However, existing methods
primarily concentrate on customizing subjects or styles, neglecting the
exploration of global geometry. In this study, we propose an approach that
focuses on the customization of 360-degree panoramas, which inherently possess
global geometric properties, using a T2I diffusion model. To achieve this, we
curate a paired image-text dataset specifically designed for the task and
subsequently employ it to fine-tune a pre-trained T2I diffusion model with
LoRA. Nevertheless, the fine-tuned model alone does not ensure the continuity
between the leftmost and rightmost sides of the synthesized images, a crucial
characteristic of 360-degree panoramas. To address this issue, we propose a
method called StitchDiffusion. Specifically, we perform pre-denoising
operations twice at each time step of the denoising process on the stitch block
consisting of the leftmost and rightmost image regions. Furthermore, a global
cropping is adopted to synthesize seamless 360-degree panoramas. Experimental
results demonstrate the effectiveness of our customized model combined with the
proposed StitchDiffusion in generating high-quality 360-degree panoramic
images. Moreover, our customized model exhibits exceptional generalization
ability in producing scenes unseen in the fine-tuning dataset. Code is
available at https://github.com/littlewhitesea/StitchDiffusion.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18844" title="Abstract">arXiv:2310.18844</a> [<a href="/pdf/2310.18844" title="Download PDF">pdf</a>, <a href="/format/2310.18844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BanditPAM++: Faster $k$-medoids Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+M">Mo Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+R">Ryan Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Donghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Thrun%2C+S">Sebastian Thrun</a>, 
<a href="/search/cs?searchtype=author&query=Piech%2C+C">Chris Piech</a>, 
<a href="/search/cs?searchtype=author&query=Shomorony%2C+I">Ilan Shomorony</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M+J">Martin Jinye Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Clustering is a fundamental task in data science with wide-ranging
applications. In $k$-medoids clustering, cluster centers must be actual
datapoints and arbitrary distance metrics may be used; these features allow for
greater interpretability of the cluster centers and the clustering of exotic
objects in $k$-medoids clustering, respectively. $k$-medoids clustering has
recently grown in popularity due to the discovery of more efficient $k$-medoids
algorithms. In particular, recent research has proposed BanditPAM, a randomized
$k$-medoids algorithm with state-of-the-art complexity and clustering accuracy.
In this paper, we present BanditPAM++, which accelerates BanditPAM via two
algorithmic improvements, and is $O(k)$ faster than BanditPAM in complexity and
substantially faster than BanditPAM in wall-clock runtime. First, we
demonstrate that BanditPAM has a special structure that allows the reuse of
clustering information $\textit{within}$ each iteration. Second, we demonstrate
that BanditPAM has additional structure that permits the reuse of information
$\textit{across}$ different iterations. These observations inspire our proposed
algorithm, BanditPAM++, which returns the same clustering solutions as
BanditPAM but often several times faster. For example, on the CIFAR10 dataset,
BanditPAM++ returns the same results as BanditPAM but runs over 10$\times$
faster. Finally, we provide a high-performance C++ implementation of
BanditPAM++, callable from Python and R, that may be of interest to
practitioners at https://github.com/motiwari/BanditPAM. Auxiliary code to
reproduce all of our experiments via a one-line script is available at
https://github.com/ThrunGroup/BanditPAM_plusplus_experiments.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18845" title="Abstract">arXiv:2310.18845</a> [<a href="/pdf/2310.18845" title="Download PDF">pdf</a>, <a href="/format/2310.18845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application of Collaborative Learning Paradigms within Software  Engineering Education: A Systematic Mapping Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia%2C+R">Rita Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Treude%2C+C">Christoph Treude</a>, 
<a href="/search/cs?searchtype=author&query=Valentine%2C+A">Andrew Valentine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Collaboration is used in Software Engineering (SE) to develop software.
Industry seeks SE graduates with collaboration skills to contribute to
productive software development. SE educators can use Collaborative Learning
(CL) to help students develop collaboration skills. This paper uses a
Systematic Mapping Study (SMS) to examine the application of the CL educational
theory in SE Education. The SMS identified 14 papers published between 2011 and
2022. We used qualitative analysis to classify the papers into four CL
paradigms: Conditions, Effect, Interactions, and Computer-Supported
Collaborative Learning (CSCL). We found a high interest in CSCL, with a shift
in student interaction research to computer-mediated technologies. We discussed
the 14 papers in depth, describing their goals and further analysing the CSCL
research. Almost half the papers did not achieve the appropriate level of
supporting evidence; however, calibrating the instruments presented could
strengthen findings and support multiple CL paradigms, especially opportunities
to learn at the social and community levels, where research was lacking. Though
our results demonstrate limited CL educational theory applied in SE Education,
we discuss future work to layer the theory on existing study designs for more
effective teaching strategies.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18846" title="Abstract">arXiv:2310.18846</a> [<a href="/pdf/2310.18846" title="Download PDF">pdf</a>, <a href="/format/2310.18846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INCODE: Implicit Neural Conditioning with Prior Knowledge Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kazerouni%2C+A">Amirhossein Kazerouni</a>, 
<a href="/search/cs?searchtype=author&query=Azad%2C+R">Reza Azad</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+A">Alireza Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Merhof%2C+D">Dorit Merhof</a>, 
<a href="/search/cs?searchtype=author&query=Bagci%2C+U">Ulas Bagci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Implicit Neural Representations (INRs) have revolutionized signal
representation by leveraging neural networks to provide continuous and smooth
representations of complex data. However, existing INRs face limitations in
capturing fine-grained details, handling noise, and adapting to diverse signal
types. To address these challenges, we introduce INCODE, a novel approach that
enhances the control of the sinusoidal-based activation function in INRs using
deep prior knowledge. INCODE comprises a harmonizer network and a composer
network, where the harmonizer network dynamically adjusts key parameters of the
activation function. Through a task-specific pre-trained model, INCODE adapts
the task-specific parameters to optimize the representation process. Our
approach not only excels in representation, but also extends its prowess to
tackle complex tasks such as audio, image, and 3D shape reconstructions, as
well as intricate challenges such as neural radiance fields (NeRFs), and
inverse problems, including denoising, super-resolution, inpainting, and CT
reconstruction. Through comprehensive experiments, INCODE demonstrates its
superiority in terms of robustness, accuracy, quality, and convergence rate,
broadening the scope of signal representation. Please visit the project's
website for details on the proposed method and access to the code.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18847" title="Abstract">arXiv:2310.18847</a> [<a href="/pdf/2310.18847" title="Download PDF">pdf</a>, <a href="/format/2310.18847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> World Model Based Sim2Real Transfer for Visual Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lekkala%2C+K">Kiran Lekkala</a>, 
<a href="/search/cs?searchtype=author&query=Itti%2C+L">Laurent Itti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review at the International Conference on Robotics and Automation 2024; Accepted at NeurIPS 2023, Robot Learning Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Sim2Real transfer has gained popularity because it helps transfer from
inexpensive simulators to real world. This paper presents a novel system that
fuses components in a traditional \textit{World Model} into a robust system,
trained entirely within a simulator, that \textit{Zero-Shot} transfers to the
real world. To facilitate transfer, we use an intermediary representation that
are based on \textit{Bird's Eye View (BEV)} images. Thus, our robot learns to
navigate in a simulator by first learning to translate from complex
\textit{First-Person View (FPV)} based RGB images to BEV representations, then
learning to navigate using those representations. Later, when tested in the
real world, the robot uses the perception model that translates FPV-based RGB
images to embeddings that are used by the downstream policy. The incorporation
of state-checking modules using \textit{Anchor images} and \textit{Mixture
Density LSTM} not only interpolates uncertain and missing observations but also
enhances the robustness of the model when exposed to the real-world
environment. We trained the model using data collected using a
\textit{Differential drive} robot in the CARLA simulator. Our methodology's
effectiveness is shown through the deployment of trained models onto a
\textit{Real world Differential drive} robot. Lastly we release a comprehensive
codebase, dataset and models for training and deployment that are available to
the public.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18849" title="Abstract">arXiv:2310.18849</a> [<a href="/pdf/2310.18849" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning-based Compressed Domain Multimedia for Man and Machine: A  Taxonomy and Application to Point Cloud Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seleem%2C+A">Abdelrahman Seleem</a> (1, 2, 4), 
<a href="/search/cs?searchtype=author&query=Guarda%2C+A+F+R">Andr&#xe9; F. R. Guarda</a> (2), 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+N+M+M">Nuno M. M. Rodrigues</a> (2, 3), 
<a href="/search/cs?searchtype=author&query=Pereira%2C+F">Fernando Pereira</a> (1, 2) ((1) Instituto Superior T&#xe9;cnico - Universidade de Lisboa, Lisbon, Portugal, (2) Instituto de Telecomunica&#xe7;&#xf5;es, Portugal, (3) ESTG, Polit&#xe9;cnico de Leiria, Leiria, Portugal, (4) Faculty of Computers and Information, South Valley University, Qena, Egypt)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In the current golden age of multimedia, human visualization is no longer the
single main target, with the final consumer often being a machine which
performs some processing or computer vision tasks. In both cases, deep learning
plays a undamental role in extracting features from the multimedia
representation data, usually producing a compressed representation referred to
as latent representation. The increasing development and adoption of deep
learning-based solutions in a wide area of multimedia applications have opened
an exciting new vision where a common compressed multimedia representation is
used for both man and machine. The main benefits of this vision are two-fold:
i) improved performance for the computer vision tasks, since the effects of
coding artifacts are mitigated; and ii) reduced computational complexity, since
prior decoding is not required. This paper proposes the first taxonomy for
designing compressed domain computer vision solutions driven by the
architecture and weights compatibility with an available spatio-temporal
computer vision processor. The potential of the proposed taxonomy is
demonstrated for the specific case of point cloud classification by designing
novel compressed domain processors using the JPEG Pleno Point Cloud Coding
standard under development and adaptations of the PointGrid classifier.
Experimental results show that the designed compressed domain point cloud
classification solutions can significantly outperform the spatial-temporal
domain classification benchmarks when applied to the decompressed data,
containing coding artifacts, and even surpass their performance when applied to
the original uncompressed data.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18850" title="Abstract">arXiv:2310.18850</a> [<a href="/pdf/2310.18850" title="Download PDF">pdf</a>, <a href="/format/2310.18850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Data Augmentations on Self-/Semi-/Fully- Supervised  Pre-trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+S">Shentong Mo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Data augmentation has become a standard component of vision pre-trained
models to capture the invariance between augmented views. In practice,
augmentation techniques that mask regions of a sample with zero/mean values or
patches from other samples are commonly employed in pre-trained models with
self-/semi-/fully-supervised contrastive losses. However, the underlying
mechanism behind the effectiveness of these augmentation techniques remains
poorly explored. To investigate the problems, we conduct an empirical study to
quantify how data augmentation affects performance. Concretely, we apply 4
types of data augmentations termed with Random Erasing, CutOut, CutMix and
MixUp to a series of self-/semi-/fully- supervised pre-trained models. We
report their performance on vision tasks such as image classification, object
detection, instance segmentation, and semantic segmentation. We then explicitly
evaluate the invariance and diversity of the feature embedding. We observe
that: 1) Masking regions of the images decreases the invariance of the learned
feature embedding while providing a more considerable diversity. 2) Manual
annotations do not change the invariance or diversity of the learned feature
embedding. 3) The MixUp approach improves the diversity significantly, with
only a marginal decrease in terms of the invariance.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18852" title="Abstract">arXiv:2310.18852</a> [<a href="/pdf/2310.18852" title="Download PDF">pdf</a>, <a href="/format/2310.18852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI for Open Science: A Multi-Agent Perspective for Ethically Translating  Data to Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yakaboski%2C+C">Chase Yakaboski</a>, 
<a href="/search/cs?searchtype=author&query=Hyde%2C+G">Gregory Hyde</a>, 
<a href="/search/cs?searchtype=author&query=Nyanhongo%2C+C">Clement Nyanhongo</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+E">Eugene Santos Jr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS AI For Science Workshop 2023. 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">AI for Science (AI4Science), particularly in the form of self-driving labs,
has the potential to sideline human involvement and hinder scientific discovery
within the broader community. While prior research has focused on ensuring the
responsible deployment of AI applications, enhancing security, and ensuring
interpretability, we also propose that promoting openness in AI4Science
discoveries should be carefully considered. In this paper, we introduce the
concept of AI for Open Science (AI4OS) as a multi-agent extension of AI4Science
with the core principle of maximizing open knowledge translation throughout the
scientific enterprise rather than a single organizational unit. We use the
established principles of Knowledge Discovery and Data Mining (KDD) to
formalize a language around AI4OS. We then discuss three principle stages of
knowledge translation embedded in AI4Science systems and detail specific points
where openness can be applied to yield an AI4OS alternative. Lastly, we
formulate a theoretical metric to assess AI4OS with a supporting ethical
argument highlighting its importance. Our goal is that by drawing attention to
AI4OS we can ensure the natural consequence of AI4Science (e.g., self-driving
labs) is a benefit not only for its developers but for society as a whole.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18859" title="Abstract">arXiv:2310.18859</a> [<a href="/pdf/2310.18859" title="Download PDF">pdf</a>, <a href="/format/2310.18859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable  Large Mixture-of-Experts Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhixu Du</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiangyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qilin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yongkai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H+%22">Hai &quot;Helen&quot; Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiran Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Mixture-of-Experts (MoE) has emerged as a favorable architecture in the era
of large models due to its inherent advantage, i.e., enlarging model capacity
without incurring notable computational overhead. Yet, the realization of such
benefits often results in ineffective GPU memory utilization, as large portions
of the model parameters remain dormant during inference. Moreover, the memory
demands of large models consistently outpace the memory capacity of
contemporary GPUs. Addressing this, we introduce SiDA (Sparsity-inspired
Data-Aware), an efficient inference approach tailored for large MoE models.
SiDA judiciously exploits both the system's main memory, which is now abundant
and readily scalable, and GPU memory by capitalizing on the inherent sparsity
on expert activation in MoE models. By adopting a data-aware perspective, SiDA
achieves enhanced model efficiency with a neglectable performance drop.
Specifically, SiDA attains a remarkable speedup in MoE inference with up to
3.93X throughput increasing, up to 75% latency reduction, and up to 80% GPU
memory saving with down to 1% performance drop. This work paves the way for
scalable and efficient deployment of large MoE models, even in
memory-constrained systems.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18861" title="Abstract">arXiv:2310.18861</a> [<a href="/pdf/2310.18861" title="Download PDF">pdf</a>, <a href="/format/2310.18861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Peer-to-Peer Deep Learning for Beyond-5G IoT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pranav%2C+S">Srinivasa Pranav</a>, 
<a href="/search/cs?searchtype=author&query=Moura%2C+J+M+F">Jos&#xe9; M. F. Moura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We present P2PL, a practical multi-device peer-to-peer deep learning
algorithm that, unlike the federated learning paradigm, does not require
coordination from edge servers or the cloud. This makes P2PL well-suited for
the sheer scale of beyond-5G computing environments like smart cities that
otherwise create range, latency, bandwidth, and single point of failure issues
for federated approaches.
<br />P2PL introduces max norm synchronization to catalyze training, retains
on-device deep model training to preserve privacy, and leverages local
inter-device communication to implement distributed consensus. Each device
iteratively alternates between two phases: 1) on-device learning and 2)
distributed cooperation where they combine model parameters with nearby
devices. We empirically show that all participating devices achieve the same
test performance attained by federated and centralized training -- even with
100 devices and relaxed singly stochastic consensus weights. We extend these
experimental results to settings with diverse network topologies, sparse and
intermittent communication, and non-IID data distributions.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18862" title="Abstract">arXiv:2310.18862</a> [<a href="/pdf/2310.18862" title="Download PDF">pdf</a>, <a href="/format/2310.18862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactually Probing Language Identity in Multilingual Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+A">Anirudh Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Govindarajan%2C+V+S">Venkata S Govindarajan</a>, 
<a href="/search/cs?searchtype=author&query=Mahowald%2C+K">Kyle Mahowald</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, MRL Workshop @ EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Techniques in causal analysis of language models illuminate how linguistic
information is organized in LLMs. We use one such technique, AlterRep, a method
of counterfactual probing, to explore the internal structure of multilingual
models (mBERT and XLM-R). We train a linear classifier on a binary language
identity task, to classify tokens between Language X and Language Y. Applying a
counterfactual probing procedure, we use the classifier weights to project the
embeddings into the null space and push the resulting embeddings either in the
direction of Language X or Language Y. Then we evaluate on a masked language
modeling task. We find that, given a template in Language X, pushing towards
Language Y systematically increases the probability of Language Y words, above
and beyond a third-party control language. But it does not specifically push
the model towards translation-equivalent words in Language Y. Pushing towards
Language X (the same direction as the template) has a minimal effect, but
somewhat degrades these models. Overall, we take these results as further
evidence of the rich structure of massive multilingual language models, which
include both a language-specific and language-general component. And we show
that counterfactual probing can be fruitfully applied to multilingual models.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18863" title="Abstract">arXiv:2310.18863</a> [<a href="/pdf/2310.18863" title="Download PDF">pdf</a>, <a href="/format/2310.18863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The diminishing state of shared reality on US television news
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosseinmardi%2C+H">Homa Hosseinmardi</a>, 
<a href="/search/cs?searchtype=author&query=Wolken%2C+S">Samuel Wolken</a>, 
<a href="/search/cs?searchtype=author&query=Rothschild%2C+D+M">David M. Rothschild</a>, 
<a href="/search/cs?searchtype=author&query=Watts%2C+D+J">Duncan J. Watts</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computers and Society (cs.CY); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">The potential for a large, diverse population to coexist peacefully is
thought to depend on the existence of a ``shared reality:'' a public sphere in
which participants are exposed to similar facts about similar topics. A
generation ago, broadcast television news was widely considered to serve this
function; however, since the rise of cable news in the 1990s, critics and
scholars have worried that the corresponding fragmentation and segregation of
audiences along partisan lines has caused this shared reality to be lost. Here
we examine this concern using a unique combination of data sets tracking the
production (since 2012) and consumption (since 2016) of television news content
on the three largest cable and broadcast networks respectively. With regard to
production, we find strong evidence for the ``loss of shared reality
hypothesis:'' while broadcast continues to cover similar topics with similar
language, cable news networks have become increasingly distinct, both from
broadcast news and each other, diverging both in terms of content and language.
With regard to consumption, we find more mixed evidence: while broadcast news
has indeed declined in popularity, it remains the dominant source of news for
roughly 50\% more Americans than does cable; moreover, its decline, while
somewhat attributable to cable, appears driven more by a shift away from news
consumption altogether than a growth in cable consumption. We conclude that
shared reality on US television news is indeed diminishing, but is more robust
than previously thought and is declining for somewhat different reasons.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18865" title="Abstract">arXiv:2310.18865</a> [<a href="/pdf/2310.18865" title="Download PDF">pdf</a>, <a href="/format/2310.18865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUST: A Multilingual Student-Teacher Learning approach for low-resource  speech recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farooq%2C+M+U">Muhammad Umar Farooq</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+R">Rehan Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Hain%2C+T">Thomas Hain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for IEEE ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Student-teacher learning or knowledge distillation (KD) has been previously
used to address data scarcity issue for training of speech recognition (ASR)
systems. However, a limitation of KD training is that the student model classes
must be a proper or improper subset of the teacher model classes. It prevents
distillation from even acoustically similar languages if the character sets are
not same. In this work, the aforementioned limitation is addressed by proposing
a MUltilingual Student-Teacher (MUST) learning which exploits a posteriors
mapping approach. A pre-trained mapping model is used to map posteriors from a
teacher language to the student language ASR. These mapped posteriors are used
as soft labels for KD learning. Various teacher ensemble schemes are
experimented to train an ASR model for low-resource languages. A model trained
with MUST learning reduces relative character error rate (CER) up to 9.5% in
comparison with a baseline monolingual ASR.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18867" title="Abstract">arXiv:2310.18867</a> [<a href="/pdf/2310.18867" title="Download PDF">pdf</a>, <a href="/format/2310.18867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-Engineering and Transformer-based Question Generation and  Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amyeen%2C+R">Rubaba Amyeen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Question generation has numerous applications in the educational context.
Question generation can prove helpful for students when reviewing content and
testing themselves. Furthermore, a question generation model can aid teachers
by lessening the burden of creating assessments and other practice material.
This paper aims to find the best method to generate questions from textual data
through a transformer model and prompt engineering. In this research, we
finetuned a pretrained distilBERT model on the SQuAD question answering dataset
to generate questions. In addition to training a transformer model, prompt
engineering was applied to generate questions effectively using the LLaMA
model. The generated questions were compared against the baseline questions in
the SQuAD dataset to evaluate the effectiveness of four different prompts. All
four prompts demonstrated over 60% similarity on average. Of the
prompt-generated questions, 30% achieved a high similarity score greater than
70%.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18868" title="Abstract">arXiv:2310.18868</a> [<a href="/pdf/2310.18868" title="Download PDF">pdf</a>, <a href="/format/2310.18868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlation Aware Sparsified Mean Estimation Using Random Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shuli Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+P">Pranay Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+G">Gauri Joshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 13 figures. Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023), New Orleans, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the problem of communication-efficient distributed vector mean
estimation, a commonly used subroutine in distributed optimization and
Federated Learning (FL). Rand-$k$ sparsification is a commonly used technique
to reduce communication cost, where each client sends $k &lt; d$ of its
coordinates to the server. However, Rand-$k$ is agnostic to any correlations,
that might exist between clients in practical scenarios. The recently proposed
Rand-$k$-Spatial estimator leverages the cross-client correlation information
at the server to improve Rand-$k$'s performance. Yet, the performance of
Rand-$k$-Spatial is suboptimal. We propose the Rand-Proj-Spatial estimator with
a more flexible encoding-decoding procedure, which generalizes the encoding of
Rand-$k$ by projecting the client vectors to a random $k$-dimensional subspace.
We utilize Subsampled Randomized Hadamard Transform (SRHT) as the projection
matrix and show that Rand-Proj-Spatial with SRHT outperforms Rand-$k$-Spatial,
using the correlation information more efficiently. Furthermore, we propose an
approach to incorporate varying degrees of correlation and suggest a practical
variant of Rand-Proj-Spatial when the correlation information is not available
to the server. Experiments on real-world distributed optimization tasks
showcase the superior performance of Rand-Proj-Spatial compared to
Rand-$k$-Spatial and other more sophisticated sparsification techniques.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18874" title="Abstract">arXiv:2310.18874</a> [<a href="/pdf/2310.18874" title="Download PDF">pdf</a>, <a href="/format/2310.18874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HDMNet: A Hierarchical Matching Network with Double Attention for  Large-scale Outdoor LiDAR Point Cloud Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+W">Weiyi Xue</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+F">Fan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Outdoor LiDAR point clouds are typically large-scale and complexly
distributed. To achieve efficient and accurate registration, emphasizing the
similarity among local regions and prioritizing global local-to-local matching
is of utmost importance, subsequent to which accuracy can be enhanced through
cost-effective fine registration. In this paper, a novel hierarchical neural
network with double attention named HDMNet is proposed for large-scale outdoor
LiDAR point cloud registration. Specifically, A novel feature consistency
enhanced double-soft matching network is introduced to achieve two-stage
matching with high flexibility while enlarging the receptive field with high
efficiency in a patch-to patch manner, which significantly improves the
registration performance. Moreover, in order to further utilize the sparse
matching information from deeper layer, we develop a novel trainable embedding
mask to incorporate the confidence scores of correspondences obtained from pose
estimation of deeper layer, eliminating additional computations. The
high-confidence keypoints in the sparser point cloud of the deeper layer
correspond to a high-confidence spatial neighborhood region in shallower layer,
which will receive more attention, while the features of non-key regions will
be masked. Extensive experiments are conducted on two large-scale outdoor LiDAR
point cloud datasets to demonstrate the high accuracy and efficiency of the
proposed HDMNet.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18877" title="Abstract">arXiv:2310.18877</a> [<a href="/pdf/2310.18877" title="Download PDF">pdf</a>, <a href="/format/2310.18877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-trained Speech Processing Models Contain Human-Like Biases that  Propagate to Speech Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Slaughter%2C+I">Isaac Slaughter</a>, 
<a href="/search/cs?searchtype=author&query=Greenberg%2C+C">Craig Greenberg</a>, 
<a href="/search/cs?searchtype=author&query=Schwartz%2C+R">Reva Schwartz</a>, 
<a href="/search/cs?searchtype=author&query=Caliskan%2C+A">Aylin Caliskan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Previous work has established that a person's demographics and speech style
affect how well speech processing models perform for them. But where does this
bias come from? In this work, we present the Speech Embedding Association Test
(SpEAT), a method for detecting bias in one type of model used for many speech
tasks: pre-trained models. The SpEAT is inspired by word embedding association
tests in natural language processing, which quantify intrinsic bias in a
model's representations of different concepts, such as race or valence
(something's pleasantness or unpleasantness) and capture the extent to which a
model trained on large-scale socio-cultural data has learned human-like biases.
Using the SpEAT, we test for six types of bias in 16 English speech models
(including 4 models also trained on multilingual data), which come from the
wav2vec 2.0, HuBERT, WavLM, and Whisper model families. We find that 14 or more
models reveal positive valence (pleasantness) associations with abled people
over disabled people, with European-Americans over African-Americans, with
females over males, with U.S. accented speakers over non-U.S. accented
speakers, and with younger people over older people. Beyond establishing that
pre-trained speech models contain these biases, we also show that they can have
real world effects. We compare biases found in pre-trained models to biases in
downstream models adapted to the task of Speech Emotion Recognition (SER) and
find that in 66 of the 96 tests performed (69%), the group that is more
associated with positive valence as indicated by the SpEAT also tends to be
predicted as speaking with higher valence by the downstream model. Our work
provides evidence that, like text and image-based models, pre-trained speech
based-models frequently learn human-like biases. Our work also shows that bias
found in pre-trained models can propagate to the downstream task of SER.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18882" title="Abstract">arXiv:2310.18882</a> [<a href="/pdf/2310.18882" title="Download PDF">pdf</a>, <a href="/format/2310.18882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Learning of Generalized Structured Matrices for Efficient  Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Changwoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hun-Seok Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper investigates efficient deep neural networks (DNNs) to replace
dense unstructured weight matrices with structured ones that possess desired
properties. The challenge arises because the optimal weight matrix structure in
popular neural network models is obscure in most cases and may vary from layer
to layer even in the same network. Prior structured matrices proposed for
efficient DNNs were mostly hand-crafted without a generalized framework to
systematically learn them. To address this issue, we propose a generalized and
differentiable framework to learn efficient structures of weight matrices by
gradient descent. We first define a new class of structured matrices that
covers a wide range of structured matrices in the literature by adjusting the
structural parameters. Then, the frequency-domain differentiable
parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to
learn the structural parameters by proximal gradient descent. Finally, we
introduce an effective initialization method for the proposed scheme. Our
method learns efficient DNNs with structured matrices, achieving lower
complexity and/or higher performance than prior approaches that employ
low-rank, block-sparse, or block-low-rank matrices.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18884" title="Abstract">arXiv:2310.18884</a> [<a href="/pdf/2310.18884" title="Download PDF">pdf</a>, <a href="/format/2310.18884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple and Asymmetric Graph Contrastive Learning without Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Teng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Huaisheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Graph Contrastive Learning (GCL) has shown superior performance in
representation learning in graph-structured data. Despite their success, most
existing GCL methods rely on prefabricated graph augmentation and homophily
assumptions. Thus, they fail to generalize well to heterophilic graphs where
connected nodes may have different class labels and dissimilar features. In
this paper, we study the problem of conducting contrastive learning on
homophilic and heterophilic graphs. We find that we can achieve promising
performance simply by considering an asymmetric view of the neighboring nodes.
The resulting simple algorithm, Asymmetric Contrastive Learning for Graphs
(GraphACL), is easy to implement and does not rely on graph augmentations and
homophily assumptions. We provide theoretical and empirical evidence that
GraphACL can capture one-hop local neighborhood information and two-hop
monophily similarity, which are both important for modeling heterophilic
graphs. Experimental results show that the simple GraphACL significantly
outperforms state-of-the-art graph contrastive learning and self-supervised
learning methods on homophilic and heterophilic graphs. The code of GraphACL is
available at https://github.com/tengxiao1/GraphACL.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18885" title="Abstract">arXiv:2310.18885</a> [<a href="/pdf/2310.18885" title="Download PDF">pdf</a>, <a href="/format/2310.18885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A foundational neural operator that continuously learns without  forgetting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripura%2C+T">Tapas Tripura</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souvik Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine learning has witnessed substantial growth, leading to the development
of advanced artificial intelligence models crafted to address a wide range of
real-world challenges spanning various domains, such as computer vision,
natural language processing, and scientific computing. Nevertheless, the
creation of custom models for each new task remains a resource-intensive
undertaking, demanding considerable computational time and memory resources. In
this study, we introduce the concept of the Neural Combinatorial Wavelet Neural
Operator (NCWNO) as a foundational model for scientific computing. This model
is specifically designed to excel in learning from a diverse spectrum of
physics and continuously adapt to the solution operators associated with
parametric partial differential equations (PDEs). The NCWNO leverages a gated
structure that employs local wavelet experts to acquire shared features across
multiple physical systems, complemented by a memory-based ensembling approach
among these local wavelet experts. This combination enables rapid adaptation to
new challenges. The proposed foundational model offers two key advantages: (i)
it can simultaneously learn solution operators for multiple parametric PDEs,
and (ii) it can swiftly generalize to new parametric PDEs with minimal
fine-tuning. The proposed NCWNO is the first foundational operator learning
algorithm distinguished by its (i) robustness against catastrophic forgetting,
(ii) the maintenance of positive transfer for new parametric PDEs, and (iii)
the facilitation of knowledge transfer across dissimilar tasks. Through an
extensive set of benchmark examples, we demonstrate that the NCWNO can
outperform task-specific baseline operator learning frameworks with minimal
hyperparameter tuning at the prediction stage. We also show that with minimal
fine-tuning, the NCWNO performs accurate combinatorial learning of new
parametric PDEs.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18887" title="Abstract">arXiv:2310.18887</a> [<a href="/pdf/2310.18887" title="Download PDF">pdf</a>, <a href="/format/2310.18887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yihong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hariharan%2C+B">Bharath Hariharan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised monocular depth estimation techniques have demonstrated
encouraging results but typically assume that the scene is static. These
techniques suffer when trained on dynamical scenes, where apparent object
motion can equally be explained by hypothesizing the object's independent
motion, or by altering its depth. This ambiguity causes depth estimators to
predict erroneous depth for moving objects. To resolve this issue, we introduce
Dynamo-Depth, an unifying approach that disambiguates dynamical motion by
jointly learning monocular depth, 3D independent flow field, and motion
segmentation from unlabeled monocular videos. Specifically, we offer our key
insight that a good initial estimation of motion segmentation is sufficient for
jointly learning depth and independent motion despite the fundamental
underlying ambiguity. Our proposed method achieves state-of-the-art performance
on monocular depth estimation on Waymo Open and nuScenes Dataset with
significant improvement in the depth of moving objects. Code and additional
results are available at https://dynamo-depth.github.io.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18888" title="Abstract">arXiv:2310.18888</a> [<a href="/pdf/2310.18888" title="Download PDF">pdf</a>, <a href="/format/2310.18888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D2NO: Efficient Handling of Heterogeneous Input Function Spaces with  Distributed Deep Neural Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zecheng Zhang</a>, 
<a href="/search/math?searchtype=author&query=Moya%2C+C">Christian Moya</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+G">Guang Lin</a>, 
<a href="/search/math?searchtype=author&query=Schaeffer%2C+H">Hayden Schaeffer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural operators have been applied in various scientific fields, such as
solving parametric partial differential equations, dynamical systems with
control, and inverse problems. However, challenges arise when dealing with
input functions that exhibit heterogeneous properties, requiring multiple
sensors to handle functions with minimal regularity. To address this issue,
discretization-invariant neural operators have been used, allowing the sampling
of diverse input functions with different sensor locations. However, existing
frameworks still require an equal number of sensors for all functions. In our
study, we propose a novel distributed approach to further relax the
discretization requirements and solve the heterogeneous dataset challenges. Our
method involves partitioning the input function space and processing individual
input functions using independent and separate neural networks. A centralized
neural network is used to handle shared information across all output
functions. This distributed methodology reduces the number of gradient descent
back-propagation steps, improving efficiency while maintaining accuracy. We
demonstrate that the corresponding neural network is a universal approximator
of continuous nonlinear operators and present four numerical examples to
validate its performance.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18890" title="Abstract">arXiv:2310.18890</a> [<a href="/pdf/2310.18890" title="Download PDF">pdf</a>, <a href="/format/2310.18890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Generalized Multi-stage Clustering: Multi-view Self-distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiatai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing multi-stage clustering methods independently learn the salient
features from multiple views and then perform the clustering task.
Particularly, multi-view clustering (MVC) has attracted a lot of attention in
multi-view or multi-modal scenarios. MVC aims at exploring common semantics and
pseudo-labels from multiple views and clustering in a self-supervised manner.
However, limited by noisy data and inadequate feature learning, such a
clustering paradigm generates overconfident pseudo-labels that mis-guide the
model to produce inaccurate predictions. Therefore, it is desirable to have a
method that can correct this pseudo-label mistraction in multi-stage clustering
to avoid the bias accumulation. To alleviate the effect of overconfident
pseudo-labels and improve the generalization ability of the model, this paper
proposes a novel multi-stage deep MVC framework where multi-view
self-distillation (DistilMVC) is introduced to distill dark knowledge of label
distribution. Specifically, in the feature subspace at different hierarchies,
we explore the common semantics of multiple views through contrastive learning
and obtain pseudo-labels by maximizing the mutual information between views.
Additionally, a teacher network is responsible for distilling pseudo-labels
into dark knowledge, supervising the student network and improving its
predictive capabilities to enhance the robustness. Extensive experiments on
real-world multi-view datasets show that our method has better clustering
performance than state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18891" title="Abstract">arXiv:2310.18891</a> [<a href="/pdf/2310.18891" title="Download PDF">pdf</a>, <a href="/format/2310.18891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Interaction-Aware Dynamical Models and Decision Making for  Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crosato%2C+L">Luca Crosato</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+K">Kai Tian</a>, 
<a href="/search/cs?searchtype=author&query=Shum%2C+H+P+H">Hubert P. H Shum</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+E+S+L">Edmond S. L. Ho</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yafei Wang</a>, 
<a href="/search/cs?searchtype=author&query=We%2C+C">Chongfeng We</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">Interaction-aware Autonomous Driving (IAAD) is a rapidly growing field of
research that focuses on the development of autonomous vehicles (AVs) that are
capable of interacting safely and efficiently with human road users. This is a
challenging task, as it requires the autonomous vehicle to be able to
understand and predict the behaviour of human road users. In this literature
review, the current state of IAAD research is surveyed in this work. Commencing
with an examination of terminology, attention is drawn to challenges and
existing models employed for modelling the behaviour of drivers and
pedestrians. Next, a comprehensive review is conducted on various techniques
proposed for interaction modelling, encompassing cognitive methods, machine
learning approaches, and game-theoretic methods. The conclusion is reached
through a discussion of potential advantages and risks associated with IAAD,
along with the illumination of pivotal research inquiries necessitating future
exploration.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18893" title="Abstract">arXiv:2310.18893</a> [<a href="/pdf/2310.18893" title="Download PDF">pdf</a>, <a href="/format/2310.18893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ever Evolving Evaluator (EV3): Towards Flexible and Reliable  Meta-Optimization for Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Li Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zoghi%2C+M">Masrour Zoghi</a>, 
<a href="/search/cs?searchtype=author&query=Tennenholtz%2C+G">Guy Tennenholtz</a>, 
<a href="/search/cs?searchtype=author&query=Karimzadehgan%2C+M">Maryam Karimzadehgan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World (RealML-2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">We introduce EV3, a novel meta-optimization framework designed to efficiently
train scalable machine learning models through an intuitive
explore-assess-adapt protocol. In each iteration of EV3, we explore various
model parameter updates, assess them using pertinent evaluation methods, and
adapt the model based on the optimal updates and previous progress history. EV3
offers substantial flexibility without imposing stringent constraints like
differentiability on the key objectives relevant to the tasks of interest.
Moreover, this protocol welcomes updates with biased gradients and allows for
the use of a diversity of losses and optimizers. Additionally, in scenarios
with multiple objectives, it can be used to dynamically prioritize tasks. With
inspiration drawn from evolutionary algorithms, meta-learning, and neural
architecture search, we investigate an application of EV3 to knowledge
distillation. Our experimental results illustrate EV3's capability to safely
explore model spaces, while hinting at its potential applicability across
numerous domains due to its inherent flexibility and adaptability.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18894" title="Abstract">arXiv:2310.18894</a> [<a href="/pdf/2310.18894" title="Download PDF">pdf</a>, <a href="/format/2310.18894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergence of Shape Bias in Convolutional Neural Networks through  Activation Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianqin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Ziqi Wen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangfan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T+S">Tai Sing Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as NeurIPS 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Current deep-learning models for object recognition are known to be heavily
biased toward texture. In contrast, human visual systems are known to be biased
toward shape and structure. What could be the design principles in human visual
systems that led to this difference? How could we introduce more shape bias
into the deep learning models? In this paper, we report that sparse coding, a
ubiquitous principle in the brain, can in itself introduce shape bias into the
network. We found that enforcing the sparse coding constraint using a
non-differential Top-K operation can lead to the emergence of structural
encoding in neurons in convolutional neural networks, resulting in a smooth
decomposition of objects into parts and subparts and endowing the networks with
shape bias. We demonstrated this emergence of shape bias and its functional
benefits for different network structures with various datasets. For object
recognition convolutional neural networks, the shape bias leads to greater
robustness against style and pattern change distraction. For the image
synthesis generative adversary networks, the emerged shape bias leads to more
coherent and decomposable structures in the synthesized images. Ablation
studies suggest that sparse codes tend to encode structures, whereas the more
distributed codes tend to favor texture. Our code is host at the github
repository: \url{https://github.com/Crazy-Jack/nips2023_shape_vs_texture}
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18895" title="Abstract">arXiv:2310.18895</a> [<a href="/pdf/2310.18895" title="Download PDF">pdf</a>, <a href="/format/2310.18895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Task-Specific Timeliness With Edge-Assisted Scheduling for  Status Update
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingzhou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lehan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+Z">Zhaojun Nan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuxuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhisheng Niu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication as a Special Issue: The Role of Freshness and Semantic Measures in the Transmission of Information for Next Generation Networks paper in the IEEE Journal on Selected Areas in Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Intelligent real-time applications, such as video surveillance, demand
intensive computation to extract status information from raw sensing data. This
poses a substantial challenge in orchestrating computation and communication
resources to provide fresh status information. In this paper, we consider a
scenario where multiple energy-constrained devices served by an edge server. To
extract status information, each device can either do the computation locally
or offload it to the edge server. A scheduling policy is needed to determine
when and where to compute for each device, taking into account communication
and computation capabilities, as well as task-specific timeliness requirements.
To that end, we first model the timeliness requirements as general penalty
functions of Age of Information (AoI). A convex optimization problem is
formulated to provide a lower bound of the minimum AoI penalty given system
parameters. Using KKT conditions, we proposed a novel scheduling policy which
evaluates status update priorities based on communication and computation
delays and task-specific timeliness requirements. The proposed policy is
applied to an object tracking application and carried out on a large video
dataset. Simulation results show that our policy improves tracking accuracy
compared with scheduling policies based on video content information.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18899" title="Abstract">arXiv:2310.18899</a> [<a href="/pdf/2310.18899" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-task deep learning for large-scale building detail extraction from  high-resolution satellite imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhuo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qingsong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jinzhao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhiwei Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhixin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Understanding urban dynamics and promoting sustainable development requires
comprehensive insights about buildings. While geospatial artificial
intelligence has advanced the extraction of such details from Earth
observational data, existing methods often suffer from computational
inefficiencies and inconsistencies when compiling unified building-related
datasets for practical applications. To bridge this gap, we introduce the
Multi-task Building Refiner (MT-BR), an adaptable neural network tailored for
simultaneous extraction of spatial and attributional building details from
high-resolution satellite imagery, exemplified by building rooftops, urban
functional types, and roof architectural types. Notably, MT-BR can be
fine-tuned to incorporate additional building details, extending its
applicability. For large-scale applications, we devise a novel spatial sampling
scheme that strategically selects limited but representative image samples.
This process optimizes both the spatial distribution of samples and the urban
environmental characteristics they contain, thus enhancing extraction
effectiveness while curtailing data preparation expenditures. We further
enhance MT-BR's predictive performance and generalization capabilities through
the integration of advanced augmentation techniques. Our quantitative results
highlight the efficacy of the proposed methods. Specifically, networks trained
with datasets curated via our sampling method demonstrate improved predictive
accuracy relative to those using alternative sampling approaches, with no
alterations to network architecture. Moreover, MT-BR consistently outperforms
other state-of-the-art methods in extracting building details across various
metrics. The real-world practicality is also demonstrated in an application
across Shanghai, generating a unified dataset that encompasses both the spatial
and attributional details of buildings.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18902" title="Abstract">arXiv:2310.18902</a> [<a href="/pdf/2310.18902" title="Download PDF">pdf</a>, <a href="/format/2310.18902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NetPanorama: A Declarative Grammar for Network Construction,  Transformation, and Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scott-Brown%2C+J">James Scott-Brown</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+B">Benjamin Bach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This paper introduces NetPanorama, a domain-specific language and declarative
grammar for interactive network visualizations. Exploring complex networks with
multivariate, geographical, or temporal information often require bespoke
visualization designs, such as adjacency matrices, arc-diagrams, small
multiples, timelines, or geographic map visualizations. However, creating these
requires implementing data loading, data transformations, visualization, and
interactivity, which is time-consuming and slows down the iterative exploration
of this huge design space. With NetPanorama, a developer specifies a network
visualization design as a pipeline of parameterizable steps. Our specification
and reference implementation aims to facilitate visualization development and
reuse; allow for easy design exploration and iteration; and make data
transformation and visual mapping decisions transparent. Documentation, source
code, examples, and an interactive online editor can be found online:
https://netpanorama.netlify.app/
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18904" title="Abstract">arXiv:2310.18904</a> [<a href="/pdf/2310.18904" title="Download PDF">pdf</a>, <a href="/format/2310.18904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifiable Contrastive Learning with Automatic Feature Importance  Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yisen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing contrastive learning methods rely on pairwise sample contrast
$z_x^\top z_{x'}$ to learn data representations, but the learned features often
lack clear interpretability from a human perspective. Theoretically, it lacks
feature identifiability and different initialization may lead to totally
different features. In this paper, we study a new method named tri-factor
contrastive learning (triCL) that involves a 3-factor contrast in the form of
$z_x^\top S z_{x'}$, where $S=\text{diag}(s_1,\dots,s_k)$ is a learnable
diagonal matrix that automatically captures the importance of each feature. We
show that by this simple extension, triCL can not only obtain identifiable
features that eliminate randomness but also obtain more interpretable features
that are ordered according to the importance matrix $S$. We show that features
with high importance have nice interpretability by capturing common classwise
features, and obtain superior performance when evaluated for image retrieval
using a few features. The proposed triCL objective is general and can be
applied to different contrastive learning methods like SimCLR and CLIP. We
believe that it is a better alternative to existing 2-factor contrastive
learning by improving its identifiability and interpretability with minimal
overhead. Code is available at
https://github.com/PKU-ML/Tri-factor-Contrastive-Learning.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18906" title="Abstract">arXiv:2310.18906</a> [<a href="/pdf/2310.18906" title="Download PDF">pdf</a>, <a href="/format/2310.18906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stacking the Odds: Transformer-Based Ensemble for AI-Generated Text  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Duke Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Naing%2C+K+M+N">Khaing Myat Noe Naing</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+A">Aditya Joshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an ALTA 2023 Shared Task Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper reports our submission under the team name `SynthDetectives' to
the ALTA 2023 Shared Task. We use a stacking ensemble of Transformers for the
task of AI-generated text detection. Our approach is novel in terms of its
choice of models in that we use accessible and lightweight models in the
ensemble. We show that ensembling the models results in an improved accuracy in
comparison with using them individually. Our approach achieves an accuracy
score of 0.9555 on the official test data provided by the shared task
organisers.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18908" title="Abstract">arXiv:2310.18908</a> [<a href="/pdf/2310.18908" title="Download PDF">pdf</a>, <a href="/format/2310.18908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating the Rate-Distortion Function by Wasserstein Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yibo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Eckstein%2C+S">Stephan Eckstein</a>, 
<a href="/search/cs?searchtype=author&query=Nutz%2C+M">Marcel Nutz</a>, 
<a href="/search/cs?searchtype=author&query=Mandt%2C+S">Stephan Mandt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as conference paper at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">In the theory of lossy compression, the rate-distortion (R-D) function $R(D)$
describes how much a data source can be compressed (in bit-rate) at any given
level of fidelity (distortion). Obtaining $R(D)$ for a given data source
establishes the fundamental performance limit for all compression algorithms.
We propose a new method to estimate $R(D)$ from the perspective of optimal
transport. Unlike the classic Blahut--Arimoto algorithm which fixes the support
of the reproduction distribution in advance, our Wasserstein gradient descent
algorithm learns the support of the optimal reproduction distribution by moving
particles. We prove its local convergence and analyze the sample complexity of
our R-D estimator based on a connection to entropic optimal transport.
Experimentally, we obtain comparable or tighter bounds than state-of-the-art
neural network methods on low-rate sources while requiring considerably less
tuning and computation effort. We also highlight a connection to
maximum-likelihood deconvolution and introduce a new class of sources that can
be used as test cases with known solutions to the R-D problem.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18910" title="Abstract">arXiv:2310.18910</a> [<a href="/pdf/2310.18910" title="Download PDF">pdf</a>, <a href="/format/2310.18910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstanT: Semi-supervised Learning with Instance-dependent Thresholds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Muyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Runze Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as poster for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Semi-supervised learning (SSL) has been a fundamental challenge in machine
learning for decades. The primary family of SSL algorithms, known as
pseudo-labeling, involves assigning pseudo-labels to confident unlabeled
instances and incorporating them into the training set. Therefore, the
selection criteria of confident instances are crucial to the success of SSL.
Recently, there has been growing interest in the development of SSL methods
that use dynamic or adaptive thresholds. Yet, these methods typically apply the
same threshold to all samples, or use class-dependent thresholds for instances
belonging to a certain class, while neglecting instance-level information. In
this paper, we propose the study of instance-dependent thresholds, which has
the highest degree of freedom compared with existing methods. Specifically, we
devise a novel instance-dependent threshold function for all unlabeled
instances by utilizing their instance-level ambiguity and the
instance-dependent error rates of pseudo-labels, so instances that are more
likely to have incorrect pseudo-labels will have higher thresholds.
Furthermore, we demonstrate that our instance-dependent threshold function
provides a bounded probabilistic guarantee for the correctness of the
pseudo-labels it assigns.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18911" title="Abstract">arXiv:2310.18911</a> [<a href="/pdf/2310.18911" title="Download PDF">pdf</a>, <a href="/ps/2310.18911" title="Download PostScript">ps</a>, <a href="/format/2310.18911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering Gender Bias within Journalist-Politician Interaction in  Indian Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+B">Brisha Jain</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+M">Mainack Mondal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Gender bias in political discourse is a significant problem on today's social
media. Previous studies found that the gender of politicians indeed influences
the content directed towards them by the general public. However, these works
are particularly focused on the global north, which represents individualistic
culture. Furthermore, they did not address whether there is gender bias even
within the interaction between popular journalists and politicians in the
global south. These understudied journalist-politician interactions are
important (more so in collectivistic cultures like the global south) as they
can significantly affect public sentiment and help set gender-biased social
norms. In this work, using large-scale data from Indian Twitter we address this
research gap.
<br />We curated a gender-balanced set of 100 most-followed Indian journalists on
Twitter and 100 most-followed politicians. Then we collected 21,188 unique
tweets posted by these journalists that mentioned these politicians. Our
analysis revealed that there is a significant gender bias -- the frequency with
which journalists mention male politicians vs. how frequently they mention
female politicians is statistically significantly different ($p&lt;&lt;0.05$). In
fact, median tweets from female journalists mentioning female politicians
received ten times fewer likes than median tweets from female journalists
mentioning male politicians. However, when we analyzed tweet content, our
emotion score analysis and topic modeling analysis did not reveal any
significant gender-based difference within the journalists' tweets towards
politicians. Finally, we found a potential reason for the significant gender
bias: the number of popular male Indian politicians is almost twice as large as
the number of popular female Indian politicians, which might have resulted in
the observed bias. We conclude by discussing the implications of this work.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18912" title="Abstract">arXiv:2310.18912</a> [<a href="/pdf/2310.18912" title="Download PDF">pdf</a>, <a href="/format/2310.18912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentence Bag Graph Formulation for Biomedical Distant Supervision  Relation Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+T">Tianming Liang</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+G">Gaurav Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Liang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Maozu Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We introduce a novel graph-based framework for alleviating key challenges in
distantly-supervised relation extraction and demonstrate its effectiveness in
the challenging and important domain of biomedical data. Specifically, we
propose a graph view of sentence bags referring to an entity pair, which
enables message-passing based aggregation of information related to the entity
pair over the sentence bag. The proposed framework alleviates the common
problem of noisy labeling in distantly supervised relation extraction and also
effectively incorporates inter-dependencies between sentences within a bag.
Extensive experiments on two large-scale biomedical relation datasets and the
widely utilized NYT dataset demonstrate that our proposed framework
significantly outperforms the state-of-the-art methods for biomedical distant
supervision relation extraction while also providing excellent performance for
relation extraction in the general text mining domain.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18913" title="Abstract">arXiv:2310.18913</a> [<a href="/pdf/2310.18913" title="Download PDF">pdf</a>, <a href="/format/2310.18913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiasing Algorithm through Model Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Limisiewicz%2C+T">Tomasz Limisiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Mare%C4%8Dek%2C+D">David Mare&#x10d;ek</a>, 
<a href="/search/cs?searchtype=author&query=Musil%2C+T">Tom&#xe1;&#x161; Musil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Large language models are becoming the go-to solution for various language
tasks. However, with growing capacity, models are prone to rely on spurious
correlations stemming from biases and stereotypes present in the training data.
This work proposes a novel method for detecting and mitigating gender bias in
language models. We perform causal analysis to identify problematic model
components and discover that mid-upper feed-forward layers are most prone to
convey biases. Based on the analysis results, we adapt the model by multiplying
these layers by a linear projection. Our titular method, DAMA, significantly
decreases bias as measured by diverse metrics while maintaining the model's
performance on downstream tasks. We release code for our method and models,
which retrain LLaMA's state-of-the-art performance while being significantly
less biased.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18917" title="Abstract">arXiv:2310.18917</a> [<a href="/pdf/2310.18917" title="Download PDF">pdf</a>, <a href="/format/2310.18917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TiV-NeRF: Tracking and Mapping via Time-Varying Representation with  Dynamic Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+C">Chengyao Duan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiliu Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Previous attempts to integrate Neural Radiance Fields (NeRF) into
Simultaneous Localization and Mapping (SLAM) framework either rely on the
assumption of static scenes or treat dynamic objects as outliers. However, most
of real-world scenarios is dynamic. In this paper, we propose a time-varying
representation to track and reconstruct the dynamic scenes. Our system
simultaneously maintains two processes, tracking process and mapping process.
For tracking process, the entire input images are uniformly sampled and
training of the RGB images are self-supervised. For mapping process, we
leverage know masks to differentiate dynamic objects and static backgrounds,
and we apply distinct sampling strategies for two types of areas. The
parameters optimization for both processes are made up by two stages, the first
stage associates time with 3D positions to convert the deformation field to the
canonical field. And the second associates time with 3D positions in canonical
field to obtain colors and Signed Distance Function (SDF). Besides, We propose
a novel keyframe selection strategy based on the overlapping rate. We evaluate
our approach on two publicly available synthetic datasets and validate that our
method is more effective compared to current state-of-the-art dynamic mapping
methods.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18918" title="Abstract">arXiv:2310.18918</a> [<a href="/pdf/2310.18918" title="Download PDF">pdf</a>, <a href="/format/2310.18918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperbolic Graph Neural Networks at Scale: A Meta Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+N">Nurendra Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+N">Nikhil Rao</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+C+K">Chandan K. Reddy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023. 14 pages of main paper, 5 pages of supplementary
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The progress in hyperbolic neural networks (HNNs) research is hindered by
their absence of inductive bias mechanisms, which are essential for
generalizing to new tasks and facilitating scalable learning over large
datasets. In this paper, we aim to alleviate these issues by learning
generalizable inductive biases from the nodes' local subgraph and transfer them
for faster learning over new subgraphs with a disjoint set of nodes, edges, and
labels in a few-shot setting. We introduce a novel method, Hyperbolic GRAph
Meta Learner (H-GRAM), that, for the tasks of node classification and link
prediction, learns transferable information from a set of support local
subgraphs in the form of hyperbolic meta gradients and label hyperbolic
protonets to enable faster learning over a query set of new tasks dealing with
disjoint subgraphs. Furthermore, we show that an extension of our meta-learning
framework also mitigates the scalability challenges seen in HNNs faced by
existing approaches. Our comparative analysis shows that H-GRAM effectively
learns and transfers information in multiple challenging few-shot settings
compared to other state-of-the-art baselines. Additionally, we demonstrate
that, unlike standard HNNs, our approach is able to scale over large graph
datasets and improve performance over its Euclidean counterparts.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18919" title="Abstract">arXiv:2310.18919</a> [<a href="/pdf/2310.18919" title="Download PDF">pdf</a>, <a href="/format/2310.18919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posterior Sampling with Delayed Feedback for Reinforcement Learning with  Linear Function Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+N+L">Nikki Lijing Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+M">Ming Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi-An Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent studies in reinforcement learning (RL) have made significant progress
by leveraging function approximation to alleviate the sample complexity hurdle
for better performance. Despite the success, existing provably efficient
algorithms typically rely on the accessibility of immediate feedback upon
taking actions. The failure to account for the impact of delay in observations
can significantly degrade the performance of real-world systems due to the
regret blow-up. In this work, we tackle the challenge of delayed feedback in RL
with linear function approximation by employing posterior sampling, which has
been shown to empirically outperform the popular UCB algorithms in a wide range
of regimes. We first introduce Delayed-PSVI, an optimistic value-based
algorithm that effectively explores the value function space via noise
perturbation with posterior sampling. We provide the first analysis for
posterior sampling algorithms with delayed feedback in RL and show our
algorithm achieves $\widetilde{O}(\sqrt{d^3H^3 T} + d^2H^2 E[\tau])$ worst-case
regret in the presence of unknown stochastic delays. Here $E[\tau]$ is the
expected delay. To further improve its computational efficiency and to expand
its applicability in high-dimensional RL problems, we incorporate a
gradient-based approximate sampling scheme via Langevin dynamics for
Delayed-LPSVI, which maintains the same order-optimal regret guarantee with
$\widetilde{O}(dHK)$ computational cost. Empirical evaluations are performed to
demonstrate the statistical and computational efficacy of our algorithms.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18920" title="Abstract">arXiv:2310.18920</a> [<a href="/pdf/2310.18920" title="Download PDF">pdf</a>, <a href="/format/2310.18920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Multi-Person Pose Tracking with A Confidence Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zehua Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wenhang Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhenghui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Multimedia. 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Human pose estimation and tracking are fundamental tasks for understanding
human behaviors in videos. Existing top-down framework-based methods usually
perform three-stage tasks: human detection, pose estimation and tracking.
Although promising results have been achieved, these methods rely heavily on
high-performance detectors and may fail to track persons who are occluded or
miss-detected. To overcome these problems, in this paper, we develop a novel
keypoint confidence network and a tracking pipeline to improve human detection
and pose estimation in top-down approaches. Specifically, the keypoint
confidence network is designed to determine whether each keypoint is occluded,
and it is incorporated into the pose estimation module. In the tracking
pipeline, we propose the Bbox-revision module to reduce missing detection and
the ID-retrieve module to correct lost trajectories, improving the performance
of the detection stage. Experimental results show that our approach is
universal in human detection and pose estimation, achieving state-of-the-art
performance on both PoseTrack 2017 and 2018 datasets.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18921" title="Abstract">arXiv:2310.18921</a> [<a href="/pdf/2310.18921" title="Download PDF">pdf</a>, <a href="/ps/2310.18921" title="Download PostScript">ps</a>, <a href="/format/2310.18921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QWID: Quantized Weed Identification Deep neural network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rathore%2C+P+S">Parikshit Singh Rathore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we present an efficient solution for weed classification in
agriculture. We focus on optimizing model performance at inference while
respecting the constraints of the agricultural domain. We propose a Quantized
Deep Neural Network model that classifies a dataset of 9 weed classes using
8-bit integer (int8) quantization, a departure from standard 32-bit floating
point (fp32) models. Recognizing the hardware resource limitations in
agriculture, our model balances model size, inference time, and accuracy,
aligning with practical requirements. We evaluate the approach on ResNet-50 and
InceptionV3 architectures, comparing their performance against their int8
quantized versions. Transfer learning and fine-tuning are applied using the
DeepWeeds dataset. The results show staggering model size and inference time
reductions while maintaining accuracy in real-world production scenarios like
Desktop, Mobile and Raspberry Pi. Our work sheds light on a promising direction
for efficient AI in agriculture, holding potential for broader applications.
<br />Code: https://github.com/parikshit14/QNN-for-weed
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18924" title="Abstract">arXiv:2310.18924</a> [<a href="/pdf/2310.18924" title="Download PDF">pdf</a>, <a href="/format/2310.18924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Remaining Useful Life Prediction of Lithium-ion Batteries using  Spatio-temporal Multimodal Attention Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suh%2C+S">Sungho Suh</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+D+A">Dhruv Aditya Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Bello%2C+H">Hymalai Bello</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+M+S">Mayank Shekhar Jha</a>, 
<a href="/search/cs?searchtype=author&query=Lukowicz%2C+P">Paul Lukowicz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Lithium-ion batteries are widely used in various applications, including
electric vehicles and renewable energy storage. The prediction of the remaining
useful life (RUL) of batteries is crucial for ensuring reliable and efficient
operation, as well as reducing maintenance costs. However, determining the life
cycle of batteries in real-world scenarios is challenging, and existing methods
have limitations in predicting the number of cycles iteratively. In addition,
existing works often oversimplify the datasets, neglecting important features
of the batteries such as temperature, internal resistance, and material type.
To address these limitations, this paper proposes a two-stage remaining useful
life prediction scheme for Lithium-ion batteries using a spatio-temporal
multimodal attention network (ST-MAN). The proposed model is designed to
iteratively predict the number of cycles required for the battery to reach the
end of its useful life, based on available data. The proposed ST-MAN is to
capture the complex spatio-temporal dependencies in the battery data, including
the features that are often neglected in existing works. Experimental results
demonstrate that the proposed ST-MAN model outperforms existing CNN and
LSTM-based methods, achieving state-of-the-art performance in predicting the
remaining useful life of Li-ion batteries. The proposed method has the
potential to improve the reliability and efficiency of battery operations and
is applicable in various industries, including automotive and renewable energy.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18926" title="Abstract">arXiv:2310.18926</a> [<a href="/pdf/2310.18926" title="Download PDF">pdf</a>, <a href="/format/2310.18926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHAIN: Exploring Global-Local Spatio-Temporal Information for Improved  Self-Supervised Video Hashing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+R">Rukai Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jingkuan Song</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Heng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yanzhao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Ke Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Compressing videos into binary codes can improve retrieval speed and reduce
storage overhead. However, learning accurate hash codes for video retrieval can
be challenging due to high local redundancy and complex global dependencies
between video frames, especially in the absence of labels. Existing
self-supervised video hashing methods have been effective in designing
expressive temporal encoders, but have not fully utilized the temporal dynamics
and spatial appearance of videos due to less challenging and unreliable
learning tasks. To address these challenges, we begin by utilizing the
contrastive learning task to capture global spatio-temporal information of
videos for hashing. With the aid of our designed augmentation strategies, which
focus on spatial and temporal variations to create positive pairs, the learning
framework can generate hash codes that are invariant to motion, scale, and
viewpoint. Furthermore, we incorporate two collaborative learning tasks, i.e.,
frame order verification and scene change regularization, to capture local
spatio-temporal details within video frames, thereby enhancing the perception
of temporal structure and the modeling of spatio-temporal relationships. Our
proposed Contrastive Hashing with Global-Local Spatio-temporal Information
(CHAIN) outperforms state-of-the-art self-supervised video hashing methods on
four video benchmark datasets. Our codes will be released.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18928" title="Abstract">arXiv:2310.18928</a> [<a href="/pdf/2310.18928" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A transfer learning approach with convolutional neural network for Face  Mask Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Younesi%2C+A">Abolfazl Younesi</a>, 
<a href="/search/cs?searchtype=author&query=Afrouzian%2C+R">Reza Afrouzian</a>, 
<a href="/search/cs?searchtype=author&query=Seyfari%2C+Y">Yousef Seyfari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, in Persian language, 8 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Advanced Signal Processing, vol. 5, no. 1, Spring and
  Summer 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Due to the epidemic of the coronavirus (Covid-19) and its rapid spread around
the world, the world has faced an enormous crisis. To prevent the spread of the
coronavirus, the World Health Organization (WHO) has introduced the use of
masks and keeping social distance as the best preventive method. So, developing
an automatic monitoring system for detecting facemasks in some crowded places
is essential. To do this, we propose a mask recognition system based on
transfer learning and Inception v3 architecture. In the proposed method, two
datasets are used simultaneously for training including the Simulated Mask Face
Dataset (SMFD) and MaskedFace-Net (MFN) This paper tries to increase the
accuracy of the proposed system by optimally setting hyper-parameters and
accurately designing the fully connected layers. The main advantage of the
proposed method is that in addition to masked and unmasked faces, it can also
detect cases of incorrect use of mask. Therefore, the proposed method
classifies the input face images into three categories. Experimental results
show the high accuracy and efficiency of the proposed method; so, this method
has achieved an accuracy of 99.47% and 99.33% in training and test data
respectively
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18929" title="Abstract">arXiv:2310.18929</a> [<a href="/pdf/2310.18929" title="Download PDF">pdf</a>, <a href="/format/2310.18929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Ontological Model of User Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdel-Keream%2C+M">Mona Abdel-Keream</a>, 
<a href="/search/cs?searchtype=author&query=Be%C3%9Fler%2C+D">Daniel Be&#xdf;ler</a>, 
<a href="/search/cs?searchtype=author&query=Janssen%2C+A">Ayden Janssen</a>, 
<a href="/search/cs?searchtype=author&query=Jongebloed%2C+S">Sascha Jongebloed</a>, 
<a href="/search/cs?searchtype=author&query=Nolte%2C+R">Robin Nolte</a>, 
<a href="/search/cs?searchtype=author&query=Pomarlan%2C+M">Mihai Pomarlan</a>, 
<a href="/search/cs?searchtype=author&query=Porzel%2C+R">Robert Porzel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The notion of preferences plays an important role in many disciplines
including service robotics which is concerned with scenarios in which robots
interact with humans. These interactions can be favored by robots taking human
preferences into account. This raises the issue of how preferences should be
represented to support such preference-aware decision making. Several formal
accounts for a notion of preferences exist. However, these approaches fall
short on defining the nature and structure of the options that a robot has in a
given situation. In this work, we thus investigate a formal model of
preferences where options are non-atomic entities that are defined by the
complex situations they bring about.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18930" title="Abstract">arXiv:2310.18930</a> [<a href="/pdf/2310.18930" title="Download PDF">pdf</a>, <a href="/format/2310.18930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrofitting Light-weight Language Models for Emotions using Supervised  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+S">Sapan Shah</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Sreedhar Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+P">Pushpak Bhattacharyya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Camera Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present a novel retrofitting method to induce emotion aspects into
pre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates
pre-trained network weights using contrastive learning so that the text
fragments exhibiting similar emotions are encoded nearby in the representation
space, and the fragments with different emotion content are pushed apart. While
doing so, it also ensures that the linguistic knowledge already present in PLMs
is not inadvertently perturbed. The language models retrofitted by our method,
i.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, as
evaluated through different clustering and retrieval metrics. For the
downstream tasks on sentiment analysis and sarcasm detection, they perform
better than their pre-trained counterparts (about 1% improvement in F1-score)
and other existing approaches. Additionally, a more significant boost in
performance is observed for the retrofitted models over pre-trained ones in
few-shot learning setting.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18932" title="Abstract">arXiv:2310.18932</a> [<a href="/pdf/2310.18932" title="Download PDF">pdf</a>, <a href="/format/2310.18932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self Attention with Temporal Prior: Can We Learn More from Arrow of  Time?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K+G">Kyung Geun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B+T">Byeong Tak Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Many of diverse phenomena in nature often inherently encode both short and
long term temporal dependencies, short term dependencies especially resulting
from the direction of flow of time. In this respect, we discovered experimental
evidences suggesting that {\it interrelations} of these events are higher for
closer time stamps. However, to be able for attention based models to learn
these regularities in short term dependencies, it requires large amounts of
data which are often infeasible. This is due to the reason that, while they are
good at learning piece wised temporal dependencies, attention based models lack
structures that encode biases in time series. As a resolution, we propose a
simple and efficient method that enables attention layers to better encode
short term temporal bias of these data sets by applying learnable, adaptive
kernels directly to the attention matrices. For the experiments, we chose
various prediction tasks using Electronic Health Records (EHR) data sets since
they are great examples that have underlying long and short term temporal
dependencies. The results of our experiments show exceptional classification
results compared to best performing models on most of the task and data sets.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18933" title="Abstract">arXiv:2310.18933</a> [<a href="/pdf/2310.18933" title="Download PDF">pdf</a>, <a href="/format/2310.18933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Poisoning is All You Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jha%2C+R+D">Rishi D. Jha</a>, 
<a href="/search/cs?searchtype=author&query=Hayase%2C+J">Jonathan Hayase</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sewoong Oh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In a backdoor attack, an adversary injects corrupted data into a model's
training dataset in order to gain control over its predictions on images with a
specific attacker-defined trigger. A typical corrupted training example
requires altering both the image, by applying the trigger, and the label.
Models trained on clean images, therefore, were considered safe from backdoor
attacks. However, in some common machine learning scenarios, the training
labels are provided by potentially malicious third-parties. This includes
crowd-sourced annotation and knowledge distillation. We, hence, investigate a
fundamental question: can we launch a successful backdoor attack by only
corrupting labels? We introduce a novel approach to design label-only backdoor
attacks, which we call FLIP, and demonstrate its strengths on three datasets
(CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32,
ResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels
corrupted, FLIP achieves a near-perfect attack success rate of 99.4% while
suffering only a 1.8% drop in the clean test accuracy. Our approach builds upon
the recent advances in trajectory matching, originally introduced for dataset
distillation.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18935" title="Abstract">arXiv:2310.18935</a> [<a href="/pdf/2310.18935" title="Download PDF">pdf</a>, <a href="/format/2310.18935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU  Networks on Nearly-orthogonal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kou%2C+Y">Yiwen Kou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zixiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages, 7 figures. In NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">The implicit bias towards solutions with favorable properties is believed to
be a key reason why neural networks trained by gradient-based optimization can
generalize well. While the implicit bias of gradient flow has been widely
studied for homogeneous neural networks (including ReLU and leaky ReLU
networks), the implicit bias of gradient descent is currently only understood
for smooth neural networks. Therefore, implicit bias in non-smooth neural
networks trained by gradient descent remains an open question. In this paper,
we aim to answer this question by studying the implicit bias of gradient
descent for training two-layer fully connected (leaky) ReLU neural networks. We
showed that when the training data are nearly-orthogonal, for leaky ReLU
activation function, gradient descent will find a network with a stable rank
that converges to $1$, whereas for ReLU activation function, gradient descent
will find a neural network with a stable rank that is upper bounded by a
constant. Additionally, we show that gradient descent will find a neural
network such that all the training data points have the same normalized margin
asymptotically. Experiments on both synthetic and real data backup our
theoretical findings.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18936" title="Abstract">arXiv:2310.18936</a> [<a href="/pdf/2310.18936" title="Download PDF">pdf</a>, <a href="/ps/2310.18936" title="Download PostScript">ps</a>, <a href="/format/2310.18936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Examples Are Not Real Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yisen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The existence of adversarial examples has been a mystery for years and
attracted much interest. A well-known theory by \citet{ilyas2019adversarial}
explains adversarial vulnerability from a data perspective by showing that one
can extract non-robust features from adversarial examples and these features
alone are useful for classification. However, the explanation remains quite
counter-intuitive since non-robust features are mostly noise features to
humans. In this paper, we re-examine the theory from a larger context by
incorporating multiple learning paradigms. Notably, we find that contrary to
their good usefulness under supervised learning, non-robust features attain
poor usefulness when transferred to other self-supervised learning paradigms,
such as contrastive learning, masked image modeling, and diffusion models. It
reveals that non-robust features are not really as useful as robust or natural
features that enjoy good transferability between these paradigms. Meanwhile,
for robustness, we also show that naturally trained encoders from robust
features are largely non-robust under AutoAttack. Our cross-paradigm
examination suggests that the non-robust features are not really useful but
more like paradigm-wise shortcuts, and robust features alone might be
insufficient to attain reliable model robustness. Code is available at
\url{https://github.com/PKU-ML/AdvNotRealFeatures}.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18937" title="Abstract">arXiv:2310.18937</a> [<a href="/pdf/2310.18937" title="Download PDF">pdf</a>, <a href="/format/2310.18937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Utility of &quot;Even if...&quot; Semifactual Explanation to Optimise Positive  Outcomes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kenny%2C+E+M">Eoin M. Kenny</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weipeng Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">When users receive either a positive or negative outcome from an automated
system, Explainable AI (XAI) has almost exclusively focused on how to mutate
negative outcomes into positive ones by crossing a decision boundary using
counterfactuals (e.g., \textit{"If you earn 2k more, we will accept your loan
application"}). Here, we instead focus on \textit{positive} outcomes, and take
the novel step of using XAI to optimise them (e.g., \textit{"Even if you wish
to half your down-payment, we will still accept your loan application"}).
Explanations such as these that employ "even if..." reasoning, and do not cross
a decision boundary, are known as semifactuals. To instantiate semifactuals in
this context, we introduce the concept of \textit{Gain} (i.e., how much a user
stands to benefit from the explanation), and consider the first causal
formalisation of semifactuals. Tests on benchmark datasets show our algorithms
are better at maximising gain compared to prior work, and that causality is
important in the process. Most importantly however, a user study supports our
main hypothesis by showing people find semifactual explanations more useful
than counterfactuals when they receive the positive outcome of a loan
acceptance.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18938" title="Abstract">arXiv:2310.18938</a> [<a href="/pdf/2310.18938" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Algorithms to Predict Chess960 Result and Develop  Opening Themes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deo%2C+S">Shreyan Deo</a>, 
<a href="/search/cs?searchtype=author&query=Dwivedi%2C+N">Nishchal Dwivedi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures and 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work focuses on the analysis of Chess 960, also known as Fischer Random
Chess, a variant of traditional chess where the starting positions of the
pieces are randomized. The study aims to predict the game outcome using machine
learning techniques and develop an opening theme for each starting position.
The first part of the analysis utilizes machine learning models to predict the
game result based on certain moves in each position. The methodology involves
segregating raw data from .pgn files into usable formats and creating datasets
comprising approximately 500 games for each starting position. Three machine
learning algorithms -- KNN Clustering, Random Forest, and Gradient Boosted
Trees -- have been used to predict the game outcome. To establish an opening
theme, the board is divided into five regions: center, white kingside, white
queenside, black kingside, and black queenside. The data from games played by
top engines in all 960 positions is used to track the movement of pieces in the
opening. By analysing the change in the number of pieces in each region at
specific moves, the report predicts the region towards which the game is
developing. These models provide valuable insights into predicting game
outcomes and understanding the opening theme in Chess 960.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18940" title="Abstract">arXiv:2310.18940</a> [<a href="/pdf/2310.18940" title="Download PDF">pdf</a>, <a href="/format/2310.18940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Agents with Reinforcement Learning for Strategic Play in the  Werewolf Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zelai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Agents built with large language models (LLMs) have recently achieved great
advancements. However, most of the efforts focus on single-agent or cooperative
settings, leaving more general multi-agent environments underexplored. We
propose a new framework powered by reinforcement learning (RL) to develop
strategic language agents, i.e., LLM-based agents with strategic thinking
ability, for a popular language game, Werewolf. Werewolf is a social deduction
game with hidden roles that involves both cooperation and competition and
emphasizes deceptive communication and diverse gameplay. Our agent tackles this
game by first using LLMs to reason about potential deceptions and generate a
set of strategically diverse actions. Then an RL policy, which selects an
action from the candidates, is learned by population-based training to enhance
the agents' decision-making ability. By combining LLMs with the RL policy, our
agent produces a variety of emergent strategies, achieves the highest win rate
against other LLM-based agents, and stays robust against adversarial human
players in the Werewolf game.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18944" title="Abstract">arXiv:2310.18944</a> [<a href="/pdf/2310.18944" title="Download PDF">pdf</a>, <a href="/format/2310.18944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S2F-NER: Exploring Sequence-to-Forest Generation for Complex Entity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yongxiu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yue Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Named Entity Recognition (NER) remains challenging due to the complex
entities, like nested, overlapping, and discontinuous entities. Existing
approaches, such as sequence-to-sequence (Seq2Seq) generation and span-based
classification, have shown impressive performance on various NER subtasks, but
they are difficult to scale to datasets with longer input text because of
either exposure bias issue or inefficient computation. In this paper, we
propose a novel Sequence-to-Forest generation paradigm, S2F-NER, which can
directly extract entities in sentence via a Forest decoder that decode multiple
entities in parallel rather than sequentially. Specifically, our model generate
each path of each tree in forest autoregressively, where the maximum depth of
each tree is three (which is the shortest feasible length for complex NER and
is far smaller than the decoding length of Seq2Seq). Based on this novel
paradigm, our model can elegantly mitigates the exposure bias problem and keep
the simplicity of Seq2Seq. Experimental results show that our model
significantly outperforms the baselines on three discontinuous NER datasets and
on two nested NER datasets, especially for discontinuous entity recognition.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18946" title="Abstract">arXiv:2310.18946</a> [<a href="/pdf/2310.18946" title="Download PDF">pdf</a>, <a href="/format/2310.18946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video Frame Interpolation with Many-to-many Splatting and Spatial  Selective Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+P">Ping Hu</a>, 
<a href="/search/cs?searchtype=author&query=Niklaus%2C+S">Simon Niklaus</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sclaroff%2C+S">Stan Sclaroff</a>, 
<a href="/search/cs?searchtype=author&query=Saenko%2C+K">Kate Saenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> T-PAMI. arXiv admin note: substantial text overlap with <a href="/abs/2204.03513">arXiv:2204.03513</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">In this work, we first propose a fully differentiable Many-to-Many (M2M)
splatting framework to interpolate frames efficiently. Given a frame pair, we
estimate multiple bidirectional flows to directly forward warp the pixels to
the desired time step before fusing overlapping pixels. In doing so, each
source pixel renders multiple target pixels and each target pixel can be
synthesized from a larger area of visual context, establishing a many-to-many
splatting scheme with robustness to undesirable artifacts. For each input frame
pair, M2M has a minuscule computational overhead when interpolating an
arbitrary number of in-between frames, hence achieving fast multi-frame
interpolation. However, directly warping and fusing pixels in the intensity
domain is sensitive to the quality of motion estimation and may suffer from
less effective representation capacity. To improve interpolation accuracy, we
further extend an M2M++ framework by introducing a flexible Spatial Selective
Refinement (SSR) component, which allows for trading computational efficiency
for interpolation quality and vice versa. Instead of refining the entire
interpolated frame, SSR only processes difficult regions selected under the
guidance of an estimated error map, thereby avoiding redundant computation.
Evaluation on multiple benchmark datasets shows that our method is able to
improve the efficiency while maintaining competitive video interpolation
quality, and it can be adjusted to use more or less compute as needed.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18948" title="Abstract">arXiv:2310.18948</a> [<a href="/pdf/2310.18948" title="Download PDF">pdf</a>, <a href="/format/2310.18948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building a Safer Maritime Environment Through Multi-Path Long-Term  Vessel Trajectory Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spadon%2C+G">Gabriel Spadon</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+J">Jay Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+M">Matthew Smith</a>, 
<a href="/search/cs?searchtype=author&query=Vela%2C+S">Sarah Vela</a>, 
<a href="/search/cs?searchtype=author&query=Gehrmann%2C+R">Romina Gehrmann</a>, 
<a href="/search/cs?searchtype=author&query=Eden%2C+D">Derek Eden</a>, 
<a href="/search/cs?searchtype=author&query=van+Berkel%2C+J">Joshua van Berkel</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+A">Amilcar Soares</a>, 
<a href="/search/cs?searchtype=author&query=Fablet%2C+R">Ronan Fablet</a>, 
<a href="/search/cs?searchtype=author&query=Pelot%2C+R">Ronald Pelot</a>, 
<a href="/search/cs?searchtype=author&query=Matwin%2C+S">Stan Matwin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 13 figures, 6 tables, 27 equations, and 1 algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Probability (math.PR)

</div>
<p class="mathjax">Maritime transport is paramount to global economic growth and environmental
sustainability. In this regard, the Automatic Identification System (AIS) data
plays a significant role by offering real-time streaming data on vessel
movement, which allows for enhanced traffic surveillance, assisting in vessel
safety by avoiding vessel-to-vessel collisions and proactively preventing
vessel-to-whale ones. This paper tackles an intrinsic problem to trajectory
forecasting: the effective multi-path long-term vessel trajectory forecasting
on engineered sequences of AIS data. We utilize an encoder-decoder model with
Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12
hours of vessel trajectories using 1 to 3 hours of AIS data. We feed the model
with probabilistic features engineered from the AIS data that refer to the
potential route and destination of each trajectory so that the model,
leveraging convolutional layers for spatial feature learning and a
position-aware attention mechanism that increases the importance of recent
timesteps of a sequence during temporal feature learning, forecasts the vessel
trajectory taking the potential route and destination into account. The F1
Score of these features is approximately 85% and 75%, indicating their
efficiency in supplementing the neural network. We trialed our model in the
Gulf of St. Lawrence, one of the North Atlantic Right Whales (NARW) habitats,
achieving an R2 score exceeding 98% with varying techniques and features.
Despite the high R2 score being attributed to well-defined shipping lanes, our
model demonstrates superior complex decision-making during path selection. In
addition, our model shows enhanced accuracy, with average and median
forecasting errors of 11km and 6km, respectively. Our study confirms the
potential of geographical data engineering and trajectory forecasting models
for preserving marine life species.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18949" title="Abstract">arXiv:2310.18949</a> [<a href="/pdf/2310.18949" title="Download PDF">pdf</a>, <a href="/format/2310.18949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Customize StyleGAN with One Hand Sketch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaocong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Generating images from human sketches typically requires dedicated networks
trained from scratch. In contrast, the emergence of the pre-trained
Vision-Language models (e.g., CLIP) has propelled generative applications based
on controlling the output imagery of existing StyleGAN models with text inputs
or reference images. Parallelly, our work proposes a framework to control
StyleGAN imagery with a single user sketch. In particular, we learn a
conditional distribution in the latent space of a pre-trained StyleGAN model
via energy-based learning and propose two novel energy functions leveraging
CLIP for cross-domain semantic supervision. Once trained, our model can
generate multi-modal images semantically aligned with the input sketch.
Quantitative evaluations on synthesized datasets have shown that our approach
improves significantly from previous methods in the one-shot regime. The
superiority of our method is further underscored when experimenting with a wide
range of human sketches of diverse styles and poses. Surprisingly, our models
outperform the previous baseline regarding both the range of sketch inputs and
image qualities despite operating with a stricter setting: with no extra
training data and single sketch input.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18951" title="Abstract">arXiv:2310.18951</a> [<a href="/pdf/2310.18951" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multimodal Ecological Civilization Pattern Recommendation Method Based  on Large Language Models and Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhihang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yunqiang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zhiqiang Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The Ecological Civilization Pattern Recommendation System (ECPRS) aims to
recommend suitable ecological civilization patterns for target regions,
promoting sustainable development and reducing regional disparities. However,
the current representative recommendation methods are not suitable for
recommending ecological civilization patterns in a geographical context. There
are two reasons for this. Firstly, regions have spatial heterogeneity, and the
(ECPRS)needs to consider factors like climate, topography, vegetation, etc., to
recommend civilization patterns adapted to specific ecological environments,
ensuring the feasibility and practicality of the recommendations. Secondly, the
abstract features of the ecological civilization patterns in the real world
have not been fully utilized., resulting in poor richness in their embedding
representations and consequently, lower performance of the recommendation
system. Considering these limitations, we propose the ECPR-MML method.
Initially, based on the novel method UGPIG, we construct a knowledge graph to
extract regional representations incorporating spatial heterogeneity features.
Following that, inspired by the significant progress made by Large Language
Models (LLMs) in the field of Natural Language Processing (NLP), we employ
Large LLMs to generate multimodal features for ecological civilization patterns
in the form of text and images. We extract and integrate these multimodal
features to obtain semantically rich representations of ecological
civilization. Through extensive experiments, we validate the performance of our
ECPR-MML model. Our results show that F1@5 is 2.11% higher compared to
state-of-the-art models, 2.02% higher than NGCF, and 1.16% higher than UGPIG.
Furthermore, multimodal data can indeed enhance recommendation performance.
However, the data generated by LLM is not as effective as real data to a
certain extent.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18953" title="Abstract">arXiv:2310.18953</a> [<a href="/pdf/2310.18953" title="Download PDF">pdf</a>, <a href="/format/2310.18953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TIC-TAC: A Framework To Learn And Evaluate Your Covariance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shukla%2C+M">Megh Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Salzmann%2C+M">Mathieu Salzmann</a>, 
<a href="/search/cs?searchtype=author&query=Alahi%2C+A">Alexandre Alahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures. Please feel free to provide feedback!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">We study the problem of unsupervised heteroscedastic covariance estimation,
where the goal is to learn the multivariate target distribution $\mathcal{N}(y,
\Sigma_y | x )$ given an observation $x$. This problem is particularly
challenging as $\Sigma_{y}$ varies for different samples (heteroscedastic) and
no annotation for the covariance is available (unsupervised). Typically,
state-of-the-art methods predict the mean $f_{\theta}(x)$ and covariance
$\textrm{Cov}(f_{\theta}(x))$ of the target distribution through two neural
networks trained using the negative log-likelihood. This raises two questions:
(1) Does the predicted covariance truly capture the randomness of the predicted
mean? (2) In the absence of ground-truth annotation, how can we quantify the
performance of covariance estimation? We address (1) by deriving TIC: Taylor
Induced Covariance, which captures the randomness of the multivariate
$f_{\theta}(x)$ by incorporating its gradient and curvature around $x$ through
the second order Taylor polynomial. Furthermore, we tackle (2) by introducing
TAC: Task Agnostic Correlations, a metric which leverages conditioning of the
normal distribution to evaluate the covariance. We verify the effectiveness of
TIC through multiple experiments spanning synthetic (univariate, multivariate)
and real-world datasets (UCI Regression, LSP, and MPII Human Pose Estimation).
Our experiments show that TIC outperforms state-of-the-art in accurately
learning the covariance, as quantified through TAC.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18954" title="Abstract">arXiv:2310.18954</a> [<a href="/pdf/2310.18954" title="Download PDF">pdf</a>, <a href="/format/2310.18954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mask Propagation for Efficient Video Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+Y">Yuetian Weng</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+M">Mingfei Han</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Haoyu He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xiaojun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Video Semantic Segmentation (VSS) involves assigning a semantic label to each
pixel in a video sequence. Prior work in this field has demonstrated promising
results by extending image semantic segmentation models to exploit temporal
relationships across video frames; however, these approaches often incur
significant computational costs. In this paper, we propose an efficient mask
propagation framework for VSS, called MPVSS. Our approach first employs a
strong query-based image segmentor on sparse key frames to generate accurate
binary masks and class predictions. We then design a flow estimation module
utilizing the learned queries to generate a set of segment-aware flow maps,
each associated with a mask prediction from the key frame. Finally, the
mask-flow pairs are warped to serve as the mask predictions for the non-key
frames. By reusing predictions from key frames, we circumvent the need to
process a large volume of video frames individually with resource-intensive
segmentors, alleviating temporal redundancy and significantly reducing
computational costs. Extensive experiments on VSPW and Cityscapes demonstrate
that our mask propagation framework achieves SOTA accuracy and efficiency
trade-offs. For instance, our best model with Swin-L backbone outperforms the
SOTA MRCFA using MiT-B5 by 4.0% mIoU, requiring only 26% FLOPs on the VSPW
dataset. Moreover, our framework reduces up to 4x FLOPs compared to the
per-frame Mask2Former baseline with only up to 2% mIoU degradation on the
Cityscapes validation set. Code is available at
https://github.com/ziplab/MPVSS.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18955" title="Abstract">arXiv:2310.18955</a> [<a href="/pdf/2310.18955" title="Download PDF">pdf</a>, <a href="/format/2310.18955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Playing in the Dark: No-regret Learning with Adversarial Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Abhishek Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Vaze%2C+R">Rahul Vaze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We study a generalization of the classic Online Convex Optimization (OCO)
framework by considering additional long-term adversarial constraints.
Specifically, after an online policy decides its action on a round, in addition
to a convex cost function, the adversary also reveals a set of $k$ convex
constraints. The cost and the constraint functions could change arbitrarily
with time, and no information about the future functions is assumed to be
available. In this paper, we propose a meta-policy that simultaneously achieves
a sublinear cumulative constraint violation and a sublinear regret. This is
achieved via a black box reduction of the constrained problem to the standard
OCO problem for a recursively constructed sequence of surrogate cost functions.
We show that optimal performance bounds can be achieved by solving the
surrogate problem using any adaptive OCO policy enjoying a standard
data-dependent regret bound. A new Lyapunov-based proof technique is presented
that reveals a connection between regret and certain sequential inequalities
through a novel decomposition result. We conclude the paper by highlighting
applications to online multi-task learning and network control problems.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18956" title="Abstract">arXiv:2310.18956</a> [<a href="/pdf/2310.18956" title="Download PDF">pdf</a>, <a href="/format/2310.18956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Towle%2C+B">Benjamin Towle</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Ke Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> FINDINGS-EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Reply suggestion systems represent a staple component of many instant
messaging and email systems. However, the requirement to produce sets of
replies, rather than individual replies, makes the task poorly suited for
out-of-the-box retrieval architectures, which only consider individual
message-reply similarity. As a result, these system often rely on additional
post-processing modules to diversify the outputs. However, these approaches are
ultimately bottlenecked by the performance of the initial retriever, which in
practice struggles to present a sufficiently diverse range of options to the
downstream diversification module, leading to the suggestions being less
relevant to the user. In this paper, we consider a novel approach that
radically simplifies this pipeline through an autoregressive text-to-text
retrieval model, that learns the smart reply task end-to-end from a dataset of
(message, reply set) pairs obtained via bootstrapping. Empirical results show
this method consistently outperforms a range of state-of-the-art baselines
across three datasets, corresponding to a 5.1%-17.9% improvement in relevance,
and a 0.5%-63.1% improvement in diversity compared to the best baseline
approach. We make our code publicly available.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18961" title="Abstract">arXiv:2310.18961</a> [<a href="/pdf/2310.18961" title="Download PDF">pdf</a>, <a href="/format/2310.18961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qihang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shibo He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiming Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Zero-shot anomaly detection (ZSAD) requires detection models trained using
auxiliary data to detect anomalies without any training sample in a target
dataset. It is a crucial task when training data is not accessible due to
various concerns, \eg, data privacy, yet it is challenging since the models
need to generalize to anomalies across different domains where the appearance
of foreground objects, abnormal regions, and background features, such as
defects/tumors on different products/organs, can vary significantly. Recently
large pre-trained vision-language models (VLMs), such as CLIP, have
demonstrated strong zero-shot recognition ability in various vision tasks,
including anomaly detection. However, their ZSAD performance is weak since the
VLMs focus more on modeling the class semantics of the foreground objects
rather than the abnormality/normality in the images. In this paper we introduce
a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across
different domains. The key insight of AnomalyCLIP is to learn object-agnostic
text prompts that capture generic normality and abnormality in an image
regardless of its foreground objects. This allows our model to focus on the
abnormal image regions rather than the object semantics, enabling generalized
normality and abnormality recognition on diverse types of objects. Large-scale
experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP
achieves superior zero-shot performance of detecting and segmenting anomalies
in datasets of highly diverse class semantics from various defect inspection
and medical imaging domains. Code will be made available at
https://github.com/zqhang/AnomalyCLIP.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18964" title="Abstract">arXiv:2310.18964</a> [<a href="/pdf/2310.18964" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs and Finetuning: Benchmarking cross-domain performance for hate  speech detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nasir%2C+A">Ahmad Nasir</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Aadish Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Jaidka%2C+K">Kokil Jaidka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper compares different pre-trained and fine-tuned large language
models (LLMs) for hate speech detection. Our research underscores challenges in
LLMs' cross-domain validity and overfitting risks. Through evaluations, we
highlight the need for fine-tuned models that grasp the nuances of hate speech
through greater label heterogeneity. We conclude with a vision for the future
of hate speech detection, emphasizing cross-domain generalizability and
appropriate benchmarking practices.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18965" title="Abstract">arXiv:2310.18965</a> [<a href="/pdf/2310.18965" title="Download PDF">pdf</a>, <a href="/format/2310.18965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Power of Counting by Nonuniform Families of Polynomial-Size Finite  Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamakami%2C+T">Tomoyuki Yamakami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> (A4, 10pt, 21 pages) This paper corrects and extends a preliminary report published in the Proceedings of the 24th International Symposium on Fundamentals of Computation Theory (FCT 2023), Trier, Germany, September 18-24, 2023, Lecture Notes in Computer Science, vol. 14292, pp. 421-435, Springer Cham, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Lately, there have been intensive studies on strengths and limitations of
nonuniform families of promise decision problems solvable by various types of
polynomial-size finite automata families, where "polynomial-size" refers to the
polynomially-bounded state complexity of a finite automata family. In this line
of study, we further expand the scope of these studies to families of partial
counting and gap functions, defined in terms of nonuniform families of
polynomial-size nondeterministic finite automata, and their relevant families
of promise decision problems. Counting functions have an ability of counting
the number of accepting computation paths produced by nondeterministic finite
automata. With no unproven hardness assumption, we show numerous separations
and collapses of complexity classes of those partial counting and gap function
families and their induced promise decision problem families. We also
investigate their relationships to pushdown automata families of polynomial
stack-state complexity.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18966" title="Abstract">arXiv:2310.18966</a> [<a href="/pdf/2310.18966" title="Download PDF">pdf</a>, <a href="/format/2310.18966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spacecraft Autonomous Decision-Planning for Collision Avoidance: a  Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bourriez%2C+N">Nicolas Bourriez</a>, 
<a href="/search/cs?searchtype=author&query=Loizeau%2C+A">Adrien Loizeau</a>, 
<a href="/search/cs?searchtype=author&query=Abdin%2C+A+F">Adam F. Abdin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint accepted in the 74th International Astronautical Congress (IAC) - Baku, Azerbaijan, 2-6 October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The space environment around the Earth is becoming increasingly populated by
both active spacecraft and space debris. To avoid potential collision events,
significant improvements in Space Situational Awareness (SSA) activities and
Collision Avoidance (CA) technologies are allowing the tracking and maneuvering
of spacecraft with increasing accuracy and reliability. However, these
procedures still largely involve a high level of human intervention to make the
necessary decisions. For an increasingly complex space environment, this
decision-making strategy is not likely to be sustainable. Therefore, it is
important to successfully introduce higher levels of automation for key Space
Traffic Management (STM) processes to ensure the level of reliability needed
for navigating a large number of spacecraft. These processes range from
collision risk detection to the identification of the appropriate action to
take and the execution of avoidance maneuvers. This work proposes an
implementation of autonomous CA decision-making capabilities on spacecraft
based on Reinforcement Learning (RL) techniques. A novel methodology based on a
Partially Observable Markov Decision Process (POMDP) framework is developed to
train the Artificial Intelligence (AI) system on board the spacecraft,
considering epistemic and aleatory uncertainties. The proposed framework
considers imperfect monitoring information about the status of the debris in
orbit and allows the AI system to effectively learn stochastic policies to
perform accurate Collision Avoidance Maneuvers (CAMs). The objective is to
successfully delegate the decision-making process for autonomously implementing
a CAM to the spacecraft without human intervention. This approach would allow
for a faster response in the decision-making process and for highly
decentralized operations.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18969" title="Abstract">arXiv:2310.18969</a> [<a href="/pdf/2310.18969" title="Download PDF">pdf</a>, <a href="/format/2310.18969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Vision Transformers for Image Classification in Class  Embedding Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vilas%2C+M+G">Martina G. Vilas</a>, 
<a href="/search/cs?searchtype=author&query=Schauml%C3%B6ffel%2C+T">Timothy Schauml&#xf6;ffel</a>, 
<a href="/search/cs?searchtype=author&query=Roig%2C+G">Gemma Roig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite the growing use of transformer models in computer vision, a
mechanistic understanding of these networks is still needed. This work
introduces a method to reverse-engineer Vision Transformers trained to solve
image classification tasks. Inspired by previous research in NLP, we
demonstrate how the inner representations at any level of the hierarchy can be
projected onto the learned class embedding space to uncover how these networks
build categorical representations for their predictions. We use our framework
to show how image tokens develop class-specific representations that depend on
attention mechanisms and contextual information, and give insights on how
self-attention and MLP layers differentially contribute to this categorical
composition. We additionally demonstrate that this method (1) can be used to
determine the parts of an image that would be important for detecting the class
of interest, and (2) exhibits significant advantages over traditional linear
probing approaches. Taken together, our results position our proposed framework
as a powerful tool for mechanistic interpretability and explainability
research.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18970" title="Abstract">arXiv:2310.18970</a> [<a href="/pdf/2310.18970" title="Download PDF">pdf</a>, <a href="/format/2310.18970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRIAGE: Characterizing and auditing training data for improved  regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seedat%2C+N">Nabeel Seedat</a>, 
<a href="/search/cs?searchtype=author&query=Crabb%C3%A9%2C+J">Jonathan Crabb&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhaozhi Qian</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Data quality is crucial for robust machine learning algorithms, with the
recent interest in data-centric AI emphasizing the importance of training data
characterization. However, current data characterization methods are largely
focused on classification settings, with regression settings largely
understudied. To address this, we introduce TRIAGE, a novel data
characterization framework tailored to regression tasks and compatible with a
broad class of regressors. TRIAGE utilizes conformal predictive distributions
to provide a model-agnostic scoring method, the TRIAGE score. We operationalize
the score to analyze individual samples' training dynamics and characterize
samples as under-, over-, or well-estimated by the model. We show that TRIAGE's
characterization is consistent and highlight its utility to improve performance
via data sculpting/filtering, in multiple regression settings. Additionally,
beyond sample level, we show TRIAGE enables new approaches to dataset selection
and feature acquisition. Overall, TRIAGE highlights the value unlocked by data
characterization in real-world regression applications
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18974" title="Abstract">arXiv:2310.18974</a> [<a href="/pdf/2310.18974" title="Download PDF">pdf</a>, <a href="/format/2310.18974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EtiCor: Corpus for Analyzing LLMs for Etiquettes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dwivedi%2C+A">Ashutosh Dwivedi</a>, 
<a href="/search/cs?searchtype=author&query=Lavania%2C+P">Pradhyumna Lavania</a>, 
<a href="/search/cs?searchtype=author&query=Modi%2C+A">Ashutosh Modi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023, Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Etiquettes are an essential ingredient of day-to-day interactions among
people. Moreover, etiquettes are region-specific, and etiquettes in one region
might contradict those in other regions. In this paper, we propose EtiCor, an
Etiquettes Corpus, having texts about social norms from five different regions
across the globe. The corpus provides a test bed for evaluating LLMs for
knowledge and understanding of region-specific etiquettes. Additionally, we
propose the task of Etiquette Sensitivity. We experiment with state-of-the-art
LLMs (Delphi, Falcon40B, and GPT-3.5). Initial results indicate that LLMs,
mostly fail to understand etiquettes from regions from non-Western world.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18975" title="Abstract">arXiv:2310.18975</a> [<a href="/pdf/2310.18975" title="Download PDF">pdf</a>, <a href="/format/2310.18975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blacksmith: Fast Adversarial Training of Vision Transformers via a  Mixture of Single-step and Multi-step Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salmani%2C+M">Mahdi Salmani</a>, 
<a href="/search/cs?searchtype=author&query=Farashah%2C+A+D">Alireza Dehghanpour Farashah</a>, 
<a href="/search/cs?searchtype=author&query=Azizmalayeri%2C+M">Mohammad Azizmalayeri</a>, 
<a href="/search/cs?searchtype=author&query=Amiri%2C+M">Mahdi Amiri</a>, 
<a href="/search/cs?searchtype=author&query=Eslami%2C+N">Navid Eslami</a>, 
<a href="/search/cs?searchtype=author&query=Manzuri%2C+M+T">Mohammad Taghi Manzuri</a>, 
<a href="/search/cs?searchtype=author&query=Rohban%2C+M+H">Mohammad Hossein Rohban</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite the remarkable success achieved by deep learning algorithms in
various domains, such as computer vision, they remain vulnerable to adversarial
perturbations. Adversarial Training (AT) stands out as one of the most
effective solutions to address this issue; however, single-step AT can lead to
Catastrophic Overfitting (CO). This scenario occurs when the adversarially
trained network suddenly loses robustness against multi-step attacks like
Projected Gradient Descent (PGD). Although several approaches have been
proposed to address this problem in Convolutional Neural Networks (CNNs), we
found out that they do not perform well when applied to Vision Transformers
(ViTs). In this paper, we propose Blacksmith, a novel training strategy to
overcome the CO problem, specifically in ViTs. Our approach utilizes either of
PGD-2 or Fast Gradient Sign Method (FGSM) randomly in a mini-batch during the
adversarial training of the neural network. This will increase the diversity of
our training attacks, which could potentially mitigate the CO issue. To manage
the increased training time resulting from this combination, we craft the PGD-2
attack based on only the first half of the layers, while FGSM is applied
end-to-end. Through our experiments, we demonstrate that our novel method
effectively prevents CO, achieves PGD-2 level performance, and outperforms
other existing techniques including N-FGSM, which is the state-of-the-art
method in fast training for CNNs.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18979" title="Abstract">arXiv:2310.18979</a> [<a href="/pdf/2310.18979" title="Download PDF">pdf</a>, <a href="/format/2310.18979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Methodology of Algorithm Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mendling%2C+J">Jan Mendling</a>, 
<a href="/search/cs?searchtype=author&query=Leopold%2C+H">Henrik Leopold</a>, 
<a href="/search/cs?searchtype=author&query=Meyerhenke%2C+H">Henning Meyerhenke</a>, 
<a href="/search/cs?searchtype=author&query=Depaire%2C+B">Beno&#xee;t Depaire</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Research on algorithms has drastically increased in recent years. Various
sub-disciplines of computer science investigate algorithms according to
different objectives and standards. This plurality of the field has led to
various methodological advances that have not yet been transferred to
neighboring sub-disciplines. The central roadblock for a better knowledge
exchange is the lack of a common methodological framework integrating the
perspectives of these sub-disciplines. It is the objective of this paper to
develop a research framework for algorithm engineering. Our framework builds on
three areas discussed in the philosophy of science: ontology, epistemology and
methodology. In essence, ontology describes algorithm engineering as being
concerned with algorithmic problems, algorithmic tasks, algorithm designs and
algorithm implementations. Epistemology describes the body of knowledge of
algorithm engineering as a collection of prescriptive and descriptive
knowledge, residing in World 3 of Popper's Three Worlds model. Methodology
refers to the steps how we can systematically enhance our knowledge of specific
algorithms. The framework helps us to identify and discuss various validity
concerns relevant to any algorithm engineering contribution. In this way, our
framework has important implications for researching algorithms in various
areas of computer science.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18983" title="Abstract">arXiv:2310.18983</a> [<a href="/pdf/2310.18983" title="Download PDF">pdf</a>, <a href="/format/2310.18983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DCQA: Document-Level Chart Question Answering towards Complex Reasoning  and Common-Sense Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+A">Anran Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Luwei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xingjiao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Junjie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zisong Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+N">Nian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Cheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Visually-situated languages such as charts and plots are omnipresent in
real-world documents. These graphical depictions are human-readable and are
often analyzed in visually-rich documents to address a variety of questions
that necessitate complex reasoning and common-sense responses. Despite the
growing number of datasets that aim to answer questions over charts, most only
address this task in isolation, without considering the broader context of
document-level question answering. Moreover, such datasets lack adequate
common-sense reasoning information in their questions. In this work, we
introduce a novel task named document-level chart question answering (DCQA).
The goal of this task is to conduct document-level question answering,
extracting charts or plots in the document via document layout analysis (DLA)
first and subsequently performing chart question answering (CQA). The newly
developed benchmark dataset comprises 50,010 synthetic documents integrating
charts in a wide range of styles (6 styles in contrast to 3 for PlotQA and
ChartQA) and includes 699,051 questions that demand a high degree of reasoning
ability and common-sense understanding. Besides, we present the development of
a potent question-answer generation engine that employs table data, a rich
color set, and basic question templates to produce a vast array of reasoning
question-answer pairs automatically. Based on DCQA, we devise an OCR-free
transformer for document-level chart-oriented understanding, capable of DLA and
answering complex reasoning and common-sense questions over charts in an
OCR-free manner. Our DCQA dataset is expected to foster research on
understanding visualizations in documents, especially for scenarios that
require complex reasoning for charts in the visually-rich document. We
implement and evaluate a set of baselines, and our proposed method achieves
comparable results.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18986" title="Abstract">arXiv:2310.18986</a> [<a href="/pdf/2310.18986" title="Download PDF">pdf</a>, <a href="/format/2310.18986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controllable Group Choreography using Contrastive Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+N">Nhat Le</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+T">Tuong Do</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+K">Khoa Do</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Hien Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tjiputra%2C+E">Erman Tjiputra</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+Q+D">Quang D. Tran</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Anh Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ACM Transactions on Graphics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Music-driven group choreography poses a considerable challenge but holds
significant potential for a wide range of industrial applications. The ability
to generate synchronized and visually appealing group dance motions that are
aligned with music opens up opportunities in many fields such as entertainment,
advertising, and virtual performances. However, most of the recent works are
not able to generate high-fidelity long-term motions, or fail to enable
controllable experience. In this work, we aim to address the demand for
high-quality and customizable group dance generation by effectively governing
the consistency and diversity of group choreographies. In particular, we
utilize a diffusion-based generative approach to enable the synthesis of
flexible number of dancers and long-term group dances, while ensuring coherence
to the input music. Ultimately, we introduce a Group Contrastive Diffusion
(GCD) strategy to enhance the connection between dancers and their group,
presenting the ability to control the consistency or diversity level of the
synthesized group animation via the classifier-guidance sampling technique.
Through intensive experiments and evaluation, we demonstrate the effectiveness
of our approach in producing visually captivating and consistent group dance
motions. The experimental results show the capability of our method to achieve
the desired levels of consistency and diversity, while maintaining the overall
quality of the generated group choreography.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18987" title="Abstract">arXiv:2310.18987</a> [<a href="/pdf/2310.18987" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NP-SBFL: Bridging the Gap Between Spectrum-Based Fault Localization and  Faulty Neural Pathways Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashemifar%2C+S">Soroush Hashemifar</a>, 
<a href="/search/cs?searchtype=author&query=Parsa%2C+S">Saeed Parsa</a>, 
<a href="/search/cs?searchtype=author&query=Kalaee%2C+A">Akram Kalaee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Neural and Evolutionary Computing (cs.NE); Software Engineering (cs.SE)

</div>
<p class="mathjax">Deep learning has revolutionized various real-world applications, but the
quality of Deep Neural Networks (DNNs) remains a concern. DNNs are complex and
have millions of parameters, making it difficult to determine their
contributions to fulfilling a task. Moreover, the behavior of a DNN is highly
influenced by the data used during training, making it challenging to collect
enough data to exercise all potential DNN behavior under all possible
scenarios. This paper proposes a novel NP-SBFL method that adapts
spectrum-based fault localization (SBFL) to locate faulty neural pathways. Our
method identifies critical neurons using the layer-wise relevance propagation
(LRP) technique and determines which critical neurons are faulty. We propose a
multi-stage gradient ascent (MGA), an extension of gradient ascent, to
effectively activate a sequence of neurons one at a time while maintaining the
activation of previous neurons. We evaluated the effectiveness of our method on
two commonly used datasets, MNIST and CIFAR-10, two baselines DeepFault and
NP-SBFL-GA, and three suspicious neuron measures, Tarantula, Ochiai, and
Barinel. The empirical results showed that NP-SBFL-MGA is statistically more
effective than the baselines at identifying suspicious paths and synthesizing
adversarial inputs. Particularly, Tarantula on NP-SBFL-MGA had the highest
fault detection rate at 96.75%, surpassing DeepFault on Ochiai (89.90%) and
NP-SBFL-GA on Ochiai (60.61%). Our approach also yielded comparable results to
the baselines in synthesizing naturalness inputs, and we found a positive
correlation between the coverage of critical paths and the number of failed
tests in DNN fault localization.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18992" title="Abstract">arXiv:2310.18992</a> [<a href="/pdf/2310.18992" title="Download PDF">pdf</a>, <a href="/format/2310.18992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bipartite Graph Pre-training for Unsupervised Extractive Summarization  with Graph Convolutional Auto-Encoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Q">Qianren Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shaobo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiarui Li</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+X">Xiaolei Gu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shizhu He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pre-trained sentence representations are crucial for identifying significant
sentences in unsupervised document extractive summarization. However, the
traditional two-step paradigm of pre-training and sentence-ranking, creates a
gap due to differing optimization objectives. To address this issue, we argue
that utilizing pre-trained embeddings derived from a process specifically
designed to optimize cohensive and distinctive sentence representations helps
rank significant sentences. To do so, we propose a novel graph pre-training
auto-encoder to obtain sentence embeddings by explicitly modelling
intra-sentential distinctive features and inter-sentential cohesive features
through sentence-word bipartite graphs. These pre-trained sentence
representations are then utilized in a graph-based ranking algorithm for
unsupervised summarization. Our method produces predominant performance for
unsupervised summarization frameworks by providing summary-worthy sentence
representations. It surpasses heavy BERT- or RoBERTa-based sentence
representations in downstream tasks.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18998" title="Abstract">arXiv:2310.18998</a> [<a href="/pdf/2310.18998" title="Download PDF">pdf</a>, <a href="/format/2310.18998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A 0.21-ps FOM Capacitor-Less Analog LDO with Dual-Range Load Current for  Biomedical Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Engur%2C+Y">Yasemin Engur</a>, 
<a href="/search/eess?searchtype=author&query=Shoaran%2C+M">Mahsa Shoaran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents an output capacitor-less low-dropout regulator (LDO) with
a bias switching scheme for biomedical applications with dual-range load
currents. Power optimization is crucial for systems with multiple activation
modes such as neural interfaces, IoT and edge devices with varying load
currents. To enable rapid switching between low and high current states, a
flipped voltage follower (FVF) configuration is utilized, along with a super
source follower buffer to drive the power transistor. Two feedback loops and an
on-chip compensation capacitor Cc maintain the stability of the regulator under
various load conditions. The LDO was implemented in a 65nm CMOS process with
1.5V input and 1.2V output voltages. The measured quiescent current is as low
as 3uA and 50uA for the 0-500uA and 5-15mA load current ranges, respectively.
An undershoot voltage of 100mV is observed when the load current switches from
0 to 15mA within 80ns, with a maximum current efficiency of 99.98%. Our design
achieved a low Figure-of-Merit of 0.21ps, outperforming state-of-the-art analog
LDOs.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18999" title="Abstract">arXiv:2310.18999</a> [<a href="/pdf/2310.18999" title="Download PDF">pdf</a>, <a href="/format/2310.18999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynPoint: Dynamic Neural Point For View Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaichen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+J">Jia-Xing Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Sangyun Shin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Kai Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Markham%2C+A">Andrew Markham</a>, 
<a href="/search/cs?searchtype=author&query=Trigoni%2C+N">Niki Trigoni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The introduction of neural radiance fields has greatly improved the
effectiveness of view synthesis for monocular videos. However, existing
algorithms face difficulties when dealing with uncontrolled or lengthy
scenarios, and require extensive training time specific to each new scenario.
To tackle these limitations, we propose DynPoint, an algorithm designed to
facilitate the rapid synthesis of novel views for unconstrained monocular
videos. Rather than encoding the entirety of the scenario information into a
latent representation, DynPoint concentrates on predicting the explicit 3D
correspondence between neighboring frames to realize information aggregation.
Specifically, this correspondence prediction is achieved through the estimation
of consistent depth and scene flow information across frames. Subsequently, the
acquired correspondence is utilized to aggregate information from multiple
reference frames to a target frame, by constructing hierarchical neural point
clouds. The resulting framework enables swift and accurate view synthesis for
desired views of target frames. The experimental results obtained demonstrate
the considerable acceleration of training time achieved - typically an order of
magnitude - by our proposed method while yielding comparable outcomes compared
to prior approaches. Furthermore, our method exhibits strong robustness in
handling long-duration videos without learning a canonical representation of
video content.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19000" title="Abstract">arXiv:2310.19000</a> [<a href="/pdf/2310.19000" title="Download PDF">pdf</a>, <a href="/format/2310.19000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Nonlinear Filtering using Triangular Transport Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grange%2C+D">Daniel Grange</a>, 
<a href="/search/cs?searchtype=author&query=Baptista%2C+R">Ricardo Baptista</a>, 
<a href="/search/cs?searchtype=author&query=Taghvaei%2C+A">Amirhossein Taghvaei</a>, 
<a href="/search/cs?searchtype=author&query=Tannenbaum%2C+A">Allen Tannenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Phillips%2C+S">Sean Phillips</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">The distributed filtering problem sequentially estimates a global state
variable using observations from a network of local sensors with different
measurement models. In this work, we introduce a novel methodology for
distributed nonlinear filtering by combining techniques from transportation of
measures, dimensionality reduction, and consensus algorithms. We illustrate our
methodology on a satellite pose estimation problem from a network of direct and
indirect observers. The numerical results serve as a proof of concept, offering
new venues for theoretical and applied research in the domain of distributed
filtering.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19001" title="Abstract">arXiv:2310.19001</a> [<a href="/pdf/2310.19001" title="Download PDF">pdf</a>, <a href="/format/2310.19001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianfei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyang Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hao He</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chaofan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianjiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiangchao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, Accept in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper studies the problem of weakly open-vocabulary semantic
segmentation (WOVSS), which learns to segment objects of arbitrary classes
using mere image-text pairs. Existing works turn to enhance the vanilla vision
transformer by introducing explicit grouping recognition, i.e., employing
several group tokens/centroids to cluster the image tokens and perform the
group-text alignment. Nevertheless, these methods suffer from a granularity
inconsistency regarding the usage of group tokens, which are aligned in the
all-to-one v.s. one-to-one manners during the training and inference phases,
respectively. We argue that this discrepancy arises from the lack of elaborate
supervision for each group token. To bridge this granularity gap, this paper
explores explicit supervision for the group tokens from the prototypical
knowledge. To this end, this paper proposes the non-learnable prototypical
regularization (NPR) where non-learnable prototypes are estimated from source
features to serve as supervision and enable contrastive matching of the group
tokens. This regularization encourages the group tokens to segment objects with
less redundancy and capture more comprehensive semantic regions, leading to
increased compactness and richness. Based on NPR, we propose the prototypical
guidance segmentation network (PGSeg) that incorporates multi-modal
regularization by leveraging prototypical sources from both images and texts at
different levels, progressively enhancing the segmentation capability with
diverse prototypical patterns. Experimental results show that our proposed
method achieves state-of-the-art performance on several benchmark datasets. The
source code is available at https://github.com/Ferenas/PGSeg.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19006" title="Abstract">arXiv:2310.19006</a> [<a href="/pdf/2310.19006" title="Download PDF">pdf</a>, <a href="/ps/2310.19006" title="Download PostScript">ps</a>, <a href="/format/2310.19006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Weisfeiler-Leman Dimension of Existential Conjunctive Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%B6bel%2C+A">Andreas G&#xf6;bel</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+L+A">Leslie Ann Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+M">Marc Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 4 figures, abstract shortened due to ArXiv requirements
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Databases (cs.DB); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">The Weisfeiler-Leman (WL) dimension of a graph parameter $f$ is the minimum
$k$ such that, if $G_1$ and $G_2$ are indistinguishable by the $k$-dimensional
WL-algorithm then $f(G_1)=f(G_2)$. The WL-dimension of $f$ is $\infty$ if no
such $k$ exists. We study the WL-dimension of graph parameters characterised by
the number of answers from a fixed conjunctive query to the graph. Given a
conjunctive query $\varphi$, we quantify the WL-dimension of the function that
maps every graph $G$ to the number of answers of $\varphi$ in $G$.
<br />The works of Dvor\'ak (J. Graph Theory 2010), Dell, Grohe, and Rattan (ICALP
2018), and Neuen (ArXiv 2023) have answered this question for full conjunctive
queries, which are conjunctive queries without existentially quantified
variables. For such queries $\varphi$, the WL-dimension is equal to the
treewidth of the Gaifman graph of $\varphi$.
<br />In this work, we give a characterisation that applies to all conjunctive
qureies. Given any conjunctive query $\varphi$, we prove that its WL-dimension
is equal to the semantic extension width $\mathsf{sew}(\varphi)$, a novel width
measure that can be thought of as a combination of the treewidth of $\varphi$
and its quantified star size, an invariant introduced by Durand and Mengel
(ICDT 2013) describing how the existentially quantified variables of $\varphi$
are connected with the free variables. Using the recently established
equivalence between the WL-algorithm and higher-order Graph Neural Networks
(GNNs) due to Morris et al. (AAAI 2019), we obtain as a consequence that the
function counting answers to a conjunctive query $\varphi$ cannot be computed
by GNNs of order smaller than $\mathsf{sew}(\varphi)$.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19007" title="Abstract">arXiv:2310.19007</a> [<a href="/pdf/2310.19007" title="Download PDF">pdf</a>, <a href="/format/2310.19007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Behavior Alignment via Reward Function Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Dhawal Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Chandak%2C+Y">Yash Chandak</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+S+M">Scott M. Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+P+S">Philip S. Thomas</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+B+C">Bruno Castro da Silva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> (Spotlight) Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Designing reward functions for efficiently guiding reinforcement learning
(RL) agents toward specific behaviors is a complex task. This is challenging
since it requires the identification of reward structures that are not sparse
and that avoid inadvertently inducing undesirable behaviors. Naively modifying
the reward structure to offer denser and more frequent feedback can lead to
unintended outcomes and promote behaviors that are not aligned with the
designer's intended goal. Although potential-based reward shaping is often
suggested as a remedy, we systematically investigate settings where deploying
it often significantly impairs performance. To address these issues, we
introduce a new framework that uses a bi-level objective to learn
\emph{behavior alignment reward functions}. These functions integrate auxiliary
rewards reflecting a designer's heuristics and domain knowledge with the
environment's primary rewards. Our approach automatically determines the most
effective way to blend these types of feedback, thereby enhancing robustness
against heuristic reward misspecification. Remarkably, it can also adapt an
agent's policy optimization process to mitigate suboptimalities resulting from
limitations and biases inherent in the underlying RL algorithms. We evaluate
our method's efficacy on a diverse set of tasks, from small-scale experiments
to high-dimensional control challenges. We investigate heuristic auxiliary
rewards of varying quality -- some of which are beneficial and others
detrimental to the learning process. Our results show that our framework offers
a robust and principled way to integrate designer-specified heuristics. It not
only addresses key shortcomings of existing approaches but also consistently
leads to high-performing solutions, even when given misaligned or
poorly-specified auxiliary reward functions.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19011" title="Abstract">arXiv:2310.19011</a> [<a href="/pdf/2310.19011" title="Download PDF">pdf</a>, <a href="/format/2310.19011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Test-Time Adaptation for Super-Resolution with Second-Order  Degradation and Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zeshuai Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuokun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+S">Shuaicheng Niu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+H">Thomas H. Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Mingkui Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image super-resolution (SR) aims to learn a mapping from low-resolution (LR)
to high-resolution (HR) using paired HR-LR training images. Conventional SR
methods typically gather the paired training data by synthesizing LR images
from HR images using a predetermined degradation model, e.g., Bicubic
down-sampling. However, the realistic degradation type of test images may
mismatch with the training-time degradation type due to the dynamic changes of
the real-world scenarios, resulting in inferior-quality SR images. To address
this, existing methods attempt to estimate the degradation model and train an
image-specific model, which, however, is quite time-consuming and impracticable
to handle rapidly changing domain shifts. Moreover, these methods largely
concentrate on the estimation of one degradation type (e.g., blur degradation),
overlooking other degradation types like noise and JPEG in real-world test-time
scenarios, thus limiting their practicality. To tackle these problems, we
present an efficient test-time adaptation framework for SR, named SRTTA, which
is able to quickly adapt SR models to test domains with different/unknown
degradation types. Specifically, we design a second-order degradation scheme to
construct paired data based on the degradation type of the test image, which is
predicted by a pre-trained degradation classifier. Then, we adapt the SR model
by implementing feature-level reconstruction learning from the initial test
image to its second-order degraded counterparts, which helps the SR model
generate plausible HR images. Extensive experiments are conducted on newly
synthesized corrupted DIV2K datasets with 8 different degradations and several
real-world datasets, demonstrating that our SRTTA framework achieves an
impressive improvement over existing methods with satisfying speed. The source
code is available at https://github.com/DengZeshuai/SRTTA.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19013" title="Abstract">arXiv:2310.19013</a> [<a href="/pdf/2310.19013" title="Download PDF">pdf</a>, <a href="/format/2310.19013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing simultaneous autoscaling for serverless cloud computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ship%2C+H">Harold Ship</a>, 
<a href="/search/cs?searchtype=author&query=Shindin%2C+E">Evgeny Shindin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Arroyo%2C+D">Diana Arroyo</a>, 
<a href="/search/cs?searchtype=author&query=Tantawi%2C+A">Asser Tantawi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">This paper explores resource allocation in serverless cloud computing
platforms and proposes an optimization approach for autoscaling systems.
Serverless computing relieves users from resource management tasks, enabling
focus on application functions. However, dynamic resource allocation and
function replication based on changing loads remain crucial. Typically,
autoscalers in these platforms utilize threshold-based mechanisms to adjust
function replicas independently. We model applications as interconnected graphs
of functions, where requests probabilistically traverse the graph, triggering
associated function execution. Our objective is to develop a control policy
that optimally allocates resources on servers, minimizing failed requests and
response time in reaction to load changes. Using a fluid approximation model
and Separated Continuous Linear Programming (SCLP), we derive an optimal
control policy that determines the number of resources per replica and the
required number of replicas over time. We evaluate our approach using a
simulation framework built with Python and simpy. Comparing against
threshold-based autoscaling, our approach demonstrates significant improvements
in average response times and failed requests, ranging from 15% to over 300% in
most cases. We also explore the impact of system and workload parameters on
performance, providing insights into the behavior of our optimization approach
under different conditions. Overall, our study contributes to advancing
resource allocation strategies, enhancing efficiency and reliability in
serverless cloud computing platforms.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19014" title="Abstract">arXiv:2310.19014</a> [<a href="/pdf/2310.19014" title="Download PDF">pdf</a>, <a href="/format/2310.19014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIS-aided Real-time Beam Tracking for a Mobile User via Bayesian  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Junshuo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Xiong%2C+R">Rujing Xiong</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+J">Jialong Lu</a>, 
<a href="/search/eess?searchtype=author&query=Mi%2C+T">Tiebin Mi</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+R+C">Robert C. Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The conventional beam management procedure mandates that the user equipment
(UE) periodically measure the received signal reference power (RSRP) and
transmit these measurements to the base station (BS). The challenge lies in
balancing the number of beams used: it should be large enough to identify
high-RSRP beams but small enough to minimize reporting overhead. This paper
investigates this essential performance-versus-overhead trade-off using
Bayesian optimization. The proposed approach represents the first application
of real-time beam tracking via Bayesian optimization in RIS-assisted
communication systems. Simulation results validate the effectiveness of this
scheme.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19018" title="Abstract">arXiv:2310.19018</a> [<a href="/pdf/2310.19018" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Securing Refugee Identity: A Literature Review on Blockchain-based Smart  Contract
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahad%2C+M+T">Md Taimur Ahad</a>, 
<a href="/search/cs?searchtype=author&query=Emon%2C+Y+R">Yousuf Rayhan Emon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Identity documentation for refugees is a complex process and crucial for host
nations. A secured identity management system ensures both security and the
efficient provision of services for the host nation and the donor
organizations. Realizing the benefits, a handful of studies enriched the
blockchain-based security identification for refugees. The research studies
presented the introductory, conceptual, and practical solution related to the
blockchain-based smart contract. There is a common agreement in the studies
that blockchain-based smart contract not only streamlines refugee identity
verification but also safeguards against unauthorized entries. Since it is a
technology as well, it has been essential to know the present status of the
technology in the social context. In such a situation it becomes essential to
review the existing research studies to provide insight for future studies. In
this study, we reviewed current studies using a thematic approach. Our findings
suggest researchers are more inclined to provide conceptual models as the
models are important in advancing technology; however, the models need to be
implemented for practical advances. However, the main contribution of this
study is that this study gathers current efforts in smart contract-based
refugee identity management. This study is important for the refugee host
nations as well as for stakeholders. Knowledge gained from the study is
expected to provide insight into how the technology can be developed using
existing theory and implementation frameworks.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19019" title="Abstract">arXiv:2310.19019</a> [<a href="/pdf/2310.19019" title="Download PDF">pdf</a>, <a href="/format/2310.19019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language  Modeling Likewise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+N">Nan He</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+H">Hanyu Lai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chenyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zirui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Junting Pan</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+R">Ruoyu Qin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Ruofan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Rui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Gangming Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Z">Zhaohui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shaoqing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Ding Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+M">Mingjie Zhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures, 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) exhibit impressive reasoning and data
augmentation capabilities in various NLP tasks. However, what about small
models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant
fundamentals, chain of thought, and common mistakes for most NLP samples, which
makes annotation more than just an answer, thus allowing other models to learn
"why" instead of just "what". The TeacherLM-7.1B model achieved a zero-shot
score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even
more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we
augmented 58 NLP datasets and taught various student models with different
parameters from OPT and BLOOM series in a multi-task setting. The experimental
results indicate that the data augmentation provided by TeacherLM has brought
significant benefits. We will release the TeacherLM series of models and
augmented datasets as open-source.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19024" title="Abstract">arXiv:2310.19024</a> [<a href="/pdf/2310.19024" title="Download PDF">pdf</a>, <a href="/format/2310.19024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPGAN-Control: A Controllable Fingerprint Generator for Training with  Synthetic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shoshan%2C+A">Alon Shoshan</a>, 
<a href="/search/cs?searchtype=author&query=Bhonker%2C+N">Nadav Bhonker</a>, 
<a href="/search/cs?searchtype=author&query=Baruch%2C+E+B">Emanuel Ben Baruch</a>, 
<a href="/search/cs?searchtype=author&query=Nizan%2C+O">Ori Nizan</a>, 
<a href="/search/cs?searchtype=author&query=Kviatkovsky%2C+I">Igor Kviatkovsky</a>, 
<a href="/search/cs?searchtype=author&query=Engelsma%2C+J">Joshua Engelsma</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+M">Manoj Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Medioni%2C+G">Gerard Medioni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Training fingerprint recognition models using synthetic data has recently
gained increased attention in the biometric community as it alleviates the
dependency on sensitive personal data. Existing approaches for fingerprint
generation are limited in their ability to generate diverse impressions of the
same finger, a key property for providing effective data for training
recognition models. To address this gap, we present FPGAN-Control, an identity
preserving image generation framework which enables control over the
fingerprint's image appearance (e.g., fingerprint type, acquisition device,
pressure level) of generated fingerprints. We introduce a novel appearance loss
that encourages disentanglement between the fingerprint's identity and
appearance properties. In our experiments, we used the publicly available NIST
SD302 (N2N) dataset for training the FPGAN-Control model. We demonstrate the
merits of FPGAN-Control, both quantitatively and qualitatively, in terms of
identity preservation level, degree of appearance control, and low
synthetic-to-real domain gap. Finally, training recognition models using only
synthetic datasets generated by FPGAN-Control lead to recognition accuracies
that are on par or even surpass models trained using real data. To the best of
our knowledge, this is the first work to demonstrate this.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19025" title="Abstract">arXiv:2310.19025</a> [<a href="/pdf/2310.19025" title="Download PDF">pdf</a>, <a href="/ps/2310.19025" title="Download PostScript">ps</a>, <a href="/format/2310.19025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Improved Relaxation for Oracle-Efficient Adversarial Contextual  Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banihashem%2C+K">Kiarash Banihashem</a>, 
<a href="/search/cs?searchtype=author&query=Hajiaghayi%2C+M">MohammadTaghi Hajiaghayi</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Suho Shin</a>, 
<a href="/search/cs?searchtype=author&query=Springer%2C+M">Max Springer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
<p class="mathjax">We present an oracle-efficient relaxation for the adversarial contextual
bandits problem, where the contexts are sequentially drawn i.i.d from a known
distribution and the cost sequence is chosen by an online adversary. Our
algorithm has a regret bound of
$O(T^{\frac{2}{3}}(K\log(|\Pi|))^{\frac{1}{3}})$ and makes at most $O(K)$ calls
per round to an offline optimization oracle, where $K$ denotes the number of
actions, $T$ denotes the number of rounds and $\Pi$ denotes the set of
policies. This is the first result to improve the prior best bound of
$O((TK)^{\frac{2}{3}}(\log(|\Pi|))^{\frac{1}{3}})$ as obtained by Syrgkanis et
al. at NeurIPS 2016, and the first to match the original bound of Langford and
Zhang at NeurIPS 2007 which was obtained for the stochastic case.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19029" title="Abstract">arXiv:2310.19029</a> [<a href="/pdf/2310.19029" title="Download PDF">pdf</a>, <a href="/format/2310.19029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALMA: Arabic Sense-Annotated Corpus and WSD Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jarrar%2C+M">Mustafa Jarrar</a>, 
<a href="/search/cs?searchtype=author&query=Malaysha%2C+S">Sanad Malaysha</a>, 
<a href="/search/cs?searchtype=author&query=Hammouda%2C+T">Tymaa Hammouda</a>, 
<a href="/search/cs?searchtype=author&query=Khalilia%2C+M">Mohammed Khalilia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">SALMA, the first Arabic sense-annotated corpus, consists of ~34K tokens,
which are all sense-annotated. The corpus is annotated using two different
sense inventories simultaneously (Modern and Ghani). SALMA novelty lies in how
tokens and senses are associated. Instead of linking a token to only one
intended sense, SALMA links a token to multiple senses and provides a score to
each sense. A smart web-based annotation tool was developed to support scoring
multiple senses against a given word. In addition to sense annotations, we also
annotated the corpus using six types of named entities. The quality of our
annotations was assessed using various metrics (Kappa, Linear Weighted Kappa,
Quadratic Weighted Kappa, Mean Average Error, and Root Mean Square Error),
which show very high inter-annotator agreement. To establish a Word Sense
Disambiguation baseline using our SALMA corpus, we developed an end-to-end Word
Sense Disambiguation system using Target Sense Verification. We used this
system to evaluate three Target Sense Verification models available in the
literature. Our best model achieved an accuracy with 84.2% using Modern and
78.7% using Ghani. The full corpus and the annotation tool are open-source and
publicly available at https://sina.birzeit.edu/salma/.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19034" title="Abstract">arXiv:2310.19034</a> [<a href="/pdf/2310.19034" title="Download PDF">pdf</a>, <a href="/format/2310.19034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArBanking77: Intent Detection Neural Model and a New Dataset in Modern  and Dialectical Arabic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jarrar%2C+M">Mustafa Jarrar</a>, 
<a href="/search/cs?searchtype=author&query=Birim%2C+A">Ahmet Birim</a>, 
<a href="/search/cs?searchtype=author&query=Khalilia%2C+M">Mohammed Khalilia</a>, 
<a href="/search/cs?searchtype=author&query=Erden%2C+M">Mustafa Erden</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+S">Sana Ghanem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper presents the ArBanking77, a large Arabic dataset for intent
detection in the banking domain. Our dataset was arabized and localized from
the original English Banking77 dataset, which consists of 13,083 queries to
ArBanking77 dataset with 31,404 queries in both Modern Standard Arabic (MSA)
and Palestinian dialect, with each query classified into one of the 77 classes
(intents). Furthermore, we present a neural model, based on AraBERT, fine-tuned
on ArBanking77, which achieved an F1-score of 0.9209 and 0.8995 on MSA and
Palestinian dialect, respectively. We performed extensive experimentation in
which we simulated low-resource settings, where the model is trained on a
subset of the data and augmented with noisy queries to simulate colloquial
terms, mistakes and misspellings found in real NLP systems, especially live
chat queries. The data and the models are publicly available at
https://sina.birzeit.edu/arbanking77.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19035" title="Abstract">arXiv:2310.19035</a> [<a href="/pdf/2310.19035" title="Download PDF">pdf</a>, <a href="/format/2310.19035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Invariant Graph Learning via Environment Augmentation Learn  Invariance?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaiwen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+B">Binghui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">James Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, 34 pages, 35 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Invariant graph representation learning aims to learn the invariance among
data from different environments for out-of-distribution generalization on
graphs. As the graph environment partitions are usually expensive to obtain,
augmenting the environment information has become the de facto approach.
However, the usefulness of the augmented environment information has never been
verified. In this work, we find that it is fundamentally impossible to learn
invariant graph representations via environment augmentation without additional
assumptions. Therefore, we develop a set of minimal assumptions, including
variation sufficiency and variation consistency, for feasible invariant graph
learning. We then propose a new framework Graph invAriant Learning Assistant
(GALA). GALA incorporates an assistant model that needs to be sensitive to
graph environment changes or distribution shifts. The correctness of the proxy
predictions by the assistant model hence can differentiate the variations in
spurious subgraphs. We show that extracting the maximally invariant subgraph to
the proxy predictions provably identifies the underlying invariant subgraph for
successful OOD generalization under the established minimal assumptions.
Extensive experiments on datasets including DrugOOD with various graph
distribution shifts confirm the effectiveness of GALA.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19038" title="Abstract">arXiv:2310.19038</a> [<a href="/pdf/2310.19038" title="Download PDF">pdf</a>, <a href="/format/2310.19038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Decision-Based Black-Box Adversarial Attack with Gradient  Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xingshuo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaotong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qimai Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xianchao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IJCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Decision-based methods have shown to be effective in black-box adversarial
attacks, as they can obtain satisfactory performance and only require to access
the final model prediction. Gradient estimation is a critical step in black-box
adversarial attacks, as it will directly affect the query efficiency. Recent
works have attempted to utilize gradient priors to facilitate score-based
methods to obtain better results. However, these gradient priors still suffer
from the edge gradient discrepancy issue and the successive iteration gradient
direction issue, thus are difficult to simply extend to decision-based methods.
In this paper, we propose a novel Decision-based Black-box Attack framework
with Gradient Priors (DBA-GP), which seamlessly integrates the data-dependent
gradient prior and time-dependent prior into the gradient estimation procedure.
First, by leveraging the joint bilateral filter to deal with each random
perturbation, DBA-GP can guarantee that the generated perturbations in edge
locations are hardly smoothed, i.e., alleviating the edge gradient discrepancy,
thus remaining the characteristics of the original image as much as possible.
Second, by utilizing a new gradient updating strategy to automatically adjust
the successive iteration gradient direction, DBA-GP can accelerate the
convergence speed, thus improving the query efficiency. Extensive experiments
have demonstrated that the proposed method outperforms other strong baselines
significantly.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19046" title="Abstract">arXiv:2310.19046</a> [<a href="/pdf/2310.19046" title="Download PDF">pdf</a>, <a href="/format/2310.19046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as Evolutionary Optimizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengcai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Caishun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xinghua Qu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Ke Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+Y">Yew-Soon Ong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Evolutionary algorithms (EAs) have achieved remarkable success in tackling
complex combinatorial optimization problems. However, EAs often demand
carefully-designed operators with the aid of domain expertise to achieve
satisfactory performance. In this work, we present the first study on large
language models (LLMs) as evolutionary combinatorial optimizers. The main
advantage is that it requires minimal domain knowledge and human efforts, as
well as no additional training of the model. This approach is referred to as
LLM-driven EA (LMEA). Specifically, in each generation of the evolutionary
search, LMEA instructs the LLM to select parent solutions from current
population, and perform crossover and mutation to generate offspring solutions.
Then, LMEA evaluates these new solutions and include them into the population
for the next generation. LMEA is equipped with a self-adaptation mechanism that
controls the temperature of the LLM. This enables it to balance between
exploration and exploitation and prevents the search from getting stuck in
local optima. We investigate the power of LMEA on the classical traveling
salesman problems (TSPs) widely used in combinatorial optimization research.
Notably, the results show that LMEA performs competitively to traditional
heuristics in finding high-quality solutions on TSP instances with up to 20
nodes. Additionally, we also study the effectiveness of LLM-driven
crossover/mutation and the self-adaptation mechanism in evolutionary search. In
summary, our results reveal the great potentials of LLMs as evolutionary
optimizers for solving combinatorial problems. We hope our research shall
inspire future explorations on LLM-driven EAs for complex optimization
challenges.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19049" title="Abstract">arXiv:2310.19049</a> [<a href="/pdf/2310.19049" title="Download PDF">pdf</a>, <a href="/format/2310.19049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimation of Semiconductor Power Losses Through Automatic Thermal  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sanz-Alcaine%2C+J+M">Jose Miguel Sanz-Alcaine</a>, 
<a href="/search/eess?searchtype=author&query=Sebastian%2C+E">Eduardo Sebastian</a>, 
<a href="/search/eess?searchtype=author&query=Perez-Cebolla%2C+F+J">Francisco Jose Perez-Cebolla</a>, 
<a href="/search/eess?searchtype=author&query=Arruti%2C+A">Asier Arruti</a>, 
<a href="/search/eess?searchtype=author&query=Bernal-Ruiz%2C+C">Carlos Bernal-Ruiz</a>, 
<a href="/search/eess?searchtype=author&query=Aizpuru%2C+I">Iosu Aizpuru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The optimal design of power converters requires accurate knowledge of the
dissipation elements of its system to achieve the desired performance and
security requirements. Calorimetric methods have surpassed classical electrical
methods for the estimation of semiconductor power losses but have mechanical
limitations and resort to analytical electrothermal equivalent circuits for
this task. These electrothermal models are highly dependent on the topology and
technology used on the power converter leading to either simplifications that
underestimate the thermal effects or intractable sets of differential
equations. To overcome these issues, we propose a novel data-driven
identification method to characterize the thermal dynamics of power converters
allowing the designer to obtain semiconductor total power losses only by means
of temperature measurements without the need of a calorimeter. Given a set of
power vs.temperature profiles, our solution identifies the linear model that
best fits the data. The solution is based on an optimization problem that
allows not only accurate identification but also coding of the desired modeling
requirements, such as dynamics' invertibility to allow the estimation of power
losses from the temperature profiles. The proposed methodology can be applied
to any power converter topology. Furthermore, by obtaining a linear model,
standard control theory techniques can be exploited to analyze and control the
thermal dynamics. Real experiments validate the generality and accuracy of the
proposal.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19052" title="Abstract">arXiv:2310.19052</a> [<a href="/pdf/2310.19052" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Emotional Landscape of Music: An Analysis of Valence  Trends and Genre Variations in Spotify Music Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Shruti Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Mookherjee%2C+S">Shashwat Mookherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, Accepted at the 18th International Conference for Internet Technology and Secured Transactions, 13-15 November, 2023, St Anne's College, Oxford, UK
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper conducts an intricate analysis of musical emotions and trends
using Spotify music data, encompassing audio features and valence scores
extracted through the Spotipi API. Employing regression modeling, temporal
analysis, mood transitions, and genre investigation, the study uncovers
patterns within music-emotion relationships. Regression models linear, support
vector, random forest, and ridge, are employed to predict valence scores.
Temporal analysis reveals shifts in valence distribution over time, while mood
transition exploration illuminates emotional dynamics within playlists. The
research contributes to nuanced insights into music's emotional fabric,
enhancing comprehension of the interplay between music and emotions through
years.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19053" title="Abstract">arXiv:2310.19053</a> [<a href="/pdf/2310.19053" title="Download PDF">pdf</a>, <a href="/format/2310.19053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Datasets and Benchmarks for Nanophotonic Structure and Parametric Design  Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jungtaek Kim</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hinder%2C+O">Oliver Hinder</a>, 
<a href="/search/cs?searchtype=author&query=Leu%2C+P+W">Paul W. Leu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 31 figures, 4 tables. Accepted at the 37th Conference on Neural Information Processing Systems (NeurIPS 2023), Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optics (physics.optics); Machine Learning (stat.ML)

</div>
<p class="mathjax">Nanophotonic structures have versatile applications including solar cells,
anti-reflective coatings, electromagnetic interference shielding, optical
filters, and light emitting diodes. To design and understand these nanophotonic
structures, electrodynamic simulations are essential. These simulations enable
us to model electromagnetic fields over time and calculate optical properties.
In this work, we introduce frameworks and benchmarks to evaluate nanophotonic
structures in the context of parametric structure design problems. The
benchmarks are instrumental in assessing the performance of optimization
algorithms and identifying an optimal structure based on target optical
properties. Moreover, we explore the impact of varying grid sizes in
electrodynamic simulations, shedding light on how evaluation fidelity can be
strategically leveraged in enhancing structure designs.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19054" title="Abstract">arXiv:2310.19054</a> [<a href="/pdf/2310.19054" title="Download PDF">pdf</a>, <a href="/format/2310.19054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object-centric architectures enable efficient causal representation  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mansouri%2C+A">Amin Mansouri</a>, 
<a href="/search/cs?searchtype=author&query=Hartford%2C+J">Jason Hartford</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Causal representation learning has showed a variety of settings in which we
can disentangle latent variables with identifiability guarantees (up to some
reasonable equivalence class). Common to all of these approaches is the
assumption that (1) the latent variables are represented as $d$-dimensional
vectors, and (2) that the observations are the output of some injective
generative function of these latent variables. While these assumptions appear
benign, we show that when the observations are of multiple objects, the
generative function is no longer injective and disentanglement fails in
practice. We can address this failure by combining recent developments in
object-centric learning and causal representation learning. By modifying the
Slot Attention architecture <a href="/abs/2006.15055">arXiv:2006.15055</a>, we develop an object-centric
architecture that leverages weak supervision from sparse perturbations to
disentangle each object's properties. This approach is more data-efficient in
the sense that it requires significantly fewer perturbations than a comparable
approach that encodes to a Euclidean space and we show that this approach
successfully disentangles the properties of a set of objects in a series of
simple image-based disentanglement experiments.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19055" title="Abstract">arXiv:2310.19055</a> [<a href="/pdf/2310.19055" title="Download PDF">pdf</a>, <a href="/format/2310.19055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Recent Named Entity Recognition and Relation Classification  Methods with Focus on Few-Shot Learning Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alqaaidi%2C+S">Sakher Alqaaidi</a>, 
<a href="/search/cs?searchtype=author&query=Bozorgi%2C+E">Elika Bozorgi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Named entity recognition and relation classification are key stages for
extracting information from unstructured text. Several natural language
processing applications utilize the two tasks, such as information retrieval,
knowledge graph construction and completion, question answering and other
domain-specific applications, such as biomedical data mining. We present a
survey of recent approaches in the two tasks with focus on few-shot learning
approaches. Our work compares the main approaches followed in the two
paradigms. Additionally, we report the latest metric scores in the two tasks
with a structured analysis that considers the results in the few-shot learning
scope.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19056" title="Abstract">arXiv:2310.19056</a> [<a href="/pdf/2310.19056" title="Download PDF">pdf</a>, <a href="/format/2310.19056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MILL: Mutual Verification with Large Language Models for Zero-Shot Query  Expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+P">Pengyue Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiding Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaopeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+C">Changying Hao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Query expansion is a commonly-used technique in many search systems to better
represent users' information needs with additional query terms. Existing
studies for this task usually propose to expand a query with retrieved or
generated contextual documents. However, both types of methods have clear
limitations. For retrieval-based methods, the documents retrieved with the
original query might not be accurate enough to reveal the search intent,
especially when the query is brief or ambiguous. For generation-based methods,
existing models can hardly be trained or aligned on a particular corpus, due to
the lack of corpus-specific labeled data. In this paper, we propose a novel
Large Language Model (LLM) based mutual verification framework for query
expansion, which alleviates the aforementioned limitations. Specifically, we
first design a query-query-document generation pipeline, which can effectively
leverage the contextual knowledge encoded in LLMs to generate sub-queries and
corresponding documents from multiple perspectives. Next, we employ a mutual
verification method for both generated and retrieved contextual documents,
where 1) retrieved documents are filtered with the external contextual
knowledge in generated documents, and 2) generated documents are filtered with
the corpus-specific knowledge in retrieved documents. Overall, the proposed
method allows retrieved and generated documents to complement each other to
finalize a better query expansion. We conduct extensive experiments on three
information retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO.
The results demonstrate that our method outperforms other baselines
significantly.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19057" title="Abstract">arXiv:2310.19057</a> [<a href="/pdf/2310.19057" title="Download PDF">pdf</a>, <a href="/format/2310.19057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unique Training Strategy to Enhance Language Models Capabilities for  Health Mention Detection from Social Media Content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+P+I">Pervaiz Iqbal Khan</a>, 
<a href="/search/cs?searchtype=author&query=Asim%2C+M+N">Muhammad Nabeel Asim</a>, 
<a href="/search/cs?searchtype=author&query=Dengel%2C+A">Andreas Dengel</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+S">Sheraz Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">An ever-increasing amount of social media content requires advanced AI-based
computer programs capable of extracting useful information. Specifically, the
extraction of health-related content from social media is useful for the
development of diverse types of applications including disease spread,
mortality rate prediction, and finding the impact of diverse types of drugs on
diverse types of diseases. Language models are competent in extracting the
syntactic and semantics of text. However, they face a hard time extracting
similar patterns from social media texts. The primary reason for this shortfall
lies in the non-standardized writing style commonly employed by social media
users. Following the need for an optimal language model competent in extracting
useful patterns from social media text, the key goal of this paper is to train
language models in such a way that they learn to derive generalized patterns.
The key goal is achieved through the incorporation of random weighted
perturbation and contrastive learning strategies. On top of a unique training
strategy, a meta predictor is proposed that reaps the benefits of 5 different
language models for discriminating posts of social media text into non-health
and health-related classes. Comprehensive experimentation across 3 public
benchmark datasets reveals that the proposed training strategy improves the
performance of the language models up to 3.87%, in terms of F1-score, as
compared to their performance with traditional training. Furthermore, the
proposed meta predictor outperforms existing health mention classification
predictors across all 3 benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19059" title="Abstract">arXiv:2310.19059</a> [<a href="/pdf/2310.19059" title="Download PDF">pdf</a>, <a href="/format/2310.19059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Escaping Saddle Points in Heterogeneous Federated Learning via  Distributed SGD with Communication Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sijin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhize Li</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC)

</div>
<p class="mathjax">We consider the problem of finding second-order stationary points of
heterogeneous federated learning (FL). Previous works in FL mostly focus on
first-order convergence guarantees, which do not rule out the scenario of
unstable saddle points. Meanwhile, it is a key bottleneck of FL to achieve
communication efficiency without compensating the learning accuracy, especially
when local data are highly heterogeneous across different clients. Given this,
we propose a novel algorithm Power-EF that only communicates compressed
information via a novel error-feedback scheme. To our knowledge, Power-EF is
the first distributed and compressed SGD algorithm that provably escapes saddle
points in heterogeneous FL without any data homogeneity assumptions. In
particular, Power-EF improves to second-order stationary points after visiting
first-order (possibly saddle) points, using additional gradient queries and
communication rounds only of almost the same order required by first-order
convergence, and the convergence rate exhibits a linear speedup in terms of the
number of workers. Our theory improves/recovers previous results, while
extending to much more tolerant settings on the local data. Numerical
experiments are provided to complement the theory.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19060" title="Abstract">arXiv:2310.19060</a> [<a href="/pdf/2310.19060" title="Download PDF">pdf</a>, <a href="/format/2310.19060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shuhuai Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sishuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shicheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lu Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures, code is available at <a href="https://github.com/RenShuhuai-Andy/TESTA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large-scale video-language pre-training has made remarkable strides in
advancing video-language understanding tasks. However, the heavy computational
burden of video encoding remains a formidable efficiency bottleneck,
particularly for long-form videos. These videos contain massive visual tokens
due to their inherent 3D properties and spatiotemporal redundancy, making it
challenging to capture complex temporal and spatial relationships. To tackle
this issue, we propose an efficient method called TEmporal-Spatial Token
Aggregation (TESTA). TESTA condenses video semantics by adaptively aggregating
similar frames, as well as similar patches within each frame. TESTA can reduce
the number of visual tokens by 75% and thus accelerate video encoding. Building
upon TESTA, we introduce a pre-trained video-language model equipped with a
divided space-time token aggregation module in each video encoder block. We
evaluate our model on five datasets for paragraph-to-video retrieval and
long-form VideoQA tasks. Experimental results show that TESTA improves
computing efficiency by 1.7 times, and achieves significant performance gains
from its scalability in processing longer input frames, e.g., +13.7 R@1 on
QuerYD and +6.5 R@1 on Condensed Movie.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19061" title="Abstract">arXiv:2310.19061</a> [<a href="/pdf/2310.19061" title="Download PDF">pdf</a>, <a href="/format/2310.19061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal ChatGPT for Medical Applications: an Experimental Study of  GPT-4V
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiling Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Rong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lifang He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we critically evaluate the capabilities of the
state-of-the-art multimodal large language model, i.e., GPT-4 with Vision
(GPT-4V), on Visual Question Answering (VQA) task. Our experiments thoroughly
assess GPT-4V's proficiency in answering questions paired with images using
both pathology and radiology datasets from 11 modalities (e.g. Microscopy,
Dermoscopy, X-ray, CT, etc.) and fifteen objects of interests (brain, liver,
lung, etc.). Our datasets encompass a comprehensive range of medical inquiries,
including sixteen distinct question types. Throughout our evaluations, we
devised textual prompts for GPT-4V, directing it to synergize visual and
textual information. The experiments with accuracy score conclude that the
current version of GPT-4V is not recommended for real-world diagnostics due to
its unreliable and suboptimal accuracy in responding to diagnostic medical
questions. In addition, we delineate seven unique facets of GPT-4V's behavior
in medical VQA, highlighting its constraints within this complex arena. The
complete details of our evaluation cases are accessible at
https://github.com/ZhilingYan/GPT4V-Medical-Report.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19062" title="Abstract">arXiv:2310.19062</a> [<a href="/pdf/2310.19062" title="Download PDF">pdf</a>, <a href="/format/2310.19062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A multi-modal table tennis robot system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziegler%2C+A">Andreas Ziegler</a>, 
<a href="/search/cs?searchtype=author&query=Gossard%2C+T">Thomas Gossard</a>, 
<a href="/search/cs?searchtype=author&query=Vetter%2C+K">Karl Vetter</a>, 
<a href="/search/cs?searchtype=author&query=Tebbe%2C+J">Jonas Tebbe</a>, 
<a href="/search/cs?searchtype=author&query=Zell%2C+A">Andreas Zell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for RoboLetics: Workshop on Robot Learning in Athletics @CoRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, robotic table tennis has become a popular research challenge
for perception and robot control. Here, we present an improved table tennis
robot system with high accuracy vision detection and fast robot reaction. Based
on previous work, our system contains a KUKA robot arm with 6 DOF, with four
frame-based cameras and two additional event-based cameras. We developed a
novel calibration approach to calibrate this multimodal perception system. For
table tennis, spin estimation is crucial. Therefore, we introduced a novel, and
more accurate spin estimation approach. Finally, we show how combining the
output of an event-based camera and a Spiking Neural Network (SNN) can be used
for accurate ball detection.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19063" title="Abstract">arXiv:2310.19063</a> [<a href="/pdf/2310.19063" title="Download PDF">pdf</a>, <a href="/format/2310.19063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Aggregation in Joint Sound Classification and Localization  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Healy%2C+B">Brendan Healy</a>, 
<a href="/search/cs?searchtype=author&query=McNamee%2C+P">Patrick McNamee</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadabadi%2C+Z+N">Zahra Nili Ahmadabadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This study addresses the application of deep learning techniques in joint
sound signal classification and localization networks. Current state-of-the-art
sound source localization deep learning networks lack feature aggregation
within their architecture. Feature aggregation enhances model performance by
enabling the consolidation of information from different feature scales,
thereby improving feature robustness and invariance. This is particularly
important in SSL networks, which must differentiate direct and indirect
acoustic signals. To address this gap, we adapt feature aggregation techniques
from computer vision neural networks to signal detection neural networks.
Additionally, we propose the Scale Encoding Network (SEN) for feature
aggregation to encode features from various scales, compressing the network for
more computationally efficient aggregation. To evaluate the efficacy of feature
aggregation in SSL networks, we integrated the following computer vision
feature aggregation sub-architectures into a SSL control architecture: Path
Aggregation Network (PANet), Weighted Bi-directional Feature Pyramid Network
(BiFPN), and SEN. These sub-architectures were evaluated using two metrics for
signal classification and two metrics for direction-of-arrival regression.
PANet and BiFPN are established aggregators in computer vision models, while
the proposed SEN is a more compact aggregator. The results suggest that models
incorporating feature aggregations outperformed the control model, the Sound
Event Localization and Detection network (SELDnet), in both sound signal
classification and localization. The feature aggregation techniques enhance the
performance of sound detection neural networks, particularly in
direction-of-arrival regression.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19064" title="Abstract">arXiv:2310.19064</a> [<a href="/pdf/2310.19064" title="Download PDF">pdf</a>, <a href="/format/2310.19064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Learnability of Apple Tasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raman%2C+V">Vinod Raman</a>, 
<a href="/search/cs?searchtype=author&query=Subedi%2C+U">Unique Subedi</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+A">Ananth Raman</a>, 
<a href="/search/cs?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In online binary classification under \textit{apple tasting} feedback, the
learner only observes the true label if it predicts "1". First studied by
\cite{helmbold2000apple}, we revisit this classical partial-feedback setting
and study online learnability from a combinatorial perspective. We show that
the Littlestone dimension continues to prove a tight quantitative
characterization of apple tasting in the agnostic setting, closing an open
question posed by \cite{helmbold2000apple}. In addition, we give a new
combinatorial parameter, called the Effective width, that tightly quantifies
the minimax expected mistakes in the realizable setting. As a corollary, we use
the Effective width to establish a \textit{trichotomy} of the minimax expected
number of mistakes in the realizable setting. In particular, we show that in
the realizable setting, the expected number of mistakes for any learner under
apple tasting feedback can only be $\Theta(1), \Theta(\sqrt{T})$, or
$\Theta(T)$.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19065" title="Abstract">arXiv:2310.19065</a> [<a href="/pdf/2310.19065" title="Download PDF">pdf</a>, <a href="/format/2310.19065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating LLP Methods: Challenges and Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Franco%2C+G">Gabriel Franco</a>, 
<a href="/search/cs?searchtype=author&query=Comarela%2C+G">Giovanni Comarela</a>, 
<a href="/search/cs?searchtype=author&query=Crovella%2C+M">Mark Crovella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Learning from Label Proportions (LLP) is an established machine learning
problem with numerous real-world applications. In this setting, data items are
grouped into bags, and the goal is to learn individual item labels, knowing
only the features of the data and the proportions of labels in each bag.
Although LLP is a well-established problem, it has several unusual aspects that
create challenges for benchmarking learning methods. Fundamental complications
arise because of the existence of different LLP variants, i.e., dependence
structures that can exist between items, labels, and bags. Accordingly, the
first algorithmic challenge is the generation of variant-specific datasets
capturing the diversity of dependence structures and bag characteristics. The
second methodological challenge is model selection, i.e., hyperparameter
tuning; due to the nature of LLP, model selection cannot easily use the
standard machine learning paradigm. The final benchmarking challenge consists
of properly evaluating LLP solution methods across various LLP variants. We
note that there is very little consideration of these issues in prior work, and
there are no general solutions for these challenges proposed to date. To
address these challenges, we develop methods capable of generating LLP datasets
meeting the requirements of different variants. We use these methods to
generate a collection of datasets encompassing the spectrum of LLP problem
characteristics, which can be used in future evaluation studies. Additionally,
we develop guidelines for benchmarking LLP algorithms, including the model
selection and evaluation steps. Finally, we illustrate the new methods and
guidelines by performing an extensive benchmark of a set of well-known LLP
algorithms. We show that choosing the best algorithm depends critically on the
LLP variant and model selection method, demonstrating the need for our proposed
approach.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19066" title="Abstract">arXiv:2310.19066</a> [<a href="/pdf/2310.19066" title="Download PDF">pdf</a>, <a href="/format/2310.19066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gauge-optimal approximate learning for small data classification  problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vecchi%2C+E">Edoardo Vecchi</a>, 
<a href="/search/cs?searchtype=author&query=Bassetti%2C+D">Davide Bassetti</a>, 
<a href="/search/cs?searchtype=author&query=Graziato%2C+F">Fabio Graziato</a>, 
<a href="/search/cs?searchtype=author&query=Pospisil%2C+L">Lukas Pospisil</a>, 
<a href="/search/cs?searchtype=author&query=Horenko%2C+I">Illia Horenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Small data learning problems are characterized by a significant discrepancy
between the limited amount of response variable observations and the large
feature space dimension. In this setting, the common learning tools struggle to
identify the features important for the classification task from those that
bear no relevant information, and cannot derive an appropriate learning rule
which allows to discriminate between different classes. As a potential solution
to this problem, here we exploit the idea of reducing and rotating the feature
space in a lower-dimensional gauge and propose the Gauge-Optimal Approximate
Learning (GOAL) algorithm, which provides an analytically tractable joint
solution to the dimension reduction, feature segmentation and classification
problems for small data learning problems. We prove that the optimal solution
of the GOAL algorithm consists in piecewise-linear functions in the Euclidean
space, and that it can be approximated through a monotonically convergent
algorithm which presents -- under the assumption of a discrete segmentation of
the feature space -- a closed-form solution for each optimization substep and
an overall linear iteration cost scaling. The GOAL algorithm has been compared
to other state-of-the-art machine learning (ML) tools on both synthetic data
and challenging real-world applications from climate science and bioinformatics
(i.e., prediction of the El Nino Southern Oscillation and inference of
epigenetically-induced gene-activity networks from limited experimental data).
The experimental results show that the proposed algorithm outperforms the
reported best competitors for these problems both in learning performance and
computational cost.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19067" title="Abstract">arXiv:2310.19067</a> [<a href="/pdf/2310.19067" title="Download PDF">pdf</a>, <a href="/format/2310.19067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expanding memory in recurrent spiking networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balafrej%2C+I">Ismael Balafrej</a>, 
<a href="/search/cs?searchtype=author&query=Alibart%2C+F">Fabien Alibart</a>, 
<a href="/search/cs?searchtype=author&query=Rouat%2C+J">Jean Rouat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Recurrent spiking neural networks (RSNNs) are notoriously difficult to train
because of the vanishing gradient problem that is enhanced by the binary nature
of the spikes. In this paper, we review the ability of the current
state-of-the-art RSNNs to solve long-term memory tasks, and show that they have
strong constraints both in performance, and for their implementation on
hardware analog neuromorphic processors. We present a novel spiking neural
network that circumvents these limitations. Our biologically inspired neural
network uses synaptic delays, branching factor regularization and a novel
surrogate derivative for the spiking function. The proposed network proves to
be more successful in using the recurrent connections on memory tasks.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19068" title="Abstract">arXiv:2310.19068</a> [<a href="/pdf/2310.19068" title="Download PDF">pdf</a>, <a href="/ps/2310.19068" title="Download PostScript">ps</a>, <a href="/format/2310.19068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sketching Algorithms for Sparse Dictionary Learning: PTAS and Turnstile  Streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dexter%2C+G">Gregory Dexter</a>, 
<a href="/search/cs?searchtype=author&query=Drineas%2C+P">Petros Drineas</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+D+P">David P. Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Yasuda%2C+T">Taisuke Yasuda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Sketching algorithms have recently proven to be a powerful approach both for
designing low-space streaming algorithms as well as fast polynomial time
approximation schemes (PTAS). In this work, we develop new techniques to extend
the applicability of sketching-based approaches to the sparse dictionary
learning and the Euclidean $k$-means clustering problems. In particular, we
initiate the study of the challenging setting where the dictionary/clustering
assignment for each of the $n$ input points must be output, which has
surprisingly received little attention in prior work. On the fast algorithms
front, we obtain a new approach for designing PTAS's for the $k$-means
clustering problem, which generalizes to the first PTAS for the sparse
dictionary learning problem. On the streaming algorithms front, we obtain new
upper bounds and lower bounds for dictionary learning and $k$-means clustering.
In particular, given a design matrix $\mathbf A\in\mathbb R^{n\times d}$ in a
turnstile stream, we show an $\tilde O(nr/\epsilon^2 + dk/\epsilon)$ space
upper bound for $r$-sparse dictionary learning of size $k$, an $\tilde
O(n/\epsilon^2 + dk/\epsilon)$ space upper bound for $k$-means clustering, as
well as an $\tilde O(n)$ space upper bound for $k$-means clustering on random
order row insertion streams with a natural "bounded sensitivity" assumption. On
the lower bounds side, we obtain a general $\tilde\Omega(n/\epsilon +
dk/\epsilon)$ lower bound for $k$-means clustering, as well as an
$\tilde\Omega(n/\epsilon^2)$ lower bound for algorithms which can estimate the
cost of a single fixed set of candidate centers.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19069" title="Abstract">arXiv:2310.19069</a> [<a href="/pdf/2310.19069" title="Download PDF">pdf</a>, <a href="/format/2310.19069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Cluster Selection for Personalized Federated Learning: A  Multi-Armed Bandit Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+Z">Zhou Ni</a>, 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+M">Morteza Hashemi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning (FL) offers a decentralized training approach for machine
learning models, prioritizing data privacy. However, the inherent heterogeneity
in FL networks, arising from variations in data distribution, size, and device
capabilities, poses challenges in user federation. Recognizing this,
Personalized Federated Learning (PFL) emphasizes tailoring learning processes
to individual data profiles. In this paper, we address the complexity of
clustering users in PFL, especially in dynamic networks, by introducing a
dynamic Upper Confidence Bound (dUCB) algorithm inspired by the multi-armed
bandit (MAB) approach. The dUCB algorithm ensures that new users can
effectively find the best cluster for their data distribution by balancing
exploration and exploitation. The performance of our algorithm is evaluated in
various cases, showing its effectiveness in handling dynamic federated learning
scenarios.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19070" title="Abstract">arXiv:2310.19070</a> [<a href="/pdf/2310.19070" title="Download PDF">pdf</a>, <a href="/format/2310.19070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Myriad: Large Multimodal Model by Applying Vision Experts for Industrial  Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanze Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haolin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shihao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guangming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing industrial anomaly detection (IAD) methods predict anomaly scores
for both anomaly detection and localization. However, they struggle to perform
a multi-turn dialog and detailed descriptions for anomaly regions, e.g., color,
shape, and categories of industrial anomalies. Recently, large multimodal
(i.e., vision and language) models (LMMs) have shown eminent perception
abilities on multiple vision tasks such as image captioning, visual
understanding, visual reasoning, etc., making it a competitive potential choice
for more comprehensible anomaly detection. However, the knowledge about anomaly
detection is absent in existing general LMMs, while training a specific LMM for
anomaly detection requires a tremendous amount of annotated data and massive
computation resources. In this paper, we propose a novel large multi-modal
model by applying vision experts for industrial anomaly detection (dubbed
Myriad), which leads to definite anomaly detection and high-quality anomaly
description. Specifically, we adopt MiniGPT-4 as the base LMM and design an
Expert Perception module to embed the prior knowledge from vision experts as
tokens which are intelligible to Large Language Models (LLMs). To compensate
for the errors and confusions of vision experts, we introduce a domain adapter
to bridge the visual representation gaps between generic and industrial images.
Furthermore, we propose a Vision Expert Instructor, which enables the Q-Former
to generate IAD domain vision-language tokens according to vision expert prior.
Extensive experiments on MVTec-AD and VisA benchmarks demonstrate that our
proposed method not only performs favorably against state-of-the-art methods
under the 1-class and few-shot settings, but also provide definite anomaly
prediction along with detailed descriptions in IAD domain.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19075" title="Abstract">arXiv:2310.19075</a> [<a href="/pdf/2310.19075" title="Download PDF">pdf</a>, <a href="/format/2310.19075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bespoke Solvers for Generative Flow Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaul%2C+N">Neta Shaul</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+J">Juan Perez</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R+T+Q">Ricky T. Q. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Thabet%2C+A">Ali Thabet</a>, 
<a href="/search/cs?searchtype=author&query=Pumarola%2C+A">Albert Pumarola</a>, 
<a href="/search/cs?searchtype=author&query=Lipman%2C+Y">Yaron Lipman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diffusion or flow-based models are powerful generative paradigms that are
notoriously hard to sample as samples are defined as solutions to
high-dimensional Ordinary or Stochastic Differential Equations (ODEs/SDEs)
which require a large Number of Function Evaluations (NFE) to approximate well.
Existing methods to alleviate the costly sampling process include model
distillation and designing dedicated ODE solvers. However, distillation is
costly to train and sometimes can deteriorate quality, while dedicated solvers
still require relatively large NFE to produce high quality samples. In this
paper we introduce "Bespoke solvers", a novel framework for constructing custom
ODE solvers tailored to the ODE of a given pre-trained flow model. Our approach
optimizes an order consistent and parameter-efficient solver (e.g., with 80
learnable parameters), is trained for roughly 1% of the GPU time required for
training the pre-trained model, and significantly improves approximation and
generation quality compared to dedicated solvers. For example, a Bespoke solver
for a CIFAR10 model produces samples with Fr\'echet Inception Distance (FID) of
2.73 with 10 NFE, and gets to 1% of the Ground Truth (GT) FID (2.59) for this
model with only 20 NFE. On the more challenging ImageNet-64$\times$64, Bespoke
samples at 2.2 FID with 10 NFE, and gets within 2% of GT FID (1.71) with 20
NFE.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19077" title="Abstract">arXiv:2310.19077</a> [<a href="/pdf/2310.19077" title="Download PDF">pdf</a>, <a href="/format/2310.19077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Packet Scheduling in Multihop Networks with End-to-End  Deadline Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsanikidis%2C+C">Christos Tsanikidis</a>, 
<a href="/search/cs?searchtype=author&query=Ghaderi%2C+J">Javad Ghaderi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Scheduling packets with end-to-end deadline constraints in multihop networks
is an important problem that has been notoriously difficult to tackle.
Recently, there has been progress on this problem in the worst-case traffic
setting, with the objective of maximizing the number of packets delivered
within their deadlines. Specifically, the proposed algorithms were shown to
achieve $\Omega(1/\log(L))$ fraction of the optimal objective value if the
minimum link capacity in the network is $C_{\min}=\Omega(\log (L))$, where $L$
is the maximum length of a packet's route in the network (which is bounded by
the packet's maximum deadline). However, such guarantees can be quite
pessimistic due to the strict worst-case traffic assumption and may not
accurately reflect real-world settings. In this work, we aim to address this
limitation by exploring whether it is possible to design algorithms that
achieve a constant fraction of the optimal value while relaxing the worst-case
traffic assumption.
<br />We provide a positive answer by demonstrating that in stochastic traffic
settings, such as i.i.d. packet arrivals, near-optimal,
$(1-\epsilon)$-approximation algorithms can be designed if $C_{\min} =
\Omega\big(\frac{\log (L/\epsilon) } {\epsilon^2}\big)$. To the best of our
knowledge, this is the first result that shows this problem can be solved
near-optimally under nontrivial assumptions on traffic and link capacity. We
further present extended simulations using real network traces with
non-stationary traffic, which demonstrate that our algorithms outperform
worst-case-based algorithms in practical settings.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19079" title="Abstract">arXiv:2310.19079</a> [<a href="/pdf/2310.19079" title="Download PDF">pdf</a>, <a href="/format/2310.19079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Twin-Driven Network Architecture for Video Streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haojun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shisheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xuemin Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, submitted to IEEE Network Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Digital twin (DT) is revolutionizing the emerging video streaming services
through tailored network management. By integrating diverse advanced
communication technologies, DTs are promised to construct a holistic
virtualized network for better network management performance. To this end, we
develop a DT-driven network architecture for video streaming (DTN4VS) to enable
network virtualization and tailored network management. With the architecture,
various types of DTs can characterize physical entities' status, separate the
network management functions from the network controller, and empower the
functions with emulated data and tailored strategies. To further enhance
network management performance, three potential approaches are proposed, i.e.,
domain data exploitation, performance evaluation, and adaptive DT model update.
We present a case study pertaining to DT-assisted network slicing for short
video streaming, followed by some open research issues for DTN4VS.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19080" title="Abstract">arXiv:2310.19080</a> [<a href="/pdf/2310.19080" title="Download PDF">pdf</a>, <a href="/format/2310.19080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward Finetuning for Faster and More Accurate Unsupervised Object  Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+K+Z">Katie Z Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenzhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yurong You</a>, 
<a href="/search/cs?searchtype=author&query=Benaim%2C+S">Sagie Benaim</a>, 
<a href="/search/cs?searchtype=author&query=Phoo%2C+C+P">Cheng Perng Phoo</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+M">Mark Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hariharan%2C+B">Bharath Hariharan</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+K+Q">Kilian Q. Weinberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in machine learning have shown that Reinforcement Learning
from Human Feedback (RLHF) can improve machine learning models and align them
with human preferences. Although very successful for Large Language Models
(LLMs), these advancements have not had a comparable impact in research for
autonomous vehicles -- where alignment with human expectations can be
imperative. In this paper, we propose to adapt similar RL-based methods to
unsupervised object discovery, i.e. learning to detect objects from LiDAR
points without any training labels. Instead of labels, we use simple heuristics
to mimic human feedback. More explicitly, we combine multiple heuristics into a
simple reward function that positively correlates its score with bounding box
accuracy, \ie, boxes containing objects are scored higher than those without.
We start from the detector's own predictions to explore the space and reinforce
boxes with high rewards through gradient updates. Empirically, we demonstrate
that our approach is not only more accurate, but also orders of magnitudes
faster to train compared to prior works on object discovery.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19081" title="Abstract">arXiv:2310.19081</a> [<a href="/pdf/2310.19081" title="Download PDF">pdf</a>, <a href="/format/2310.19081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Audio Analyzer: a Framework to Industrialize the Research on Audio  Forensics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Puglisi%2C+V+F">Valerio Francesco Puglisi</a>, 
<a href="/search/cs?searchtype=author&query=Giudice%2C+O">Oliver Giudice</a>, 
<a href="/search/cs?searchtype=author&query=Battiato%2C+S">Sebastiano Battiato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Deep Audio Analyzer is an open source speech framework that aims to simplify
the research and the development process of neural speech processing pipelines,
allowing users to conceive, compare and share results in a fast and
reproducible way. This paper describes the core architecture designed to
support several tasks of common interest in the audio forensics field, showing
possibility of creating new tasks thus customizing the framework. By means of
Deep Audio Analyzer, forensics examiners (i.e. from Law Enforcement Agencies)
and researchers will be able to visualize audio features, easily evaluate
performances on pretrained models, to create, export and share new audio
analysis workflows by combining deep neural network models with few clicks. One
of the advantages of this tool is to speed up research and practical
experimentation, in the field of audio forensics analysis thus also improving
experimental reproducibility by exporting and sharing pipelines. All features
are developed in modules accessible by the user through a Graphic User
Interface. Index Terms: Speech Processing, Deep Learning Audio, Deep Learning
Audio Pipeline creation, Audio Forensics.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19083" title="Abstract">arXiv:2310.19083</a> [<a href="/pdf/2310.19083" title="Download PDF">pdf</a>, <a href="/format/2310.19083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backward Reachability Analysis of Perturbed Continuous-Time Linear  Systems Using Set Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wetzlinger%2C+M">Mark Wetzlinger</a>, 
<a href="/search/math?searchtype=author&query=Althoff%2C+M">Matthias Althoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Backward reachability analysis computes the set of states that reach a target
set under the competing influence of control input and disturbances. Depending
on their interplay, the backward reachable set either represents all states
that can be steered into the target set or all states that cannot avoid
entering it -- the corresponding solutions can be used for controller synthesis
and safety verification, respectively. A popular technique for backward
reachable set computation solves Hamilton-Jacobi-Isaacs equations, which scales
exponentially with the state dimension due to gridding the state space. In this
work, we instead use set propagation techniques to design backward reachability
algorithms for linear time-invariant systems. Crucially, the proposed
algorithms scale only polynomially with the state dimension. Our numerical
examples demonstrate the tightness of the obtained backward reachable sets and
show an overwhelming improvement of our proposed algorithms over
state-of-the-art methods regarding scalability, as systems with well over a
hundred states can now be analyzed.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19084" title="Abstract">arXiv:2310.19084</a> [<a href="/pdf/2310.19084" title="Download PDF">pdf</a>, <a href="/format/2310.19084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Roles of Scaling and Instruction Tuning in Language Perception: Model  vs. Human Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Changjiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jixing Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent large language models (LLMs) have revealed strong abilities to
understand natural language. Since most of them share the same basic structure,
i.e. the transformer block, possible contributors to their success in the
training process are scaling and instruction tuning. However, how these factors
affect the models' language perception is unclear. This work compares the
self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different
sizes (7B, 13B, 30B, 65B), together with eye saccade, an aspect of human
reading attention, to assess the effect of scaling and instruction tuning on
language perception. Results show that scaling enhances the human resemblance
and improves the effective attention by reducing the trivial pattern reliance,
while instruction tuning does not. However, instruction tuning significantly
enhances the models' sensitivity to instructions. We also find that current
LLMs are consistently closer to non-native than native speakers in attention,
suggesting a sub-optimal language perception of all models. Our code and data
used in the analysis is available on GitHub.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19089" title="Abstract">arXiv:2310.19089</a> [<a href="/pdf/2310.19089" title="Download PDF">pdf</a>, <a href="/format/2310.19089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pushdown Layers: Encoding Recursive Structure in Transformer Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murty%2C+S">Shikhar Murty</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+P">Pratyusha Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Andreas%2C+J">Jacob Andreas</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C+D">Christopher D. Manning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Long Papers)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recursion is a prominent feature of human language, and fundamentally
challenging for self-attention due to the lack of an explicit recursive-state
tracking mechanism. Consequently, Transformer language models poorly capture
long-tail recursive structure and exhibit sample-inefficient syntactic
generalization. This work introduces Pushdown Layers, a new self-attention
layer that models recursive state via a stack tape that tracks estimated depths
of every token in an incremental parse of the observed prefix. Transformer LMs
with Pushdown Layers are syntactic language models that autoregressively and
synchronously update this stack tape as they predict new tokens, in turn using
the stack tape to softly modulate attention over tokens -- for instance,
learning to "skip" over closed constituents. When trained on a corpus of
strings annotated with silver constituency parses, Transformers equipped with
Pushdown Layers achieve dramatically better and 3-5x more sample-efficient
syntactic generalization, while maintaining similar perplexities. Pushdown
Layers are a drop-in replacement for standard self-attention. We illustrate
this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed
WikiText-103, leading to improvements on several GLUE text classification
tasks.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19090" title="Abstract">arXiv:2310.19090</a> [<a href="/pdf/2310.19090" title="Download PDF">pdf</a>, <a href="/format/2310.19090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> gafro: Geometric Algebra for Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B6w%2C+T">Tobias L&#xf6;w</a>, 
<a href="/search/cs?searchtype=author&query=Abbet%2C+P">Philip Abbet</a>, 
<a href="/search/cs?searchtype=author&query=Calinon%2C+S">Sylvain Calinon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Geometry is a fundamental part of robotics and there have been various
frameworks of representation over the years. Recently, geometric algebra has
gained attention for its property of unifying many of those previous ideas into
one algebra. While there are already efficient open-source implementations of
geometric algebra available, none of them is targeted at robotics applications.
We want to address this shortcoming with our library gafro. This article
presents an overview of the implementation details as well as a tutorial of
gafro, an efficient c++ library targeting robotics applications using geometric
algebra. The library focuses on using conformal geometric algebra. Hence,
various geometric primitives are available for computation as well as rigid
body transformations. The modeling of robotic systems is also an important
aspect of the library. It implements various algorithms for calculating the
kinematics and dynamics of such systems as well as objectives for optimisation
problems. The software stack is completed by python bindings in pygafro and a
ROS interface in gafro_ros.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19091" title="Abstract">arXiv:2310.19091</a> [<a href="/pdf/2310.19091" title="Download PDF">pdf</a>, <a href="/format/2310.19091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Gap: Towards an Expanded Toolkit for ML-Supported  Decision-Making in the Public Sector
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abaigar%2C+U+F">Unai Fischer Abaigar</a>, 
<a href="/search/cs?searchtype=author&query=Kern%2C+C">Christoph Kern</a>, 
<a href="/search/cs?searchtype=author&query=Barda%2C+N">Noam Barda</a>, 
<a href="/search/cs?searchtype=author&query=Kreuter%2C+F">Frauke Kreuter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Methodology (stat.ME)

</div>
<p class="mathjax">Machine Learning (ML) systems are becoming instrumental in the public sector,
with applications spanning areas like criminal justice, social welfare,
financial fraud detection, and public health. While these systems offer great
potential benefits to institutional decision-making processes, such as improved
efficiency and reliability, they still face the challenge of aligning intricate
and nuanced policy objectives with the precise formalization requirements
necessitated by ML models. In this paper, we aim to bridge the gap between ML
and public sector decision-making by presenting a comprehensive overview of key
technical challenges where disjunctions between policy goals and ML models
commonly arise. We concentrate on pivotal points of the ML pipeline that
connect the model to its operational environment, delving into the significance
of representative training data and highlighting the importance of a model
setup that facilitates effective decision-making. Additionally, we link these
challenges with emerging methodological advancements, encompassing causal ML,
domain adaptation, uncertainty quantification, and multi-objective
optimization, illustrating the path forward for harmonizing ML and public
sector objectives.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19093" title="Abstract">arXiv:2310.19093</a> [<a href="/pdf/2310.19093" title="Download PDF">pdf</a>, <a href="/format/2310.19093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending the Cooperative Dual-Task Space in Conformal Geometric Algebra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B6w%2C+T">Tobias L&#xf6;w</a>, 
<a href="/search/cs?searchtype=author&query=Calinon%2C+S">Sylvain Calinon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this work, we are presenting an extension of the cooperative dual-task
space (CDTS) in conformal geometric algebra. The CDTS was first defined using
dual quaternion algebra and is a well established framework for the simplified
definition of tasks using two manipulators. By integrating conformal geometric
algebra, we aim to further enhance the geometric expressiveness and thus
simplify the modeling of various tasks. We show this formulation by first
presenting the CDTS and then its extension that is based around a cooperative
pointpair. This extension keeps all the benefits of the original formulation
that is based on dual quaternions, but adds more tools for geometric modeling
of the dual-arm tasks. We also present how this CGA-CDTS can be seamlessly
integrated with an optimal control framework in geometric algebra that was
derived in previous work. In the experiments, we demonstrate how to model
different objectives and constraints using the CGA-CDTS. Using a setup of two
Franka Emika robots we then show the effectiveness of our approach using model
predictive control in real world experiments.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19094" title="Abstract">arXiv:2310.19094</a> [<a href="/pdf/2310.19094" title="Download PDF">pdf</a>, <a href="/format/2310.19094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Characterization of NVMe Flash Devices with Zoned Namespaces  (ZNS)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doekemeijer%2C+K">Krijn Doekemeijer</a>, 
<a href="/search/cs?searchtype=author&query=Tehrany%2C+N">Nick Tehrany</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+B">Balakrishnan Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B8rling%2C+M">Matias Bj&#xf8;rling</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A">Animesh Trivedi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper to appear in the <a href="https://clustercomp.org/2023/program/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The recent emergence of NVMe flash devices with Zoned Namespace support, ZNS
SSDs, represents a significant new advancement in flash storage. ZNS SSDs
introduce a new storage abstraction of append-only zones with a set of new I/O
(i.e., append) and management (zone state machine transition) commands. With
the new abstraction and commands, ZNS SSDs offer more control to the host
software stack than a non-zoned SSD for flash management, which is known to be
complex (because of garbage collection, scheduling, block allocation,
parallelism management, overprovisioning). ZNS SSDs are, consequently, gaining
adoption in a variety of applications (e.g., file systems, key-value stores,
and databases), particularly latency-sensitive big-data applications. Despite
this enthusiasm, there has yet to be a systematic characterization of ZNS SSD
performance with its zoned storage model abstractions and I/O operations. This
work addresses this crucial shortcoming. We report on the performance features
of a commercially available ZNS SSD (13 key observations), explain how these
features can be incorporated into publicly available state-of-the-art ZNS
emulators, and recommend guidelines for ZNS SSD application developers. All
artifacts (code and data sets) of this study are publicly available at
https://github.com/stonet-research/NVMeBenchmarks.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19096" title="Abstract">arXiv:2310.19096</a> [<a href="/pdf/2310.19096" title="Download PDF">pdf</a>, <a href="/ps/2310.19096" title="Download PostScript">ps</a>, <a href="/format/2310.19096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Circuit Width Estimation via Effect Typing and Linear Dependency (Long  Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colledan%2C+A">Andrea Colledan</a>, 
<a href="/search/cs?searchtype=author&query=Lago%2C+U+D">Ugo Dal Lago</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages (excluding references), 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Circuit description languages are a class of quantum programming languages in
which programs are classical and produce a description of a quantum
computation, in the form of a quantum circuit. Since these programs can
leverage all the expressive power of high-level classical languages, circuit
description languages have been successfully used to describe complex and
practical quantum algorithms, whose circuits, however, may involve many more
qubits and gate applications than current quantum architectures can actually
muster. In this paper, we present Proto-Quipper-R, a circuit description
language endowed with a linear dependent type-and-effect system capable of
deriving parametric upper bounds on the width of the circuits produced by a
program. We prove both the standard type safety results and that the resulting
resource analysis is correct with respect to a big-step operational semantics.
We also show that our approach is expressive enough to verify realistic quantum
algorithms.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19099" title="Abstract">arXiv:2310.19099</a> [<a href="/pdf/2310.19099" title="Download PDF">pdf</a>, <a href="/format/2310.19099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Web3 Meets AI Marketplace: Exploring Opportunities, Analyzing  Challenges, and Suggesting Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peihao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE); Cryptography and Security (cs.CR); Computer Science and Game Theory (cs.GT); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Web3 and AI have been among the most discussed fields over the recent years,
with substantial hype surrounding each field's potential to transform the world
as we know it. However, as the hype settles, it's evident that neither AI nor
Web3 can address all challenges independently. Consequently, the intersection
of AI and Web3 is gaining increased attention, emerging as a new field with the
potential to address the limitations of each. In this article, we will focus on
the integration of web3 and the AI marketplace, where AI services and products
can be provided in a decentralized manner (DeAI). A comprehensive review is
provided by summarizing the opportunities and challenges on this topic.
Additionally, we offer analyses and solutions to address these challenges.
We've developed a framework that lets users pay with any kind of cryptocurrency
to get AI services. Additionally, they can also enjoy AI services for free on
our platform by simply locking up their assets temporarily in the protocol.
This unique approach is a first in the industry. Before this, offering free AI
services in the web3 community wasn't possible. Our solution opens up exciting
opportunities for the AI marketplace in the web3 space to grow and be widely
adopted.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19102" title="Abstract">arXiv:2310.19102</a> [<a href="/pdf/2310.19102" title="Download PDF">pdf</a>, <a href="/format/2310.19102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Atom: Low-bit Quantization for Efficient and Accurate LLM Serving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chien-Yu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zihao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lequn Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Size Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ceze%2C+L">Luis Ceze</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Arvind Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kasikci%2C+B">Baris Kasikci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The growing demand for Large Language Models (LLMs) in applications such as
content generation, intelligent chatbots, and sentiment analysis poses
considerable challenges for LLM service providers. To efficiently use GPU
resources and boost throughput, batching multiple requests has emerged as a
popular paradigm; to further speed up batching, LLM quantization techniques
reduce memory consumption and increase computing capacity. However, prevalent
quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully
leverage the capabilities of modern GPUs, such as 4-bit integer operators,
resulting in sub-optimal performance.
<br />To maximize LLMs' serving throughput, we introduce Atom, a low-bit
quantization method that achieves high throughput improvements with negligible
accuracy loss. Atom significantly boosts serving throughput by using low-bit
operators and considerably reduces memory consumption via low-bit quantization.
It attains high accuracy by applying a novel mixed-precision and fine-grained
quantization process. We evaluate Atom on 4-bit weight-activation quantization
setups in the serving context. Atom improves end-to-end throughput by up to
$7.73\times$ compared to the FP16 and by $2.53\times$ compared to INT8
quantization, while maintaining the same latency target.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19103" title="Abstract">arXiv:2310.19103</a> [<a href="/pdf/2310.19103" title="Download PDF">pdf</a>, <a href="/format/2310.19103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proving Linear Mode Connectivity of Neural Networks via Optimal  Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferbach%2C+D">Damien Ferbach</a>, 
<a href="/search/cs?searchtype=author&query=Goujaud%2C+B">Baptiste Goujaud</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>, 
<a href="/search/cs?searchtype=author&query=Dieuleveut%2C+A">Aymeric Dieuleveut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The energy landscape of high-dimensional non-convex optimization problems is
crucial to understanding the effectiveness of modern deep neural network
architectures. Recent works have experimentally shown that two different
solutions found after two runs of a stochastic training are often connected by
very simple continuous paths (e.g., linear) modulo a permutation of the
weights. In this paper, we provide a framework theoretically explaining this
empirical observation. Based on convergence rates in Wasserstein distance of
empirical measures, we show that, with high probability, two wide enough
two-layer neural networks trained with stochastic gradient descent are linearly
connected. Additionally, we express upper and lower bounds on the width of each
layer of two deep neural networks with independent neuron weights to be
linearly connected. Finally, we empirically demonstrate the validity of our
approach by showing how the dimension of the support of the weight distribution
of neurons, which dictates Wasserstein convergence rates is correlated with
linear mode connectivity.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19105" title="Abstract">arXiv:2310.19105</a> [<a href="/pdf/2310.19105" title="Download PDF">pdf</a>, <a href="/format/2310.19105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Updated Standard for Secure Satellite Communications: Analysis of  Satellites, Attack Vectors, Existing Standards, and Enterprise and Security  Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Protik%2C+R+C">Rupok Chowdhury Protik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Satellites play a vital role in remote communication where traditional
communication mediums struggle to provide benefits over associated costs and
efficiency. In recent years, satellite communication has achieved utter
interest in the industry due to the achievement of high data rates through the
massive deployment of LEO satellites. Because of the complex diversity in types
of satellites, communication methodologies, technological obstacles,
environmental limitations, elements in the entire ecosystem, massive financial
impact, geopolitical conflict and domination, easier access to satellite
communications, and various other reasons, the threat vectors are rising in the
threat landscape. To achieve resilience against those, only technological
solutions are not enough. An effective approach will be through security
standards. However, there is a considerable gap in the industry regarding a
generic security standard framework for satellite communication and space data
systems. A few countries and space agencies have their own standard framework
and private policies. However, many of those are either private, serve the
specific requirements of specific missions, or have not been updated for a long
time.
<br />This project report will focus on identifying, categorizing, comparing, and
assessing elements, threat landscape, enterprise security architectures, and
available public standards of satellite communication and space data systems.
After that, it will utilize the knowledge to propose an updated standard
framework for secure satellite communications and space data systems.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19106" title="Abstract">arXiv:2310.19106</a> [<a href="/pdf/2310.19106" title="Download PDF">pdf</a>, <a href="/format/2310.19106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PACuna: Automated Fine-Tuning of Language Models for Particle  Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sulc%2C+A">Antonin Sulc</a>, 
<a href="/search/cs?searchtype=author&query=Kammering%2C+R">Raimund Kammering</a>, 
<a href="/search/cs?searchtype=author&query=Eichler%2C+A">Annika Eichler</a>, 
<a href="/search/cs?searchtype=author&query=Wilksen%2C+T">Tim Wilksen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Navigating the landscape of particle accelerators has become increasingly
challenging with recent surges in contributions. These intricate devices
challenge comprehension, even within individual facilities. To address this, we
introduce PACuna, a fine-tuned language model refined through publicly
available accelerator resources like conferences, pre-prints, and books. We
automated data collection and question generation to minimize expert
involvement and make the data publicly available. PACuna demonstrates
proficiency in addressing intricate accelerator questions, validated by
experts. Our approach shows adapting language models to scientific domains by
fine-tuning technical texts and auto-generated corpora capturing the latest
developments can further produce pre-trained models to answer some intricate
questions that commercially available assistants cannot and can serve as
intelligent assistants for individual facilities.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19109" title="Abstract">arXiv:2310.19109</a> [<a href="/pdf/2310.19109" title="Download PDF">pdf</a>, <a href="/ps/2310.19109" title="Download PostScript">ps</a>, <a href="/format/2310.19109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Task and Weight Prioritization Curriculum Learning for  Multimodal Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alsan%2C+H+F">Huseyin Fuat Alsan</a>, 
<a href="/search/cs?searchtype=author&query=Arsan%2C+T">Taner Arsan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper explores post-disaster analytics using multimodal deep learning
models trained with curriculum learning method. Studying post-disaster
analytics is important as it plays a crucial role in mitigating the impact of
disasters by providing timely and accurate insights into the extent of damage
and the allocation of resources. We propose a curriculum learning strategy to
enhance the performance of multimodal deep learning models. Curriculum learning
emulates the progressive learning sequence in human education by training deep
learning models on increasingly complex data. Our primary objective is to
develop a curriculum-trained multimodal deep learning model, with a particular
focus on visual question answering (VQA) capable of jointly processing image
and text data, in conjunction with semantic segmentation for disaster analytics
using the
FloodNet\footnote{https://github.com/BinaLab/FloodNet-Challenge-EARTHVISION2021}
dataset. To achieve this, U-Net model is used for semantic segmentation and
image encoding. A custom built text classifier is used for visual question
answering. Existing curriculum learning methods rely on manually defined
difficulty functions. We introduce a novel curriculum learning approach termed
Dynamic Task and Weight Prioritization (DATWEP), which leverages a
gradient-based method to automatically decide task difficulty during curriculum
learning training, thereby eliminating the need for explicit difficulty
computation. The integration of DATWEP into our multimodal model shows
improvement on VQA performance. Source code is available at
https://github.com/fualsan/DATWEP.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19112" title="Abstract">arXiv:2310.19112</a> [<a href="/pdf/2310.19112" title="Download PDF">pdf</a>, <a href="/format/2310.19112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient IoT Inference via Context-Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rastikerdar%2C+M+M">Mohammad Mehdi Rastikerdar</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Shiwei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+H">Hui Guan</a>, 
<a href="/search/cs?searchtype=author&query=Ganesan%2C+D">Deepak Ganesan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">While existing strategies for optimizing deep learning-based classification
models on low-power platforms assume the models are trained on all classes of
interest, this paper posits that adopting context-awareness i.e. focusing
solely on the likely classes in the current context, can substantially enhance
performance in resource-constrained environments. We propose a new paradigm,
CACTUS, for scalable and efficient context-aware classification where a
micro-classifier recognizes a small set of classes relevant to the current
context and, when context change happens, rapidly switches to another suitable
micro-classifier. CACTUS has several innovations including optimizing the
training cost of context-aware classifiers, enabling on-the-fly context-aware
switching between classifiers, and selecting the best context-aware classifiers
given limited resources. We show that CACTUS achieves significant benefits in
accuracy, latency, and compute budget across a range of datasets and IoT
platforms.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19113" title="Abstract">arXiv:2310.19113</a> [<a href="/pdf/2310.19113" title="Download PDF">pdf</a>, <a href="/format/2310.19113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic V2X Autonomous Perception from Road-to-Vehicle Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jiayao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+F">Fan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+F">Fuyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tingliang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Fenglei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+R">Rui Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Vehicle-to-everything (V2X) perception is an innovative technology that
enhances vehicle perception accuracy, thereby elevating the security and
reliability of autonomous systems. However, existing V2X perception methods
focus on static scenes from mainly vehicle-based vision, which is constrained
by sensor capabilities and communication loads. To adapt V2X perception models
to dynamic scenes, we propose to build V2X perception from road-to-vehicle
vision and present Adaptive Road-to-Vehicle Perception (AR2VP) method. In
AR2VP,we leverage roadside units to offer stable, wide-range sensing
capabilities and serve as communication hubs. AR2VP is devised to tackle both
intra-scene and inter-scene changes. For the former, we construct a dynamic
perception representing module, which efficiently integrates vehicle
perceptions, enabling vehicles to capture a more comprehensive range of dynamic
factors within the scene.Moreover, we introduce a road-to-vehicle perception
compensating module, aimed at preserving the maximized roadside unit perception
information in the presence of intra-scene changes.For inter-scene changes, we
implement an experience replay mechanism leveraging the roadside unit's storage
capacity to retain a subset of historical scene data, maintaining model
robustness in response to inter-scene shifts. We conduct perception experiment
on 3D object detection and segmentation, and the results show that AR2VP excels
in both performance-bandwidth trade-offs and adaptability within dynamic
environments.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19119" title="Abstract">arXiv:2310.19119</a> [<a href="/pdf/2310.19119" title="Download PDF">pdf</a>, <a href="/format/2310.19119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Out-of-distribution Object Detection through Bayesian Uncertainty  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenglin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bouaynaya%2C+N">Nidhal Bouaynaya</a>, 
<a href="/search/cs?searchtype=author&query=Calinescu%2C+R">Radu Calinescu</a>, 
<a href="/search/cs?searchtype=author&query=Mihaylova%2C+L">Lyudmila Mihaylova</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 26th International Conference on Information Fusion (FUSION),
  1-8, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The superior performance of object detectors is often established under the
condition that the test samples are in the same distribution as the training
data. However, in many practical applications, out-of-distribution (OOD)
instances are inevitable and usually lead to uncertainty in the results. In
this paper, we propose a novel, intuitive, and scalable probabilistic object
detection method for OOD detection. Unlike other uncertainty-modeling methods
that either require huge computational costs to infer the weight distributions
or rely on model training through synthetic outlier data, our method is able to
distinguish between in-distribution (ID) data and OOD data via weight parameter
sampling from proposed Gaussian distributions based on pre-trained networks. We
demonstrate that our Bayesian object detector can achieve satisfactory OOD
identification performance by reducing the FPR95 score by up to 8.19% and
increasing the AUROC score by up to 13.94% when trained on BDD100k and VOC
datasets as the ID datasets and evaluated on COCO2017 dataset as the OOD
dataset.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19122" title="Abstract">arXiv:2310.19122</a> [<a href="/pdf/2310.19122" title="Download PDF">pdf</a>, <a href="/ps/2310.19122" title="Download PostScript">ps</a>, <a href="/format/2310.19122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Variable-Length Coding with Non-zero Leakage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zamani%2C+A">Amirreza Zamani</a>, 
<a href="/search/cs?searchtype=author&query=Oechtering%2C+T+J">Tobias J. Oechtering</a>, 
<a href="/search/cs?searchtype=author&query=Skoglund%2C+M">Mikael Skoglund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2306.13184">arXiv:2306.13184</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A private compression design problem is studied, where an encoder observes
useful data $Y$, wishes to compress it using variable length code and
communicates it through an unsecured channel. Since $Y$ is correlated with
private data $X$, the encoder uses a private compression mechanism to design
encoded message $\cal C$ and sends it over the channel. An adversary is assumed
to have access to the output of the encoder, i.e., $\cal C$, and tries to
estimate $X$. Furthermore, it is assumed that both encoder and decoder have
access to a shared secret key $W$. In this work, we generalize the perfect
privacy (secrecy) assumption and consider a non-zero leakage between the
private data $X$ and encoded message $\cal C$. The design goal is to encode
message $\cal C$ with minimum possible average length that satisfies
non-perfect privacy constraints. We find upper and lower bounds on the average
length of the encoded message using different privacy metrics and study them in
special cases. For the achievability we use two-part construction coding and
extended versions of Functional Representation Lemma. Lastly, in an example we
show that the bounds can be asymptotically tight.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19123" title="Abstract">arXiv:2310.19123</a> [<a href="/pdf/2310.19123" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three Dogmas, a Puzzle and its Solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdelwahab%2C+E+M">Elnaserledinellah Mahmood Abdelwahab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 99 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Acad. (N.Y.) 2023, Vol. 12,1:3-101
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Modern Logics, as formulated notably by Frege, Russell and Tarski involved
basic assumptions about Natural Languages in general and Indo-European
Languages in particular, which are contested by Linguists. Based upon those
assumptions, formal Languages were designed to overcome what Logicians claimed
to be 'defects' of Natural Language. In this paper we show that those
assumptions contradict basic principles of Arabic. More specifically: The
Logicians ideas, that within Natural Language words refer to objects,
'ToBe'-constructions represent identity statements, Indefinite Descriptions
must be replaced by existential quantifiers to form meaningful Sentences and
Symbols can have no interpretation-independent meanings, are all falsified
using undisputed principles of Arabic. The here presented falsification serves
two purposes. First, it is used as a factual basis for the rejection of
approaches adopting Semantic axioms of Mathematical Logics as Models for
meaning of Arabic Syntax. Second, it shows a way to approach the important
computational problem: Satisfiability (SAT). The described way is based upon
the realization that parsing Arabic utilizes the existence of
'meaning-particles' within Syntax to efficiently recognize words, phrases and
Sentences. Similar meaning-particles are shown to exist in 3CNF formulas,
which, when properly handled within the machinery of 3SAT-Solvers, enable
structural conditions to be imposed on formulas, sufficient alone to guarantee
the efficient production of non-exponentially sized Free Binary Decision
Diagrams (FBDDs). We show, why known exponential Lower Bounds on sizes of FBDDs
do not contradict our results and reveal practical evidence, obtained for
multiplication circuits, supporting our claims.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19124" title="Abstract">arXiv:2310.19124</a> [<a href="/pdf/2310.19124" title="Download PDF">pdf</a>, <a href="/format/2310.19124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Software engineering for deep learning applications: usage of SWEng and  MLops tools in GitHub repositories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panourgia%2C+E">Evangelia Panourgia</a> (Athens University of Economics and Business), 
<a href="/search/cs?searchtype=author&query=Plessas%2C+T">Theodoros Plessas</a> (Athens University of Economics and Business), 
<a href="/search/cs?searchtype=author&query=Spinellis%2C+D">Diomidis Spinellis</a> (Athens University of Economics and Business, Delft University of Technology)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The rising popularity of deep learning (DL) methods and techniques has
invigorated interest in the topic of SE4DL, the application of software
engineering (SE) practices on deep learning software. Despite the novel
engineering challenges brought on by the data-driven and non-deterministic
paradigm of DL software, little work has been invested into developing
AI-targeted SE tools. On the other hand, tools tackling more general
engineering issues in DL are actively used and referred to under the umbrella
term of ``MLOps tools''. Furthermore, the available literature supports the
utility of conventional SE tooling in DL software development. Building upon
previous MSR research on tool usage in open-source software works, we identify
conventional and MLOps tools adopted in popular applied DL projects that use
Python as the main programming language. About 70% of the GitHub repositories
mined contained at least one conventional SE tool. Software configuration
management tools are the most adopted, while the opposite applies to
maintenance tools. Substantially fewer MLOps tools were in use, with only 9
tools out of a sample of 80 used in at least one repository. The majority of
them were open-source rather than proprietary. One of these tools, TensorBoard,
was found to be adopted in about half of the repositories in our study.
Consequently, the use of conventional SE tooling demonstrates its relevance to
DL software. Further research is recommended on the adoption of MLOps tooling
by open-source projects, focusing on the relevance of particular tool types,
the development of required tools, as well as ways to promote the use of
already available tools.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19125" title="Abstract">arXiv:2310.19125</a> [<a href="/pdf/2310.19125" title="Download PDF">pdf</a>, <a href="/format/2310.19125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partial Orderings as Heuristic for Multi-Objective Model-Based Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lustosa%2C+A">Andre Lustosa</a>, 
<a href="/search/cs?searchtype=author&query=Menzies%2C+T">Tim Menzies</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Model-based reasoning is becoming increasingly common in software
engineering. The process of building and analyzing models helps stakeholders to
understand the ramifications of their software decisions. But complex models
can confuse and overwhelm stakeholders when these models have too many
candidate solutions. We argue here that a technique based on partial orderings
lets humans find acceptable solutions via a binary chop needing $O(log(N))$
queries (or less). This paper checks the value of this approach via the iSNEAK
partial ordering tool. Pre-experimentally, we were concerned that (a)~our
automated methods might produce models that were unacceptable to humans; and
that (b)~our human-in-the-loop methods might actual overlooking significant
optimizations. Hence, we checked the acceptability of the solutions found by
iSNEAK via a human-in-the-loop double-blind evaluation study of 20 Brazilian
programmers. We also checked if iSNEAK misses significant optimizations (in a
corpus of 16 SE models of size ranging up to 1000 attributes by comparing it
against two rival technologies (the genetic algorithms preferred by the
interactive search-based SE community; and the sequential model optimizers
developed by the SE configuration community~\citep{flash_vivek}). iSNEAK 's
solutions were found to be human acceptable (and those solutions took far less
time to generate, with far fewer questions to any stakeholder). Significantly,
our methods work well even for multi-objective models with competing goals (in
this work we explore models with four to five goals). These results motivate
more work on partial ordering for many-goal model-based problems.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19126" title="Abstract">arXiv:2310.19126</a> [<a href="/pdf/2310.19126" title="Download PDF">pdf</a>, <a href="/format/2310.19126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Worst-case Performance of Popular Approximate Nearest Neighbor Search  Implementations: Guarantees and Limitations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Indyk%2C+P">Piotr Indyk</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haike Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG); Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph-based approaches to nearest neighbor search are popular and powerful
tools for handling large datasets in practice, but they have limited
theoretical guarantees. We study the worst-case performance of recent
graph-based approximate nearest neighbor search algorithms, such as HNSW, NSG
and DiskANN. For DiskANN, we show that its "slow preprocessing" version
provably supports approximate nearest neighbor search query with constant
approximation ratio and poly-logarithmic query time, on data sets with bounded
"intrinsic" dimension. For the other data structure variants studied, including
DiskANN with "fast preprocessing", HNSW and NSG, we present a family of
instances on which the empirical query time required to achieve a "reasonable"
accuracy is linear in instance size. For example, for DiskANN, we show that the
query procedure can take at least $0.1 n$ steps on instances of size $n$ before
it encounters any of the $5$ nearest neighbors of the query.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19127" title="Abstract">arXiv:2310.19127</a> [<a href="/pdf/2310.19127" title="Download PDF">pdf</a>, <a href="/format/2310.19127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Representation for Non-compositional and Compositional  Expressions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Ziheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+S">Suma Bhat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work is accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Accurate processing of non-compositional language relies on generating good
representations for such expressions. In this work, we study the representation
of language non-compositionality by proposing a language model, PIER, that
builds on BART and can create semantically meaningful and contextually
appropriate representations for English potentially idiomatic expressions
(PIEs). PIEs are characterized by their non-compositionality and contextual
ambiguity in their literal and idiomatic interpretations. Via intrinsic
evaluation on embedding quality and extrinsic evaluation on PIE processing and
NLU tasks, we show that representations generated by PIER result in 33% higher
homogeneity score for embedding clustering than BART, whereas 3.12% and 3.29%
gains in accuracy and sequence accuracy for PIE sense classification and span
detection compared to the state-of-the-art IE representation model, GIEA. These
gains are achieved without sacrificing PIER's performance on NLU tasks (+/- 1%
accuracy) compared to BART.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19130" title="Abstract">arXiv:2310.19130</a> [<a href="/pdf/2310.19130" title="Download PDF">pdf</a>, <a href="/format/2310.19130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Women Wearing Lipstick: Measuring the Bias Between an Object and Its  Related Gender
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sabir%2C+A">Ahmed Sabir</a>, 
<a href="/search/cs?searchtype=author&query=Padr%C3%B3%2C+L">Llu&#xed;s Padr&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we investigate the impact of objects on gender bias in image
captioning systems. Our results show that only gender-specific objects have a
strong gender bias (e.g., women-lipstick). In addition, we propose a visual
semantic-based gender score that measures the degree of bias and can be used as
a plug-in for any image captioning system. Our experiments demonstrate the
utility of the gender score, since we observe that our score can measure the
bias relation between a caption and its related gender; therefore, our score
can be used as an additional metric to the existing Object Gender Co-Occ
approach. Code and data are publicly available at
\url{https://github.com/ahmedssabir/GenderScore}.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19137" title="Abstract">arXiv:2310.19137</a> [<a href="/pdf/2310.19137" title="Download PDF">pdf</a>, <a href="/format/2310.19137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automaton Distillation: Neuro-Symbolic Transfer Learning for Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singireddy%2C+S">Suraj Singireddy</a>, 
<a href="/search/cs?searchtype=author&query=Beckus%2C+A">Andre Beckus</a>, 
<a href="/search/cs?searchtype=author&query=Atia%2C+G">George Atia</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Sumit Jha</a>, 
<a href="/search/cs?searchtype=author&query=Velasquez%2C+A">Alvaro Velasquez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement learning (RL) is a powerful tool for finding optimal policies
in sequential decision processes. However, deep RL methods suffer from two
weaknesses: collecting the amount of agent experience required for practical RL
problems is prohibitively expensive, and the learned policies exhibit poor
generalization on tasks outside of the training distribution. To mitigate these
issues, we introduce automaton distillation, a form of neuro-symbolic transfer
learning in which Q-value estimates from a teacher are distilled into a
low-dimensional representation in the form of an automaton. We then propose two
methods for generating Q-value estimates: static transfer, which reasons over
an abstract Markov Decision Process constructed based on prior knowledge, and
dynamic transfer, where symbolic information is extracted from a teacher Deep
Q-Network (DQN). The resulting Q-value estimates from either method are used to
bootstrap learning in the target environment via a modified DQN loss function.
We list several failure modes of existing automaton-based transfer methods and
demonstrate that both static and dynamic automaton distillation decrease the
time required to find optimal policies for various decision tasks.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19138" title="Abstract">arXiv:2310.19138</a> [<a href="/pdf/2310.19138" title="Download PDF">pdf</a>, <a href="/format/2310.19138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backward and Forward Inference in Interacting Independent-Cascade  Processes: A Scalable and Convergent Message-Passing Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+N">Nouman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+K">Kangle Mu</a>, 
<a href="/search/cs?searchtype=author&query=Moharrami%2C+M">Mehrdad Moharrami</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+V">Vijay Subramanian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problems of estimating the past and future evolutions of two
diffusion processes that spread concurrently on a network. Specifically, given
a known network $G=(V, \overrightarrow{E})$ and a (possibly noisy) snapshot
$\mathcal{O}_n$ of its state taken at (a possibly unknown) time $W$, we wish to
determine the posterior distributions of the initial state of the network and
the infection times of its nodes. These distributions are useful in finding
source nodes of epidemics and rumors -- $\textit{backward inference}$ -- , and
estimating the spread of a fixed set of source nodes -- $\textit{forward
inference}$.
<br />To model the interaction between the two processes, we study an extension of
the independent-cascade (IC) model where, when a node gets infected with either
process, its susceptibility to the other one changes. First, we derive the
exact joint probability of the initial state of the network and the
observation-snapshot $\mathcal{O}_n$. Then, using the machinery of
factor-graphs, factor-graph transformations, and the generalized
distributive-law, we derive a Belief-Propagation (BP) based algorithm that is
scalable to large networks and can converge on graphs of arbitrary topology (at
a likely expense in approximation accuracy).
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19139" title="Abstract">arXiv:2310.19139</a> [<a href="/pdf/2310.19139" title="Download PDF">pdf</a>, <a href="/format/2310.19139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Back to the Future! Studying Data Cleanness in Defects4J and its Impact  on Fault Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A+R">An Ran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rafi%2C+M+N">Md Nakhla Rafi</a>, 
<a href="/search/cs?searchtype=author&query=Tse-Hsun">Tse-Hsun</a> (Peter)
<a href="/search/cs?searchtype=author&query=Chen">Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shaohua Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">For software testing research, Defects4J stands out as the primary benchmark
dataset, offering a controlled environment to study real bugs from prominent
open-source systems. However, prior research indicates that Defects4J might
include tests added post-bug report, embedding developer knowledge and
affecting fault localization efficacy. In this paper, we examine Defects4J's
fault-triggering tests, emphasizing the implications of developer knowledge of
SBFL techniques. We study the timelines of changes made to these tests
concerning bug report creation. Then, we study the effectiveness of SBFL
techniques without developer knowledge in the tests. We found that 1) 55% of
the fault-triggering tests were newly added to replicate the bug or to test for
regression; 2) 22% of the fault-triggering tests were modified after the bug
reports were created, containing developer knowledge of the bug; 3) developers
often modify the tests to include new assertions or change the test code to
reflect the changes in the source code; and 4) the performance of SBFL
techniques degrades significantly (up to --415% for Mean First Rank) when
evaluated on the bugs without developer knowledge. We provide a dataset of bugs
without developer insights, aiding future SBFL evaluations in Defects4J and
informing considerations for future bug benchmarks.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19141" title="Abstract">arXiv:2310.19141</a> [<a href="/pdf/2310.19141" title="Download PDF">pdf</a>, <a href="/format/2310.19141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optical STAR-RIS-Aided VLC Systems: RSMA Versus NOMA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maraqa%2C+O">Omar Maraqa</a>, 
<a href="/search/cs?searchtype=author&query=Aboagye%2C+S">Sylvester Aboagye</a>, 
<a href="/search/cs?searchtype=author&query=Ngatched%2C+T+M+N">Telex M. N. Ngatched</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures, this work has been submitted to the IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">A critical concern within the realm of visible light communications (VLC)
pertains to enhancing system data rate, particularly in scenarios where the
direct line-of-sight (LoS) connection is obstructed by obstacles. The
deployment of meta-surface-based simultaneous transmission and reflection
reconfigurable intelligent surface (STAR-RIS) has emerged to combat challenging
LoS blockage scenarios and to provide 360 coverage in radio-frequency wireless
systems. Recently, the concept of optical simultaneous transmission and
reflection reconfigurable intelligent surface (OSTAR-RIS) has been promoted for
VLC systems. This work is dedicated to studying the performance of OSTAR-RIS in
detail and unveiling the VLC system performance gain under such technology.
Specifically, we propose a novel multi-user indoor VLC system that is assisted
by OSTAR-RIS. To improve the sum rate performance of the proposed system, both
power-domain non-orthogonal multiple access (NOMA) and rate splitting multiple
access (RSMA) are investigated in this work. To realize this, a sum rate
maximization problem that jointly optimizes the roll and yaw angles of the
reflector elements as well as the refractive index of the refractor elements in
OSTAR-RIS is formulated, solved, and evaluated. The maximization problem takes
into account practical considerations, such as the presence of non-users (i.e.,
blockers) and the orientation of the recipient's device. The sine-cosine
meta-heuristic algorithm is employed to get the optimal solution of the
formulated non-convex optimization problem. Moreover, the study delves into the
sum energy efficiency optimization of the proposed system. Simulation results
indicate that the proposed OSTAR-RIS RSMA-aided VLC system outperforms the
OSTAR-RIS NOMA-based VLC system in terms of both the sum rate and the sum
energy efficiency.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19142" title="Abstract">arXiv:2310.19142</a> [<a href="/pdf/2310.19142" title="Download PDF">pdf</a>, <a href="/format/2310.19142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAG-GNN: Reinforcement Learning Boosted Graph Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lecheng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiarui Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While Graph Neural Networks (GNNs) recently became powerful tools in graph
learning tasks, considerable efforts have been spent on improving GNNs'
structural encoding ability. A particular line of work proposed subgraph GNNs
that use subgraph information to improve GNNs' expressivity and achieved great
success. However, such effectivity sacrifices the efficiency of GNNs by
enumerating all possible subgraphs. In this paper, we analyze the necessity of
complete subgraph enumeration and show that a model can achieve a comparable
level of expressivity by considering a small subset of the subgraphs. We then
formulate the identification of the optimal subset as a combinatorial
optimization problem and propose Magnetic Graph Neural Network (MAG-GNN), a
reinforcement learning (RL) boosted GNN, to solve the problem. Starting with a
candidate subgraph set, MAG-GNN employs an RL agent to iteratively update the
subgraphs to locate the most expressive set for prediction. This reduces the
exponential complexity of subgraph enumeration to the constant complexity of a
subgraph search algorithm while keeping good expressivity. We conduct extensive
experiments on many datasets, showing that MAG-GNN achieves competitive
performance to state-of-the-art methods and even outperforms many subgraph
GNNs. We also demonstrate that MAG-GNN effectively reduces the running time of
subgraph GNNs.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19145" title="Abstract">arXiv:2310.19145</a> [<a href="/pdf/2310.19145" title="Download PDF">pdf</a>, <a href="/format/2310.19145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Follow Object-Centric Image Editing Instructions Faithfully
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+T">Tuhin Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+K">Kanishk Singh</a>, 
<a href="/search/cs?searchtype=author&query=Saakyan%2C+A">Arkadiy Saakyan</a>, 
<a href="/search/cs?searchtype=author&query=Muresan%2C+S">Smaranda Muresan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 (Long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Natural language instructions are a powerful interface for editing the
outputs of text-to-image diffusion models. However, several challenges need to
be addressed: 1) underspecification (the need to model the implicit meaning of
instructions) 2) grounding (the need to localize where the edit has to be
performed), 3) faithfulness (the need to preserve the elements of the image not
affected by the edit instruction). Current approaches focusing on image editing
with natural language instructions rely on automatically generated paired data,
which, as shown in our investigation, is noisy and sometimes nonsensical,
exacerbating the above issues. Building on recent advances in segmentation,
Chain-of-Thought prompting, and visual question answering, we significantly
improve the quality of the paired data. In addition, we enhance the supervision
signal by highlighting parts of the image that need to be changed by the
instruction. The model fine-tuned on the improved data is capable of performing
fine-grained object-centric edits better than state-of-the-art baselines,
mitigating the problems outlined above, as shown by automatic and human
evaluations. Moreover, our model is capable of generalizing to domains unseen
during training, such as visual metaphors.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19149" title="Abstract">arXiv:2310.19149</a> [<a href="/pdf/2310.19149" title="Download PDF">pdf</a>, <a href="/ps/2310.19149" title="Download PostScript">ps</a>, <a href="/format/2310.19149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Constructions of Unique Neighbor Expanders from Error-correcting  Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kopparty%2C+S">Swastik Kopparty</a>, 
<a href="/search/cs?searchtype=author&query=Ron-Zewi%2C+N">Noga Ron-Zewi</a>, 
<a href="/search/cs?searchtype=author&query=Saraf%2C+S">Shubhangi Saraf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">In this note, we give very simple constructions of unique neighbor expander
graphs starting from spectral or combinatorial expander graphs of mild
expansion. These constructions and their analysis are simple variants of the
constructions of LDPC error-correcting codes from expanders, given by
Sipser-Spielman\cite{SS96} (and Tanner\cite{Tanner81}), and their analysis. We
also show how to obtain expanders with many unique neighbors using similar
ideas.
<br />There were many exciting results on this topic recently, starting with
Asherov-Dinur\cite{AD23} and Hsieh-McKenzie-Mohanty-Paredes\cite{HMMP23}, who
gave a similar construction of unique neighbor expander graphs, but using more
sophisticated ingredients (such as almost-Ramanujan graphs) and a more involved
analysis. Subsequent beautiful works of Cohen-Roth-TaShma\cite{CRT23} and
Golowich\cite{Golowich23} gave even stronger objects (lossless expanders), but
also using sophisticated ingredients.
<br />The main contribution of this work is that we get much more elementary
constructions of unique neighbor expanders and with a simpler analysis.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19152" title="Abstract">arXiv:2310.19152</a> [<a href="/pdf/2310.19152" title="Download PDF">pdf</a>, <a href="/format/2310.19152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BERT Lost Patience Won&#x27;t Be Robust to Adversarial Slowdown
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coalson%2C+Z">Zachary Coalson</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+G">Gabriel Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Bobba%2C+R">Rakesh Bobba</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Sanghyun Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 [Poster]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In this paper, we systematically evaluate the robustness of multi-exit
language models against adversarial slowdown. To audit their robustness, we
design a slowdown attack that generates natural adversarial text bypassing
early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a
comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark
against adversarial slowdown. We then show our attack significantly reduces the
computational savings provided by the three methods in both white-box and
black-box settings. The more complex a mechanism is, the more vulnerable it is
to adversarial slowdown. We also perform a linguistic analysis of the perturbed
text inputs, identifying common perturbation patterns that our attack
generates, and comparing them with standard adversarial text attacks. Moreover,
we show that adversarial training is ineffective in defeating our slowdown
attack, but input sanitization with a conversational model, e.g., ChatGPT, can
remove perturbations effectively. This result suggests that future work is
needed for developing efficient yet robust multi-exit models. Our code is
available at: https://github.com/ztcoalson/WAFFLE
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19153" title="Abstract">arXiv:2310.19153</a> [<a href="/pdf/2310.19153" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Experimental Evaluation of a Haptic Robot-Assisted System for  Femur Fracture Surgery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alruwaili%2C+F+H">Fayez H. Alruwaili</a>, 
<a href="/search/cs?searchtype=author&query=Clancy%2C+M+P">Michael P. Clancy</a>, 
<a href="/search/cs?searchtype=author&query=Saeedi-Hosseiny%2C+M+S">Marzieh S. Saeedi-Hosseiny</a>, 
<a href="/search/cs?searchtype=author&query=Logar%2C+J+A">Jacob A. Logar</a>, 
<a href="/search/cs?searchtype=author&query=Papachristou%2C+C">Charalampos Papachristou</a>, 
<a href="/search/cs?searchtype=author&query=Haydel%2C+C">Christopher Haydel</a>, 
<a href="/search/cs?searchtype=author&query=Parvizi%2C+J">Javad Parvizi</a>, 
<a href="/search/cs?searchtype=author&query=Iordachita%2C+I+I">Iulian I. Iordachita</a>, 
<a href="/search/cs?searchtype=author&query=Abedin-Nasab%2C+M+H">Mohammad H. Abedin-Nasab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is to be submitted to an IEEE journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In the face of challenges encountered during femur fracture surgery, such as
the high rates of malalignment and X-ray exposure to operating personnel,
robot-assisted surgery has emerged as an alternative to conventional
state-of-the-art surgical methods. This paper introduces the development of
Robossis, a haptic system for robot-assisted femur fracture surgery. Robossis
comprises a 7-DOF haptic controller and a 6-DOF surgical robot. A unilateral
control architecture is developed to address the kinematic mismatch and the
motion transfer between the haptic controller and the Robossis surgical robot.
A real-time motion control pipeline is designed to address the motion transfer
and evaluated through experimental testing. The analysis illustrates that the
Robossis surgical robot can adhere to the desired trajectory from the haptic
controller with an average translational error of 0.32 mm and a rotational
error of 0.07 deg. Additionally, a haptic rendering pipeline is developed to
resolve the kinematic mismatch by constraining the haptic controller (user
hand) movement within the permissible joint limits of the Robossis surgical
robot. Lastly, in a cadaveric lab test, the Robossis system assisted surgeons
during a mock femur fracture surgery. The result shows that Robossis can
provide an intuitive solution for surgeons to perform femur fracture surgery.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19155" title="Abstract">arXiv:2310.19155</a> [<a href="/pdf/2310.19155" title="Download PDF">pdf</a>, <a href="/format/2310.19155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-World Implementation of Reinforcement Learning Based Energy  Coordination for a Cluster of Households
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gokhale%2C+G">Gargya Gokhale</a>, 
<a href="/search/eess?searchtype=author&query=Tiben%2C+N">Niels Tiben</a>, 
<a href="/search/eess?searchtype=author&query=Verwee%2C+M">Marie-Sophie Verwee</a>, 
<a href="/search/eess?searchtype=author&query=Lahariya%2C+M">Manu Lahariya</a>, 
<a href="/search/eess?searchtype=author&query=Claessens%2C+B">Bert Claessens</a>, 
<a href="/search/eess?searchtype=author&query=Develder%2C+C">Chris Develder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures, workshop article accepted at RLEM'23 (BuildSys'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Given its substantial contribution of 40\% to global power consumption, the
built environment has received increasing attention to serve as a source of
flexibility to assist the modern power grid. In that respect, previous research
mainly focused on energy management of individual buildings. In contrast, in
this paper, we focus on aggregated control of a set of residential buildings,
to provide grid supporting services, that eventually should include ancillary
services. In particular, we present a real-life pilot study that studies the
effectiveness of reinforcement-learning (RL) in coordinating the power
consumption of 8 residential buildings to jointly track a target power signal.
Our RL approach relies solely on observed data from individual households and
does not require any explicit building models or simulators, making it
practical to implement and easy to scale. We show the feasibility of our
proposed RL-based coordination strategy in a real-world setting. In a 4-week
case study, we demonstrate a hierarchical control system, relying on an
RL-based ranking system to select which households to activate flex assets
from, and a real-time PI control-based power dispatch mechanism to control the
selected assets. Our results demonstrate satisfactory power tracking, and the
effectiveness of the RL-based ranks which are learnt in a purely data-driven
manner.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19156" title="Abstract">arXiv:2310.19156</a> [<a href="/pdf/2310.19156" title="Download PDF">pdf</a>, <a href="/format/2310.19156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Poisoning Retrieval Corpora by Injecting Adversarial Passages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zexuan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Ziqing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wettig%2C+A">Alexander Wettig</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. Our code is available at <a href="https://github.com/princeton-nlp/corpus-poisoning">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Dense retrievers have achieved state-of-the-art performance in various
information retrieval tasks, but to what extent can they be safely deployed in
real-world applications? In this work, we propose a novel attack for dense
retrieval systems in which a malicious user generates a small number of
adversarial passages by perturbing discrete tokens to maximize similarity with
a provided set of training queries. When these adversarial passages are
inserted into a large retrieval corpus, we show that this attack is highly
effective in fooling these systems to retrieve them for queries that were not
seen by the attacker. More surprisingly, these adversarial passages can
directly generalize to out-of-domain queries and corpora with a high success
attack rate -- for instance, we find that 50 generated passages optimized on
Natural Questions can mislead &gt;94% of questions posed in financial documents or
online forums. We also benchmark and compare a range of state-of-the-art dense
retrievers, both unsupervised and supervised. Although different systems
exhibit varying levels of vulnerability, we show they can all be successfully
attacked by injecting up to 500 passages, a small fraction compared to a
retrieval corpus of millions of passages.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19158" title="Abstract">arXiv:2310.19158</a> [<a href="/pdf/2310.19158" title="Download PDF">pdf</a>, <a href="/format/2310.19158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perspectives from India: Challenges and Opportunities for Computational  Tools to Enhance Confidence in Published Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakravorti%2C+T">Tatiana Chakravorti</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Koneru%2C+S">Sai Koneru</a>, 
<a href="/search/cs?searchtype=author&query=Rajtmajer%2C+S">Sarah Rajtmajer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Over the past decade, a crisis of confidence in published scientific findings
has catalyzed widespread response from the research community, particularly in
the West. These responses have included policy discussions and changes to
existing practice as well as computational infrastructure to support and
evaluate research. Our work studies Indian researchers' awareness, perceptions,
and challenges around research integrity. We explore opportunities for
Artificial Intelligence (AI)-powered tools to evaluate reproducibility and
replicability, centering cultural perspectives. We discuss requirements for
such tools, including signals within papers and metadata to be included, and
system hybridity (fully-AI vs. collaborative human-AI). We draw upon 19
semi-structured interviews and 72 follow-up surveys with researchers at
universities throughout India. Our findings highlight the need for
computational tools to contextualize confidence in published research. In
particular, researchers prefer approaches that enable human-AI collaboration.
Additionally, our findings emphasize the shortcomings of current incentive
structures for publication, funding, and promotion.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19159" title="Abstract">arXiv:2310.19159</a> [<a href="/pdf/2310.19159" title="Download PDF">pdf</a>, <a href="/format/2310.19159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer Learning in Transformer-Based Demand Forecasting For Home  Energy Management System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gokhale%2C+G">Gargya Gokhale</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gompel%2C+J">Jonas Van Gompel</a>, 
<a href="/search/cs?searchtype=author&query=Claessens%2C+B">Bert Claessens</a>, 
<a href="/search/cs?searchtype=author&query=Develder%2C+C">Chris Develder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures, workshop article at BALANCES, BuildSys'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Increasingly, homeowners opt for photovoltaic (PV) systems and/or battery
storage to minimize their energy bills and maximize renewable energy usage.
This has spurred the development of advanced control algorithms that maximally
achieve those goals. However, a common challenge faced while developing such
controllers is the unavailability of accurate forecasts of household power
consumption, especially for shorter time resolutions (15 minutes) and in a
data-efficient manner. In this paper, we analyze how transfer learning can help
by exploiting data from multiple households to improve a single house's load
forecasting. Specifically, we train an advanced forecasting model (a temporal
fusion transformer) using data from multiple different households, and then
finetune this global model on a new household with limited data (i.e. only a
few days). The obtained models are used for forecasting power consumption of
the household for the next 24 hours~(day-ahead) at a time resolution of 15
minutes, with the intention of using these forecasts in advanced controllers
such as Model Predictive Control. We show the benefit of this transfer learning
setup versus solely using the individual new household's data, both in terms of
(i) forecasting accuracy ($\sim$15\% MAE reduction) and (ii) control
performance ($\sim$2\% energy cost reduction), using real-world household data.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19163" title="Abstract">arXiv:2310.19163</a> [<a href="/pdf/2310.19163" title="Download PDF">pdf</a>, <a href="/format/2310.19163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning  with Active Data Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+D">Dzung Pham</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Shreyas Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Houmansadr%2C+A">Amir Houmansadr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated learning (FL) has recently emerged as a privacy-preserving approach
for machine learning in domains that rely on user interactions, particularly
recommender systems (RS) and online learning to rank (OLTR). While there has
been substantial research on the privacy of traditional FL, little attention
has been paid to studying the privacy properties of these interaction-based FL
(IFL) systems. In this work, we show that IFL can introduce unique challenges
concerning user privacy, particularly when the central server has knowledge and
control over the items that users interact with. Specifically, we demonstrate
the threat of reconstructing user interactions by presenting RAIFLE, a general
optimization-based reconstruction attack framework customized for IFL. RAIFLE
employs Active Data Manipulation (ADM), a novel attack technique unique to IFL,
where the server actively manipulates the training features of the items to
induce adversarial behaviors in the local FL updates. We show that RAIFLE is
more impactful than existing FL privacy attacks in the IFL context, and
describe how it can undermine privacy defenses like secure aggregation and
private information retrieval. Based on our findings, we propose and discuss
countermeasure guidelines to mitigate our attack in the context of federated
RS/OLTR specifically and IFL more broadly.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19166" title="Abstract">arXiv:2310.19166</a> [<a href="/pdf/2310.19166" title="Download PDF">pdf</a>, <a href="/format/2310.19166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Power of Explainability in Forecast-Informed Deep Learning Models  for Flood Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jimeng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Stebliankin%2C+V">Vitalii Stebliankin</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+G">Giri Narasimhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Floods can cause horrific harm to life and property. However, they can be
mitigated or even avoided by the effective use of hydraulic structures such as
dams, gates, and pumps. By pre-releasing water via these structures in advance
of extreme weather events, water levels are sufficiently lowered to prevent
floods. In this work, we propose FIDLAR, a Forecast Informed Deep Learning
Architecture, achieving flood management in watersheds with hydraulic
structures in an optimal manner by balancing out flood mitigation and
unnecessary wastage of water via pre-releases. We perform experiments with
FIDLAR using data from the South Florida Water Management District, which
manages a coastal area that is highly prone to frequent storms and floods.
Results show that FIDLAR performs better than the current state-of-the-art with
several orders of magnitude speedup and with provably better pre-release
schedules. The dramatic speedups make it possible for FIDLAR to be used for
real-time flood management. The main contribution of this paper is the
effective use of tools for model explainability, allowing us to understand the
contribution of the various environmental factors towards its decisions.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19167" title="Abstract">arXiv:2310.19167</a> [<a href="/pdf/2310.19167" title="Download PDF">pdf</a>, <a href="/format/2310.19167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rare Event Probability Learning by Normalizing Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhenggqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Daniel%2C+L">Luca Daniel</a>, 
<a href="/search/cs?searchtype=author&query=Boning%2C+D+S">Duane S. Boning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">A rare event is defined by a low probability of occurrence. Accurate
estimation of such small probabilities is of utmost importance across diverse
domains. Conventional Monte Carlo methods are inefficient, demanding an
exorbitant number of samples to achieve reliable estimates. Inspired by the
exact sampling capabilities of normalizing flows, we revisit this challenge and
propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS
first learns a sequence of proposal distributions associated with predefined
nested subset events by minimizing KL divergence losses. Next, it estimates the
rare event probability by utilizing importance sampling in conjunction with the
last proposal. The efficacy of our NOFIS method is substantiated through
comprehensive qualitative visualizations, affirming the optimality of the
learned proposal distribution, as well as a series of quantitative experiments
encompassing $10$ distinct test cases, which highlight NOFIS's superiority over
baseline approaches.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19168" title="Abstract">arXiv:2310.19168</a> [<a href="/pdf/2310.19168" title="Download PDF">pdf</a>, <a href="/format/2310.19168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species  Classification and Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sastry%2C+S">Srikumar Sastry</a>, 
<a href="/search/cs?searchtype=author&query=Khanal%2C+S">Subash Khanal</a>, 
<a href="/search/cs?searchtype=author&query=Dhakal%2C+A">Aayush Dhakal</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Di Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+N">Nathan Jacobs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a metadata-aware self-supervised learning~(SSL)~framework useful
for fine-grained classification and ecological mapping of bird species around
the world. Our framework unifies two SSL strategies: Contrastive Learning~(CL)
and Masked Image Modeling~(MIM), while also enriching the embedding space with
metadata available with ground-level imagery of birds. We separately train
uni-modal and cross-modal ViT on a novel cross-view global bird species dataset
containing ground-level imagery, metadata (location, time), and corresponding
satellite imagery. We demonstrate that our models learn fine-grained and
geographically conditioned features of birds, by evaluating on two downstream
tasks: fine-grained visual classification~(FGVC) and cross-modal retrieval.
Pre-trained models learned using our framework achieve SotA performance on FGVC
of iNAT-2021 birds and in transfer learning settings for CUB-200-2011 and
NABirds datasets. Moreover, the impressive cross-modal retrieval performance of
our model enables the creation of species distribution maps across any
geographic region. The dataset and source code will be released at
https://github.com/mvrl/BirdSAT}.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19170" title="Abstract">arXiv:2310.19170</a> [<a href="/pdf/2310.19170" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A technique to avoid Blockchain Denial of Service (BDoS) and Selfish  Mining Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Habib%2C+M+A">Md. Ahsan Habib</a>, 
<a href="/search/cs?searchtype=author&query=Manik%2C+M+M+H">Md. Motaleb Hossen Manik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Blockchain denial of service (BDoS) and selfish mining are the two most
crucial attacks on blockchain technology. A classical DoS attack targets the
computer network to limit, restrict, or stop accessing the system of authorized
users which is ineffective against renowned cryptocurrencies like Bitcoin,
Ethereum, etc. Unlike the conventional DoS, the BDoS affects the system's
mechanism design to manipulate the incentive structure to discourage honest
miners to participate in the mining process. In contrast, in a selfish mining
attack, the adversary miner keeps its discovered block private to fork the
chain intentionally that aiming to increase the incentive of the adversary
miner. This paper proposed a technique to successfully avoid BDoS and selfish
mining attacks. The existing infrastructure of blockchain technology doesn't
need to be changed a lot to incorporate the proposed solution.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19172" title="Abstract">arXiv:2310.19172</a> [<a href="/pdf/2310.19172" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification of the Most Significant Parameter for Optimizing the  Performance of RPL Routing Protocol in IoT Using Taguchi Design of  Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanaboina%2C+C+S">Chandra Sekhar Sanaboina</a>, 
<a href="/search/cs?searchtype=author&query=Sanaboina%2C+P">Pallamsetty Sanaboina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Internet of Things (IoT) consists of a wide variety of devices with limited
power sources. Due to the adhered reason, energy consumption is considered as
one of the major challenges in the IoT environment. In this research article,
an attempt is made to optimize the existing Routing Protocol (RPL) towards a
green technology. It focuses on finding the most significant parameter in the
RPL using Taguchi Design of Experiments. It emphasizes the effects of five
input factors, such as Network Size, Mobility Speed, DIO_DOUBLING,
DIO_MIN_INTERVAL, and Redundancy Constant on only one output parameter Power
Consumption. The findings show that DIO_MIN_INTERVAL is the leading factor that
has a significant effect on the power consumption in RPL. After determining the
most significant factor that affects the power consumption, measures can be
taken to optimize the performance of RPL by applying some optimization
techniques. COOJA simulator is used to carry out the simulations required for
this research article.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19173" title="Abstract">arXiv:2310.19173</a> [<a href="/pdf/2310.19173" title="Download PDF">pdf</a>, <a href="/format/2310.19173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can we Quantify Trust? Towards a Trust-based Resilient SIoT Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sagar%2C+S">Subhash Sagar</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+A">Adnan Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Q+Z">Quan Z. Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Zaib%2C+M">Munazza Zaib</a>, 
<a href="/search/cs?searchtype=author&query=Sufyan%2C+F">Farhan Sufyan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The emerging yet promising paradigm of the Social Internet of Things (SIoT)
integrates the notion of the Internet of Things with human social networks. In
SIoT, objects, i.e., things, have the capability to socialize with the other
objects in the SIoT network and can establish their social network autonomously
by modeling human behaviour. The notion of trust is imperative in realizing
these characteristics of socialization in order to assess the reliability of
autonomous collaboration. The perception of trust is evolving in the era of
SIoT as an extension to traditional security triads in an attempt to offer
secure and reliable services, and is considered as an imperative aspect of any
SIoT system for minimizing the probable risk of autonomous decision-making.
This research investigates the idea of trust quantification by employing trust
measurement in terms of direct trust, indirect trust as a recommendation, and
the degree of SIoT relationships in terms of social similarities
(community-of-interest, friendship, and co-work relationships). A weighted sum
approach is subsequently employed to synthesize all the trust features in order
to ascertain a single trust score. The experimental evaluation demonstrates the
effectiveness of the proposed model in segregating trustworthy and
untrustworthy objects and via identifying the dynamic behaviour (i.e.,
trust-related attacks) of the SIoT objects.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19174" title="Abstract">arXiv:2310.19174</a> [<a href="/pdf/2310.19174" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting recovery following stroke: deep learning, multimodal data and  feature selection using explainable AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=White%2C+A">Adam White</a>, 
<a href="/search/cs?searchtype=author&query=Saranti%2C+M">Margarita Saranti</a>, 
<a href="/search/cs?searchtype=author&query=Garcez%2C+A+d">Artur d&#x27;Avila Garcez</a>, 
<a href="/search/cs?searchtype=author&query=Hope%2C+T+M+H">Thomas M. H. Hope</a>, 
<a href="/search/cs?searchtype=author&query=Price%2C+C+J">Cathy J. Price</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+H">Howard Bowman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning offers great potential for automated prediction of
post-stroke symptoms and their response to rehabilitation. Major challenges for
this endeavour include the very high dimensionality of neuroimaging data, the
relatively small size of the datasets available for learning, and how to
effectively combine neuroimaging and tabular data (e.g. demographic information
and clinical characteristics). This paper evaluates several solutions based on
two strategies. The first is to use 2D images that summarise MRI scans. The
second is to select key features that improve classification accuracy.
Additionally, we introduce the novel approach of training a convolutional
neural network (CNN) on images that combine regions-of-interest extracted from
MRIs, with symbolic representations of tabular data. We evaluate a series of
CNN architectures (both 2D and a 3D) that are trained on different
representations of MRI and tabular data, to predict whether a composite measure
of post-stroke spoken picture description ability is in the aphasic or
non-aphasic range. MRI and tabular data were acquired from 758 English speaking
stroke survivors who participated in the PLORAS study. The classification
accuracy for a baseline logistic regression was 0.678 for lesion size alone,
rising to 0.757 and 0.813 when initial symptom severity and recovery time were
successively added. The highest classification accuracy 0.854 was observed when
8 regions-of-interest was extracted from each MRI scan and combined with lesion
size, initial severity and recovery time in a 2D Residual Neural Network.Our
findings demonstrate how imaging and tabular data can be combined for high
post-stroke classification accuracy, even when the dataset is small in machine
learning terms. We conclude by proposing how the current models could be
improved to achieve even higher levels of accuracy using images from hospital
scanners.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19177" title="Abstract">arXiv:2310.19177</a> [<a href="/pdf/2310.19177" title="Download PDF">pdf</a>, <a href="/format/2310.19177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustifying Language Models with Test-Time Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McDermott%2C+N+T">Noah Thomas McDermott</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junfeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+C">Chengzhi Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages 2 Figures Submitted to ICLR Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large-scale language models achieved state-of-the-art performance over a
number of language tasks. However, they fail on adversarial language examples,
which are sentences optimized to fool the language models but with similar
semantic meanings for humans. While prior work focuses on making the language
model robust at training time, retraining for robustness is often unrealistic
for large-scale foundation models. Instead, we propose to make the language
models robust at test time. By dynamically adapting the input sentence with
predictions from masked words, we show that we can reverse many language
adversarial attacks. Since our approach does not require any training, it works
for novel tasks at test time and can adapt to novel adversarial corruptions.
Visualizations and empirical results on two popular sentence classification
datasets demonstrate that our method can repair adversarial language attacks
over 65% o
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19180" title="Abstract">arXiv:2310.19180</a> [<a href="/pdf/2310.19180" title="Download PDF">pdf</a>, <a href="/format/2310.19180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peike Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Alex Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprints
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">With rapid advances in generative artificial intelligence, the text-to-music
synthesis task has emerged as a promising direction for music generation from
scratch. However, finer-grained control over multi-track generation remains an
open challenge. Existing models exhibit strong raw generation capability but
lack the flexibility to compose separate tracks and combine them in a
controllable manner, differing from typical workflows of human composers. To
address this issue, we propose JEN-1 Composer, a unified framework to
efficiently model marginal, conditional, and joint distributions over
multi-track music via a single model. JEN-1 Composer framework exhibits the
capacity to seamlessly incorporate any diffusion-based music generation system,
\textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music
generation. We introduce a curriculum training strategy aimed at incrementally
instructing the model in the transition from single-track generation to the
flexible generation of multi-track combinations. During the inference, users
have the ability to iteratively produce and choose music tracks that meet their
preferences, subsequently creating an entire musical composition incrementally
following the proposed Human-AI co-composition workflow. Quantitative and
qualitative assessments demonstrate state-of-the-art performance in
controllable and high-fidelity multi-track music synthesis. The proposed JEN-1
Composer represents a significant advance toward interactive AI-facilitated
music creation and composition. Demos will be available at
https://jenmusic.ai/audio-demos.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19181" title="Abstract">arXiv:2310.19181</a> [<a href="/pdf/2310.19181" title="Download PDF">pdf</a>, <a href="/format/2310.19181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Chatbots to PhishBots? -- Preventing Phishing scams created using  ChatGPT, Google Bard and Claude
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+S+S">Sayak Saha Roy</a>, 
<a href="/search/cs?searchtype=author&query=Thota%2C+P">Poojitha Thota</a>, 
<a href="/search/cs?searchtype=author&query=Naragam%2C+K+V">Krishna Vamsi Naragam</a>, 
<a href="/search/cs?searchtype=author&query=Nilizadeh%2C+S">Shirin Nilizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The advanced capabilities of Large Language Models (LLMs) have made them
invaluable across various applications, from conversational agents and content
creation to data analysis, research, and innovation. However, their
effectiveness and accessibility also render them susceptible to abuse for
generating malicious content, including phishing attacks. This study explores
the potential of using four popular commercially available LLMs - ChatGPT (GPT
3.5 Turbo), GPT 4, Claude and Bard to generate functional phishing attacks
using a series of malicious prompts. We discover that these LLMs can generate
both phishing emails and websites that can convincingly imitate well-known
brands, and also deploy a range of evasive tactics for the latter to elude
detection mechanisms employed by anti-phishing systems. Notably, these attacks
can be generated using unmodified, or "vanilla," versions of these LLMs,
without requiring any prior adversarial exploits such as jailbreaking. As a
countermeasure, we build a BERT based automated detection tool that can be used
for the early detection of malicious prompts to prevent LLMs from generating
phishing content attaining an accuracy of 97\% for phishing website prompts,
and 94\% for phishing email prompts.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19182" title="Abstract">arXiv:2310.19182</a> [<a href="/pdf/2310.19182" title="Download PDF">pdf</a>, <a href="/format/2310.19182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Trainable Projection for Robust Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Junjiao Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yen-Cheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+J+S">James Seale Smith</a>, 
<a href="/search/cs?searchtype=author&query=Kira%2C+Z">Zsolt Kira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Robust fine-tuning aims to achieve competitive in-distribution (ID)
performance while maintaining the out-of-distribution (OOD) robustness of a
pre-trained model when transferring it to a downstream task. Recently,
projected gradient descent has been successfully used in robust fine-tuning by
constraining the deviation from the initialization of the fine-tuned model
explicitly through projection. However, algorithmically, two limitations
prevent this method from being adopted more widely, scalability and efficiency.
In this paper, we propose a new projection-based fine-tuning algorithm, Fast
Trainable Projection (FTP) for computationally efficient learning of per-layer
projection constraints, resulting in an average $35\%$ speedup on our
benchmarks compared to prior works. FTP can be combined with existing
optimizers such as AdamW, and be used in a plug-and-play fashion. Finally, we
show that FTP is a special instance of hyper-optimizers that tune the
hyper-parameters of optimizers in a learnable manner through nested
differentiation. Empirically, we show superior robustness on OOD datasets,
including domain shifts and natural corruptions, across four different vision
tasks with five different pre-trained models. Additionally, we demonstrate that
FTP is broadly applicable and beneficial to other learning scenarios such as
low-label and continual learning settings thanks to its easy adaptability. The
code will be available at https://github.com/GT-RIPL/FTP.git.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19185" title="Abstract">arXiv:2310.19185</a> [<a href="/pdf/2310.19185" title="Download PDF">pdf</a>, <a href="/format/2310.19185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robotic Barrier Construction through Weaved, Inflatable Tubes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">H. J. Kim</a>, 
<a href="/search/cs?searchtype=author&query=Abdel-Raziq%2C+H">H. Abdel-Raziq</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">X. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Siskovic%2C+A+Y">A. Y. Siskovic</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+S">S. Patil</a>, 
<a href="/search/cs?searchtype=author&query=Petersen%2C+K+H">K. H. Petersen</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+H+L">H. L. Kao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this article, we present a mechanism and related path planning algorithm
to construct light-duty barriers out of extruded, inflated tubes weaved around
existing environmental features. Our extruded tubes are based on everted
vine-robots and in this context, we present a new method to steer their growth.
We characterize the mechanism in terms of accuracy resilience, and, towards
their use as barriers, the ability of the tubes to withstand distributed loads.
We further explore an algorithm which, given a feature map and the size and
direction of the external load, can determine where and how to extrude the
barrier. Finally, we showcase the potential of this method in an autonomously
extruded two-layer wall weaved around three pipes. While preliminary, our work
indicates that this method has the potential for barrier construction in
cluttered environments, e.g. shelters against wind or snow. Future work may
show how to achieve tighter weaves, how to leverage weave friction for improved
strength, how to assess barrier performance for feedback control, and how to
operate the extrusion mechanism off of a mobile robot.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19187" title="Abstract">arXiv:2310.19187</a> [<a href="/pdf/2310.19187" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Haptic-Enhanced Virtual Reality Simulator for Robot-Assisted Femur  Fracture Surgery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alruwaili%2C+F+H">Fayez H. Alruwaili</a>, 
<a href="/search/cs?searchtype=author&query=Halim-Banoub%2C+D+W">David W. Halim-Banoub</a>, 
<a href="/search/cs?searchtype=author&query=Rodgers%2C+J">Jessica Rodgers</a>, 
<a href="/search/cs?searchtype=author&query=Dalkilic%2C+A">Adam Dalkilic</a>, 
<a href="/search/cs?searchtype=author&query=Haydel%2C+C">Christopher Haydel</a>, 
<a href="/search/cs?searchtype=author&query=Parvizi%2C+J">Javad Parvizi</a>, 
<a href="/search/cs?searchtype=author&query=Iordachita%2C+I+I">Iulian I. Iordachita</a>, 
<a href="/search/cs?searchtype=author&query=Abedin-Nasab%2C+M+H">Mohammad H. Abedin-Nasab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is submitted to the IEEE Haptic Symposium 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper, we develop a virtual reality (VR) simulator for the Robossis
robot-assisted femur fracture surgery. Due to the steep learning curve for such
procedures, a VR simulator is essential for training surgeon(s) and staff. The
Robossis Surgical Simulator (RSS) is designed to immerse user(s) in a realistic
surgery setting using the Robossis system as completed in a previous real-world
cadaveric procedure. The RSS is designed to interface the Sigma-7 Haptic
Controller with the Robossis Surgical Robot (RSR) and the Meta Quest VR
headset. Results show that the RSR follows user commands in 6 DOF and prevents
the overlapping of bone segments. This development demonstrates a promising
avenue for future implementation of the Robossis system.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19188" title="Abstract">arXiv:2310.19188</a> [<a href="/pdf/2310.19188" title="Download PDF">pdf</a>, <a href="/format/2310.19188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T">Ta-Ying Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Gadelha%2C+M">Matheus Gadelha</a>, 
<a href="/search/cs?searchtype=author&query=Pirk%2C+S">Soren Pirk</a>, 
<a href="/search/cs?searchtype=author&query=Groueix%2C+T">Thibault Groueix</a>, 
<a href="/search/cs?searchtype=author&query=Mech%2C+R">Radomir Mech</a>, 
<a href="/search/cs?searchtype=author&query=Markham%2C+A">Andrew Markham</a>, 
<a href="/search/cs?searchtype=author&query=Trigoni%2C+N">Niki Trigoni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present 3DMiner -- a pipeline for mining 3D shapes from challenging
large-scale unannotated image datasets. Unlike other unsupervised 3D
reconstruction methods, we assume that, within a large-enough dataset, there
must exist images of objects with similar shapes but varying backgrounds,
textures, and viewpoints. Our approach leverages the recent advances in
learning self-supervised image representations to cluster images with
geometrically similar shapes and find common image correspondences between
them. We then exploit these correspondences to obtain rough camera estimates as
initialization for bundle-adjustment. Finally, for every image cluster, we
apply a progressive bundle-adjusting reconstruction method to learn a neural
occupancy field representing the underlying shape. We show that this procedure
is robust to several types of errors introduced in previous steps (e.g., wrong
camera poses, images containing dissimilar shapes, etc.), allowing us to obtain
shape and pose annotations for images in-the-wild. When using images from Pix3D
chairs, our method is capable of producing significantly better results than
state-of-the-art unsupervised 3D reconstruction techniques, both quantitatively
and qualitatively. Furthermore, we show how 3DMiner can be applied to
in-the-wild data by reconstructing shapes present in images from the LAION-5B
dataset. Project Page: https://ttchengab.github.io/3dminerOfficial
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19193" title="Abstract">arXiv:2310.19193</a> [<a href="/pdf/2310.19193" title="Download PDF">pdf</a>, <a href="/format/2310.19193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Watching Social Issue Videos among YouTube and TikTok Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+S">Shuo Niu</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+D">Dilasha Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Ghimire%2C+A">Abhisan Ghimire</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhicong Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The openness and influence of video-sharing platforms (VSPs) such as YouTube
and TikTok attracted creators to share videos on various social issues.
Although social issue videos (SIVs) affect public opinions and breed
misinformation, how VSP users obtain information and interact with SIVs is
under-explored. This work surveyed 659 YouTube and 127 TikTok users to
understand the motives for consuming SIVs on VSPs. We found that VSP users are
primarily motivated by the information and entertainment gratifications to use
the platform. VSP users use SIVs for information-seeking purposes and find
YouTube and TikTok convenient to interact with SIVs. VSP users moderately watch
SIVs for entertainment and inactively engage in social interactions. SIV
consumption is associated with information and socialization gratifications of
the platform. VSP users appreciate the diversity of information and opinions
but would also do their own research and are concerned about the misinformation
and echo chamber problems.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19195" title="Abstract">arXiv:2310.19195</a> [<a href="/pdf/2310.19195" title="Download PDF">pdf</a>, <a href="/format/2310.19195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Evolution of Mobile Networks: From 1G to 7G
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zontou%2C+E">Ellie Zontou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The evolution of cellular networks has played a pivotal role in shaping the
modern telecommunications landscape. This paper explores the journey of
cellular network generations, beginning with the introduction of Japan's first
commercial 1G network by Nippon Telegraph and Telephone (NTT) Corporation in
1979. This analog wireless network quickly expanded to become the country's
first national 1G network within a remarkably short period.
<br />The transition from analog to digital networks marked a significant turning
point in the wireless industry, enabled by advancements in MOSFET
(Metal-Oxide-Semiconductor Field Effect Transistor) technology. MOSFET,
originally developed at Bell Labs in 1959, underwent modifications to suit
cellular networks in the early 1990s, facilitating the shift to digital
wireless mobile networks. The advent of the 2G generation brought forth the
first commercial digital cellular network in 1991, sparking recognition among
manufacturers and mobile network operators of the importance of robust networks
and efficient architecture. As the wireless industry continued to experience
exponential growth, the significance of effective network infrastructure became
increasingly evident.
<br />In this research, our aim is to provide a comprehensive overview of the
entire spectrum of cellular network generations, ranging from 1G to the
potential future of 7G. By tracing the evolution of these networks, we aim to
shed light on the transformative developments that have shaped the
telecommunications landscape and explore the possibilities that lie ahead in
the realm of cellular technology.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19199" title="Abstract">arXiv:2310.19199</a> [<a href="/pdf/2310.19199" title="Download PDF">pdf</a>, <a href="/format/2310.19199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Immersive 3D Simulator for Drone-as-a-Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiamin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Alkouz%2C+B">Balsam Alkouz</a>, 
<a href="/search/cs?searchtype=author&query=Bouguettaya%2C+A">Athman Bouguettaya</a>, 
<a href="/search/cs?searchtype=author&query=Abusafia%2C+A">Amani Abusafia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures. This is an accepted demo paper and it will appear in the International Conference on Service-Oriented Computing (ICSOC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We propose a 3D simulator tailored for the Drone-as-a-Service framework. The
simulator enables employing dynamic algorithms for addressing realistic
delivery scenarios. We present the simulator's architectural design and its use
of an energy consumption model for drone deliveries. We introduce two primary
operational modes within the simulator: the edit mode and the runtime mode.
Beyond its simulation capabilities, our simulator serves as a valuable data
collection resource, facilitating the creation of datasets through simulated
scenarios. Our simulator empowers researchers by providing an intuitive
platform to visualize and interact with delivery environments. Moreover, it
enables rigorous algorithm testing in a safe simulation setting, thus obviating
the need for real-world drone deployments. Demo: https://youtu.be/HOLfo1JiFJ0
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19201" title="Abstract">arXiv:2310.19201</a> [<a href="/pdf/2310.19201" title="Download PDF">pdf</a>, <a href="/ps/2310.19201" title="Download PostScript">ps</a>, <a href="/format/2310.19201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Problems in DAOs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+J+Z">Joshua Z. Tan</a>, 
<a href="/search/cs?searchtype=author&query=Merk%2C+T">Tara Merk</a>, 
<a href="/search/cs?searchtype=author&query=Hubbard%2C+S">Sarah Hubbard</a>, 
<a href="/search/cs?searchtype=author&query=Oak%2C+E+R">Eliza R. Oak</a>, 
<a href="/search/cs?searchtype=author&query=Pirovich%2C+J">Joni Pirovich</a>, 
<a href="/search/cs?searchtype=author&query=Rennie%2C+E">Ellie Rennie</a>, 
<a href="/search/cs?searchtype=author&query=Hoefer%2C+R">Rolf Hoefer</a>, 
<a href="/search/cs?searchtype=author&query=Zargham%2C+M">Michael Zargham</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+J">Jason Potts</a>, 
<a href="/search/cs?searchtype=author&query=Berg%2C+C">Chris Berg</a>, 
<a href="/search/cs?searchtype=author&query=Youngblom%2C+R">Reuben Youngblom</a>, 
<a href="/search/cs?searchtype=author&query=De+Filippi%2C+P">Primavera De Filippi</a>, 
<a href="/search/cs?searchtype=author&query=Frey%2C+S">Seth Frey</a>, 
<a href="/search/cs?searchtype=author&query=Strnad%2C+J">Jeff Strnad</a>, 
<a href="/search/cs?searchtype=author&query=Mannan%2C+M">Morshed Mannan</a>, 
<a href="/search/cs?searchtype=author&query=Nabben%2C+K">Kelsie Nabben</a>, 
<a href="/search/cs?searchtype=author&query=Elrifai%2C+S+N">Silke Noa Elrifai</a>, 
<a href="/search/cs?searchtype=author&query=Hartnell%2C+J">Jake Hartnell</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+B+M">Benjamin Mako Hill</a>, 
<a href="/search/cs?searchtype=author&query=Maddox%2C+A">Alexia Maddox</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+W">Woojin Lim</a>, 
<a href="/search/cs?searchtype=author&query=South%2C+T">Tobin South</a>, 
<a href="/search/cs?searchtype=author&query=Juels%2C+A">Ari Juels</a>, 
<a href="/search/cs?searchtype=author&query=Boneh%2C+D">Dan Boneh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Decentralized autonomous organizations (DAOs) are a new, rapidly-growing
class of organizations governed by smart contracts. Here we describe how
researchers can contribute to the emerging science of DAOs and other
digitally-constituted organizations. From granular privacy primitives to
mechanism designs to model laws, we identify high-impact problems in the DAO
ecosystem where existing gaps might be tackled through a new data set or by
applying tools and ideas from existing research fields such as political
science, computer science, economics, law, and organizational science. Our
recommendations encompass exciting research questions as well as promising
business opportunities. We call on the wider research community to join the
global effort to invent the next generation of organizations.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19204" title="Abstract">arXiv:2310.19204</a> [<a href="/pdf/2310.19204" title="Download PDF">pdf</a>, <a href="/format/2310.19204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can ChatGPT advance software testing intelligence? An experience report  on metamorphic testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luu%2C+Q">Quang-Hung Luu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T+Y">Tsong Yueh Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages (short communications), 2 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">While ChatGPT is a well-known artificial intelligence chatbot being used to
answer human's questions, one may want to discover its potential in advancing
software testing. We examine the capability of ChatGPT in advancing the
intelligence of software testing through a case study on metamorphic testing
(MT), a state-of-the-art software testing technique. We ask ChatGPT to generate
candidates of metamorphic relations (MRs), which are basically necessary
properties of the object program and which traditionally require human
intelligence to identify. These MR candidates are then evaluated in terms of
correctness by domain experts. We show that ChatGPT can be used to generate new
correct MRs to test several software systems. Having said that, the majority of
MR candidates are either defined vaguely or incorrect, especially for systems
that have never been tested with MT. ChatGPT can be used to advance software
testing intelligence by proposing MR candidates that can be later adopted for
implementing tests; but human intelligence should still inevitably be involved
to justify and rectify their correctness.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19206" title="Abstract">arXiv:2310.19206</a> [<a href="/pdf/2310.19206" title="Download PDF">pdf</a>, <a href="/format/2310.19206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging generative artificial intelligence to simulate student  learning behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Songlin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Student simulation presents a transformative approach to enhance learning
outcomes, advance educational research, and ultimately shape the future of
effective pedagogy. We explore the feasibility of using large language models
(LLMs), a remarkable achievement in AI, to simulate student learning behaviors.
Unlike conventional machine learning based prediction, we leverage LLMs to
instantiate virtual students with specific demographics and uncover intricate
correlations among learning experiences, course materials, understanding
levels, and engagement. Our objective is not merely to predict learning
outcomes but to replicate learning behaviors and patterns of real students. We
validate this hypothesis through three experiments. The first experiment, based
on a dataset of N = 145, simulates student learning outcomes from demographic
data, revealing parallels with actual students concerning various demographic
factors. The second experiment (N = 4524) results in increasingly realistic
simulated behaviors with more assessment history for virtual students
modelling. The third experiment (N = 27), incorporating prior knowledge and
course interactions, indicates a strong link between virtual students' learning
behaviors and fine-grained mappings from test questions, course materials,
engagement and understanding levels. Collectively, these findings deepen our
understanding of LLMs and demonstrate its viability for student simulation,
empowering more adaptable curricula design to enhance inclusivity and
educational effectiveness.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19208" title="Abstract">arXiv:2310.19208</a> [<a href="/pdf/2310.19208" title="Download PDF">pdf</a>, <a href="/format/2310.19208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LitCab: Lightweight Calibration of Language Models on Outputs of Varied  Lengths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Khalifa%2C+M">Muhammad Khalifa</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">A model is considered well-calibrated when its probability estimate aligns
with the actual likelihood of the output being correct. Calibrating language
models (LMs) is crucial, as it plays a vital role in detecting and mitigating
hallucinations, a common issue of LMs, as well as building more trustworthy
models. Yet, popular neural model calibration techniques are not well-suited
for LMs due to their lack of flexibility in discerning answer correctness and
their high computational costs. For instance, post-processing methods like
temperature scaling are often unable to reorder the candidate generations.
Moreover, training-based methods require finetuning the entire model, which is
impractical due to the increasing sizes of modern LMs. In this paper, we
present LitCab, a lightweight calibration mechanism consisting of a single
linear layer taking the input text representation and manipulateing the LM
output logits. LitCab improves model calibration by only adding &lt; 2% of the
original model parameters. For evaluation, we construct CaT, a benchmark
consisting of 7 text generation tasks, covering responses ranging from short
phrases to paragraphs. We test LitCab with Llama2-7B, where it improves
calibration across all tasks, by reducing the average ECE score by 20%. We
further conduct a comprehensive evaluation with 7 popular open-sourced LMs from
GPT and LLaMA families, yielding the following key findings: (1) Larger models
within the same family exhibit better calibration on tasks with short
generation tasks, but not necessarily for longer ones. (2) GPT-family models
show superior calibration compared to LLaMA, Llama2 and Vicuna models despite
having much fewer parameters. (3) Finetuning pretrained model (e.g., LLaMA)
with samples of limited purpose (e.g., conversations) may lead to worse
calibration, highlighting the importance of finetuning setups for calibrating
LMs.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19210" title="Abstract">arXiv:2310.19210</a> [<a href="/pdf/2310.19210" title="Download PDF">pdf</a>, <a href="/format/2310.19210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Category Discovery with Clustering Assignment Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiangli Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xinglin Pan</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+I">Irwin King</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICONIP 2023,This paper has been nominated for ICONIP2023 Best Paper Award
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Generalized category discovery (GCD) is a recently proposed open-world task.
Given a set of images consisting of labeled and unlabeled instances, the goal
of GCD is to automatically cluster the unlabeled samples using information
transferred from the labeled dataset. The unlabeled dataset comprises both
known and novel classes. The main challenge is that unlabeled novel class
samples and unlabeled known class samples are mixed together in the unlabeled
dataset. To address the GCD without knowing the class number of unlabeled
dataset, we propose a co-training-based framework that encourages clustering
consistency. Specifically, we first introduce weak and strong augmentation
transformations to generate two sufficiently different views for the same
sample. Then, based on the co-training assumption, we propose a consistency
representation learning strategy, which encourages consistency between
feature-prototype similarity and clustering assignment. Finally, we use the
discriminative embeddings learned from the semi-supervised representation
learning process to construct an original sparse network and use a community
detection method to obtain the clustering results and the number of categories
simultaneously. Extensive experiments show that our method achieves
state-of-the-art performance on three generic benchmarks and three fine-grained
visual recognition datasets. Especially in the ImageNet-100 data set, our
method significantly exceeds the best baseline by 15.5\% and 7.0\% on the
\texttt{Novel} and \texttt{All} classes, respectively.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19211" title="Abstract">arXiv:2310.19211</a> [<a href="/pdf/2310.19211" title="Download PDF">pdf</a>, <a href="/format/2310.19211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigative Pattern Detection Framework for Counterterrorism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muramudalige%2C+S+R">Shashika R. Muramudalige</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+B+W+K">Benjamin W. K. Hung</a>, 
<a href="/search/cs?searchtype=author&query=Libretti%2C+R">Rosanne Libretti</a>, 
<a href="/search/cs?searchtype=author&query=Klausen%2C+J">Jytte Klausen</a>, 
<a href="/search/cs?searchtype=author&query=Jayasumana%2C+A+P">Anura P. Jayasumana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Law-enforcement investigations aimed at preventing attacks by violent
extremists have become increasingly important for public safety. The problem is
exacerbated by the massive data volumes that need to be scanned to identify
complex behaviors of extremists and groups. Automated tools are required to
extract information to respond queries from analysts, continually scan new
information, integrate them with past events, and then alert about emerging
threats. We address challenges in investigative pattern detection and develop
an Investigative Pattern Detection Framework for Counterterrorism (INSPECT).
The framework integrates numerous computing tools that include machine learning
techniques to identify behavioral indicators and graph pattern matching
techniques to detect risk profiles/groups. INSPECT also automates multiple
tasks for large-scale mining of detailed forensic biographies, forming
knowledge networks, and querying for behavioral indicators and radicalization
trajectories. INSPECT targets human-in-the-loop mode of investigative search
and has been validated and evaluated using an evolving dataset on domestic
jihadism.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19212" title="Abstract">arXiv:2310.19212</a> [<a href="/pdf/2310.19212" title="Download PDF">pdf</a>, <a href="/format/2310.19212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EHRTutor: Enhancing Patient Understanding of Discharge Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zonghai Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huixue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=ouyang%2C+F">Feiyun ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS'23 Workshop on Generative AI for Education (GAIED)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models have shown success as a tutor in education in various
fields. Educating patients about their clinical visits plays a pivotal role in
patients' adherence to their treatment plans post-discharge. This paper
presents EHRTutor, an innovative multi-component framework leveraging the Large
Language Model (LLM) for patient education through conversational
question-answering. EHRTutor first formulates questions pertaining to the
electronic health record discharge instructions. It then educates the patient
through conversation by administering each question as a test. Finally, it
generates a summary at the end of the conversation. Evaluation results using
LLMs and domain experts have shown a clear preference for EHRTutor over the
baseline. Moreover, EHRTutor also offers a framework for generating synthetic
patient education dialogues that can be used for future in-house system
training.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19215" title="Abstract">arXiv:2310.19215</a> [<a href="/pdf/2310.19215" title="Download PDF">pdf</a>, <a href="/format/2310.19215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the accuracy and efficiency of group-wise clipping in differentially  private optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bu%2C+Z">Zhiqi Bu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+S">Sheng Zha</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Recent advances have substantially improved the accuracy, memory cost, and
training speed of differentially private (DP) deep learning, especially on
large vision and language models with millions to billions of parameters. In
this work, we thoroughly study the per-sample gradient clipping style, a key
component in DP optimization. We show that different clipping styles have the
same time complexity but instantiate an accuracy-memory trade-off: while the
all-layer clipping (of coarse granularity) is the most prevalent and usually
gives the best accuracy, it incurs heavier memory cost compared to other
group-wise clipping, such as the layer-wise clipping (of finer granularity). We
formalize this trade-off through our convergence theory and complexity
analysis. Importantly, we demonstrate that the accuracy gap between group-wise
clipping and all-layer clipping becomes smaller for larger models, while the
memory advantage of the group-wise clipping remains. Consequently, the
group-wise clipping allows DP optimization of large models to achieve high
accuracy and low peak memory simultaneously.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19216" title="Abstract">arXiv:2310.19216</a> [<a href="/pdf/2310.19216" title="Download PDF">pdf</a>, <a href="/format/2310.19216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Status Updates for Minimizing Age of Correlated Information in  IoT Networks with Energy Harvesting Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H+H">Howard H.Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+N">Nikolaos Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q.S.Quek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Many real-time applications of the Internet of Things (IoT) need to deal with
correlated information generated by multiple sensors. The design of efficient
status update strategies that minimize the Age of Correlated Information (AoCI)
is a key factor. In this paper, we consider an IoT network consisting of
sensors equipped with the energy harvesting (EH) capability. We optimize the
average AoCI at the data fusion center (DFC) by appropriately managing the
energy harvested by sensors, whose true battery states are unobservable during
the decision-making process. Particularly, we first formulate the dynamic
status update procedure as a partially observable Markov decision process
(POMDP), where the environmental dynamics are unknown to the DFC. In order to
address the challenges arising from the causality of energy usage, unknown
environmental dynamics, unobservability of sensors'true battery states, and
large-scale discrete action space, we devise a deep reinforcement learning
(DRL)-based dynamic status update algorithm. The algorithm leverages the
advantages of the soft actor-critic and long short-term memory techniques.
Meanwhile, it incorporates our proposed action decomposition and mapping
mechanism. Extensive simulations are conducted to validate the effectiveness of
our proposed algorithm by comparing it with available DRL algorithms for
POMDPs.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19218" title="Abstract">arXiv:2310.19218</a> [<a href="/pdf/2310.19218" title="Download PDF">pdf</a>, <a href="/format/2310.19218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Federated Unlearning: A Taxonomy, Challenges and Future  Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiaxi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">With the development of trustworthy Federated Learning (FL), the requirement
of implementing right to be forgotten gives rise to the area of Federated
Unlearning (FU). Comparing to machine unlearning, a major challenge of FU lies
in the decentralized and privacy-preserving nature of FL, in which clients
jointly train a global model without sharing their raw data, making it
substantially more intricate to selectively unlearn specific information. In
that regard, many efforts have been made to tackle the challenges of FU and
have achieved significant progress. In this paper, we present a comprehensive
survey of FU. Specially, we provide the existing algorithms, objectives,
evaluation metrics, and identify some challenges of FU. By reviewing and
comparing some studies, we summarize them into a taxonomy for various schemes,
potential applications and future directions.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19220" title="Abstract">arXiv:2310.19220</a> [<a href="/pdf/2310.19220" title="Download PDF">pdf</a>, <a href="/ps/2310.19220" title="Download PostScript">ps</a>, <a href="/format/2310.19220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Stream to Pool: Dynamic Pricing Beyond i.i.d. Arrivals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+T">Titing Cui</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+S">Su Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lavastida%2C+T">Thomas Lavastida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">The dynamic pricing problem has been extensively studied under the
\textbf{stream} model: A stream of customers arrives sequentially, each with an
independently and identically distributed valuation. However, this formulation
is not entirely reflective of the real world. In many scenarios, high-valuation
customers tend to make purchases earlier and leave the market, leading to a
\emph{shift} in the valuation distribution. Thus motivated, we consider a model
where a \textbf{pool} of $n$ non-strategic unit-demand customers interact
repeatedly with the seller. Each customer monitors the price intermittently
according to an independent Poisson process and makes a purchase if the
observed price is lower than her \emph{private} valuation, whereupon she leaves
the market permanently. We present a minimax \emph{optimal} algorithm that
efficiently computes a non-adaptive policy which guarantees a $1/k$ fraction of
the optimal revenue, given any set of $k$ prices. Moreover, we present an
adaptive \emph{learn-then-earn} policy based on a novel \emph{debiasing}
approach, and prove an $\tilde O(kn^{3/4})$ regret bound. We further improve
the bound to $\tilde O(k^{3/4} n^{3/4})$ using martingale concentration
inequalities.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19222" title="Abstract">arXiv:2310.19222</a> [<a href="/pdf/2310.19222" title="Download PDF">pdf</a>, <a href="/format/2310.19222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum Knowledge Orthogonality Reconstruction with Gradients in  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Velipasalar%2C+S">Senem Velipasalar</a>, 
<a href="/search/cs?searchtype=author&query=Gursoy%2C+M+C">M. Cenk Gursoy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning (FL) aims at keeping client data local to preserve
privacy. Instead of gathering the data itself, the server only collects
aggregated gradient updates from clients. Following the popularity of FL, there
has been considerable amount of work, revealing the vulnerability of FL
approaches by reconstructing the input data from gradient updates. Yet, most
existing works assume an FL setting with unrealistically small batch size, and
have poor image quality when the batch size is large. Other works modify the
neural network architectures or parameters to the point of being suspicious,
and thus, can be detected by clients. Moreover, most of them can only
reconstruct one sample input from a large batch. To address these limitations,
we propose a novel and completely analytical approach, referred to as the
maximum knowledge orthogonality reconstruction (MKOR), to reconstruct clients'
input data. Our proposed method reconstructs a mathematically proven high
quality image from large batches. MKOR only requires the server to send
secretly modified parameters to clients and can efficiently and inconspicuously
reconstruct the input images from clients' gradient updates. We evaluate MKOR's
performance on the MNIST, CIFAR-100, and ImageNet dataset and compare it with
the state-of-the-art works. The results show that MKOR outperforms the existing
approaches, and draws attention to a pressing need for further research on the
privacy protection of FL so that comprehensive defense approaches can be
developed.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19223" title="Abstract">arXiv:2310.19223</a> [<a href="/pdf/2310.19223" title="Download PDF">pdf</a>, <a href="/format/2310.19223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modular Anti-noise Deep Learning Network for Robotic Grasp Detection  Based on RGB Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaocong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While traditional methods relies on depth sensors, the current trend leans
towards utilizing cost-effective RGB images, despite their absence of depth
cues. This paper introduces an interesting approach to detect grasping pose
from a single RGB image. To this end, we propose a modular learning network
augmented with grasp detection and semantic segmentation, tailored for robots
equipped with parallel-plate grippers. Our network not only identifies
graspable objects but also fuses prior grasp analyses with semantic
segmentation, thereby boosting grasp detection precision. Significantly, our
design exhibits resilience, adeptly handling blurred and noisy visuals. Key
contributions encompass a trainable network for grasp detection from RGB
images, a modular design facilitating feasible grasp implementation, and an
architecture robust against common image distortions. We demonstrate the
feasibility and accuracy of our proposed approach through practical experiments
and evaluations.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19224" title="Abstract">arXiv:2310.19224</a> [<a href="/pdf/2310.19224" title="Download PDF">pdf</a>, <a href="/format/2310.19224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHAMMI: A benchmark for channel-adaptive models in microscopy imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zitong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+C">Chau Pham</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Doron%2C+M">Michael Doron</a>, 
<a href="/search/cs?searchtype=author&query=Moshkov%2C+N">Nikita Moshkov</a>, 
<a href="/search/cs?searchtype=author&query=Plummer%2C+B+A">Bryan A. Plummer</a>, 
<a href="/search/cs?searchtype=author&query=Caicedo%2C+J+C">Juan C. Caicedo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS Track on Datasets and Benchmarks, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Most neural networks assume that input images have a fixed number of channels
(three for RGB images). However, there are many settings where the number of
channels may vary, such as microscopy images where the number of channels
changes depending on instruments and experimental goals. Yet, there has not
been a systemic attempt to create and evaluate neural networks that are
invariant to the number and type of channels. As a result, trained models
remain specific to individual studies and are hardly reusable for other
microscopy settings. In this paper, we present a benchmark for investigating
channel-adaptive models in microscopy imaging, which consists of 1) a dataset
of varied-channel single-cell images, and 2) a biologically relevant evaluation
framework. In addition, we adapted several existing techniques to create
channel-adaptive models and compared their performance on this benchmark to
fixed-channel, baseline models. We find that channel-adaptive models can
generalize better to out-of-domain tasks and can be computationally efficient.
We contribute a curated dataset (https://doi.org/10.5281/zenodo.7988357) and an
evaluation API (https://github.com/broadinstitute/MorphEm.git) to facilitate
objective comparisons in future research and applications.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19225" title="Abstract">arXiv:2310.19225</a> [<a href="/pdf/2310.19225" title="Download PDF">pdf</a>, <a href="/format/2310.19225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Configuration Machines: FPGA Implementation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Felicetti%2C+M+J">Matthew J. Felicetti</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dianhui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 9 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Neural networks for industrial applications generally have additional
constraints such as response speed, memory size and power usage. Randomized
learners can address some of these issues. However, hardware solutions can
provide better resource reduction whilst maintaining the model's performance.
Stochastic configuration networks (SCNs) are a prime choice in industrial
applications due to their merits and feasibility for data modelling. Stochastic
Configuration Machines (SCMs) extend this to focus on reducing the memory
constraints by limiting the randomized weights to a binary value with a scalar
for each node and using a mechanism model to improve the learning performance
and result interpretability. This paper aims to implement SCM models on a field
programmable gate array (FPGA) and introduce binary-coded inputs to the
algorithm. Results are reported for two benchmark and two industrial datasets,
including SCM with single-layer and deep architectures.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19226" title="Abstract">arXiv:2310.19226</a> [<a href="/pdf/2310.19226" title="Download PDF">pdf</a>, <a href="/format/2310.19226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knolling bot 2.0: Enhancing Object Organization with Self-supervised  Graspability Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuhang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizhuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lipson%2C+H">Hod Lipson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by the NeurIPS 2023 Robot Learning Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Building on recent advancements in transformer based approaches for domestic
robots performing knolling, the art of organizing scattered items into neat
arrangements. This paper introduces Knolling bot 2.0. Recognizing the
challenges posed by piles of objects or items situated closely together, this
upgraded system incorporates a self-supervised graspability estimation model.
If objects are deemed ungraspable, an additional behavior will be executed to
separate the objects before knolling the table. By integrating this grasp
prediction mechanism with existing visual perception and transformer based
knolling models, an advanced system capable of decluttering and organizing even
more complex and densely populated table settings is demonstrated. Experimental
evaluations demonstrate the effectiveness of this module, yielding a
graspability prediction accuracy of 95.7%.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19231" title="Abstract">arXiv:2310.19231</a> [<a href="/pdf/2310.19231" title="Download PDF">pdf</a>, <a href="/format/2310.19231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> There Are No Data Like More Data- Datasets for Deep Learning in Earth  Observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmitt%2C+M">Michael Schmitt</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+S+A">Seyed Ali Ahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yonghao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Taskin%2C+G">Gulsen Taskin</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+U">Ujjwal Verma</a>, 
<a href="/search/cs?searchtype=author&query=Sica%2C+F">Francescopaolo Sica</a>, 
<a href="/search/cs?searchtype=author&query=Hansch%2C+R">Ronny Hansch</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published in IEEE Geoscience and Remote Sensing Magazine, vol. 11,
  no. 3, pp. 63-97, Sept. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Carefully curated and annotated datasets are the foundation of machine
learning, with particularly data-hungry deep neural networks forming the core
of what is often called Artificial Intelligence (AI). Due to the massive
success of deep learning applied to Earth Observation (EO) problems, the focus
of the community has been largely on the development of ever-more sophisticated
deep neural network architectures and training strategies largely ignoring the
overall importance of datasets. For that purpose, numerous task-specific
datasets have been created that were largely ignored by previously published
review articles on AI for Earth observation. With this article, we want to
change the perspective and put machine learning datasets dedicated to Earth
observation data and applications into the spotlight. Based on a review of the
historical developments, currently available resources are described and a
perspective for future developments is formed. We hope to contribute to an
understanding that the nature of our data is what distinguishes the Earth
observation community from many other communities that apply deep learning
techniques to image data, and that a detailed understanding of EO data
peculiarities is among the core competencies of our discipline.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19232" title="Abstract">arXiv:2310.19232</a> [<a href="/pdf/2310.19232" title="Download PDF">pdf</a>, <a href="/format/2310.19232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapter Pruning using Tropical Characterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+R">Rishabh Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Vaidya%2C+T">Tushar Vaidya</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023, Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Adapters are widely popular parameter-efficient transfer learning approaches
in natural language processing that insert trainable modules in between layers
of a pre-trained language model. Apart from several heuristics, however, there
has been a lack of studies analyzing the optimal number of adapter parameters
needed for downstream applications. In this paper, we propose an adapter
pruning approach by studying the tropical characteristics of trainable modules.
We cast it as an optimization problem that aims to prune parameters from the
adapter layers without changing the orientation of underlying tropical
hypersurfaces. Our experiments on five NLP datasets show that tropical geometry
tends to identify more relevant parameters to prune when compared with the
magnitude-based baseline, while a combined approach works best across the
tasks.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19233" title="Abstract">arXiv:2310.19233</a> [<a href="/pdf/2310.19233" title="Download PDF">pdf</a>, <a href="/format/2310.19233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Real-World Meeting Summarization Systems using Large Language  Models: A Practical Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laskar%2C+M+T+R">Md Tahmid Rahman Laskar</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xue-Yong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=TN%2C+S+B">Shashi Bhushan TN</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Industry Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper studies how to effectively build meeting summarization systems for
real-world usage using large language models (LLMs). For this purpose, we
conduct an extensive evaluation and comparison of various closed-source and
open-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findings
reveal that most closed-source LLMs are generally better in terms of
performance. However, much smaller open-source models like LLaMA- 2 (7B and
13B) could still achieve performance comparable to the large closed-source
models even in zero-shot scenarios. Considering the privacy concerns of
closed-source models for only being accessible via API, alongside the high cost
associated with using fine-tuned versions of the closed-source models, the
opensource models that can achieve competitive performance are more
advantageous for industrial use. Balancing performance with associated costs
and privacy concerns, the LLaMA-2-7B model looks more promising for industrial
usage. In sum, this paper offers practical insights on using LLMs for
real-world business meeting summarization, shedding light on the trade-offs
between performance and cost.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19240" title="Abstract">arXiv:2310.19240</a> [<a href="/pdf/2310.19240" title="Download PDF">pdf</a>, <a href="/format/2310.19240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context  Evaluation Benchmark for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwan%2C+W">Wai-Chung Kwan</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingshan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yusen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangyou Li</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and data are available at <a href="https://github.com/KwanWaiChung/M4LE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Managing long sequences has become an important and necessary feature for
large language models (LLMs). However, it is still an open question of how to
comprehensively and systematically evaluate the long-sequence capability of
LLMs. One of the reasons is that conventional and widely-used benchmarks mainly
consist of short sequences. In this paper, we propose M4LE, a Multi-ability,
Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation.
M4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task
types and 12 domains. To alleviate the scarcity of tasks with naturally long
sequences and incorporate multiple-ability assessment, we propose an automatic
approach (but with negligible human annotations) to convert short-sequence
tasks into a unified long-sequence scenario where LLMs have to identify single
or multiple relevant spans in long contexts based on explicit or semantic
hints. Specifically, the scenario includes five different types of abilities:
(1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span;
(4) semantic multiple-span; and (5) global context understanding. The resulting
samples in M4LE are evenly distributed from 1k to 8k input length. We conducted
a systematic evaluation on 11 well-established LLMs, especially those optimized
for long-sequence inputs. Our results reveal that: 1) Current LLMs struggle to
understand long context, particularly when tasks require multiple-span
attention. 2) Semantic retrieval task is more difficult for competent LLMs. 3)
Models fine-tuned on longer text with position interpolation have comparable
performance to those using Neural Tangent Kernel (NTK) aware scaling methods
without fine-tuning. We make our benchmark publicly available to encourage
future research in this challenging area.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19247" title="Abstract">arXiv:2310.19247</a> [<a href="/pdf/2310.19247" title="Download PDF">pdf</a>, <a href="/format/2310.19247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-guided Boundary Learning for Imbalanced Social Event  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiaqian Ren</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhengtao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TKDE 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> TKDE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Real-world social events typically exhibit a severe class-imbalance
distribution, which makes the trained detection model encounter a serious
generalization challenge. Most studies solve this problem from the frequency
perspective and emphasize the representation or classifier learning for tail
classes. While in our observation, compared to the rarity of classes, the
calibrated uncertainty estimated from well-trained evidential deep learning
networks better reflects model performance. To this end, we propose a novel
uncertainty-guided class imbalance learning framework - UCL$_{SED}$, and its
variant - UCL-EC$_{SED}$, for imbalanced social event detection tasks. We aim
to improve the overall model performance by enhancing model generalization to
those uncertain classes. Considering performance degradation usually comes from
misclassifying samples as their confusing neighboring classes, we focus on
boundary learning in latent space and classifier learning with high-quality
uncertainty estimation. First, we design a novel uncertainty-guided contrastive
learning loss, namely UCL and its variant - UCL-EC, to manipulate
distinguishable representation distribution for imbalanced data. During
training, they force all classes, especially uncertain ones, to adaptively
adjust a clear separable boundary in the feature space. Second, to obtain more
robust and accurate class uncertainty, we combine the results of multi-view
evidential classifiers via the Dempster-Shafer theory under the supervision of
an additional calibration method. We conduct experiments on three severely
imbalanced social event datasets including Events2012\_100, Events2018\_100,
and CrisisLexT\_7. Our model significantly improves social event representation
and classification tasks in almost all classes, especially those uncertain
ones.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19248" title="Abstract">arXiv:2310.19248</a> [<a href="/pdf/2310.19248" title="Download PDF">pdf</a>, <a href="/format/2310.19248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IMPRESS: Evaluating the Resilience of Imperceptible Perturbations  Against Unauthorized Data Usage in Diffusion-Based Generative AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+B">Bochuan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changjiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinyuan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinghui Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 11 figures, 9 tables. Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Diffusion-based image generation models, such as Stable Diffusion or DALL-E
2, are able to learn from given images and generate high-quality samples
following the guidance from prompts. For instance, they can be used to create
artistic images that mimic the style of an artist based on his/her original
artworks or to maliciously edit the original images for fake content. However,
such ability also brings serious ethical issues without proper authorization
from the owner of the original images. In response, several attempts have been
made to protect the original images from such unauthorized data usage by adding
imperceptible perturbations, which are designed to mislead the diffusion model
and make it unable to properly generate new samples. In this work, we introduce
a perturbation purification platform, named IMPRESS, to evaluate the
effectiveness of imperceptible perturbations as a protective measure. IMPRESS
is based on the key observation that imperceptible perturbations could lead to
a perceptible inconsistency between the original image and the
diffusion-reconstructed image, which can be used to devise a new optimization
strategy for purifying the image, which may weaken the protection of the
original image from unauthorized data usage (e.g., style mimicking, malicious
editing). The proposed IMPRESS platform offers a comprehensive evaluation of
several contemporary protection methods, and can be used as an evaluation
platform for future protection methods.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19250" title="Abstract">arXiv:2310.19250</a> [<a href="/pdf/2310.19250" title="Download PDF">pdf</a>, <a href="/format/2310.19250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessment of Differentially Private Synthetic Data for Utility and  Fairness in End-to-End Machine Learning Pipelines for Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pereira%2C+M">Mayana Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Kshirsagar%2C+M">Meghana Kshirsagar</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Sumit Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Dodhia%2C+R">Rahul Dodhia</a>, 
<a href="/search/cs?searchtype=author&query=Ferres%2C+J+L">Juan Lavista Ferres</a>, 
<a href="/search/cs?searchtype=author&query=de+Sousa%2C+R">Rafael de Sousa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2106.10241">arXiv:2106.10241</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Differentially private (DP) synthetic data sets are a solution for sharing
data while preserving the privacy of individual data providers. Understanding
the effects of utilizing DP synthetic data in end-to-end machine learning
pipelines impacts areas such as health care and humanitarian action, where data
is scarce and regulated by restrictive privacy laws. In this work, we
investigate the extent to which synthetic data can replace real, tabular data
in machine learning pipelines and identify the most effective synthetic data
generation techniques for training and evaluating machine learning models. We
investigate the impacts of differentially private synthetic data on downstream
classification tasks from the point of view of utility as well as fairness. Our
analysis is comprehensive and includes representatives of the two main types of
synthetic data generation algorithms: marginal-based and GAN-based. To the best
of our knowledge, our work is the first that: (i) proposes a training and
evaluation framework that does not assume that real data is available for
testing the utility and fairness of machine learning models trained on
synthetic data; (ii) presents the most extensive analysis of synthetic data set
generation algorithms in terms of utility and fairness when used for training
machine learning models; and (iii) encompasses several different definitions of
fairness. Our findings demonstrate that marginal-based synthetic data
generators surpass GAN-based ones regarding model training utility for tabular
data. Indeed, we show that models trained using data generated by
marginal-based algorithms can exhibit similar utility to models trained using
real data. Our analysis also reveals that the marginal-based synthetic data
generator MWEM PGM can train models that simultaneously achieve utility and
fairness characteristics close to those obtained by models trained with real
data.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19251" title="Abstract">arXiv:2310.19251</a> [<a href="/pdf/2310.19251" title="Download PDF">pdf</a>, <a href="/format/2310.19251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-trained Recommender Systems: A Causal Debiasing Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Ziqian Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Hao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+N">Nghia Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Kveton%2C+B">Branislav Kveton</a>, 
<a href="/search/cs?searchtype=author&query=Deoras%2C+A">Anoop Deoras</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, WSDM 24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent studies on pre-trained vision/language models have demonstrated the
practical benefit of a new, promising solution-building paradigm in AI where
models can be pre-trained on broad data describing a generic task space and
then adapted successfully to solve a wide range of downstream tasks, even when
training data is severely limited (e.g., in zero- or few-shot learning
scenarios). Inspired by such progress, we investigate in this paper the
possibilities and challenges of adapting such a paradigm to the context of
recommender systems, which is less investigated from the perspective of
pre-trained model. In particular, we propose to develop a generic recommender
that captures universal interaction patterns by training on generic user-item
interaction data extracted from different domains, which can then be fast
adapted to improve few-shot learning performance in unseen new domains (with
limited data).
<br />However, unlike vision/language data which share strong conformity in the
semantic space, universal patterns underlying recommendation data collected
across different domains (e.g., different countries or different E-commerce
platforms) are often occluded by both in-domain and cross-domain biases
implicitly imposed by the cultural differences in their user and item bases, as
well as their uses of different e-commerce platforms. As shown in our
experiments, such heterogeneous biases in the data tend to hinder the
effectiveness of the pre-trained model. To address this challenge, we further
introduce and formalize a causal debiasing perspective, which is substantiated
via a hierarchical Bayesian deep learning model, named PreRec. Our empirical
studies on real-world data show that the proposed model could significantly
improve the recommendation performance in zero- and few-shot learning settings
under both cross-market and cross-platform scenarios.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19252" title="Abstract">arXiv:2310.19252</a> [<a href="/pdf/2310.19252" title="Download PDF">pdf</a>, <a href="/format/2310.19252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Evaluation Metrics for Semantic Segmentation: Optimization  and Evaluation of Fine-grained Intersection over Union
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Berman%2C+M">Maxim Berman</a>, 
<a href="/search/cs?searchtype=author&query=Rannen-Triki%2C+A">Amal Rannen-Triki</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Tuia%2C+D">Devis Tuia</a>, 
<a href="/search/cs?searchtype=author&query=Tuytelaars%2C+T">Tinne Tuytelaars</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiaqian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Blaschko%2C+M+B">Matthew B. Blaschko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Semantic segmentation datasets often exhibit two types of imbalance:
\textit{class imbalance}, where some classes appear more frequently than others
and \textit{size imbalance}, where some objects occupy more pixels than others.
This causes traditional evaluation metrics to be biased towards
\textit{majority classes} (e.g. overall pixel-wise accuracy) and \textit{large
objects} (e.g. mean pixel-wise accuracy and per-dataset mean intersection over
union). To address these shortcomings, we propose the use of fine-grained mIoUs
along with corresponding worst-case metrics, thereby offering a more holistic
evaluation of segmentation techniques. These fine-grained metrics offer less
bias towards large objects, richer statistical information, and valuable
insights into model and dataset auditing. Furthermore, we undertake an
extensive benchmark study, where we train and evaluate 15 modern neural
networks with the proposed metrics on 12 diverse natural and aerial
segmentation datasets. Our benchmark study highlights the necessity of not
basing evaluations on a single metric and confirms that fine-grained mIoUs
reduce the bias towards large objects. Moreover, we identify the crucial role
played by architecture designs and loss functions, which lead to best practices
in optimizing fine-grained metrics. The code is available at
\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19253" title="Abstract">arXiv:2310.19253</a> [<a href="/pdf/2310.19253" title="Download PDF">pdf</a>, <a href="/format/2310.19253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flow-based Distributionally Robust Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jonghyeok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">We present a computationally efficient framework, called \texttt{FlowDRO},
for solving flow-based distributionally robust optimization (DRO) problems with
Wasserstein uncertainty sets, when requiring the worst-case distribution (also
called the Least Favorable Distribution, LFD) to be continuous so that the
algorithm can be scalable to problems with larger sample sizes and achieve
better generalization capability for the induced robust algorithms. To tackle
the computationally challenging infinitely dimensional optimization problem, we
leverage flow-based models, continuous-time invertible transport maps between
the data distribution and the target distribution, and develop a Wasserstein
proximal gradient flow type of algorithm. In practice, we parameterize the
transport maps by a sequence of neural networks progressively trained in blocks
by gradient descent. Our computational framework is general, can handle
high-dimensional data with large sample sizes, and can be useful for various
applications. We demonstrate its usage in adversarial learning,
distributionally robust hypothesis testing, and a new mechanism for data-driven
distribution perturbation differential privacy, where the proposed method gives
strong empirical performance on real high-dimensional data.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19256" title="Abstract">arXiv:2310.19256</a> [<a href="/pdf/2310.19256" title="Download PDF">pdf</a>, <a href="/format/2310.19256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Data-Driven Safety Certification for Systems Subject to Unknown  Disturbances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rober%2C+N">Nicholas Rober</a>, 
<a href="/search/cs?searchtype=author&query=Mahesh%2C+K">Karan Mahesh</a>, 
<a href="/search/cs?searchtype=author&query=Paine%2C+T+M">Tyler M. Paine</a>, 
<a href="/search/cs?searchtype=author&query=Greene%2C+M+L">Max L. Greene</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Steven Lee</a>, 
<a href="/search/cs?searchtype=author&query=Monteiro%2C+S+T">Sildomar T. Monteiro</a>, 
<a href="/search/cs?searchtype=author&query=Benjamin%2C+M+R">Michael R. Benjamin</a>, 
<a href="/search/cs?searchtype=author&query=How%2C+J+P">Jonathan P. How</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Deploying autonomous systems in safety critical settings necessitates methods
to verify their safety properties. This is challenging because real-world
systems may be subject to disturbances that affect their performance, but are
unknown a priori. This work develops a safety-verification strategy wherein
data is collected online and incorporated into a reachability analysis approach
to check in real-time that the system avoids dangerous regions of the state
space. Specifically, we employ an optimization-based moving horizon estimator
(MHE) to characterize the disturbance affecting the system, which is
incorporated into an online reachability calculation. Reachable sets are
calculated using a computational graph analysis tool to predict the possible
future states of the system and verify that they satisfy safety constraints. We
include theoretical arguments proving our approach generates reachable sets
that bound the future states of the system, as well as numerical results
demonstrating how it can be used for safety verification. Finally, we present
results from hardware experiments demonstrating our approach's ability to
perform online reachability calculations for an unmanned surface vehicle
subject to currents and actuator failures.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19257" title="Abstract">arXiv:2310.19257</a> [<a href="/pdf/2310.19257" title="Download PDF">pdf</a>, <a href="/format/2310.19257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A High-Resolution Dataset for Instance Detection with Multi-View  Instance Capture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qianqian Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunhan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+N">Nahyun Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeeeun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanan Li</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+S">Shu Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023, Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Instance detection (InsDet) is a long-lasting problem in robotics and
computer vision, aiming to detect object instances (predefined by some visual
examples) in a cluttered scene. Despite its practical significance, its
advancement is overshadowed by Object Detection, which aims to detect objects
belonging to some predefined classes. One major reason is that current InsDet
datasets are too small in scale by today's standards. For example, the popular
InsDet dataset GMU (published in 2016) has only 23 instances, far less than
COCO (80 classes), a well-known object detection dataset published in 2014. We
are motivated to introduce a new InsDet dataset and protocol. First, we define
a realistic setup for InsDet: training data consists of multi-view instance
captures, along with diverse scene images allowing synthesizing training images
by pasting instance images on them with free box annotations. Second, we
release a real-world database, which contains multi-view capture of 100 object
instances, and high-resolution (6k x 8k) testing images. Third, we extensively
study baseline methods for InsDet on our dataset, analyze their performance and
suggest future work. Somewhat surprisingly, using the off-the-shelf
class-agnostic segmentation model (Segment Anything Model, SAM) and the
self-supervised feature representation DINOv2 performs the best, achieving &gt;10
AP better than end-to-end trained InsDet models that repurpose object detectors
(e.g., FasterRCNN and RetinaNet).
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19258" title="Abstract">arXiv:2310.19258</a> [<a href="/pdf/2310.19258" title="Download PDF">pdf</a>, <a href="/format/2310.19258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Online Source-free Domain Adaptation for Object Detection by  Unsupervised Data Acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiangyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yanyuan Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingqiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dayoub%2C+F">Feras Dayoub</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Effective object detection in mobile robots is challenged by deployment in
diverse and unfamiliar environments. Online Source-Free Domain Adaptation
(O-SFDA) offers real-time model adaptation using a stream of unlabeled data
from a target domain. However, not all captured frames in mobile robotics
contain information that is beneficial for adaptation, particularly when there
is a strong domain shift. This paper introduces a novel approach to enhance
O-SFDA for adaptive object detection in mobile robots via unsupervised data
acquisition. Our methodology prioritizes the most informative unlabeled samples
for inclusion in the online training process. Empirical evaluation on a
real-world dataset reveals that our method outperforms existing
state-of-the-art O-SFDA techniques, demonstrating the viability of unsupervised
data acquisition for improving adaptive object detection in mobile robots.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19261" title="Abstract">arXiv:2310.19261</a> [<a href="/pdf/2310.19261" title="Download PDF">pdf</a>, <a href="/format/2310.19261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversify &amp; Conquer: Outcome-directed Curriculum RL via  Out-of-Distribution Disagreement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+D">Daesol Cho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seungjae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">H. Jin Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Reinforcement learning (RL) often faces the challenges of uninformed search
problems where the agent should explore without access to the domain knowledge
such as characteristics of the environment or external rewards. To tackle these
challenges, this work proposes a new approach for curriculum RL called
Diversify for Disagreement &amp; Conquer (D2C). Unlike previous curriculum learning
methods, D2C requires only a few examples of desired outcomes and works in any
environment, regardless of its geometry or the distribution of the desired
outcome examples. The proposed method performs diversification of the
goal-conditional classifiers to identify similarities between visited and
desired outcome states and ensures that the classifiers disagree on states from
out-of-distribution, which enables quantifying the unexplored region and
designing an arbitrary goal-conditioned intrinsic reward signal in a simple and
intuitive way. The proposed method then employs bipartite matching to define a
curriculum learning objective that produces a sequence of well-adjusted
intermediate goals, which enable the agent to automatically explore and conquer
the unexplored region. We present experimental results demonstrating that D2C
outperforms prior curriculum RL methods in both quantitative and qualitative
aspects, even with the arbitrarily distributed desired outcome examples.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19263" title="Abstract">arXiv:2310.19263</a> [<a href="/pdf/2310.19263" title="Download PDF">pdf</a>, <a href="/ps/2310.19263" title="Download PostScript">ps</a>, <a href="/format/2310.19263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Metadata-Driven Approach to Understand Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T+W">Ting Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaqi Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have achieved remarkable success in various
applications, but their performance can be sensitive to specific data
properties of the graph datasets they operate on. Current literature on
understanding the limitations of GNNs has primarily employed a
$\textit{model-driven}$ approach that leverage heuristics and domain knowledge
from network science or graph theory to model the GNN behaviors, which is
time-consuming and highly subjective. In this work, we propose a
$\textit{metadata-driven}$ approach to analyze the sensitivity of GNNs to graph
data properties, motivated by the increasing availability of graph learning
benchmarks. We perform a multivariate sparse regression analysis on the
metadata derived from benchmarking GNN performance across diverse datasets,
yielding a set of salient data properties. To validate the effectiveness of our
data-driven approach, we focus on one identified data property, the degree
distribution, and investigate how this property influences GNN performance
through theoretical analysis and controlled experiments. Our theoretical
findings reveal that datasets with more balanced degree distribution exhibit
better linear separability of node representations, thus leading to better GNN
performance. We also conduct controlled experiments using synthetic datasets
with varying degree distributions, and the results align well with our
theoretical findings. Collectively, both the theoretical analysis and
controlled experiments verify that the proposed metadata-driven approach is
effective in identifying critical data properties for GNNs.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19264" title="Abstract">arXiv:2310.19264</a> [<a href="/pdf/2310.19264" title="Download PDF">pdf</a>, <a href="/format/2310.19264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sound of Story: Multi-modal Storytelling with Audio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bae%2C+J">Jaeyeon Bae</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+S">Seokhoon Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Seokun Kang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+N">Namgi Han</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jae-Yon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyounghun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehwan Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023, project: <a href="https://github.com/Sosdatasets/SoS_Dataset/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Storytelling is multi-modal in the real world. When one tells a story, one
may use all of the visualizations and sounds along with the story itself.
However, prior studies on storytelling datasets and tasks have paid little
attention to sound even though sound also conveys meaningful semantics of the
story. Therefore, we propose to extend story understanding and telling areas by
establishing a new component called "background sound" which is story
context-based audio without any linguistic information. For this purpose, we
introduce a new dataset, called "Sound of Story (SoS)", which has paired image
and text sequences with corresponding sound or background music for a story. To
the best of our knowledge, this is the largest well-curated dataset for
storytelling with sound. Our SoS dataset consists of 27,354 stories with 19.6
images per story and 984 hours of speech-decoupled audio such as background
music and other sounds. As benchmark tasks for storytelling with sound and the
dataset, we propose retrieval tasks between modalities, and audio generation
tasks from image-text sequences, introducing strong baselines for them. We
believe the proposed dataset and tasks may shed light on the multi-modal
understanding of storytelling in terms of sound. Downloading the dataset and
baseline codes for each task will be released in the link:
https://github.com/Sosdatasets/SoS_Dataset.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19267" title="Abstract">arXiv:2310.19267</a> [<a href="/pdf/2310.19267" title="Download PDF">pdf</a>, <a href="/format/2310.19267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of the CLAIMSCAN-2023: Uncovering Truth in Social Media through  Claim Detection and Identification of Claim Spans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sundriyal%2C+M">Megha Sundriyal</a>, 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+M+S">Md Shad Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">A significant increase in content creation and information exchange has been
made possible by the quick development of online social media platforms, which
has been very advantageous. However, these platforms have also become a haven
for those who disseminate false information, propaganda, and fake news. Claims
are essential in forming our perceptions of the world, but sadly, they are
frequently used to trick people by those who spread false information. To
address this problem, social media giants employ content moderators to filter
out fake news from the actual world. However, the sheer volume of information
makes it difficult to identify fake news effectively. Therefore, it has become
crucial to automatically identify social media posts that make such claims,
check their veracity, and differentiate between credible and false claims. In
response, we presented CLAIMSCAN in the 2023 Forum for Information Retrieval
Evaluation (FIRE'2023). The primary objectives centered on two crucial tasks:
Task A, determining whether a social media post constitutes a claim, and Task
B, precisely identifying the words or phrases within the post that form the
claim. Task A received 40 registrations, demonstrating a strong interest and
engagement in this timely challenge. Meanwhile, Task B attracted participation
from 28 teams, highlighting its significance in the digital era of
misinformation.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19268" title="Abstract">arXiv:2310.19268</a> [<a href="/pdf/2310.19268" title="Download PDF">pdf</a>, <a href="/format/2310.19268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via  Social Commonsense and Linguistic Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xi%2C+R">Ruijie Xi</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M+P">Munindar P. Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
<p class="mathjax">Given the increasing realism of social interactions online, social media
offers an unprecedented avenue to evaluate real-life moral scenarios. We
examine posts from Reddit, where authors and commenters share their moral
judgments on who is blameworthy. We employ computational techniques to
investigate factors influencing moral judgments, including (1) events
activating social commonsense and (2) linguistic signals. To this end, we focus
on excerpt-which we term moral sparks-from original posts that commenters
include to indicate what motivates their moral judgments. By examining over
24,672 posts and 175,988 comments, we find that event-related negative personal
traits (e.g., immature and rude) attract attention and stimulate blame,
implying a dependent relationship between moral sparks and blameworthiness.
Moreover, language that impacts commenters' cognitive processes to depict
events and characters enhances the probability of an excerpt become a moral
spark, while factual and concrete descriptions tend to inhibit this effect.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19270" title="Abstract">arXiv:2310.19270</a> [<a href="/pdf/2310.19270" title="Download PDF">pdf</a>, <a href="/format/2310.19270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariant kernels on Riemannian symmetric spaces: a harmonic-analytic  approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Da+Costa%2C+N">Nathael Da Costa</a>, 
<a href="/search/cs?searchtype=author&query=Mostajeran%2C+C">Cyrus Mostajeran</a>, 
<a href="/search/cs?searchtype=author&query=Ortega%2C+J">Juan-Pablo Ortega</a>, 
<a href="/search/cs?searchtype=author&query=Said%2C+S">Salem Said</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Differential Geometry (math.DG); Machine Learning (stat.ML)

</div>
<p class="mathjax">This work aims to prove that the classical Gaussian kernel, when defined on a
non-Euclidean symmetric space, is never positive-definite for any choice of
parameter. To achieve this goal, the paper develops new geometric and
analytical arguments. These provide a rigorous characterization of the
positive-definiteness of the Gaussian kernel, which is complete but for a
limited number of scenarios in low dimensions that are treated by numerical
computations. Chief among these results are the L$^{\!\scriptscriptstyle
p}$-$\hspace{0.02cm}$Godement theorems (where $p = 1,2$), which provide
verifiable necessary and sufficient conditions for a kernel defined on a
symmetric space of non-compact type to be positive-definite. A celebrated
theorem, sometimes called the Bochner-Godement theorem, already gives such
conditions and is far more general in its scope, but is especially hard to
apply. Beyond the connection with the Gaussian kernel, the new results in this
work lay out a blueprint for the study of invariant kernels on symmetric
spaces, bringing forth specific harmonic analysis tools that suggest many
future applications.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19271" title="Abstract">arXiv:2310.19271</a> [<a href="/pdf/2310.19271" title="Download PDF">pdf</a>, <a href="/format/2310.19271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to love diligent trolls: Accounting for rater effects in the  dialogue safety task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ilagan%2C+M+J">Michael John Ilagan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accept-Findings at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Chatbots have the risk of generating offensive utterances, which must be
avoided. Post-deployment, one way for a chatbot to continuously improve is to
source utterance/label pairs from feedback by live users. However, among users
are trolls, who provide training examples with incorrect labels. To de-troll
training data, previous work removed training examples that have high
user-aggregated cross-validation (CV) error. However, CV is expensive; and in a
coordinated attack, CV may be overwhelmed by trolls in number and in
consistency among themselves. In the present work, I address both limitations
by proposing a solution inspired by methodology in automated essay scoring
(AES): have multiple users rate each utterance, then perform latent class
analysis (LCA) to infer correct labels. As it does not require GPU
computations, LCA is inexpensive. In experiments, I found that the AES-like
solution can infer training labels with high accuracy when trolls are
consistent, even when trolls are the majority.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19272" title="Abstract">arXiv:2310.19272</a> [<a href="/pdf/2310.19272" title="Download PDF">pdf</a>, <a href="/format/2310.19272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NPCL: Neural Processes for Uncertainty-Aware Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Saurav Jha</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+D">Dong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">He Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a poster at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Continual learning (CL) aims to train deep neural networks efficiently on
streaming data while limiting the forgetting caused by new tasks. However,
learning transferable knowledge with less interference between tasks is
difficult, and real-world deployment of CL models is limited by their inability
to measure predictive uncertainties. To address these issues, we propose
handling CL tasks with neural processes (NPs), a class of meta-learners that
encode different tasks into probabilistic distributions over functions all
while providing reliable uncertainty estimates. Specifically, we propose an
NP-based CL approach (NPCL) with task-specific modules arranged in a
hierarchical latent variable model. We tailor regularizers on the learned
latent distributions to alleviate forgetting. The uncertainty estimation
capabilities of the NPCL can also be used to handle the task head/module
inference challenge in CL. Our experiments show that the NPCL outperforms
previous CL approaches. We validate the effectiveness of uncertainty estimation
in the NPCL for identifying novel data and evaluating instance-level model
confidence. Code is available at \url{https://github.com/srvCodes/NPCL}.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19273" title="Abstract">arXiv:2310.19273</a> [<a href="/pdf/2310.19273" title="Download PDF">pdf</a>, <a href="/format/2310.19273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Memory Perturbation Equation: Understanding Model&#x27;s Sensitivity to  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nickl%2C+P">Peter Nickl</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tailor%2C+D">Dharmesh Tailor</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6llenhoff%2C+T">Thomas M&#xf6;llenhoff</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+E">Mohammad Emtiyaz Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Understanding model's sensitivity to its training data is crucial but can
also be challenging and costly, especially during training. To simplify such
issues, we present the Memory-Perturbation Equation (MPE) which relates model's
sensitivity to perturbation in its training data. Derived using Bayesian
principles, the MPE unifies existing sensitivity measures, generalizes them to
a wide-variety of models and algorithms, and unravels useful properties
regarding sensitivities. Our empirical results show that sensitivity estimates
obtained during training can be used to faithfully predict generalization on
unseen test data. The proposed equation is expected to be useful for future
research on robust and adaptive learning.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19274" title="Abstract">arXiv:2310.19274</a> [<a href="/pdf/2310.19274" title="Download PDF">pdf</a>, <a href="/format/2310.19274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction of Effective Elastic Moduli of Rocks using Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+J">Jaehong Chung</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+R">Rasool Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">WaiChing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+W">Wei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Mukerji%2C+T">Tapan Mukerji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph); Geophysics (physics.geo-ph)

</div>
<p class="mathjax">This study presents a Graph Neural Networks (GNNs)-based approach for
predicting the effective elastic moduli of rocks from their digital CT-scan
images. We use the Mapper algorithm to transform 3D digital rock images into
graph datasets, encapsulating essential geometrical information. These graphs,
after training, prove effective in predicting elastic moduli. Our GNN model
shows robust predictive capabilities across various graph sizes derived from
various subcube dimensions. Not only does it perform well on the test dataset,
but it also maintains high prediction accuracy for unseen rocks and unexplored
subcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs)
reveals the superior performance of GNNs in predicting unseen rock properties.
Moreover, the graph representation of microstructures significantly reduces GPU
memory requirements (compared to the grid representation for CNNs), enabling
greater flexibility in the batch size selection. This work demonstrates the
potential of GNN models in enhancing the prediction accuracy of rock properties
and boosting the efficiency of digital rock analysis.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19275" title="Abstract">arXiv:2310.19275</a> [<a href="/pdf/2310.19275" title="Download PDF">pdf</a>, <a href="/format/2310.19275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eliciting Topic Hierarchies from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Grace Li</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+T">Tao Long</a>, 
<a href="/search/cs?searchtype=author&query=Chilton%2C+L+B">Lydia B. Chilton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Finding topics to write about can be a mentally demanding process. However,
topic hierarchies can help writers explore topics of varying levels of
specificity. In this paper, we use large language models (LLMs) to help
construct topic hierarchies. Although LLMs have access to such knowledge, it
can be difficult to elicit due to issues of specificity, scope, and repetition.
We designed and tested three different prompting techniques to find one that
maximized accuracy. We found that prepending the general topic area to a prompt
yielded the most accurate results with 85% accuracy. We discuss applications of
this research including STEM writing, education, and content creation.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19277" title="Abstract">arXiv:2310.19277</a> [<a href="/pdf/2310.19277" title="Download PDF">pdf</a>, <a href="/format/2310.19277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering based Multiple Anchors High-Dimensional Model Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Xiong%2C+M">Meixin Xiong</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+L">Liuhong Chen</a>, 
<a href="/search/math?searchtype=author&query=Ming%2C+J">Ju Ming</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this work, a cut high-dimensional model representation (cut-HDMR)
expansion based on multiple anchors is constructed via the clustering method.
Specifically, a set of random input realizations is drawn from the parameter
space and grouped by the centroidal Voronoi tessellation (CVT) method. Then for
each cluster, the centroid is set as the reference, thereby the corresponding
zeroth-order term can be determined directly. While for non-zero order terms of
each cut-HDMR, a set of discrete points is selected for each input component,
and the Lagrange interpolation method is applied. For a new input, the cut-HDMR
corresponding to the nearest centroid is used to compute its response.
Numerical experiments with high-dimensional integral and elliptic stochastic
partial differential equation as backgrounds show that the CVT based multiple
anchors cut-HDMR can alleviate the negative impact of a single inappropriate
anchor point, and has higher accuracy than the average of several expansions.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19283" title="Abstract">arXiv:2310.19283</a> [<a href="/pdf/2310.19283" title="Download PDF">pdf</a>, <a href="/format/2310.19283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature  Extraction for IMU-based Human Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Enokibori%2C+Y">Yu Enokibori</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper proposes rTsfNet, a DNN model with Multi-head 3D Rotation and Time
Series Feature Extraction, as a new DNN model for IMU-based human activity
recognition (HAR). rTsfNet automatically selects 3D bases from which features
should be derived by deriving 3D rotation parameters within the DNN. Then, time
series features (TSFs), the wisdom of many researchers, are derived and realize
HAR using MLP. Although a model that does not use CNN, it achieved the highest
accuracy than existing models under well-managed benchmark conditions and
multiple datasets: UCI HAR, PAMAP2, Daphnet, and OPPORTUNITY, which target
different activities.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19285" title="Abstract">arXiv:2310.19285</a> [<a href="/pdf/2310.19285" title="Download PDF">pdf</a>, <a href="/format/2310.19285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Facilitating Graph Neural Networks with Random Walk on Simplicial  Complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Cai Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">Node-level random walk has been widely used to improve Graph Neural Networks.
However, there is limited attention to random walk on edge and, more generally,
on $k$-simplices. This paper systematically analyzes how random walk on
different orders of simplicial complexes (SC) facilitates GNNs in their
theoretical expressivity. First, on $0$-simplices or node level, we establish a
connection between existing positional encoding (PE) and structure encoding
(SE) methods through the bridge of random walk. Second, on $1$-simplices or
edge level, we bridge edge-level random walk and Hodge $1$-Laplacians and
design corresponding edge PE respectively. In the spatial domain, we directly
make use of edge level random walk to construct EdgeRWSE. Based on the spectral
analysis of Hodge $1$-Laplcians, we propose Hodge1Lap, a permutation
equivariant and expressive edge-level positional encoding. Third, we generalize
our theory to random walk on higher-order simplices and propose the general
principle to design PE on simplices based on random walk and Hodge Laplacians.
Inter-level random walk is also introduced to unify a wide range of simplicial
networks. Extensive experiments verify the effectiveness of our random
walk-based methods.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19287" title="Abstract">arXiv:2310.19287</a> [<a href="/pdf/2310.19287" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Scalability and Reliability in Semi-Decentralized Federated  Learning With Blockchain: Trust Penalization and Asynchronous Functionality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+A+K">Ajay Kumar Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+A">Faijan Ahamad Khan</a>, 
<a href="/search/cs?searchtype=author&query=Shaikh%2C+M+A">Mohammed Afaan Shaikh</a>, 
<a href="/search/cs?searchtype=author&query=Jaberzadeh%2C+A">Amir Jaberzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+J">Jason Geng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in 2023 IEEE Ubiquitous Computing, Electronics &amp; Mobile Communication Conference (IEEE UEMCON)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The paper presents an innovative approach to address the challenges of
scalability and reliability in Distributed Federated Learning by leveraging the
integration of blockchain technology. The paper focuses on enhancing the
trustworthiness of participating nodes through a trust penalization mechanism
while also enabling asynchronous functionality for efficient and robust model
updates. By combining Semi-Decentralized Federated Learning with Blockchain
(SDFL-B), the proposed system aims to create a fair, secure and transparent
environment for collaborative machine learning without compromising data
privacy. The research presents a comprehensive system architecture,
methodologies, experimental results, and discussions that demonstrate the
advantages of this novel approach in fostering scalable and reliable SDFL-B
systems.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19289" title="Abstract">arXiv:2310.19289</a> [<a href="/pdf/2310.19289" title="Download PDF">pdf</a>, <a href="/format/2310.19289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMLNet: Adversarial Mutual Learning Neural Network for  Non-AutoRegressive Multi-Horizon Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Multi-horizon time series forecasting, crucial across diverse domains,
demands high accuracy and speed. While AutoRegressive (AR) models excel in
short-term predictions, they suffer speed and error issues as the horizon
extends. Non-AutoRegressive (NAR) models suit long-term predictions but
struggle with interdependence, yielding unrealistic results. We introduce
AMLNet, an innovative NAR model that achieves realistic forecasts through an
online Knowledge Distillation (KD) approach. AMLNet harnesses the strengths of
both AR and NAR models by training a deep AR decoder and a deep NAR decoder in
a collaborative manner, serving as ensemble teachers that impart knowledge to a
shallower NAR decoder. This knowledge transfer is facilitated through two key
mechanisms: 1) outcome-driven KD, which dynamically weights the contribution of
KD losses from the teacher models, enabling the shallow NAR decoder to
incorporate the ensemble's diversity; and 2) hint-driven KD, which employs
adversarial training to extract valuable insights from the model's hidden
states for distillation. Extensive experimentation showcases AMLNet's
superiority over conventional AR and NAR models, thereby presenting a promising
avenue for multi-horizon time series forecasting that enhances accuracy and
expedites computation.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19290" title="Abstract">arXiv:2310.19290</a> [<a href="/pdf/2310.19290" title="Download PDF">pdf</a>, <a href="/format/2310.19290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing eyebrow region for morphed image detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zafar%2C+A">Abdullah Zafar</a>, 
<a href="/search/cs?searchtype=author&query=Busch%2C+C">Christoph Busch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Facial images in passports are designated as primary identifiers for the
verification of travelers according to the International Civil Aviation
Organization (ICAO). Hence, it is important to ascertain the sanctity of the
facial images stored in the electronic Machine-Readable Travel Document
(eMRTD). With the introduction of automated border control (ABC) systems that
rely on face recognition for the verification of travelers, it is even more
crucial to have a system to ensure that the image stored in the eMRTD is free
from any alteration that can hinder or abuse the normal working of a facial
recognition system. One such attack against these systems is the face-morphing
attack. Even though many techniques exist to detect morphed images, morphing
algorithms are also improving to evade these detections. In this work, we
analyze the eyebrow region for morphed image detection. The proposed method is
based on analyzing the frequency content of the eyebrow region. The method was
evaluated on two datasets that each consisted of morphed images created using
two algorithms. The findings suggest that the proposed method can serve as a
valuable tool in morphed image detection, and can be used in various
applications where image authenticity is critical.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19292" title="Abstract">arXiv:2310.19292</a> [<a href="/pdf/2310.19292" title="Download PDF">pdf</a>, <a href="/format/2310.19292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusing Temporal Graphs into Transformers for Time-Sensitive Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xin Su</a>, 
<a href="/search/cs?searchtype=author&query=Howard%2C+P">Phillip Howard</a>, 
<a href="/search/cs?searchtype=author&query=Hakim%2C+N">Nagib Hakim</a>, 
<a href="/search/cs?searchtype=author&query=Bethard%2C+S">Steven Bethard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Answering time-sensitive questions from long documents requires temporal
reasoning over the times in questions and documents. An important open question
is whether large language models can perform such reasoning solely using a
provided text document, or whether they can benefit from additional temporal
information extracted using other systems. We address this research question by
applying existing temporal information extraction systems to construct temporal
graphs of events, times, and temporal relations in questions and documents. We
then investigate different approaches for fusing these graphs into Transformer
models. Experimental results show that our proposed approach for fusing
temporal graphs into input text substantially enhances the temporal reasoning
capabilities of Transformer models with or without fine-tuning. Additionally,
our proposed method outperforms various graph convolution-based approaches and
establishes a new state-of-the-art performance on SituatedQA and three splits
of TimeQA.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19295" title="Abstract">arXiv:2310.19295</a> [<a href="/pdf/2310.19295" title="Download PDF">pdf</a>, <a href="/format/2310.19295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROAM: memory-efficient large DNN training via optimized operator  ordering and memory layout
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+H">Huiyao Shu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Ang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Ziji Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hanyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">As deep learning models continue to increase in size, the memory requirements
for training have surged. While high-level techniques like offloading,
recomputation, and compression can alleviate memory pressure, they also
introduce overheads. However, a memory-efficient execution plan that includes a
reasonable operator execution order and tensor memory layout can significantly
increase the models' memory efficiency and reduce overheads from high-level
techniques. In this paper, we propose ROAM which operates on computation graph
level to derive memory-efficient execution plan with optimized operator order
and tensor memory layout for models. We first propose sophisticated theories
that carefully consider model structure and training memory load to support
optimization for large complex graphs that have not been well supported in the
past. An efficient tree-based algorithm is further proposed to search task
divisions automatically, along with delivering high performance and
effectiveness to solve the problem. Experiments show that ROAM achieves a
substantial memory reduction of 35.7%, 13.3%, and 27.2% compared to Pytorch and
two state-of-the-art methods and offers a remarkable 53.7x speedup. The
evaluation conducted on the expansive GPT2-XL further validates ROAM's
scalability.
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19297" title="Abstract">arXiv:2310.19297</a> [<a href="/pdf/2310.19297" title="Download PDF">pdf</a>, <a href="/format/2310.19297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Measuring Fairness in Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teo%2C+C+T+H">Christopher T. H. Teo</a>, 
<a href="/search/cs?searchtype=author&query=Abdollahzadeh%2C+M">Milad Abdollahzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+N">Ngai-Man Cheung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)

</div>
<p class="mathjax">Recently, there has been increased interest in fair generative models. In
this work, we conduct, for the first time, an in-depth study on fairness
measurement, a critical component in gauging progress on fair generative
models. We make three contributions. First, we conduct a study that reveals
that the existing fairness measurement framework has considerable measurement
errors, even when highly accurate sensitive attribute (SA) classifiers are
used. These findings cast doubts on previously reported fairness improvements.
Second, to address this issue, we propose CLassifier Error-Aware Measurement
(CLEAM), a new framework which uses a statistical model to account for
inaccuracies in SA classifiers. Our proposed CLEAM reduces measurement errors
significantly, e.g., 4.98% $\rightarrow$ 0.62% for StyleGAN2 w.r.t. Gender.
Additionally, CLEAM achieves this with minimal additional overhead. Third, we
utilize CLEAM to measure fairness in important text-to-image generator and
GANs, revealing considerable biases in these models that raise concerns about
their applications. Code and more resources:
https://sutd-visual-computing-group.github.io/CLEAM/.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19301" title="Abstract">arXiv:2310.19301</a> [<a href="/pdf/2310.19301" title="Download PDF">pdf</a>, <a href="/format/2310.19301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond  Visual Common Sense
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kankan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+E">Eason Lai</a>, 
<a href="/search/cs?searchtype=author&query=Yeong%2C+W+B+A">Wei Bin Au Yeong</a>, 
<a href="/search/cs?searchtype=author&query=Mouratidis%2C+K">Kyriakos Mouratidis</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jing Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the camera-ready version of the paper that will be published in the EMNLP 2023 Findings (Singapore, 6-10 December 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Humans possess a strong capability for reasoning beyond common sense. For
example, given an unconventional image of a goldfish laying on the table next
to an empty fishbowl, a human would effortlessly determine that the fish is not
inside the fishbowl. The case, however, may be different for a vision-language
model, whose reasoning could gravitate towards the common scenario that the
fish is inside the bowl, despite the visual input. In this paper, we introduce
a novel probing dataset named ROME (reasoning beyond commonsense knowledge) to
evaluate whether the state-of-the-art pre-trained vision-language models have
the reasoning capability to correctly interpret counter-intuitive content. ROME
contains images that defy commonsense knowledge with regards to color, shape,
material, size and positional relation. Experiments on the state-of-the-art
pre-trained vision-language models reveal that most of these models are still
largely incapable of interpreting counter-intuitive scenarios. We hope that
ROME will spur further investigations on reasoning beyond commonsense knowledge
in vision-language research.
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19303" title="Abstract">arXiv:2310.19303</a> [<a href="/pdf/2310.19303" title="Download PDF">pdf</a>, <a href="/format/2310.19303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extracting user needs with Chat-GPT for dialogue recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sato%2C+Y">Yugen Sato</a>, 
<a href="/search/cs?searchtype=author&query=Nakajima%2C+T">Taisei Nakajima</a>, 
<a href="/search/cs?searchtype=author&query=Kawamoto%2C+T">Tatsuki Kawamoto</a>, 
<a href="/search/cs?searchtype=author&query=Takagi%2C+T">Tomohiro Takagi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Large-scale language models (LLMs), such as ChatGPT, are becoming
increasingly sophisticated and exhibit human-like capabilities, playing an
essential role in assisting humans in a variety of everyday tasks. An important
application of AI is interactive recommendation systems that respond to human
inquiries and make recommendations tailored to the user. In most conventional
interactive recommendation systems, the language model is used only as a
dialogue model, and there is a separate recommendation system. This is due to
the fact that the language model used as a dialogue system does not have the
capability to serve as a recommendation system. Therefore, we will realize the
construction of a dialogue system with recommendation capability by using
OpenAI's Chat-GPT, which has a very high inference capability as a dialogue
system and the ability to generate high-quality sentences, and verify the
effectiveness of the system.
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19304" title="Abstract">arXiv:2310.19304</a> [<a href="/pdf/2310.19304" title="Download PDF">pdf</a>, <a href="/format/2310.19304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Federated Learning over Vertically and Horizontally  Partitioned Data for Financial Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kadhe%2C+S+R">Swanand Ravindra Kadhe</a>, 
<a href="/search/cs?searchtype=author&query=Ludwig%2C+H">Heiko Ludwig</a>, 
<a href="/search/cs?searchtype=author&query=Baracaldo%2C+N">Nathalie Baracaldo</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+A">Alan King</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Houck%2C+K">Keith Houck</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+A">Ambrish Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Purcell%2C+M">Mark Purcell</a>, 
<a href="/search/cs?searchtype=author&query=Holohan%2C+N">Naoise Holohan</a>, 
<a href="/search/cs?searchtype=author&query=Takeuchi%2C+M">Mikio Takeuchi</a>, 
<a href="/search/cs?searchtype=author&query=Kawahara%2C+R">Ryo Kawahara</a>, 
<a href="/search/cs?searchtype=author&query=Drucker%2C+N">Nir Drucker</a>, 
<a href="/search/cs?searchtype=author&query=Shaul%2C+H">Hayim Shaul</a>, 
<a href="/search/cs?searchtype=author&query=Kushnir%2C+E">Eyal Kushnir</a>, 
<a href="/search/cs?searchtype=author&query=Soceanu%2C+O">Omri Soceanu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Prize Winner in the U.S. Privacy Enhancing Technologies (PETs) Prize Challenge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The effective detection of evidence of financial anomalies requires
collaboration among multiple entities who own a diverse set of data, such as a
payment network system (PNS) and its partner banks. Trust among these financial
institutions is limited by regulation and competition. Federated learning (FL)
enables entities to collaboratively train a model when data is either
vertically or horizontally partitioned across the entities. However, in
real-world financial anomaly detection scenarios, the data is partitioned both
vertically and horizontally and hence it is not possible to use existing FL
approaches in a plug-and-play manner.
<br />Our novel solution, PV4FAD, combines fully homomorphic encryption (HE),
secure multi-party computation (SMPC), differential privacy (DP), and
randomization techniques to balance privacy and accuracy during training and to
prevent inference threats at model deployment time. Our solution provides input
privacy through HE and SMPC, and output privacy against inference time attacks
through DP. Specifically, we show that, in the honest-but-curious threat model,
banks do not learn any sensitive features about PNS transactions, and the PNS
does not learn any information about the banks' dataset but only learns
prediction labels. We also develop and analyze a DP mechanism to protect output
privacy during inference. Our solution generates high-utility models by
significantly reducing the per-bank noise level while satisfying distributed
DP. To ensure high accuracy, our approach produces an ensemble model, in
particular, a random forest. This enables us to take advantage of the
well-known properties of ensembles to reduce variance and increase accuracy.
Our solution won second prize in the first phase of the U.S. Privacy Enhancing
Technologies (PETs) Prize Challenge.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19308" title="Abstract">arXiv:2310.19308</a> [<a href="/pdf/2310.19308" title="Download PDF">pdf</a>, <a href="/format/2310.19308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Free from Bellman Completeness: Trajectory Stitching via Model-based  Return-conditioned Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhaoyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chuning Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Runlong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Q">Qiwen Cui</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon Shaolei Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Off-policy dynamic programming (DP) techniques such as $Q$-learning have
proven to be an important technique for solving sequential decision-making
problems. However, in the presence of function approximation such algorithms
are not guaranteed to converge, often diverging due to the absence of
Bellman-completeness in the function classes considered, a crucial condition
for the success of DP-based methods. In this paper, we show how off-policy
learning techniques based on return-conditioned supervised learning (RCSL) are
able to circumvent these challenges of Bellman completeness, converging under
significantly more relaxed assumptions inherited from supervised learning. We
prove there exists a natural environment in which if one uses two-layer
multilayer perceptron as the function approximator, the layer width needs to
grow linearly with the state space size to satisfy Bellman-completeness while a
constant layer width is enough for RCSL. These findings take a step towards
explaining the superior empirical performance of RCSL methods compared to
DP-based methods in environments with near-optimal datasets. Furthermore, in
order to learn from sub-optimal datasets, we propose a simple framework called
MBRCSL, granting RCSL methods the ability of dynamic programming to stitch
together segments from distinct trajectories. MBRCSL leverages learned dynamics
models and forward sampling to accomplish trajectory stitching while avoiding
the need for Bellman completeness that plagues all dynamic programming
algorithms. We propose both theoretical analysis and experimental evaluation to
back these claims, outperforming state-of-the-art model-free and model-based
offline RL algorithms across several simulated robotics problems.
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19311" title="Abstract">arXiv:2310.19311</a> [<a href="/pdf/2310.19311" title="Download PDF">pdf</a>, <a href="/format/2310.19311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relation-driven Query of Multiple Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuhan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zikun Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+W">Weiwei Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haidong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+D">Di Weng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yingcai Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Querying time series based on their relations is a crucial part of multiple
time series analysis. By retrieving and understanding time series relations,
analysts can easily detect anomalies and validate hypotheses in complex time
series datasets. However, current relation extraction approaches, including
knowledge- and data-driven ones, tend to be laborious and do not support
heterogeneous relations. By conducting a formative study with 11 experts, we
concluded 6 time series relations, including correlation, causality,
similarity, lag, arithmetic, and meta, and summarized three pain points in
querying time series involving these relations. We proposed RelaQ, an
interactive system that supports the time series query via relation
specifications. RelaQ allows users to intuitively specify heterogeneous
relations when querying multiple time series, understand the query results
based on a scalable, multi-level visualization, and explore possible relations
beyond the existing queries. RelaQ is evaluated with two use cases and a user
study with 12 participants, showing promising effectiveness and usability.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19313" title="Abstract">arXiv:2310.19313</a> [<a href="/pdf/2310.19313" title="Download PDF">pdf</a>, <a href="/format/2310.19313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L2T-DLN: Learning to Teach with Dynamic Loss Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hai%2C+Z">Zhoyang Hai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liyuan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiabi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengzheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yunita%2C+M">Mirna Yunita</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the concept of teaching being introduced to the machine learning
community, a teacher model start using dynamic loss functions to teach the
training of a student model. The dynamic intends to set adaptive loss functions
to different phases of student model learning. In existing works, the teacher
model 1) merely determines the loss function based on the present states of the
student model, i.e., disregards the experience of the teacher; 2) only utilizes
the states of the student model, e.g., training iteration number and
loss/accuracy from training/validation sets, while ignoring the states of the
loss function. In this paper, we first formulate the loss adjustment as a
temporal task by designing a teacher model with memory units, and, therefore,
enables the student learning to be guided by the experience of the teacher
model. Then, with a dynamic loss network, we can additionally use the states of
the loss to assist the teacher learning in enhancing the interactions between
the teacher and the student model. Extensive experiments demonstrate our
approach can enhance student learning and improve the performance of various
deep models on real-world tasks, including classification, objective detection,
and semantic segmentation scenarios.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19314" title="Abstract">arXiv:2310.19314</a> [<a href="/pdf/2310.19314" title="Download PDF">pdf</a>, <a href="/ps/2310.19314" title="Download PostScript">ps</a>, <a href="/format/2310.19314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The minimax property in infinite two-person win-lose games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holzman%2C+R">Ron Holzman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH); Combinatorics (math.CO)

</div>
<p class="mathjax">We explore a version of the minimax theorem for two-person win-lose games
with infinitely many pure strategies. In the countable case, we give a
combinatorial condition on the game which implies the minimax property. In the
general case, we prove that a game satisfies the minimax property along with
all its subgames if and only if none of its subgames is isomorphic to the
"larger number game." This generalizes a recent theorem of Hanneke, Livni and
Moran. We also propose several applications of our results outside of game
theory.
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19321" title="Abstract">arXiv:2310.19321</a> [<a href="/pdf/2310.19321" title="Download PDF">pdf</a>, <a href="/format/2310.19321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D4Explainer: In-Distribution GNN Explanations via Discrete Denoising  Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jialin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shirley Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhijit Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+R">Rex Ying</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023, Camera Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The widespread deployment of Graph Neural Networks (GNNs) sparks significant
interest in their explainability, which plays a vital role in model auditing
and ensuring trustworthy graph learning. The objective of GNN explainability is
to discern the underlying graph structures that have the most significant
impact on model predictions. Ensuring that explanations generated are reliable
necessitates consideration of the in-distribution property, particularly due to
the vulnerability of GNNs to out-of-distribution data. Unfortunately,
prevailing explainability methods tend to constrain the generated explanations
to the structure of the original graph, thereby downplaying the significance of
the in-distribution property and resulting in explanations that lack
reliability. To address these challenges, we propose D4Explainer, a novel
approach that provides in-distribution GNN explanations for both counterfactual
and model-level explanation scenarios. The proposed D4Explainer incorporates
generative graph distribution learning into the optimization objective, which
accomplishes two goals: 1) generate a collection of diverse counterfactual
graphs that conform to the in-distribution property for a given instance, and
2) identify the most discriminative graph patterns that contribute to a
specific class prediction, thus serving as model-level explanations. It is
worth mentioning that D4Explainer is the first unified framework that combines
both counterfactual and model-level explanations. Empirical evaluations
conducted on synthetic and real-world datasets provide compelling evidence of
the state-of-the-art performance achieved by D4Explainer in terms of
explanation accuracy, faithfulness, diversity, and robustness.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19322" title="Abstract">arXiv:2310.19322</a> [<a href="/pdf/2310.19322" title="Download PDF">pdf</a>, <a href="/format/2310.19322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProNet: Progressive Neural Network for Multi-Horizon Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yang Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we introduce ProNet, an novel deep learning approach designed
for multi-horizon time series forecasting, adaptively blending autoregressive
(AR) and non-autoregressive (NAR) strategies. Our method involves dividing the
forecasting horizon into segments, predicting the most crucial steps in each
segment non-autoregressively, and the remaining steps autoregressively. The
segmentation process relies on latent variables, which effectively capture the
significance of individual time steps through variational inference. In
comparison to AR models, ProNet showcases remarkable advantages, requiring
fewer AR iterations, resulting in faster prediction speed, and mitigating error
accumulation. On the other hand, when compared to NAR models, ProNet takes into
account the interdependency of predictions in the output space, leading to
improved forecasting accuracy. Our comprehensive evaluation, encompassing four
large datasets, and an ablation study, demonstrate the effectiveness of ProNet,
highlighting its superior performance in terms of accuracy and prediction
speed, outperforming state-of-the-art AR and NAR forecasting models.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19324" title="Abstract">arXiv:2310.19324</a> [<a href="/pdf/2310.19324" title="Download PDF">pdf</a>, <a href="/format/2310.19324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TempME: Towards the Explainability of Temporal Graph Neural Networks via  Motif Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jialin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+R">Rex Ying</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023, Camera Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Temporal graphs are widely used to model dynamic systems with time-varying
interactions. In real-world scenarios, the underlying mechanisms of generating
future interactions in dynamic systems are typically governed by a set of
recurring substructures within the graph, known as temporal motifs. Despite the
success and prevalence of current temporal graph neural networks (TGNN), it
remains uncertain which temporal motifs are recognized as the significant
indications that trigger a certain prediction from the model, which is a
critical challenge for advancing the explainability and trustworthiness of
current TGNNs. To address this challenge, we propose a novel approach, called
Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal
motifs guiding the prediction of TGNNs. Derived from the information bottleneck
principle, TempME extracts the most interaction-related motifs while minimizing
the amount of contained information to preserve the sparsity and succinctness
of the explanation. Events in the explanations generated by TempME are verified
to be more spatiotemporally correlated than those of existing approaches,
providing more understandable insights. Extensive experiments validate the
superiority of TempME, with up to 8.21% increase in terms of explanation
accuracy across six real-world datasets and up to 22.96% increase in boosting
the prediction Average Precision of current TGNNs.
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19331" title="Abstract">arXiv:2310.19331</a> [<a href="/pdf/2310.19331" title="Download PDF">pdf</a>, <a href="/format/2310.19331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdapINT: A Flexible and Adaptive In-Band Network Telemetry System Based  on Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Penghui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pi%2C+Y">Yibo Pi</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zijian Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jianxin Liao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In-band Network Telemetry (INT) has emerged as a promising network
measurement technology. However, existing network telemetry systems lack the
flexibility to meet diverse telemetry requirements and are also difficult to
adapt to dynamic network environments. In this paper, we propose AdapINT, a
versatile and adaptive in-band network telemetry framework assisted by
dual-timescale probes, including long-period auxiliary probes (APs) and
short-period dynamic probes (DPs). Technically, the APs collect basic network
status information, which is used for the path planning of DPs. To achieve full
network coverage, we propose an auxiliary probes path deployment (APPD)
algorithm based on the Depth-First-Search (DFS). The DPs collect specific
network information for telemetry tasks. To ensure that the DPs can meet
diverse telemetry requirements and adapt to dynamic network environments, we
apply the deep reinforcement learning (DRL) technique and transfer learning
method to design the dynamic probes path deployment (DPPD) algorithm. The
evaluation results show that AdapINT can redesign the telemetry system
according to telemetry requirements and network environments. AdapINT can
reduce telemetry latency by 75\% in online games and video conferencing
scenarios. For overhead-aware networks, AdapINT can reduce control overheads by
34\% in cloud computing services.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19334" title="Abstract">arXiv:2310.19334</a> [<a href="/pdf/2310.19334" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Two-Minute Feedback: Digital, Lecture-Accompanying Survey as a  Continuous Feedback Instrument
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Egetenmeier%2C+A">Armin Egetenmeier</a>, 
<a href="/search/cs?searchtype=author&query=Strickroth%2C+S">Sven Strickroth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to the International Journal of Information and Education Technology (IJIET)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Detailed feedback on courses and lecture content is essential for their
improvement and also serves as a tool for reflection. However, feedback methods
are often only used sporadically, especially in mass courses, because
collecting and analyzing feedback in a timely manner is often a challenge for
teachers. Moreover, the current situation of the students or the changing
workload during the semester are usually not taken into account either. For a
holistic investigation, the article used a digital survey format as formative
feedback which attempts to measure student stress in a quantitative part and to
address the participants' reflection in a qualitative part, as well as to
collect general suggestions for improvement (based on the so-called One-Minute
Paper) at two educational institutions. The feedback during the semester is
evaluated qualitatively and discussed on a meta-level and special features
(e.g. reflections on student work ethic or other courses) are addressed. The
results show a low, but constant rate of feedback. Responses mostly cover
topics of the lecture content or organizational aspects and were intensively
used to report issues within the lecture. In addition, artificial intelligence
(AI) support in the form of a large language model was tested and showed
promising results in summarizing the open-ended responses for the teacher.
Finally, the experiences from the lecturers are reflected upon and the results
as well as possibilities for improvement are discussed.
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19335" title="Abstract">arXiv:2310.19335</a> [<a href="/pdf/2310.19335" title="Download PDF">pdf</a>, <a href="/ps/2310.19335" title="Download PostScript">ps</a>, <a href="/format/2310.19335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> USSR is in P/poly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balaji%2C+N">Nikhil Balaji</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+S">Samir Datta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at SOSA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Computational Geometry (cs.CG); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The Sum of Square Roots (SSR) problem is the following computational problem:
Given positive integers $a_1, \dots, a_k$, and signs $\delta_1, \dots, \delta_k
\in \{-1, 1\}$, check if $\sum_{i=1}^k \delta_i \sqrt{a_i} &gt; 0$. The problem is
known to have a polynomial time algorithm on the real RAM model of computation,
however no sub-exponential time algorithm is known in the bit or Turing model
of computation. The precise computational complexity of SSR has been a
notorious open problem ~\cite{ggj} over the last four decades. The problem is
known to admit an upper bound in the third level of the \emph{Counting
Hierarchy}, i.e., $\CHtwo$ and no non-trivial lower bounds are known. Even when
the input numbers are \emph{small}, i.e., given in \emph{unary}, no better
complexity bound was known prior to our work. In this paper, we show that the
unary variant (USSR) of the sum of square roots problem is considerably easier
by giving a $P/poly$ upper bound.
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19336" title="Abstract">arXiv:2310.19336</a> [<a href="/pdf/2310.19336" title="Download PDF">pdf</a>, <a href="/format/2310.19336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Considerations for the Control Design of Augmentative Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guptasarma%2C+S">Shivani Guptasarma</a> (1), 
<a href="/search/cs?searchtype=author&query=Kennedy%2C+M">Monroe Kennedy III</a> (1) ((1) Stanford University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages. Presented at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2021) Workshop on Building and Evaluating Ethical Robotic Systems, Prague, Czech Republic, 28-30 September 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Robotic systems that are intended to augment human capabilities commonly
require the use of semi-autonomous control and artificial sensing, while at the
same time aiming to empower the user to make decisions and take actions. This
work identifies principles and techniques from the literature that can help to
resolve this apparent contradiction. It is postulated that augmentative robots
must function as tools that have partial agency, as collaborative agents that
provide conditional transparency, and ideally, serve as extensions of the human
body.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19339" title="Abstract">arXiv:2310.19339</a> [<a href="/pdf/2310.19339" title="Download PDF">pdf</a>, <a href="/format/2310.19339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Realisability and Cobordisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maestracci%2C+V">Valentin Maestracci</a> (AMU), 
<a href="/search/cs?searchtype=author&query=Seiller%2C+T">Thomas Seiller</a> (CNRS, LIPN, LIPN)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> FSCD, Sapienza Universit{\`a} Di Roma, Jul 2023, Rome, Italy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Cobordism categories are known to be compact closed. They can therefore be
used to define non-degenerate models of multiplicative linear logic by
combining the Int construction with double glueing. In this work we detail such
construction in the case of low-dimensional cobordisms, and exhibit a connexion
between those models and the model of Interaction graphs introduced by Seiller.
In particular, we exhibit how the so-called trefoil property is a consequence
of the associativity of composition of higher structures, providing a first
step toward establishing models as obtained from a double glueing construction.
We discuss possible extensions to higher-dimensional cobordisms categories
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19341" title="Abstract">arXiv:2310.19341</a> [<a href="/pdf/2310.19341" title="Download PDF">pdf</a>, <a href="/format/2310.19341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skywork: A More Open Bilingual Foundation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianwen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lichang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haihua Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Biye Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Cheng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%BC%2C+W">Weiwei L&#xfc;</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Rui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xilin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuejie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lunan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wenjun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Peng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaokun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yutuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chuanhai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanqi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yongyi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaojuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shuicheng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Han Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yahui Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this technical report, we present Skywork-13B, a family of large language
models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both
English and Chinese texts. This bilingual foundation model is the most
extensively trained and openly published LLMs of comparable size to date. We
introduce a two-stage training methodology using a segmented corpus, targeting
general purpose training and then domain-specific enhancement training,
respectively. We show that our model not only excels on popular benchmarks, but
also achieves \emph{state of the art} performance in Chinese language modeling
on diverse domains. Furthermore, we propose a novel leakage detection method,
demonstrating that test data contamination is a pressing issue warranting
further investigation by the LLM community. To spur future research, we release
Skywork-13B along with checkpoints obtained during intermediate stages of the
training process. We are also releasing part of our SkyPile corpus, a
collection of over 150 billion tokens of web text, which is the largest high
quality open Chinese pre-training corpus to date. We hope Skywork-13B and our
open corpus will serve as a valuable open-source resource to democratize access
to high-quality LLMs.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19342" title="Abstract">arXiv:2310.19342</a> [<a href="/pdf/2310.19342" title="Download PDF">pdf</a>, <a href="/format/2310.19342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-Only Model Inversion Attacks via Knowledge Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+N">Ngoc-Bao Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasegaran%2C+K">Keshigeyan Chandrasegaran</a>, 
<a href="/search/cs?searchtype=author&query=Abdollahzadeh%2C+M">Milad Abdollahzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+N">Ngai-Man Cheung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Neurips 2023. The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In a model inversion (MI) attack, an adversary abuses access to a machine
learning (ML) model to infer and reconstruct private training data. Remarkable
progress has been made in the white-box and black-box setups, where the
adversary has access to the complete model or the model's soft output
respectively. However, there is very limited study in the most challenging but
practically important setup: Label-only MI attacks, where the adversary only
has access to the model's predicted label (hard label) without confidence
scores nor any other model information.
<br />In this work, we propose LOKT, a novel approach for label-only MI attacks.
Our idea is based on transfer of knowledge from the opaque target model to
surrogate models. Subsequently, using these surrogate models, our approach can
harness advanced white-box attacks. We propose knowledge transfer based on
generative modelling, and introduce a new model, Target model-assisted ACGAN
(T-ACGAN), for effective knowledge transfer. Our method casts the challenging
label-only MI into the more tractable white-box setup. We provide analysis to
support that surrogate models based on our approach serve as effective proxies
for the target model for MI. Our experiments show that our method significantly
outperforms existing SOTA Label-only MI attack by more than 15% across all MI
benchmarks. Furthermore, our method compares favorably in terms of query
budget. Our study highlights rising privacy threats for ML models even when
minimal information (i.e., hard labels) is exposed. Our study highlights rising
privacy threats for ML models even when minimal information (i.e., hard labels)
is exposed. Our code, demo, models and reconstructed data are available at our
project page: https://ngoc-nguyen-0.github.io/lokt/
</p>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19345" title="Abstract">arXiv:2310.19345</a> [<a href="/pdf/2310.19345" title="Download PDF">pdf</a>, <a href="/format/2310.19345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and  INES
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Savoldi%2C+B">Beatrice Savoldi</a>, 
<a href="/search/cs?searchtype=author&query=Gaido%2C+M">Marco Gaido</a>, 
<a href="/search/cs?searchtype=author&query=Negri%2C+M">Matteo Negri</a>, 
<a href="/search/cs?searchtype=author&query=Bentivogli%2C+L">Luisa Bentivogli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WMT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As part of the WMT-2023 "Test suites" shared task, in this paper we summarize
the results of two test suites evaluations: MuST-SHE-WMT23 and INES. By
focusing on the en-de and de-en language pairs, we rely on these newly created
test suites to investigate systems' ability to translate feminine and masculine
gender and produce gender-inclusive translations. Furthermore we discuss
metrics associated with our test suites and validate them by means of human
evaluations. Our results indicate that systems achieve reasonable and
comparable performance in correctly translating both feminine and masculine
gender forms for naturalistic gender phenomena. Instead, the generation of
inclusive language forms in translation emerges as a challenging task for all
the evaluated MT models, indicating room for future improvements and research
on the topic.
</p>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19347" title="Abstract">arXiv:2310.19347</a> [<a href="/pdf/2310.19347" title="Download PDF">pdf</a>, <a href="/format/2310.19347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Factual Consistency of Text Summarization by Adversarially  Decoupling Comprehension and Embellishment Abilities of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Huawen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Ting-En Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zekun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuchuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianli Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite the recent progress in text summarization made by large language
models (LLMs), they often generate summaries that are factually inconsistent
with original articles, known as "hallucinations" in text generation. Unlike
previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes
but more sophisticated ones, such as imposing cause and effect, adding false
details, and overgeneralizing, etc. These hallucinations are challenging to
detect through traditional methods, which poses great challenges for improving
the factual consistency of text summarization. In this paper, we propose an
adversarially DEcoupling method to disentangle the Comprehension and
EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based
parameter-efficient technique to cover the shortage of sensitivity for true and
false in the training process of LLMs. In this way, LLMs are less confused
about embellishing and understanding, thus can execute the instructions more
accurately and have enhanced abilities to distinguish hallucinations.
Experimental results show that DECENT significantly improves the reliability of
text summarization based on LLMs.
</p>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19349" title="Abstract">arXiv:2310.19349</a> [<a href="/pdf/2310.19349" title="Download PDF">pdf</a>, <a href="/format/2310.19349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Japanese SimCSE Technical Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsukagoshi%2C+H">Hayato Tsukagoshi</a>, 
<a href="/search/cs?searchtype=author&query=Sasano%2C+R">Ryohei Sasano</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+K">Koichi Takeda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We report the development of Japanese SimCSE, Japanese sentence embedding
models fine-tuned with SimCSE. Since there is a lack of sentence embedding
models for Japanese that can be used as a baseline in sentence embedding
research, we conducted extensive experiments on Japanese sentence embeddings
involving 24 pre-trained Japanese or multilingual language models, five
supervised datasets, and four unsupervised datasets. In this report, we provide
the detailed training setup for Japanese SimCSE and their evaluation results.
</p>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19351" title="Abstract">arXiv:2310.19351</a> [<a href="/pdf/2310.19351" title="Download PDF">pdf</a>, <a href="/format/2310.19351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi- and Weakly-Supervised Domain Generalization for Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Furuta%2C+R">Ryosuke Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+Y">Yoichi Sato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Object detectors do not work well when domains largely differ between
training and testing data. To solve this problem, domain generalization
approaches, which require training data with ground-truth labels from multiple
domains, have been proposed. However, it is time-consuming and labor-intensive
to collect those data for object detection because not only class labels but
also bounding boxes must be annotated. To overcome the problem of domain gap in
object detection without requiring expensive annotations, we propose to
consider two new problem settings: semi-supervised domain generalizable object
detection (SS-DGOD) and weakly-supervised DGOD (WS-DGOD). In contrast to the
conventional domain generalization for object detection that requires labeled
data from multiple domains, SS-DGOD and WS-DGOD require labeled data only from
one domain and unlabeled or weakly-labeled data from multiple domains for
training. We show that object detectors can be effectively trained on the
proposed settings with the same student-teacher learning framework, where a
student network is trained with pseudo labels output from a teacher on the
unlabeled or weakly-labeled data. The experimental results demonstrate that the
object detectors trained on the proposed settings significantly outperform
baseline detectors trained on one labeled domain data and perform comparably to
or better than those trained on unsupervised domain adaptation (UDA) settings,
while ours do not use target domain data for training in contrast to UDA.
</p>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19352" title="Abstract">arXiv:2310.19352</a> [<a href="/pdf/2310.19352" title="Download PDF">pdf</a>, <a href="/ps/2310.19352" title="Download PostScript">ps</a>, <a href="/format/2310.19352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-implicit Eulerian method for the fluid structure interaction of  elastic membranes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ciallella%2C+M">Mirco Ciallella</a>, 
<a href="/search/math?searchtype=author&query=Milcent%2C+T">Thomas Milcent</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper we propose a novel and general approach to design semi-implicit
methods for the simulation of fluid-structure interaction problems in a fully
Eulerian framework. In order to properly present the new method, we focus on
the two-dimensional version of the general model developed to describe full
membrane elasticity. The approach consists in treating the elastic source term
by writing an evolution equation on the structure stress tensor, even if it is
nonlinear. Then, it is possible to show that its semi-implicit discretization
allows us to add to the linear system of the Navier-Stokes equations some
consistent dissipation terms that depend on the local deformation and stiffness
of the membrane. Due to the linearly implicit discretization, the approach does
not need iterative solvers and can be easily applied to any Eulerian framework
for fluid-structure interaction. Its stability properties are studied by
performing a Von Neumann analysis on a simplified one-dimensional model and
proving that, thanks to the additional dissipation, the discretized coupled
system is unconditionally stable. Several numerical experiments are shown for
two-dimensional problems by comparing the new method to the original explicit
scheme and studying the effect of structure stiffness and mesh refinement on
the membrane dynamics. The newly designed scheme is able to relax the time step
restrictions that affect the explicit method and reduce crucially the
computational costs, especially when very stiff membranes are under
consideration.
</p>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19359" title="Abstract">arXiv:2310.19359</a> [<a href="/pdf/2310.19359" title="Download PDF">pdf</a>, <a href="/format/2310.19359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introducing instance label correlation in multiple instance learning.  Application to cancer detection on histopathological images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morales-%C3%81lvarez%2C+P">Pablo Morales-&#xc1;lvarez</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+A">Arne Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>, 
<a href="/search/cs?searchtype=author&query=Molina%2C+R">Rafael Molina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 6 figures, 6 tables. Published at Pattern Recognition journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the last years, the weakly supervised paradigm of multiple instance
learning (MIL) has become very popular in many different areas. A paradigmatic
example is computational pathology, where the lack of patch-level labels for
whole-slide images prevents the application of supervised models. Probabilistic
MIL methods based on Gaussian Processes (GPs) have obtained promising results
due to their excellent uncertainty estimation capabilities. However, these are
general-purpose MIL methods that do not take into account one important fact:
in (histopathological) images, the labels of neighboring patches are expected
to be correlated. In this work, we extend a state-of-the-art GP-based MIL
method, which is called VGPMIL-PR, to exploit such correlation. To do so, we
develop a novel coupling term inspired by the statistical physics Ising model.
We use variational inference to estimate all the model parameters.
Interestingly, the VGPMIL-PR formulation is recovered when the weight that
regulates the strength of the Ising term vanishes. The performance of the
proposed method is assessed in two real-world problems of prostate cancer
detection. We show that our model achieves better results than other
state-of-the-art probabilistic MIL methods. We also provide different
visualizations and analysis to gain insights into the influence of the novel
Ising term. These insights are expected to facilitate the application of the
proposed model to other research areas.
</p>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19360" title="Abstract">arXiv:2310.19360</a> [<a href="/pdf/2310.19360" title="Download PDF">pdf</a>, <a href="/format/2310.19360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from  a Minimax Game Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiansheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yisen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Adversarial Training (AT) has become arguably the state-of-the-art algorithm
for extracting robust features. However, researchers recently notice that AT
suffers from severe robust overfitting problems, particularly after learning
rate (LR) decay. In this paper, we explain this phenomenon by viewing
adversarial training as a dynamic minimax game between the model trainer and
the attacker. Specifically, we analyze how LR decay breaks the balance between
the minimax game by empowering the trainer with a stronger memorization
ability, and show such imbalance induces robust overfitting as a result of
memorizing non-robust features. We validate this understanding with extensive
experiments, and provide a holistic view of robust overfitting from the
dynamics of both the two game players. This understanding further inspires us
to alleviate robust overfitting by rebalancing the two players by either
regularizing the trainer's capacity or improving the attack strength.
Experiments show that the proposed ReBalanced Adversarial Training (ReBAT) can
attain good robustness and does not suffer from robust overfitting even after
very long training. Code is available at https://github.com/PKU-ML/ReBAT.
</p>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19366" title="Abstract">arXiv:2310.19366</a> [<a href="/pdf/2310.19366" title="Download PDF">pdf</a>, <a href="/format/2310.19366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Certificates: 6G-ready Access Control for the Service-Based  Architecture with Decentralized Identifiers and Verifiable Credentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garzon%2C+S+R">Sandro Rodriguez Garzon</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+H+D">Hai Dinh Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+M+M">Maria Mora Martinez</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BCpper%2C+A">Axel K&#xfc;pper</a>, 
<a href="/search/cs?searchtype=author&query=Einsiedler%2C+H+J">Hans Joachim Einsiedler</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+D">Daniela Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In 6G, mobile networks are poised to transition from monolithic structures
owned and operated by single mobile network operators into multi-stakeholder
networks where various parties contribute with infrastructure, resources, and
services. This shift brings forth a critical challenge: Ensuring secure and
trustful cross-domain access control. This paper introduces a novel technical
concept and a prototype, outlining and implementing a 5G Service-based
Architecture that utilizes Decentralized Identifiers and Verifiable Credentials
to authenticate and authorize network functions among each other rather than
relying on traditional X.509 certificates or OAuth2.0 access tokens. This
decentralized approach to identity and permission management for network
functions in 6G reduces the risk of a single point of failure associated with
centralized public key infrastructures, unifies access control mechanisms, and
paves the way for lesser complex and more trustful cross-domain key management
for highly collaborative network functions of a future Service-based
Architecture in 6G.
</p>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19367" title="Abstract">arXiv:2310.19367</a> [<a href="/pdf/2310.19367" title="Download PDF">pdf</a>, <a href="/ps/2310.19367" title="Download PostScript">ps</a>, <a href="/format/2310.19367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Pseudo-Linearization-Based Model Predictive Controller Design  Direct Data-Driven Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sekine%2C+M">Mikiya Sekine</a>, 
<a href="/search/eess?searchtype=author&query=Tsuruhara%2C+S">Satoshi Tsuruhara</a>, 
<a href="/search/eess?searchtype=author&query=Ito%2C+K">Kazuhisa Ito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 16 figures, 4 tables, To be submitted to IEEE Transactions on Control Systems Technology (TCST)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">To reduce the typical time-consuming routines of plant modeling for
model-based controller designs, the fictitious reference iterative tuning
(FRIT) has been proposed and has proven to be effective in many applications.
However, it is generally difficult to select a reference model properly without
information on the plant, which significantly affects the control performance
and sometimes leads to considerable performance degradation. To address this
problem, we propose a pseudo-linearization (PL) method using FRIT and design a
new controller for nonlinear systems that combines data-driven and model-based
control. This design considers the input constraints using model predictive
control. The effectiveness of the proposed method was evaluated according to
several practical references using numerical simulations for nonlinear classes
and experiments involving artificial muscles with hysteresis characteristics.
</p>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19368" title="Abstract">arXiv:2310.19368</a> [<a href="/pdf/2310.19368" title="Download PDF">pdf</a>, <a href="/format/2310.19368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Color Equivariant Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lengyel%2C+A">Attila Lengyel</a>, 
<a href="/search/cs?searchtype=author&query=Strafforello%2C+O">Ombretta Strafforello</a>, 
<a href="/search/cs?searchtype=author&query=Bruintjes%2C+R">Robert-Jan Bruintjes</a>, 
<a href="/search/cs?searchtype=author&query=Gielisse%2C+A">Alexander Gielisse</a>, 
<a href="/search/cs?searchtype=author&query=van+Gemert%2C+J">Jan van Gemert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Code available on <a href="https://github.com/Attila94/ceconv">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Color is a crucial visual cue readily exploited by Convolutional Neural
Networks (CNNs) for object recognition. However, CNNs struggle if there is data
imbalance between color variations introduced by accidental recording
conditions. Color invariance addresses this issue but does so at the cost of
removing all color information, which sacrifices discriminative power. In this
paper, we propose Color Equivariant Convolutions (CEConvs), a novel deep
learning building block that enables shape feature sharing across the color
spectrum while retaining important color information. We extend the notion of
equivariance from geometric to photometric transformations by incorporating
parameter sharing over hue-shifts in a neural network. We demonstrate the
benefits of CEConvs in terms of downstream performance to various tasks and
improved robustness to color changes, including train-test distribution shifts.
Our approach can be seamlessly integrated into existing architectures, such as
ResNets, and offers a promising solution for addressing color-based domain
shifts in CNNs.
</p>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19372" title="Abstract">arXiv:2310.19372</a> [<a href="/pdf/2310.19372" title="Download PDF">pdf</a>, <a href="/format/2310.19372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RGB-X Object Detection via Scene-Specific Fusion Modules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deevi%2C+S+A">Sri Aditya Deevi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Connor Lee</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+L">Lu Gan</a>, 
<a href="/search/cs?searchtype=author&query=Nagesh%2C+S">Sushruth Nagesh</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+G">Gaurav Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+S">Soon-Jo Chung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Multimodal deep sensor fusion has the potential to enable autonomous vehicles
to visually understand their surrounding environments in all weather
conditions. However, existing deep sensor fusion methods usually employ
convoluted architectures with intermingled multimodal features, requiring large
coregistered multimodal datasets for training. In this work, we present an
efficient and modular RGB-X fusion network that can leverage and fuse
pretrained single-modal models via scene-specific fusion modules, thereby
enabling joint input-adaptive network architectures to be created using small,
coregistered multimodal datasets. Our experiments demonstrate the superiority
of our method compared to existing works on RGB-thermal and RGB-gated datasets,
performing fusion using only a small amount of additional parameters. Our code
is available at https://github.com/dsriaditya999/RGBXFusion.
</p>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19373" title="Abstract">arXiv:2310.19373</a> [<a href="/pdf/2310.19373" title="Download PDF">pdf</a>, <a href="/format/2310.19373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster QUBO Brute-Force Solving using Gray Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCcke%2C+S">Sascha M&#xfc;cke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, originally published as an "ML2R Coding Nugget"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">This article describes an improved brute-force solving strategy for Quadratic
Unconstrained Binary Optimization (QUBO) problems that is faster than naive
approaches and easily parallelizable. It exploits the Gray code ordering of
natural numbers to allow for a more efficient evaluation of the QUBO objective
function. The implementation in Python is discussed in detail, and an
additional C implementation is provided.
</p>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19378" title="Abstract">arXiv:2310.19378</a> [<a href="/pdf/2310.19378" title="Download PDF">pdf</a>, <a href="/format/2310.19378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-shot Hybrid Domain Adaptation of Image Generators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hengjia Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Linxuan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuqi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+X">Xiaohui Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiaobo Ren</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaofei He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Can a pre-trained generator be adapted to the hybrid of multiple target
domains and generate images with integrated attributes of them? In this work,
we introduce a new task -- Few-shot Hybrid Domain Adaptation (HDA). Given a
source generator and several target domains, HDA aims to acquire an adapted
generator that preserves the integrated attributes of all target domains,
without overriding the source domain's characteristics. Compared with Domain
Adaptation (DA), HDA offers greater flexibility and versatility to adapt
generators to more composite and expansive domains. Simultaneously, HDA also
presents more challenges than DA as we have access only to images from
individual target domains and lack authentic images from the hybrid domain. To
address this issue, we introduce a discriminator-free framework that directly
encodes different domains' images into well-separable subspaces. To achieve
HDA, we propose a novel directional subspace loss comprised of a distance loss
and a direction loss. Concretely, the distance loss blends the attributes of
all target domains by reducing the distances from generated images to all
target subspaces. The direction loss preserves the characteristics from the
source domain by guiding the adaptation along the perpendicular to subspaces.
Experiments show that our method can obtain numerous domain-specific attributes
in a single adapted generator, which surpasses the baseline methods in semantic
similarity, image fidelity, and cross-domain consistency.
</p>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19380" title="Abstract">arXiv:2310.19380</a> [<a href="/pdf/2310.19380" title="Download PDF">pdf</a>, <a href="/format/2310.19380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic  Token Mixer for Visual Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+M">Meng Lou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong-Yu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sibei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yizhou Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent studies have integrated convolution into transformers to introduce
inductive bias and improve generalization performance. However, the static
nature of conventional convolution prevents it from dynamically adapting to
input variations, resulting in a representation discrepancy between convolution
and self-attention as self-attention calculates attention matrices dynamically.
Furthermore, when stacking token mixers that consist of convolution and
self-attention to form a deep network, the static nature of convolution hinders
the fusion of features previously generated by self-attention into convolution
kernels. These two limitations result in a sub-optimal representation capacity
of the constructed networks. To find a solution, we propose a lightweight Dual
Dynamic Token Mixer (D-Mixer) that aggregates global information and local
details in an input-dependent way. D-Mixer works by applying an efficient
global attention module and an input-dependent depthwise convolution separately
on evenly split feature segments, endowing the network with strong inductive
bias and an enlarged effective receptive field. We use D-Mixer as the basic
building block to design TransXNet, a novel hybrid CNN-Transformer vision
backbone network that delivers compelling performance. In the ImageNet-1K image
classification task, TransXNet-T surpasses Swin-T by 0.3\% in top-1 accuracy
while requiring less than half of the computational cost. Furthermore,
TransXNet-S and TransXNet-B exhibit excellent model scalability, achieving
top-1 accuracy of 83.8\% and 84.6\% respectively, with reasonable computational
costs. Additionally, our proposed network architecture demonstrates strong
generalization capabilities in various dense prediction tasks, outperforming
other state-of-the-art networks while having lower computational costs.
</p>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19381" title="Abstract">arXiv:2310.19381</a> [<a href="/pdf/2310.19381" title="Download PDF">pdf</a>, <a href="/format/2310.19381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Protecting Publicly Available Data With Machine Learning Shortcuts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+N+M">Nicolas M. M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Burgert%2C+M">Maximilian Burgert</a>, 
<a href="/search/cs?searchtype=author&query=Debus%2C+P">Pascal Debus</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J">Jennifer Williams</a>, 
<a href="/search/cs?searchtype=author&query=Sperl%2C+P">Philip Sperl</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6ttinger%2C+K">Konstantin B&#xf6;ttinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at BMVC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Machine-learning (ML) shortcuts or spurious correlations are artifacts in
datasets that lead to very good training and test performance but severely
limit the model's generalization capability. Such shortcuts are insidious
because they go unnoticed due to good in-domain test performance. In this
paper, we explore the influence of different shortcuts and show that even
simple shortcuts are difficult to detect by explainable AI methods. We then
exploit this fact and design an approach to defend online databases against
crawlers: providers such as dating platforms, clothing manufacturers, or used
car dealers have to deal with a professionalized crawling industry that grabs
and resells data points on a large scale. We show that a deterrent can be
created by deliberately adding ML shortcuts. Such augmented datasets are then
unusable for ML use cases, which deters crawlers and the unauthorized use of
data from the internet. Using real-world data from three use cases, we show
that the proposed approach renders such collected data unusable, while the
shortcut is at the same time difficult to notice in human perception. Thus, our
proposed approach can serve as a proactive protection against illegitimate data
crawling.
</p>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19382" title="Abstract">arXiv:2310.19382</a> [<a href="/pdf/2310.19382" title="Download PDF">pdf</a>, <a href="/format/2310.19382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Volterra black-box models identification methods: direct collocation vs  least squares
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sidorov%2C+D">Denis Sidorov</a>, 
<a href="/search/math?searchtype=author&query=Tynda%2C+A">Aleksandr Tynda</a>, 
<a href="/search/math?searchtype=author&query=Muratov%2C+V">Vladislav Muratov</a>, 
<a href="/search/math?searchtype=author&query=Yanitsky%2C+E">Eugeny Yanitsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The Volterra integral-functional series is the classic approach for nonlinear
black box dynamical systems modeling. It is widely employed in many domains
including radiophysics, aerodynamics, electronic and electrical engineering and
many other. Identifying the time-varying functional parameters, also known as
Volterra kernels, poses a difficulty due to the curse of dimensionality. This
refers to the exponential growth in the number of model parameters as the
complexity of the input-output response increases. The least squares method
(LSM) is widely acknowledged as the standard approach for tackling the issue of
identifying parameters. Unfortunately, the LSM suffers with many drawbacks such
as the sensitivity to outliers causing biased estimation, multicollinearity,
overfitting and inefficiency with large datasets. This paper presents
alternative approach based on direct estimation of the Volterra kernels using
the collocation method. Two model examples are studied. It is found that the
collocation method presents a promising alternative for optimization,
surpassing the traditional least squares method when it comes to the Volterra
kernels identification including the case when input and output signals suffer
from considerable measurement errors.
</p>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19387" title="Abstract">arXiv:2310.19387</a> [<a href="/pdf/2310.19387" title="Download PDF">pdf</a>, <a href="/format/2310.19387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Othello is Solved
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takizawa%2C+H">Hiroki Takizawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The game of Othello is one of the world's most complex and popular games that
has yet to be computationally solved. Othello has roughly ten octodecillion (10
to the 58th power) possible game records and ten octillion (10 to the 28th
power) possible game position. The challenge of solving Othello, determining
the outcome of a game with no mistake made by either player, has long been a
grand challenge in computer science. This paper announces a significant
milestone: Othello is now solved, computationally proved that perfect play by
both players lead to a draw. Strong Othello software has long been built using
heuristically designed search techniques. Solving a game provides the solution
which enables software to play the game perfectly.
</p>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19388" title="Abstract">arXiv:2310.19388</a> [<a href="/pdf/2310.19388" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A numerical tool for efficient analysis and optimization of offshore  wind turbine jacket substructure considering realistic boundary and loading  conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mantey%2C+S+K">Selase Kwame Mantey</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">The jacket substructure is a critical component of the offshore wind turbine
(OWT) that is the interface between the transition piece at the top and the
grouted connection. This paper presents a comprehensive study on the
optimization of a jacket substructure to achieve greater cost efficiency while
maintain acceptable structural performance. A fast parametric finite element
modelling (FEM) approach for jacket substructures was firstly proposed. The
generated models took into account realistic loading conditions, including
self-weight, wind load and section-dependent wave load, and soil-pile
interaction. Parametric studies were conducted afterwards to investigate the
trends of the mass and response of the jacket substructure with respect to the
variation of geometric and sectional parameters. Optimizations of the jacket
substructure were carried out using parametric optimization and numerical
genetic algorithm (GA) optimization under three different optimization
strategies corresponding to three groups of objective and constraint functions.
The trends obtained by parametric analysis were used to guide the parameter
selection in parametric optimization, while a rank-based mutation GA was
established with the proposed efficient FEM embedded in as the solver to the
optimization objective and constraint functions. Parametric optimization gained
its advantage in computational efficiency, and the mass reduction were 6.2%,10%
and 14.8% for the three strategies respectively. GA optimization was more
aggressive as the mass reductions were 16.8%,22.3% and 34.3% for the three
strategies, but was relatively more computational intense. The two proposed
optimization methods and the three optimization strategies are both expected to
be applied in practical engineering design of OWT jacket substructure with good
optimization output and high computational efficiency.
</p>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19391" title="Abstract">arXiv:2310.19391</a> [<a href="/pdf/2310.19391" title="Download PDF">pdf</a>, <a href="/format/2310.19391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Fair Metric: Bridging Causality, Individual Fairness, and  Adversarial Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ehyaei%2C+A">Ahmad-Reza Ehyaei</a>, 
<a href="/search/cs?searchtype=author&query=Farnadi%2C+G">Golnoosh Farnadi</a>, 
<a href="/search/cs?searchtype=author&query=Samadi%2C+S">Samira Samadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Adversarial perturbation is used to expose vulnerabilities in machine
learning models, while the concept of individual fairness aims to ensure
equitable treatment regardless of sensitive attributes. Despite their initial
differences, both concepts rely on metrics to generate similar input data
instances. These metrics should be designed to align with the data's
characteristics, especially when it is derived from causal structure and should
reflect counterfactuals proximity. Previous attempts to define such metrics
often lack general assumptions about data or structural causal models. In this
research, we introduce a causal fair metric formulated based on causal
structures that encompass sensitive attributes. For robustness analysis, the
concept of protected causal perturbation is presented. Additionally, we delve
into metric learning, proposing a method for metric estimation and deployment
in real-world problems. The introduced metric has applications in the fields
adversarial training, fair learning, algorithmic recourse, and causal
reinforcement learning.
</p>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19394" title="Abstract">arXiv:2310.19394</a> [<a href="/pdf/2310.19394" title="Download PDF">pdf</a>, <a href="/format/2310.19394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightSAGE: Graph Neural Networks for Large Scale Item Retrieval in  Shopee&#x27;s Advertisement Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+M">Dang Minh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yifan Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph Neural Network (GNN) is the trending solution for item retrieval in
recommendation problems. Most recent reports, however, focus heavily on new
model architectures. This may bring some gaps when applying GNN in the
industrial setup, where, besides the model, constructing the graph and handling
data sparsity also play critical roles in the overall success of the project.
In this work, we report how GNN is applied for large-scale e-commerce item
retrieval at Shopee. We introduce our simple yet novel and impactful techniques
in graph construction, modeling, and handling data skewness. Specifically, we
construct high-quality item graphs by combining strong-signal user behaviors
with high-precision collaborative filtering (CF) algorithm. We then develop a
new GNN architecture named LightSAGE to produce high-quality items' embeddings
for vector search. Finally, we design multiple strategies to handle cold-start
and long-tail items, which are critical in an advertisement (ads) system. Our
models bring improvement in offline evaluations, online A/B tests, and are
deployed to the main traffic of Shopee's Recommendation Advertisement system.
</p>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19400" title="Abstract">arXiv:2310.19400</a> [<a href="/pdf/2310.19400" title="Download PDF">pdf</a>, <a href="/format/2310.19400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed multi-agent magnetic field norm SLAM with Gaussian processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Viset%2C+F">Frida Viset</a>, 
<a href="/search/cs?searchtype=author&query=Helmons%2C+R">Rudy Helmons</a>, 
<a href="/search/cs?searchtype=author&query=Kok%2C+M">Manon Kok</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 26th International Conference on Information Fusion, Charleston,
  SC, USA, June 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Accurately estimating the positions of multi-agent systems in indoor
environments is challenging due to the lack of Global Navigation Satelite
System (GNSS) signals. Noisy measurements of position and orientation can cause
the integrated position estimate to drift without bound. Previous research has
proposed using magnetic field simultaneous localization and mapping (SLAM) to
compensate for position drift in a single agent. Here, we propose two novel
algorithms that allow multiple agents to apply magnetic field SLAM using their
own and other agents measurements.
<br />Our first algorithm is a centralized approach that uses all measurements
collected by all agents in a single extended Kalman filter. This algorithm
simultaneously estimates the agents position and orientation and the magnetic
field norm in a central unit that can communicate with all agents at all times.
In cases where a central unit is not available, and there are communication
drop-outs between agents, our second algorithm is a distributed approach that
can be employed.
<br />We tested both algorithms by estimating the position of magnetometers carried
by three people in an optical motion capture lab with simulated odometry and
simulated communication dropouts between agents. We show that both algorithms
are able to compensate for drift in a case where single-agent SLAM is not. We
also discuss the conditions for the estimate from our distributed algorithm to
converge to the estimate from the centralized algorithm, both theoretically and
experimentally.
<br />Our experiments show that, for a communication drop-out rate of 80 percent,
our proposed distributed algorithm, on average, provides a more accurate
position estimate than single-agent SLAM. Finally, we demonstrate the
drift-compensating abilities of our centralized algorithm on a real-life
pedestrian localization problem with multiple agents moving inside a building.
</p>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19402" title="Abstract">arXiv:2310.19402</a> [<a href="/pdf/2310.19402" title="Download PDF">pdf</a>, <a href="/format/2310.19402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PlayTest: A Gamified Test Generator for Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldmeier%2C+P">Patric Feldmeier</a>, 
<a href="/search/cs?searchtype=author&query=Straubinger%2C+P">Philipp Straubinger</a>, 
<a href="/search/cs?searchtype=author&query=Fraser%2C+G">Gordon Fraser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages with 4 figures, to be published in Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Games are usually created incrementally, requiring repeated testing of the
same scenarios, which is a tedious and error-prone task for game developers.
Therefore, we aim to alleviate this game testing process by encapsulating it
into a game called Playtest, which transforms the tiring testing process into a
competitive game with a purpose. Playtest automates the generation of valuable
test cases based on player actions, without the players even realising it. We
envision the use of Playtest to crowdsource the task of testing games by giving
players access to the respective games through our tool in the playtesting
phases during the development process.
</p>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19403" title="Abstract">arXiv:2310.19403</a> [<a href="/pdf/2310.19403" title="Download PDF">pdf</a>, <a href="/format/2310.19403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lightweight Method to Generate Unanswerable Questions in English
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gautam%2C+V">Vagrant Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miaoran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Klakow%2C+D">Dietrich Klakow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">If a question cannot be answered with the available information, robust
systems for question answering (QA) should know _not_ to answer. One way to
build QA models that do this is with additional training data comprised of
unanswerable questions, created either by employing annotators or through
automated methods for unanswerable question generation. To show that the model
complexity of existing automated approaches is not justified, we examine a
simpler data augmentation method for unanswerable question generation in
English: performing antonym and entity swaps on answerable questions. Compared
to the prior state-of-the-art, data generated with our training-free and
lightweight strategy results in better models (+1.6 F1 points on SQuAD 2.0 data
with BERT-large), and has higher human-judged relatedness and readability. We
quantify the raw benefits of our approach compared to no augmentation across
multiple encoder models, using different amounts of generated data, and also on
TydiQA-MinSpan data (+9.3 F1 points with BERT-large). Our results establish
swaps as a simple but strong baseline for future work.
</p>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19405" title="Abstract">arXiv:2310.19405</a> [<a href="/pdf/2310.19405" title="Download PDF">pdf</a>, <a href="/format/2310.19405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Radar-Lidar Fusion for Object Detection by Designing Effective  Convolution Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Munir%2C+F">Farzeen Munir</a>, 
<a href="/search/cs?searchtype=author&query=Azam%2C+S">Shoaib Azam</a>, 
<a href="/search/cs?searchtype=author&query=Kucner%2C+T">Tomasz Kucner</a>, 
<a href="/search/cs?searchtype=author&query=Kyrki%2C+V">Ville Kyrki</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+M">Moongu Jeon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ITSC conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Object detection is a core component of perception systems, providing the ego
vehicle with information about its surroundings to ensure safe route planning.
While cameras and Lidar have significantly advanced perception systems, their
performance can be limited in adverse weather conditions. In contrast,
millimeter-wave technology enables radars to function effectively in such
conditions. However, relying solely on radar for building a perception system
doesn't fully capture the environment due to the data's sparse nature. To
address this, sensor fusion strategies have been introduced. We propose a
dual-branch framework to integrate radar and Lidar data for enhanced object
detection. The primary branch focuses on extracting radar features, while the
auxiliary branch extracts Lidar features. These are then combined using
additive attention. Subsequently, the integrated features are processed through
a novel Parallel Forked Structure (PFS) to manage scale variations. A region
proposal head is then utilized for object detection. We evaluated the
effectiveness of our proposed method on the Radiate dataset using COCO metrics.
The results show that it surpasses state-of-the-art methods by $1.89\%$ and
$2.61\%$ in favorable and adverse weather conditions, respectively. This
underscores the value of radar-Lidar fusion in achieving precise object
detection and localization, especially in challenging weather conditions.
</p>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19407" title="Abstract">arXiv:2310.19407</a> [<a href="/pdf/2310.19407" title="Download PDF">pdf</a>, <a href="/format/2310.19407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource Constrained Semantic Segmentation for Waste Sorting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cascina%2C+E">Elisa Cascina</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrino%2C+A">Andrea Pellegrino</a>, 
<a href="/search/cs?searchtype=author&query=Tozzi%2C+L">Lorenzo Tozzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This work addresses the need for efficient waste sorting strategies in
Materials Recovery Facilities to minimize the environmental impact of rising
waste. We propose resource-constrained semantic segmentation models for
segmenting recyclable waste in industrial settings. Our goal is to develop
models that fit within a 10MB memory constraint, suitable for edge applications
with limited processing capacity. We perform the experiments on three networks:
ICNet, BiSeNet (Xception39 backbone), and ENet. Given the aforementioned
limitation, we implement quantization and pruning techniques on the broader
nets, achieving positive results while marginally impacting the Mean IoU
metric. Furthermore, we propose a combination of Focal and Lov\'asz loss that
addresses the implicit class imbalance resulting in better performance compared
with the Cross-entropy loss function.
</p>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19408" title="Abstract">arXiv:2310.19408</a> [<a href="/pdf/2310.19408" title="Download PDF">pdf</a>, <a href="/format/2310.19408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable underwater assembly with reconfigurable visual fiducials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lensgraf%2C+S">Samuel Lensgraf</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Ankita Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Pediredla%2C+A">Adithya Pediredla</a>, 
<a href="/search/cs?searchtype=author&query=Balkcom%2C+D">Devin Balkcom</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A+Q">Alberto Quattrini Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present a scalable combined localization infrastructure deployment and
task planning algorithm for underwater assembly. Infrastructure is autonomously
modified to suit the needs of manipulation tasks based on an uncertainty model
on the infrastructure's positional accuracy. Our uncertainty model can be
combined with the noise characteristics from multiple devices. For the task
planning problem, we propose a layer-based clustering approach that completes
the manipulation tasks one cluster at a time. We employ movable visual fiducial
markers as infrastructure and an autonomous underwater vehicle (AUV) for
manipulation tasks. The proposed task planning algorithm is computationally
simple, and we implement it on AUV without any offline computation
requirements. Combined hardware experiments and simulations over large datasets
show that the proposed technique is scalable to large areas.
</p>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19409" title="Abstract">arXiv:2310.19409</a> [<a href="/pdf/2310.19409" title="Download PDF">pdf</a>, <a href="/ps/2310.19409" title="Download PostScript">ps</a>, <a href="/format/2310.19409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increased Multiplexing Gain with Reconfigurable Surfaces: Simultaneous  Channel Orthogonalization and Information Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alegria%2C+J+V">Juan Vidal Alegria</a>, 
<a href="/search/cs?searchtype=author&query=Vieira%2C+J">Joao Vieira</a>, 
<a href="/search/cs?searchtype=author&query=Rusek%2C+F">Fredrik Rusek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, accepted at IEEE GLOBECOM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Reconfigurable surface (RS) has been shown to be an effective solution for
improving wireless communication links in general multi-user multiple-input
multiple-output (MU-MIMO) setting. Current research efforts have been largely
directed towards the study of reconfigurable intelligent surface (RIS), which
corresponds to an RS made of passive reconfigurable elements with only phase
shifting capabilities. RIS constitutes a cost- and energy- efficient solution
for increased beamforming gain since it allows to generate constructive
interference towards desired directions, e.g., towards a base station (BS).
However, in many situations, multiplexing gain may have greater impact on the
achievable transmission rates and number of simultaneously connected devices,
while RIS has only been able to achieve minor improvements in this aspect.
Recent work has proposed the use of alternative RS technologies, namely
amplitude-reconfigurable intelligent surface (ARIS) and fully-reconfigurable
intelligent surface (FRIS), to achieve perfect orthogonalization of MU-MIMO
channels, thus allowing for maximum multiplexing gain at reduced complexity. In
this work we consider the use of ARIS and FRIS for simultaneously
orthogonalizing a MU-MIMO channel, while embedding extra information in the
orthogonalized channel. We show that the resulting achievable rates allow for
full exploitation of the degrees of freedom in a MU-MIMO system with excess of
BS antennas.
</p>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19410" title="Abstract">arXiv:2310.19410</a> [<a href="/pdf/2310.19410" title="Download PDF">pdf</a>, <a href="/format/2310.19410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generated Distributions Are All You Need for Membership Inference  Attacks Against Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Ning Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+R">Rui Wen</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative models have demonstrated revolutionary success in various visual
creation tasks, but in the meantime, they have been exposed to the threat of
leaking private information of their training data. Several membership
inference attacks (MIAs) have been proposed to exhibit the privacy
vulnerability of generative models by classifying a query image as a training
dataset member or nonmember. However, these attacks suffer from major
limitations, such as requiring shadow models and white-box access, and either
ignoring or only focusing on the unique property of diffusion models, which
block their generalization to multiple generative models. In contrast, we
propose the first generalized membership inference attack against a variety of
generative models such as generative adversarial networks, [variational]
autoencoders, implicit functions, and the emerging diffusion models. We
leverage only generated distributions from target generators and auxiliary
non-member datasets, therefore regarding target generators as black boxes and
agnostic to their architectures or application scenarios. Experiments validate
that all the generative models are vulnerable to our attack. For instance, our
work achieves attack AUC $&gt;0.99$ against DDPM, DDIM, and FastDPM trained on
CIFAR-10 and CelebA. And the attack against VQGAN, LDM (for the
text-conditional generation), and LIIF achieves AUC $&gt;0.90.$ As a result, we
appeal to our community to be aware of such privacy leakage risks when
designing and publishing generative models.
</p>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19413" title="Abstract">arXiv:2310.19413</a> [<a href="/pdf/2310.19413" title="Download PDF">pdf</a>, <a href="/format/2310.19413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CARPE-ID: Continuously Adaptable Re-identification for Personalized  Robot Assistance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rollo%2C+F">Federico Rollo</a>, 
<a href="/search/cs?searchtype=author&query=Zunino%2C+A">Andrea Zunino</a>, 
<a href="/search/cs?searchtype=author&query=Tsagarakis%2C+N">Nikolaos Tsagarakis</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+E+M">Enrico Mingo Hoffman</a>, 
<a href="/search/cs?searchtype=author&query=Ajoudani%2C+A">Arash Ajoudani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In today's Human-Robot Interaction (HRI) scenarios, a prevailing tendency
exists to assume that the robot shall cooperate with the closest individual or
that the scene involves merely a singular human actor. However, in realistic
scenarios, such as shop floor operations, such an assumption may not hold and
personalized target recognition by the robot in crowded environments is
required. To fulfil this requirement, in this work, we propose a person
re-identification module based on continual visual adaptation techniques that
ensure the robot's seamless cooperation with the appropriate individual even
subject to varying visual appearances or partial or complete occlusions. We
test the framework singularly using recorded videos in a laboratory environment
and an HRI scenario, i.e., a person-following task by a mobile robot. The
targets are asked to change their appearance during tracking and to disappear
from the camera field of view to test the challenging cases of occlusion and
outfit variations. We compare our framework with one of the state-of-the-art
Multi-Object Tracking (MOT) methods and the results show that the CARPE-ID can
accurately track each selected target throughout the experiments in all the
cases (except two limit cases). At the same time, the s-o-t-a MOT has a mean of
4 tracking errors for each video.
</p>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19415" title="Abstract">arXiv:2310.19415</a> [<a href="/pdf/2310.19415" title="Download PDF">pdf</a>, <a href="/format/2310.19415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-to-3D with classifier score distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuan-Chen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Ding Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Song-Hai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojuan Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our project page is <a href="https://xinyu-andy.github.io/Classifier-Score-Distillation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
<p class="mathjax">Text-to-3D generation has made remarkable progress recently, particularly
with methods based on Score Distillation Sampling (SDS) that leverages
pre-trained 2D diffusion models. While the usage of classifier-free guidance is
well acknowledged to be crucial for successful optimization, it is considered
an auxiliary trick rather than the most essential component. In this paper, we
re-evaluate the role of classifier-free guidance in score distillation and
discover a surprising finding: the guidance alone is enough for effective
text-to-3D generation tasks. We name this method Classifier Score Distillation
(CSD), which can be interpreted as using an implicit classification model for
generation. This new perspective reveals new insights for understanding
existing techniques. We validate the effectiveness of CSD across a variety of
text-to-3D tasks including shape generation, texture synthesis, and shape
editing, achieving results superior to those of state-of-the-art methods. Our
project page is https://xinyu-andy.github.io/Classifier-Score-Distillation
</p>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19418" title="Abstract">arXiv:2310.19418</a> [<a href="/pdf/2310.19418" title="Download PDF">pdf</a>, <a href="/format/2310.19418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GaitFormer: Learning Gait Representations with Noisy Multi-Task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cosma%2C+A">Adrian Cosma</a>, 
<a href="/search/cs?searchtype=author&query=Radoi%2C+E">Emilian Radoi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Sensors 2022, Image and Video Processing and Recognition Based on Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Gait analysis is proven to be a reliable way to perform person identification
without relying on subject cooperation. Walking is a biometric that does not
significantly change in short periods of time and can be regarded as unique to
each person. So far, the study of gait analysis focused mostly on
identification and demographics estimation, without considering many of the
pedestrian attributes that appearance-based methods rely on. In this work,
alongside gait-based person identification, we explore pedestrian attribute
identification solely from movement patterns. We propose DenseGait, the largest
dataset for pretraining gait analysis systems containing 217K anonymized
tracklets, annotated automatically with 42 appearance attributes. DenseGait is
constructed by automatically processing video streams and offers the full array
of gait covariates present in the real world. We make the dataset available to
the research community. Additionally, we propose GaitFormer, a
transformer-based model that after pretraining in a multi-task fashion on
DenseGait, achieves 92.5% accuracy on CASIA-B and 85.33% on FVG, without
utilizing any manually annotated data. This corresponds to a +14.2% and +9.67%
accuracy increase compared to similar methods. Moreover, GaitFormer is able to
accurately identify gender information and a multitude of appearance attributes
utilizing only movement patterns. The code to reproduce the experiments is made
publicly.
</p>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19420" title="Abstract">arXiv:2310.19420</a> [<a href="/pdf/2310.19420" title="Download PDF">pdf</a>, <a href="/format/2310.19420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean BERTs make erratic language teachers: the effectiveness of latent  bootstrapping in low-resource settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samuel%2C+D">David Samuel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submission to the BabyLM shared at CoNLL (EMNLP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper explores the use of latent bootstrapping, an alternative
self-supervision technique, for pretraining language models. Unlike the typical
practice of using self-supervision on discrete subwords, latent bootstrapping
leverages contextualized embeddings for a richer supervision signal. We conduct
experiments to assess how effective this approach is for acquiring linguistic
knowledge from limited resources. Specifically, our experiments are based on
the BabyLM shared task, which includes pretraining on two small curated corpora
and an evaluation on four linguistic benchmarks.
</p>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19424" title="Abstract">arXiv:2310.19424</a> [<a href="/pdf/2310.19424" title="Download PDF">pdf</a>, <a href="/format/2310.19424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Curriculum Reinforcement Learning for Unsupervised Discovery  of Skills
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seongun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyowoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaesik Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2023. First two authors contributed equally. Code at <a href="https://github.com/seongun-kim/vcrl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Mutual information-based reinforcement learning (RL) has been proposed as a
promising framework for retrieving complex skills autonomously without a
task-oriented reward function through mutual information (MI) maximization or
variational empowerment. However, learning complex skills is still challenging,
due to the fact that the order of training skills can largely affect sample
efficiency. Inspired by this, we recast variational empowerment as curriculum
learning in goal-conditioned RL with an intrinsic reward function, which we
name Variational Curriculum RL (VCRL). From this perspective, we propose a
novel approach to unsupervised skill discovery based on information theory,
called Value Uncertainty Variational Curriculum (VUVC). We prove that, under
regularity conditions, VUVC accelerates the increase of entropy in the visited
states compared to the uniform curriculum. We validate the effectiveness of our
approach on complex navigation and robotic manipulation tasks in terms of
sample efficiency and state coverage speed. We also demonstrate that the skills
discovered by our method successfully complete a real-world robot navigation
task in a zero-shot setup and that incorporating these skills with a global
planner further increases the performance.
</p>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19425" title="Abstract">arXiv:2310.19425</a> [<a href="/pdf/2310.19425" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial intelligence and the limits of the humanities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duch%2C+W">W&#x142;odzis&#x142;aw Duch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The complexity of cultures in the modern world is now beyond human
comprehension. Cognitive sciences cast doubts on the traditional explanations
based on mental models. The core subjects in humanities may lose their
importance. Humanities have to adapt to the digital age. New, interdisciplinary
branches of humanities emerge. Instant access to information will be replaced
by instant access to knowledge. Understanding the cognitive limitations of
humans and the opportunities opened by the development of artificial
intelligence and interdisciplinary research necessary to address global
challenges is the key to the revitalization of humanities. Artificial
intelligence will radically change humanities, from art to political sciences
and philosophy, making these disciplines attractive to students and enabling
them to go beyond current limitations.
</p>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19427" title="Abstract">arXiv:2310.19427</a> [<a href="/pdf/2310.19427" title="Download PDF">pdf</a>, <a href="/format/2310.19427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic  Detection of Infeasible Plans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyowoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seongun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaesik Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. First two authors contributed equally. Code at <a href="http://github.com/leekwoon/rgg">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Diffusion-based planning has shown promising results in long-horizon,
sparse-reward tasks by training trajectory diffusion models and conditioning
the sampled trajectories using auxiliary guidance functions. However, due to
their nature as generative models, diffusion models are not guaranteed to
generate feasible plans, resulting in failed execution and precluding planners
from being useful in safety-critical applications. In this work, we propose a
novel approach to refine unreliable plans generated by diffusion models by
providing refining guidance to error-prone plans. To this end, we suggest a new
metric named restoration gap for evaluating the quality of individual plans
generated by the diffusion model. A restoration gap is estimated by a gap
predictor which produces restoration gap guidance to refine a diffusion
planner. We additionally present an attribution map regularizer to prevent
adversarial refining guidance that could be generated from the sub-optimal gap
predictor, which enables further refinement of infeasible plans. We demonstrate
the effectiveness of our approach on three different benchmarks in offline
control settings that require long-horizon planning. We also illustrate that
our approach presents explainability by presenting the attribution maps of the
gap predictor and highlighting error-prone transitions, allowing for a deeper
understanding of the generated plans.
</p>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19432" title="Abstract">arXiv:2310.19432</a> [<a href="/pdf/2310.19432" title="Download PDF">pdf</a>, <a href="/format/2310.19432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining the Decisions of Deep Policy Networks for Robotic  Manipulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seongun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaesik Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep policy networks enable robots to learn behaviors to solve various
real-world complex tasks in an end-to-end fashion. However, they lack
transparency to provide the reasons of actions. Thus, such a black-box model
often results in low reliability and disruptive actions during the deployment
of the robot in practice. To enhance its transparency, it is important to
explain robot behaviors by considering the extent to which each input feature
contributes to determining a given action. In this paper, we present an
explicit analysis of deep policy models through input attribution methods to
explain how and to what extent each input feature affects the decisions of the
robot policy models. To this end, we present two methods for applying input
attribution methods to robot policy networks: (1) we measure the importance
factor of each joint torque to reflect the influence of the motor torque on the
end-effector movement, and (2) we modify a relevance propagation method to
handle negative inputs and outputs in deep policy networks properly. To the
best of our knowledge, this is the first report to identify the dynamic changes
of input attributions of multi-modal sensor inputs in deep policy networks
online for robotic manipulation.
</p>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19439" title="Abstract">arXiv:2310.19439</a> [<a href="/pdf/2310.19439" title="Download PDF">pdf</a>, <a href="/format/2310.19439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymmetric Diffusion Based Channel-Adaptive Secure Wireless Semantic  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xintian Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hansong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Q">Qianqian Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Semantic communication has emerged as a new deep learning-based communication
paradigm that drives the research of end-to-end data transmission in tasks like
image classification, and image reconstruction. However, the security problem
caused by semantic attacks has not been well explored, resulting in
vulnerabilities within semantic communication systems exposed to potential
semantic perturbations. In this paper, we propose a secure semantic
communication system, DiffuSeC, which leverages the diffusion model and deep
reinforcement learning (DRL) to address this issue. With the diffusing module
in the sender end and the asymmetric denoising module in the receiver end, the
DiffuSeC mitigates the perturbations added by semantic attacks, including data
source attacks and channel attacks. To further improve the robustness under
unstable channel conditions caused by semantic attacks, we developed a
DRL-based channel-adaptive diffusion step selection scheme to achieve stable
performance under fluctuating environments. A timestep synchronization scheme
is designed for diffusion timestep coordination between the two ends.
Simulation results demonstrate that the proposed DiffuSeC shows higher robust
accuracy than previous works under a wide range of channel conditions, and can
quickly adjust the model state according to signal-to-noise ratios (SNRs) in
unstable environments.
</p>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19441" title="Abstract">arXiv:2310.19441</a> [<a href="/pdf/2310.19441" title="Download PDF">pdf</a>, <a href="/format/2310.19441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Gaussian Splatting from Markerless Motion Capture can  Reconstruct Infants Movements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cotton%2C+R+J">R. James Cotton</a>, 
<a href="/search/cs?searchtype=author&query=Peyton%2C+C">Colleen Peyton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Easy access to precise 3D tracking of movement could benefit many aspects of
rehabilitation. A challenge to achieving this goal is that while there are many
datasets and pretrained algorithms for able-bodied adults, algorithms trained
on these datasets often fail to generalize to clinical populations including
people with disabilities, infants, and neonates. Reliable movement analysis of
infants and neonates is important as spontaneous movement behavior is an
important indicator of neurological function and neurodevelopmental disability,
which can help guide early interventions. We explored the application of
dynamic Gaussian splatting to sparse markerless motion capture (MMC) data. Our
approach leverages semantic segmentation masks to focus on the infant,
significantly improving the initialization of the scene. Our results
demonstrate the potential of this method in rendering novel views of scenes and
tracking infant movements. This work paves the way for advanced movement
analysis tools that can be applied to diverse clinical populations, with a
particular emphasis on early detection in infants.
</p>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19443" title="Abstract">arXiv:2310.19443</a> [<a href="/pdf/2310.19443" title="Download PDF">pdf</a>, <a href="/format/2310.19443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotically accurate and locking-free finite element implementation  of first order shear deformation theory for plates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Le%2C+K+C">Khanh Chau Le</a>, 
<a href="/search/math?searchtype=author&query=Bui%2C+H+G">Hoang Giang Bui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Classical Physics (physics.class-ph)

</div>
<p class="mathjax">A formulation of the asymptotically exact first-order shear deformation
theory for linear-elastic homogeneous plates in the rescaled coordinates and
angles of rotation is considered. This allows the development of its
asymptotically accurate and shear-locking-free finite element implementation.
As applications, numerical simulations are performed for circular and
rectangular plates, showing complete agreement between the analytical solution
and the numerical solutions based on two-dimensional theory and
three-dimensional elasticity theory.
</p>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19444" title="Abstract">arXiv:2310.19444</a> [<a href="/pdf/2310.19444" title="Download PDF">pdf</a>, <a href="/format/2310.19444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-for-All: Bridge the Gap Between Heterogeneous Architectures in  Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhiwei Hao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jianyuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yehui Tang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chang Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Knowledge distillation~(KD) has proven to be a highly effective approach for
enhancing model performance through a teacher-student training scheme. However,
most existing distillation methods are designed under the assumption that the
teacher and student models belong to the same model family, particularly the
hint-based approaches. By using centered kernel alignment (CKA) to compare the
learned features between heterogeneous teacher and student models, we observe
significant feature divergence. This divergence illustrates the ineffectiveness
of previous hint-based methods in cross-architecture distillation. To tackle
the challenge in distilling heterogeneous models, we propose a simple yet
effective one-for-all KD framework called OFA-KD, which significantly improves
the distillation performance between heterogeneous architectures. Specifically,
we project intermediate features into an aligned latent space such as the
logits space, where architecture-specific information is discarded.
Additionally, we introduce an adaptive target enhancement scheme to prevent the
student from being disturbed by irrelevant information. Extensive experiments
with various architectures, including CNN, Transformer, and MLP, demonstrate
the superiority of our OFA-KD framework in enabling distillation between
heterogeneous architectures. Specifically, when equipped with our OFA-KD, the
student models achieve notable performance improvements, with a maximum gain of
8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code
and checkpoints can be found at https://github.com/Hao840/OFAKD.
</p>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19447" title="Abstract">arXiv:2310.19447</a> [<a href="/pdf/2310.19447" title="Download PDF">pdf</a>, <a href="/format/2310.19447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Grouping in Large Scenes with Occlusion-aware Spatio-temporal  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinsong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+L">Lingfeng Gu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yu-Kun Lai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE T-CSVT, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Group detection, especially for large-scale scenes, has many potential
applications for public safety and smart cities. Existing methods fail to cope
with frequent occlusions in large-scale scenes with multiple people, and are
difficult to effectively utilize spatio-temporal information. In this paper, we
propose an end-to-end framework,GroupTransformer, for group detection in
large-scale scenes. To deal with the frequent occlusions caused by multiple
people, we design an occlusion encoder to detect and suppress severely occluded
person crops. To explore the potential spatio-temporal relationship, we propose
spatio-temporal transformers to simultaneously extract trajectory information
and fuse inter-person features in a hierarchical manner. Experimental results
on both large-scale and small-scale scenes demonstrate that our method achieves
better performance compared with state-of-the-art methods. On large-scale
scenes, our method significantly boosts the performance in terms of precision
and F1 score by more than 10%. On small-scale scenes, our method still improves
the performance of F1 score by more than 5%. The project page with code can be
found at <a href="http://cic.tju.edu.cn/faculty/likun/projects/GroupTrans.">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19449" title="Abstract">arXiv:2310.19449</a> [<a href="/pdf/2310.19449" title="Download PDF">pdf</a>, <a href="/format/2310.19449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Scale Application of Fault Injection into PyTorch Models -- an  Extension to PyTorchFI for Validation Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Graafe%2C+R">Ralf Graafe</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+Q+S">Qutub Syed Sha</a>, 
<a href="/search/cs?searchtype=author&query=Geissler%2C+F">Florian Geissler</a>, 
<a href="/search/cs?searchtype=author&query=Paulitsch%2C+M">Michael Paulitsch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted in DSN2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Transient or permanent faults in hardware can render the output of Neural
Networks (NN) incorrect without user-specific traces of the error, i.e. silent
data errors (SDE). On the other hand, modern NNs also possess an inherent
redundancy that can tolerate specific faults. To establish a safety case, it is
necessary to distinguish and quantify both types of corruptions. To study the
effects of hardware (HW) faults on software (SW) in general and NN models in
particular, several fault injection (FI) methods have been established in
recent years. Current FI methods focus on the methodology of injecting faults
but often fall short of accounting for large-scale FI tests, where many fault
locations based on a particular fault model need to be analyzed in a short
time. Results need to be concise, repeatable, and comparable. To address these
requirements and enable fault injection as the default component in a machine
learning development cycle, we introduce a novel fault injection framework
called PyTorchALFI (Application Level Fault Injection for PyTorch) based on
PyTorchFI. PyTorchALFI provides an efficient way to define randomly generated
and reusable sets of faults to inject into PyTorch models, defines complex test
scenarios, enhances data sets, and generates test KPIs while tightly coupling
fault-free, faulty, and modified NN. In this paper, we provide details about
the definition of test scenarios, software architecture, and several examples
of how to use the new framework to apply iterative changes in fault location
and number, compare different model modifications, and analyze test results.
</p>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19452" title="Abstract">arXiv:2310.19452</a> [<a href="/pdf/2310.19452" title="Download PDF">pdf</a>, <a href="/format/2310.19452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating Zero-Knowledge Succinct Non-interactive Argument of  Knowledge for Blockchain-based Identity Management with off-chain  computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kothari%2C+P">Pranay Kothari</a>, 
<a href="/search/cs?searchtype=author&query=Chopra%2C+D">Deepak Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Manjot Singh</a>, 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+S">Shivam Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Dwivedi%2C+R">Rudresh Dwivedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In today's world, secure and efficient biometric authentication is of keen
importance. Traditional authentication methods are no longer considered
reliable due to their susceptibility to cyber-attacks. Biometric
authentication, particularly fingerprint authentication, has emerged as a
promising alternative, but it raises concerns about the storage and use of
biometric data, as well as centralized storage, which could make it vulnerable
to cyber-attacks. In this paper, a novel blockchain-based fingerprint
authentication system is proposed that integrates zk-SNARKs, which are
zero-knowledge proofs that enable secure and efficient authentication without
revealing sensitive biometric information. A KNN-based approach on the FVC2002,
FVC2004 and FVC2006 datasets is used to generate a cancelable template for
secure, faster, and robust biometric registration and authentication which is
stored using the Interplanetary File System. The proposed approach provides an
average accuracy of 99.01%, 98.97% and 98.52% over the FVC2002, FVC2004 and
FVC2006 datasets respectively for fingerprint authentication. Incorporation of
zk-SNARK facilitates smaller proof size. Overall, the proposed method has the
potential to provide a secure and efficient solution for blockchain-based
identity management.
</p>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19453" title="Abstract">arXiv:2310.19453</a> [<a href="/pdf/2310.19453" title="Download PDF">pdf</a>, <a href="/format/2310.19453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALT: Towards Fine-grained Alignment between Language and CTR Models for  Click-Through Rate Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hangyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jianghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Click-through rate (CTR) prediction plays as a core function module in
various personalized online services. According to the data modality and input
format, the models for CTR prediction can be mainly classified into two
categories. The first one is the traditional CTR models that take as inputs the
one-hot encoded ID features of tabular modality, which aims to capture the
collaborative signals via feature interaction modeling. The second category
takes as inputs the sentences of textual modality obtained by hard prompt
templates, where pretrained language models (PLMs) are adopted to extract the
semantic knowledge. These two lines of research generally focus on different
characteristics of the same input data (i.e., textual and tabular modalities),
forming a distinct complementary relationship with each other. Therefore, in
this paper, we propose to conduct fine-grained feature-level Alignment between
Language and CTR models (ALT) for CTR prediction. Apart from the common
CLIP-like instance-level contrastive learning, we further design a novel joint
reconstruction pretraining task for both masked language and tabular modeling.
Specifically, the masked data of one modality (i.e., tokens or features) has to
be recovered with the help of the other modality, which establishes the
feature-level interaction and alignment via sufficient mutual information
extraction between dual modalities. Moreover, we propose three different
finetuning strategies with the option to train the aligned language and CTR
models separately or jointly for downstream CTR prediction tasks, thus
accommodating the varying efficacy and efficiency requirements for industrial
applications. Extensive experiments on three real-world datasets demonstrate
that ALT outperforms SOTA baselines, and is highly compatible for various
language and CTR models.
</p>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19454" title="Abstract">arXiv:2310.19454</a> [<a href="/pdf/2310.19454" title="Download PDF">pdf</a>, <a href="/format/2310.19454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMM and MMMSynth: Clustering of heterogeneous tabular data, and  synthetic data generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumari%2C+C">Chandrani Kumari</a>, 
<a href="/search/cs?searchtype=author&query=Siddharthan%2C+R">Rahul Siddharthan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We provide new algorithms for two tasks relating to heterogeneous tabular
datasets: clustering, and synthetic data generation. Tabular datasets typically
consist of heterogeneous data types (numerical, ordinal, categorical) in
columns, but may also have hidden cluster structure in their rows: for example,
they may be drawn from heterogeneous (geographical, socioeconomic,
methodological) sources, such that the outcome variable they describe (such as
the presence of a disease) may depend not only on the other variables but on
the cluster context. Moreover, sharing of biomedical data is often hindered by
patient confidentiality laws, and there is current interest in algorithms to
generate synthetic tabular data from real data, for example via deep learning.
<br />We demonstrate a novel EM-based clustering algorithm, MMM (``Madras Mixture
Model''), that outperforms standard algorithms in determining clusters in
synthetic heterogeneous data, and recovers structure in real data. Based on
this, we demonstrate a synthetic tabular data generation algorithm, MMMsynth,
that pre-clusters the input data, and generates cluster-wise synthetic data
assuming cluster-specific data distributions for the input columns. We
benchmark this algorithm by testing the performance of standard ML algorithms
when they are trained on synthetic data and tested on real published datasets.
Our synthetic data generation algorithm outperforms other literature
tabular-data generators, and approaches the performance of training purely with
real data.
</p>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19455" title="Abstract">arXiv:2310.19455</a> [<a href="/pdf/2310.19455" title="Download PDF">pdf</a>, <a href="/ps/2310.19455" title="Download PostScript">ps</a>, <a href="/format/2310.19455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arborescences, Colorful Forests, and Popularity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kavitha%2C+T">Telikepalli Kavitha</a>, 
<a href="/search/cs?searchtype=author&query=Makino%2C+K">Kazuhisa Makino</a>, 
<a href="/search/cs?searchtype=author&query=Schlotter%2C+I">Ildik&#xf3; Schlotter</a>, 
<a href="/search/cs?searchtype=author&query=Yokoi%2C+Y">Yu Yokoi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Our input is a directed, rooted graph $G = (V \cup \{r\},E)$ where each
vertex in $V$ has a partial order preference over its incoming edges. The
preferences of a vertex extend naturally to preferences over arborescences
rooted at $r$. We seek a popular arborescence in $G$, i.e., one for which there
is no "more popular" arborescence. Popular arborescences have applications in
liquid democracy or collective decision making; however, they need not exist in
every input instance. The popular arborescence problem is to decide if a given
input instance admits a popular arborescence or not. We show a polynomial-time
algorithm for this problem, whose computational complexity was not known
previously.
<br />Our algorithm is combinatorial, and can be regarded as a primal-dual
algorithm. It searches for an arborescence along with its dual certificate, a
chain of subsets of $E$, witnessing its popularity. In fact, our algorithm
solves the more general popular common base problem in the intersection of two
matroids, where one matroid is the partition matroid defined by any partition
$E = \bigcup_{v\in V} \delta(v)$ and the other is an arbitrary matroid on $E$
of rank $|V|$, with each $v \in V$ having a partial order over elements in
$\delta(v)$. We extend our algorithm to the case with forced or forbidden
edges.
<br />We also study the related popular colorful forest (or more generally, the
popular common independent set) problem where edges are partitioned into color
classes, and the task is to find a colorful forest that is popular within the
set of all colorful forests. For the case with weak rankings, we formulate the
popular colorful forest polytope, and thus show that a minimum-cost popular
colorful forest can be computed efficiently. By contrast, we prove that it is
NP-hard to compute a minimum-cost popular arborescence, even when rankings are
strict.
</p>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19459" title="Abstract">arXiv:2310.19459</a> [<a href="/pdf/2310.19459" title="Download PDF">pdf</a>, <a href="/format/2310.19459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security Challenges for Cloud or Fog Computing-Based AI Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pakmehr%2C+A">Amir Pakmehr</a>, 
<a href="/search/cs?searchtype=author&query=A%C3%9Fmuth%2C+A">Andreas A&#xdf;muth</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+C+P">Christoph P. Neumann</a>, 
<a href="/search/cs?searchtype=author&query=Pirkl%2C+G">Gerald Pirkl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Security challenges for Cloud or Fog-based machine learning services pose
several concerns. Securing the underlying Cloud or Fog services is essential,
as successful attacks against these services, on which machine learning
applications rely, can lead to significant impairments of these applications.
Because the requirements for AI applications can also be different, we
differentiate according to whether they are used in the Cloud or in a Fog
Computing network. This then also results in different threats or attack
possibilities. For Cloud platforms, the responsibility for security can be
divided between different parties. Security deficiencies at a lower level can
have a direct impact on the higher level where user data is stored. While
responsibilities are simpler for Fog Computing networks, by moving services to
the edge of the network, we have to secure them against physical access to the
devices. We conclude by outlining specific information security requirements
for AI applications.
</p>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19460" title="Abstract">arXiv:2310.19460</a> [<a href="/pdf/2310.19460" title="Download PDF">pdf</a>, <a href="/format/2310.19460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion Probabilistic Models for Hardware-Impaired  Communication Systems: Towards Wireless Generative AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Letafati%2C+M">Mehdi Letafati</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Samad Ali</a>, 
<a href="/search/cs?searchtype=author&query=Latva-aho%2C+M">Matti Latva-aho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2309.08568">arXiv:2309.08568</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Thanks to the outstanding achievements from state-of-the-art generative
models like ChatGPT and diffusion models, generative AI has gained substantial
attention across various industrial and academic domains. In this paper,
denoising diffusion probabilistic models (DDPMs) are proposed for a practical
finite-precision wireless communication system with hardware-impaired
transceivers. The intuition behind DDPM is to decompose the data generation
process over the so-called "denoising" steps. Inspired by this, a DDPM-based
receiver is proposed for a practical wireless communication scheme that faces
realistic non-idealities, including hardware impairments (HWI), channel
distortions, and quantization errors. It is shown that our approach provides
network resilience under low-SNR regimes, near-invariant reconstruction
performance with respect to different HWI levels and quantization errors, and
robust out-of-distribution performance against non-Gaussian noise. Moreover,
the reconstruction performance of our scheme is evaluated in terms of cosine
similarity and mean-squared error (MSE), highlighting more than 25 dB
improvement compared to the conventional deep neural network (DNN)-based
receivers.
</p>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19461" title="Abstract">arXiv:2310.19461</a> [<a href="/pdf/2310.19461" title="Download PDF">pdf</a>, <a href="/format/2310.19461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FoodFresh: Multi-Chain Design for an Inter-Institutional Food Supply  Chain Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stangl%2C+P">Philipp Stangl</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+C+P">Christoph P. Neumann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">We consider the problem of supply chain data visibility in a
blockchain-enabled supply chain network. Existing methods typically record
transactions happening in a supply chain on a single blockchain and are limited
in their ability to deal with different levels of data visibility. To address
this limitation, we present FoodFresh -- a multi-chain consortium where
organizations store immutable data on their blockchains. A decentralized hub
coordinates the cross-chain exchange of digital assets among the heterogeneous
blockchains. Mechanisms for enabling blockchain interoperability help to
preserve the benefits of independent sovereign blockchains while allowing for
data sharing across blockchain boundaries.
</p>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19462" title="Abstract">arXiv:2310.19462</a> [<a href="/pdf/2310.19462" title="Download PDF">pdf</a>, <a href="/format/2310.19462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constituency Parsing using LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xuefeng Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jialong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yulong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Constituency parsing is a fundamental yet unsolved natural language
processing task. In this paper, we explore the potential of recent large
language models (LLMs) that have exhibited remarkable performance across
various domains and tasks to tackle this task. We employ three linearization
strategies to transform output trees into symbol sequences, such that LLMs can
solve constituency parsing by generating linearized trees. We conduct
experiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT,
LLaMA, and Alpaca, comparing their performance against the state-of-the-art
constituency parsers. Our experiments encompass zero-shot, few-shot, and
full-training learning settings, and we evaluate the models on one in-domain
and five out-of-domain test datasets. Our findings reveal insights into LLMs'
performance, generalization abilities, and challenges in constituency parsing.
</p>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19463" title="Abstract">arXiv:2310.19463</a> [<a href="/pdf/2310.19463" title="Download PDF">pdf</a>, <a href="/ps/2310.19463" title="Download PostScript">ps</a>, <a href="/format/2310.19463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chrestien%2C+L">Leah Chrestien</a>, 
<a href="/search/cs?searchtype=author&query=Pevn%C3%BD%2C+T">Tom&#xe1;s Pevn&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=Edelkamp%2C+S">Stefan Edelkamp</a>, 
<a href="/search/cs?searchtype=author&query=Komenda%2C+A">Anton&#xed;n Komenda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In imitation learning for planning, parameters of heuristic functions are
optimized against a set of solved problem instances. This work revisits the
necessary and sufficient conditions of strictly optimally efficient heuristics
for forward search algorithms, mainly A* and greedy best-first search, which
expand only states on the returned optimal path. It then proposes a family of
loss functions based on ranking tailored for a given variant of the forward
search algorithm. Furthermore, from a learning theory point of view, it
discusses why optimizing cost-to-goal \hstar\ is unnecessarily difficult. The
experimental comparison on a diverse set of problems unequivocally supports the
derived theory.
</p>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19464" title="Abstract">arXiv:2310.19464</a> [<a href="/pdf/2310.19464" title="Download PDF">pdf</a>, <a href="/format/2310.19464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Neural Fields by Mixtures of Neural Implicit Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+T">Tackgeun You</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Mijeong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jungtaek Kim</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bohyung Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose a novel approach to learning the generative neural fields
represented by linear combinations of implicit basis networks. Our algorithm
learns basis networks in the form of implicit neural representations and their
coefficients in a latent space by either conducting meta-learning or adopting
auto-decoding paradigms. The proposed method easily enlarges the capacity of
generative neural fields by increasing the number of basis networks while
maintaining the size of a network for inference to be small through their
weighted model averaging. Consequently, sampling instances using the model is
efficient in terms of latency and memory footprint. Moreover, we customize
denoising diffusion probabilistic model for a target task to sample latent
mixture coefficients, which allows our final model to generate unseen data
effectively. Experiments show that our approach achieves competitive generation
performance on diverse benchmarks for images, voxel data, and NeRF scenes
without sophisticated designs for specific modalities and domains.
</p>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19468" title="Abstract">arXiv:2310.19468</a> [<a href="/pdf/2310.19468" title="Download PDF">pdf</a>, <a href="/format/2310.19468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret-Minimization Algorithms for Multi-Agent Cooperative Learning  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+J">Jialin Yi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Thesis submitted to London School of Economics and Political Science for PhD in Statistics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA); Machine Learning (stat.ML)

</div>
<p class="mathjax">A Multi-Agent Cooperative Learning (MACL) system is an artificial
intelligence (AI) system where multiple learning agents work together to
complete a common task. Recent empirical success of MACL systems in various
domains (e.g. traffic control, cloud computing, robotics) has sparked active
research into the design and analysis of MACL systems for sequential decision
making problems. One important metric of the learning algorithm for decision
making problems is its regret, i.e. the difference between the highest
achievable reward and the actual reward that the algorithm gains. The design
and development of a MACL system with low-regret learning algorithms can create
huge economic values. In this thesis, I analyze MACL systems for different
sequential decision making problems. Concretely, the Chapter 3 and 4
investigate the cooperative multi-agent multi-armed bandit problems, with
full-information or bandit feedback, in which multiple learning agents can
exchange their information through a communication network and the agents can
only observe the rewards of the actions they choose. Chapter 5 considers the
communication-regret trade-off for online convex optimization in the
distributed setting. Chapter 6 discusses how to form high-productive teams for
agents based on their unknown but fixed types using adaptive incremental
matchings. For the above problems, I present the regret lower bounds for
feasible learning algorithms and provide the efficient algorithms to achieve
this bound. The regret bounds I present in Chapter 3, 4 and 5 quantify how the
regret depends on the connectivity of the communication network and the
communication delay, thus giving useful guidance on design of the communication
protocol in MACL systems
</p>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19470" title="Abstract">arXiv:2310.19470</a> [<a href="/pdf/2310.19470" title="Download PDF">pdf</a>, <a href="/format/2310.19470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grokking Tickets: Lottery Tickets Accelerate Grokking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Minegishi%2C+G">Gouki Minegishi</a>, 
<a href="/search/cs?searchtype=author&query=Iwasawa%2C+Y">Yusuke Iwasawa</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Grokking is one of the most surprising puzzles in neural network
generalization: a network first reaches a memorization solution with perfect
training accuracy and poor generalization, but with further training, it
reaches a perfectly generalized solution. We aim to analyze the mechanism of
grokking from the lottery ticket hypothesis, identifying the process to find
the lottery tickets (good sparse subnetworks) as the key to describing the
transitional phase between memorization and generalization. We refer to these
subnetworks as ''Grokking tickets'', which is identified via magnitude pruning
after perfect generalization. First, using ''Grokking tickets'', we show that
the lottery tickets drastically accelerate grokking compared to the dense
networks on various configurations (MLP and Transformer, and an arithmetic and
image classification tasks). Additionally, to verify that ''Grokking ticket''
are a more critical factor than weight norms, we compared the ''good''
subnetworks with a dense network having the same L1 and L2 norms. Results show
that the subnetworks generalize faster than the controlled dense model. In
further investigations, we discovered that at an appropriate pruning rate,
grokking can be achieved even without weight decay. We also show that speedup
does not happen when using tickets identified at the memorization solution or
transition between memorization and generalization or when pruning networks at
the initialization (Random pruning, Grasp, SNIP, and Synflow). The results
indicate that the weight norm of network parameters is not enough to explain
the process of grokking, but the importance of finding good subnetworks to
describe the transition from memorization to generalization. The implementation
code can be accessed via this link:
\url{https://github.com/gouki510/Grokking-Tickets}.
</p>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19477" title="Abstract">arXiv:2310.19477</a> [<a href="/pdf/2310.19477" title="Download PDF">pdf</a>, <a href="/format/2310.19477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VDIP-TGV: Blind Image Deconvolution via Variational Deep Image Prior  Empowered by Total Generalized Variation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tingting Wu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhiyan Du</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+F">Feng-Lei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+T">Tieyong Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Recovering clear images from blurry ones with an unknown blur kernel is a
challenging problem. Deep image prior (DIP) proposes to use the deep network as
a regularizer for a single image rather than as a supervised model, which
achieves encouraging results in the nonblind deblurring problem. However, since
the relationship between images and the network architectures is unclear, it is
hard to find a suitable architecture to provide sufficient constraints on the
estimated blur kernels and clean images. Also, DIP uses the sparse maximum a
posteriori (MAP), which is insufficient to enforce the selection of the
recovery image. Recently, variational deep image prior (VDIP) was proposed to
impose constraints on both blur kernels and recovery images and take the
standard deviation of the image into account during the optimization process by
the variational principle. However, we empirically find that VDIP struggles
with processing image details and tends to generate suboptimal results when the
blur kernel is large. Therefore, we combine total generalized variational (TGV)
regularization with VDIP in this paper to overcome these shortcomings of VDIP.
TGV is a flexible regularization that utilizes the characteristics of partial
derivatives of varying orders to regularize images at different scales,
reducing oil painting artifacts while maintaining sharp edges. The proposed
VDIP-TGV effectively recovers image edges and details by supplementing extra
gradient information through TGV. Additionally, this model is solved by the
alternating direction method of multipliers (ADMM), which effectively combines
traditional algorithms and deep learning methods. Experiments show that our
proposed VDIP-TGV surpasses various state-of-the-art models quantitatively and
qualitatively.
</p>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19483" title="Abstract">arXiv:2310.19483</a> [<a href="/pdf/2310.19483" title="Download PDF">pdf</a>, <a href="/ps/2310.19483" title="Download PostScript">ps</a>, <a href="/format/2310.19483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Interpolation and Approximation Error Estimates Using a Novel  Taylor-like Formula
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chaskalovic%2C+J">Joel Chaskalovic</a>, 
<a href="/search/math?searchtype=author&query=Assous%2C+F">Franck Assous</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we present an approach to enhance interpolation and
approximation error estimates. Based on a previously derived first-order
Taylor-like formula, we demonstrate its applicability in improving the
$P_1$-interpolation error estimate. Following the same principles, we also
develop a novel numerical scheme for the heat equation that yields a better
error estimate compared to the classical implicit finite differences scheme.
</p>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19488" title="Abstract">arXiv:2310.19488</a> [<a href="/pdf/2310.19488" title="Download PDF">pdf</a>, <a href="/format/2310.19488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoLLM: Integrating Collaborative Embeddings into Large Language Models  for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jizhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+K">Keqin Bao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiangnan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Leveraging Large Language Models as Recommenders (LLMRec) has gained
significant attention and introduced fresh perspectives in user preference
modeling. Existing LLMRec approaches prioritize text semantics, usually
neglecting the valuable collaborative information from user-item interactions
in recommendations. While these text-emphasizing approaches excel in cold-start
scenarios, they may yield sub-optimal performance in warm-start situations. In
pursuit of superior recommendations for both cold and warm start scenarios, we
introduce CoLLM, an innovative LLMRec methodology that seamlessly incorporates
collaborative information into LLMs for recommendation. CoLLM captures
collaborative information through an external traditional model and maps it to
the input token embedding space of LLM, forming collaborative embeddings for
LLM usage. Through this external integration of collaborative information,
CoLLM ensures effective modeling of collaborative information without modifying
the LLM itself, providing the flexibility to employ various collaborative
information modeling techniques. Extensive experiments validate that CoLLM
adeptly integrates collaborative information into LLMs, resulting in enhanced
recommendation performance. We release the code and data at
https://github.com/zyang1580/CoLLM.
</p>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19489" title="Abstract">arXiv:2310.19489</a> [<a href="/pdf/2310.19489" title="Download PDF">pdf</a>, <a href="/format/2310.19489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Meta-Learning-Based KKL Observer Design for Nonlinear Dynamical  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Trommer%2C+L">Lukas Trommer</a>, 
<a href="/search/eess?searchtype=author&query=Oksuz%2C+H+Y">Halil Yigit Oksuz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 22nd European Control Conference (ECC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The theory of Kazantzis-Kravaris/Luenberger (KKL) observer design introduces
a methodology that uses a nonlinear transformation map and its left inverse to
estimate the state of a nonlinear system through the introduction of a linear
observer state space. Data-driven approaches using artificial neural networks
have demonstrated the ability to accurately approximate these transformation
maps. This paper presents a novel approach to observer design for nonlinear
dynamical systems through meta-learning, a concept in machine learning that
aims to optimize learning models for fast adaptation to a distribution of tasks
through an improved focus on the intrinsic properties of the underlying
learning problem. We introduce a framework that leverages information from
measurements of the system output to design a learning-based KKL observer
capable of online adaptation to a variety of system conditions and attributes.
To validate the effectiveness of our approach, we present comprehensive
experimental results for the estimation of nonlinear system states with varying
initial conditions and internal parameters, demonstrating high accuracy,
generalization capability, and robustness against noise.
</p>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19495" title="Abstract">arXiv:2310.19495</a> [<a href="/pdf/2310.19495" title="Download PDF">pdf</a>, <a href="/format/2310.19495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Visual Navigation of Underwater Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sunbeam%2C+M">M. Sunbeam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper aims to briefly survey deep learning methods for visual navigation
of underwater robotics. The scope of this paper includes the visual perception
of underwater robotics with deep learning methods, the available visual
underwater datasets, imitation learning, and reinforcement learning methods for
navigation. Additionally, relevant works will be categorized under the
imitation learning or deep learning paradigm for underwater robots for clarity
of the training methodologies in the current landscape. Literature that uses
deep learning algorithms to process non-visual data for underwater navigation
will not be considered, except as contrasting examples.
</p>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19503" title="Abstract">arXiv:2310.19503</a> [<a href="/pdf/2310.19503" title="Download PDF">pdf</a>, <a href="/format/2310.19503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust, Accountability, and Autonomy in Knowledge Graph-based AI for  Self-determination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ib%C3%A1%C3%B1ez%2C+L">Luis-Daniel Ib&#xe1;&#xf1;ez</a>, 
<a href="/search/cs?searchtype=author&query=Domingue%2C+J">John Domingue</a>, 
<a href="/search/cs?searchtype=author&query=Kirrane%2C+S">Sabrina Kirrane</a>, 
<a href="/search/cs?searchtype=author&query=Seneviratne%2C+O">Oshani Seneviratne</a>, 
<a href="/search/cs?searchtype=author&query=Third%2C+A">Aisling Third</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+M">Maria-Esther Vidal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Knowledge Graphs (KGs) have emerged as fundamental platforms for powering
intelligent decision-making and a wide range of Artificial Intelligence (AI)
services across major corporations such as Google, Walmart, and AirBnb. KGs
complement Machine Learning (ML) algorithms by providing data context and
semantics, thereby enabling further inference and question-answering
capabilities. The integration of KGs with neuronal learning (e.g., Large
Language Models (LLMs)) is currently a topic of active research, commonly named
neuro-symbolic AI. Despite the numerous benefits that can be accomplished with
KG-based AI, its growing ubiquity within online services may result in the loss
of self-determination for citizens as a fundamental societal issue. The more we
rely on these technologies, which are often centralised, the less citizens will
be able to determine their own destinies. To counter this threat, AI
regulation, such as the European Union (EU) AI Act, is being proposed in
certain regions. The regulation sets what technologists need to do, leading to
questions concerning: How can the output of AI systems be trusted? What is
needed to ensure that the data fuelling and the inner workings of these
artefacts are transparent? How can AI be made accountable for its
decision-making? This paper conceptualises the foundational topics and research
pillars to support KG-based AI for self-determination. Drawing upon this
conceptual framework, challenges and opportunities for citizen
self-determination are illustrated and analysed in a real-world scenario. As a
result, we propose a research agenda aimed at accomplishing the recommended
objectives.
</p>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19507" title="Abstract">arXiv:2310.19507</a> [<a href="/pdf/2310.19507" title="Download PDF">pdf</a>, <a href="/format/2310.19507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysing Multi-Agent Systems using 1-safe Petri Nets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adobbati%2C+F">Federica Adobbati</a>, 
<a href="/search/cs?searchtype=author&query=Mikulski%2C+%C5%81">&#x141;ukasz Mikulski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://ceur-ws.org/Vol-3170/paper8.pdf">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Petri Nets and Software Engineering 2022, Bergen, Norway, June
  20th, {CEUR} Workshop Proceedings 3170, 2022, 139-155
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">In the modelling and analysis of large, real systems, the main problem in
their efficient processing is the size of the global model. One of the popular
approaches that address this issue is the decomposition of such global model
into much smaller submodels and interaction between them. In this paper we
discuss the translation of multi-agent systems with the common-action-based
synchronization to 1-safe Petri nets. We prove that the composition in terms of
transition systems is equivalent to the transition-based fusion of nets
modelling different agents. We also address the issue of permanent disabling of
some parts of the system by constraints implied by the synchronization and
discuss the methods of solving it without the computation of the entire global
model.
</p>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19509" title="Abstract">arXiv:2310.19509</a> [<a href="/pdf/2310.19509" title="Download PDF">pdf</a>, <a href="/format/2310.19509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SparseByteNN: A Novel Mobile Inference Acceleration Framework Based on  Fine-Grained Group Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haitao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiashi Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+C">Chenqian Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Lean Fu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fangmin Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">To address the challenge of increasing network size, researchers have
developed sparse models through network pruning. However, maintaining model
accuracy while achieving significant speedups on general computing devices
remains an open problem. In this paper, we present a novel mobile inference
acceleration framework SparseByteNN, which leverages fine-grained kernel
sparsity to achieve real-time execution as well as high accuracy. Our framework
consists of two parts: (a) A fine-grained kernel sparsity schema with a
sparsity granularity between structured pruning and unstructured pruning. It
designs multiple sparse patterns for different operators. Combined with our
proposed whole network rearrangement strategy, the schema achieves a high
compression rate and high precision at the same time. (b) Inference engine
co-optimized with the sparse pattern. The conventional wisdom is that this
reduction in theoretical FLOPs does not translate into real-world efficiency
gains. We aim to correct this misconception by introducing a family of
efficient sparse kernels for ARM and WebAssembly. Equipped with our efficient
implementation of sparse primitives, we show that sparse versions of
MobileNet-v1 outperform strong dense baselines on the efficiency-accuracy
curve. Experimental results on Qualcomm 855 show that for 30% sparse
MobileNet-v1, SparseByteNN achieves 1.27x speedup over the dense version and
1.29x speedup over the state-of-the-art sparse inference engine MNN with a
slight accuracy drop of 0.224%. The source code of SparseByteNN will be
available at https://github.com/lswzjuer/SparseByteNN
</p>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19511" title="Abstract">arXiv:2310.19511</a> [<a href="/pdf/2310.19511" title="Download PDF">pdf</a>, <a href="/format/2310.19511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rule-Based Lloyd Algorithm for Multi-Robot Motion Planning and Control  with Safety and Convergence Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boldrer%2C+M">Manuel Boldrer</a>, 
<a href="/search/cs?searchtype=author&query=Serra-Gomez%2C+A">Alvaro Serra-Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Lyons%2C+L">Lorenzo Lyons</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>, 
<a href="/search/cs?searchtype=author&query=Ferranti%2C+L">Laura Ferranti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents a distributed rule-based Lloyd algorithm (RBL) for
multi-robot motion planning and control. The main limitations of the basic
Loyd-based algorithm (LB) concern deadlock issues and the failure to address
dynamic constraints effectively. Our contribution is twofold. First, we show
how RBL is able to provide safety and convergence to the goal region without
relying on communication between robots, nor neighbors control inputs, nor
synchronization between the robots. We considered both case of holonomic and
non-holonomic robots with control inputs saturation. Second, we show that the
Lloyd-based algorithm (without rules) can be successfully used as a safety
layer for learning-based approaches, leading to non-negligible benefits. We
further prove the soundness, reliability, and scalability of RBL through
extensive simulations, an updated comparison with the state of the art, and
experimental validations on small-scale car-like robots.
</p>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19512" title="Abstract">arXiv:2310.19512</a> [<a href="/pdf/2310.19512" title="Download PDF">pdf</a>, <a href="/format/2310.19512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VideoCrafter1: Open Diffusion Models for High-Quality Video Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haoxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Menghan Xia</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yingqing He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cun%2C+X">Xiaodong Cun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shaoshu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+J">Jinbo Xing</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaofang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+C">Chao Weng</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech Report; Github: <a href="https://github.com/AILab-CVC/VideoCrafter">this https URL</a> Homepage: <a href="https://ailab-cvc.github.io/videocrafter/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video generation has increasingly gained interest in both academia and
industry. Although commercial tools can generate plausible videos, there is a
limited number of open-source models available for researchers and engineers.
In this work, we introduce two diffusion models for high-quality video
generation, namely text-to-video (T2V) and image-to-video (I2V) models. T2V
models synthesize a video based on a given text input, while I2V models
incorporate an additional image input. Our proposed T2V model can generate
realistic and cinematic-quality videos with a resolution of $1024 \times 576$,
outperforming other open-source T2V models in terms of quality. The I2V model
is designed to produce videos that strictly adhere to the content of the
provided reference image, preserving its content, structure, and style. This
model is the first open-source I2V foundation model capable of transforming a
given image into a video clip while maintaining content preservation
constraints. We believe that these open-source video generation models will
contribute significantly to the technological advancements within the
community.
</p>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19514" title="Abstract">arXiv:2310.19514</a> [<a href="/pdf/2310.19514" title="Download PDF">pdf</a>, <a href="/format/2310.19514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Earth Mover&#x27;s Distance in Truly-Subquadratic Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beretta%2C+L">Lorenzo Beretta</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+A">Aviad Rubinstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">We design an additive approximation scheme for estimating the cost of the
min-weight bipartite matching problem: given a bipartite graph with
non-negative edge costs and $\varepsilon &gt; 0$, our algorithm estimates the cost
of matching all but $O(\varepsilon)$-fraction of the vertices in truly
subquadratic time $O(n^{2-\delta(\varepsilon)})$.
<br />Our algorithm has a natural interpretation for computing the Earth Mover's
Distance (EMD), up to a $\varepsilon$-additive approximation. Notably, we make
no assumptions about the underlying metric (more generally, the costs do not
have to satisfy triangle inequality). Note that compared to the size of the
instance (an arbitrary $n \times n$ cost matrix), our algorithm runs in {\em
sublinear} time.
<br />Our algorithm can approximate a slightly more general problem:
max-cardinality bipartite matching with a knapsack constraint, where the goal
is to maximize the number of vertices that can be matched up to a total cost
$B$.
</p>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19516" title="Abstract">arXiv:2310.19516</a> [<a href="/pdf/2310.19516" title="Download PDF">pdf</a>, <a href="/format/2310.19516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Context-Aware Natural Answers for Questions in 3D Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dwedari%2C+M+M">Mohammed Munzer Dwedari</a>, 
<a href="/search/cs?searchtype=author&query=Niessner%2C+M">Matthias Niessner</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D+Z">Dave Zhenyu Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">3D question answering is a young field in 3D vision-language that is yet to
be explored. Previous methods are limited to a pre-defined answer space and
cannot generate answers naturally. In this work, we pivot the question
answering task to a sequence generation task to generate free-form natural
answers for questions in 3D scenes (Gen3DQA). To this end, we optimize our
model directly on the language rewards to secure the global sentence semantics.
Here, we also adapt a pragmatic language understanding reward to further
improve the sentence quality. Our method sets a new SOTA on the ScanQA
benchmark (CIDEr score 72.22/66.57 on the test sets).
</p>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19519" title="Abstract">arXiv:2310.19519</a> [<a href="/pdf/2310.19519" title="Download PDF">pdf</a>, <a href="/format/2310.19519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Neural Causal Model for Interactive Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jialin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xinyan Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Peng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Methodology (stat.ME)

</div>
<p class="mathjax">Survivor bias in observational data leads the optimization of recommender
systems towards local optima. Currently most solutions re-mines existing
human-system collaboration patterns to maximize longer-term satisfaction by
reinforcement learning. However, from the causal perspective, mitigating
survivor effects requires answering a counterfactual problem, which is
generally unidentifiable and inestimable. In this work, we propose a neural
causal model to achieve counterfactual inference. Specifically, we first build
a learnable structural causal model based on its available graphical
representations which qualitatively characterizes the preference transitions.
Mitigation of the survivor bias is achieved though counterfactual consistency.
To identify the consistency, we use the Gumbel-max function as structural
constrains. To estimate the consistency, we apply reinforcement optimizations,
and use Gumbel-Softmax as a trade-off to get a differentiable function. Both
theoretical and empirical studies demonstrate the effectiveness of our
solution.
</p>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19521" title="Abstract">arXiv:2310.19521</a> [<a href="/pdf/2310.19521" title="Download PDF">pdf</a>, <a href="/format/2310.19521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum principle preserving time implicit DGSEM for linear scalar  hyperbolic conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Milani%2C+R">Riccardo Milani</a>, 
<a href="/search/math?searchtype=author&query=Renac%2C+F">Florent Renac</a>, 
<a href="/search/math?searchtype=author&query=Ruel%2C+J">Jean Ruel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We investigate the properties of the high-order discontinuous Galerkin
spectral element method (DGSEM) with implicit backward-Euler time stepping for
the approximation of hyperbolic linear scalar conservation equation in multiple
space dimensions. We first prove that the DGSEM scheme in one space dimension
preserves a maximum principle for the cell-averaged solution when the time step
is large enough. This property however no longer holds in multiple space
dimensions and we propose to use the flux-corrected transport limiting [Boris
and Book, J. Comput. Phys., 11 (1973)] based on a low-order approximation using
graph viscosity to impose a maximum principle on the cell-averaged solution.
These results allow to use a linear scaling limiter [Zhang and Shu, J. Comput.
Phys., 229 (2010)] in order to impose a maximum principle at nodal values
within elements. Then, we investigate the inversion of the linear systems
resulting from the time implicit discretization at each time step. We prove
that the diagonal blocks are invertible and provide efficient algorithms for
their inversion. Numerical experiments in one and two space dimensions are
presented to illustrate the conclusions of the present analyses.
</p>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19522" title="Abstract">arXiv:2310.19522</a> [<a href="/pdf/2310.19522" title="Download PDF">pdf</a>, <a href="/format/2310.19522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Natural Domain Foundation Models Useful for Medical Image  Classification?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huix%2C+J+P">Joana Pal&#xe9;s Huix</a>, 
<a href="/search/cs?searchtype=author&query=Ganeshan%2C+A+R">Adithya Raju Ganeshan</a>, 
<a href="/search/cs?searchtype=author&query=Haslum%2C+J+F">Johan Fredin Haslum</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%B6derberg%2C+M">Magnus S&#xf6;derberg</a>, 
<a href="/search/cs?searchtype=author&query=Matsoukas%2C+C">Christos Matsoukas</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+K">Kevin Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The deep learning field is converging towards the use of general foundation
models that can be easily adapted for diverse tasks. While this paradigm shift
has become common practice within the field of natural language processing,
progress has been slower in computer vision. In this paper we attempt to
address this issue by investigating the transferability of various
state-of-the-art foundation models to medical image classification tasks.
Specifically, we evaluate the performance of five foundation models, namely
SAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical
imaging datasets. We explore different training settings to fully harness the
potential of these models. Our study shows mixed results. DINOv2 in particular,
consistently outperforms the standard practice of ImageNet pretraining.
However, other foundation models failed to consistently beat this established
baseline indicating limitations in their transferability to medical image
classification tasks.
</p>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19527" title="Abstract">arXiv:2310.19527</a> [<a href="/pdf/2310.19527" title="Download PDF">pdf</a>, <a href="/format/2310.19527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoupled Actor-Critic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nauman%2C+M">Michal Nauman</a>, 
<a href="/search/cs?searchtype=author&query=Cygan%2C+M">Marek Cygan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Actor-Critic methods are in a stalemate of two seemingly irreconcilable
problems. Firstly, critic proneness towards overestimation requires sampling
temporal-difference targets from a conservative policy optimized using
lower-bound Q-values. Secondly, well-known results show that policies that are
optimistic in the face of uncertainty yield lower regret levels. To remedy this
dichotomy, we propose Decoupled Actor-Critic (DAC). DAC is an off-policy
algorithm that learns two distinct actors by gradient backpropagation: a
conservative actor used for temporal-difference learning and an optimistic
actor used for exploration. We test DAC on DeepMind Control tasks in low and
high replay ratio regimes and ablate multiple design choices. Despite minimal
computational overhead, DAC achieves state-of-the-art performance and sample
efficiency on locomotion tasks.
</p>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19531" title="Abstract">arXiv:2310.19531</a> [<a href="/pdf/2310.19531" title="Download PDF">pdf</a>, <a href="/format/2310.19531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfoEntropy Loss to Mitigate Bias of Learning Difficulties for  Generative Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhenpeng Su</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xue Bai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zijia Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+G">Guiguang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Songlin Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Generative language models are usually pretrained on large text corpus via
predicting the next token (i.e., sub-word/word/phrase) given the previous ones.
Recent works have demonstrated the impressive performance of large generative
language models on downstream tasks. However, existing generative language
models generally neglect an inherent challenge in text corpus during training,
i.e., the imbalance between frequent tokens and infrequent ones. It can lead a
language model to be dominated by common and easy-to-learn tokens, thereby
overlooking the infrequent and difficult-to-learn ones. To alleviate that, we
propose an Information Entropy Loss (InfoEntropy Loss) function. During
training, it can dynamically assess the learning difficulty of a to-be-learned
token, according to the information entropy of the corresponding predicted
probability distribution over the vocabulary. Then it scales the training loss
adaptively, trying to lead the model to focus more on the difficult-to-learn
tokens. On the Pile dataset, we train generative language models at different
scales of 436M, 1.1B, and 6.7B parameters. Experiments reveal that models
incorporating the proposed InfoEntropy Loss can gain consistent performance
improvement on downstream benchmarks.
</p>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19535" title="Abstract">arXiv:2310.19535</a> [<a href="/pdf/2310.19535" title="Download PDF">pdf</a>, <a href="/format/2310.19535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revitalizing Legacy Video Content: Deinterlacing with Bidirectional  Information Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhaowei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Schroers%2C+C">Christopher Schroers</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Due to old CRT display technology and limited transmission bandwidth, early
film and TV broadcasts commonly used interlaced scanning. This meant each field
contained only half of the information. Since modern displays require full
frames, this has spurred research into deinterlacing, i.e. restoring the
missing information in legacy video content. In this paper, we present a
deep-learning-based method for deinterlacing animated and live-action content.
Our proposed method supports bidirectional spatio-temporal information
propagation across multiple scales to leverage information in both space and
time. More specifically, we design a Flow-guided Refinement Block (FRB) which
performs feature refinement including alignment, fusion, and rectification.
Additionally, our method can process multiple fields simultaneously, reducing
per-frame processing time, and potentially enabling real-time processing. Our
experimental results demonstrate that our proposed method achieves superior
performance compared to existing methods.
</p>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19536" title="Abstract">arXiv:2310.19536</a> [<a href="/pdf/2310.19536" title="Download PDF">pdf</a>, <a href="/format/2310.19536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Batch Inverse Reinforcement Learning: Learn to Reward from  Imperfect Demonstration for Interactive Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jialin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xinyan Su</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zeyu He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Rewards serve as a measure of user satisfaction and act as a limiting factor
in interactive recommender systems. In this research, we focus on the problem
of learning to reward (LTR), which is fundamental to reinforcement learning.
Previous approaches either introduce additional procedures for learning to
reward, thereby increasing the complexity of optimization, or assume that
user-agent interactions provide perfect demonstrations, which is not feasible
in practice. Ideally, we aim to employ a unified approach that optimizes both
the reward and policy using compositional demonstrations. However, this
requirement presents a challenge since rewards inherently quantify user
feedback on-policy, while recommender agents approximate off-policy future
cumulative valuation. To tackle this challenge, we propose a novel batch
inverse reinforcement learning paradigm that achieves the desired properties.
Our method utilizes discounted stationary distribution correction to combine
LTR and recommender agent evaluation. To fulfill the compositional requirement,
we incorporate the concept of pessimism through conservation. Specifically, we
modify the vanilla correction using Bellman transformation and enforce KL
regularization to constrain consecutive policy updates. We use two real-world
datasets which represent two compositional coverage to conduct empirical
studies, the results also show that the proposed method relatively improves
both effectiveness (2.3\%) and efficiency (11.53\%)
</p>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19537" title="Abstract">arXiv:2310.19537</a> [<a href="/pdf/2310.19537" title="Download PDF">pdf</a>, <a href="/format/2310.19537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On consequences of finetuning on data with highly discriminative  features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masarczyk%2C+W">Wojciech Masarczyk</a>, 
<a href="/search/cs?searchtype=author&query=Trzci%C5%84ski%2C+T">Tomasz Trzci&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Ostaszewski%2C+M">Mateusz Ostaszewski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 -- UniReps Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the era of transfer learning, training neural networks from scratch is
becoming obsolete. Transfer learning leverages prior knowledge for new tasks,
conserving computational resources. While its advantages are well-documented,
we uncover a notable drawback: networks tend to prioritize basic data patterns,
forsaking valuable pre-learned features. We term this behavior "feature
erosion" and analyze its impact on network performance and internal
representations.
</p>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19539" title="Abstract">arXiv:2310.19539</a> [<a href="/pdf/2310.19539" title="Download PDF">pdf</a>, <a href="/format/2310.19539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Representation to Improve Team Problem Solving in Real-Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doboli%2C+A">Alex Doboli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures. arXiv admin note: text overlap with <a href="/abs/2308.06273">arXiv:2308.06273</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper proposes a novel representation to support computing metrics that
help understanding and improving in real-time a team's behavior during problem
solving in real-life. Even though teams are important in modern activities,
there is little computing aid to improve their activity. The representation
captures the different mental images developed, enhanced, and utilized during
solving. A case study illustrates the representation.
</p>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19540" title="Abstract">arXiv:2310.19540</a> [<a href="/pdf/2310.19540" title="Download PDF">pdf</a>, <a href="/format/2310.19540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IterInv: Iterative Inversion for Pixel-Level T2I Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chuanming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Weijer%2C+J">Joost van de Weijer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted paper at NeurIPS 2023 Workshop on Diffusion Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Large-scale text-to-image diffusion models have been a ground-breaking
development in generating convincing images following an input text prompt. The
goal of image editing research is to give users control over the generated
images by modifying the text prompt. Current image editing techniques are
relying on DDIM inversion as a common practice based on the Latent Diffusion
Models (LDM). However, the large pretrained T2I models working on the latent
space as LDM suffer from losing details due to the first compression stage with
an autoencoder mechanism. Instead, another mainstream T2I pipeline working on
the pixel level, such as Imagen and DeepFloyd-IF, avoids this problem. They are
commonly composed of several stages, normally with a text-to-image stage
followed by several super-resolution stages. In this case, the DDIM inversion
is unable to find the initial noise to generate the original image given that
the super-resolution diffusion models are not compatible with the DDIM
technique. According to our experimental findings, iteratively concatenating
the noisy image as the condition is the root of this problem. Based on this
observation, we develop an iterative inversion (IterInv) technique for this
stream of T2I models and verify IterInv with the open-source DeepFloyd-IF
model. By combining our method IterInv with a popular image editing method, we
prove the application prospects of IterInv. The code will be released at
\url{https://github.com/Tchuanm/IterInv.git}.
</p>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19542" title="Abstract">arXiv:2310.19542</a> [<a href="/pdf/2310.19542" title="Download PDF">pdf</a>, <a href="/format/2310.19542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Image-Related Inductive Biases in Single-Branch Visual  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chuanming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Weijer%2C+J">Joost van de Weijer</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianlin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yongmei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite achieving state-of-the-art performance in visual tracking, recent
single-branch trackers tend to overlook the weak prior assumptions associated
with the Vision Transformer (ViT) encoder and inference pipeline. Moreover, the
effectiveness of discriminative trackers remains constrained due to the
adoption of the dual-branch pipeline. To tackle the inferior effectiveness of
the vanilla ViT, we propose an Adaptive ViT Model Prediction tracker (AViTMP)
to bridge the gap between single-branch network and discriminative models.
Specifically, in the proposed encoder AViT-Enc, we introduce an adaptor module
and joint target state embedding to enrich the dense embedding paradigm based
on ViT. Then, we combine AViT-Enc with a dense-fusion decoder and a
discriminative target model to predict accurate location. Further, to mitigate
the limitations of conventional inference practice, we present a novel
inference pipeline called CycleTrack, which bolsters the tracking robustness in
the presence of distractors via bidirectional cycle tracking verification.
Lastly, we propose a dual-frame update inference strategy that adeptively
handles significant challenges in long-term scenarios. In the experiments, we
evaluate AViTMP on ten tracking benchmarks for a comprehensive assessment,
including LaSOT, LaSOTExtSub, AVisT, etc. The experimental results
unequivocally establish that AViTMP attains state-of-the-art performance,
especially on long-time tracking and robustness.
</p>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19545" title="Abstract">arXiv:2310.19545</a> [<a href="/pdf/2310.19545" title="Download PDF">pdf</a>, <a href="/format/2310.19545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MENTOR: Human Perception-Guided Pretraining for Iris Presentation  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crum%2C+C+R">Colton R. Crum</a>, 
<a href="/search/cs?searchtype=author&query=Czajka%2C+A">Adam Czajka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Incorporating human salience into the training of CNNs has boosted
performance in difficult tasks such as biometric presentation attack detection.
However, collecting human annotations is a laborious task, not to mention the
questions of how and where (in the model architecture) to efficiently
incorporate this information into model's training once annotations are
obtained. In this paper, we introduce MENTOR (huMan pErceptioN-guided
preTraining fOr iris pResentation attack detection), which addresses both of
these issues through two unique rounds of training. First, we train an
autoencoder to learn human saliency maps given an input iris image (both real
and fake examples). Once this representation is learned, we utilize the trained
autoencoder in two different ways: (a) as a pre-trained backbone for an iris
presentation attack detector, and (b) as a human-inspired annotator of salient
features on unknown data. We show that MENTOR's benefits are threefold: (a)
significant boost in iris PAD performance when using the human
perception-trained encoder's weights compared to general-purpose weights (e.g.
ImageNet-sourced, or random), (b) capability of generating infinite number of
human-like saliency maps for unseen iris PAD samples to be used in any human
saliency-guided training paradigm, and (c) increase in efficiency of iris PAD
model training. Sources codes and weights are offered along with the paper.
</p>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19554" title="Abstract">arXiv:2310.19554</a> [<a href="/pdf/2310.19554" title="Download PDF">pdf</a>, <a href="/format/2310.19554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harvest Video Foundation Models via Efficient Post-Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yizhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kunchang Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yinan He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yali Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Limin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Building video-language foundation models is costly and difficult due to the
redundant nature of video data and the lack of high-quality video-language
datasets. In this paper, we propose an efficient framework to harvest video
foundation models from image ones. Our method is intuitively simple by randomly
dropping input video patches and masking out input text during the
post-pretraining procedure. The patch dropping boosts the training efficiency
significantly and text masking enforces the learning of cross-modal fusion. We
conduct extensive experiments to validate the effectiveness of our method on a
wide range of video-language downstream tasks including various zero-shot
tasks, video question answering, and video-text retrieval. Despite its
simplicity, our method achieves state-of-the-art performances, which are
comparable to some heavily pretrained video foundation models. Our method is
extremely efficient and can be trained in less than one day on 8 GPUs,
requiring only WebVid-10M as pretraining data. We hope our method can serve as
a simple yet strong counterpart for prevalent video foundation models, provide
useful insights when building them, and make large pretrained models more
accessible and sustainable. This is part of the InternVideo project
\url{https://github.com/OpenGVLab/InternVideo}.
</p>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19555" title="Abstract">arXiv:2310.19555</a> [<a href="/pdf/2310.19555" title="Download PDF">pdf</a>, <a href="/format/2310.19555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Force Rendering and Its Evaluation of a Friction-based Walking Sensation  Display for a Seated User
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kato%2C+G">Ginga Kato</a>, 
<a href="/search/cs?searchtype=author&query=Kuroda%2C+Y">Yoshihiro Kuroda</a>, 
<a href="/search/cs?searchtype=author&query=Kiyokawa%2C+K">Kiyoshi Kiyokawa</a>, 
<a href="/search/cs?searchtype=author&query=Takemura%2C+H">Haruo Takemura</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Visualization and Computer Graphics, vol. 24,
  no. 4, pp. 1506-1514, April 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Most existing locomotion devices that represent the sensation of walking
target a user who is actually performing a walking motion. Here, we attempted
to represent the walking sensation, especially a kinesthetic sensation and
advancing feeling (the sense of moving forward) while the user remains seated.
To represent the walking sensation using a relatively simple device, we focused
on the force rendering and its evaluation of the longitudinal friction force
applied on the sole during walking. Based on the measurement of the friction
force applied on the sole during actual walking, we developed a novel friction
force display that can present the friction force without the influence of body
weight. Using performance evaluation testing, we found that the proposed method
can stably and rapidly display friction force. Also, we developed a virtual
reality (VR) walk-through system that is able to present the friction force
through the proposed device according to the avatar's walking motion in a
virtual world. By evaluating the realism, we found that the proposed device can
represent a more realistic advancing feeling than vibration feedback.
</p>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19558" title="Abstract">arXiv:2310.19558</a> [<a href="/pdf/2310.19558" title="Download PDF">pdf</a>, <a href="/format/2310.19558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-preserving Federated Primal-dual Learning for Non-convex and  Non-smooth Problems with Model Sparsification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chien-Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+C">Chong-Yung Chi</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q. S. Quek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning (FL) has been recognized as a rapidly growing research
area, where the model is trained over massively distributed clients under the
orchestration of a parameter server (PS) without sharing clients' data. This
paper delves into a class of federated problems characterized by non-convex and
non-smooth loss functions, that are prevalent in FL applications but
challenging to handle due to their intricate non-convexity and non-smoothness
nature and the conflicting requirements on communication efficiency and privacy
protection. In this paper, we propose a novel federated primal-dual algorithm
with bidirectional model sparsification tailored for non-convex and non-smooth
FL problems, and differential privacy is applied for strong privacy guarantee.
Its unique insightful properties and some privacy and convergence analyses are
also presented for the FL algorithm design guidelines. Extensive experiments on
real-world data are conducted to demonstrate the effectiveness of the proposed
algorithm and much superior performance than some state-of-the-art FL
algorithms, together with the validation of all the analytical results and
properties.
</p>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19559" title="Abstract">arXiv:2310.19559</a> [<a href="/pdf/2310.19559" title="Download PDF">pdf</a>, <a href="/format/2310.19559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangled Counterfactual Learning for Physical Audiovisual  Commonsense Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Changsheng Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yapeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+M">Mengshi Qi</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huadong Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in 37th Conference on Neural Information Processing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose a Disentangled Counterfactual Learning~(DCL)
approach for physical audiovisual commonsense reasoning. The task aims to infer
objects' physics commonsense based on both video and audio input, with the main
challenge is how to imitate the reasoning ability of humans. Most of the
current methods fail to take full advantage of different characteristics in
multi-modal data, and lacking causal reasoning ability in models impedes the
progress of implicit physical knowledge inferring. To address these issues, our
proposed DCL method decouples videos into static (time-invariant) and dynamic
(time-varying) factors in the latent space by the disentangled sequential
encoder, which adopts a variational autoencoder (VAE) to maximize the mutual
information with a contrastive loss function. Furthermore, we introduce a
counterfactual learning module to augment the model's reasoning ability by
modeling physical knowledge relationships among different objects under
counterfactual intervention. Our proposed method is a plug-and-play module that
can be incorporated into any baseline. In experiments, we show that our
proposed method improves baseline methods and achieves state-of-the-art
performance. Our source code is available at https://github.com/Andy20178/DCL.
</p>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19561" title="Abstract">arXiv:2310.19561</a> [<a href="/pdf/2310.19561" title="Download PDF">pdf</a>, <a href="/format/2310.19561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-parametric regression for robot learning on manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lopez-Custodio%2C+P+C">P. C. Lopez-Custodio</a>, 
<a href="/search/cs?searchtype=author&query=Bharath%2C+K">K. Bharath</a>, 
<a href="/search/cs?searchtype=author&query=Kucukyilmaz%2C+A">A. Kucukyilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Preston%2C+S+P">S. P. Preston</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Many of the tools available for robot learning were designed for Euclidean
data. However, many applications in robotics involve manifold-valued data. A
common example is orientation; this can be represented as a 3-by-3 rotation
matrix or a quaternion, the spaces of which are non-Euclidean manifolds. In
robot learning, manifold-valued data are often handled by relating the manifold
to a suitable Euclidean space, either by embedding the manifold or by
projecting the data onto one or several tangent spaces. These approaches can
result in poor predictive accuracy, and convoluted algorithms. In this paper,
we propose an "intrinsic" approach to regression that works directly within the
manifold. It involves taking a suitable probability distribution on the
manifold, letting its parameter be a function of a predictor variable, such as
time, then estimating that function non-parametrically via a "local likelihood"
method that incorporates a kernel. We name the method kernelised likelihood
estimation. The approach is conceptually simple, and generally applicable to
different manifolds. We implement it with three different types of
manifold-valued data that commonly appear in robotics applications. The results
of these experiments show better predictive accuracy than projection-based
algorithms.
</p>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19563" title="Abstract">arXiv:2310.19563</a> [<a href="/pdf/2310.19563" title="Download PDF">pdf</a>, <a href="/format/2310.19563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven optimal control via linear programming: boundedness  guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Falconi%2C+L">Lucia Falconi</a>, 
<a href="/search/eess?searchtype=author&query=Martinelli%2C+A">Andrea Martinelli</a>, 
<a href="/search/eess?searchtype=author&query=Lygeros%2C+J">John Lygeros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The linear programming (LP) approach is, together with value iteration and
policy iteration, one of the three fundamental methods to solve optimal control
problems in a dynamic programming setting. Despite its simple formulation,
versatility, and predisposition to be employed in model-free settings, the LP
approach has not enjoyed the same popularity as the other methods. The reason
is the often poor scalability of the exact LP approach and the difficulty to
obtain bounded solutions for a reasonable amount of constraints. We mitigate
these issues here, by investigating fundamental geometric features of the LP
and developing sufficient conditions to guarantee finite solutions with minimal
constraints. In the model-free context, we show that boundedness can be
guaranteed by a suitable choice of dataset and objective function.
</p>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19567" title="Abstract">arXiv:2310.19567</a> [<a href="/pdf/2310.19567" title="Download PDF">pdf</a>, <a href="/format/2310.19567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CreoleVal: Multilingual Multitask Benchmarks for Creoles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lent%2C+H">Heather Lent</a>, 
<a href="/search/cs?searchtype=author&query=Tatariya%2C+K">Kushal Tatariya</a>, 
<a href="/search/cs?searchtype=author&query=Dabre%2C+R">Raj Dabre</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fekete%2C+M">Marcell Fekete</a>, 
<a href="/search/cs?searchtype=author&query=Ploeger%2C+E">Esther Ploeger</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Li Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Heje%2C+H+E">Hans Erik Heje</a>, 
<a href="/search/cs?searchtype=author&query=Kanojia%2C+D">Diptesh Kanojia</a>, 
<a href="/search/cs?searchtype=author&query=Belony%2C+P">Paul Belony</a>, 
<a href="/search/cs?searchtype=author&query=Bollmann%2C+M">Marcel Bollmann</a>, 
<a href="/search/cs?searchtype=author&query=Grobol%2C+L">Lo&#xef;c Grobol</a>, 
<a href="/search/cs?searchtype=author&query=de+Lhoneux%2C+M">Miryam de Lhoneux</a>, 
<a href="/search/cs?searchtype=author&query=Hershcovich%2C+D">Daniel Hershcovich</a>, 
<a href="/search/cs?searchtype=author&query=DeGraff%2C+M">Michel DeGraff</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%B8gaard%2C+A">Anders S&#xf8;gaard</a>, 
<a href="/search/cs?searchtype=author&query=Bjerva%2C+J">Johannes Bjerva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Creoles represent an under-explored and marginalized group of languages, with
few available resources for NLP research. While the genealogical ties between
Creoles and other highly-resourced languages imply a significant potential for
transfer learning, this potential is hampered due to this lack of annotated
data. In this work we present CreoleVal, a collection of benchmark datasets
spanning 8 different NLP tasks, covering up to 28 Creole languages; it is an
aggregate of brand new development datasets for machine comprehension, relation
classification, and machine translation for Creoles, in addition to a practical
gateway to a handful of preexisting benchmarks. For each benchmark, we conduct
baseline experiments in a zero-shot setting in order to further ascertain the
capabilities and limitations of transfer learning for Creoles. Ultimately, the
goal of CreoleVal is to empower research on Creoles in NLP and computational
linguistics. We hope this resource will contribute to technological inclusion
for Creole language users around the globe.
</p>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19568" title="Abstract">arXiv:2310.19568</a> [<a href="/pdf/2310.19568" title="Download PDF">pdf</a>, <a href="/format/2310.19568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DataZoo: Streamlining Traffic Classification Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luxemburk%2C+J">Jan Luxemburk</a>, 
<a href="/search/cs?searchtype=author&query=Hynek%2C+K">Karel Hynek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The machine learning communities, such as those around computer vision or
natural language processing, have developed numerous supportive tools and
benchmark datasets to accelerate the development. In contrast, the network
traffic classification field lacks standard benchmark datasets for most tasks,
and the available supportive software is rather limited in scope. This paper
aims to address the gap and introduces DataZoo, a toolset designed to
streamline dataset management in network traffic classification and to reduce
the space for potential mistakes in the evaluation setup. DataZoo provides a
standardized API for accessing three extensive datasets -- CESNET-QUIC22,
CESNET-TLS22, and CESNET-TLS-Year22. Moreover, it includes methods for feature
scaling and realistic dataset partitioning, taking into consideration temporal
and service-related factors. The DataZoo toolset simplifies the creation of
realistic evaluation scenarios, making it easier to cross-compare
classification methods and reproduce results.
</p>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19571" title="Abstract">arXiv:2310.19571</a> [<a href="/pdf/2310.19571" title="Download PDF">pdf</a>, <a href="/format/2310.19571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A numerical study of the Dirichlet-to-Neumann operator in planar domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chaigneau%2C+A">Adrien Chaigneau</a>, 
<a href="/search/math?searchtype=author&query=Grebenkov%2C+D+S">Denis S. Grebenkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We numerically investigate the generalized Steklov problem for the modified
Helmholtz equation and focus on the relation between its spectrum and the
geometric structure of the domain. We address three distinct aspects: (i) the
asymptotic behavior of eigenvalues for polygonal domains; (ii) the dependence
of the integrals of eigenfunctions on the domain symmetries; and (iii) the
localization and exponential decay of Steklov eigenfunctions away from the
boundary for smooth shapes and in the presence of corners. For this purpose, we
implemented two complementary numerical methods to compute the eigenvalues and
eigenfunctions of the associated Dirichlet-to-Neumann operator for various
simply-connected planar domains. We also discuss applications of the obtained
results in the theory of diffusion-controlled reactions and formulate several
conjectures with relevance in spectral geometry.
</p>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19572" title="Abstract">arXiv:2310.19572</a> [<a href="/pdf/2310.19572" title="Download PDF">pdf</a>, <a href="/format/2310.19572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Input-label Mapping with Demonstration Replay for In-context  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zhuocheng Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiahao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xunliang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In-context learning (ICL) is an emerging capability of large autoregressive
language models where a few input-label demonstrations are appended to the
input to enhance the model's understanding of downstream NLP tasks, without
directly adjusting the model parameters. The effectiveness of ICL can be
attributed to the strong language modeling capabilities of large language
models (LLMs), which enable them to learn the mapping between input and labels
based on in-context demonstrations. Despite achieving promising results, the
causal nature of language modeling in ICL restricts the attention to be
backward only, i.e., a token only attends to its previous tokens, failing to
capture the full input-label information and limiting the model's performance.
In this paper, we propose a novel ICL method called Repeated Demonstration with
Sliding Causal Attention, (RdSca). Specifically, we duplicate later
demonstrations and concatenate them to the front, allowing the model to
`observe' the later information even under the causal restriction. Besides, we
introduce sliding causal attention, which customizes causal attention to avoid
information leakage. Experimental results show that our method significantly
improves the input-label mapping in ICL demonstrations. We also conduct an
in-depth analysis of how to customize the causal attention without training,
which has been an unexplored area in previous research.
</p>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19573" title="Abstract">arXiv:2310.19573</a> [<a href="/pdf/2310.19573" title="Download PDF">pdf</a>, <a href="/format/2310.19573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Uncertainty based Active Learning on Tabular Data using Boosted  Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shankaranarayana%2C+S+M">Sharath M Shankaranarayana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Supervised machine learning relies on the availability of good labelled data
for model training. Labelled data is acquired by human annotation, which is a
cumbersome and costly process, often requiring subject matter experts. Active
learning is a sub-field of machine learning which helps in obtaining the
labelled data efficiently by selecting the most valuable data instances for
model training and querying the labels only for those instances from the human
annotator. Recently, a lot of research has been done in the field of active
learning, especially for deep neural network based models. Although deep
learning shines when dealing with image\textual\multimodal data, gradient
boosting methods still tend to achieve much better results on tabular data. In
this work, we explore active learning for tabular data using boosted trees.
Uncertainty based sampling in active learning is the most commonly used
querying strategy, wherein the labels of those instances are sequentially
queried for which the current model prediction is maximally uncertain. Entropy
is often the choice for measuring uncertainty. However, entropy is not exactly
a measure of model uncertainty. Although there has been a lot of work in deep
learning for measuring model uncertainty and employing it in active learning,
it is yet to be explored for non-neural network models. To this end, we explore
the effectiveness of boosted trees based model uncertainty methods in active
learning. Leveraging this model uncertainty, we propose an uncertainty based
sampling in active learning for regression tasks on tabular data. Additionally,
we also propose a novel cost-effective active learning method for regression
tasks along with an improved cost-effective active learning method for
classification tasks.
</p>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19574" title="Abstract">arXiv:2310.19574</a> [<a href="/pdf/2310.19574" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skip-WaveNet: A Wavelet based Multi-scale Architecture to Trace Firn  Layers in Radar Echograms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Varshney%2C+D">Debvrat Varshney</a>, 
<a href="/search/cs?searchtype=author&query=Yari%2C+M">Masoud Yari</a>, 
<a href="/search/cs?searchtype=author&query=Ibikunle%2C+O">Oluwanisola Ibikunle</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jilu Li</a>, 
<a href="/search/cs?searchtype=author&query=Paden%2C+J">John Paden</a>, 
<a href="/search/cs?searchtype=author&query=Rahnemoonfar%2C+M">Maryam Rahnemoonfar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Echograms created from airborne radar sensors capture the profile of firn
layers present on top of an ice sheet. Accurate tracking of these layers is
essential to calculate the snow accumulation rates, which are required to
investigate the contribution of polar ice cap melt to sea level rise. However,
automatically processing the radar echograms to detect the underlying firn
layers is a challenging problem. In our work, we develop wavelet-based
multi-scale deep learning architectures for these radar echograms to improve
firn layer detection. We show that wavelet based architectures improve the
optimal dataset scale (ODS) and optimal image scale (OIS) F-scores by 3.99% and
3.7%, respectively, over the non-wavelet architecture. Further, our proposed
Skip-WaveNet architecture generates new wavelets in each iteration, achieves
higher generalizability as compared to state-of-the-art firn layer detection
networks, and estimates layer depths with a mean absolute error of 3.31 pixels
and 94.3% average precision. Such a network can be used by scientists to trace
firn layers, calculate the annual snow accumulation rates, estimate the
resulting surface mass balance of the ice sheet, and help project global sea
level rise.
</p>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19579" title="Abstract">arXiv:2310.19579</a> [<a href="/pdf/2310.19579" title="Download PDF">pdf</a>, <a href="/ps/2310.19579" title="Download PostScript">ps</a>, <a href="/format/2310.19579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Navigation Logic for Recursive Programs with Dynamic Thread Creation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lakenbrink%2C+R">Roman Lakenbrink</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller-Olm%2C+M">Markus M&#xfc;ller-Olm</a>, 
<a href="/search/cs?searchtype=author&query=Ohrem%2C+C">Christoph Ohrem</a>, 
<a href="/search/cs?searchtype=author&query=Gutsfeld%2C+J">Jens Gutsfeld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Dynamic Pushdown Networks (DPNs) are a model for multithreaded programs with
recursion and dynamic creation of threads. In this paper, we propose a temporal
logic called NTL for reasoning about the call- and return- as well as thread
creation behaviour of DPNs. Using tree automata techniques, we investigate the
model checking problem for the novel logic and show that its complexity is not
higher than that of LTL model checking against pushdown systems despite a more
expressive logic and a more powerful system model. The same holds true for the
satisfiability problem when compared to the satisfiability problem for a
related logic for reasoning about the call- and return-behaviour of pushdown
systems. Overall, this novel logic offers a promising approach for the
verification of recursive programs with dynamic thread creation.
</p>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19580" title="Abstract">arXiv:2310.19580</a> [<a href="/pdf/2310.19580" title="Download PDF">pdf</a>, <a href="/format/2310.19580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Perceptual Shape Loss for Monocular 3D Face Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Otto%2C+C">Christopher Otto</a>, 
<a href="/search/cs?searchtype=author&query=Chandran%2C+P">Prashanth Chandran</a>, 
<a href="/search/cs?searchtype=author&query=Zoss%2C+G">Gaspard Zoss</a>, 
<a href="/search/cs?searchtype=author&query=Gross%2C+M">Markus Gross</a>, 
<a href="/search/cs?searchtype=author&query=Gotardo%2C+P">Paulo Gotardo</a>, 
<a href="/search/cs?searchtype=author&query=Bradley%2C+D">Derek Bradley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to PG 2023. Project page: <a href="https://studios.disneyresearch.com/2023/10/09/a-perceptual-shape-loss-for-monocular-3d-face-reconstruction/">this https URL</a> Video: <a href="https://www.youtube.com/watch?v=RYdyoIZEuUI">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Graphics Forum, vol. 42, no. 7, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Monocular 3D face reconstruction is a wide-spread topic, and existing
approaches tackle the problem either through fast neural network inference or
offline iterative reconstruction of face geometry. In either case
carefully-designed energy functions are minimized, commonly including loss
terms like a photometric loss, a landmark reprojection loss, and others. In
this work we propose a new loss function for monocular face capture, inspired
by how humans would perceive the quality of a 3D face reconstruction given a
particular image. It is widely known that shading provides a strong indicator
for 3D shape in the human visual system. As such, our new 'perceptual' shape
loss aims to judge the quality of a 3D face estimate using only shading cues.
Our loss is implemented as a discriminator-style neural network that takes an
input face image and a shaded render of the geometry estimate, and then
predicts a score that perceptually evaluates how well the shaded render matches
the given image. This 'critic' network operates on the RGB image and geometry
render alone, without requiring an estimate of the albedo or illumination in
the scene. Furthermore, our loss operates entirely in image space and is thus
agnostic to mesh topology. We show how our new perceptual shape loss can be
combined with traditional energy terms for monocular 3D face optimization and
deep neural network regression, improving upon current state-of-the-art
results.
</p>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19582" title="Abstract">arXiv:2310.19582</a> [<a href="/pdf/2310.19582" title="Download PDF">pdf</a>, <a href="/format/2310.19582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-interpretable and deep features for image privacy classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baranouskaya%2C+D">Darya Baranouskaya</a>, 
<a href="/search/cs?searchtype=author&query=Cavallaro%2C+A">Andrea Cavallaro</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Image Processing (ICIP),
  Kuala Lumpur, Malaysia, 2023, pp. 3489-3492
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Privacy is a complex, subjective and contextual concept that is difficult to
define. Therefore, the annotation of images to train privacy classifiers is a
challenging task. In this paper, we analyse privacy classification datasets and
the properties of controversial images that are annotated with contrasting
privacy labels by different assessors. We discuss suitable features for image
privacy classification and propose eight privacy-specific and
human-interpretable features. These features increase the performance of deep
learning models and, on their own, improve the image representation for privacy
classification compared with much higher dimensional deep features.
</p>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19583" title="Abstract">arXiv:2310.19583</a> [<a href="/pdf/2310.19583" title="Download PDF">pdf</a>, <a href="/format/2310.19583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View  Stereo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vats%2C+V+K">Vibhas K. Vats</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+S">Sripad Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Crandall%2C+D+J">David J. Crandall</a>, 
<a href="/search/cs?searchtype=author&query=Reza%2C+M+A">Md. Alimoor Reza</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+S">Soon-heung Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in WACV 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Traditional multi-view stereo (MVS) methods rely heavily on photometric and
geometric consistency constraints, but newer machine learning-based MVS methods
check geometric consistency across multiple source views only as a
post-processing step. In this paper, we present a novel approach that
explicitly encourages geometric consistency of reference view depth maps across
multiple source views at different scales during learning (see Fig. 1). We find
that adding this geometric consistency loss significantly accelerates learning
by explicitly penalizing geometrically inconsistent pixels, reducing the
training iteration requirements to nearly half that of other MVS methods. Our
extensive experiments show that our approach achieves a new state-of-the-art on
the DTU and BlendedMVS datasets, and competitive results on the Tanks and
Temples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt
to enforce multi-view, multi-scale geometric consistency during learning.
</p>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19586" title="Abstract">arXiv:2310.19586</a> [<a href="/pdf/2310.19586" title="Download PDF">pdf</a>, <a href="/format/2310.19586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Multi-kernel Maximum Correntropy Kalman Filter for  Disturbance Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shilei Li</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+D">Dawei Shi</a>, 
<a href="/search/eess?searchtype=author&query=Lou%2C+Y">Yunjiang Lou</a>, 
<a href="/search/eess?searchtype=author&query=Zou%2C+W">Wulin Zou</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+L">Ling Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in IEEE Transactions on Automatic Control (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Disturbance observers have been attracting continuing research efforts and
are widely used in many applications. Among them, the Kalman filter-based
disturbance observer is an attractive one since it estimates both the state and
the disturbance simultaneously, and is optimal for a linear system with
Gaussian noises. Unfortunately, The noise in the disturbance channel typically
exhibits a heavy-tailed distribution because the nominal disturbance dynamics
usually do not align with the practical ones. To handle this issue, we propose
a generalized multi-kernel maximum correntropy Kalman filter for disturbance
estimation, which is less conservative by adopting different kernel bandwidths
for different channels and exhibits excellent performance both with and without
external disturbance. The convergence of the fixed point iteration and the
complexity of the proposed algorithm are given. Simulations on a robotic
manipulator reveal that the proposed algorithm is very efficient in disturbance
estimation with moderate algorithm complexity.
</p>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19588" title="Abstract">arXiv:2310.19588</a> [<a href="/pdf/2310.19588" title="Download PDF">pdf</a>, <a href="/format/2310.19588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPATD: Dual-Phase Audio Transformer for Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jialu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinzhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Youshan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE DDP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recent high-performance transformer-based speech enhancement models
demonstrate that time domain methods could achieve similar performance as
time-frequency domain methods. However, time-domain speech enhancement systems
typically receive input audio sequences consisting of a large number of time
steps, making it challenging to model extremely long sequences and train models
to perform adequately. In this paper, we utilize smaller audio chunks as input
to achieve efficient utilization of audio information to address the above
challenges. We propose a dual-phase audio transformer for denoising (DPATD), a
novel model to organize transformer layers in a deep structure to learn clean
audio sequences for denoising. DPATD splits the audio input into smaller
chunks, where the input length can be proportional to the square root of the
original sequence length. Our memory-compressed explainable attention is
efficient and converges faster compared to the frequently used self-attention
module. Extensive experiments demonstrate that our model outperforms
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19589" title="Abstract">arXiv:2310.19589</a> [<a href="/pdf/2310.19589" title="Download PDF">pdf</a>, <a href="/format/2310.19589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message  Passing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J+Y">Jung Yeon Park</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+L+L+S">Lawson L.S. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Data over non-Euclidean manifolds, often discretized as surface meshes,
naturally arise in computer graphics and biological and physical systems. In
particular, solutions to partial differential equations (PDEs) over manifolds
depend critically on the underlying geometry. While graph neural networks have
been successfully applied to PDEs, they do not incorporate surface geometry and
do not consider local gauge symmetries of the manifold. Alternatively, recent
works on gauge equivariant convolutional and attentional architectures on
meshes leverage the underlying geometry but underperform in modeling surface
PDEs with complex nonlinear dynamics. To address these issues, we introduce a
new gauge equivariant architecture using nonlinear message passing. Our novel
architecture achieves higher performance than either convolutional or
attentional networks on domains with highly complex and nonlinear dynamics.
However, similar to the non-mesh case, design trade-offs favor convolutional,
attentional, or message passing networks for different tasks; we investigate in
which circumstances our message passing method provides the most benefit.
</p>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19590" title="Abstract">arXiv:2310.19590</a> [<a href="/pdf/2310.19590" title="Download PDF">pdf</a>, <a href="/format/2310.19590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operator Learning Enhanced Physics-informed Neural Networks for Solving  Partial Differential Equations Characterized by Sharp Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhiping Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Karniadakis%2C+G+E">George Em Karniadakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Physics-informed Neural Networks (PINNs) have been shown as a promising
approach for solving both forward and inverse problems of partial differential
equations (PDEs). Meanwhile, the neural operator approach, including methods
such as Deep Operator Network (DeepONet) and Fourier neural operator (FNO), has
been introduced and extensively employed in approximating solution of PDEs.
Nevertheless, to solve problems consisting of sharp solutions poses a
significant challenge when employing these two approaches. To address this
issue, we propose in this work a novel framework termed Operator Learning
Enhanced Physics-informed Neural Networks (OL-PINN). Initially, we utilize
DeepONet to learn the solution operator for a set of smooth problems relevant
to the PDEs characterized by sharp solutions. Subsequently, we integrate the
pre-trained DeepONet with PINN to resolve the target sharp solution problem. We
showcase the efficacy of OL-PINN by successfully addressing various problems,
such as the nonlinear diffusion-reaction equation, the Burgers equation and the
incompressible Navier-Stokes equation at high Reynolds number. Compared with
the vanilla PINN, the proposed method requires only a small number of residual
points to achieve a strong generalization capability. Moreover, it
substantially enhances accuracy, while also ensuring a robust training process.
Furthermore, OL-PINN inherits the advantage of PINN for solving inverse
problems. To this end, we apply the OL-PINN approach for solving problems with
only partial boundary conditions, which usually cannot be solved by the
classical numerical methods, showing its capacity in solving ill-posed problems
and consequently more complex inverse problems.
</p>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19591" title="Abstract">arXiv:2310.19591</a> [<a href="/pdf/2310.19591" title="Download PDF">pdf</a>, <a href="/ps/2310.19591" title="Download PostScript">ps</a>, <a href="/format/2310.19591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction of Locally Stationary Data Using Expert Advice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=V%27yugin%2C+V">Vladimir V&#x27;yugin</a>, 
<a href="/search/cs?searchtype=author&query=Trunov%2C+V">Vladimir Trunov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 psges
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The problem of continuous machine learning is studied. Within the framework
of the game-theoretic approach, when for calculating the next forecast, no
assumptions about the stochastic nature of the source that generates the data
flow are used -- the source can be analog, algorithmic or probabilistic, its
parameters can change at random times, when building a prognostic model, only
structural assumptions are used about the nature of data generation. An online
forecasting algorithm for a locally stationary time series is presented. An
estimate of the efficiency of the proposed algorithm is obtained.
</p>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19594" title="Abstract">arXiv:2310.19594</a> [<a href="/pdf/2310.19594" title="Download PDF">pdf</a>, <a href="/format/2310.19594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Superpolynomial smoothed complexity of 3-FLIP in Local Max-Cut
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michel%2C+L">Lukas Michel</a>, 
<a href="/search/cs?searchtype=author&query=Scott%2C+A">Alex Scott</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We construct a graph with $n$ vertices where the smoothed runtime of the
3-FLIP algorithm for the 3-Opt Local Max-Cut problem can be as large as
$2^{\Omega(\sqrt{n})}$. This provides the first example where a local search
algorithm for the Max-Cut problem can fail to be efficient in the framework of
smoothed analysis. We also give a new construction of graphs where the runtime
of the FLIP algorithm for the Local Max-Cut problem is $2^{\Omega(n)}$ for any
pivot rule. This graph is much smaller and has a simpler structure than
previous constructions.
</p>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19596" title="Abstract">arXiv:2310.19596</a> [<a href="/pdf/2310.19596" title="Download PDF">pdf</a>, <a href="/format/2310.19596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMaAA: Making Large Language Models as Active Annotators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanzeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yongliang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Ming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+L">Lei Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prevalent supervised learning methods in natural language processing (NLP)
are notoriously data-hungry, which demand large amounts of high-quality
annotated data. In practice, acquiring such data is a costly endeavor.
Recently, the superior few-shot performance of large language models (LLMs) has
propelled the development of dataset generation, where the training data are
solely synthesized from LLMs. However, such an approach usually suffers from
low-quality issues, and requires orders of magnitude more labeled data to
achieve satisfactory performance. To fully exploit the potential of LLMs and
make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as
annotators and puts them into an active learning loop to determine what to
annotate efficiently. To learn robustly with pseudo labels, we optimize both
the annotation and training processes: (1) we draw k-NN examples from a small
demonstration pool as in-context examples, and (2) we adopt the example
reweighting technique to assign training samples with learnable weights.
Compared with previous approaches, LLMaAA features both efficiency and
reliability. We conduct experiments and analysis on two classic NLP tasks,
named entity recognition and relation extraction. With LLMaAA, task-specific
models trained from LLM-generated labels can outperform the teacher within only
hundreds of annotated examples, which is much more cost-effective than other
baselines.
</p>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19602" title="Abstract">arXiv:2310.19602</a> [<a href="/pdf/2310.19602" title="Download PDF">pdf</a>, <a href="/format/2310.19602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DCHT: Deep Complex Hybrid Transformer for Speech Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jialu Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Youshan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE DDP conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Most of the current deep learning-based approaches for speech enhancement
only operate in the spectrogram or waveform domain. Although a cross-domain
transformer combining waveform- and spectrogram-domain inputs has been
proposed, its performance can be further improved. In this paper, we present a
novel deep complex hybrid transformer that integrates both spectrogram and
waveform domains approaches to improve the performance of speech enhancement.
The proposed model consists of two parts: a complex Swin-Unet in the
spectrogram domain and a dual-path transformer network (DPTnet) in the waveform
domain. We first construct a complex Swin-Unet network in the spectrogram
domain and perform speech enhancement in the complex audio spectrum. We then
introduce improved DPT by adding memory-compressed attention. Our model is
capable of learning multi-domain features to reduce existing noise on different
domains in a complementary way. The experimental results on the
BirdSoundsDenoising dataset and the VCTK+DEMAND dataset indicate that our
method can achieve better performance compared to state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19603" title="Abstract">arXiv:2310.19603</a> [<a href="/pdf/2310.19603" title="Download PDF">pdf</a>, <a href="/format/2310.19603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Kalman Filters Can Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hovart%2C+B">Blanka Hovart</a>, 
<a href="/search/cs?searchtype=author&query=Kratsios%2C+A">Anastasis Kratsios</a>, 
<a href="/search/cs?searchtype=author&query=Limmer%2C+Y">Yannick Limmer</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xuwei Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Numerical Analysis (math.NA); Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep Kalman filters (DKFs) are a class of neural network models that generate
Gaussian probability measures from sequential data. Though DKFs are inspired by
the Kalman filter, they lack concrete theoretical ties to the stochastic
filtering problem, thus limiting their applicability to areas where traditional
model-based filters have been used, e.g.\ model calibration for bond and option
prices in mathematical finance. We address this issue in the mathematical
foundations of deep learning by exhibiting a class of continuous-time DKFs
which can approximately implement the conditional law of a broad class of
non-Markovian and conditionally Gaussian signal processes given noisy
continuous-times measurements. Our approximation results hold uniformly over
sufficiently regular compact subsets of paths, where the approximation error is
quantified by the worst-case 2-Wasserstein distance computed uniformly over the
given compact set of paths.
</p>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19607" title="Abstract">arXiv:2310.19607</a> [<a href="/pdf/2310.19607" title="Download PDF">pdf</a>, <a href="/format/2310.19607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technical Report on the Learning of Case Relevance in Case-Based  Reasoning with Abstract Argumentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paulino-Passos%2C+G">Guilherme Paulino-Passos</a>, 
<a href="/search/cs?searchtype=author&query=Toni%2C+F">Francesca Toni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Case-based reasoning is known to play an important role in several legal
settings. In this paper we focus on a recent approach to case-based reasoning,
supported by an instantiation of abstract argumentation whereby arguments
represent cases and attack between arguments results from outcome disagreement
between cases and a notion of relevance. In this context, relevance is
connected to a form of specificity among cases. We explore how relevance can be
learnt automatically in practice with the help of decision trees, and explore
the combination of case-based reasoning with abstract argumentation (AA-CBR)
and learning of case relevance for prediction in legal settings. Specifically,
we show that, for two legal datasets, AA-CBR and decision-tree-based learning
of case relevance perform competitively in comparison with decision trees. We
also show that AA-CBR with decision-tree-based learning of case relevance
results in a more compact representation than their decision tree counterparts,
which could be beneficial for obtaining cognitively tractable explanations.
</p>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19608" title="Abstract">arXiv:2310.19608</a> [<a href="/pdf/2310.19608" title="Download PDF">pdf</a>, <a href="/format/2310.19608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Feynman--Kac training of partial Bayesian neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mair%2C+S">Sebastian Mair</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6n%2C+T+B">Thomas B. Sch&#xf6;n</a>, 
<a href="/search/cs?searchtype=author&query=Sj%C3%B6lund%2C+J">Jens Sj&#xf6;lund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Recently, partial Bayesian neural networks (pBNNs), which only consider a
subset of the parameters to be stochastic, were shown to perform competitively
with full Bayesian neural networks. However, pBNNs are often multi-modal in the
latent-variable space and thus challenging to approximate with parametric
models. To address this problem, we propose an efficient sampling-based
training strategy, wherein the training of a pBNN is formulated as simulating a
Feynman--Kac model. We then describe variations of sequential Monte Carlo
samplers that allow us to simultaneously estimate the parameters and the latent
posterior distribution of this model at a tractable computational cost. We show
on various synthetic and real-world datasets that our proposed training scheme
outperforms the state of the art in terms of predictive performance.
</p>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19619" title="Abstract">arXiv:2310.19619</a> [<a href="/pdf/2310.19619" title="Download PDF">pdf</a>, <a href="/format/2310.19619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards A Holistic Landscape of Situated Theory of Mind in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziqiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sansom%2C+J">Jacob Sansom</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Run Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+J">Joyce Chai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Theme Track, Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have generated considerable interest and debate
regarding their potential emergence of Theory of Mind (ToM). Several recent
inquiries reveal a lack of robust ToM in these models and pose a pressing
demand to develop new benchmarks, as current ones primarily focus on different
aspects of ToM and are prone to shortcuts and data leakage. In this position
paper, we seek to answer two road-blocking questions: (1) How can we taxonomize
a holistic landscape of machine ToM? (2) What is a more effective evaluation
protocol for machine ToM? Following psychological studies, we taxonomize
machine ToM into 7 mental state categories and delineate existing benchmarks to
identify under-explored aspects of ToM. We argue for a holistic and situated
evaluation of ToM to break ToM into individual components and treat LLMs as an
agent who is physically situated in environments and socially situated in
interactions with humans. Such situated evaluation provides a more
comprehensive assessment of mental states and potentially mitigates the risk of
shortcuts and data leakage. We further present a pilot study in a grid world
setup as a proof of concept. We hope this position paper can facilitate future
research to integrate ToM with LLMs and offer an intuitive means for
researchers to better position their work in the landscape of ToM. Project
page: https://github.com/Mars-tin/awesome-theory-of-mind
</p>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19620" title="Abstract">arXiv:2310.19620</a> [<a href="/pdf/2310.19620" title="Download PDF">pdf</a>, <a href="/format/2310.19620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Trajectory Models are Scalable Motion Predictors and Planners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qiao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiduo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">Danjiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jingzhe Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Derun Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Simian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Ningyi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+G">Guangzhi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Motion prediction and planning are vital tasks in autonomous driving, and
recent efforts have shifted to machine learning-based approaches. The
challenges include understanding diverse road topologies, reasoning traffic
dynamics over a long time horizon, interpreting heterogeneous behaviors, and
generating policies in a large continuous state space. Inspired by the success
of large language models in addressing similar complexities through model
scaling, we introduce a scalable trajectory model called State Transformer
(STR). STR reformulates the motion prediction and motion planning problems by
arranging observations, states, and actions into one unified sequence modeling
task. With a simple model design, STR consistently outperforms baseline
approaches in both problems. Remarkably, experimental results reveal that large
trajectory models (LTMs), such as STR, adhere to the scaling laws by presenting
outstanding adaptability and learning efficiency. Qualitative results further
demonstrate that LTMs are capable of making plausible predictions in scenarios
that diverge significantly from the training data distribution. LTMs also learn
to make complex reasonings for long-term planning, without explicit loss
designs or costly high-level annotations.
</p>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19624" title="Abstract">arXiv:2310.19624</a> [<a href="/pdf/2310.19624" title="Download PDF">pdf</a>, <a href="/format/2310.19624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Post-Training Quantization of Protein Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Shuang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+N">Ning Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yanfeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+A">Aimin Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Recent advancements in unsupervised protein language models (ProteinLMs),
like ESM-1b and ESM-2, have shown promise in different protein prediction
tasks. However, these models face challenges due to their high computational
demands, significant memory needs, and latency, restricting their usage on
devices with limited resources. To tackle this, we explore post-training
quantization (PTQ) for ProteinLMs, focusing on ESMFold, a simplified version of
AlphaFold based on ESM-2 ProteinLM. Our study is the first attempt to quantize
all weights and activations of ProteinLMs. We observed that the typical uniform
quantization method performs poorly on ESMFold, causing a significant drop in
TM-Score when using 8-bit quantization. We conducted extensive quantization
experiments, uncovering unique challenges associated with ESMFold, particularly
highly asymmetric activation ranges before Layer Normalization, making
representation difficult using low-bit fixed-point formats. To address these
challenges, we propose a new PTQ method for ProteinLMs, utilizing piecewise
linear quantization for asymmetric activation values to ensure accurate
approximation. We demonstrated the effectiveness of our method in protein
structure prediction tasks, demonstrating that ESMFold can be accurately
quantized to low-bit widths without compromising accuracy. Additionally, we
applied our method to the contact prediction task, showcasing its versatility.
In summary, our study introduces an innovative PTQ method for ProteinLMs,
addressing specific quantization challenges and potentially leading to the
development of more efficient ProteinLMs with significant implications for
various protein-related applications.
</p>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19626" title="Abstract">arXiv:2310.19626</a> [<a href="/pdf/2310.19626" title="Download PDF">pdf</a>, <a href="/format/2310.19626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformation vs Tradition: Artificial General Intelligence (AGI) for  Arts and Humanities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qian Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junwen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianze Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hale%2C+J">John Hale</a>, 
<a href="/search/cs?searchtype=author&query=Gibbs%2C+J">John Gibbs</a>, 
<a href="/search/cs?searchtype=author&query=Rasheed%2C+K">Khaled Rasheed</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ninghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+G">Gengchen Mai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianming Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Recent advances in artificial general intelligence (AGI), particularly large
language models and creative image generation systems have demonstrated
impressive capabilities on diverse tasks spanning the arts and humanities.
However, the swift evolution of AGI has also raised critical questions about
its responsible deployment in these culturally significant domains
traditionally seen as profoundly human. This paper provides a comprehensive
analysis of the applications and implications of AGI for text, graphics, audio,
and video pertaining to arts and the humanities. We survey cutting-edge systems
and their usage in areas ranging from poetry to history, marketing to film, and
communication to classical art. We outline substantial concerns pertaining to
factuality, toxicity, biases, and public safety in AGI systems, and propose
mitigation strategies. The paper argues for multi-stakeholder collaboration to
ensure AGI promotes creativity, knowledge, and cultural values without
undermining truth or human dignity. Our timely contribution summarizes a
rapidly developing field, highlighting promising directions while advocating
for responsible progress centering on human flourishing. The analysis lays the
groundwork for further research on aligning AGI's technological capacities with
enduring social goods.
</p>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19629" title="Abstract">arXiv:2310.19629</a> [<a href="/pdf/2310.19629" title="Download PDF">pdf</a>, <a href="/format/2310.19629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuoman Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bo Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Code and data are available at: <a href="https://github.com/vLAR-group/RayDF">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">In this paper, we study the problem of continuous 3D shape representations.
The majority of existing successful methods are coordinate-based implicit
neural representations. However, they are inefficient to render novel views or
recover explicit surface points. A few works start to formulate 3D shapes as
ray-based neural functions, but the learned structures are inferior due to the
lack of multi-view geometry consistency. To tackle these challenges, we propose
a new framework called RayDF. It consists of three major components: 1) the
simple ray-surface distance field, 2) the novel dual-ray visibility classifier,
and 3) a multi-view consistency optimization module to drive the learned
ray-surface distances to be multi-view geometry consistent. We extensively
evaluate our method on three public datasets, demonstrating remarkable
performance in 3D surface point reconstruction on both synthetic and
challenging real-world 3D scenes, clearly surpassing existing coordinate-based
and ray-based baselines. Most notably, our method achieves a 1000x faster speed
than coordinate-based methods to render an 800x800 depth image, showing the
superiority of our method for 3D shape representation. Our code and data are
available at https://github.com/vLAR-group/RayDF
</p>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19630" title="Abstract">arXiv:2310.19630</a> [<a href="/pdf/2310.19630" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutional Neural Networks for Automatic Detection of Intact  Adenovirus from TEM Imaging with Debris, Broken and Artefacts Particles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rukundo%2C+O">Olivier Rukundo</a>, 
<a href="/search/cs?searchtype=author&query=Behanova%2C+A">Andrea Behanova</a>, 
<a href="/search/cs?searchtype=author&query=De+Feo%2C+R">Riccardo De Feo</a>, 
<a href="/search/cs?searchtype=author&query=Ronkko%2C+S">Seppo Ronkko</a>, 
<a href="/search/cs?searchtype=author&query=Oja%2C+J">Joni Oja</a>, 
<a href="/search/cs?searchtype=author&query=Tohka%2C+J">Jussi Tohka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Regular monitoring of the primary particles and purity profiles of a drug
product during development and manufacturing processes is essential for
manufacturers to avoid product variability and contamination. Transmission
electron microscopy (TEM) imaging helps manufacturers predict how changes
affect particle characteristics and purity for virus-based gene therapy vector
products and intermediates. Since intact particles can characterize efficacious
products, it is beneficial to automate the detection of intact adenovirus
against a non-intact-viral background mixed with debris, broken, and artefact
particles. In the presence of such particles, detecting intact adenoviruses
becomes more challenging. To overcome the challenge, due to such a presence, we
developed a software tool for semi-automatic annotation and segmentation of
adenoviruses and a software tool for automatic segmentation and detection of
intact adenoviruses in TEM imaging systems. The developed semi-automatic tool
exploited conventional image analysis techniques while the automatic tool was
built based on convolutional neural networks and image analysis techniques. Our
quantitative and qualitative evaluations showed outstanding true positive
detection rates compared to false positive and negative rates where
adenoviruses were nicely detected without mistaking them for real debris,
broken adenoviruses, and/or staining artefacts.
</p>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19634" title="Abstract">arXiv:2310.19634</a> [<a href="/pdf/2310.19634" title="Download PDF">pdf</a>, <a href="/format/2310.19634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iris: Dynamic Privacy Preserving Search in Structured Peer-to-Peer  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aktypi%2C+A">Angeliki Aktypi</a>, 
<a href="/search/cs?searchtype=author&query=Rasmussen%2C+K">Kasper Rasmussen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In structured peer-to-peer networks like Chord, the users manage to retrieve
the information they seek by asking other nodes from the network for the
information they search. Revealing to other nodes the search target makes
structured peer-to-peer networks unsuitable for applications that demand query
privacy, i.e., hiding the query's target from the intermediate nodes that take
part in the routing. This paper studies the query privacy of structured P2P
networks, particularly the Chord protocol.
<br />We initially observe that already proposed privacy notions, such as
$k$-anonymity, do not allow us to reason about the privacy guarantees of a
query in Chord in the presence of a strong adversary. Thus, we introduce a new
privacy notion that we call $(\alpha,\delta)$-privacy that allows us to
evaluate the privacy guarantees even when considering the worst-case scenario
regarding an attacker's background knowledge.
<br />We then design Iris, an algorithm that allows a requester to conceal the
target of a query in Chord from the intermediate nodes that take part in the
routing. Iris achieves that by having the requester query for other than the
target addresses so as reaching each one of them allows the requester to get
closer to the target address.
<br />We perform a security analysis of the proposed algorithm, based on the
privacy notion we introduce. We also develop a prototype of the algorithm in
Matlab and evaluate its performance. Our analysis proves Iris to be
$(\alpha,\delta)$-private while introducing a modest performance overhead.
</p>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19635" title="Abstract">arXiv:2310.19635</a> [<a href="/pdf/2310.19635" title="Download PDF">pdf</a>, <a href="/format/2310.19635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bidirectional Captioning for Clinically Accurate and Interpretable  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quigley%2C+K">Keegan Quigley</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+M">Miriam Cha</a>, 
<a href="/search/cs?searchtype=author&query=Barua%2C+J">Josh Barua</a>, 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+G">Geeticka Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Berkowitz%2C+S">Seth Berkowitz</a>, 
<a href="/search/cs?searchtype=author&query=Horng%2C+S">Steven Horng</a>, 
<a href="/search/cs?searchtype=author&query=Golland%2C+P">Polina Golland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures. Code release to follow
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vision-language pretraining has been shown to produce high-quality visual
encoders which transfer efficiently to downstream computer vision tasks. While
generative language models have gained widespread attention, image captioning
has thus far been mostly overlooked as a form of cross-modal pretraining in
favor of contrastive learning, especially in medical image analysis. In this
paper, we experiment with bidirectional captioning of radiology reports as a
form of pretraining and compare the quality and utility of learned embeddings
with those from contrastive pretraining methods. We optimize a CNN encoder,
transformer decoder architecture named RadTex for the radiology domain. Results
show that not only does captioning pretraining yield visual encoders that are
competitive with contrastive pretraining (CheXpert competition multi-label AUC
of 89.4%), but also that our transformer decoder is capable of generating
clinically relevant reports (captioning macro-F1 score of 0.349 using CheXpert
labeler) and responding to prompts with targeted, interactive outputs.
</p>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19636" title="Abstract">arXiv:2310.19636</a> [<a href="/pdf/2310.19636" title="Download PDF">pdf</a>, <a href="/format/2310.19636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leave No Stone Unturned: Mine Extra Knowledge for Imbalanced Facial  Expression Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Lixiong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuannan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weihong Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Facial expression data is characterized by a significant imbalance, with most
collected data showing happy or neutral expressions and fewer instances of fear
or disgust. This imbalance poses challenges to facial expression recognition
(FER) models, hindering their ability to fully understand various human
emotional states. Existing FER methods typically report overall accuracy on
highly imbalanced test sets but exhibit low performance in terms of the mean
accuracy across all expression classes. In this paper, our aim is to address
the imbalanced FER problem. Existing methods primarily focus on learning
knowledge of minor classes solely from minor-class samples. However, we propose
a novel approach to extract extra knowledge related to the minor classes from
both major and minor class samples. Our motivation stems from the belief that
FER resembles a distribution learning task, wherein a sample may contain
information about multiple classes. For instance, a sample from the major class
surprise might also contain useful features of the minor class fear. Inspired
by that, we propose a novel method that leverages re-balanced attention maps to
regularize the model, enabling it to extract transformation invariant
information about the minor classes from all training samples. Additionally, we
introduce re-balanced smooth labels to regulate the cross-entropy loss, guiding
the model to pay more attention to the minor classes by utilizing the extra
information regarding the label distribution of the imbalanced training data.
Extensive experiments on different datasets and backbones show that the two
proposed modules work together to regularize the model and achieve
state-of-the-art performance under the imbalanced FER task. Code is available
at https://github.com/zyh-uaiaaaa.
</p>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19639" title="Abstract">arXiv:2310.19639</a> [<a href="/pdf/2310.19639" title="Download PDF">pdf</a>, <a href="/ps/2310.19639" title="Download PostScript">ps</a>, <a href="/format/2310.19639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved $P_1$-interpolation error estimates in $W^{1,p}(]0,1[)$:  Application to finite element method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chaskalovic%2C+J">Joel Chaskalovic</a>, 
<a href="/search/math?searchtype=author&query=Assous%2C+F">Franck Assous</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Based on a new Taylor-like formula, we derived an improved interpolation
error estimate in $W^{1,p}$. We compare it with the classical error estimates
based on the standard Taylor formula, and also with the corresponding
interpolation error estimate, derived from the mean value theorem. We then
assess the improvement in accuracy we can get from this formula, leading to a
significant reduction in finite element computation costs.
</p>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19640" title="Abstract">arXiv:2310.19640</a> [<a href="/pdf/2310.19640" title="Download PDF">pdf</a>, <a href="/format/2310.19640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed coordinate Node link Visualization for Co_authorship Hypergraph  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nafar%2C+M">Mohsen Nafar</a>, 
<a href="/search/cs?searchtype=author&query=Zenouzagh%2C+H+A">Hamed Azami Zenouzagh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We present an algorithmic technique for visualizing the co-authorship
networks and other networks modeled with hypergraphs (set systems). As more
than two researchers can co-author a paper, a direct representation of the
interaction of researchers through their joint works cannot be adequately
modeled with direct links between the author-nodes. A hypergraph representation
of a co-authorship network treats researchers/authors as nodes and papers as
hyperedges (sets of authors). The visualization algorithm that we propose is
based on one of the well-studied approaches representing both authors and
papers as nodes of different classes. Our approach resembles some known ones
like anchored maps but introduces some special techniques for optimizing the
vertex positioning. The algorithm involves both continuous (force-directed)
optimization and discrete optimization for determining the node coordinates.
Moreover, one of the novelties of this work is classifying nodes and links
using different colors. This usage has a meaningful purpose that helps the
viewer to obtain valuable information from the visualization and increases the
readability of the layout. The algorithm is tuned to enable the viewer to
answer questions specific to co-authorship network studies.
</p>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19641" title="Abstract">arXiv:2310.19641</a> [<a href="/pdf/2310.19641" title="Download PDF">pdf</a>, <a href="/format/2310.19641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DistNet2D: Leveraging long-range temporal information for efficient  segmentation and tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ollion%2C+J">Jean Ollion</a>, 
<a href="/search/cs?searchtype=author&query=Maliet%2C+M">Martin Maliet</a>, 
<a href="/search/cs?searchtype=author&query=Giuglaris%2C+C">Caroline Giuglaris</a>, 
<a href="/search/cs?searchtype=author&query=Vacher%2C+E">Elise Vacher</a>, 
<a href="/search/cs?searchtype=author&query=Deforet%2C+M">Maxime Deforet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 5 figures, 9 supp figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Data Analysis, Statistics and Probability (physics.data-an); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Extracting long tracks and lineages from videomicroscopy requires an
extremely low error rate, which is challenging on complex datasets of dense or
deforming cells. Leveraging temporal context is key to overcome this challenge.
We propose DistNet2D, a new deep neural network (DNN) architecture for 2D cell
segmentation and tracking that leverages both mid- and long-term temporal
context. DistNet2D considers seven frames at the input and uses a
post-processing procedure that exploits information from the entire movie to
correct segmentation errors. DistNet2D outperforms two recent methods on two
experimental datasets, one containing densely packed bacterial cells and the
other containing eukaryotic cells. It has been integrated into an ImageJ-based
graphical user interface for 2D data visualization, curation, and training.
Finally, we demonstrate the performance of DistNet2D on correlating the size
and shape of cells with their transport properties over large statistics, for
both bacterial and eukaryotic cells.
</p>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19642" title="Abstract">arXiv:2310.19642</a> [<a href="/pdf/2310.19642" title="Download PDF">pdf</a>, <a href="/format/2310.19642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistent Query Answering for Primary Keys on Rooted Tree Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koutris%2C+P">Paraschos Koutris</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+X">Xiating Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Wijsen%2C+J">Jef Wijsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in PODS'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">We study the data complexity of consistent query answering (CQA) on databases
that may violate the primary key constraints. A repair is a maximal subset of
the database satisfying the primary key constraints. For a Boolean query q, the
problem CERTAINTY(q) takes a database as input, and asks whether or not each
repair satisfies q. The computational complexity of CERTAINTY(q) has been
established whenever q is a self-join-free Boolean conjunctive query, or a (not
necessarily self-join-free) Boolean path query. In this paper, we take one more
step towards a general classification for all Boolean conjunctive queries by
considering the class of rooted tree queries. In particular, we show that for
every rooted tree query q, CERTAINTY(q) is in FO, NL-hard $\cap$ LFP, or
coNP-complete, and it is decidable (in polynomial time), given q, which of the
three cases applies. We also extend our classification to larger classes of
queries with simple primary keys. Our classification criteria rely on query
homomorphisms and our polynomial-time fixpoint algorithm is based on a novel
use of context-free grammar (CFG).
</p>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19646" title="Abstract">arXiv:2310.19646</a> [<a href="/pdf/2310.19646" title="Download PDF">pdf</a>, <a href="/format/2310.19646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic 3D modeling by combining SBFEM and transfinite element shape  functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gravenkamp%2C+H">Hauke Gravenkamp</a>, 
<a href="/search/math?searchtype=author&query=Saputra%2C+A+A">Albert A. Saputra</a>, 
<a href="/search/math?searchtype=author&query=Eisentr%C3%A4ger%2C+S">Sascha Eisentr&#xe4;ger</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Three-dimensional image-based modeling by combining SBFEM and
  transfinite element shape functions, Computational Mechanics 66 (2020),
  911-30
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The scaled boundary finite element method (SBFEM) has recently been employed
as an efficient means to model three-dimensional structures, in particular when
the geometry is provided as a voxel-based image. To this end, an octree
decomposition of the computational domain is deployed and each cubic cell is
treated as an SBFEM subdomain. The surfaces of each subdomain are discretized
in the finite element sense. We improve on this idea by combining the
semi-analytical concept of the SBFEM with certain transition elements on the
subdomains' surfaces. Thus, we avoid the triangulation of surfaces employed in
previous works and consequently reduce the number of surface elements and
degrees of freedom. In addition, these discretizations allow coupling elements
of arbitrary order such that local p-refinement can be achieved
straightforwardly.
</p>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19647" title="Abstract">arXiv:2310.19647</a> [<a href="/pdf/2310.19647" title="Download PDF">pdf</a>, <a href="/ps/2310.19647" title="Download PostScript">ps</a>, <a href="/format/2310.19647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast swap regret minimization and applications to approximate correlated  equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Binghui Peng</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+A">Aviad Rubinstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We give a simple and computationally efficient algorithm that, for any
constant $\varepsilon&gt;0$, obtains $\varepsilon T$-swap regret within only $T =
\mathsf{polylog}(n)$ rounds; this is an exponential improvement compared to the
super-linear number of rounds required by the state-of-the-art algorithm, and
resolves the main open problem of [Blum and Mansour 2007]. Our algorithm has an
exponential dependence on $\varepsilon$, but we prove a new, matching lower
bound.
<br />Our algorithm for swap regret implies faster convergence to
$\varepsilon$-Correlated Equilibrium ($\varepsilon$-CE) in several regimes: For
normal form two-player games with $n$ actions, it implies the first uncoupled
dynamics that converges to the set of $\varepsilon$-CE in polylogarithmic
rounds; a $\mathsf{polylog}(n)$-bit communication protocol for $\varepsilon$-CE
in two-player games (resolving an open problem mentioned by
[Babichenko-Rubinstein'2017, Goos-Rubinstein'2018, Ganor-CS'2018]; and an
$\tilde{O}(n)$-query algorithm for $\varepsilon$-CE (resolving an open problem
of [Babichenko'2020] and obtaining the first separation between
$\varepsilon$-CE and $\varepsilon$-Nash equilibrium in the query complexity
model).
<br />For extensive-form games, our algorithm implies a PTAS for $\mathit{normal}$
$\mathit{form}$ $\mathit{correlated}$ $\mathit{equilibria}$, a solution concept
often conjectured to be computationally intractable (e.g. [Stengel-Forges'08,
Fujii'23]).
</p>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19650" title="Abstract">arXiv:2310.19650</a> [<a href="/pdf/2310.19650" title="Download PDF">pdf</a>, <a href="/format/2310.19650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KeyGen2Vec: Learning Document Embedding via Multi-label Keyword  Generation in Question-Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%27mah%2C+I">Iftitahu Ni&#x27;mah</a>, 
<a href="/search/cs?searchtype=author&query=Khoshrou%2C+S">Samaneh Khoshrou</a>, 
<a href="/search/cs?searchtype=author&query=Menkovski%2C+V">Vlado Menkovski</a>, 
<a href="/search/cs?searchtype=author&query=Pechenizkiy%2C+M">Mykola Pechenizkiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Arxiv preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Representing documents into high dimensional embedding space while preserving
the structural similarity between document sources has been an ultimate goal
for many works on text representation learning. Current embedding models,
however, mainly rely on the availability of label supervision to increase the
expressiveness of the resulting embeddings. In contrast, unsupervised
embeddings are cheap, but they often cannot capture implicit structure in
target corpus, particularly for samples that come from different distribution
with the pretraining source.
<br />Our study aims to loosen up the dependency on label supervision by learning
document embeddings via Sequence-to-Sequence (Seq2Seq) text generator.
Specifically, we reformulate keyphrase generation task into multi-label keyword
generation in community-based Question Answering (cQA). Our empirical results
show that KeyGen2Vec in general is superior than multi-label keyword classifier
by up to 14.7% based on Purity, Normalized Mutual Information (NMI), and
F1-Score metrics. Interestingly, although in general the absolute advantage of
learning embeddings through label supervision is highly positive across
evaluation datasets, KeyGen2Vec is shown to be competitive with classifier that
exploits topic label supervision in Yahoo! cQA with larger number of latent
topic labels.
</p>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19651" title="Abstract">arXiv:2310.19651</a> [<a href="/pdf/2310.19651" title="Download PDF">pdf</a>, <a href="/format/2310.19651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamics of Instruction Tuning: Each Ability of Large Language Models  Has Its Own Growth Pace
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chiyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanchao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jianhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+Y">Yuejiao Fei</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhenzhong Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Instruction tuning is a burgeoning method to elicit the general intelligence
of Large Language Models (LLMs). However, the creation of instruction data is
still largely heuristic, leading to significant variation in quality and
distribution across existing datasets. Experimental conclusions drawn from
these datasets are also inconsistent, with some studies emphasizing the
importance of scaling instruction numbers, while others argue that a limited
number of samples suffice. To better understand data construction guidelines,
we deepen our focus from the overall model performance to the growth of each
underlying ability, such as creative writing, code generation, and logical
reasoning. We systematically investigate the effects of data volume, parameter
size, and data construction methods on the development of various abilities,
using hundreds of model checkpoints (7b to 33b) fully instruction-tuned on a
new collection of over 40k human-curated instruction data. This proposed
dataset is stringently quality-controlled and categorized into ten distinct LLM
abilities. Our study reveals three primary findings: (i) Despite data volume
and parameter scale directly impacting models' overall performance, some
abilities are more responsive to their increases and can be effectively trained
using limited data, while some are highly resistant to these changes. (ii)
Human-curated data strongly outperforms synthetic data from GPT-4 in efficiency
and can constantly enhance model performance with volume increases, but is
unachievable with synthetic data. (iii) Instruction data brings powerful
cross-ability generalization, with evaluation results on out-of-domain data
mirroring the first two observations. Furthermore, we demonstrate how these
findings can guide more efficient data constructions, leading to practical
performance improvements on public benchmarks.
</p>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19654" title="Abstract">arXiv:2310.19654</a> [<a href="/pdf/2310.19654" title="Download PDF">pdf</a>, <a href="/format/2310.19654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient  image-text retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Youbo Lei</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+F">Feifei He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+Y">Yingbin Mo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+J">Si Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+D">Defeng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haonan Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the success of large-scale visual-language pretraining models and the
wide application of image-text retrieval in industry areas, reducing the model
size and streamlining their terminal-device deployment have become urgently
necessary. The mainstream model structures for image-text retrieval are
single-stream and dual-stream, both aiming to close the semantic gap between
visual and textual modalities. Dual-stream models excel at offline indexing and
fast inference, while single-stream models achieve more accurate cross-model
alignment by employing adequate feature fusion. We propose a multi-teacher
cross-modality alignment distillation (MCAD) technique to integrate the
advantages of single-stream and dual-stream models. By incorporating the fused
single-stream features into the image and text features of the dual-stream
model, we formulate new modified teacher features and logits. Then, we conduct
both logit and feature distillation to boost the capability of the student
dual-stream model, achieving high retrieval performance without increasing
inference complexity. Extensive experiments demonstrate the remarkable
performance and high efficiency of MCAD on image-text retrieval tasks.
Furthermore, we implement a mobile CLIP model on Snapdragon clips with only 93M
running memory and 30ms search latency, without apparent performance
degradation of the original large CLIP.
</p>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19658" title="Abstract">arXiv:2310.19658</a> [<a href="/pdf/2310.19658" title="Download PDF">pdf</a>, <a href="/format/2310.19658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining Tree Model Decisions in Natural Language for Network  Intrusion Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziems%2C+N">Noah Ziems</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Flanagan%2C+J">John Flanagan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS XAIA Workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Network intrusion detection (NID) systems which leverage machine learning
have been shown to have strong performance in practice when used to detect
malicious network traffic. Decision trees in particular offer a strong balance
between performance and simplicity, but require users of NID systems to have
background knowledge in machine learning to interpret. In addition, they are
unable to provide additional outside information as to why certain features may
be important for classification.
<br />In this work, we explore the use of large language models (LLMs) to provide
explanations and additional background knowledge for decision tree NID systems.
Further, we introduce a new human evaluation framework for decision tree
explanations, which leverages automatically generated quiz questions that
measure human evaluators' understanding of decision tree inference. Finally, we
show LLM generated decision tree explanations correlate highly with human
ratings of readability, quality, and use of background knowledge while
simultaneously providing better understanding of decision boundaries.
</p>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19660" title="Abstract">arXiv:2310.19660</a> [<a href="/pdf/2310.19660" title="Download PDF">pdf</a>, <a href="/format/2310.19660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable-by-Design Text Classification with Iteratively Generated  Concept Bottleneck
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ludan%2C+J+M">Josh Magnus Ludan</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+Q">Qing Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yue Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dugan%2C+L">Liam Dugan</a>, 
<a href="/search/cs?searchtype=author&query=Yatskar%2C+M">Mark Yatskar</a>, 
<a href="/search/cs?searchtype=author&query=Callison-Burch%2C+C">Chris Callison-Burch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Deep neural networks excel in text classification tasks, yet their
application in high-stakes domains is hindered by their lack of
interpretability. To address this, we propose Text Bottleneck Models (TBMs), an
intrinsically interpretable text classification framework that offers both
global and local explanations. Rather than directly predicting the output
label, TBMs predict categorical values for a sparse set of salient concepts and
use a linear layer over those concept values to produce the final prediction.
These concepts can be automatically discovered and measured by a Large Language
Model (LLM), without the need for human curation. On 12 diverse datasets, using
GPT-4 for both concept generation and measurement, we show that TBMs can rival
the performance of established black-box baselines such as GPT-4 fewshot and
finetuned DeBERTa, while falling short against finetuned GPT-3.5. Overall, our
findings suggest that TBMs are a promising new framework that enhances
interpretability, with minimal performance tradeoffs, particularly for
general-domain text.
</p>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19662" title="Abstract">arXiv:2310.19662</a> [<a href="/pdf/2310.19662" title="Download PDF">pdf</a>, <a href="/format/2310.19662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating synthetic power grids using exponential random graphs models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Giacomarra%2C+F">Francesco Giacomarra</a>, 
<a href="/search/eess?searchtype=author&query=Bet%2C+G">Gianmarco Bet</a>, 
<a href="/search/eess?searchtype=author&query=Zocca%2C+A">Alessandro Zocca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Probability (math.PR); Statistics Theory (math.ST); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)

</div>
<p class="mathjax">Synthetic power grids enable secure, real-world energy system simulations and
are crucial for algorithm testing, resilience assessment, and policy
formulation. We propose a novel method for the generation of synthetic
transmission power grids using Exponential Random Graph (ERG) models. Our two
main contributions are: (1) the formulation of an ERG model tailored
specifically for capturing the topological nuances of power grids, and (2) a
general procedure for estimating the parameters of such a model conditioned on
working with connected graphs. From a modeling perspective, we identify the
edge counts per bus type and $k$-triangles as crucial topological
characteristics for synthetic power grid generation. From a technical
perspective, we develop a rigorous methodology to estimate the parameters of an
ERG constrained to the space of connected graphs. The proposed model is
flexible, easy to implement, and successfully captures the desired topological
properties of power grids.
</p>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19663" title="Abstract">arXiv:2310.19663</a> [<a href="/pdf/2310.19663" title="Download PDF">pdf</a>, <a href="/ps/2310.19663" title="Download PostScript">ps</a>, <a href="/format/2310.19663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A linear doubly stabilized Crank-Nicolson scheme for the Allen-Cahn  equation with a general mobility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hou%2C+D">Dianming Hou</a>, 
<a href="/search/math?searchtype=author&query=Qiao%2C+Z">Zhonghua Qiao</a>, 
<a href="/search/math?searchtype=author&query=Ju%2C+L">Lili Ju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, a linear second order numerical scheme is developed and
investigated for the Allen-Cahn equation with a general positive mobility. In
particular, our fully discrete scheme is mainly constructed based on the
Crank-Nicolson formula for temporal discretization and the central finite
difference method for spatial approximation, and two extra stabilizing terms
are also introduced for the purpose of improving numerical stability. The
proposed scheme is shown to unconditionally preserve the maximum bound
principle (MBP) under mild restrictions on the stabilization parameters, which
is of practical importance for achieving good accuracy and stability
simultaneously. With the help of uniform boundedness of the numerical solutions
due to MBP, we then successfully derive $H^{1}$-norm and $L^{\infty}$-norm
error estimates for the Allen-Cahn equation with a constant and a variable
mobility, respectively. Moreover, the energy stability of the proposed scheme
is also obtained in the sense that the discrete free energy is uniformly
bounded by the one at the initial time plus a {\color{black}constant}. Finally,
some numerical experiments are carried out to verify the theoretical results
and illustrate the performance of the proposed scheme with a time adaptive
strategy.
</p>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19666" title="Abstract">arXiv:2310.19666</a> [<a href="/pdf/2310.19666" title="Download PDF">pdf</a>, <a href="/format/2310.19666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Shikai Fang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shibo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhe%2C+S">Shandian Zhe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Tensor decomposition is an important tool for multiway data analysis. In
practice, the data is often sparse yet associated with rich temporal
information. Existing methods, however, often under-use the time information
and ignore the structural knowledge within the sparsely observed tensor
entries. To overcome these limitations and to better capture the underlying
temporal structure, we propose Dynamic EMbedIngs fOr dynamic Tensor
dEcomposition (DEMOTE). We develop a neural diffusion-reaction process to
estimate dynamic embeddings for the entities in each tensor mode. Specifically,
based on the observed tensor entries, we build a multi-partite graph to encode
the correlation between the entities. We construct a graph diffusion process to
co-evolve the embedding trajectories of the correlated entities and use a
neural network to construct a reaction process for each individual entity. In
this way, our model can capture both the commonalities and personalities during
the evolution of the embeddings for different entities. We then use a neural
network to model the entry value as a nonlinear function of the embedding
trajectories. For model estimation, we combine ODE solvers to develop a
stochastic mini-batch learning algorithm. We propose a stratified sampling
method to balance the cost of processing each mini-batch so as to improve the
overall efficiency. We show the advantage of our approach in both simulation
study and real-world applications. The code is available at
https://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes.
</p>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19668" title="Abstract">arXiv:2310.19668</a> [<a href="/pdf/2310.19668" title="Download PDF">pdf</a>, <a href="/format/2310.19668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DrM: Mastering Visual Reinforcement Learning through Dormant Ratio  Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guowei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruijie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yongyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhecheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Tianying Ji</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiaxin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+P">Pu Hua</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuzhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Ze%2C+Y">Yanjie Ze</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Visual reinforcement learning (RL) has shown promise in continuous control
tasks. Despite its progress, current algorithms are still unsatisfactory in
virtually every aspect of the performance such as sample efficiency, asymptotic
performance, and their robustness to the choice of random seeds. In this paper,
we identify a major shortcoming in existing visual RL methods that is the
agents often exhibit sustained inactivity during early training, thereby
limiting their ability to explore effectively. Expanding upon this crucial
observation, we additionally unveil a significant correlation between the
agents' inclination towards motorically inactive exploration and the absence of
neuronal activity within their policy networks. To quantify this inactivity, we
adopt dormant ratio as a metric to measure inactivity in the RL agent's
network. Empirically, we also recognize that the dormant ratio can act as a
standalone indicator of an agent's activity level, regardless of the received
reward signals. Leveraging the aforementioned insights, we introduce DrM, a
method that uses three core mechanisms to guide agents'
exploration-exploitation trade-offs by actively minimizing the dormant ratio.
Experiments demonstrate that DrM achieves significant improvements in sample
efficiency and asymptotic performance with no broken seeds (76 seeds in total)
across three continuous control benchmark environments, including DeepMind
Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first
model-free algorithm that consistently solves tasks in both the Dog and
Manipulator domains from the DeepMind Control Suite as well as three dexterous
hand manipulation tasks without demonstrations in Adroit, all based on pixel
observations.
</p>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19670" title="Abstract">arXiv:2310.19670</a> [<a href="/pdf/2310.19670" title="Download PDF">pdf</a>, <a href="/format/2310.19670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatiotemporal Attention Enhances Lidar-Based Robot Navigation in  Dynamic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Heuvel%2C+J">Jorge de Heuvel</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xiangyu Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weixian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sethuraman%2C+T">Tharun Sethuraman</a>, 
<a href="/search/cs?searchtype=author&query=Bennewitz%2C+M">Maren Bennewitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Foresighted robot navigation in dynamic indoor environments with
cost-efficient hardware necessitates the use of a lightweight yet dependable
controller. So inferring the scene dynamics from sensor readings without
explicit object tracking is a pivotal aspect of foresighted navigation among
pedestrians. In this paper, we introduce a spatiotemporal attention pipeline
for enhanced navigation based on 2D lidar sensor readings. This pipeline is
complemented by a novel lidar-state representation that emphasizes dynamic
obstacles over static ones. Subsequently, the attention mechanism enables
selective scene perception across both space and time, resulting in improved
overall navigation performance within dynamic scenarios. We thoroughly
evaluated the approach in different scenarios and simulators, finding good
generalization to unseen environments. The results demonstrate outstanding
performance compared to state-of-the-art methods, thereby enabling the seamless
deployment of the learned controller on a real robot.
</p>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19671" title="Abstract">arXiv:2310.19671</a> [<a href="/pdf/2310.19671" title="Download PDF">pdf</a>, <a href="/format/2310.19671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models: The Need for Nuance in Current Debates and a  Pragmatic Perspective on Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Dijk%2C+B+M+A">Bram M.A. van Dijk</a>, 
<a href="/search/cs?searchtype=author&query=Kouwenhoven%2C+T">Tom Kouwenhoven</a>, 
<a href="/search/cs?searchtype=author&query=Spruit%2C+M+R">Marco R. Spruit</a>, 
<a href="/search/cs?searchtype=author&query=van+Duijn%2C+M+J">Max J. van Duijn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 0 figures, Forthcoming in Proceedings of the 27th Conference on Computational Natural Language Learningn (CoNLL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Current Large Language Models (LLMs) are unparalleled in their ability to
generate grammatically correct, fluent text. LLMs are appearing rapidly, and
debates on LLM capacities have taken off, but reflection is lagging behind.
Thus, in this position paper, we first zoom in on the debate and critically
assess three points recurring in critiques of LLM capacities: i) that LLMs only
parrot statistical patterns in the training data; ii) that LLMs master formal
but not functional language competence; and iii) that language learning in LLMs
cannot inform human language learning. Drawing on empirical and theoretical
arguments, we show that these points need more nuance. Second, we outline a
pragmatic perspective on the issue of `real' understanding and intentionality
in LLMs. Understanding and intentionality pertain to unobservable mental states
we \textit{attribute} to other humans because they have \textit{pragmatic
value}: they allow us to abstract away from complex underlying mechanics and
predict behaviour effectively. We reflect on the circumstances under which it
would make sense for humans to similarly attribute mental states to LLMs,
thereby outlining a pragmatic philosophical context for LLMs as an increasingly
prominent technology in society.
</p>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19673" title="Abstract">arXiv:2310.19673</a> [<a href="/pdf/2310.19673" title="Download PDF">pdf</a>, <a href="/format/2310.19673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Analysis of a Novel Radial Deployment Mechanism of Payloads  in Sounding Rockets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Singh%2C+T+P+G">Thakur Pranav G. Singh</a>, 
<a href="/search/eess?searchtype=author&query=Anand%2C+U">Utkarsh Anand</a>, 
<a href="/search/eess?searchtype=author&query=Agrawal%2C+T">Tanvi Agrawal</a>, 
<a href="/search/eess?searchtype=author&query=G%2C+S">Srinivas G</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 Pages, 11 Figures, 1 Table, Submitted to Journal of Applied Mechanics and is in Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This research paper introduces an innovative payload deployment mechanism
tailored for sounding rockets, addressing a crucial challenge in the field. The
problem statement revolves around the need to efficiently and compactly deploy
multiple payloads during a single rocket launch. This mechanism, designed to be
exceptionally suitable for sounding rockets, features a cylindrical carrier
structure equipped with multiple independently operable deployment ports.
Powered by a motor, the carrier structure rotates to enable radial ejection of
payloads. In this paper, we present the mechanism's design and conduct a
comprehensive performance analysis. This analysis encompasses an examination of
structural stability, system dynamics, motor torque, and power requirements.
Additionally, we develop a simulation model to assess payload deployment
behavior under various conditions. Our findings demonstrate the viability and
efficiency of this proposed mechanism for deploying multiple payloads within a
single sounding rocket launch. Its adaptability to accommodate diverse payload
types and sizes enhances its versatility. Moreover, the mechanism's radial
deployment capability allows payloads to be released at different altitudes,
thereby offering greater flexibility for scientific experiments. In summary,
this innovative payload radial deployment mechanism represents a significant
advancement in sounding rocket technology and holds promise for a wide array of
applications in both scientific and commercial missions.
</p>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19674" title="Abstract">arXiv:2310.19674</a> [<a href="/pdf/2310.19674" title="Download PDF">pdf</a>, <a href="/format/2310.19674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model order reduction of an ultraweak and optimally stable variational  formulation for parametrized reactive transport problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Engwer%2C+C">Christian Engwer</a>, 
<a href="/search/math?searchtype=author&query=Ohlberger%2C+M">Mario Ohlberger</a>, 
<a href="/search/math?searchtype=author&query=Renelt%2C+L">Lukas Renelt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This contribution introduces a model order reduction approach for an
advection-reaction problem with a parametrized reaction function. The
underlying discretization uses an ultraweak formulation with an $L^2$-like
trial space and an 'optimal' test space as introduced by Demkowicz et al. This
ensures the stability of the discretization and in addition allows for a
symmetric reformulation of the problem in terms of a dual solution which can
also be interpreted as the normal equations of an adjoint least-squares
problem. Classic model order reduction techniques can then be applied to the
space of dual solutions which also immediately gives a reduced primal space. We
show that the necessary computations do not require the reconstruction of any
primal solutions and can instead be performed entirely on the space of dual
solutions. We prove exponential convergence of the Kolmogorov $N$-width and
show that a greedy algorithm produces quasi-optimal approximation spaces for
both the primal and the dual solution space. Numerical experiments based on the
benchmark problem of a catalytic filter confirm the applicability of the
proposed method.
</p>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19676" title="Abstract">arXiv:2310.19676</a> [<a href="/pdf/2310.19676" title="Download PDF">pdf</a>, <a href="/ps/2310.19676" title="Download PostScript">ps</a>, <a href="/format/2310.19676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyPE: Attention with Hyperbolic Biases for Relative Positional Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Angelotti%2C+G">Giorgio Angelotti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Independent Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In Transformer-based architectures, the attention mechanism is inherently
permutation-invariant with respect to the input sequence's tokens. To impose
sequential order, token positions are typically encoded using a scheme with
either fixed or learnable parameters. We introduce Hyperbolic Positional
Encoding (HyPE), a novel method that utilizes hyperbolic functions' properties
to encode tokens' relative positions. This approach biases the attention
mechanism without the necessity of storing the $O(L^2)$ values of the mask,
with $L$ being the length of the input sequence. HyPE leverages preliminary
concatenation operations and matrix multiplications, facilitating the encoding
of relative distances indirectly incorporating biases into the softmax
computation. This design ensures compatibility with FlashAttention-2 and
supports the gradient backpropagation for any potential learnable parameters
within the encoding. We analytically demonstrate that, by careful
hyperparameter selection, HyPE can approximate the attention bias of ALiBi,
thereby offering promising generalization capabilities for contexts extending
beyond the lengths encountered during pretraining. The experimental evaluation
of HyPE is proposed as a direction for future research.
</p>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19677" title="Abstract">arXiv:2310.19677</a> [<a href="/pdf/2310.19677" title="Download PDF">pdf</a>, <a href="/format/2310.19677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoCa: Measuring Human-Language Model Alignment on Causal and Moral  Judgment Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+A">Allen Nie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Amdekar%2C+A">Atharva Amdekar</a>, 
<a href="/search/cs?searchtype=author&query=Piech%2C+C">Chris Piech</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T+H">Tatsu H. Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Gerstenberg%2C+T">Tobias Gerstenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 7 figures. NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Human commonsense understanding of the physical and social world is organized
around intuitive theories. These theories support making causal and moral
judgments. When something bad happens, we naturally ask: who did what, and why?
A rich literature in cognitive science has studied people's causal and moral
intuitions. This work has revealed a number of factors that systematically
influence people's judgments, such as the violation of norms and whether the
harm is avoidable or inevitable. We collected a dataset of stories from 24
cognitive science papers and developed a system to annotate each story with the
factors they investigated. Using this dataset, we test whether large language
models (LLMs) make causal and moral judgments about text-based scenarios that
align with those of human participants. On the aggregate level, alignment has
improved with more recent LLMs. However, using statistical analyses, we find
that LLMs weigh the different factors quite differently from human
participants. These results show how curated, challenge datasets combined with
insights from cognitive science can help us go beyond comparisons based merely
on aggregate metrics: we uncover LLMs implicit tendencies and show to what
extent these align with human intuitions.
</p>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19680" title="Abstract">arXiv:2310.19680</a> [<a href="/pdf/2310.19680" title="Download PDF">pdf</a>, <a href="/format/2310.19680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Pre-trained Language Model into Neural Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S">Soon-Jae Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+C">Chang-Sung Jeong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural Machine Translation (NMT) has become a significant technology in
natural language processing through extensive research and development.
However, the deficiency of high-quality bilingual language pair data still
poses a major challenge to improving NMT performance. Recent studies are
exploring the use of contextual information from pre-trained language model
(PLM) to address this problem. Yet, the issue of incompatibility between PLM
and NMT model remains unresolved. This study proposes a PLM-integrated NMT
(PiNMT) model to overcome the identified problems. The PiNMT model consists of
three critical components, PLM Multi Layer Converter, Embedding Fusion, and
Cosine Alignment, each playing a vital role in providing effective PLM
information to NMT. Furthermore, two training strategies, Separate Learning
Rates and Dual Step Training, are also introduced in this paper. By
implementing the proposed PiNMT model and training strategy, we achieved
state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset.
This study's outcomes are noteworthy as they demonstrate a novel approach for
efficiently integrating PLM with NMT to overcome incompatibility and enhance
performance.
</p>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19681" title="Abstract">arXiv:2310.19681</a> [<a href="/pdf/2310.19681" title="Download PDF">pdf</a>, <a href="/format/2310.19681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed multi-UAV shield formation based on virtual surface  constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guinaldo%2C+M">Mar&#xed;a Guinaldo</a>, 
<a href="/search/eess?searchtype=author&query=S%C3%A1nchez-Moreno%2C+J">Jos&#xe9; S&#xe1;nchez-Moreno</a>, 
<a href="/search/eess?searchtype=author&query=Zaragoza%2C+S">Salvador Zaragoza</a>, 
<a href="/search/eess?searchtype=author&query=Ma%C3%B1as-%C3%81lvarez%2C+F+J">Francisco Jos&#xe9; Ma&#xf1;as-&#xc1;lvarez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper proposes a method for the deployment of a multi-agent system of
unmanned aerial vehicles (UAVs) as a shield with potential applications in the
protection of infrastructures. For this purpose, a distributed control law
based on the gradient of a potential function is proposed to acquire the
desired shield shape, which is modeled as a quadric surface in the 3D space.
The graph of the formation is a Delaunay triangulation, which guarantees the
formation to be rigid. An algorithm is proposed to design the formation (target
distances between agents and interconnections) to distribute the agents over
the virtual surface, where the input parameters are just the parametrization of
the quadric and the number of agents of the system. Proofs of system stability
with the proposed control law are provided, as well as a new method to
guarantee that the resulting triangulation over the surface is Delaunay, which
can be executed locally. Simulation and experimental results illustrate the
effectiveness of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19684" title="Abstract">arXiv:2310.19684</a> [<a href="/pdf/2310.19684" title="Download PDF">pdf</a>, <a href="/format/2310.19684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Density Estimation for Entry Guidance Problems using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rataczak%2C+J+A">Jens A. Rataczak</a>, 
<a href="/search/eess?searchtype=author&query=Amato%2C+D">Davide Amato</a>, 
<a href="/search/eess?searchtype=author&query=McMahon%2C+J+W">Jay W. McMahon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Currently under revision for the AIAA Journal of Guidance Control and Dynamics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work presents a deep-learning approach to estimate atmospheric density
profiles for use in planetary entry guidance problems. A long short-term memory
(LSTM) neural network is trained to learn the mapping between measurements
available onboard an entry vehicle and the density profile through which it is
flying. Measurements include the spherical state representation, Cartesian
sensed acceleration components, and a surface-pressure measurement. Training
data for the network is initially generated by performing a Monte Carlo
analysis of an entry mission at Mars using the fully numerical
predictor-corrector guidance (FNPEG) algorithm that utilizes an exponential
density model, while the truth density profiles are sampled from MarsGRAM. A
curriculum learning procedure is developed to refine the LSTM network's
predictions for integration within the FNPEG algorithm. The trained LSTM is
capable of both predicting the density profile through which the vehicle will
fly and reconstructing the density profile through which it has already flown.
The performance of the FNPEG algorithm is assessed for three different density
estimation techniques: an exponential model, an exponential model augmented
with a first-order fading-memory filter, and the LSTM network. Results
demonstrate that using the LSTM model results in superior terminal accuracy
compared to the other two techniques when considering both noisy and noiseless
measurements.
</p>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19685" title="Abstract">arXiv:2310.19685</a> [<a href="/pdf/2310.19685" title="Download PDF">pdf</a>, <a href="/format/2310.19685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DGFN: Double Generative Flow Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lau%2C+E">Elaine Lau</a>, 
<a href="/search/cs?searchtype=author&query=Vemgal%2C+N">Nikhil Vemgal</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+E">Emmanuel Bengio</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Deep learning is emerging as an effective tool in drug discovery, with
potential applications in both predictive and generative models. Generative
Flow Networks (GFlowNets/GFNs) are a recently introduced method recognized for
the ability to generate diverse candidates, in particular in small molecule
generation tasks. In this work, we introduce double GFlowNets (DGFNs). Drawing
inspiration from reinforcement learning and Double Deep Q-Learning, we
introduce a target network used to sample trajectories, while updating the main
network with these sampled trajectories. Empirical results confirm that DGFNs
effectively enhance exploration in sparse reward domains and high-dimensional
state spaces, both challenging aspects of de-novo design in drug discovery.
</p>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19686" title="Abstract">arXiv:2310.19686</a> [<a href="/pdf/2310.19686" title="Download PDF">pdf</a>, <a href="/format/2310.19686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can input reconstruction be used to directly estimate uncertainty of a  regression U-Net model? -- Application to proton therapy dose prediction for  head and neck cancer patients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huet-Dastarac%2C+M">Margerie Huet-Dastarac</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dan Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Steve Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">John Lee</a>, 
<a href="/search/cs?searchtype=author&query=Montero%2C+A+B">Ana Barragan Montero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures and 3 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Estimating the uncertainty of deep learning models in a reliable and
efficient way has remained an open problem, where many different solutions have
been proposed in the literature. Most common methods are based on Bayesian
approximations, like Monte Carlo dropout (MCDO) or Deep ensembling (DE), but
they have a high inference time (i.e. require multiple inference passes) and
might not work for out-of-distribution detection (OOD) data (i.e. similar
uncertainty for in-distribution (ID) and OOD). In safety critical environments,
like medical applications, accurate and fast uncertainty estimation methods,
able to detect OOD data, are crucial, since wrong predictions can jeopardize
patients safety. In this study, we present an alternative direct uncertainty
estimation method and apply it for a regression U-Net architecture. The method
consists in the addition of a branch from the bottleneck which reconstructs the
input. The input reconstruction error can be used as a surrogate of the model
uncertainty. For the proof-of-concept, our method is applied to proton therapy
dose prediction in head and neck cancer patients. Accuracy, time-gain, and OOD
detection are analyzed for our method in this particular application and
compared with the popular MCDO and DE. The input reconstruction method showed a
higher Pearson correlation coefficient with the prediction error (0.620) than
DE and MCDO (between 0.447 and 0.612). Moreover, our method allows an easier
identification of OOD (Z-score of 34.05). It estimates the uncertainty
simultaneously to the regression task, therefore requires less time or
computational resources.
</p>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19687" title="Abstract">arXiv:2310.19687</a> [<a href="/pdf/2310.19687" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentiment Analysis in Digital Spaces: An Overview of Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ayravainen%2C+L+E+M">Laura E.M. Ayravainen</a>, 
<a href="/search/cs?searchtype=author&query=Hinds%2C+J">Joanne Hinds</a>, 
<a href="/search/cs?searchtype=author&query=Davidson%2C+B+I">Brittany I. Davidson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 4 figures, 6 tables, 3 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Sentiment analysis (SA) is commonly applied to digital textual data,
revealing insight into opinions and feelings. Many systematic reviews have
summarized existing work, but often overlook discussions of validity and
scientific practices. Here, we present an overview of reviews, synthesizing 38
systematic reviews, containing 2,275 primary studies. We devise a bespoke
quality assessment framework designed to assess the rigor and quality of
systematic review methodologies and reporting standards. Our findings show
diverse applications and methods, limited reporting rigor, and challenges over
time. We discuss how future research and practitioners can address these issues
and highlight their importance across numerous applications.
</p>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19690" title="Abstract">arXiv:2310.19690</a> [<a href="/pdf/2310.19690" title="Download PDF">pdf</a>, <a href="/format/2310.19690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Practical Non-Adversarial Distribution Alignment via Variational  Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Ziyu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Usman%2C+B">Ben Usman</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Inouye%2C+D+I">David I. Inouye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Distribution alignment can be used to learn invariant representations with
applications in fairness and robustness. Most prior works resort to adversarial
alignment methods but the resulting minimax problems are unstable and
challenging to optimize. Non-adversarial likelihood-based approaches either
require model invertibility, impose constraints on the latent prior, or lack a
generic framework for alignment. To overcome these limitations, we propose a
non-adversarial VAE-based alignment method that can be applied to any model
pipeline. We develop a set of alignment upper bounds (including a noisy bound)
that have VAE-like objectives but with a different perspective. We carefully
compare our method to prior VAE-based alignment approaches both theoretically
and empirically. Finally, we demonstrate that our novel alignment losses can
replace adversarial losses in standard invariant representation learning
pipelines without modifying the original architectures -- thereby significantly
broadening the applicability of non-adversarial alignment methods.
</p>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19691" title="Abstract">arXiv:2310.19691</a> [<a href="/pdf/2310.19691" title="Download PDF">pdf</a>, <a href="/ps/2310.19691" title="Download PostScript">ps</a>, <a href="/format/2310.19691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Context Connects Counterfactual Fairness to Robust Prediction and  Group Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anthis%2C+J+R">Jacy Reese Anthis</a>, 
<a href="/search/cs?searchtype=author&query=Veitch%2C+V">Victor Veitch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (stat.ML)

</div>
<p class="mathjax">Counterfactual fairness requires that a person would have been classified in
the same way by an AI or other algorithmic system if they had a different
protected class, such as a different race or gender. This is an intuitive
standard, as reflected in the U.S. legal system, but its use is limited because
counterfactuals cannot be directly observed in real-world data. On the other
hand, group fairness metrics (e.g., demographic parity or equalized odds) are
less intuitive but more readily observed. In this paper, we use $\textit{causal
context}$ to bridge the gaps between counterfactual fairness, robust
prediction, and group fairness. First, we motivate counterfactual fairness by
showing that there is not necessarily a fundamental trade-off between fairness
and accuracy because, under plausible conditions, the counterfactually fair
predictor is in fact accuracy-optimal in an unbiased target distribution.
Second, we develop a correspondence between the causal graph of the
data-generating process and which, if any, group fairness metrics are
equivalent to counterfactual fairness. Third, we show that in three common
fairness contexts$\unicode{x2013}$measurement error, selection on label, and
selection on predictors$\unicode{x2013}$counterfactual fairness is equivalent
to demographic parity, equalized odds, and calibration, respectively.
Counterfactual fairness can sometimes be tested by measuring relatively simple
group fairness metrics.
</p>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19692" title="Abstract">arXiv:2310.19692</a> [<a href="/pdf/2310.19692" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elimination of Static Hazards in Asynchronous Sequential Circuits using  Quantum dot Cellular Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Angshuman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+C">Chiradeep Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+A+K">Ankan Kumar Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+R">Ratna Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=De%2C+D">Debashis De</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proc. 2015 2nd International Conference on Microelectronics, Circuits and Systems (Micro 2015), Kolkata, India, 2015, vol. II, pp. 140-145
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Logic in Computer Science (cs.LO); Systems and Control (eess.SY); Quantum Physics (quant-ph)

</div>
<p class="mathjax">There is nowhere else in emerging technology, but in Quantum-dot Cellular
Automata, one can find high speed, low power operation, and high packaging
density, which deals with electrostatic interaction between electrons within a
cell. Literature survey lacks in hazards free design of QCA circuit. Hazards
create ambiguous and unpredictable output, which can be avoided. This work
considers both hazards and hazards-free asynchronous sequential circuits; both
are compared in terms of kink energy, and a better one has been proposed. The
circuit simulation has been verified in the QCADesigner tool.
</p>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19694" title="Abstract">arXiv:2310.19694</a> [<a href="/pdf/2310.19694" title="Download PDF">pdf</a>, <a href="/format/2310.19694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutional State Space Models for Long-Range Spatiotemporal Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+J+T+H">Jimmy T.H. Smith</a>, 
<a href="/search/cs?searchtype=author&query=De+Mello%2C+S">Shalini De Mello</a>, 
<a href="/search/cs?searchtype=author&query=Kautz%2C+J">Jan Kautz</a>, 
<a href="/search/cs?searchtype=author&query=Linderman%2C+S+W">Scott W. Linderman</a>, 
<a href="/search/cs?searchtype=author&query=Byeon%2C+W">Wonmin Byeon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Effectively modeling long spatiotemporal sequences is challenging due to the
need to model complex spatial correlations and long-range temporal dependencies
simultaneously. ConvLSTMs attempt to address this by updating tensor-valued
states with recurrent neural networks, but their sequential computation makes
them slow to train. In contrast, Transformers can process an entire
spatiotemporal sequence, compressed into tokens, in parallel. However, the cost
of attention scales quadratically in length, limiting their scalability to
longer sequences. Here, we address the challenges of prior methods and
introduce convolutional state space models (ConvSSM) that combine the tensor
modeling ideas of ConvLSTM with the long sequence modeling approaches of state
space methods such as S4 and S5. First, we demonstrate how parallel scans can
be applied to convolutional recurrences to achieve subquadratic parallelization
and fast autoregressive generation. We then establish an equivalence between
the dynamics of ConvSSMs and SSMs, which motivates parameterization and
initialization strategies for modeling long-range dependencies. The result is
ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling.
ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon
Moving-MNIST experiment while training 3X faster than ConvLSTM and generating
samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds
the performance of state-of-the-art methods on challenging DMLab, Minecraft and
Habitat prediction benchmarks and enables new directions for modeling long
spatiotemporal sequences.
</p>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19695" title="Abstract">arXiv:2310.19695</a> [<a href="/pdf/2310.19695" title="Download PDF">pdf</a>, <a href="/format/2310.19695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep-learning-based decomposition of overlapping-sparse images:  application at the vertex of neutrino interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alonso-Monsalve%2C+S">Sa&#xfa;l Alonso-Monsalve</a>, 
<a href="/search/cs?searchtype=author&query=Sgalaberna%2C+D">Davide Sgalaberna</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xingyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Molines%2C+A">Adrien Molines</a>, 
<a href="/search/cs?searchtype=author&query=McGrew%2C+C">Clark McGrew</a>, 
<a href="/search/cs?searchtype=author&query=Rubbia%2C+A">Andr&#xe9; Rubbia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
<p class="mathjax">Image decomposition plays a crucial role in various computer vision tasks,
enabling the analysis and manipulation of visual content at a fundamental
level. Overlapping images, which occur when multiple objects or scenes
partially occlude each other, pose unique challenges for decomposition
algorithms. The task intensifies when working with sparse images, where the
scarcity of meaningful information complicates the precise extraction of
components. This paper presents a solution that leverages the power of deep
learning to accurately extract individual objects within multi-dimensional
overlapping-sparse images, with a direct application in high-energy physics
with decomposition of overlaid elementary particles obtained from imaging
detectors. In particular, the proposed approach tackles a highly complex yet
unsolved problem: identifying and measuring independent particles at the vertex
of neutrino interactions, where one expects to observe detector images with
multiple indiscernible overlapping charged particles. By decomposing the image
of the detector activity at the vertex through deep learning, it is possible to
infer the kinematic parameters of the identified low-momentum particles - which
otherwise would remain neglected - and enhance the reconstructed energy
resolution of the neutrino event. We also present an additional step - that can
be tuned directly on detector data - combining the above method with a
fully-differentiable generative model to improve the image decomposition
further and, consequently, the resolution of the measured parameters, achieving
unprecedented results. This improvement is crucial for precisely measuring the
parameters that govern neutrino flavour oscillations and searching for
asymmetries between matter and antimatter.
</p>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19697" title="Abstract">arXiv:2310.19697</a> [<a href="/pdf/2310.19697" title="Download PDF">pdf</a>, <a href="/format/2310.19697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A nonlinear spectral core-periphery detection method for multiplex  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bergermann%2C+K">Kai Bergermann</a>, 
<a href="/search/math?searchtype=author&query=Stoll%2C+M">Martin Stoll</a>, 
<a href="/search/math?searchtype=author&query=Tudisco%2C+F">Francesco Tudisco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Core-periphery detection aims to separate the nodes of a complex network into
two subsets: a core that is densely connected to the entire network and a
periphery that is densely connected to the core but sparsely connected
internally. The definition of core-periphery structure in multiplex networks
that record different types of interactions between the same set of nodes but
on different layers is nontrivial since a node may belong to the core in some
layers and to the periphery in others. The current state-of-the-art approach
relies on linear combinations of individual layer degree vectors whose layer
weights need to be chosen a-priori. We propose a nonlinear spectral method for
multiplex networks that simultaneously optimizes a node and a layer coreness
vector by maximizing a suitable nonconvex homogeneous objective function by an
alternating fixed point iteration. We prove global optimality and convergence
guarantees for admissible hyper-parameter choices and convergence to local
optima for the remaining cases. We derive a quantitative measure for the
quality of a given multiplex core-periphery structure that allows the
determination of the optimal core size. Numerical experiments on synthetic and
real-world networks illustrate that our approach is robust against noisy layers
and outperforms baseline methods with respect to a variety of core-periphery
quality measures. In particular, all methods based on layer aggregation are
improved when used in combination with the novel optimized layer coreness
vector weights. As the runtime of our method depends linearly on the number of
edges of the network it is scalable to large-scale multiplex networks.
</p>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19698" title="Abstract">arXiv:2310.19698</a> [<a href="/pdf/2310.19698" title="Download PDF">pdf</a>, <a href="/format/2310.19698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and  Limitations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrov%2C+A">Aleksandar Petrov</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Bibi%2C+A">Adel Bibi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Context-based fine-tuning methods, including prompting, in-context learning,
soft prompting (also known as prompt tuning), and prefix-tuning, have gained
popularity due to their ability to often match the performance of full
fine-tuning with a fraction of the parameters. Despite their empirical
successes, there is little theoretical understanding of how these techniques
influence the internal computation of the model and their expressiveness
limitations. We show that despite the continuous embedding space being more
expressive than the discrete token space, soft-prompting and prefix-tuning are
strictly less expressive than full fine-tuning, even with the same number of
learnable parameters. Concretely, context-based fine-tuning cannot change the
relative attention pattern over the content and can only bias the outputs of an
attention layer in a fixed direction. This suggests that while techniques like
prompting, in-context learning, soft prompting, and prefix-tuning can
effectively elicit skills present in the pretrained model, they cannot learn
novel tasks that require new attention patterns.
</p>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19699" title="Abstract">arXiv:2310.19699</a> [<a href="/pdf/2310.19699" title="Download PDF">pdf</a>, <a href="/format/2310.19699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Logical Execution Time Model for Both Determinism and Low  Latency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Sen Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+D">Dong Li</a>, 
<a href="/search/eess?searchtype=author&query=Sifat%2C+A+H">Ashrarul H. Sifat</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+S">Shao-Yu Huang</a>, 
<a href="/search/eess?searchtype=author&query=Deng%2C+X">Xuanliang Deng</a>, 
<a href="/search/eess?searchtype=author&query=Jung%2C+C">Changhee Jung</a>, 
<a href="/search/eess?searchtype=author&query=Williams%2C+R">Ryan Williams</a>, 
<a href="/search/eess?searchtype=author&query=Zeng%2C+H">Haibo Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Operating Systems (cs.OS); Symbolic Computation (cs.SC)

</div>
<p class="mathjax">The Logical Execution Time (LET) programming model has recently received
considerable attention, particularly because of its timing and dataflow
determinism. In LET, task computation appears always to take the same amount of
time (called the task's LET interval), and the task reads (resp. writes) at the
beginning (resp. end) of the interval. Compared to other communication
mechanisms, such as implicit communication and Dynamic Buffer Protocol (DBP),
LET performs worse on many metrics, such as end-to-end latency (including
reaction time and data age) and time disparity jitter. Compared with the
default LET setting, the flexible LET (fLET) model shrinks the LET interval
while still guaranteeing schedulability by introducing the virtual offset to
defer the read operation and using the virtual deadline to move up the write
operation. Therefore, fLET has the potential to significantly improve the
end-to-end timing performance while keeping the benefits of deterministic
behavior on timing and dataflow.
<br />To fully realize the potential of fLET, we consider the problem of optimizing
the assignments of its virtual offsets and deadlines. We propose new
abstractions to describe the task communication pattern and new optimization
algorithms to explore the solution space efficiently. The algorithms leverage
the linearizability of communication patterns and utilize symbolic operations
to achieve efficient optimization while providing a theoretical guarantee. The
framework supports optimizing multiple performance metrics and guarantees
bounded suboptimality when optimizing end-to-end latency. Experimental results
show that our optimization algorithms improve upon the default LET and its
existing extensions and significantly outperform implicit communication and DBP
in terms of various metrics, such as end-to-end latency, time disparity, and
its jitter.
</p>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19702" title="Abstract">arXiv:2310.19702</a> [<a href="/pdf/2310.19702" title="Download PDF">pdf</a>, <a href="/format/2310.19702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rank and Select on Degenerate Strings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bille%2C+P">Philip Bille</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B8rtz%2C+I+L">Inge Li G&#xf8;rtz</a>, 
<a href="/search/cs?searchtype=author&query=Stordalen%2C+T">Tord Stordalen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">A 'degenerate string' is a sequence of subsets of some alphabet; it
represents any string obtainable by selecting one character from each set.
Recently, Alanko et al. generalized the rank-select problem to degenerate
strings, where given a character 'c' and position 'i' the goal is to find
either the 'i'th set containing 'c' or the number of occurrences of 'c' in the
first 'i' sets [SEA 2023]. The problem has applications to pangenomics; in
another work by Alanko et al. they use it as the basis for a compact
representation of 'de Bruijn Graphs' that supports fast membership queries. In
this paper we revisit the rank-select problem on degenerate strings, providing
reductions to rank-select on regular strings. Plugging in standard data
structures, we improve the time bounds for queries exponentially while
essentially matching, or improving, the space bounds. Furthermore, we provide a
lower bound on space that shows that our reductions lead to succinct data
structures in a wide range of cases. Finally, we implement our data structures,
which match the space of the most compact data structure of Alanko et al. while
answering queries five to seven times faster.
</p>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19704" title="Abstract">arXiv:2310.19704</a> [<a href="/pdf/2310.19704" title="Download PDF">pdf</a>, <a href="/format/2310.19704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Knowledge Editing of Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazzia%2C+V">Vittorio Mazzia</a>, 
<a href="/search/cs?searchtype=author&query=Pedrani%2C+A">Alessandro Pedrani</a>, 
<a href="/search/cs?searchtype=author&query=Caciolai%2C+A">Andrea Caciolai</a>, 
<a href="/search/cs?searchtype=author&query=Rottmann%2C+K">Kay Rottmann</a>, 
<a href="/search/cs?searchtype=author&query=Bernardi%2C+D">Davide Bernardi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep neural networks are becoming increasingly pervasive in academia and
industry, matching and surpassing human performance on a wide variety of fields
and related tasks. However, just as humans, even the largest artificial neural
networks make mistakes, and once-correct predictions can become invalid as the
world progresses in time. Augmenting datasets with samples that account for
mistakes or up-to-date information has become a common workaround in practical
applications. However, the well-known phenomenon of catastrophic forgetting
poses a challenge in achieving precise changes in the implicitly memorized
knowledge of neural network parameters, often requiring a full model
re-training to achieve desired behaviors. That is expensive, unreliable, and
incompatible with the current trend of large self-supervised pre-training,
making it necessary to find more efficient and effective methods for adapting
neural network models to changing data. To address this need, knowledge editing
is emerging as a novel area of research that aims to enable reliable,
data-efficient, and fast changes to a pre-trained target model, without
affecting model behaviors on previously learned tasks. In this survey, we
provide a brief review of this recent artificial intelligence field of
research. We first introduce the problem of editing neural networks, formalize
it in a common framework and differentiate it from more notorious branches of
research such as continuous learning. Next, we provide a review of the most
relevant knowledge editing approaches and datasets proposed so far, grouping
works under four different families: regularization techniques, meta-learning,
direct model editing, and architectural strategies. Finally, we outline some
intersections with other fields of research and potential directions for future
works.
</p>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19708" title="Abstract">arXiv:2310.19708</a> [<a href="/pdf/2310.19708" title="Download PDF">pdf</a>, <a href="/format/2310.19708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Language Models For Specialized Domains: A Colorful Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eitan%2C+D">Daniel Eitan</a>, 
<a href="/search/cs?searchtype=author&query=Pirchi%2C+M">Menachem Pirchi</a>, 
<a href="/search/cs?searchtype=author&query=Glazer%2C+N">Neta Glazer</a>, 
<a href="/search/cs?searchtype=author&query=Meital%2C+S">Shai Meital</a>, 
<a href="/search/cs?searchtype=author&query=Ayach%2C+G">Gil Ayach</a>, 
<a href="/search/cs?searchtype=author&query=Shamsian%2C+A">Aviv Shamsian</a>, 
<a href="/search/cs?searchtype=author&query=Navon%2C+A">Aviv Navon</a>, 
<a href="/search/cs?searchtype=author&query=Hetz%2C+G">Gil Hetz</a>, 
<a href="/search/cs?searchtype=author&query=Keshet%2C+J">Joseph Keshet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">General purpose language models (LMs) encounter difficulties when processing
domain-specific jargon and terminology, which are frequently utilized in
specialized fields such as medicine or industrial settings. Moreover, they
often find it challenging to interpret mixed speech that blends general
language with specialized jargon. This poses a challenge for automatic speech
recognition systems operating within these specific domains. In this work, we
introduce a novel approach that integrates domain-specific or secondary LM into
general-purpose LM. This strategy involves labeling, or ``coloring'', each word
to indicate its association with either the general or the domain-specific LM.
We develop an optimized algorithm that enhances the beam search algorithm to
effectively handle inferences involving colored words. Our evaluations indicate
that this approach is highly effective in integrating jargon into language
tasks. Notably, our method substantially lowers the error rate for
domain-specific words without compromising performance in the general domain.
</p>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19710" title="Abstract">arXiv:2310.19710</a> [<a href="/pdf/2310.19710" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity of the Online Distrust Ecosystem and its Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Illari%2C+L">Lucia Illari</a>, 
<a href="/search/cs?searchtype=author&query=Restrepo%2C+N+J">Nicholas J. Restrepo</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+N+F">Neil F. Johnson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Collective human distrust (and its associated mis-disinformation) is one of
the most complex phenomena of our time. e.g. distrust of medical expertise, or
climate change science, or democratic election outcomes, and even distrust of
fact-checked events in the current Israel-Hamas and Ukraine-Russia conflicts.
So what makes the online distrust ecosystem so resilient? How has it evolved
during and since the pandemic? And how well have Facebook mitigation policies
worked during this time period? We analyze a Facebook network of interconnected
in-built communities (Facebook pages) totaling roughly 100 million users who
pre-pandemic were just focused on distrust of vaccines. Mapping out this
dynamical network from 2019 to 2023, we show that it has quickly self-healed in
the wake of Facebook's mitigation campaigns which include shutdowns. This
confirms and extends our earlier finding that Facebook's ramp-ups during COVID
were ineffective (e.g. November 2020). Our findings show that future
interventions must be chosen to resonate across multiple topics and across
multiple geographical scales. Unlike many recent studies, our findings do not
rely on third-party black-box tools whose accuracy for rigorous scientific
research is unproven, hence raising doubts about such studies' conclusions, nor
is our network built using fleeting hyperlink mentions which have questionable
relevance.
</p>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19717" title="Abstract">arXiv:2310.19717</a> [<a href="/pdf/2310.19717" title="Download PDF">pdf</a>, <a href="/format/2310.19717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Support matrix machine: A review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumari%2C+A">Anuradha Kumari</a>, 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+M">Mushir Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+R">Rupal Shah</a>, 
<a href="/search/cs?searchtype=author&query=Tanveer%2C+M">M. Tanveer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Support vector machine (SVM) is one of the most studied paradigms in the
realm of machine learning for classification and regression problems. It relies
on vectorized input data. However, a significant portion of the real-world data
exists in matrix format, which is given as input to SVM by reshaping the
matrices into vectors. The process of reshaping disrupts the spatial
correlations inherent in the matrix data. Also, converting matrices into
vectors results in input data with a high dimensionality, which introduces
significant computational complexity. To overcome these issues in classifying
matrix input data, support matrix machine (SMM) is proposed. It represents one
of the emerging methodologies tailored for handling matrix input data. The SMM
method preserves the structural information of the matrix data by using the
spectral elastic net property which is a combination of the nuclear norm and
Frobenius norm. This article provides the first in-depth analysis of the
development of the SMM model, which can be used as a thorough summary by both
novices and experts. We discuss numerous SMM variants, such as robust, sparse,
class imbalance, and multi-class classification models. We also analyze the
applications of the SMM model and conclude the article by outlining potential
future research avenues and possibilities that may motivate academics to
advance the SMM algorithm.
</p>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19726" title="Abstract">arXiv:2310.19726</a> [<a href="/pdf/2310.19726" title="Download PDF">pdf</a>, <a href="/format/2310.19726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Path to Simpler Models Starts With Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Semenova%2C+L">Lesia Semenova</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Harry Chen</a>, 
<a href="/search/cs?searchtype=author&query=Parr%2C+R">Ronald Parr</a>, 
<a href="/search/cs?searchtype=author&query=Rudin%2C+C">Cynthia Rudin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">The Rashomon set is the set of models that perform approximately equally well
on a given dataset, and the Rashomon ratio is the fraction of all models in a
given hypothesis space that are in the Rashomon set. Rashomon ratios are often
large for tabular datasets in criminal justice, healthcare, lending, education,
and in other areas, which has practical implications about whether simpler
models can attain the same level of accuracy as more complex models. An open
question is why Rashomon ratios often tend to be large. In this work, we
propose and study a mechanism of the data generation process, coupled with
choices usually made by the analyst during the learning process, that
determines the size of the Rashomon ratio. Specifically, we demonstrate that
noisier datasets lead to larger Rashomon ratios through the way that
practitioners train models. Additionally, we introduce a measure called pattern
diversity, which captures the average difference in predictions between
distinct classification patterns in the Rashomon set, and motivate why it tends
to increase with label noise. Our results explain a key aspect of why simpler
models often tend to perform as well as black box models on complex, noisier
datasets.
</p>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19727" title="Abstract">arXiv:2310.19727</a> [<a href="/pdf/2310.19727" title="Download PDF">pdf</a>, <a href="/format/2310.19727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Medical Instructions with Conditional Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belkadi%2C+S">Samuel Belkadi</a>, 
<a href="/search/cs?searchtype=author&query=Micheletti%2C+N">Nicolo Micheletti</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lifeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Del-Pinto%2C+W">Warren Del-Pinto</a>, 
<a href="/search/cs?searchtype=author&query=Nenadic%2C+G">Goran Nenadic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to: Workshop on Synthetic Data Generation with Generative AI (SyntheticData4ML Workshop) at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Access to real-world medical instructions is essential for medical research
and healthcare quality improvement. However, access to real medical
instructions is often limited due to the sensitive nature of the information
expressed. Additionally, manually labelling these instructions for training and
fine-tuning Natural Language Processing (NLP) models can be tedious and
expensive. We introduce a novel task-specific model architecture,
Label-To-Text-Transformer (\textbf{LT3}), tailored to generate synthetic
medical instructions based on provided labels, such as a vocabulary list of
medications and their attributes. LT3 is trained on a vast corpus of medical
instructions extracted from the MIMIC-III database, allowing the model to
produce valuable synthetic medical instructions. We evaluate LT3's performance
by contrasting it with a state-of-the-art Pre-trained Language Model (PLM), T5,
analysing the quality and diversity of generated texts. We deploy the generated
synthetic data to train the SpacyNER model for the Named Entity Recognition
(NER) task over the n2c2-2018 dataset. The experiments show that the model
trained on synthetic data can achieve a 96-98\% F1 score at Label Recognition
on Drug, Frequency, Route, Strength, and Form. LT3 codes and data will be
shared at \url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}
</p>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19731" title="Abstract">arXiv:2310.19731</a> [<a href="/pdf/2310.19731" title="Download PDF">pdf</a>, <a href="/format/2310.19731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViR: Vision Retention Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hatamizadeh%2C+A">Ali Hatamizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Ranzinger%2C+M">Michael Ranzinger</a>, 
<a href="/search/cs?searchtype=author&query=Kautz%2C+J">Jan Kautz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision Transformers (ViTs) have attracted a lot of popularity in recent
years, due to their exceptional capabilities in modeling long-range spatial
dependencies and scalability for large scale training. Although the training
parallelism of self-attention mechanism plays an important role in retaining
great performance, its quadratic complexity baffles the application of ViTs in
many scenarios which demand fast inference. This effect is even more pronounced
in applications in which autoregressive modeling of input features is required.
In Natural Language Processing (NLP), a new stream of efforts have proposed
parallelizable models with recurrent formulation that allows for efficient
inference in generative applications. Inspired by this trend, we propose a new
class of computer vision models, dubbed Vision Retention Networks (ViR), with
dual parallel and recurrent formulations, which strike an optimal balance
between fast inference and parallel training with competitive performance. In
particular, ViR scales favorably for image throughput and memory consumption in
tasks that require higher-resolution images due to its flexible formulation in
processing large sequence lengths. The ViR is the first attempt to realize dual
parallel and recurrent equivalency in a general vision backbone for recognition
tasks. We have validated the effectiveness of ViR through extensive experiments
with different dataset sizes and various image resolutions and achieved
competitive performance. Our code and pretrained models will be made publicly
available.
</p>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19733" title="Abstract">arXiv:2310.19733</a> [<a href="/pdf/2310.19733" title="Download PDF">pdf</a>, <a href="/format/2310.19733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Reward Estimation with Preference Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+R">Sayak Ray Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xingyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Natarajan%2C+N">Nagarajan Natarajan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Learning from preference-based feedback has recently gained considerable
traction as a promising approach to align generative models with human
interests. Instead of relying on numerical rewards, the generative models are
trained using reinforcement learning with human feedback (RLHF). These
approaches first solicit feedback from human labelers typically in the form of
pairwise comparisons between two possible actions, then estimate a reward model
using these comparisons, and finally employ a policy based on the estimated
reward model. An adversarial attack in any step of the above pipeline might
reveal private and sensitive information of human labelers. In this work, we
adopt the notion of label differential privacy (DP) and focus on the problem of
reward estimation from preference-based feedback while protecting privacy of
each individual labelers. Specifically, we consider the parametric
Bradley-Terry-Luce (BTL) model for such pairwise comparison feedback involving
a latent reward parameter $\theta^* \in \mathbb{R}^d$. Within a standard
minimax estimation framework, we provide tight upper and lower bounds on the
error in estimating $\theta^*$ under both local and central models of DP. We
show, for a given privacy budget $\epsilon$ and number of samples $n$, that the
additional cost to ensure label-DP under local model is $\Theta \big(\frac{1}{
e^\epsilon-1}\sqrt{\frac{d}{n}}\big)$, while it is
$\Theta\big(\frac{\text{poly}(d)}{\epsilon n} \big)$ under the weaker central
model. We perform simulations on synthetic data that corroborate these
theoretical results.
</p>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19736" title="Abstract">arXiv:2310.19736</a> [<a href="/pdf/2310.19736" title="Download PDF">pdf</a>, <a href="/format/2310.19736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Large Language Models: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zishan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Renren Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+D">Dan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Supryadi">Supryadi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Linhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+B">Bojian Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+D">Deyi Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated remarkable capabilities across
a broad spectrum of tasks. They have attracted significant attention and been
deployed in numerous downstream applications. Nevertheless, akin to a
double-edged sword, LLMs also present potential risks. They could suffer from
private data leaks or yield inappropriate, harmful, or misleading content.
Additionally, the rapid progress of LLMs raises concerns about the potential
emergence of superintelligent systems without adequate safeguards. To
effectively capitalize on LLM capacities as well as ensure their safe and
beneficial development, it is critical to conduct a rigorous and comprehensive
evaluation of LLMs.
<br />This survey endeavors to offer a panoramic perspective on the evaluation of
LLMs. We categorize the evaluation of LLMs into three major groups: knowledge
and capability evaluation, alignment evaluation and safety evaluation. In
addition to the comprehensive review on the evaluation methodologies and
benchmarks on these three aspects, we collate a compendium of evaluations
pertaining to LLMs' performance in specialized domains, and discuss the
construction of comprehensive evaluation platforms that cover LLM evaluations
on capabilities, alignment, safety, and applicability.
<br />We hope that this comprehensive overview will stimulate further research
interests in the evaluation of LLMs, with the ultimate goal of making
evaluation serve as a cornerstone in guiding the responsible development of
LLMs. We envision that this will channel their evolution into a direction that
maximizes societal benefit while minimizing potential risks. A curated list of
related papers has been publicly available at
https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.
</p>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19737" title="Abstract">arXiv:2310.19737</a> [<a href="/pdf/2310.19737" title="Download PDF">pdf</a>, <a href="/format/2310.19737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Attacks and Defenses in Large Language Models: Old and New  Threats
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schwinn%2C+L">Leo Schwinn</a>, 
<a href="/search/cs?searchtype=author&query=Dobre%2C+D">David Dobre</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnnemann%2C+S">Stephan G&#xfc;nnemann</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Over the past decade, there has been extensive research aimed at enhancing
the robustness of neural networks, yet this problem remains vastly unsolved.
Here, one major impediment has been the overestimation of the robustness of new
defense approaches due to faulty defense evaluations. Flawed robustness
evaluations necessitate rectifications in subsequent works, dangerously slowing
down the research and providing a false sense of security. In this context, we
will face substantial challenges associated with an impending adversarial arms
race in natural language processing, specifically with closed-source Large
Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We
provide a first set of prerequisites to improve the robustness assessment of
new approaches and reduce the amount of faulty evaluations. Additionally, we
identify embedding space attacks on LLMs as another viable threat model for the
purposes of generating malicious content in open-sourced models. Finally, we
demonstrate on a recently proposed defense that, without LLM-specific best
practices in place, it is easy to overestimate the robustness of a new
approach.
</p>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19740" title="Abstract">arXiv:2310.19740</a> [<a href="/pdf/2310.19740" title="Download PDF">pdf</a>, <a href="/format/2310.19740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Evaluation: Exploring the Synergy of Large Language Models  and Humans for Open-ended Generation Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qintong Li</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Leyang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We release our resources at \url{<a href="https://github.com/qtli/CoEval">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Humans are widely involved in the evaluation of open-ended natural language
generation tasks (NLG) that demand creativity, as automatic metrics often
exhibit weak correlations with human judgments. Large language models (LLMs)
recently have emerged as a scalable and cost-effective alternative to human
evaluations. However, both humans and LLMs have limitations, i.e., inherent
subjectivity and unreliable judgments, particularly for open-ended tasks that
require adaptable metrics tailored to diverse task requirements. To explore the
synergy between humans and LLM-based evaluators and address the challenges of
existing inconsistent evaluation criteria in open-ended NLG tasks, we propose a
Collaborative Evaluation pipeline CoEval, involving the design of a checklist
of task-specific criteria and the detailed evaluation of texts, in which LLM
generates initial ideation, and then humans engage in scrutiny. We conducted a
series of experiments to investigate the mutual effects between LLMs and humans
in CoEval. Results show that, by utilizing LLMs, CoEval effectively evaluates
lengthy texts, saving significant time and reducing human evaluation outliers.
Human scrutiny still plays a role, revising around 20% of LLM evaluation scores
for ultimate reliability.
</p>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19743" title="Abstract">arXiv:2310.19743</a> [<a href="/pdf/2310.19743" title="Download PDF">pdf</a>, <a href="/format/2310.19743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tell Me What Is Good About This Property: Leveraging Reviews For  Segment-Personalized Image Collection Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wysoczanska%2C+M">Monika Wysoczanska</a>, 
<a href="/search/cs?searchtype=author&query=Beladev%2C+M">Moran Beladev</a>, 
<a href="/search/cs?searchtype=author&query=Assaraf%2C+K+L">Karen Lastmann Assaraf</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fengjun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kleinfeld%2C+O">Ofri Kleinfeld</a>, 
<a href="/search/cs?searchtype=author&query=Amsalem%2C+G">Gil Amsalem</a>, 
<a href="/search/cs?searchtype=author&query=Boker%2C+H+H">Hadas Harush Boker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Image collection summarization techniques aim to present a compact
representation of an image gallery through a carefully selected subset of
images that captures its semantic content. When it comes to web content,
however, the ideal selection can vary based on the user's specific intentions
and preferences. This is particularly relevant at Booking.com, where presenting
properties and their visual summaries that align with users' expectations is
crucial. To address this challenge, we consider user intentions in the
summarization of property visuals by analyzing property reviews and extracting
the most significant aspects mentioned by users. By incorporating the insights
from reviews in our visual summaries, we enhance the summaries by presenting
the relevant content to a user. Moreover, we achieve it without the need for
costly annotations. Our experiments, including human perceptual studies,
demonstrate the superiority of our cross-modal approach, which we coin as
CrossSummarizer over the no-personalization and image-based clustering
baselines.
</p>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19750" title="Abstract">arXiv:2310.19750</a> [<a href="/pdf/2310.19750" title="Download PDF">pdf</a>, <a href="/format/2310.19750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Thought Embeddings for Stance Detection on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gatto%2C+J">Joseph Gatto</a>, 
<a href="/search/cs?searchtype=author&query=Sharif%2C+O">Omar Sharif</a>, 
<a href="/search/cs?searchtype=author&query=Preum%2C+S+M">Sarah Masud Preum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP-2023, 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Stance detection on social media is challenging for Large Language Models
(LLMs), as emerging slang and colloquial language in online conversations often
contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has
recently been shown to improve performance on stance detection tasks --
alleviating some of these issues. However, COT prompting still struggles with
implicit stance identification. This challenge arises because many samples are
initially challenging to comprehend before a model becomes familiar with the
slang and evolving knowledge related to different topics, all of which need to
be acquired through the training data. In this study, we address this problem
by introducing COT Embeddings which improve COT performance on stance detection
tasks by embedding COT reasonings and integrating them into a traditional
RoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text
encoders can leverage COT reasonings with minor errors or hallucinations that
would otherwise distort the COT output label. 2) Text encoders can overlook
misleading COT reasoning when a sample's prediction heavily depends on
domain-specific patterns. Our model achieves SOTA performance on multiple
stance detection datasets collected from social media.
</p>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19752" title="Abstract">arXiv:2310.19752</a> [<a href="/pdf/2310.19752" title="Download PDF">pdf</a>, <a href="/format/2310.19752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+Q">Qi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuanhong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Juhua Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by NeurIPS'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision-language pre-training methods, e.g., CLIP, demonstrate an impressive
zero-shot performance on visual categorizations with the class proxy from the
text embedding of the class name. However, the modality gap between the text
and vision space can result in a sub-optimal performance. We theoretically show
that the gap cannot be reduced sufficiently by minimizing the contrastive loss
in CLIP and the optimal proxy for vision tasks may reside only in the vision
space. Therefore, given unlabeled target vision data, we propose to learn the
vision proxy directly with the help from the text proxy for zero-shot transfer.
Moreover, according to our theoretical analysis, strategies are developed to
further refine the pseudo label obtained by the text proxy to facilitate the
intra-modal proxy learning (InMaP) for vision. Experiments on extensive
downstream tasks confirm the effectiveness and efficiency of our proposal.
Concretely, InMaP can obtain the vision proxy within one minute on a single GPU
while improving the zero-shot accuracy from $77.02\%$ to $80.21\%$ on ImageNet
with ViT-L/14@336 pre-trained by CLIP. Code is available at
\url{https://github.com/idstcv/InMaP}.
</p>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19754" title="Abstract">arXiv:2310.19754</a> [<a href="/pdf/2310.19754" title="Download PDF">pdf</a>, <a href="/format/2310.19754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rail journey cost calculator for Great Britain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Botta%2C+F">Federico Botta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Accessibility of different places, such as hospitals or areas with jobs, is
important in understanding transportation systems, urban environments, and
potential inequalities in what services and opportunities different people can
reach. Often, research in this area is framed around the question of whether
people living in an area are able to reach certain destinations within a
prespecified time frame. However, the cost of such journeys, and whether they
are affordable, is often omitted or not considered to the same level. Here, we
present a Python package and an associated data set which allows to analyse the
cost of train journeys in Great Britain. We present the original data set we
used to construct this, the Python package we developed to analyse it, and the
output data set which we generated. We envisage our work to allow researchers,
policy makers, and other stakeholders, to investigate questions around the cost
of train journeys, any geographical or social inequalities arising from this,
and how the transport system could be improved.
</p>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19758" title="Abstract">arXiv:2310.19758</a> [<a href="/pdf/2310.19758" title="Download PDF">pdf</a>, <a href="/ps/2310.19758" title="Download PostScript">ps</a>, <a href="/format/2310.19758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypocoercivity for Linear ODEs and Strong Stability for Runge--Kutta  Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Achleitner%2C+F">Franz Achleitner</a>, 
<a href="/search/math?searchtype=author&query=Arnold%2C+A">Anton Arnold</a>, 
<a href="/search/math?searchtype=author&query=J%C3%BCngel%2C+A">Ansgar J&#xfc;ngel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this note, we connect two different topics from linear algebra and
numerical analysis: hypocoercivity of semi-dissipative matrices and strong
stability for explicit Runge--Kutta schemes. Linear autonomous ODE systems with
a non-coercive matrix are called hypocoercive if they still exhibit uniform
exponential decay towards the steady state. Strong stability is a property of
time-integration schemes for ODEs that preserve the temporal monotonicity of
the discrete solutions. It is proved that explicit Runge--Kutta schemes are
strongly stable with respect to semi-dissipative, asymptotically stable
matrices if the hypocoercivity index is sufficiently small compared to the
order of the scheme. Otherwise, the Runge--Kutta schemes are in general not
strongly stable. As a corollary, explicit Runge--Kutta schemes of order $p\in
4\N$ with $s=p$ stages turn out to be \emph{not} strongly stable. This result
was proved in \cite{AAJ23}, filling a gap left open in \cite{SunShu19}. Here,
we present an alternative, direct proof.
</p>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19759" title="Abstract">arXiv:2310.19759</a> [<a href="/pdf/2310.19759" title="Download PDF">pdf</a>, <a href="/ps/2310.19759" title="Download PostScript">ps</a>, <a href="/format/2310.19759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-player Domino games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Menibus%2C+B+H">Benjamin Hellouin de Menibus</a>, 
<a href="/search/cs?searchtype=author&query=Pallen%2C+R">R&#xe9;mi Pallen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Computability in Europe 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We introduce a 2-player game played on an infinite grid, initially empty,
where each player in turn chooses a vertex and colours it. The first player
aims to create some pattern from a target set, while the second player aims to
prevent it.
<br />We study the problem of deciding which player wins, and prove that it is
undecidable. We also consider a variant where the turn order is not alternating
but given by a balanced word, and we characterise the decidable and undecidable
cases.
</p>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19760" title="Abstract">arXiv:2310.19760</a> [<a href="/pdf/2310.19760" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epidemic outbreak prediction using machine learning models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pramod%2C+A">Akshara Pramod</a>, 
<a href="/search/cs?searchtype=author&query=Abhishek%2C+J">JS Abhishek</a>, 
<a href="/search/cs?searchtype=author&query=K%2C+D+S">Dr. Suganthi K</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 tables, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI); Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">In today's world,the risk of emerging and re-emerging epidemics have
increased.The recent advancement in healthcare technology has made it possible
to predict an epidemic outbreak in a region.Early prediction of an epidemic
outbreak greatly helps the authorities to be prepared with the necessary
medications and logistics required to keep things in control. In this article,
we try to predict the epidemic outbreak (influenza, hepatitis and malaria) for
the state of New York, USA using machine and deep learning algorithms, and a
portal has been created for the same which can alert the authorities and health
care organizations of the region in case of an outbreak. The algorithm takes
historical data to predict the possible number of cases for 5 weeks into the
future. Non-clinical factors like google search trends,social media data and
weather data have also been used to predict the probability of an outbreak.
</p>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19763" title="Abstract">arXiv:2310.19763</a> [<a href="/pdf/2310.19763" title="Download PDF">pdf</a>, <a href="/format/2310.19763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autoregressive Renaissance in Neural PDE Solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+Y+R">Yolanne Yi Ran Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented as a workshop poster at ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Recent developments in the field of neural partial differential equation
(PDE) solvers have placed a strong emphasis on neural operators. However, the
paper "Message Passing Neural PDE Solver" by Brandstetter et al. published in
ICLR 2022 revisits autoregressive models and designs a message passing graph
neural network that is comparable with or outperforms both the state-of-the-art
Fourier Neural Operator and traditional classical PDE solvers in its
generalization capabilities and performance. This blog post delves into the key
contributions of this work, exploring the strategies used to address the common
problem of instability in autoregressive models and the design choices of the
message passing graph neural network architecture.
</p>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19773" title="Abstract">arXiv:2310.19773</a> [<a href="/pdf/2310.19773" title="Download PDF">pdf</a>, <a href="/format/2310.19773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MM-VID: Advancing Video Understanding with GPT-4V(ision)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kevin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Faisal Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chung-Ching Lin</a>, 
<a href="/search/cs?searchtype=author&query=Azarnasab%2C+E">Ehsan Azarnasab</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Lin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yumao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Ce Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page at <a href="https://multimodal-vid.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present MM-VID, an integrated system that harnesses the capabilities of
GPT-4V, combined with specialized tools in vision, audio, and speech, to
facilitate advanced video understanding. MM-VID is designed to address the
challenges posed by long-form videos and intricate tasks such as reasoning
within hour-long content and grasping storylines spanning multiple episodes.
MM-VID uses a video-to-script generation with GPT-4V to transcribe multimodal
elements into a long textual script. The generated script details character
movements, actions, expressions, and dialogues, paving the way for large
language models (LLMs) to achieve video understanding. This enables advanced
capabilities, including audio description, character identification, and
multimodal high-level comprehension. Experimental results demonstrate the
effectiveness of MM-VID in handling distinct video genres with various video
lengths. Additionally, we showcase its potential when applied to interactive
environments, such as video games and graphic user interfaces.
</p>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19775" title="Abstract">arXiv:2310.19775</a> [<a href="/pdf/2310.19775" title="Download PDF">pdf</a>, <a href="/format/2310.19775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open  Challenges and Interdisciplinary Research Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Longo%2C+L">Luca Longo</a>, 
<a href="/search/cs?searchtype=author&query=Brcic%2C+M">Mario Brcic</a>, 
<a href="/search/cs?searchtype=author&query=Cabitza%2C+F">Federico Cabitza</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaesik Choi</a>, 
<a href="/search/cs?searchtype=author&query=Confalonieri%2C+R">Roberto Confalonieri</a>, 
<a href="/search/cs?searchtype=author&query=Del+Ser%2C+J">Javier Del Ser</a>, 
<a href="/search/cs?searchtype=author&query=Guidotti%2C+R">Riccardo Guidotti</a>, 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+Y">Yoichi Hayashi</a>, 
<a href="/search/cs?searchtype=author&query=Herrera%2C+F">Francisco Herrera</a>, 
<a href="/search/cs?searchtype=author&query=Holzinger%2C+A">Andreas Holzinger</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Richard Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Khosravi%2C+H">Hassan Khosravi</a>, 
<a href="/search/cs?searchtype=author&query=Lecue%2C+F">Freddy Lecue</a>, 
<a href="/search/cs?searchtype=author&query=Malgieri%2C+G">Gianclaudio Malgieri</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A1ez%2C+A">Andr&#xe9;s P&#xe1;ez</a>, 
<a href="/search/cs?searchtype=author&query=Samek%2C+W">Wojciech Samek</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Johannes Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Speith%2C+T">Timo Speith</a>, 
<a href="/search/cs?searchtype=author&query=Stumpf%2C+S">Simone Stumpf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">As systems based on opaque Artificial Intelligence (AI) continue to flourish
in diverse real-world applications, understanding these black box models has
become paramount. In response, Explainable AI (XAI) has emerged as a field of
research with practical and ethical benefits across various domains. This paper
not only highlights the advancements in XAI and its application in real-world
scenarios but also addresses the ongoing challenges within XAI, emphasizing the
need for broader perspectives and collaborative efforts. We bring together
experts from diverse fields to identify open problems, striving to synchronize
research agendas and accelerate XAI in practical applications. By fostering
collaborative discussion and interdisciplinary cooperation, we aim to propel
XAI forward, contributing to its continued success. Our goal is to put forward
a comprehensive proposal for advancing XAI. To achieve this goal, we present a
manifesto of 27 open problems categorized into nine categories. These
challenges encapsulate the complexities and nuances of XAI and offer a road map
for future research. For each problem, we provide promising research directions
in the hope of harnessing the collective intelligence of interested
stakeholders.
</p>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19776" title="Abstract">arXiv:2310.19776</a> [<a href="/pdf/2310.19776" title="Download PDF">pdf</a>, <a href="/format/2310.19776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learn to Categorize or Categorize to Learn? Self-Coding for Generalized  Category Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rastegar%2C+S">Sarah Rastegar</a>, 
<a href="/search/cs?searchtype=author&query=Doughty%2C+H">Hazel Doughty</a>, 
<a href="/search/cs?searchtype=author&query=Snoek%2C+C+G+M">Cees G. M. Snoek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the quest for unveiling novel categories at test time, we confront the
inherent limitations of traditional supervised recognition models that are
restricted by a predefined category set. While strides have been made in the
realms of self-supervised and open-world learning towards test-time category
discovery, a crucial yet often overlooked question persists: what exactly
delineates a \textit{category}? In this paper, we conceptualize a
\textit{category} through the lens of optimization, viewing it as an optimal
solution to a well-defined problem. Harnessing this unique conceptualization,
we propose a novel, efficient and self-supervised method capable of discovering
previously unknown categories at test time. A salient feature of our approach
is the assignment of minimum length category codes to individual data
instances, which encapsulates the implicit category hierarchy prevalent in
real-world datasets. This mechanism affords us enhanced control over category
granularity, thereby equipping our model to handle fine-grained categories
adeptly. Experimental evaluations, bolstered by state-of-the-art benchmark
comparisons, testify to the efficacy of our solution in managing unknown
categories at test time. Furthermore, we fortify our proposition with a
theoretical foundation, providing proof of its optimality. Our code is
available at: \url{https://github.com/SarahRastegar/InfoSieve}.
</p>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19778" title="Abstract">arXiv:2310.19778</a> [<a href="/pdf/2310.19778" title="Download PDF">pdf</a>, <a href="/format/2310.19778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing AI Support for Human Involvement in AI-assisted Decision  Making: A Taxonomy of Human-AI Interactions from a Systematic Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gomez%2C+C">Catalina Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S+M">Sue Min Cho</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chien-Ming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Unberath%2C+M">Mathias Unberath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Efforts in levering Artificial Intelligence (AI) in decision support systems
have disproportionately focused on technological advancements, often
overlooking the alignment between algorithmic outputs and human expectations.
To address this, explainable AI promotes AI development from a more
human-centered perspective. Determining what information AI should provide to
aid humans is vital, however, how the information is presented, e. g., the
sequence of recommendations and the solicitation of interpretations, is equally
crucial. This motivates the need to more precisely study Human-AI interaction
as a pivotal component of AI-based decision support. While several empirical
studies have evaluated Human-AI interactions in multiple application domains in
which interactions can take many forms, there is not yet a common vocabulary to
describe human-AI interaction protocols. To address this gap, we describe the
results of a systematic review of the AI-assisted decision making literature,
analyzing 105 selected articles, which grounds the introduction of a taxonomy
of interaction patterns that delineate various modes of human-AI interactivity.
We find that current interactions are dominated by simplistic collaboration
paradigms and report comparatively little support for truly interactive
functionality. Our taxonomy serves as a valuable tool to understand how
interactivity with AI is currently supported in decision-making contexts and
foster deliberate choices of interaction designs.
</p>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19784" title="Abstract">arXiv:2310.19784</a> [<a href="/pdf/2310.19784" title="Download PDF">pdf</a>, <a href="/format/2310.19784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CustomNet: Zero-shot Object Customization with Variable-Viewpoints in  Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Ziyang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Mingdeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zhongang Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chun Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project webpage available at <a href="https://jiangyzy.github.io/CustomNet/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Incorporating a customized object into image generation presents an
attractive feature in text-to-image generation. However, existing
optimization-based and encoder-based methods are hindered by drawbacks such as
time-consuming optimization, insufficient identity preservation, and a
prevalent copy-pasting effect. To overcome these limitations, we introduce
CustomNet, a novel object customization approach that explicitly incorporates
3D novel view synthesis capabilities into the object customization process.
This integration facilitates the adjustment of spatial position relationships
and viewpoints, yielding diverse outputs while effectively preserving object
identity. Moreover, we introduce delicate designs to enable location control
and flexible background control through textual descriptions or specific
user-defined images, overcoming the limitations of existing 3D novel view
synthesis methods. We further leverage a dataset construction pipeline that can
better handle real-world objects and complex backgrounds. Equipped with these
designs, our method facilitates zero-shot object customization without
test-time optimization, offering simultaneous control over the viewpoints,
location, and background. As a result, our CustomNet ensures enhanced identity
preservation and generates diverse, harmonious outputs.
</p>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19785" title="Abstract">arXiv:2310.19785</a> [<a href="/pdf/2310.19785" title="Download PDF">pdf</a>, <a href="/format/2310.19785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What&#x27;s &quot;up&quot; with vision-language models? Investigating their struggle  with spatial reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamath%2C+A">Amita Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent vision-language (VL) models are powerful, but can they reliably
distinguish "right" from "left"? We curate three new corpora to quantify model
comprehension of such basic spatial relations. These tests isolate spatial
reasoning more precisely than existing datasets like VQAv2, e.g., our What'sUp
benchmark contains sets of photographs varying only the spatial relations of
objects, keeping their identity fixed (see Figure 1: models must comprehend not
only the usual case of a dog under a table, but also, the same dog on top of
the same table). We evaluate 18 VL models, finding that all perform poorly,
e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56%
accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of
this surprising behavior, finding: 1) that popular vision-language pretraining
corpora like LAION-2B contain little reliable data for learning spatial
relationships; and 2) that basic modeling interventions like up-weighting
preposition-containing instances or fine-tuning on our corpora are not
sufficient to address the challenges our benchmarks pose. We are hopeful that
these corpora will facilitate further research, and we release our data and
code at https://github.com/amitakamath/whatsup_vlms.
</p>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19786" title="Abstract">arXiv:2310.19786</a> [<a href="/pdf/2310.19786" title="Download PDF">pdf</a>, <a href="/ps/2310.19786" title="Download PostScript">ps</a>, <a href="/format/2310.19786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From External to Swap Regret 2.0: An Efficient Reduction and Oblivious  Adversary for Large Action Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dagan%2C+Y">Yuval Dagan</a>, 
<a href="/search/cs?searchtype=author&query=Daskalakis%2C+C">Constantinos Daskalakis</a>, 
<a href="/search/cs?searchtype=author&query=Fishelson%2C+M">Maxwell Fishelson</a>, 
<a href="/search/cs?searchtype=author&query=Golowich%2C+N">Noah Golowich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We provide a novel reduction from swap-regret minimization to external-regret
minimization, which improves upon the classical reductions of Blum-Mansour
[BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the
space of actions. We show that, whenever there exists a no-external-regret
algorithm for some hypothesis class, there must also exist a no-swap-regret
algorithm for that same class. For the problem of learning with expert advice,
our result implies that it is possible to guarantee that the swap regret is
bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$
per iteration complexity, where $N$ is the number of experts, while the
classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$
rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes
with an associated lower bound, which -- in contrast to that in [BM07] -- holds
for oblivious and $\ell_1$-constrained adversaries and learners that can employ
distributions over experts, showing that the number of rounds must be
$\tilde\Omega(N/\epsilon^2)$ or exponential in $1/\epsilon$.
<br />Our reduction implies that, if no-regret learning is possible in some game,
then this game must have approximate correlated equilibria, of arbitrarily good
approximation. This strengthens the folklore implication of no-regret learning
that approximate coarse correlated equilibria exist. Importantly, it provides a
sufficient condition for the existence of correlated equilibrium which vastly
extends the requirement that the action set is finite, thus answering a
question left open by [DG22; Ass+23]. Moreover, it answers several outstanding
questions about equilibrium computation and/or learning in games.
</p>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19789" title="Abstract">arXiv:2310.19789</a> [<a href="/pdf/2310.19789" title="Download PDF">pdf</a>, <a href="/format/2310.19789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffEnc: Variational Diffusion with a Learned Encoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+B+M+G">Beatrix M. G. Nielsen</a>, 
<a href="/search/cs?searchtype=author&query=Christensen%2C+A">Anders Christensen</a>, 
<a href="/search/cs?searchtype=author&query=Dittadi%2C+A">Andrea Dittadi</a>, 
<a href="/search/cs?searchtype=author&query=Winther%2C+O">Ole Winther</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Diffusion models may be viewed as hierarchical variational autoencoders
(VAEs) with two improvements: parameter sharing for the conditional
distributions in the generative process and efficient computation of the loss
as independent terms over the hierarchy. We consider two changes to the
diffusion model that retain these advantages while adding flexibility to the
model. Firstly, we introduce a data- and depth-dependent mean function in the
diffusion process, which leads to a modified diffusion loss. Our proposed
framework, DiffEnc, achieves state-of-the-art likelihood on CIFAR-10. Secondly,
we let the ratio of the noise variance of the reverse encoder process and the
generative process be a free weight parameter rather than being fixed to 1.
This leads to theoretical insights: For a finite depth hierarchy, the evidence
lower bound (ELBO) can be used as an objective for a weighted diffusion loss
approach and for optimizing the noise schedule specifically for inference. For
the infinite-depth hierarchy, on the other hand, the weight parameter has to be
1 to have a well-defined ELBO.
</p>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19791" title="Abstract">arXiv:2310.19791</a> [<a href="/pdf/2310.19791" title="Download PDF">pdf</a>, <a href="/format/2310.19791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LILO: Learning Interpretable Libraries by Compressing and Documenting  Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grand%2C+G">Gabriel Grand</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+L">Lionel Wong</a>, 
<a href="/search/cs?searchtype=author&query=Bowers%2C+M">Matthew Bowers</a>, 
<a href="/search/cs?searchtype=author&query=Olausson%2C+T+X">Theo X. Olausson</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Muxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Andreas%2C+J">Jacob Andreas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
<p class="mathjax">While large language models (LLMs) now excel at code generation, a key aspect
of software development is the art of refactoring: consolidating code into
libraries of reusable and readable programs. In this paper, we introduce LILO,
a neurosymbolic framework that iteratively synthesizes, compresses, and
documents code to build libraries tailored to particular problem domains. LILO
combines LLM-guided program synthesis with recent algorithmic advances in
automated refactoring from Stitch: a symbolic compression system that
efficiently identifies optimal lambda abstractions across large code corpora.
To make these abstractions interpretable, we introduce an auto-documentation
(AutoDoc) procedure that infers natural language names and docstrings based on
contextual examples of usage. In addition to improving human readability, we
find that AutoDoc boosts performance by helping LILO's synthesizer to interpret
and deploy learned abstractions. We evaluate LILO on three inductive program
synthesis benchmarks for string editing, scene reasoning, and graphics
composition. Compared to existing neural and symbolic methods - including the
state-of-the-art library learning algorithm DreamCoder - LILO solves more
complex tasks and learns richer libraries that are grounded in linguistic
knowledge.
</p>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19792" title="Abstract">arXiv:2310.19792</a> [<a href="/pdf/2310.19792" title="Download PDF">pdf</a>, <a href="/format/2310.19792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Eval4NLP 2023 Shared Task on Prompting Large Language Models as  Explainable Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leiter%2C+C">Christoph Leiter</a>, 
<a href="/search/cs?searchtype=author&query=Opitz%2C+J">Juri Opitz</a>, 
<a href="/search/cs?searchtype=author&query=Deutsch%2C+D">Daniel Deutsch</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dror%2C+R">Rotem Dror</a>, 
<a href="/search/cs?searchtype=author&query=Eger%2C+S">Steffen Eger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With an increasing number of parameters and pre-training data, generative
large language models (LLMs) have shown remarkable capabilities to solve tasks
with minimal or no task-related examples. Notably, LLMs have been successfully
employed as evaluation metrics in text generation tasks. Within this context,
we introduce the Eval4NLP 2023 shared task that asks participants to explore
prompting and score extraction for machine translation (MT) and summarization
evaluation. Specifically, we propose a novel competition setting in which we
select a list of allowed LLMs and disallow fine-tuning to ensure a focus on
prompting. We present an overview of participants' approaches and evaluate them
on a new reference-free test set spanning three language pairs for MT and a
summarization dataset. Notably, despite the task's restrictions, the
best-performing systems achieve results on par with or even surpassing recent
reference-free metrics developed using larger models, including GEMBA and
Comet-Kiwi-XXL. Finally, as a separate track, we perform a small-scale human
evaluation of the plausibility of explanations given by the LLMs.
</p>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19795" title="Abstract">arXiv:2310.19795</a> [<a href="/pdf/2310.19795" title="Download PDF">pdf</a>, <a href="/format/2310.19795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimMMDG: A Simple and Effective Framework for Multi-modal Domain  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Nejjar%2C+I">Ismail Nejjar</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Han Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chatzi%2C+E">Eleni Chatzi</a>, 
<a href="/search/cs?searchtype=author&query=Fink%2C+O">Olga Fink</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In real-world scenarios, achieving domain generalization (DG) presents
significant challenges as models are required to generalize to unknown target
distributions. Generalizing to unseen multi-modal distributions poses even
greater difficulties due to the distinct properties exhibited by different
modalities. To overcome the challenges of achieving domain generalization in
multi-modal scenarios, we propose SimMMDG, a simple yet effective multi-modal
DG framework. We argue that mapping features from different modalities into the
same embedding space impedes model generalization. To address this, we propose
splitting the features within each modality into modality-specific and
modality-shared components. We employ supervised contrastive learning on the
modality-shared features to ensure they possess joint properties and impose
distance constraints on modality-specific features to promote diversity. In
addition, we introduce a cross-modal translation module to regularize the
learned features, which can also be used for missing-modality generalization.
We demonstrate that our framework is theoretically well-supported and achieves
strong performance in multi-modal DG on the EPIC-Kitchens dataset and the novel
Human-Animal-Cartoon (HAC) dataset introduced in this paper. Our source code
and HAC dataset are available at https://github.com/donghao51/SimMMDG.
</p>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19797" title="Abstract">arXiv:2310.19797</a> [<a href="/pdf/2310.19797" title="Download PDF">pdf</a>, <a href="/format/2310.19797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEFT: Dexterous Fine-Tuning for Real-World Hand Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kannan%2C+A">Aditya Kannan</a>, 
<a href="/search/cs?searchtype=author&query=Shaw%2C+K">Kenneth Shaw</a>, 
<a href="/search/cs?searchtype=author&query=Bahl%2C+S">Shikhar Bahl</a>, 
<a href="/search/cs?searchtype=author&query=Mannam%2C+P">Pragna Mannam</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+D">Deepak Pathak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In CoRL 2023. Website at <a href="https://dexterous-finetuning.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Dexterity is often seen as a cornerstone of complex manipulation. Humans are
able to perform a host of skills with their hands, from making food to
operating tools. In this paper, we investigate these challenges, especially in
the case of soft, deformable objects as well as complex, relatively
long-horizon tasks. However, learning such behaviors from scratch can be data
inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous
Fine-Tuning for Hand Policies), that leverages human-driven priors, which are
executed directly in the real world. In order to improve upon these priors,
DEFT involves an efficient online optimization procedure. With the integration
of human-based learning and online fine-tuning, coupled with a soft robotic
hand, DEFT demonstrates success across various tasks, establishing a robust,
data-efficient pathway toward general dexterous manipulation. Please see our
website at https://dexterous-finetuning.github.io for video results.
</p>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19798" title="Abstract">arXiv:2310.19798</a> [<a href="/pdf/2310.19798" title="Download PDF">pdf</a>, <a href="/format/2310.19798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-Based Dovetail Joint Shape Optimization for Stiffness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xingyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+C">Chenyue Cai</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+R+P">Ryan P. Adams</a>, 
<a href="/search/cs?searchtype=author&query=Rusinkiewicz%2C+S">Szymon Rusinkiewicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM SCF 2023: Proceedings of the 8th Annual ACM Symposium on Computational Fabrication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">It is common to manufacture an object by decomposing it into parts that can
be assembled. This decomposition is often required by size limits of the
machine, the complex structure of the shape, etc. To make it possible to easily
assemble the final object, it is often desirable to design geometry that
enables robust connections between the subcomponents. In this project, we study
the task of dovetail-joint shape optimization for stiffness using
gradient-based optimization. This optimization requires a differentiable
simulator that is capable of modeling the contact between the two parts of a
joint, making it possible to reason about the gradient of the stiffness with
respect to shape parameters. Our simulation approach uses a penalty method that
alternates between optimizing each side of the joint, using the adjoint method
to compute gradients. We test our method by optimizing the joint shapes in
three different joint shape spaces, and evaluate optimized joint shapes in both
simulation and real-world tests. The experiments show that optimized joint
shapes achieve higher stiffness, both synthetically and in real-world tests.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Tue, 31 Oct 23</h3>
<dl>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18346" title="Abstract">arXiv:2310.18346</a> (cross-list from eess.IV) [<a href="/pdf/2310.18346" title="Download PDF">pdf</a>, <a href="/format/2310.18346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Free Distillation Improves Efficiency and Privacy in Federated  Thorax Disease Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the IEEE EMBS International Conference on Data Science and Engineering in Healthcare, Medicine &amp; Biology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Thorax disease analysis in large-scale, multi-centre, and multi-scanner
settings is often limited by strict privacy policies. Federated learning (FL)
offers a potential solution, while traditional parameter-based FL can be
limited by issues such as high communication costs, data leakage, and
heterogeneity. Distillation-based FL can improve efficiency, but it relies on a
proxy dataset, which is often impractical in clinical practice. To address
these challenges, we introduce a data-free distillation-based FL approach
FedKDF. In FedKDF, the server employs a lightweight generator to aggregate
knowledge from different clients without requiring access to their private data
or a proxy dataset. FedKDF combines the predictors from clients into a single,
unified predictor, which is further optimized using the learned knowledge in
the lightweight generator. Our empirical experiments demonstrate that FedKDF
offers a robust solution for efficient, privacy-preserving federated thorax
disease analysis.
</p>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18367" title="Abstract">arXiv:2310.18367</a> (cross-list from physics.chem-ph) [<a href="/pdf/2310.18367" title="Download PDF">pdf</a>, <a href="/format/2310.18367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Learning of Molecular Embeddings for Enhanced Clustering  and Emergent Properties for Chemical Compounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Gill%2C+J">Jaiveer Gill</a>, 
<a href="/search/physics?searchtype=author&query=Chakraborty%2C+R">Ratul Chakraborty</a>, 
<a href="/search/physics?searchtype=author&query=Gubba%2C+R">Reetham Gubba</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+A">Amy Liu</a>, 
<a href="/search/physics?searchtype=author&query=Jain%2C+S">Shrey Jain</a>, 
<a href="/search/physics?searchtype=author&query=Iyer%2C+C">Chirag Iyer</a>, 
<a href="/search/physics?searchtype=author&query=Khwaja%2C+O">Obaid Khwaja</a>, 
<a href="/search/physics?searchtype=author&query=Kumar%2C+S">Saurav Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The detailed analysis of molecular structures and properties holds great
potential for drug development discovery through machine learning. Developing
an emergent property in the model to understand molecules would broaden the
horizons for development with a new computational tool. We introduce various
methods to detect and cluster chemical compounds based on their SMILES data.
Our first method, analyzing the graphical structures of chemical compounds
using embedding data, employs vector search to meet our threshold value. The
results yielded pronounced, concentrated clusters, and the method produced
favorable results in querying and understanding the compounds. We also used
natural language description embeddings stored in a vector database with
GPT3.5, which outperforms the base model. Thus, we introduce a similarity
search and clustering algorithm to aid in searching for and interacting with
molecules, enhancing efficiency in chemical exploration and enabling future
development of emergent properties in molecular property prediction models.
</p>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18377" title="Abstract">arXiv:2310.18377</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.18377" title="Download PDF">pdf</a>, <a href="/format/2310.18377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-scale Foundation Models and Generative AI for BigData Neuroscience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+R">Ran Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+Z+S">Zhe Sage Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Recent advances in machine learning have made revolutionary breakthroughs in
computer games, image and natural language understanding, and scientific
discovery. Foundation models and large-scale language models (LLMs) have
recently achieved human-like intelligence thanks to BigData. With the help of
self-supervised learning (SSL) and transfer learning, these models may
potentially reshape the landscapes of neuroscience research and make a
significant impact on the future. Here we present a mini-review on recent
advances in foundation models and generative AI models as well as their
applications in neuroscience, including natural language and speech, semantic
memory, brain-machine interfaces (BMIs), and data augmentation. We argue that
this paradigm-shift framework will open new avenues for many neuroscience
research directions and discuss the accompanying challenges and opportunities.
</p>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18381" title="Abstract">arXiv:2310.18381</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.18381" title="Download PDF">pdf</a>, <a href="/format/2310.18381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveil Sleep Spindles with Concentration of Frequency and Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Shimizu%2C+R">Riki Shimizu</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+H">Hau-Tieng Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 6 figures, and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Objective: Sleep spindles contain crucial brain dynamics information. We
introduce the novel non-linear time-frequency analysis tool 'Concentration of
Frequency and Time' (ConceFT) to create an interpretable automated algorithm
for sleep spindle annotation in EEG data and to measure spindle instantaneous
frequencies (IFs). Methods: ConceFT effectively reduces stochastic EEG
influence, enhancing spindle visibility in the time-frequency representation.
Our automated spindle detection algorithm, ConceFT-Spindle (ConceFT-S), is
compared to A7 (non-deep learning) and SUMO (deep learning) using Dream and
MASS benchmark databases. We also quantify spindle IF dynamics. Results:
ConceFT-S achieves F1 scores of 0.749 in Dream and 0.786 in MASS, which is
equivalent to or surpass A7 and SUMO with statistical significance. We reveal
that spindle IF is generally nonlinear. Conclusion: ConceFT offers an accurate,
interpretable EEG-based sleep spindle detection algorithm and enables spindle
IF quantification.
</p>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18389" title="Abstract">arXiv:2310.18389</a> (cross-list from math.OC) [<a href="/pdf/2310.18389" title="Download PDF">pdf</a>, <a href="/ps/2310.18389" title="Download PostScript">ps</a>, <a href="/format/2310.18389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduction of Necessary Conditions for the Variational Collision  Avoidance Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Goodman%2C+J+R">Jacob R. Goodman</a>, 
<a href="/search/math?searchtype=author&query=Colombo%2C+L+J">Leonardo J. Colombo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8th IFAC Workshop on Lagrangian and Hamiltonian Methods for Nonlinear Control, LHMNC 2024. arXiv admin note: text overlap with <a href="/abs/2310.18057">arXiv:2310.18057</a>, <a href="/abs/2207.13574">arXiv:2207.13574</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Differential Geometry (math.DG); Dynamical Systems (math.DS)

</div>
<p class="mathjax">In this work, we study the reduction by a Lie group of symmetries of
variational collision avoidance probelms of multiple agents evolving on a
Riemannian manifold and derive necessary conditions for the reduced extremals.
The problem consists of finding non-intersecting trajectories of a given number
of agents, among a set of admissible curves, to reach a specified
configuration, based on minimizing an energy functional that depends on the
velocity, covariant acceleration and an artificial potential function used to
prevent collision among the agents.
</p>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18430" title="Abstract">arXiv:2310.18430</a> (cross-list from stat.ML) [<a href="/pdf/2310.18430" title="Download PDF">pdf</a>, <a href="/format/2310.18430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCRAGE: Synthetic Healthcare Data for Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Behal%2C+K">Keira Behal</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+J">Jiayi Chen</a>, 
<a href="/search/stat?searchtype=author&query=Fikes%2C+C">Caleb Fikes</a>, 
<a href="/search/stat?searchtype=author&query=Xiao%2C+S">Sophia Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Keywords: synthetic electronic health records, conditional denoising diffusion probabilistic model, healthcare AI, tabular data, fairness, synthetic data. This paper is the result of work completed at the 2023 Emory University Department of Mathematics REU/RET program under the direction of Project Advisor Dr. Xi Yuanzhe. This work is sponsored by NSF DMS 2051019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the field of healthcare, electronic health records (EHR) serve as crucial
training data for developing machine learning models for diagnosis, treatment,
and the management of healthcare resources. However, medical datasets are often
imbalanced in terms of sensitive attributes such as race/ethnicity, gender, and
age. Machine learning models trained on class-imbalanced EHR datasets perform
significantly worse in deployment for individuals of the minority classes
compared to samples from majority classes, which may lead to inequitable
healthcare outcomes for minority groups. To address this challenge, we propose
Minority Class Rebalancing through Augmentation by Generative modeling
(MCRAGE), a novel approach to augment imbalanced datasets using samples
generated by a deep generative model. The MCRAGE process involves training a
Conditional Denoising Diffusion Probabilistic Model (CDDPM) capable of
generating high-quality synthetic EHR samples from underrepresented classes. We
use this synthetic data to augment the existing imbalanced dataset, thereby
achieving a more balanced distribution across all classes, which can be used to
train an unbiased machine learning model. We measure the performance of MCRAGE
versus alternative approaches using Accuracy, F1 score and AUROC. We provide
theoretical justification for our method in terms of recent convergence results
for DDPMs with minimal assumptions.
</p>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18436" title="Abstract">arXiv:2310.18436</a> (cross-list from math.OC) [<a href="/pdf/2310.18436" title="Download PDF">pdf</a>, <a href="/format/2310.18436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical impulse controllability for parabolic equations by a penalized  HUM approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chorfi%2C+S+E">S. E. Chorfi</a>, 
<a href="/search/math?searchtype=author&query=Guermai%2C+G+E">G. El Guermai</a>, 
<a href="/search/math?searchtype=author&query=Maniar%2C+L">L. Maniar</a>, 
<a href="/search/math?searchtype=author&query=Zouhair%2C+W">W. Zouhair</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">This work presents a comparative study to numerically compute impulse
approximate controls for parabolic equations with various boundary conditions.
Theoretical controllability results have been recently investigated using a
logarithmic convexity estimate at a single time based on a Carleman commutator
approach. We propose a numerical algorithm for computing the impulse controls
with minimal $L^2$-norms by adapting a penalized Hilbert Uniqueness Method
(HUM) combined with a Conjugate Gradient (CG) method. We consider static
boundary conditions (Dirichlet and Neumann) and dynamic boundary conditions.
Some numerical experiments based on our developed algorithm are given to
validate and compare the theoretical impulse controllability results.
</p>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18449" title="Abstract">arXiv:2310.18449</a> (cross-list from stat.ML) [<a href="/pdf/2310.18449" title="Download PDF">pdf</a>, <a href="/format/2310.18449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Optimization with Hidden Constraints via Latent Decision Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xing%2C+W">Wenqian Xing</a>, 
<a href="/search/stat?searchtype=author&query=Lee%2C+J">Jungho Lee</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+C">Chong Liu</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+S">Shixiang Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures (exclude appendix)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">Bayesian optimization (BO) has emerged as a potent tool for addressing
intricate decision-making challenges, especially in public policy domains such
as police districting. However, its broader application in public policymaking
is hindered by the complexity of defining feasible regions and the
high-dimensionality of decisions. This paper introduces the Hidden-Constrained
Latent Space Bayesian Optimization (HC-LSBO), a novel BO method integrated with
a latent decision model. This approach leverages a variational autoencoder to
learn the distribution of feasible decisions, enabling a two-way mapping
between the original decision space and a lower-dimensional latent space. By
doing so, HC-LSBO captures the nuances of hidden constraints inherent in public
policymaking, allowing for optimization in the latent space while evaluating
objectives in the original space. We validate our method through numerical
experiments on both synthetic and real data sets, with a specific focus on
large-scale police districting problems in Atlanta, Georgia. Our results reveal
that HC-LSBO offers notable improvements in performance and efficiency compared
to the baselines.
</p>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18461" title="Abstract">arXiv:2310.18461</a> (cross-list from eess.AS) [<a href="/pdf/2310.18461" title="Download PDF">pdf</a>, <a href="/ps/2310.18461" title="Download PostScript">ps</a>, <a href="/format/2310.18461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Lossless Coding for Storage and Transmission of Multichannel  Immersive Audio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hirvonen%2C+T">Toni Hirvonen</a>, 
<a href="/search/eess?searchtype=author&query=Namazi%2C+M">Mahmoud Namazi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">In this paper, techniques for improving multichannel lossless coding are
examined. A method is proposed for the simultaneous coding of two or more
different renderings (mixes) of the same content. The signal model uses both
past samples of the upmix, and the current time samples of downmix samples to
predict the upmix. Model parameters are optimized via a general linear solver,
and the prediction residual is Rice coded. Additionally, the use of an SVD
projection prior to residual coding is proposed. A comparison is made against
various baselines, including FLAC. The proposed methods show improved
compression ratios for the storage and transmission of immersive audio.
</p>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18535" title="Abstract">arXiv:2310.18535</a> (cross-list from math.OC) [<a href="/pdf/2310.18535" title="Download PDF">pdf</a>, <a href="/format/2310.18535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Stochastic Bilevel Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+Y">Yifan Hu</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/math?searchtype=author&query=Xie%2C+Y">Yao Xie</a>, 
<a href="/search/math?searchtype=author&query=Krause%2C+A">Andreas Krause</a>, 
<a href="/search/math?searchtype=author&query=Kuhn%2C+D">Daniel Kuhn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce contextual stochastic bilevel optimization (CSBO) -- a
stochastic bilevel optimization framework with the lower-level problem
minimizing an expectation conditioned on some contextual information and the
upper-level decision variable. This framework extends classical stochastic
bilevel optimization when the lower-level decision maker responds optimally not
only to the decision of the upper-level decision maker but also to some side
information and when there are multiple or even infinite many followers. It
captures important applications such as meta-learning, personalized federated
learning, end-to-end learning, and Wasserstein distributionally robust
optimization with side information (WDRO-SI). Due to the presence of contextual
information, existing single-loop methods for classical stochastic bilevel
optimization are unable to converge. To overcome this challenge, we introduce
an efficient double-loop gradient method based on the Multilevel Monte-Carlo
(MLMC) technique and establish its sample and computational complexities. When
specialized to stochastic nonconvex optimization, our method matches existing
lower bounds. For meta-learning, the complexity of our method does not depend
on the number of tasks. Numerical experiments further validate our theoretical
results.
</p>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18552" title="Abstract">arXiv:2310.18552</a> (cross-list from physics.chem-ph) [<a href="/pdf/2310.18552" title="Download PDF">pdf</a>, <a href="/format/2310.18552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Role of Reference Points in Machine-Learned Atomistic Simulation  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lei%2C+X">Xiangyun Lei</a>, 
<a href="/search/physics?searchtype=author&query=Ye%2C+W">Weike Ye</a>, 
<a href="/search/physics?searchtype=author&query=Montoya%2C+J">Joseph Montoya</a>, 
<a href="/search/physics?searchtype=author&query=Mueller%2C+T">Tim Mueller</a>, 
<a href="/search/physics?searchtype=author&query=Hung%2C+L">Linda Hung</a>, 
<a href="/search/physics?searchtype=author&query=Hummelshoej%2C+J">Jens Hummelshoej</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces the Chemical Environment Modeling Theory (CEMT), a
novel, generalized framework designed to overcome the limitations inherent in
traditional atom-centered Machine Learning Force Field (MLFF) models, widely
used in atomistic simulations of chemical systems. CEMT demonstrated enhanced
flexibility and adaptability by allowing reference points to exist anywhere
within the modeled domain and thus, enabling the study of various model
architectures. Utilizing Gaussian Multipole (GMP) featurization functions,
several models with different reference point sets, including finite difference
grid-centered and bond-centered models, were tested to analyze the variance in
capabilities intrinsic to models built on distinct reference points. The
results underscore the potential of non-atom-centered reference points in force
training, revealing variations in prediction accuracy, inference speed and
learning efficiency. Finally, a unique connection between CEMT and real-space
orbital-free finite element Density Functional Theory (FE-DFT) is established,
and the implications include the enhancement of data efficiency and robustness.
It allows the leveraging of spatially-resolved energy densities and charge
densities from FE-DFT calculations, as well as serving as a pivotal step
towards integrating known quantum-mechanical laws into the architecture of ML
models.
</p>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18554" title="Abstract">arXiv:2310.18554</a> (cross-list from stat.ML) [<a href="/pdf/2310.18554" title="Download PDF">pdf</a>, <a href="/format/2310.18554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Regret Bounds of (Multinomial) Logistic Bandits via  Regret-to-Confidence-Set Conversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lee%2C+J">Junghyun Lee</a>, 
<a href="/search/stat?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>, 
<a href="/search/stat?searchtype=author&query=Jun%2C+K">Kwang-Sung Jun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 2 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Logistic bandit is a ubiquitous framework of modeling users' choices, e.g.,
click vs. no click for advertisement recommender system. We observe that the
prior works overlook or neglect dependencies in $S \geq \lVert \theta_\star
\rVert_2$, where $\theta_\star \in \mathbb{R}^d$ is the unknown parameter
vector, which is particularly problematic when $S$ is large, e.g., $S \geq d$.
In this work, we improve the dependency on $S$ via a novel approach called {\it
regret-to-confidence set conversion (R2CS)}, which allows us to construct a
convex confidence set based on only the \textit{existence} of an online
learning algorithm with a regret guarantee. Using R2CS, we obtain a strict
improvement in the regret bound w.r.t. $S$ in logistic bandits while retaining
computational feasibility and the dependence on other factors such as $d$ and
$T$. We apply our new confidence set to the regret analyses of logistic bandits
with a new martingale concentration step that circumvents an additional factor
of $S$. We then extend this analysis to multinomial logistic bandits and obtain
similar improvements in the regret, showing the efficacy of R2CS. While we
applied R2CS to the (multinomial) logistic model, R2CS is a generic approach
for developing confidence sets that can be used for various models, which can
be of independent interest.
</p>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18565" title="Abstract">arXiv:2310.18565</a> (cross-list from math.FA) [<a href="/pdf/2310.18565" title="Download PDF">pdf</a>, <a href="/ps/2310.18565" title="Download PostScript">ps</a>, <a href="/format/2310.18565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linearly Embedding Sparse Vectors from $\ell_2$ to $\ell_1$ via  Deterministic Dimension-Reducing Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Foucart%2C+S">Simon Foucart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This note is concerned with deterministic constructions of $m \times N$
matrices satisfying a restricted isometry property from $\ell_2$ to $\ell_1$ on
$s$-sparse vectors. Similarly to the standard ($\ell_2$ to $\ell_2$) restricted
isometry property, such constructions can be found in the regime $m \asymp
s^2$, at least in theory. With effectiveness of implementation in mind, two
simple constructions are presented in the less pleasing but still relevant
regime $m \asymp s^4$. The first one, executing a Las Vegas strategy, is
quasideterministic and applies in the real setting. The second one, exploiting
Golomb rulers, is explicit and applies to the complex setting. As a stepping
stone, an explicit isometric embedding from $\ell_2^n(\mathbb{C})$ to
$\ell_4^{cn^2}(\mathbb{C})$ is presented. Finally, the extension of the problem
from sparse vectors to low-rank matrices is raised as an open question.
</p>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18591" title="Abstract">arXiv:2310.18591</a> (cross-list from stat.ML) [<a href="/pdf/2310.18591" title="Download PDF">pdf</a>, <a href="/format/2310.18591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Decision Modeling: Learning Interpretable Representations of  Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Jarrett%2C+D">Daniel Jarrett</a>, 
<a href="/search/stat?searchtype=author&query=H%C3%BCy%C3%BCk%2C+A">Alihan H&#xfc;y&#xfc;k</a>, 
<a href="/search/stat?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proc. 38th International Conference on Machine Learning (ICML
  2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Decision analysis deals with modeling and enhancing decision processes. A
principal challenge in improving behavior is in obtaining a transparent
description of existing behavior in the first place. In this paper, we develop
an expressive, unifying perspective on inverse decision modeling: a framework
for learning parameterized representations of sequential decision behavior.
First, we formalize the forward problem (as a normative standard), subsuming
common classes of control behavior. Second, we use this to formalize the
inverse problem (as a descriptive model), generalizing existing work on
imitation/reward learning -- while opening up a much broader class of research
problems in behavior representation. Finally, we instantiate this approach with
an example (inverse bounded rational control), illustrating how this structure
enables learning (interpretable) representations of (bounded) rationality --
while naturally capturing intuitive notions of suboptimal actions, biased
beliefs, and imperfect knowledge of environments.
</p>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18593" title="Abstract">arXiv:2310.18593</a> (cross-list from stat.ML) [<a href="/pdf/2310.18593" title="Download PDF">pdf</a>, <a href="/format/2310.18593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Streaming Principal Component Analysis: Statistical and Algorithmic  Viewpoint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lee%2C+J">Junghyun Lee</a>, 
<a href="/search/stat?searchtype=author&query=Cho%2C+H">Hanseul Cho</a>, 
<a href="/search/stat?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>, 
<a href="/search/stat?searchtype=author&query=Yun%2C+C">Chulhee Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 5 figures, 4 tables. Accepted to the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Fair Principal Component Analysis (PCA) is a problem setting where we aim to
perform PCA while making the resulting representation fair in that the
projected distributions, conditional on the sensitive attributes, match one
another. However, existing approaches to fair PCA have two main problems:
theoretically, there has been no statistical foundation of fair PCA in terms of
learnability; practically, limited memory prevents us from using existing
approaches, as they explicitly rely on full access to the entire data. On the
theoretical side, we rigorously formulate fair PCA using a new notion called
\emph{probably approximately fair and optimal} (PAFO) learnability. On the
practical side, motivated by recent advances in streaming algorithms for
addressing memory limitation, we propose a new setting called \emph{fair
streaming PCA} along with a memory-efficient algorithm, fair noisy power method
(FNPM). We then provide its {\it statistical} guarantee in terms of
PAFO-learnability, which is the first of its kind in fair PCA literature.
Lastly, we verify the efficacy and memory efficiency of our algorithm on
real-world datasets.
</p>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18601" title="Abstract">arXiv:2310.18601</a> (cross-list from stat.ML) [<a href="/pdf/2310.18601" title="Download PDF">pdf</a>, <a href="/format/2310.18601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Decision Mediation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Jarrett%2C+D">Daniel Jarrett</a>, 
<a href="/search/stat?searchtype=author&query=H%C3%BCy%C3%BCk%2C+A">Alihan H&#xfc;y&#xfc;k</a>, 
<a href="/search/stat?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proc. 36th International Conference on Neural Information
  Processing Systems (NeurIPS 2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Consider learning a decision support assistant to serve as an intermediary
between (oracle) expert behavior and (imperfect) human behavior: At each time,
the algorithm observes an action chosen by a fallible agent, and decides
whether to *accept* that agent's decision, *intervene* with an alternative, or
*request* the expert's opinion. For instance, in clinical diagnosis,
fully-autonomous machine behavior is often beyond ethical affordances, thus
real-world decision support is often limited to monitoring and forecasting.
Instead, such an intermediary would strike a prudent balance between the former
(purely prescriptive) and latter (purely descriptive) approaches, while
providing an efficient interface between human mistakes and expert feedback. In
this work, we first formalize the sequential problem of *online decision
mediation* -- that is, of simultaneously learning and evaluating mediator
policies from scratch with *abstentive feedback*: In each round, deferring to
the oracle obviates the risk of error, but incurs an upfront penalty, and
reveals the otherwise hidden expert action as a new training data point.
Second, we motivate and propose a solution that seeks to trade off (immediate)
loss terms against (future) improvements in generalization error; in doing so,
we identify why conventional bandit algorithms may fail. Finally, through
experiments and sensitivities on a variety of datasets, we illustrate
consistent gains over applicable benchmarks on performance measures with
respect to the mediator policy, the learned model, and the decision-making
system as a whole.
</p>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18625" title="Abstract">arXiv:2310.18625</a> (cross-list from math.OC) [<a href="/pdf/2310.18625" title="Download PDF">pdf</a>, <a href="/format/2310.18625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Optimization of Clique-Wise Coupled Problems via  Three-Operator Splitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Watanabe%2C+Y">Yuto Watanabe</a>, 
<a href="/search/math?searchtype=author&query=Sakurama%2C+K">Kazunori Sakurama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this study, we explore distributed optimization problems with clique-wise
coupling through the lens of operator splitting. This framework of clique-wise
coupling extends beyond conventional pairwise coupled problems, encompassing
consensus optimization and formation control, and is applicable to a wide array
of examples. We first introduce a matrix, called the clique-wise duplication
(CD) matrix, which enables decoupled reformulations for operator splitting
methods and distributed computation. Leveraging this matrix, we propose a new
distributed optimization algorithm via Davis-Yin splitting (DYS), a versatile
three-operator splitting method. We then delve into the properties of this
method and demonstrate how existing consensus optimization methods (NIDS, Exact
Diffusion, and Diffusion) can be derived from our proposed method. Furthermore,
being inspired by this observation, we derive a Diffusion-like method, the
clique-based projected gradient descent (CPGD), and present Nesterov's
acceleration and in-depth convergence analysis for various step sizes. The
paper concludes with numerical examples that underscore the efficacy of our
proposed method.
</p>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18654" title="Abstract">arXiv:2310.18654</a> (cross-list from stat.ML) [<a href="/pdf/2310.18654" title="Download PDF">pdf</a>, <a href="/format/2310.18654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal discovery in a complex industrial system: A time series benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Mogensen%2C+S+W">S&#xf8;ren Wengel Mogensen</a>, 
<a href="/search/stat?searchtype=author&query=Rathsman%2C+K">Karin Rathsman</a>, 
<a href="/search/stat?searchtype=author&query=Nilsson%2C+P">Per Nilsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Causal discovery outputs a causal structure, represented by a graph, from
observed data. For time series data, there is a variety of methods, however, it
is difficult to evaluate these on real data as realistic use cases very rarely
come with a known causal graph to which output can be compared. In this paper,
we present a dataset from an industrial subsystem at the European Spallation
Source along with its causal graph which has been constructed from expert
knowledge. This provides a testbed for causal discovery from time series
observations of complex systems, and we believe this can help inform the
development of causal discovery methodology.
</p>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18656" title="Abstract">arXiv:2310.18656</a> (cross-list from eess.IV) [<a href="/pdf/2310.18656" title="Download PDF">pdf</a>, <a href="/format/2310.18656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Med-DANet V2: A Flexible Dynamic Architecture for Efficient Medical  Volumetric Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shen%2C+H">Haoran Shen</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yifu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+S">Shanshan Song</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jiangyun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recent works have shown that the computational efficiency of 3D medical image
(e.g. CT and MRI) segmentation can be impressively improved by dynamic
inference based on slice-wise complexity. As a pioneering work, a dynamic
architecture network for medical volumetric segmentation (i.e. Med-DANet) has
achieved a favorable accuracy and efficiency trade-off by dynamically selecting
a suitable 2D candidate model from the pre-defined model bank for different
slices. However, the issues of incomplete data analysis, high training costs,
and the two-stage pipeline in Med-DANet require further improvement. To this
end, this paper further explores a unified formulation of the dynamic inference
framework from the perspective of both the data itself and the model structure.
For each slice of the input volume, our proposed method dynamically selects an
important foreground region for segmentation based on the policy generated by
our Decision Network and Crop Position Network. Besides, we propose to insert a
stage-wise quantization selector to the employed segmentation model (e.g.
U-Net) for dynamic architecture adapting. Extensive experiments on BraTS 2019
and 2020 show that our method achieves comparable or better performance than
previous state-of-the-art methods with much less model complexity. Compared
with previous methods Med-DANet and TransBTS with dynamic and static
architecture respectively, our framework improves the model efficiency by up to
nearly 4.1 and 17.3 times with comparable segmentation results on BraTS 2019.
</p>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18717" title="Abstract">arXiv:2310.18717</a> (cross-list from stat.ML) [<a href="/pdf/2310.18717" title="Download PDF">pdf</a>, <a href="/format/2310.18717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Accuracy of Hotelling-Type Asymmetric Tensor Deflation: A Random  Tensor Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Seddik%2C+M+E+A">Mohamed El Amine Seddik</a>, 
<a href="/search/stat?searchtype=author&query=Guillaud%2C+M">Maxime Guillaud</a>, 
<a href="/search/stat?searchtype=author&query=Decurninge%2C+A">Alexis Decurninge</a>, 
<a href="/search/stat?searchtype=author&query=de+Morais+Goulart%2C+J+H">Jos&#xe9; Henrique de Morais Goulart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE CAMSAP 2023. See also companion paper <a href="/abs/2304.10248">arXiv:2304.10248</a> for the symmetric case. arXiv admin note: text overlap with <a href="/abs/2211.09004">arXiv:2211.09004</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work introduces an asymptotic study of Hotelling-type tensor deflation
in the presence of noise, in the regime of large tensor dimensions.
Specifically, we consider a low-rank asymmetric tensor model of the form
$\sum_{i=1}^r \beta_i{\mathcal{A}}_i + {\mathcal{W}}$ where $\beta_i\geq 0$ and
the ${\mathcal{A}}_i$'s are unit-norm rank-one tensors such that $\left|
\langle {\mathcal{A}}_i, {\mathcal{A}}_j \rangle \right| \in [0, 1]$ for $i\neq
j$ and ${\mathcal{W}}$ is an additive noise term. Assuming that the dominant
components are successively estimated from the noisy observation and
subsequently subtracted, we leverage recent advances in random tensor theory in
the regime of asymptotically large tensor dimensions to analytically
characterize the estimated singular values and the alignment of estimated and
true singular vectors at each step of the deflation procedure. Furthermore,
this result can be used to construct estimators of the signal-to-noise ratios
$\beta_i$ and the alignments between the estimated and true rank-1 signal
components.
</p>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18750" title="Abstract">arXiv:2310.18750</a> (cross-list from math.OC) [<a href="/pdf/2310.18750" title="Download PDF">pdf</a>, <a href="/ps/2310.18750" title="Download PostScript">ps</a>, <a href="/format/2310.18750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-step condensed forms for square-root maximum correntropy criterion  Kalman filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kulikova%2C+M">Maria Kulikova</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 23rd International Conference on System Theory,
  Control and Computing, 2019, Sinaia, Romania, pp. 13-18
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">This paper suggests a few novel Cholesky-based square-root algorithms for the
maximum correntropy criterion Kalman filtering. In contrast to the previously
obtained results, new algorithms are developed in the so-called {\it condensed}
form that corresponds to the {\it a priori} filtering. Square-root filter
implementations are known to possess a better conditioning and improved
numerical robustness when solving ill-conditioned estimation problems.
Additionally, the new algorithms permit easier propagation of the state
estimate and do not require a back-substitution for computing the estimate.
Performance of novel filtering methods is examined by using a fourth order
benchmark navigation system example.
</p>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18814" title="Abstract">arXiv:2310.18814</a> (cross-list from stat.ML) [<a href="/pdf/2310.18814" title="Download PDF">pdf</a>, <a href="/format/2310.18814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability of Random Forests and Coverage of Random-Forest Prediction  Intervals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+H">Huaiqing Wu</a>, 
<a href="/search/stat?searchtype=author&query=Nettleton%2C+D">Dan Nettleton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We establish stability of random forests under the mild condition that the
squared response ($Y^2$) does not have a heavy tail. In particular, our
analysis holds for the practical version of random forests that is implemented
in popular packages like \texttt{randomForest} in \texttt{R}. Empirical results
show that stability may persist even beyond our assumption and hold for
heavy-tailed $Y^2$. Using the stability property, we prove a non-asymptotic
lower bound for the coverage probability of prediction intervals constructed
from the out-of-bag error of random forests. With another mild condition that
is typically satisfied when $Y$ is continuous, we also establish a
complementary upper bound, which can be similarly established for the jackknife
prediction interval constructed from an arbitrary stable algorithm. We also
discuss the asymptotic coverage probability under assumptions weaker than those
considered in previous literature. Our work implies that random forests, with
its stability property, is an effective machine learning method that can
provide not only satisfactory point prediction but also justified interval
prediction at almost no extra computational cost.
</p>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18824" title="Abstract">arXiv:2310.18824</a> (cross-list from stat.ML) [<a href="/pdf/2310.18824" title="Download PDF">pdf</a>, <a href="/format/2310.18824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intrinsic Gaussian Vector Fields on Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Robert-Nicoud%2C+D">Daniel Robert-Nicoud</a>, 
<a href="/search/stat?searchtype=author&query=Krause%2C+A">Andreas Krause</a>, 
<a href="/search/stat?searchtype=author&query=Borovitskiy%2C+V">Viacheslav Borovitskiy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Various applications ranging from robotics to climate science require
modeling signals on non-Euclidean domains, such as the sphere. Gaussian process
models on manifolds have recently been proposed for such tasks, in particular
when uncertainty quantification is needed. In the manifold setting,
vector-valued signals can behave very differently from scalar-valued ones, with
much of the progress so far focused on modeling the latter. The former,
however, are crucial for many applications, such as modeling wind speeds or
force fields of unknown dynamical systems. In this paper, we propose novel
Gaussian process models for vector-valued signals on manifolds that are
intrinsically defined and account for the geometry of the space in
consideration. We provide computational primitives needed to deploy the
resulting Hodge-Mat\'ern Gaussian vector fields on the two-dimensional sphere
and the hypertori. Further, we highlight two generalization directions:
discrete two-dimensional meshes and "ideal" manifolds like hyperspheres, Lie
groups, and homogeneous spaces. Finally, we show that our Gaussian vector
fields constitute considerably more refined inductive biases than the extrinsic
fields proposed before.
</p>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18841" title="Abstract">arXiv:2310.18841</a> (cross-list from math.OC) [<a href="/pdf/2310.18841" title="Download PDF">pdf</a>, <a href="/ps/2310.18841" title="Download PostScript">ps</a>, <a href="/format/2310.18841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A randomized algorithm for nonconvex minimization with inexact  evaluations and complexity guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+S">Shuyao Li</a>, 
<a href="/search/math?searchtype=author&query=Wright%2C+S+J">Stephen J. Wright</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider minimization of a smooth nonconvex function with inexact oracle
access to gradient and Hessian (but not the function value) to achieve
$(\epsilon_{g}, \epsilon_{H})$-approximate second-order optimality. A novel
feature of our method is that if an approximate direction of negative curvature
is chosen as the step, we choose its sense to be positive or negative with
equal probability. We also use relative inexactness measures on gradient and
Hessian and relax the coupling between the first- and second-order tolerances
$\epsilon_{g}$ and $\epsilon_{H}$. Our convergence analysis includes both an
expectation bound based on martingale analysis and a high-probability bound
based on concentration inequalities. We apply our algorithm to empirical risk
minimization problems and obtain gradient sample complexity.
</p>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18860" title="Abstract">arXiv:2310.18860</a> (cross-list from stat.ML) [<a href="/pdf/2310.18860" title="Download PDF">pdf</a>, <a href="/format/2310.18860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayes beats Cross Validation: Efficient and Accurate Ridge Regression  via Expectation Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tew%2C+S+Y">Shu Yu Tew</a>, 
<a href="/search/stat?searchtype=author&query=Boley%2C+M">Mario Boley</a>, 
<a href="/search/stat?searchtype=author&query=Schmidt%2C+D+F">Daniel F. Schmidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a novel method for tuning the regularization hyper-parameter,
$\lambda$, of a ridge regression that is faster to compute than leave-one-out
cross-validation (LOOCV) while yielding estimates of the regression parameters
of equal, or particularly in the setting of sparse covariates, superior quality
to those obtained by minimising the LOOCV risk. The LOOCV risk can suffer from
multiple and bad local minima for finite $n$ and thus requires the
specification of a set of candidate $\lambda$, which can fail to provide good
solutions. In contrast, we show that the proposed method is guaranteed to find
a unique optimal solution for large enough $n$, under relatively mild
conditions, without requiring the specification of any difficult to determine
hyper-parameters. This is based on a Bayesian formulation of ridge regression
that we prove to have a unimodal posterior for large enough $n$, allowing for
both the optimal $\lambda$ and the regression coefficients to be jointly
learned within an iterative expectation maximization (EM) procedure.
Importantly, we show that by utilizing an appropriate preprocessing step, a
single iteration of the main EM loop can be implemented in $O(\min(n, p))$
operations, for input data with $n$ rows and $p$ columns. In contrast,
evaluating a single value of $\lambda$ using fast LOOCV costs $O(n \min(n, p))$
operations when using the same preprocessing. This advantage amounts to an
asymptotic improvement of a factor of $l$ for $l$ candidate values for
$\lambda$ (in the regime $q, p \in O(\sqrt{n})$ where $q$ is the number of
regression targets).
</p>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18897" title="Abstract">arXiv:2310.18897</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2310.18897" title="Download PDF">pdf</a>, <a href="/format/2310.18897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Subgrid-Scale Models in Discontinuous Galerkin Methods with  Neural Ordinary Differential Equations for Compressible Navier--Stokes  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Kang%2C+S">Shinhoo Kang</a>, 
<a href="/search/physics?searchtype=author&query=Constantinescu%2C+E+M">Emil M. Constantinescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 figures, 2 tables, 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">The growing computing power over the years has enabled simulations to become
more complex and accurate. However, high-fidelity simulations, while immensely
valuable for scientific discovery and problem solving, come with significant
computational demands. As a result, it is common to run a low-fidelity model
with a subgrid-scale model to reduce the computational cost, but selecting the
appropriate subgrid-scale models and tuning them are challenging. We propose a
novel method for learning the subgrid-scale model effects when simulating
partial differential equations using neural ordinary differential equations in
the context of discontinuous Galerkin (DG) spatial discretization. Our approach
learns the missing scales of the low-order DG solver at a continuous level and
hence improves the accuracy of the low-order DG approximations as well as
accelerates the filtered high-order DG simulations with a certain degree of
precision. We demonstrate the performance of our approach through
multidimensional Taylor--Green vortex examples at different Reynolds numbers
and times, which cover laminar, transitional, and turbulent regimes. The
proposed method not only reconstructs the subgrid-scale from the low-order
(1st-order) approximation but also speeds up the filtered high-order DG
(6th-order) simulation by two orders of magnitude.
</p>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18900" title="Abstract">arXiv:2310.18900</a> (cross-list from quant-ph) [<a href="/pdf/2310.18900" title="Download PDF">pdf</a>, <a href="/format/2310.18900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum algorithms for linear and non-linear fractional  reaction-diffusion equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=An%2C+D">Dong An</a>, 
<a href="/search/quant-ph?searchtype=author&query=Trivisa%2C+K">Konstantina Trivisa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">High-dimensional fractional reaction-diffusion equations have numerous
applications in the fields of biology, chemistry, and physics, and exhibit a
range of rich phenomena. While classical algorithms have an exponential
complexity in the spatial dimension, a quantum computer can produce a quantum
state that encodes the solution with only polynomial complexity, provided that
suitable input access is available. In this work, we investigate efficient
quantum algorithms for linear and nonlinear fractional reaction-diffusion
equations with periodic boundary conditions. For linear equations, we analyze
and compare the complexity of various methods, including the second-order
Trotter formula, time-marching method, and truncated Dyson series method. We
also present a novel algorithm that combines the linear combination of
Hamiltonian simulation technique with the interaction picture formalism,
resulting in optimal scaling in the spatial dimension. For nonlinear equations,
we employ the Carleman linearization method and propose a block-encoding
version that is appropriate for the dense matrices that arise from the spatial
discretization of fractional reaction-diffusion equations.
</p>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18907" title="Abstract">arXiv:2310.18907</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2310.18907" title="Download PDF">pdf</a>, <a href="/format/2310.18907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological, or Non-topological? A Deep Learning Based Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Rasul%2C+A">Ashiqur Rasul</a>, 
<a href="/search/cond-mat?searchtype=author&query=Hossain%2C+M+S">Md Shafayat Hossain</a>, 
<a href="/search/cond-mat?searchtype=author&query=Dastider%2C+A+G">Ankan Ghosh Dastider</a>, 
<a href="/search/cond-mat?searchtype=author&query=Roy%2C+H">Himaddri Roy</a>, 
<a href="/search/cond-mat?searchtype=author&query=Hasan%2C+M+Z">M. Zahid Hasan</a>, 
<a href="/search/cond-mat?searchtype=author&query=Khosru%2C+Q+D+M">Quazi D. M. Khosru</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Prediction and discovery of new materials with desired properties are at the
forefront of quantum science and technology research. A major bottleneck in
this field is the computational resources and time complexity related to
finding new materials from ab initio calculations. In this work, an effective
and robust deep learning-based model is proposed by incorporating persistent
homology and graph neural network which offers an accuracy of 91.4% and an F1
score of 88.5% in classifying topological vs. non-topological materials,
outperforming the other state-of-the-art classifier models. The incorporation
of the graph neural network encodes the underlying relation between the atoms
into the model based on their own crystalline structures and thus proved to be
an effective method to represent and process non-euclidean data like molecules
with a relatively shallow network. The persistent homology pipeline in the
suggested neural network is capable of integrating the atom-specific
topological information into the deep learning model, increasing robustness,
and gain in performance. It is believed that the presented work will be an
efficacious tool for predicting the topological class and therefore enable the
high-throughput search for novel materials in this field.
</p>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18981" title="Abstract">arXiv:2310.18981</a> (cross-list from math.OC) [<a href="/pdf/2310.18981" title="Download PDF">pdf</a>, <a href="/format/2310.18981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outsourcing policies for the Facility Location Problem with Bernoulli  Demand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Albareda-Sambola%2C+M">Maria Albareda-Sambola</a>, 
<a href="/search/math?searchtype=author&query=Fern%C3%A1ndez%2C+E">Elena Fern&#xe1;ndez</a>, 
<a href="/search/math?searchtype=author&query=Saldanha-da-Gama%2C+F">Francisco Saldanha-da-Gama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">This paper focuses on the Facility Location Problem with Bernoulli Demand, a
discrete facility location problem with uncertainty where the joint
distribution of the customers' demands is expressed by means of a set of
possible scenarios. A two-stage stochastic program with recourse is used to
select the facility locations and the a priori assignments of customers to open
plants, together with the a posteriori strategy to apply in those realizations
where the a priori solution is not feasible. Four alternative outsourcing
policies are studied for the recourse action, and a mathematical programming
formulation is presented for each of them. Extensive computational experiments
have been carried-out to analyze the performance of each of the formulations
and to compare the quality of the solutions produced by each of them relative
to the other outsourcing policies.
</p>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18988" title="Abstract">arXiv:2310.18988</a> (cross-list from stat.ML) [<a href="/pdf/2310.18988" title="Download PDF">pdf</a>, <a href="/format/2310.18988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A U-turn on Double Descent: Rethinking Parameter Counting in Statistical  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Curth%2C+A">Alicia Curth</a>, 
<a href="/search/stat?searchtype=author&query=Jeffares%2C+A">Alan Jeffares</a>, 
<a href="/search/stat?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Conventional statistical wisdom established a well-understood relationship
between model complexity and prediction error, typically presented as a
U-shaped curve reflecting a transition between under- and overfitting regimes.
However, motivated by the success of overparametrized neural networks, recent
influential work has suggested this theory to be generally incomplete,
introducing an additional regime that exhibits a second descent in test error
as the parameter count p grows past sample size n - a phenomenon dubbed double
descent. While most attention has naturally been given to the deep-learning
setting, double descent was shown to emerge more generally across non-neural
models: known cases include linear regression, trees, and boosting. In this
work, we take a closer look at evidence surrounding these more classical
statistical machine learning methods and challenge the claim that observed
cases of double descent truly extend the limits of a traditional U-shaped
complexity-generalization curve therein. We show that once careful
consideration is given to what is being plotted on the x-axes of their double
descent plots, it becomes apparent that there are implicitly multiple
complexity axes along which the parameter count grows. We demonstrate that the
second descent appears exactly (and only) when and where the transition between
these underlying axes occurs, and that its location is thus not inherently tied
to the interpolation threshold p=n. We then gain further insight by adopting a
classical nonparametric statistics perspective. We interpret the investigated
methods as smoothers and propose a generalized measure for the effective number
of parameters they use on unseen examples, using which we find that their
apparent double descent curves indeed fold back into more traditional convex
shapes - providing a resolution to tensions between double descent and
statistical intuition.
</p>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19022" title="Abstract">arXiv:2310.19022</a> (cross-list from math.OC) [<a href="/pdf/2310.19022" title="Download PDF">pdf</a>, <a href="/format/2310.19022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization Landscape of Policy Gradient Methods for Discrete-time  Static Output Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Duan%2C+J">Jingliang Duan</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jie Li</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+X">Xuyang Chen</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+K">Kai Zhao</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+S+E">Shengbo Eben Li</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+L">Lin Zhao</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Cybernetics, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">In recent times, significant advancements have been made in delving into the
optimization landscape of policy gradient methods for achieving optimal control
in linear time-invariant (LTI) systems. Compared with state-feedback control,
output-feedback control is more prevalent since the underlying state of the
system may not be fully observed in many practical settings. This paper
analyzes the optimization landscape inherent to policy gradient methods when
applied to static output feedback (SOF) control in discrete-time LTI systems
subject to quadratic cost. We begin by establishing crucial properties of the
SOF cost, encompassing coercivity, L-smoothness, and M-Lipschitz continuous
Hessian. Despite the absence of convexity, we leverage these properties to
derive novel findings regarding convergence (and nearly dimension-free rate) to
stationary points for three policy gradient methods, including the vanilla
policy gradient method, the natural policy gradient method, and the
Gauss-Newton method. Moreover, we provide proof that the vanilla policy
gradient method exhibits linear convergence towards local minima when
initialized near such minima. The paper concludes by presenting numerical
examples that validate our theoretical findings. These results not only
characterize the performance of gradient descent for optimizing the SOF problem
but also provide insights into the effectiveness of general policy gradient
methods within the realm of reinforcement learning.
</p>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19028" title="Abstract">arXiv:2310.19028</a> (cross-list from quant-ph) [<a href="/pdf/2310.19028" title="Download PDF">pdf</a>, <a href="/ps/2310.19028" title="Download PostScript">ps</a>, <a href="/format/2310.19028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Area law for the maximally mixed ground state in degenerate 1D gapped  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Arad%2C+I">Itai Arad</a>, 
<a href="/search/quant-ph?searchtype=author&query=Firanko%2C+R">Raz Firanko</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jain%2C+R">Rahul Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, version 1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Other Condensed Matter (cond-mat.other); Computational Complexity (cs.CC); Information Theory (cs.IT)

</div>
<p class="mathjax">We show an area law with logarithmic correction for the maximally mixed state
$\Omega$ in the (degenerate) ground space of a 1D gapped local Hamiltonian $H$,
which is independent of the underlying ground space degeneracy. Formally, for
$\varepsilon&gt;0$ and a bi-partition $L\cup L^c$ of the 1D lattice, we show that
<br />$$\mathrm{I}^{\varepsilon}_{\max}(L:L^c)_{\Omega} \leq
O(\log(|L|)+\log(1/\varepsilon)),$$
<br />where $|L|$ represents the number of qudits in $L$ and
$\mathrm{I}^{\epsilon}_{\max}(L:L^c)_{\Omega}$ represents the $\varepsilon$-
'smoothed maximum mutual information' with respect to the $L:L^c$ partition in
$\Omega$. As a corollary, we get an area law for the mutual information of the
form $\mathrm{I}(L:R)_\Omega \leq O(\log |L|)$. In addition, we show that
$\Omega$ can be approximated up to an $\varepsilon$ in trace norm with a state
of Schmidt rank of at most $\mathrm{poly}(|L|/\varepsilon)$.
</p>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19039" title="Abstract">arXiv:2310.19039</a> (cross-list from math.DS) [<a href="/pdf/2310.19039" title="Download PDF">pdf</a>, <a href="/format/2310.19039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning for the identification of phase-transitions in  interacting agent-based systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Evangelou%2C+N">Nikolaos Evangelou</a>, 
<a href="/search/math?searchtype=author&query=Giovanis%2C+D+G">Dimitrios G. Giovanis</a>, 
<a href="/search/math?searchtype=author&query=Kevrekidis%2C+G+A">George A. Kevrekidis</a>, 
<a href="/search/math?searchtype=author&query=Pavliotis%2C+G+A">Grigorios A. Pavliotis</a>, 
<a href="/search/math?searchtype=author&query=Kevrekidis%2C+I+G">Ioannis G. Kevrekidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Deriving closed-form, analytical expressions for reduced-order models, and
judiciously choosing the closures leading to them, has long been the strategy
of choice for studying phase- and noise-induced transitions for agent-based
models (ABMs). In this paper, we propose a data-driven framework that pinpoints
phase transitions for an ABM in its mean-field limit, using a smaller number of
variables than traditional closed-form models. To this end, we use the manifold
learning algorithm Diffusion Maps to identify a parsimonious set of data-driven
latent variables, and show that they are in one-to-one correspondence with the
expected theoretical order parameter of the ABM. We then utilize a deep
learning framework to obtain a conformal reparametrization of the data-driven
coordinates that facilitates, in our example, the identification of a single
parameter-dependent ODE in these coordinates. We identify this ODE through a
residual neural network inspired by a numerical integration scheme (forward
Euler). We then use the identified ODE -- enabled through an odd symmetry
transformation -- to construct the bifurcation diagram exhibiting the phase
transition.
</p>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19041" title="Abstract">arXiv:2310.19041</a> (cross-list from stat.ML) [<a href="/pdf/2310.19041" title="Download PDF">pdf</a>, <a href="/format/2310.19041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Linear Separation Capacity of Self-Supervised Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+S">Shulei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">Recent advances in self-supervised learning have highlighted the efficacy of
data augmentation in learning data representation from unlabeled data. Training
a linear model atop these enhanced representations can yield an adept
classifier. Despite the remarkable empirical performance, the underlying
mechanisms that enable data augmentation to unravel nonlinear data structures
into linearly separable representations remain elusive. This paper seeks to
bridge this gap by investigating under what conditions learned representations
can linearly separate manifolds when data is drawn from a multi-manifold model.
Our investigation reveals that data augmentation offers additional information
beyond observed data and can thus improve the information-theoretic optimal
rate of linear separation capacity. In particular, we show that self-supervised
learning can linearly separate manifolds with a smaller distance than
unsupervised learning, underscoring the additional benefits of data
augmentation. Our theoretical analysis further underscores that the performance
of downstream linear classifiers primarily hinges on the linear separability of
data representations rather than the size of the labeled data set, reaffirming
the viability of constructing efficient classifiers with limited labeled data
amid an expansive unlabeled data set.
</p>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19043" title="Abstract">arXiv:2310.19043</a> (cross-list from math.ST) [<a href="/pdf/2310.19043" title="Download PDF">pdf</a>, <a href="/format/2310.19043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Permutation Tests: Applications to Kernel Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kim%2C+I">Ilmun Kim</a>, 
<a href="/search/math?searchtype=author&query=Schrab%2C+A">Antonin Schrab</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent years have witnessed growing concerns about the privacy of sensitive
data. In response to these concerns, differential privacy has emerged as a
rigorous framework for privacy protection, gaining widespread recognition in
both academic and industrial circles. While substantial progress has been made
in private data analysis, existing methods often suffer from impracticality or
a significant loss of statistical efficiency. This paper aims to alleviate
these concerns in the context of hypothesis testing by introducing
differentially private permutation tests. The proposed framework extends
classical non-private permutation tests to private settings, maintaining both
finite-sample validity and differential privacy in a rigorous manner. The power
of the proposed test depends on the choice of a test statistic, and we
establish general conditions for consistency and non-asymptotic uniform power.
To demonstrate the utility and practicality of our framework, we focus on
reproducing kernel-based test statistics and introduce differentially private
kernel tests for two-sample and independence testing: dpMMD and dpHSIC. The
proposed kernel tests are straightforward to implement, applicable to various
types of data, and attain minimax optimal power across different privacy
regimes. Our empirical evaluations further highlight their competitive power
under various synthetic and real-world scenarios, emphasizing their practical
value. The code is publicly available to facilitate the implementation of our
framework.
</p>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19051" title="Abstract">arXiv:2310.19051</a> (cross-list from stat.ME) [<a href="/pdf/2310.19051" title="Download PDF">pdf</a>, <a href="/format/2310.19051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Methods for Estimating Hurst Exponent of Time Sequence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+H">Hong-Yan Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Feng%2C+Z">Zhi-Qiang Feng</a>, 
<a href="/search/stat?searchtype=author&query=Feng%2C+S">Si-Yu Feng</a>, 
<a href="/search/stat?searchtype=author&query=Zhou%2C+Y">Yu Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 8 figures, 4 tables, 24 algorithms with pseudo-codes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Mathematical Software (cs.MS)

</div>
<p class="mathjax">The Hurst exponent is a significant indicator for characterizing the
self-similarity and long-term memory properties of time sequences. It has wide
applications in physics, technologies, engineering, mathematics, statistics,
economics, psychology and so on. Currently, available methods for estimating
the Hurst exponent of time sequences can be divided into different categories:
time-domain methods and spectrum-domain methods based on the representation of
time sequence, linear regression methods and Bayesian methods based on
parameter estimation methods. Although various methods are discussed in
literature, there are still some deficiencies: the descriptions of the
estimation algorithms are just mathematics-oriented and the pseudo-codes are
missing; the effectiveness and accuracy of the estimation algorithms are not
clear; the classification of estimation methods is not considered and there is
a lack of guidance for selecting the estimation methods. In this work, the
emphasis is put on thirteen dominant methods for estimating the Hurst exponent.
For the purpose of decreasing the difficulty of implementing the estimation
methods with computer programs, the mathematical principles are discussed
briefly and the pseudo-codes of algorithms are presented with necessary
details. It is expected that the survey could help the researchers to select,
implement and apply the estimation algorithms of interest in practical
situations in an easy way.
</p>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19147" title="Abstract">arXiv:2310.19147</a> (cross-list from econ.TH) [<a href="/pdf/2310.19147" title="Download PDF">pdf</a>, <a href="/format/2310.19147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Scoring for Dynamic Information Acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Li%2C+Y">Yingkai Li</a>, 
<a href="/search/econ?searchtype=author&query=Libgober%2C+J">Jonathan Libgober</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">A principal seeks to learn about a binary state and can do so by enlisting an
agent to acquire information over time using a Poisson information arrival
technology. The agent learns about this state privately, and his effort choices
are unobserved by the principal. The principal can reward the agent with a
prize of fixed value as a function of the agent's sequence of reports and the
realized state. We identify conditions that each individually ensure that the
principal cannot do better than by eliciting a single report from the agent
after all information has been acquired. We also show that such a static
contract is suboptimal under sufficiently strong violations of these
conditions. We contrast our solution to the case where the agent acquires
information "all at once;" notably, the optimal contract in the dynamic
environment may provide strictly positive base rewards to the agent even if his
prediction about the state is incorrect.
</p>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19192" title="Abstract">arXiv:2310.19192</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.19192" title="Download PDF">pdf</a>, <a href="/format/2310.19192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Normalization in Recurrent Neural Network of Grid Cells
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Xu%2C+D">Dehong Xu</a>, 
<a href="/search/q-bio?searchtype=author&query=Gao%2C+R">Ruiqi Gao</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+W">Wen-Hao Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wei%2C+X">Xue-Xin Wei</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Grid cells in the entorhinal cortex of the mammalian brain exhibit striking
hexagon firing patterns in their response maps as the animal (e.g., a rat)
navigates in a 2D open environment. The responses of the population of grid
cells collectively form a vector in a high-dimensional neural activity space,
and this vector represents the self-position of the agent in the 2D physical
space. As the agent moves, the vector is transformed by a recurrent neural
network that takes the velocity of the agent as input. In this paper, we
propose a simple and general conformal normalization of the input velocity for
the recurrent neural network, so that the local displacement of the position
vector in the high-dimensional neural space is proportional to the local
displacement of the agent in the 2D physical space, regardless of the direction
of the input velocity. Our numerical experiments on the minimally simple linear
and non-linear recurrent networks show that conformal normalization leads to
the emergence of the hexagon grid patterns. Furthermore, we derive a new
theoretical understanding that connects conformal normalization to the
emergence of hexagon grid patterns in navigation tasks.
</p>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19198" title="Abstract">arXiv:2310.19198</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.19198" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Motor Imagery Decoding in Brain Computer Interfaces using  Riemann Tangent Space Mapping and Cross Frequency Coupling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Xiong%2C+X">Xiong Xiong</a>, 
<a href="/search/q-bio?searchtype=author&query=Su%2C+L">Li Su</a>, 
<a href="/search/q-bio?searchtype=author&query=Huang%2C+J">Jinguo Huang</a>, 
<a href="/search/q-bio?searchtype=author&query=Kang%2C+G">Guixia Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Objective: Motor Imagery (MI) serves as a crucial experimental paradigm
within the realm of Brain Computer Interfaces (BCIs), aiming to decoding motor
intentions from electroencephalogram (EEG) signals. Method: Drawing inspiration
from Riemannian geometry and Cross-Frequency Coupling (CFC), this paper
introduces a novel approach termed Riemann Tangent Space Mapping using
Dichotomous Filter Bank with Convolutional Neural Network (DFBRTS) to enhance
the representation quality and decoding capability pertaining to MI features.
DFBRTS first initiates the process by meticulously filtering EEG signals
through a Dichotomous Filter Bank, structured in the fashion of a complete
binary tree. Subsequently, it employs Riemann Tangent Space Mapping to extract
salient EEG signal features within each sub-band. Finally, a lightweight
convolutional neural network is employed for further feature extraction and
classification, operating under the joint supervision of cross-entropy and
center loss. To validate the efficacy, extensive experiments were conducted
using DFBRTS on two well-established benchmark datasets: the BCI competition IV
2a (BCIC-IV-2a) dataset and the OpenBMI dataset. The performance of DFBRTS was
benchmarked against several state-of-the-art MI decoding methods, alongside
other Riemannian geometry-based MI decoding approaches. Results: DFBRTS
significantly outperforms other MI decoding algorithms on both datasets,
achieving a remarkable classification accuracy of 78.16% for four-class and
71.58% for two-class hold-out classification, as compared to the existing
benchmarks.
</p>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19202" title="Abstract">arXiv:2310.19202</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.19202" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Motor Imagery Classification Using Adaptive Spatial Filters  Based on Particle Swarm Optimization Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Xiong%2C+X">Xiong Xiong</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+Y">Ying Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Song%2C+T">Tianyuan Song</a>, 
<a href="/search/q-bio?searchtype=author&query=Huang%2C+J">Jinguo Huang</a>, 
<a href="/search/q-bio?searchtype=author&query=Kang%2C+G">Guixia Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">As a typical self-paced brain-computer interface (BCI) system, the motor
imagery (MI) BCI has been widely applied in fields such as robot control,
stroke rehabilitation, and assistance for patients with stroke or spinal cord
injury. Many studies have focused on the traditional spatial filters obtained
through the common spatial pattern (CSP) method. However, the CSP method can
only obtain fixed spatial filters for specific input signals. Besides, CSP
method only focuses on the variance difference of two types of
electroencephalogram (EEG) signals, so the decoding ability of EEG signals is
limited. To obtain more effective spatial filters for better extraction of
spatial features that can improve classification to MI-EEG, this paper proposes
an adaptive spatial filter solving method based on particle swarm optimization
algorithm (PSO). A training and testing framework based on filter bank and
spatial filters (FBCSP-ASP) is designed for MI EEG signal classification.
Comparative experiments are conducted on two public datasets (2a and 2b) from
BCI competition IV, which show the outstanding average recognition accuracy of
FBCSP-ASP. The proposed method has achieved significant performance improvement
on MI-BCI. The classification accuracy of the proposed method has reached
74.61% and 81.19% on datasets 2a and 2b, respectively. Compared with the
baseline algorithm (FBCSP), the proposed algorithm improves 11.44% and 7.11% on
two datasets respectively. Furthermore, the analysis based on mutual
information, t-SNE and Shapley values further proves that ASP features have
excellent decoding ability for MI-EEG signals, and explains the improvement of
classification performance by the introduction of ASP features.
</p>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19214" title="Abstract">arXiv:2310.19214</a> (cross-list from stat.ML) [<a href="/pdf/2310.19214" title="Download PDF">pdf</a>, <a href="/format/2310.19214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factor Fitting, Rank Allocation, and Partitioning in Multilevel Low Rank  Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Parshakova%2C+T">Tetiana Parshakova</a>, 
<a href="/search/stat?searchtype=author&query=Hastie%2C+T">Trevor Hastie</a>, 
<a href="/search/stat?searchtype=author&query=Darve%2C+E">Eric Darve</a>, 
<a href="/search/stat?searchtype=author&query=Boyd%2C+S">Stephen Boyd</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Mathematical Software (cs.MS); Optimization and Control (math.OC)

</div>
<p class="mathjax">We consider multilevel low rank (MLR) matrices, defined as a row and column
permutation of a sum of matrices, each one a block diagonal refinement of the
previous one, with all blocks low rank given in factored form. MLR matrices
extend low rank matrices but share many of their properties, such as the total
storage required and complexity of matrix-vector multiplication. We address
three problems that arise in fitting a given matrix by an MLR matrix in the
Frobenius norm. The first problem is factor fitting, where we adjust the
factors of the MLR matrix. The second is rank allocation, where we choose the
ranks of the blocks in each level, subject to the total rank having a given
value, which preserves the total storage needed for the MLR matrix. The final
problem is to choose the hierarchical partition of rows and columns, along with
the ranks and factors. This paper is accompanied by an open source package that
implements the proposed methods.
</p>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19246" title="Abstract">arXiv:2310.19246</a> (cross-list from stat.ML) [<a href="/pdf/2310.19246" title="Download PDF">pdf</a>, <a href="/format/2310.19246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A spectral regularisation framework for latent variable models designed  for single channel applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Balshaw%2C+R">Ryan Balshaw</a>, 
<a href="/search/stat?searchtype=author&query=Heyns%2C+P+S">P. Stephan Heyns</a>, 
<a href="/search/stat?searchtype=author&query=Wilke%2C+D+N">Daniel N. Wilke</a>, 
<a href="/search/stat?searchtype=author&query=Schmidt%2C+S">Stephan Schmidt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages; 6 figures; 1 table; github; submitted to SoftwareX
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Latent variable models (LVMs) are commonly used to capture the underlying
dependencies, patterns, and hidden structure in observed data. Source
duplication is a by-product of the data hankelisation pre-processing step
common to single channel LVM applications, which hinders practical LVM
utilisation. In this article, a Python package titled
spectrally-regularised-LVMs is presented. The proposed package addresses the
source duplication issue via the addition of a novel spectral regularisation
term. This package provides a framework for spectral regularisation in single
channel LVM applications, thereby making it easier to investigate and utilise
LVMs with spectral regularisation. This is achieved via the use of symbolic or
explicit representations of potential LVM objective functions which are
incorporated into a framework that uses spectral regularisation during the LVM
parameter estimation process. The objective of this package is to provide a
consistent linear LVM optimisation framework which incorporates spectral
regularisation and caters to single channel time-series applications.
</p>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19276" title="Abstract">arXiv:2310.19276</a> (cross-list from hep-th) [<a href="/pdf/2310.19276" title="Download PDF">pdf</a>, <a href="/format/2310.19276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Regularization for the Minimum Volume Formula of Toric  Calabi-Yau 3-folds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-th?searchtype=author&query=Choi%2C+E">Eugene Choi</a>, 
<a href="/search/hep-th?searchtype=author&query=Seong%2C+R">Rak-Kyeong Seong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Theory (hep-th)</span>; Machine Learning (cs.LG); Mathematical Physics (math-ph); Algebraic Geometry (math.AG)

</div>
<p class="mathjax">We present a collection of explicit formulas for the minimum volume of
Sasaki-Einstein 5-manifolds. The cone over these 5-manifolds is a toric
Calabi-Yau 3-fold. These toric Calabi-Yau 3-folds are associated with an
infinite class of 4d N=1 supersymmetric gauge theories, which are realized as
worldvolume theories of D3-branes probing the toric Calabi-Yau 3-folds. Under
the AdS/CFT correspondence, the minimum volume of the Sasaki-Einstein base is
inversely proportional to the central charge of the corresponding 4d N=1
superconformal field theories. The presented formulas for the minimum volume
are in terms of geometric invariants of the toric Calabi-Yau 3-folds. These
explicit results are derived by implementing machine learning regularization
techniques that advance beyond previous applications of machine learning for
determining the minimum volume. Moreover, the use of machine learning
regularization allows us to present interpretable and explainable formulas for
the minimum volume. Our work confirms that, even for extensive sets of toric
Calabi-Yau 3-folds, the proposed formulas approximate the minimum volume with
remarkable accuracy.
</p>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19288" title="Abstract">arXiv:2310.19288</a> (cross-list from eess.IV) [<a href="/pdf/2310.19288" title="Download PDF">pdf</a>, <a href="/format/2310.19288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing  Image Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xiao%2C+Y">Yi Xiao</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+Q">Qiangqiang Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+K">Kui Jiang</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+J">Jiang He</a>, 
<a href="/search/eess?searchtype=author&query=Jin%2C+X">Xianyu Jin</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+L">Liangpei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE TGRS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recently, convolutional networks have achieved remarkable development in
remote sensing image Super-Resoltuion (SR) by minimizing the regression
objectives, e.g., MSE loss. However, despite achieving impressive performance,
these methods often suffer from poor visual quality with over-smooth issues.
Generative adversarial networks have the potential to infer intricate details,
but they are easy to collapse, resulting in undesirable artifacts. To mitigate
these issues, in this paper, we first introduce Diffusion Probabilistic Model
(DPM) for efficient remote sensing image SR, dubbed EDiffSR. EDiffSR is easy to
train and maintains the merits of DPM in generating perceptual-pleasant images.
Specifically, different from previous works using heavy UNet for noise
prediction, we develop an Efficient Activation Network (EANet) to achieve
favorable noise prediction performance by simplified channel attention and
simple gate operation, which dramatically reduces the computational budget.
Moreover, to introduce more valuable prior knowledge into the proposed EDiffSR,
a practical Conditional Prior Enhancement Module (CPEM) is developed to help
extract an enriched condition. Unlike most DPM-based SR models that directly
generate conditions by amplifying LR images, the proposed CPEM helps to retain
more informative cues for accurate SR. Extensive experiments on four remote
sensing datasets demonstrate that EDiffSR can restore visual-pleasant images on
simulated and real-world remote sensing images, both quantitatively and
qualitatively. The code of EDiffSR will be available at
https://github.com/XY-boy/EDiffSR
</p>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19293" title="Abstract">arXiv:2310.19293</a> (cross-list from eess.IV) [<a href="/pdf/2310.19293" title="Download PDF">pdf</a>, <a href="/format/2310.19293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FetusMapV2: Enhanced Fetal Pose Estimation in 3D Ultrasound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+C">Chaoyu Chen</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+X">Xin Yang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yuhao Huang</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+W">Wenlong Shi</a>, 
<a href="/search/eess?searchtype=author&query=Cao%2C+Y">Yan Cao</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+M">Mingyuan Luo</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+X">Xindi Hu</a>, 
<a href="/search/eess?searchtype=author&query=Zhue%2C+L">Lei Zhue</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+L">Lequan Yu</a>, 
<a href="/search/eess?searchtype=author&query=Yue%2C+K">Kejuan Yue</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yuanji Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Xiong%2C+Y">Yi Xiong</a>, 
<a href="/search/eess?searchtype=author&query=Ni%2C+D">Dong Ni</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+W">Weijun Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 11 figures, accepted by Medical Image Analysis(2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Fetal pose estimation in 3D ultrasound (US) involves identifying a set of
associated fetal anatomical landmarks. Its primary objective is to provide
comprehensive information about the fetus through landmark connections, thus
benefiting various critical applications, such as biometric measurements, plane
localization, and fetal movement monitoring. However, accurately estimating the
3D fetal pose in US volume has several challenges, including poor image
quality, limited GPU memory for tackling high dimensional data, symmetrical or
ambiguous anatomical structures, and considerable variations in fetal poses. In
this study, we propose a novel 3D fetal pose estimation framework (called
FetusMapV2) to overcome the above challenges. Our contribution is three-fold.
First, we propose a heuristic scheme that explores the complementary network
structure-unconstrained and activation-unreserved GPU memory management
approaches, which can enlarge the input image resolution for better results
under limited GPU memory. Second, we design a novel Pair Loss to mitigate
confusion caused by symmetrical and similar anatomical structures. It separates
the hidden classification task from the landmark localization task and thus
progressively eases model learning. Last, we propose a shape priors-based
self-supervised learning by selecting the relatively stable landmarks to refine
the pose online. Extensive experiments and diverse applications on a
large-scale fetal US dataset including 1000 volumes with 22 landmarks per
volume demonstrate that our method outperforms other strong competitors.
</p>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19300" title="Abstract">arXiv:2310.19300</a> (cross-list from stat.ML) [<a href="/pdf/2310.19300" title="Download PDF">pdf</a>, <a href="/format/2310.19300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stage-Aware Learning for Dynamic Treatments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ye%2C+H">Hanwen Ye</a>, 
<a href="/search/stat?searchtype=author&query=Zhou%2C+W">Wenzhuo Zhou</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+R">Ruoqing Zhu</a>, 
<a href="/search/stat?searchtype=author&query=Qu%2C+A">Annie Qu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in dynamic treatment regimes (DTRs) provide powerful optimal
treatment searching algorithms, which are tailored to individuals' specific
needs and able to maximize their expected clinical benefits. However, existing
algorithms could suffer from insufficient sample size under optimal treatments,
especially for chronic diseases involving long stages of decision-making. To
address these challenges, we propose a novel individualized learning method
which estimates the DTR with a focus on prioritizing alignment between the
observed treatment trajectory and the one obtained by the optimal regime across
decision stages. By relaxing the restriction that the observed trajectory must
be fully aligned with the optimal treatments, our approach substantially
improves the sample efficiency and stability of inverse probability weighted
based methods. In particular, the proposed learning scheme builds a more
general framework which includes the popular outcome weighted learning
framework as a special case of ours. Moreover, we introduce the notion of stage
importance scores along with an attention mechanism to explicitly account for
heterogeneity among decision stages. We establish the theoretical properties of
the proposed approach, including the Fisher consistency and finite-sample
performance bound. Empirically, we evaluate the proposed method in extensive
simulated environments and a real case study for COVID-19 pandemic.
</p>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19306" title="Abstract">arXiv:2310.19306</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2310.19306" title="Download PDF">pdf</a>, <a href="/format/2310.19306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Planning-and-Exploring Approach to Extreme-Mechanics Force Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Shi%2C+P">Pengjie Shi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Xu%2C+Z">Zhiping Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG)

</div>
<p class="mathjax">Extreme mechanical processes such as strong lattice distortion and bond
breakage during fracture are ubiquitous in nature and engineering, which often
lead to catastrophic failure of structures. However, understanding the
nucleation and growth of cracks is challenged by their multiscale
characteristics spanning from atomic-level structures at the crack tip to the
structural features where the load is applied. Molecular simulations offer an
important tool to resolve the progressive microstructural changes at crack
fronts and are widely used to explore processes therein, such as mechanical
energy dissipation, crack path selection, and dynamic instabilities (e.g.,
kinking, branching). Empirical force fields developed based on local
descriptors based on atomic positions and the bond orders do not yield
satisfying predictions of fracture, even for the nonlinear, anisotropic
stress-strain relations and the energy densities of edges. High-fidelity force
fields thus should include the tensorial nature of strain and the energetics of
rare events during fracture, which, unfortunately, have not been taken into
account in both the state-of-the-art empirical and machine-learning force
fields. Based on data generated by first-principles calculations, we develop a
neural network-based force field for fracture, NN-F$^3$, by combining
pre-sampling of the space of strain states and active-learning techniques to
explore the transition states at critical bonding distances. The capability of
NN-F$^3$ is demonstrated by studying the rupture of h-BN and twisted bilayer
graphene as model problems. The simulation results confirm recent experimental
findings and highlight the necessity to include the knowledge of electronic
structures from first-principles calculations in predicting extreme mechanical
processes.
</p>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19319" title="Abstract">arXiv:2310.19319</a> (cross-list from stat.ML) [<a href="/pdf/2310.19319" title="Download PDF">pdf</a>, <a href="/format/2310.19319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-Directed Algorithm Design for Efficient Pure Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Qin%2C+C">Chao Qin</a>, 
<a href="/search/stat?searchtype=author&query=You%2C+W">Wei You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An earlier version of this paper appeared as an extended abstract in the Proceedings of the 36th Annual Conference on Learning Theory, COLT'23, with the title "Information-Directed Selection for Top-Two Algorithms.''
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider pure-exploration problems in the context of stochastic sequential
adaptive experiments with a finite set of alternative options. The goal of the
decision-maker is to accurately answer a query question regarding the
alternatives with high confidence with minimal measurement efforts. A typical
query question is to identify the alternative with the best performance,
leading to ranking and selection problems, or best-arm identification in the
machine learning literature. We focus on the fixed-precision setting and derive
a sufficient condition for optimality in terms of a notion of strong
convergence to the optimal allocation of samples. Using dual variables, we
characterize the necessary and sufficient conditions for an allocation to be
optimal. The use of dual variables allow us to bypass the combinatorial
structure of the optimality conditions that relies solely on primal variables.
Remarkably, these optimality conditions enable an extension of top-two
algorithm design principle, initially proposed for best-arm identification.
Furthermore, our optimality conditions give rise to a straightforward yet
efficient selection rule, termed information-directed selection, which
adaptively picks from a candidate set based on information gain of the
candidates. We outline the broad contexts where our algorithmic approach can be
implemented. We establish that, paired with information-directed selection,
top-two Thompson sampling is (asymptotically) optimal for Gaussian best-arm
identification, solving a glaring open problem in the pure exploration
literature. Our algorithm is optimal for $\epsilon$-best-arm identification and
thresholding bandit problems. Our analysis also leads to a general principle to
guide adaptations of Thompson sampling for pure-exploration problems. Numerical
experiments highlight the exceptional efficiency of our proposed algorithms
relative to existing ones.
</p>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19384" title="Abstract">arXiv:2310.19384</a> (cross-list from stat.ML) [<a href="/pdf/2310.19384" title="Download PDF">pdf</a>, <a href="/format/2310.19384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep anytime-valid hypothesis testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Pandeva%2C+T">Teodora Pandeva</a>, 
<a href="/search/stat?searchtype=author&query=Forr%C3%A9%2C+P">Patrick Forr&#xe9;</a>, 
<a href="/search/stat?searchtype=author&query=Ramdas%2C+A">Aaditya Ramdas</a>, 
<a href="/search/stat?searchtype=author&query=Shekhar%2C+S">Shubhanshu Shekhar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a general framework for constructing powerful, sequential
hypothesis tests for a large class of nonparametric testing problems. The null
hypothesis for these problems is defined in an abstract form using the action
of two known operators on the data distribution. This abstraction allows for a
unified treatment of several classical tasks, such as two-sample testing,
independence testing, and conditional-independence testing, as well as modern
problems, such as testing for adversarial robustness of machine learning (ML)
models. Our proposed framework has the following advantages over classical
batch tests: 1) it continuously monitors online data streams and efficiently
aggregates evidence against the null, 2) it provides tight control over the
type I error without the need for multiple testing correction, 3) it adapts the
sample size requirement to the unknown hardness of the problem. We develop a
principled approach of leveraging the representation capability of ML models
within the testing-by-betting framework, a game-theoretic approach for
designing sequential tests. Empirical results on synthetic and real-world
datasets demonstrate that tests instantiated using our general framework are
competitive against specialized baselines on several tasks.
</p>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19385" title="Abstract">arXiv:2310.19385</a> (cross-list from physics.comp-ph) [<a href="/pdf/2310.19385" title="Download PDF">pdf</a>, <a href="/format/2310.19385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-free online learning of subgrid-scale dynamics with neural  emulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Frezat%2C+H">Hugo Frezat</a>, 
<a href="/search/physics?searchtype=author&query=Balarac%2C+G">Guillaume Balarac</a>, 
<a href="/search/physics?searchtype=author&query=Sommer%2C+J+L">Julien Le Sommer</a>, 
<a href="/search/physics?searchtype=author&query=Fablet%2C+R">Ronan Fablet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures, submitted for publication in Journal of Computational Physics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">In this paper, we propose a generic algorithm to train machine learning-based
subgrid parametrizations online, i.e., with $\textit{a posteriori}$ loss
functions for non-differentiable numerical solvers. The proposed approach
leverage neural emulators to train an approximation of the reduced state-space
solver, which is then used to allows gradient propagation through temporal
integration steps. The algorithm is able to recover most of the benefit of
online strategies without having to compute the gradient of the original
solver. It is demonstrated that training the neural emulator and
parametrization components separately with respective loss quantities is
necessary in order to minimize the propagation of some approximation bias.
</p>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19390" title="Abstract">arXiv:2310.19390</a> (cross-list from stat.ML) [<a href="/pdf/2310.19390" title="Download PDF">pdf</a>, <a href="/format/2310.19390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Manifold Gaussian Process Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Fichera%2C+B">Bernardo Fichera</a>, 
<a href="/search/stat?searchtype=author&query=Borovitskiy%2C+V">Viacheslav Borovitskiy</a>, 
<a href="/search/stat?searchtype=author&query=Krause%2C+A">Andreas Krause</a>, 
<a href="/search/stat?searchtype=author&query=Billard%2C+A">Aude Billard</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Gaussian process regression is widely used because of its ability to provide
well-calibrated uncertainty estimates and handle small or sparse datasets.
However, it struggles with high-dimensional data. One possible way to scale
this technique to higher dimensions is to leverage the implicit low-dimensional
manifold upon which the data actually lies, as postulated by the manifold
hypothesis. Prior work ordinarily requires the manifold structure to be
explicitly provided though, i.e. given by a mesh or be known to be one of the
well-known manifolds like the sphere. In contrast, in this paper we propose a
Gaussian process regression technique capable of inferring implicit structure
directly from data (labeled and unlabeled) in a fully differentiable way. For
the resulting model, we discuss its convergence to the Mat\'ern Gaussian
process on the assumed manifold. Our technique scales up to hundreds of
thousands of data points, and may improve the predictive performance and
calibration of the standard Gaussian process regression in
high-dimensional~settings.
</p>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19392" title="Abstract">arXiv:2310.19392</a> (cross-list from eess.IV) [<a href="/pdf/2310.19392" title="Download PDF">pdf</a>, <a href="/format/2310.19392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Clinical Guideline Driven Automated Linear Feature Extraction for  Vestibular Schwannoma
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wijethilake%2C+N">Navodini Wijethilake</a>, 
<a href="/search/eess?searchtype=author&query=Connor%2C+S">Steve Connor</a>, 
<a href="/search/eess?searchtype=author&query=Oviedova%2C+A">Anna Oviedova</a>, 
<a href="/search/eess?searchtype=author&query=Burger%2C+R">Rebecca Burger</a>, 
<a href="/search/eess?searchtype=author&query=Vercauteren%2C+T">Tom Vercauteren</a>, 
<a href="/search/eess?searchtype=author&query=Shapey%2C+J">Jonathan Shapey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SPIE Medical Imaging
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Vestibular Schwannoma is a benign brain tumour that grows from one of the
balance nerves. Patients may be treated by surgery, radiosurgery or with a
conservative "wait-and-scan" strategy. Clinicians typically use manually
extracted linear measurements to aid clinical decision making. This work aims
to automate and improve this process by using deep learning based segmentation
to extract relevant clinical features through computational algorithms. To the
best of our knowledge, our study is the first to propose an automated approach
to replicate local clinical guidelines. Our deep learning based segmentation
provided Dice-scores of 0.8124 +- 0.2343 and 0.8969 +- 0.0521 for extrameatal
and whole tumour regions respectively for T2 weighted MRI, whereas 0.8222 +-
0.2108 and 0.9049 +- 0.0646 were obtained for T1 weighted MRI. We propose a
novel algorithm to choose and extract the most appropriate maximum linear
measurement from the segmented regions based on the size of the extrameatal
portion of the tumour. Using this tool, clinicians will be provided with a
visual guide and related metrics relating to tumour progression that will
function as a clinical decision aid. In this study, we utilize 187 scans
obtained from 50 patients referred to a tertiary specialist neurosurgical
service in the United Kingdom. The measurements extracted manually by an expert
neuroradiologist indicated a significant correlation with the automated
measurements (p &lt; 0.0001).
</p>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19411" title="Abstract">arXiv:2310.19411</a> (cross-list from eess.IV) [<a href="/pdf/2310.19411" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Breast Cancer Diagnosis with Heuristic-assisted  Trans-Res-U-Net and Multiscale DenseNet using Mammogram Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yaqub%2C+M">Muhammad Yaqub</a>, 
<a href="/search/eess?searchtype=author&query=Jinchao%2C+F">Feng Jinchao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 17 figures, 4 Tables and Appendix A: Supplementary Material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Breast cancer (BC) significantly contributes to cancer-related mortality in
women, underscoring the criticality of early detection for optimal patient
outcomes. A mammography is a key tool for identifying and diagnosing breast
abnormalities; however, accurately distinguishing malignant mass lesions
remains challenging. To address this issue, we propose a novel deep learning
approach for BC screening utilizing mammography images. Our proposed model
comprises three distinct stages: data collection from established benchmark
sources, image segmentation employing an Atrous Convolution-based Attentive and
Adaptive Trans-Res-UNet (ACA-ATRUNet) architecture, and BC identification via
an Atrous Convolution-based Attentive and Adaptive Multi-scale DenseNet
(ACA-AMDN) model. The hyperparameters within the ACA-ATRUNet and ACA-AMDN
models are optimised using the Modified Mussel Length-based Eurasian
Oystercatcher Optimization (MML-EOO) algorithm. Performance evaluation,
leveraging multiple metrics, is conducted, and a comparative analysis against
conventional methods is presented. Our experimental findings reveal that the
proposed BC detection framework attains superior precision rates in early
disease detection, demonstrating its potential to enhance mammography-based
screening methodologies.
</p>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19419" title="Abstract">arXiv:2310.19419</a> (cross-list from nucl-th) [<a href="/pdf/2310.19419" title="Download PDF">pdf</a>, <a href="/format/2310.19419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eigenvector Continuation and Projection-Based Emulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nucl-th?searchtype=author&query=Duguet%2C+T">Thomas Duguet</a>, 
<a href="/search/nucl-th?searchtype=author&query=Ekstr%C3%B6m%2C+A">Andreas Ekstr&#xf6;m</a>, 
<a href="/search/nucl-th?searchtype=author&query=Furnstahl%2C+R+J">Richard J. Furnstahl</a>, 
<a href="/search/nucl-th?searchtype=author&query=K%C3%B6nig%2C+S">Sebastian K&#xf6;nig</a>, 
<a href="/search/nucl-th?searchtype=author&query=Lee%2C+D">Dean Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Nuclear Theory (nucl-th)</span>; Numerical Analysis (math.NA); Nuclear Experiment (nucl-ex); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Eigenvector continuation is a computational method for parametric eigenvalue
problems that uses subspace projection with a basis derived from eigenvector
snapshots from different parameter sets. It is part of a broader class of
subspace-projection techniques called reduced-basis methods. In this colloquium
article, we present the development, theory, and applications of eigenvector
continuation and projection-based emulators. We introduce the basic concepts,
discuss the underlying theory and convergence properties, and present recent
applications for quantum systems and future prospects.
</p>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19445" title="Abstract">arXiv:2310.19445</a> (cross-list from eess.IV) [<a href="/pdf/2310.19445" title="Download PDF">pdf</a>, <a href="/format/2310.19445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Federated Learning Framework for Stenosis Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Di+Cosmo%2C+M">Mariachiara Di Cosmo</a>, 
<a href="/search/eess?searchtype=author&query=Migliorelli%2C+G">Giovanna Migliorelli</a>, 
<a href="/search/eess?searchtype=author&query=Francioni%2C+M">Matteo Francioni</a>, 
<a href="/search/eess?searchtype=author&query=Mucaj%2C+A">Andi Mucaj</a>, 
<a href="/search/eess?searchtype=author&query=Maolo%2C+A">Alessandro Maolo</a>, 
<a href="/search/eess?searchtype=author&query=Aprile%2C+A">Alessandro Aprile</a>, 
<a href="/search/eess?searchtype=author&query=Frontoni%2C+E">Emanuele Frontoni</a>, 
<a href="/search/eess?searchtype=author&query=Fiorentino%2C+M+C">Maria Chiara Fiorentino</a>, 
<a href="/search/eess?searchtype=author&query=Moccia%2C+S">Sara Moccia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This study explores the use of Federated Learning (FL) for stenosis detection
in coronary angiography images (CA). Two heterogeneous datasets from two
institutions were considered: Dataset 1 includes 1219 images from 200 patients,
which we acquired at the Ospedale Riuniti of Ancona (Italy); Dataset 2 includes
7492 sequential images from 90 patients from a previous study available in the
literature. Stenosis detection was performed by using a Faster R-CNN model. In
our FL framework, only the weights of the model backbone were shared among the
two client institutions, using Federated Averaging (FedAvg) for weight
aggregation. We assessed the performance of stenosis detection using Precision
(P rec), Recall (Rec), and F1 score (F1). Our results showed that the FL
framework does not substantially affects clients 2 performance, which already
achieved good performance with local training; for client 1, instead, FL
framework increases the performance with respect to local model of +3.76%,
+17.21% and +10.80%, respectively, reaching P rec = 73.56, Rec = 67.01 and F1 =
70.13. With such results, we showed that FL may enable multicentric studies
relevant to automatic stenosis detection in CA by addressing data heterogeneity
from various institutions, while preserving patient privacy.
</p>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19450" title="Abstract">arXiv:2310.19450</a> (cross-list from stat.ML) [<a href="/pdf/2310.19450" title="Download PDF">pdf</a>, <a href="/format/2310.19450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hodge-Compositional Edge Gaussian Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yang%2C+M">Maosheng Yang</a>, 
<a href="/search/stat?searchtype=author&query=Borovitskiy%2C+V">Viacheslav Borovitskiy</a>, 
<a href="/search/stat?searchtype=author&query=Isufi%2C+E">Elvin Isufi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose principled Gaussian processes (GPs) for modeling functions defined
over the edge set of a simplicial 2-complex, a structure similar to a graph in
which edges may form triangular faces. This approach is intended for learning
flow-type data on networks where edge flows can be characterized by the
discrete divergence and curl. Drawing upon the Hodge decomposition, we first
develop classes of divergence-free and curl-free edge GPs, suitable for various
applications. We then combine them to create \emph{Hodge-compositional edge
GPs} that are expressive enough to represent any edge function. These GPs
facilitate direct and independent learning for the different Hodge components
of edge functions, enabling us to capture their relevance during hyperparameter
optimization. To highlight their practical potential, we apply them for flow
data inference in currency exchange, ocean flows and water supply networks,
comparing them to alternative models.
</p>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19491" title="Abstract">arXiv:2310.19491</a> (cross-list from math.ST) [<a href="/pdf/2310.19491" title="Download PDF">pdf</a>, <a href="/ps/2310.19491" title="Download PostScript">ps</a>, <a href="/format/2310.19491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generator Identification for Linear SDEs with Additive and  Multiplicative Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">Yuanyuan Wang</a>, 
<a href="/search/math?searchtype=author&query=Geng%2C+X">Xi Geng</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+W">Wei Huang</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+B">Biwei Huang</a>, 
<a href="/search/math?searchtype=author&query=Gong%2C+M">Mingming Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we present conditions for identifying the generator of a
linear stochastic differential equation (SDE) from the distribution of its
solution process with a given fixed initial state. These identifiability
conditions are crucial in causal inference using linear SDEs as they enable the
identification of the post-intervention distributions from its observational
distribution. Specifically, we derive a sufficient and necessary condition for
identifying the generator of linear SDEs with additive noise, as well as a
sufficient condition for identifying the generator of linear SDEs with
multiplicative noise. We show that the conditions derived for both types of
SDEs are generic. Moreover, we offer geometric interpretations of the derived
identifiability conditions to enhance their understanding. To validate our
theoretical results, we perform a series of simulations, which support and
substantiate the established findings.
</p>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19513" title="Abstract">arXiv:2310.19513</a> (cross-list from q-bio.BM) [<a href="/pdf/2310.19513" title="Download PDF">pdf</a>, <a href="/format/2310.19513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse folding for antibody sequence design using deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Dreyer%2C+F+A">Fr&#xe9;d&#xe9;ric A. Dreyer</a>, 
<a href="/search/q-bio?searchtype=author&query=Cutting%2C+D">Daniel Cutting</a>, 
<a href="/search/q-bio?searchtype=author&query=Schneider%2C+C">Constantin Schneider</a>, 
<a href="/search/q-bio?searchtype=author&query=Kenlay%2C+H">Henry Kenlay</a>, 
<a href="/search/q-bio?searchtype=author&query=Deane%2C+C+M">Charlotte M. Deane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 ICML Workshop on Computational Biology, model weights available at <a href="https://zenodo.org/record/8164693">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We consider the problem of antibody sequence design given 3D structural
information. Building on previous work, we propose a fine-tuned inverse folding
model that is specifically optimised for antibody structures and outperforms
generic protein models on sequence recovery and structure robustness when
applied on antibodies, with notable improvement on the hypervariable CDR-H3
loop. We study the canonical conformations of complementarity-determining
regions and find improved encoding of these loops into known clusters. Finally,
we consider the applications of our model to drug discovery and binder design
and evaluate the quality of proposed sequences using physics-based methods.
</p>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19515" title="Abstract">arXiv:2310.19515</a> (cross-list from physics.ao-ph) [<a href="/pdf/2310.19515" title="Download PDF">pdf</a>, <a href="/format/2310.19515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-based nowcasting of radar composites from satellite images  for severe weather
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=K%C3%BC%C3%A7%C3%BCk%2C+%C3%87">&#xc7;a&#x11f;lar K&#xfc;&#xe7;&#xfc;k</a>, 
<a href="/search/physics?searchtype=author&query=Giannakos%2C+A">Apostolos Giannakos</a>, 
<a href="/search/physics?searchtype=author&query=Schneider%2C+S">Stefan Schneider</a>, 
<a href="/search/physics?searchtype=author&query=Jann%2C+A">Alexander Jann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures, and further supplementary figures. Submitted to Artificial Intelligence for Earth Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Weather radar data are critical for nowcasting and an integral component of
numerical weather prediction models. While weather radar data provide valuable
information at high resolution, their ground-based nature limits their
availability, which impedes large-scale applications. In contrast,
meteorological satellites cover larger domains but with coarser resolution.
<br />However, with the rapid advancements in data-driven methodologies and modern
sensors aboard geostationary satellites, new opportunities are emerging to
bridge the gap between ground- and space-based observations, ultimately leading
to more skillful weather prediction with high accuracy.
<br />Here, we present a Transformer-based model for nowcasting ground-based radar
image sequences using satellite data up to two hours lead time. Trained on a
dataset reflecting severe weather conditions, the model predicts radar fields
occurring under different weather phenomena and shows robustness against
rapidly growing/decaying fields and complex field structures.
<br />Model interpretation reveals that the infrared channel centered at 10.3 $\mu
m$ (C13) contains skillful information for all weather conditions, while
lightning data have the highest relative feature importance in severe weather
conditions, particularly in shorter lead times.
<br />The model can support precipitation nowcasting across large domains without
an explicit need for radar towers, enhance numerical weather prediction and
hydrological models, and provide radar proxy for data-scarce regions. Moreover,
the open-source framework facilitates progress towards operational data-driven
nowcasting.
</p>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19548" title="Abstract">arXiv:2310.19548</a> (cross-list from math.OC) [<a href="/pdf/2310.19548" title="Download PDF">pdf</a>, <a href="/format/2310.19548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation Theory, Computing, and Deep Learning on the Wasserstein  Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fornasier%2C+M">Massimo Fornasier</a>, 
<a href="/search/math?searchtype=author&query=Heid%2C+P">Pascal Heid</a>, 
<a href="/search/math?searchtype=author&query=Sodini%2C+G+E">Giacomo Enrico Sodini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 59 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Functional Analysis (math.FA)

</div>
<p class="mathjax">The challenge of approximating functions in infinite-dimensional spaces from
finite samples is widely regarded as formidable. In this study, we delve into
the challenging problem of the numerical approximation of Sobolev-smooth
functions defined on probability spaces. Our particular focus centers on the
Wasserstein distance function, which serves as a relevant example. In contrast
to the existing body of literature focused on approximating efficiently
pointwise evaluations, we chart a new course to define functional approximants
by adopting three machine learning-based approaches: 1. Solving a finite number
of optimal transport problems and computing the corresponding Wasserstein
potentials. 2. Employing empirical risk minimization with Tikhonov
regularization in Wasserstein Sobolev spaces. 3. Addressing the problem through
the saddle point formulation that characterizes the weak form of the Tikhonov
functional's Euler-Lagrange equation. As a theoretical contribution, we furnish
explicit and quantitative bounds on generalization errors for each of these
solutions. In the proofs, we leverage the theory of metric Sobolev spaces and
we combine it with techniques of optimal transport, variational calculus, and
large deviation bounds. In our numerical implementation, we harness
appropriately designed neural networks to serve as basis functions. These
networks undergo training using diverse methodologies. This approach allows us
to obtain approximating functions that can be rapidly evaluated after training.
Consequently, our constructive solutions significantly enhance at equal
accuracy the evaluation speed, surpassing that of state-of-the-art methods by
several orders of magnitude.
</p>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19581" title="Abstract">arXiv:2310.19581</a> (cross-list from eess.AS) [<a href="/pdf/2310.19581" title="Download PDF">pdf</a>, <a href="/format/2310.19581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeing Through the Conversation: Audio-Visual Speech Separation based on  Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lee%2C+S">Suyeon Lee</a>, 
<a href="/search/eess?searchtype=author&query=Jung%2C+C">Chaeyoung Jung</a>, 
<a href="/search/eess?searchtype=author&query=Jang%2C+Y">Youngjoon Jang</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+J">Jaehun Kim</a>, 
<a href="/search/eess?searchtype=author&query=Chung%2C+J+S">Joon Son Chung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page with demo: <a href="https://mm.kaist.ac.kr/projects/avdiffuss/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computer Vision and Pattern Recognition (cs.CV); Sound (cs.SD)

</div>
<p class="mathjax">The objective of this work is to extract target speaker's voice from a
mixture of voices using visual cues. Existing works on audio-visual speech
separation have demonstrated their performance with promising intelligibility,
but maintaining naturalness remains a challenge. To address this issue, we
propose AVDiffuSS, an audio-visual speech separation model based on a diffusion
mechanism known for its capability in generating natural samples. For an
effective fusion of the two modalities for diffusion, we also propose a
cross-attention-based feature fusion mechanism. This mechanism is specifically
tailored for the speech domain to integrate the phonetic information from
audio-visual correspondence in speech generation. In this way, the fusion
process maintains the high temporal resolution of the features, without
excessive computational requirements. We demonstrate that the proposed
framework achieves state-of-the-art results on two benchmarks, including
VoxCeleb2 and LRS3, producing speech with notably better naturalness.
</p>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19614" title="Abstract">arXiv:2310.19614</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.19614" title="Download PDF">pdf</a>, <a href="/format/2310.19614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dis-inhibitory neuronal circuits can control the sign of synaptic  plasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Rossbroich%2C+J">Julian Rossbroich</a>, 
<a href="/search/q-bio?searchtype=author&query=Zenke%2C+F">Friedemann Zenke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">How neuronal circuits achieve credit assignment remains a central unsolved
question in systems neuroscience. Various studies have suggested plausible
solutions for back-propagating error signals through multi-layer networks.
These purely functionally motivated models assume distinct neuronal
compartments to represent local error signals that determine the sign of
synaptic plasticity. However, this explicit error modulation is inconsistent
with phenomenological plasticity models in which the sign depends primarily on
postsynaptic activity. Here we show how a plausible microcircuit model and
Hebbian learning rule derived within an adaptive control theory framework can
resolve this discrepancy. Assuming errors are encoded in top-down
dis-inhibitory synaptic afferents, we show that error-modulated learning
emerges naturally at the circuit level when recurrent inhibition explicitly
influences Hebbian plasticity. The same learning rule accounts for
experimentally observed plasticity in the absence of inhibition and performs
comparably to back-propagation of error (BP) on several non-linearly separable
benchmarks. Our findings bridge the gap between functional and experimentally
observed plasticity rules and make concrete predictions on inhibitory
modulation of excitatory plasticity.
</p>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19644" title="Abstract">arXiv:2310.19644</a> (cross-list from eess.AS) [<a href="/pdf/2310.19644" title="Download PDF">pdf</a>, <a href="/format/2310.19644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scenario-Aware Audio-Visual TF-GridNet for Target Speech Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pan%2C+Z">Zexu Pan</a>, 
<a href="/search/eess?searchtype=author&query=Wichern%2C+G">Gordon Wichern</a>, 
<a href="/search/eess?searchtype=author&query=Masuyama%2C+Y">Yoshiki Masuyama</a>, 
<a href="/search/eess?searchtype=author&query=Germain%2C+F+G">Francois G. Germain</a>, 
<a href="/search/eess?searchtype=author&query=Khurana%2C+S">Sameer Khurana</a>, 
<a href="/search/eess?searchtype=author&query=Hori%2C+C">Chiori Hori</a>, 
<a href="/search/eess?searchtype=author&query=Roux%2C+J+L">Jonathan Le Roux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Target speech extraction aims to extract, based on a given conditioning cue,
a target speech signal that is corrupted by interfering sources, such as noise
or competing speakers. Building upon the achievements of the state-of-the-art
(SOTA) time-frequency speaker separation model TF-GridNet, we propose
AV-GridNet, a visual-grounded variant that incorporates the face recording of a
target speaker as a conditioning factor during the extraction process.
Recognizing the inherent dissimilarities between speech and noise signals as
interfering sources, we also propose SAV-GridNet, a scenario-aware model that
identifies the type of interfering scenario first and then applies a dedicated
expert model trained specifically for that scenario. Our proposed model
achieves SOTA results on the second COG-MHEAR Audio-Visual Speech Enhancement
Challenge, outperforming other models by a significant margin, objectively and
in a listening test. We also perform an extensive analysis of the results under
the two scenarios.
</p>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19653" title="Abstract">arXiv:2310.19653</a> (cross-list from stat.ML) [<a href="/pdf/2310.19653" title="Download PDF">pdf</a>, <a href="/format/2310.19653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xiao%2C+T+Z">Tim Z. Xiao</a>, 
<a href="/search/stat?searchtype=author&query=Zenn%2C+J">Johannes Zenn</a>, 
<a href="/search/stat?searchtype=author&query=Bamler%2C+R">Robert Bamler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages + appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Variational autoencoders (VAEs) are popular models for representation
learning but their encoders are susceptible to overfitting (Cremer et al.,
2018) because they are trained on a finite training set instead of the true
(continuous) data distribution $p_{\mathrm{data}}(\mathbf{x})$. Diffusion
models, on the other hand, avoid this issue by keeping the encoder fixed. This
makes their representations less interpretable, but it simplifies training,
enabling accurate and continuous approximations of
$p_{\mathrm{data}}(\mathbf{x})$. In this paper, we show that overfitting
encoders in VAEs can be effectively mitigated by training on samples from a
pre-trained diffusion model. These results are somewhat unexpected as recent
findings (Alemohammad et al., 2023; Shumailov et al., 2023) observe a decay in
generative performance when models are trained on data generated by another
generative model. We analyze generalization performance, amortization gap, and
robustness of VAEs trained with our proposed method on three different data
sets. We find improvements in all metrics compared to both normal training and
conventional data augmentation methods, and we show that a modest amount of
samples from the diffusion model suffices to obtain these gains.
</p>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19656" title="Abstract">arXiv:2310.19656</a> (cross-list from eess.IV) [<a href="/pdf/2310.19656" title="Download PDF">pdf</a>, <a href="/format/2310.19656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Generalization in Computational Pathology: Survey and Guidelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jahanifar%2C+M">Mostafa Jahanifar</a>, 
<a href="/search/eess?searchtype=author&query=Raza%2C+M">Manahil Raza</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+K">Kesi Xu</a>, 
<a href="/search/eess?searchtype=author&query=Vuong%2C+T">Trinh Vuong</a>, 
<a href="/search/eess?searchtype=author&query=Jewsbury%2C+R">Rob Jewsbury</a>, 
<a href="/search/eess?searchtype=author&query=Shephard%2C+A">Adam Shephard</a>, 
<a href="/search/eess?searchtype=author&query=Zamanitajeddin%2C+N">Neda Zamanitajeddin</a>, 
<a href="/search/eess?searchtype=author&query=Kwak%2C+J+T">Jin Tae Kwak</a>, 
<a href="/search/eess?searchtype=author&query=Raza%2C+S+E+A">Shan E Ahmed Raza</a>, 
<a href="/search/eess?searchtype=author&query=Minhas%2C+F">Fayyaz Minhas</a>, 
<a href="/search/eess?searchtype=author&query=Rajpoot%2C+N">Nasir Rajpoot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep learning models have exhibited exceptional effectiveness in
Computational Pathology (CPath) by tackling intricate tasks across an array of
histology image analysis applications. Nevertheless, the presence of
out-of-distribution data (stemming from a multitude of sources such as
disparate imaging devices and diverse tissue preparation methods) can cause
\emph{domain shift} (DS). DS decreases the generalization of trained models to
unseen datasets with slightly different data distributions, prompting the need
for innovative \emph{domain generalization} (DG) solutions. Recognizing the
potential of DG methods to significantly influence diagnostic and prognostic
models in cancer studies and clinical practice, we present this survey along
with guidelines on achieving DG in CPath. We rigorously define various DS
types, systematically review and categorize existing DG approaches and
resources in CPath, and provide insights into their advantages, limitations,
and applicability. We also conduct thorough benchmarking experiments with 28
cutting-edge DG algorithms to address a complex DG problem. Our findings
suggest that careful experiment design and CPath-specific Stain Augmentation
technique can be very effective. However, there is no one-size-fits-all
solution for DG in CPath. Therefore, we establish clear guidelines for
detecting and managing DS depending on different scenarios. While most of the
concepts, guidelines, and recommendations are given for applications in CPath,
we believe that they are applicable to most medical image analysis tasks as
well.
</p>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19675" title="Abstract">arXiv:2310.19675</a> (cross-list from eess.IV) [<a href="/pdf/2310.19675" title="Download PDF">pdf</a>, <a href="/format/2310.19675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Principled Hierarchical Deep Learning Approach to Joint Image  Compression and Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qi%2C+S">Siyu Qi</a>, 
<a href="/search/eess?searchtype=author&query=Wijesinghe%2C+A">Achintha Wijesinghe</a>, 
<a href="/search/eess?searchtype=author&query=Chamain%2C+L+D">Lahiru D. Chamain</a>, 
<a href="/search/eess?searchtype=author&query=Ding%2C+Z">Zhi Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Among applications of deep learning (DL) involving low cost sensors, remote
image classification involves a physical channel that separates edge sensors
and cloud classifiers. Traditional DL models must be divided between an encoder
for the sensor and the decoder + classifier at the edge server. An important
challenge is to effectively train such distributed models when the connecting
channels have limited rate/capacity. Our goal is to optimize DL models such
that the encoder latent requires low channel bandwidth while still delivers
feature information for high classification accuracy. This work proposes a
three-step joint learning strategy to guide encoders to extract features that
are compact, discriminative, and amenable to common
augmentations/transformations. We optimize latent dimension through an initial
screening phase before end-to-end (E2E) training. To obtain an adjustable bit
rate via a single pre-deployed encoder, we apply entropy-based quantization
and/or manual truncation on the latent representations. Tests show that our
proposed method achieves accuracy improvement of up to 1.5% on CIFAR-10 and 3%
on CIFAR-100 over conventional E2E cross-entropy training.
</p>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19683" title="Abstract">arXiv:2310.19683</a> (cross-list from stat.ML) [<a href="/pdf/2310.19683" title="Download PDF">pdf</a>, <a href="/ps/2310.19683" title="Download PostScript">ps</a>, <a href="/format/2310.19683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Online Bootstrap for Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Palm%2C+N">Nicolai Palm</a>, 
<a href="/search/stat?searchtype=author&query=Nagler%2C+T">Thomas Nagler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)

</div>
<p class="mathjax">Resampling methods such as the bootstrap have proven invaluable in the field
of machine learning. However, the applicability of traditional bootstrap
methods is limited when dealing with large streams of dependent data, such as
time series or spatially correlated observations. In this paper, we propose a
novel bootstrap method that is designed to account for data dependencies and
can be executed online, making it particularly suitable for real-time
applications. This method is based on an autoregressive sequence of
increasingly dependent resampling weights. We prove the theoretical validity of
the proposed bootstrap scheme under general conditions. We demonstrate the
effectiveness of our approach through extensive simulations and show that it
provides reliable uncertainty quantification even in the presence of complex
data dependencies. Our work bridges the gap between classical resampling
techniques and the demands of modern data analysis, providing a valuable tool
for researchers and practitioners in dynamic, data-rich environments.
</p>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19700" title="Abstract">arXiv:2310.19700</a> (cross-list from math-ph) [<a href="/pdf/2310.19700" title="Download PDF">pdf</a>, <a href="/format/2310.19700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kinetic description and macroscopic limit of swarming dynamics with  continuous leader-follower transitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math-ph?searchtype=author&query=Cristiani%2C+E">Emiliano Cristiani</a>, 
<a href="/search/math-ph?searchtype=author&query=Loy%2C+N">Nadia Loy</a>, 
<a href="/search/math-ph?searchtype=author&query=Menci%2C+M">Marta Menci</a>, 
<a href="/search/math-ph?searchtype=author&query=Tosin%2C+A">Andrea Tosin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Physics (math-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper, we derive a kinetic description of swarming particle dynamics
in an interacting multi-agent system featuring emerging leaders and followers.
Agents are classically characterized by their position and velocity plus a
continuous parameter quantifying their degree of leadership. The microscopic
processes ruling the change of velocity and degree of leadership are
independent, non-conservative and non-local in the physical space, so as to
account for long-range interactions. Out of the kinetic description, we obtain
then a macroscopic model under a hydrodynamic limit reminiscent of that used to
tackle the hydrodynamics of weakly dissipative granular gases, thus relying in
particular on a regime of small non-conservative and short-range interactions.
Numerical simulations in one- and two-dimensional domains show that the
limiting macroscopic model is consistent with the original particle dynamics
and furthermore can reproduce classical emerging patterns typically observed in
swarms.
</p>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19711" title="Abstract">arXiv:2310.19711</a> (cross-list from math.CO) [<a href="/pdf/2310.19711" title="Download PDF">pdf</a>, <a href="/format/2310.19711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flip Graph Connectivity for Arrangements of Pseudolines and  Pseudocircles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Radtke%2C+Y+A">Yan Alves Radtke</a>, 
<a href="/search/math?searchtype=author&query=Felsner%2C+S">Stefan Felsner</a>, 
<a href="/search/math?searchtype=author&query=Obenaus%2C+J">Johannes Obenaus</a>, 
<a href="/search/math?searchtype=author&query=Roch%2C+S">Sandro Roch</a>, 
<a href="/search/math?searchtype=author&query=Scheucher%2C+M">Manfred Scheucher</a>, 
<a href="/search/math?searchtype=author&query=Vogtenhuber%2C+B">Birgit Vogtenhuber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Flip graphs of combinatorial and geometric objects are at the heart of many
deep structural insights and connections between different branches of discrete
mathematics and computer science. They also provide a natural framework for the
study of reconfiguration problems. We study flip graphs of arrangements of
pseudolines and of arrangements of pseudocircles, which are combinatorial
generalizations of lines and circles, respectively. In both cases we consider
triangle flips as local transformation and prove conjectures regarding their
connectivity.
<br />In the case of $n$ pseudolines we show that the connectivity of the flip
graph equals its minimum degree, which is exactly $n-2$. For the proof we
introduce the class of shellable line arrangements, which serve as reference
objects for the construction of disjoint paths. In fact, shellable arrangements
are elements of a flip graph of line arrangements which are vertices of a
polytope (Felsner and Ziegler; DM 241 (2001), 301--312). This polytope forms a
cluster of good connectivity in the flip graph of pseudolines. In the case of
pseudocircles we show that triangle flips induce a connected flip graph on
\emph{intersecting} arrangements and also on cylindrical intersecting
arrangements. The result for cylindrical arrangements is used in the proof for
intersecting arrangements. We also show that in both settings the diameter of
the flip graph is in $\Theta(n^3)$. Our constructions make essential use of
variants of the sweeping lemma for pseudocircle arrangements (Snoeyink and
Hershberger; Proc.\ SoCG 1989: 354--363). We finally study cylindrical
arrangements in their own right and provide new combinatorial characterizations
of this class.
</p>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19721" title="Abstract">arXiv:2310.19721</a> (cross-list from eess.IV) [<a href="/pdf/2310.19721" title="Download PDF">pdf</a>, <a href="/format/2310.19721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Promise:Prompt-driven 3D Medical Image Segmentation Using Image Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+D">Dewei Hu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiacheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Oguz%2C+I">Ipek Oguz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">To address prevalent issues in medical imaging, such as data acquisition
challenges and label availability, transfer learning from natural to medical
image domains serves as a viable strategy to produce reliable segmentation
results. However, several existing barriers between domains need to be broken
down, including addressing contrast discrepancies, managing anatomical
variability, and adapting 2D pretrained models for 3D segmentation tasks. In
this paper, we propose ProMISe,a prompt-driven 3D medical image segmentation
model using only a single point prompt to leverage knowledge from a pretrained
2D image foundation model. In particular, we use the pretrained vision
transformer from the Segment Anything Model (SAM) and integrate lightweight
adapters to extract depth-related (3D) spatial context without updating the
pretrained weights. For robust results, a hybrid network with complementary
encoders is designed, and a boundary-aware loss is proposed to achieve precise
boundaries. We evaluate our model on two public datasets for colon and pancreas
tumor segmentations, respectively. Compared to the state-of-the-art
segmentation methods with and without prompt engineering, our proposed method
achieves superior performance. The code is publicly available at
https://github.com/MedICL-VU/ProMISe.
</p>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19747" title="Abstract">arXiv:2310.19747</a> (cross-list from q-fin.CP) [<a href="/pdf/2310.19747" title="Download PDF">pdf</a>, <a href="/ps/2310.19747" title="Download PostScript">ps</a>, <a href="/format/2310.19747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characteristics of price related fluctuations in Non-Fungible Token  (NFT) market
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Szyd%C5%82o%2C+P">Pawe&#x142; Szyd&#x142;o</a>, 
<a href="/search/q-fin?searchtype=author&query=W%C4%85torek%2C+M">Marcin W&#x105;torek</a>, 
<a href="/search/q-fin?searchtype=author&query=Kwapie%C5%84%2C+J">Jaros&#x142;aw Kwapie&#x144;</a>, 
<a href="/search/q-fin?searchtype=author&query=Dro%C5%BCd%C5%BC%2C+S">Stanis&#x142;aw Dro&#x17c;d&#x17c;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Computational Engineering, Finance, and Science (cs.CE); Econometrics (econ.EM); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)

</div>
<p class="mathjax">Non-fungible token (NFT) market is a new trading invention based on the
blockchain technology which parallels the cryptocurrency market. In the present
work we study capitalization, floor price, the number of transactions, the
inter-transaction times, and the transaction volume value of a few selected
popular token collections. The results show that the fluctuations of all these
quantities are characterized by heavy-tailed probability distribution
functions, in most cases well described by the stretched exponentials, with a
trace of power-law scaling at times, long-range memory, and in several cases
even the fractal organization of fluctuations, mostly restricted to the larger
fluctuations, however. We conclude that the NFT market - even though young and
governed by a somewhat different mechanisms of trading - shares several
statistical properties with the regular financial markets. However, some
differences are visible in the specific quantitative indicators.
</p>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19767" title="Abstract">arXiv:2310.19767</a> (cross-list from eess.SP) [<a href="/pdf/2310.19767" title="Download PDF">pdf</a>, <a href="/format/2310.19767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autoregressive Attention Neural Networks for Non-Line-of-Sight User  Tracking with Dynamic Metasurface Antennas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Stylianopoulos%2C+K">Kyriakos Stylianopoulos</a>, 
<a href="/search/eess?searchtype=author&query=Bayraktar%2C+M">Murat Bayraktar</a>, 
<a href="/search/eess?searchtype=author&query=Prelcic%2C+N+G">Nuria Gonz&#xe1;lez Prelcic</a>, 
<a href="/search/eess?searchtype=author&query=Alexandropoulos%2C+G+C">George C. Alexandropoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, accepted for presentation by 2023 IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">User localization and tracking in the upcoming generation of wireless
networks have the potential to be revolutionized by technologies such as the
Dynamic Metasurface Antennas (DMAs). Commonly proposed algorithmic approaches
rely on assumptions about relatively dominant Line-of-Sight (LoS) paths, or
require pilot transmission sequences whose length is comparable to the number
of DMA elements, thus, leading to limited effectiveness and considerable
measurement overheads in blocked LoS and dynamic multipath environments. In
this paper, we present a two-stage machine-learning-based approach for user
tracking, specifically designed for non-LoS multipath settings. A newly
proposed attention-based Neural Network (NN) is first trained to map noisy
channel responses to potential user positions, regardless of user mobility
patterns. This architecture constitutes a modification of the prominent vision
transformer, specifically modified for extracting information from
high-dimensional frequency response signals. As a second stage, the NN's
predictions for the past user positions are passed through a learnable
autoregressive model to exploit the time-correlated channel information and
obtain the final position predictions. The channel estimation procedure
leverages a DMA receive architecture with partially-connected radio frequency
chains, which results to reduced numbers of pilots. The numerical evaluation
over an outdoor ray-tracing scenario illustrates that despite LoS blockage,
this methodology is capable of achieving high position accuracy across various
multipath settings.
</p>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19774" title="Abstract">arXiv:2310.19774</a> (cross-list from quant-ph) [<a href="/pdf/2310.19774" title="Download PDF">pdf</a>, <a href="/format/2310.19774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Codes for entanglement-assisted classical communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Prasad%2C+T">Tushita Prasad</a>, 
<a href="/search/quant-ph?searchtype=author&query=Grassl%2C+M">Markus Grassl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We present a new entanglement assisted classical communication scheme which
can correct a fixed number of erasures or errors. The scheme transmits
classical information over a quantum channel assisted by maximally entangled
pairs. We establish a general framework to accomplish such a task by reducing
it to a classical problem. We use direct coding or super-dense coding based on
the amount of entanglement available. This results in a combination of two
classical channels. For this scenario we present an explicit encoding scheme.
We compare our scheme with specific bounds and find certain ranges of
parameters where the scheme is optimal. The presented scheme can easily be
realized. It requires only the implementation of super-dense coding which has
been demonstrated successfully in experiments.
</p>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19777" title="Abstract">arXiv:2310.19777</a> (cross-list from math.OC) [<a href="/pdf/2310.19777" title="Download PDF">pdf</a>, <a href="/format/2310.19777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional gradients for total variation regularization with PDE  constraints: a graph cuts approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cristinelli%2C+G">Giacomo Cristinelli</a>, 
<a href="/search/math?searchtype=author&query=Iglesias%2C+J+A">Jos&#xe9; A. Iglesias</a>, 
<a href="/search/math?searchtype=author&query=Walter%2C+D">Daniel Walter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Total variation regularization has proven to be a valuable tool in the
context of optimal control of differential equations. This is particularly
attributed to the observation that TV-penalties often favor piecewise constant
minimizers with well-behaved jumpsets. On the downside, their intricate
properties significantly complicate every aspect of their analysis, from the
derivation of first-order optimality conditions to their discrete approximation
and the choice of a suitable solution algorithm. In this paper, we investigate
a general class of minimization problems with TV-regularization, comprising
both continuous and discretized control spaces, from a convex geometry
perspective. This leads to a variety of novel theoretical insights on
minimization problems with total variation regularization as well as tools for
their practical realization. First, by studying the extremal points of the
respective total variation unit balls, we enable their efficient solution by
geometry exploiting algorithms, e.g. fully-corrective generalized conditional
gradient methods. We give a detailed account on the practical realization of
such a method for piecewise constant finite element approximations of the
control on triangulations of the spatial domain. Second, in the same setting
and for suitable sequences of uniformly refined meshes, it is shown that
minimizers to discretized PDE-constrained optimal control problems approximate
solutions to a continuous limit problem involving an anisotropic total
variation reflecting the fine-scale geometry of the mesh.
</p>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19788" title="Abstract">arXiv:2310.19788</a> (cross-list from math.ST) [<a href="/pdf/2310.19788" title="Download PDF">pdf</a>, <a href="/format/2310.19788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally Optimal Best Arm Identification with a Fixed Budget
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kato%2C+M">Masahiro Kato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">This study investigates the problem of identifying the best treatment arm, a
treatment arm with the highest expected outcome. We aim to identify the best
treatment arm with a lower probability of misidentification, which has been
explored under various names across numerous research fields, including
\emph{best arm identification} (BAI) and ordinal optimization. In our
experiments, the number of treatment-allocation rounds is fixed. In each round,
a decision-maker allocates a treatment arm to an experimental unit and observes
a corresponding outcome, which follows a Gaussian distribution with a variance
different among treatment arms. At the end of the experiment, we recommend one
of the treatment arms as an estimate of the best treatment arm based on the
observations. The objective of the decision-maker is to design an experiment
that minimizes the probability of misidentifying the best treatment arm. With
this objective in mind, we develop lower bounds for the probability of
misidentification under the small-gap regime, where the gaps of the expected
outcomes between the best and suboptimal treatment arms approach zero. Then,
assuming that the variances are known, we design the
Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is
an extension of the Neyman allocation proposed by Neyman (1934) and the
Uniform-EBA strategy proposed by Bubeck et al. (2011). For the GNA-EBA
strategy, we show that the strategy is asymptotically optimal because its
probability of misidentification aligns with the lower bounds as the sample
size approaches infinity under the small-gap regime. We refer to such optimal
strategies as locally asymptotic optimal because their performance aligns with
the lower bounds within restricted situations characterized by the small-gap
regime.
</p>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19793" title="Abstract">arXiv:2310.19793</a> (cross-list from stat.ML) [<a href="/pdf/2310.19793" title="Download PDF">pdf</a>, <a href="/format/2310.19793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Learning Gaussian Multi-index Models with Gradient Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bietti%2C+A">Alberto Bietti</a>, 
<a href="/search/stat?searchtype=author&query=Bruna%2C+J">Joan Bruna</a>, 
<a href="/search/stat?searchtype=author&query=Pillaud-Vivien%2C+L">Loucas Pillaud-Vivien</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">We study gradient flow on the multi-index regression problem for
high-dimensional Gaussian data. Multi-index functions consist of a composition
of an unknown low-rank linear projection and an arbitrary unknown,
low-dimensional link function. As such, they constitute a natural template for
feature learning in neural networks.
<br />We consider a two-timescale algorithm, whereby the low-dimensional link
function is learnt with a non-parametric model infinitely faster than the
subspace parametrizing the low-rank projection. By appropriately exploiting the
matrix semigroup structure arising over the subspace correlation matrices, we
establish global convergence of the resulting Grassmannian population gradient
flow dynamics, and provide a quantitative description of its associated
`saddle-to-saddle' dynamics. Notably, the timescales associated with each
saddle can be explicitly characterized in terms of an appropriate Hermite
decomposition of the target link function. In contrast with these positive
results, we also show that the related \emph{planted} problem, where the link
function is known and fixed, in fact has a rough optimization landscape, in
which gradient flow dynamics might get trapped with high probability.
</p>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19794" title="Abstract">arXiv:2310.19794</a> (cross-list from stat.ML) [<a href="/pdf/2310.19794" title="Download PDF">pdf</a>, <a href="/format/2310.19794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Causal Bandits for Linear Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yan%2C+Z">Zirui Yan</a>, 
<a href="/search/stat?searchtype=author&query=Mukherjee%2C+A">Arpan Mukherjee</a>, 
<a href="/search/stat?searchtype=author&query=Var%C4%B1c%C4%B1%2C+B">Burak Var&#x131;c&#x131;</a>, 
<a href="/search/stat?searchtype=author&query=Tajer%2C+A">Ali Tajer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Sequential design of experiments for optimizing a reward function in causal
systems can be effectively modeled by the sequential design of interventions in
causal bandits (CBs). In the existing literature on CBs, a critical assumption
is that the causal models remain constant over time. However, this assumption
does not necessarily hold in complex systems, which constantly undergo temporal
model fluctuations. This paper addresses the robustness of CBs to such model
fluctuations. The focus is on causal systems with linear structural equation
models (SEMs). The SEMs and the time-varying pre- and post-interventional
statistical models are all unknown. Cumulative regret is adopted as the design
criteria, based on which the objective is to design a sequence of interventions
that incur the smallest cumulative regret with respect to an oracle aware of
the entire causal model and its fluctuations. First, it is established that the
existing approaches fail to maintain regret sub-linearity with even a few
instances of model deviation. Specifically, when the number of instances with
model deviation is as few as $T^\frac{1}{2L}$, where $T$ is the time horizon
and $L$ is the longest causal path in the graph, the existing algorithms will
have linear regret in $T$. Next, a robust CB algorithm is designed, and its
regret is analyzed, where upper and information-theoretic lower bounds on the
regret are established. Specifically, in a graph with $N$ nodes and maximum
degree $d$, under a general measure of model deviation $C$, the cumulative
regret is upper bounded by $\tilde{\mathcal{O}}(d^{L-\frac{1}{2}}(\sqrt{NT} +
NC))$ and lower bounded by $\Omega(d^{\frac{L}{2}-2}\max\{\sqrt{T},d^2C\})$.
Comparing these bounds establishes that the proposed algorithm achieves nearly
optimal $\tilde{\mathcal{O}}(\sqrt{T})$ regret when $C$ is $o(\sqrt{T})$ and
maintains sub-linear regret for a broader range of $C$.
</p>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19796" title="Abstract">arXiv:2310.19796</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.19796" title="Download PDF">pdf</a>, <a href="/format/2310.19796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-evaluating Retrosynthesis Algorithms with Syntheseus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Maziarz%2C+K">Krzysztof Maziarz</a>, 
<a href="/search/q-bio?searchtype=author&query=Tripp%2C+A">Austin Tripp</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+G">Guoqing Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Stanley%2C+M">Megan Stanley</a>, 
<a href="/search/q-bio?searchtype=author&query=Xie%2C+S">Shufang Xie</a>, 
<a href="/search/q-bio?searchtype=author&query=Gai%C5%84ski%2C+P">Piotr Gai&#x144;ski</a>, 
<a href="/search/q-bio?searchtype=author&query=Seidl%2C+P">Philipp Seidl</a>, 
<a href="/search/q-bio?searchtype=author&query=Segler%2C+M">Marwin Segler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The planning of how to synthesize molecules, also known as retrosynthesis,
has been a growing focus of the machine learning and chemistry communities in
recent years. Despite the appearance of steady progress, we argue that
imperfect benchmarks and inconsistent comparisons mask systematic shortcomings
of existing techniques. To remedy this, we present a benchmarking library
called syntheseus which promotes best practice by default, enabling consistent
meaningful evaluation of single-step and multi-step retrosynthesis algorithms.
We use syntheseus to re-evaluate a number of previous retrosynthesis
algorithms, and find that the ranking of state-of-the-art models changes when
evaluated carefully. We end with guidance for future works in this area.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Tue, 31 Oct 23</h3>
<dl>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1611.10283" title="Abstract">arXiv:1611.10283</a> (replaced) [<a href="/pdf/1611.10283" title="Download PDF">pdf</a>, <a href="/ps/1611.10283" title="Download PostScript">ps</a>, <a href="/format/1611.10283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted bandits or: How bandits learn distorted values that are not  expected
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kolla%2C+R+K">Ravi Kumar Kolla</a>, 
<a href="/search/cs?searchtype=author&query=A.%2C+P+L">Prashanth L.A.</a>, 
<a href="/search/cs?searchtype=author&query=Gopalan%2C+A">Aditya Gopalan</a>, 
<a href="/search/cs?searchtype=author&query=Jagannathan%2C+K">Krishna Jagannathan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+M">Michael Fu</a>, 
<a href="/search/cs?searchtype=author&query=Marcus%2C+S">Steve Marcus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The material in this paper was presented in part at the 2017 AAAI Conference on Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1906.03829" title="Abstract">arXiv:1906.03829</a> (replaced) [<a href="/pdf/1906.03829" title="Download PDF">pdf</a>, <a href="/format/1906.03829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer Learning for Hate Speech Detection in Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lanqin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ferraro%2C+G">Gabriela Ferraro</a>, 
<a href="/search/cs?searchtype=author&query=Suominen%2C+H">Hanna Suominen</a>, 
<a href="/search/cs?searchtype=author&query=Rizoiu%2C+M">Marian-Andrei Rizoiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1908.08016" title="Abstract">arXiv:1908.08016</a> (replaced) [<a href="/pdf/1908.08016" title="Download PDF">pdf</a>, <a href="/format/1908.08016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing Robustness Against Unforeseen Adversaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaufmann%2C+M">Max Kaufmann</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Daniel Kang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Basart%2C+S">Steven Basart</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xuwang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Mazeika%2C+M">Mantas Mazeika</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Akul Arora</a>, 
<a href="/search/cs?searchtype=author&query=Dziedzic%2C+A">Adam Dziedzic</a>, 
<a href="/search/cs?searchtype=author&query=Boenisch%2C+F">Franziska Boenisch</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+T">Tom Brown</a>, 
<a href="/search/cs?searchtype=author&query=Steinhardt%2C+J">Jacob Steinhardt</a>, 
<a href="/search/cs?searchtype=author&query=Hendrycks%2C+D">Dan Hendrycks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Datasets available at <a href="https://github.com/centerforaisafety/adversarial-corruptions">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1909.09485" title="Abstract">arXiv:1909.09485</a> (replaced) [<a href="/pdf/1909.09485" title="Download PDF">pdf</a>, <a href="/format/1909.09485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BSDAR: Beam Search Decoding with Attention Reward in Neural Keyphrase  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%27mah%2C+I">Iftitahu Ni&#x27;mah</a>, 
<a href="/search/cs?searchtype=author&query=Menkovski%2C+V">Vlado Menkovski</a>, 
<a href="/search/cs?searchtype=author&query=Pechenizkiy%2C+M">Mykola Pechenizkiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arxiv preprint. a preliminary study
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.00446" title="Abstract">arXiv:2011.00446</a> (replaced) [<a href="/pdf/2011.00446" title="Download PDF">pdf</a>, <a href="/format/2011.00446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Learning of Control Policies for Robust Quadruped Bounding  using Pretrained Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Anqiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yixiao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+A">Anhuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhibin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qiuguo Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IET Cyber-Systems and Robotics 2022 4(4):331-338
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.02670" title="Abstract">arXiv:2011.02670</a> (replaced) [<a href="/pdf/2011.02670" title="Download PDF">pdf</a>, <a href="/ps/2011.02670" title="Download PostScript">ps</a>, <a href="/format/2011.02670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Black-Box Approach to Post-Quantum Zero-Knowledge in Constant Rounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chia%2C+N">Nai-Hui Chia</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chung%2C+K">Kai-Min Chung</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yamakawa%2C+T">Takashi Yamakawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed a minor technical issue (see Footnote 17 in page 21) and improved the proof of Claim 4.5. (10/30/2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CRYPTO 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.12700" title="Abstract">arXiv:2101.12700</a> (replaced) [<a href="/pdf/2101.12700" title="Download PDF">pdf</a>, <a href="/format/2101.12700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reservoir Computing with Magnetic Thin Films
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dale%2C+M">Matthew Dale</a>, 
<a href="/search/cs?searchtype=author&query=Griffin%2C+D">David Griffin</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+R+F+L">Richard F. L. Evans</a>, 
<a href="/search/cs?searchtype=author&query=Jenkins%2C+S">Sarah Jenkins</a>, 
<a href="/search/cs?searchtype=author&query=O%27Keefe%2C+S">Simon O&#x27;Keefe</a>, 
<a href="/search/cs?searchtype=author&query=Sebald%2C+A">Angelika Sebald</a>, 
<a href="/search/cs?searchtype=author&query=Stepney%2C+S">Susan Stepney</a>, 
<a href="/search/cs?searchtype=author&query=Torre%2C+F">Fernando Torre</a>, 
<a href="/search/cs?searchtype=author&query=Trefzer%2C+M">Martin Trefzer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 10 figures, updated and clarified
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Materials Science (cond-mat.mtrl-sci); Hardware Architecture (cs.AR); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.05753" title="Abstract">arXiv:2103.05753</a> (replaced) [<a href="/pdf/2103.05753" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Developmental Neurosimulation Using Embodied Computational  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Alicea%2C+B">Bradly Alicea</a>, 
<a href="/search/q-bio?searchtype=author&query=Chakrabarty%2C+R">Rishabh Chakrabarty</a>, 
<a href="/search/q-bio?searchtype=author&query=Dvoretskii%2C+S">Stefan Dvoretskii</a>, 
<a href="/search/q-bio?searchtype=author&query=Gong%2C+Z">Ziyi Gong</a>, 
<a href="/search/q-bio?searchtype=author&query=Gopi%2C+A">Akshara Gopi</a>, 
<a href="/search/q-bio?searchtype=author&query=Lim%2C+A">Avery Lim</a>, 
<a href="/search/q-bio?searchtype=author&query=Parent%2C+J">Jesse Parent</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.08249" title="Abstract">arXiv:2103.08249</a> (replaced) [<a href="/e-print/2103.08249" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolving parametrized Loss for Image Classification Learning on Small  Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hai%2C+Z">Zhaoyang Hai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiabi Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article has been abandoned for publication, and the researcher will no longer participate in related research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.10895" title="Abstract">arXiv:2104.10895</a> (replaced) [<a href="/pdf/2104.10895" title="Download PDF">pdf</a>, <a href="/format/2104.10895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On convergence rates of adaptive ensemble Kalman inversion for linear  ill-posed problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Parzer%2C+F">Fabian Parzer</a>, 
<a href="/search/math?searchtype=author&query=Scherzer%2C+O">Otmar Scherzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02835" title="Abstract">arXiv:2106.02835</a> (replaced) [<a href="/pdf/2106.02835" title="Download PDF">pdf</a>, <a href="/format/2106.02835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Role of Entropy-based Loss for Learning Causal Structures with  Continuous Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weilin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+J">Jie Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+R">Ruichu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhifeng Hao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.14186" title="Abstract">arXiv:2106.14186</a> (replaced) [<a href="/pdf/2106.14186" title="Download PDF">pdf</a>, <a href="/format/2106.14186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An XAI Approach to Deep Learning Models in the Detection of DCIS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=La+Ferla%2C+M">Michele La Ferla</a>, 
<a href="/search/eess?searchtype=author&query=Montebello%2C+M">Matthew Montebello</a>, 
<a href="/search/eess?searchtype=author&query=Seychell%2C+D">Dylan Seychell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AIAI 2023. IFIP Adv.Inf.Comm.Tech. 677 (2023) 409-420
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.03913" title="Abstract">arXiv:2107.03913</a> (replaced) [<a href="/pdf/2107.03913" title="Download PDF">pdf</a>, <a href="/ps/2107.03913" title="Download PostScript">ps</a>, <a href="/format/2107.03913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Medical Profile Model: Scientific and Practical Applications in  Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blinov%2C+P">Pavel Blinov</a>, 
<a href="/search/cs?searchtype=author&query=Kokh%2C+V">Vladimir Kokh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, code available at <a href="https://github.com/sberbank-ai-lab/mimic.profile">this https URL</a>, accepted for publication at IEEE JBHI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.08051" title="Abstract">arXiv:2108.08051</a> (replaced) [<a href="/pdf/2108.08051" title="Download PDF">pdf</a>, <a href="/format/2108.08051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimised Informed RRTs for Mobile Robot Path Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maseko%2C+B+B">Bongani B. Maseko</a>, 
<a href="/search/cs?searchtype=author&query=van+Daalen%2C+C+E">Corn&#xe9; E. van Daalen</a>, 
<a href="/search/cs?searchtype=author&query=Treurnicht%2C+J">Johann Treurnicht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, published in IFAC-PapersOnLine
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IFAC-PapersOnLine,vol. 54, no. 21,2021,pp. 157-162,ISSN 2405-8963
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.09658" title="Abstract">arXiv:2109.09658</a> (replaced) [<a href="/pdf/2109.09658" title="Download PDF">pdf</a>, <a href="/format/2109.09658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FUTURE-AI: Guiding Principles and Consensus Recommendations for  Trustworthy Artificial Intelligence in Medical Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lekadir%2C+K">Karim Lekadir</a>, 
<a href="/search/cs?searchtype=author&query=Osuala%2C+R">Richard Osuala</a>, 
<a href="/search/cs?searchtype=author&query=Gallin%2C+C">Catherine Gallin</a>, 
<a href="/search/cs?searchtype=author&query=Lazrak%2C+N">Noussair Lazrak</a>, 
<a href="/search/cs?searchtype=author&query=Kushibar%2C+K">Kaisar Kushibar</a>, 
<a href="/search/cs?searchtype=author&query=Tsakou%2C+G">Gianna Tsakou</a>, 
<a href="/search/cs?searchtype=author&query=Auss%C3%B3%2C+S">Susanna Auss&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Alberich%2C+L+C">Leonor Cerd&#xe1; Alberich</a>, 
<a href="/search/cs?searchtype=author&query=Marias%2C+K">Kostas Marias</a>, 
<a href="/search/cs?searchtype=author&query=Tsiknakis%2C+M">Manolis Tsiknakis</a>, 
<a href="/search/cs?searchtype=author&query=Colantonio%2C+S">Sara Colantonio</a>, 
<a href="/search/cs?searchtype=author&query=Papanikolaou%2C+N">Nickolas Papanikolaou</a>, 
<a href="/search/cs?searchtype=author&query=Salahuddin%2C+Z">Zohaib Salahuddin</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+H+C">Henry C Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Lambin%2C+P">Philippe Lambin</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%AD-Bonmat%C3%AD%2C+L">Luis Mart&#xed;-Bonmat&#xed;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please refer to <a href="/abs/2309.12325">arXiv:2309.12325</a> for the latest FUTURE-AI framework for healthcare
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.11438" title="Abstract">arXiv:2109.11438</a> (replaced) [<a href="/pdf/2109.11438" title="Download PDF">pdf</a>, <a href="/ps/2109.11438" title="Download PostScript">ps</a>, <a href="/format/2109.11438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A special case of Vu&#x27;s conjecture: Coloring nearly disjoint graphs of  bounded maximum degree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kelly%2C+T">Tom Kelly</a>, 
<a href="/search/math?searchtype=author&query=K%C3%BChn%2C+D">Daniela K&#xfc;hn</a>, 
<a href="/search/math?searchtype=author&query=Osthus%2C+D">Deryk Osthus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages with one-page appendix; final version, to appear in Combinatorics, Probability, and Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.00269" title="Abstract">arXiv:2110.00269</a> (replaced) [<a href="/pdf/2110.00269" title="Download PDF">pdf</a>, <a href="/format/2110.00269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Knowledge Enhanced Pre-trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+G">Gang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yulong Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.03894" title="Abstract">arXiv:2110.03894</a> (replaced) [<a href="/pdf/2110.03894" title="Download PDF">pdf</a>, <a href="/format/2110.03894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Model Reprogramming with Similarity Based Mapping for  Low-Resource Spoken Command Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yen%2C+H">Hao Yen</a>, 
<a href="/search/eess?searchtype=author&query=Ku%2C+P">Pin-Jui Ku</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+H">Hu Hu</a>, 
<a href="/search/eess?searchtype=author&query=Siniscalchi%2C+S+M">Sabato Marco Siniscalchi</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Interspeech 2023. Code is available at: <a href="https://github.com/dodohow1011/SpeechAdvReprogram.">this https URL</a> Selected as Best Student Paper Candidate
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.06036" title="Abstract">arXiv:2111.06036</a> (replaced) [<a href="/e-print/2111.06036" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CubeTR: Learning to Solve The Rubiks Cube Using Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chasmai%2C+M+E">Mustafa Ebrahim Chasmai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It has untested ideas without supporting experimentation. Discontinued work in this direction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.06251" title="Abstract">arXiv:2112.06251</a> (replaced) [<a href="/pdf/2112.06251" title="Download PDF">pdf</a>, <a href="/format/2112.06251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning with Subset Stacking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Birbil%2C+S+%C4%B0">S. &#x130;lker Birbil</a>, 
<a href="/search/cs?searchtype=author&query=Yildirim%2C+S">Sinan Yildirim</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B6kalp%2C+K">Kaya G&#xf6;kalp</a>, 
<a href="/search/cs?searchtype=author&query=Aky%C3%BCz%2C+M+H">M. Hakan Aky&#xfc;z</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 9 figures, 2 tables. Code available
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.00011" title="Abstract">arXiv:2202.00011</a> (replaced) [<a href="/pdf/2202.00011" title="Download PDF">pdf</a>, <a href="/format/2202.00011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed  Video Quality Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ehrlich%2C+M">Max Ehrlich</a>, 
<a href="/search/eess?searchtype=author&query=Barker%2C+J">Jon Barker</a>, 
<a href="/search/eess?searchtype=author&query=Padmanabhan%2C+N">Namitha Padmanabhan</a>, 
<a href="/search/eess?searchtype=author&query=Davis%2C+L">Larry Davis</a>, 
<a href="/search/eess?searchtype=author&query=Tao%2C+A">Andrew Tao</a>, 
<a href="/search/eess?searchtype=author&query=Catanzaro%2C+B">Bryan Catanzaro</a>, 
<a href="/search/eess?searchtype=author&query=Shrivastava%2C+A">Abhinav Shrivastava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.06208" title="Abstract">arXiv:2202.06208</a> (replaced) [<a href="/pdf/2202.06208" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Molecular Representation Learning with Metric  Learning-enhanced Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Courty%2C+N">Nicolas Courty</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shuting Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.08361" title="Abstract">arXiv:2202.08361</a> (replaced) [<a href="/pdf/2202.08361" title="Download PDF">pdf</a>, <a href="/format/2202.08361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vectorization of a thread-parallel Jacobi singular value decomposition  method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Novakovi%C4%87%2C+V">Vedran Novakovi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A corrected version of the paper from SIAM Journal on Scientific Computing. A separate "supplementary materials" document has been appended to the main paper as Appendix for technical reasons
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM J. Sci. Comput. 45 (2023), 3; C73-C100
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Software (cs.MS)

</div>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.09172" title="Abstract">arXiv:2202.09172</a> (replaced) [<a href="/pdf/2202.09172" title="Download PDF">pdf</a>, <a href="/format/2202.09172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enumeration of corner polyhedra and 3-connected Schnyder labelings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fusy%2C+%C3%89">&#xc9;ric Fusy</a>, 
<a href="/search/math?searchtype=author&query=Narmanli%2C+E">Erkan Narmanli</a>, 
<a href="/search/math?searchtype=author&query=Schaeffer%2C+G">Gilles Schaeffer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Electronic Journal of Combinatorics, Volume 30, Issue 2, P2.17
  (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.09545" title="Abstract">arXiv:2202.09545</a> (replaced) [<a href="/pdf/2202.09545" title="Download PDF">pdf</a>, <a href="/format/2202.09545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Going Deeper into Recognizing Actions in Dark Environments: A  Comprehensive Benchmark Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuecong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianfei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Haozhi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jianxiong Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoli Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhengguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qianwen Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IJCV. Summary of UG2 2021 Track 2, 22 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.11601" title="Abstract">arXiv:2202.11601</a> (replaced) [<a href="/pdf/2202.11601" title="Download PDF">pdf</a>, <a href="/format/2202.11601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Succinct Population Protocols for Presburger Arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Czerner%2C+P">Philipp Czerner</a>, 
<a href="/search/cs?searchtype=author&query=Guttenberg%2C+R">Roland Guttenberg</a>, 
<a href="/search/cs?searchtype=author&query=Helfrich%2C+M">Martin Helfrich</a>, 
<a href="/search/cs?searchtype=author&query=Esparza%2C+J">Javier Esparza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages, 7 figures, to be published in the Journal of Computer and System Sciences, Special Issue on SAND 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.01033" title="Abstract">arXiv:2203.01033</a> (replaced) [<a href="/pdf/2203.01033" title="Download PDF">pdf</a>, <a href="/ps/2203.01033" title="Download PostScript">ps</a>, <a href="/format/2203.01033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STV+AGR: Towards Practical Verification of Strategic Ability Using  Assume-Guarantee Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>, 
<a href="/search/cs?searchtype=author&query=Mikulski%2C+%C5%81">&#x141;ukasz Mikulski</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.02128" title="Abstract">arXiv:2203.02128</a> (replaced) [<a href="/pdf/2203.02128" title="Download PDF">pdf</a>, <a href="/format/2203.02128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Robust Bayesian Optimization with $\varphi$-divergences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Husain%2C+H">Hisham Husain</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V">Vu Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Hengel%2C+A">Anton van den Hengel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera ready paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.03897" title="Abstract">arXiv:2203.03897</a> (replaced) [<a href="/pdf/2203.03897" title="Download PDF">pdf</a>, <a href="/format/2203.03897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geodesic Multi-Modal Mixup for Robust Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+C">Changdae Oh</a>, 
<a href="/search/cs?searchtype=author&query=So%2C+J">Junhyuk So</a>, 
<a href="/search/cs?searchtype=author&query=Byun%2C+H">Hoyoon Byun</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+Y">YongTaek Lim</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+M">Minchul Shin</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+J">Jong-June Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kyungwoo Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.05145" title="Abstract">arXiv:2203.05145</a> (replaced) [<a href="/pdf/2203.05145" title="Download PDF">pdf</a>, <a href="/format/2203.05145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascaded Sparse Feature Propagation Network for Interactive Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chuanyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hui Ren</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yongfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuming He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contribute equally. Accepted by BMVC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.15529" title="Abstract">arXiv:2203.15529</a> (replaced) [<a href="/pdf/2203.15529" title="Download PDF">pdf</a>, <a href="/format/2203.15529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Treatment Learning Causal Transformer for Noisy Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+I+D">I-Te Danny Hung</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi-Chieh Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE WACV 2023. The first version was finished in May 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.02371" title="Abstract">arXiv:2204.02371</a> (replaced) [<a href="/pdf/2204.02371" title="Download PDF">pdf</a>, <a href="/format/2204.02371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optical Proximity Sensing for Pose Estimation During In-Hand  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lancaster%2C+P">Patrick Lancaster</a>, 
<a href="/search/cs?searchtype=author&query=Gyawali%2C+P">Pratik Gyawali</a>, 
<a href="/search/cs?searchtype=author&query=Mavrogiannis%2C+C">Christoforos Mavrogiannis</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasa%2C+S+S">Siddhartha S. Srinivasa</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+J+R">Joshua R. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.02460" title="Abstract">arXiv:2204.02460</a> (replaced) [<a href="/pdf/2204.02460" title="Download PDF">pdf</a>, <a href="/format/2204.02460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electrostatic Brakes Enable Individual Joint Control of Underactuated,  Highly Articulated Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lancaster%2C+P">Patrick Lancaster</a>, 
<a href="/search/cs?searchtype=author&query=Mavrogiannis%2C+C">Christoforos Mavrogiannis</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasa%2C+S">Siddhartha Srinivasa</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+J">Joshua Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.08581" title="Abstract">arXiv:2204.08581</a> (replaced) [<a href="/pdf/2204.08581" title="Download PDF">pdf</a>, <a href="/format/2204.08581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Parametric Optimal Execution and Machine Learning Surrogates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Chen%2C+T">Tao Chen</a>, 
<a href="/search/q-fin?searchtype=author&query=Ludkovski%2C+M">Mike Ludkovski</a>, 
<a href="/search/q-fin?searchtype=author&query=Vo%C3%9F%2C+M">Moritz Vo&#xdf;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 8 figures. Github repo at <a href="https://github.com/moritz-voss/Parametric_Optimal_Execution_ML">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Trading and Market Microstructure (q-fin.TR)</span>; Machine Learning (cs.LG); Computational Finance (q-fin.CP)

</div>
</div>
</dd>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.10184" title="Abstract">arXiv:2204.10184</a> (replaced) [<a href="/pdf/2204.10184" title="Download PDF">pdf</a>, <a href="/format/2204.10184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INSPIRE: Distributed Bayesian Optimization for ImproviNg SPatIal REuse  in Dense WLANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bardou%2C+A">Anthony Bardou</a>, 
<a href="/search/cs?searchtype=author&query=Begin%2C+T">Thomas Begin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.06562" title="Abstract">arXiv:2205.06562</a> (replaced) [<a href="/pdf/2205.06562" title="Download PDF">pdf</a>, <a href="/format/2205.06562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A graph-based probabilistic geometric deep learning framework with  online enforcement of physical constraints to predict the criticality of  defects in porous materials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krokos%2C+V">Vasilis Krokos</a>, 
<a href="/search/cs?searchtype=author&query=Bordas%2C+S+P+A">St&#xe9;phane P. A. Bordas</a>, 
<a href="/search/cs?searchtype=author&query=Kerfriden%2C+P">Pierre Kerfriden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages; 35 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.07087" title="Abstract">arXiv:2205.07087</a> (replaced) [<a href="/pdf/2205.07087" title="Download PDF">pdf</a>, <a href="/ps/2205.07087" title="Download PostScript">ps</a>, <a href="/format/2205.07087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pattern reconstruction with restricted Boltzmann machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Genovese%2C+G">Giuseppe Genovese</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Information Retrieval (cs.IR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.13925" title="Abstract">arXiv:2205.13925</a> (replaced) [<a href="/pdf/2205.13925" title="Download PDF">pdf</a>, <a href="/format/2205.13925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DELTA: Diverse Client Sampling for Fasting Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">YongXin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoying Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.15677" title="Abstract">arXiv:2205.15677</a> (replaced) [<a href="/pdf/2205.15677" title="Download PDF">pdf</a>, <a href="/format/2205.15677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmentation-Aware Self-Supervision for Data-Efficient GAN Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Liang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yige Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Songtao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chongyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Siyuan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+P">Pengfei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item801">[801]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.16008" title="Abstract">arXiv:2205.16008</a> (replaced) [<a href="/pdf/2205.16008" title="Download PDF">pdf</a>, <a href="/format/2205.16008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More Stiffness with Less Fiber: End-to-End Fiber Path Optimization for  3D-Printed Composites
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xingyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Roeder%2C+G">Geoffrey Roeder</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+T">Tianju Xue</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+R+P">Ryan P. Adams</a>, 
<a href="/search/cs?searchtype=author&query=Rusinkiewicz%2C+S">Szymon Rusinkiewicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM SCF 2023: Proceedings of the 8th Annual ACM Symposium on Computational Fabrication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item802">[802]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.02341" title="Abstract">arXiv:2206.02341</a> (replaced) [<a href="/pdf/2206.02341" title="Download PDF">pdf</a>, <a href="/format/2206.02341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complex Locomotion Skill Learning via Differentiable Physics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yu Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiancheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiasheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yidong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuanming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chenfanfu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tiantian Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item803">[803]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.07303" title="Abstract">arXiv:2206.07303</a> (replaced) [<a href="/pdf/2206.07303" title="Download PDF">pdf</a>, <a href="/format/2206.07303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energetic Variational Neural Network Discretizations of Gradient Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+Z">Ziqing Hu</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+C">Chun Liu</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">Yiwei Wang</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+Z">Zhiliang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item804">[804]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.07550" title="Abstract">arXiv:2206.07550</a> (replaced) [<a href="/pdf/2206.07550" title="Download PDF">pdf</a>, <a href="/format/2206.07550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating and Inducing Personality in Pre-trained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+G">Guangyuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Manjie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wenjuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yixin Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item805">[805]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.09203" title="Abstract">arXiv:2206.09203</a> (replaced) [<a href="/pdf/2206.09203" title="Download PDF">pdf</a>, <a href="/format/2206.09203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Visual Reasoning under Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Manjie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+G">Guangyuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+W">Wei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yixin Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 (Datasets and Benchmarks)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item806">[806]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.12809" title="Abstract">arXiv:2206.12809</a> (replaced) [<a href="/e-print/2206.12809" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparison of AIS, X-Band Marine Radar Systems and Camera Surveillance  Systems in the Collection of Tracking Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zardoua%2C+Y">Yassir Zardoua</a>, 
<a href="/search/eess?searchtype=author&query=Astito%2C+A">Abdelali Astito</a>, 
<a href="/search/eess?searchtype=author&query=Boulaala%2C+M">Mohammed Boulaala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The journal that published this paper is no longer online. We discovered it was a predatory journal. Withdrawing this paper will allow us to publish it elsewhere. We enhanced this paper and will provide its full text as a replacement
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Recent Research and Applied Studies,
  Volume 7, Issue 4 (1) April 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item807">[807]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.14261" title="Abstract">arXiv:2206.14261</a> (replaced) [<a href="/pdf/2206.14261" title="Download PDF">pdf</a>, <a href="/format/2206.14261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised Contrastive Outlier removal for Pseudo Expectation  Maximization (SCOPE)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Menon%2C+S">Sumeet Menon</a>, 
<a href="/search/cs?searchtype=author&query=Chapman%2C+D">David Chapman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item808">[808]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.01346" title="Abstract">arXiv:2207.01346</a> (replaced) [<a href="/pdf/2207.01346" title="Download PDF">pdf</a>, <a href="/format/2207.01346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Competition Outperform Collaboration? The Role of Misbehaving Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ballotta%2C+L">Luca Ballotta</a>, 
<a href="/search/eess?searchtype=author&query=Como%2C+G">Giacomo Como</a>, 
<a href="/search/eess?searchtype=author&query=Shamma%2C+J+S">Jeff S. Shamma</a>, 
<a href="/search/eess?searchtype=author&query=Schenato%2C+L">Luca Schenato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE TAC; 17 pages, 44 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Multiagent Systems (cs.MA); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item809">[809]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.03444" title="Abstract">arXiv:2207.03444</a> (replaced) [<a href="/pdf/2207.03444" title="Download PDF">pdf</a>, <a href="/format/2207.03444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness and Bias in Robot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Londo%C3%B1o%2C+L">Laura Londo&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Hurtado%2C+J+V">Juana Valeria Hurtado</a>, 
<a href="/search/cs?searchtype=author&query=Hertz%2C+N">Nora Hertz</a>, 
<a href="/search/cs?searchtype=author&query=Kellmeyer%2C+P">Philipp Kellmeyer</a>, 
<a href="/search/cs?searchtype=author&query=Voeneky%2C+S">Silja Voeneky</a>, 
<a href="/search/cs?searchtype=author&query=Valada%2C+A">Abhinav Valada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item810">[810]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.03576" title="Abstract">arXiv:2207.03576</a> (replaced) [<a href="/pdf/2207.03576" title="Download PDF">pdf</a>, <a href="/format/2207.03576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness Evaluation of Deep Unsupervised Learning Algorithms for  Intrusion Detection Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nkashama%2C+D+K">D&#x27;Jeff Kanda Nkashama</a>, 
<a href="/search/cs?searchtype=author&query=Soltani%2C+A">Arian Soltani</a>, 
<a href="/search/cs?searchtype=author&query=Verdier%2C+J">Jean-Charles Verdier</a>, 
<a href="/search/cs?searchtype=author&query=Frappier%2C+M">Marc Frappier</a>, 
<a href="/search/cs?searchtype=author&query=Tardif%2C+P">Pierre-Martin Tardif</a>, 
<a href="/search/cs?searchtype=author&query=Kabanza%2C+F">Froduald Kabanza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Machine Learning for Cybersecurity Workshop at the International Conference on Machine Learning(ICLR-ML4CY)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item811">[811]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.05161" title="Abstract">arXiv:2207.05161</a> (replaced) [<a href="/pdf/2207.05161" title="Download PDF">pdf</a>, <a href="/format/2207.05161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What is Flagged in Uncertainty Quantification? Latent Density Models for  Uncertainty Categorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=van+Breugel%2C+B">Boris van Breugel</a>, 
<a href="/search/cs?searchtype=author&query=Crabbe%2C+J">Jonathan Crabbe</a>, 
<a href="/search/cs?searchtype=author&query=Seedat%2C+N">Nabeel Seedat</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item812">[812]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.07051" title="Abstract">arXiv:2207.07051</a> (replaced) [<a href="/pdf/2207.07051" title="Download PDF">pdf</a>, <a href="/format/2207.07051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language models show human-like content effects on reasoning tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+I">Ishita Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Lampinen%2C+A+K">Andrew K. Lampinen</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+S+C+Y">Stephanie C. Y. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Sheahan%2C+H+R">Hannah R. Sheahan</a>, 
<a href="/search/cs?searchtype=author&query=Creswell%2C+A">Antonia Creswell</a>, 
<a href="/search/cs?searchtype=author&query=Kumaran%2C+D">Dharshan Kumaran</a>, 
<a href="/search/cs?searchtype=author&query=McClelland%2C+J+L">James L. McClelland</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+F">Felix Hill</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item813">[813]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08073" title="Abstract">arXiv:2207.08073</a> (replaced) [<a href="/pdf/2207.08073" title="Download PDF">pdf</a>, <a href="/ps/2207.08073" title="Download PostScript">ps</a>, <a href="/format/2207.08073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bidding combinatorial games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kant%2C+P">Prem Kant</a>, 
<a href="/search/cs?searchtype=author&query=Larsson%2C+U">Urban Larsson</a>, 
<a href="/search/cs?searchtype=author&query=Rai%2C+R+K">Ravi K. Rai</a>, 
<a href="/search/cs?searchtype=author&query=Upasany%2C+A+V">Akshay V. Upasany</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item814">[814]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08463" title="Abstract">arXiv:2207.08463</a> (replaced) [<a href="/pdf/2207.08463" title="Download PDF">pdf</a>, <a href="/format/2207.08463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A high-order scheme for mean field games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Calzola%2C+E">Elisa Calzola</a>, 
<a href="/search/math?searchtype=author&query=Carlini%2C+E">Elisabetta Carlini</a>, 
<a href="/search/math?searchtype=author&query=Silva%2C+F+J">Francisco J. Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item815">[815]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.10894" title="Abstract">arXiv:2207.10894</a> (replaced) [<a href="/pdf/2207.10894" title="Download PDF">pdf</a>, <a href="/format/2207.10894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What are Your Pronouns? Examining Gender Pronoun Usage on Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Julie Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Emily Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>, 
<a href="/search/cs?searchtype=author&query=Muri%C4%87%2C+G">Goran Muri&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Pierri%2C+F">Francesco Pierri</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H+H">Ho-Chun Herbert Chang</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures, 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 Workshop Proceedings of the 17th International AAAI
  Conference on Web and Social Medi
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item816">[816]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.11596" title="Abstract">arXiv:2207.11596</a> (replaced) [<a href="/pdf/2207.11596" title="Download PDF">pdf</a>, <a href="/ps/2207.11596" title="Download PostScript">ps</a>, <a href="/format/2207.11596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructive comparison in bidding combinatorial games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kant%2C+P">Prem Kant</a>, 
<a href="/search/cs?searchtype=author&query=Larsson%2C+U">Urban Larsson</a>, 
<a href="/search/cs?searchtype=author&query=Rai%2C+R+K">Ravi K. Rai</a>, 
<a href="/search/cs?searchtype=author&query=Upasany%2C+A+V">Akshay V. Upasany</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item817">[817]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.13005" title="Abstract">arXiv:2207.13005</a> (replaced) [<a href="/pdf/2207.13005" title="Download PDF">pdf</a>, <a href="/format/2207.13005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hansel: A Chinese Few-Shot and Zero-Shot Entity Linking Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Z">Zifei Shan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WSDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item818">[818]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.13842" title="Abstract">arXiv:2207.13842</a> (replaced) [<a href="/pdf/2207.13842" title="Download PDF">pdf</a>, <a href="/format/2207.13842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dive into Machine Learning Algorithms for Influenza Virus Host  Prediction with Hemagglutinin Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yanhua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wojtczak%2C+D">Dominik Wojtczak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at BioSystems; V1: minor typo correction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item819">[819]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.03392" title="Abstract">arXiv:2208.03392</a> (replaced) [<a href="/pdf/2208.03392" title="Download PDF">pdf</a>, <a href="/format/2208.03392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning for Medical Applications: A Taxonomy, Current Trends,  Challenges, and Future Research Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rauniyar%2C+A">Ashish Rauniyar</a>, 
<a href="/search/cs?searchtype=author&query=Hagos%2C+D+H">Desta Haileselassie Hagos</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+D">Debesh Jha</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A5keg%C3%A5rd%2C+J+E">Jan Erik H&#xe5;keg&#xe5;rd</a>, 
<a href="/search/cs?searchtype=author&query=Bagci%2C+U">Ulas Bagci</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+D+B">Danda B. Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Vlassov%2C+V">Vladimir Vlassov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE Internet of Things Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item820">[820]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.06236" title="Abstract">arXiv:2208.06236</a> (replaced) [<a href="/pdf/2208.06236" title="Download PDF">pdf</a>, <a href="/format/2208.06236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Kolmogorov-Smirnov-Type Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Awan%2C+J">Jordan Awan</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+Y">Yue Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages before appendix and references. 3 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item821">[821]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.10027" title="Abstract">arXiv:2208.10027</a> (replaced) [<a href="/pdf/2208.10027" title="Download PDF">pdf</a>, <a href="/format/2208.10027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Invariant Representations under General Interventions on the  Response
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Du%2C+K">Kang Du</a>, 
<a href="/search/stat?searchtype=author&query=Xiang%2C+Y">Yu Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the IEEE Journal on Selected Areas in Information Theory. Special Issue: Causality: Fundamental Limits and Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item822">[822]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.00685" title="Abstract">arXiv:2209.00685</a> (replaced) [<a href="/pdf/2209.00685" title="Download PDF">pdf</a>, <a href="/format/2209.00685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SecDDR: Enabling Low-Cost Secure Memories by Protecting the DDR  Interface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fakhrzadehgan%2C+A">Ali Fakhrzadehgan</a>, 
<a href="/search/cs?searchtype=author&query=Ramrakhyani%2C+P">Prakash Ramrakhyani</a>, 
<a href="/search/cs?searchtype=author&query=Qureshi%2C+M+K">Moinuddin K. Qureshi</a>, 
<a href="/search/cs?searchtype=author&query=Erez%2C+M">Mattan Erez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 53rd IEEE/IFIP DSN, Porto, Portugal, 2023, pp. 14-27
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item823">[823]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.00859" title="Abstract">arXiv:2209.00859</a> (replaced) [<a href="/pdf/2209.00859" title="Download PDF">pdf</a>, <a href="/format/2209.00859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Adaptive Mutual Decoder for OOV-STR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinshui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Q">Qiandong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xuyang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jun Du</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+L">Lirong Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1st Place Solution to ECCV 2022 OOV-ST Challenge; ICIG 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item824">[824]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.04048" title="Abstract">arXiv:2209.04048</a> (replaced) [<a href="/pdf/2209.04048" title="Download PDF">pdf</a>, <a href="/format/2209.04048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Studying Drowsiness Detection Performance while Driving through Scalable  Machine Learning Models using Electroencephalography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rogel%2C+J+M+H">Jos&#xe9; Manuel Hidalgo Rogel</a>, 
<a href="/search/eess?searchtype=author&query=Beltr%C3%A1n%2C+E+T+M">Enrique Tom&#xe1;s Mart&#xed;nez Beltr&#xe1;n</a>, 
<a href="/search/eess?searchtype=author&query=P%C3%A9rez%2C+M+Q">Mario Quiles P&#xe9;rez</a>, 
<a href="/search/eess?searchtype=author&query=Bernal%2C+S+L">Sergio L&#xf3;pez Bernal</a>, 
<a href="/search/eess?searchtype=author&query=P%C3%A9rez%2C+G+M">Gregorio Mart&#xed;nez P&#xe9;rez</a>, 
<a href="/search/eess?searchtype=author&query=Celdr%C3%A1n%2C+A+H">Alberto Huertas Celdr&#xe1;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item825">[825]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07274" title="Abstract">arXiv:2209.07274</a> (replaced) [<a href="/pdf/2209.07274" title="Download PDF">pdf</a>, <a href="/format/2209.07274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introducing Grid WAR: Rethinking WAR for Starting Pitchers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brill%2C+R+S">Ryan S. Brill</a>, 
<a href="/search/cs?searchtype=author&query=Wyner%2C+A+J">Abraham J. Wyner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item826">[826]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07403" title="Abstract">arXiv:2209.07403</a> (replaced) [<a href="/pdf/2209.07403" title="Download PDF">pdf</a>, <a href="/format/2209.07403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Stochastic Optimization With Large Worst-Case Lipschitz  Parameter: Optimal Rates for (Non-Smooth) Convex Losses and Extension to  Non-Convex Losses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lowy%2C+A">Andrew Lowy</a>, 
<a href="/search/cs?searchtype=author&query=Razaviyayn%2C+M">Meisam Razaviyayn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appeared in the International Conference on Algorithmic Learning Theory (ALT) 2023. This version improves the runtime bound in Theorem 6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item827">[827]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.15210" title="Abstract">arXiv:2209.15210</a> (replaced) [<a href="/pdf/2209.15210" title="Download PDF">pdf</a>, <a href="/format/2209.15210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haoran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zuxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xintong Han</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu-Gang Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item828">[828]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.15217" title="Abstract">arXiv:2209.15217</a> (replaced) [<a href="/pdf/2209.15217" title="Download PDF">pdf</a>, <a href="/format/2209.15217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperbolic VAE via Latent Gaussian Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Seunghyuk Cho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Juyong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dongwoo Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, Thirty-seventh Conference on Neural Information Processing System, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item829">[829]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.15338" title="Abstract">arXiv:2209.15338</a> (replaced) [<a href="/pdf/2209.15338" title="Download PDF">pdf</a>, <a href="/format/2209.15338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Many-body Approximation for Non-negative Tensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ghalamkari%2C+K">Kazu Ghalamkari</a>, 
<a href="/search/stat?searchtype=author&query=Sugiyama%2C+M">Mahito Sugiyama</a>, 
<a href="/search/stat?searchtype=author&query=Kawahara%2C+Y">Yoshinobu Kawahara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item830">[830]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03647" title="Abstract">arXiv:2210.03647</a> (replaced) [<a href="/pdf/2210.03647" title="Download PDF">pdf</a>, <a href="/format/2210.03647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learnware: Small Models Do Big
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhi-Hua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhi-Hao Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Cite as: Zhi-Hua Zhou, Zhi-Hao Tan. Learnware: Small models do big. Science China Information Sciences, 2024, 67(1): 112102.01-112102.12
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Science China Information Sciences, 2024, 67(1):
  112102.01-112102.12
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item831">[831]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.04074" title="Abstract">arXiv:2210.04074</a> (replaced) [<a href="/pdf/2210.04074" title="Download PDF">pdf</a>, <a href="/format/2210.04074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are All Steps Equally Important? Benchmarking Essentiality Detection of  Events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yueguan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuqian Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Dan Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item832">[832]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.04135" title="Abstract">arXiv:2210.04135</a> (replaced) [<a href="/pdf/2210.04135" title="Download PDF">pdf</a>, <a href="/format/2210.04135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pramanick%2C+S">Shraman Pramanick</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+L">Li Jing</a>, 
<a href="/search/cs?searchtype=author&query=Nag%2C+S">Sayan Nag</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiachen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+H">Hardik Shah</a>, 
<a href="/search/cs?searchtype=author&query=LeCun%2C+Y">Yann LeCun</a>, 
<a href="/search/cs?searchtype=author&query=Chellappa%2C+R">Rama Chellappa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in TMLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item833">[833]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.04317" title="Abstract">arXiv:2210.04317</a> (replaced) [<a href="/pdf/2210.04317" title="Download PDF">pdf</a>, <a href="/format/2210.04317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spectral Approach to Item Response Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Duc Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Anderson Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item834">[834]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.04802" title="Abstract">arXiv:2210.04802</a> (replaced) [<a href="/pdf/2210.04802" title="Download PDF">pdf</a>, <a href="/format/2210.04802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in  Fine-tuned Source Code Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajipour%2C+H">Hossein Hajipour</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Ning Yu</a>, 
<a href="/search/cs?searchtype=author&query=Staicu%2C+C">Cristian-Alexandru Staicu</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item835">[835]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05448" title="Abstract">arXiv:2210.05448</a> (replaced) [<a href="/pdf/2210.05448" title="Download PDF">pdf</a>, <a href="/format/2210.05448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based  Policy Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+A">Arrasy Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Carlucho%2C+I">Ignacio Carlucho</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6pner%2C+N">Niklas H&#xf6;pner</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item836">[836]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07764" title="Abstract">arXiv:2210.07764</a> (replaced) [<a href="/pdf/2210.07764" title="Download PDF">pdf</a>, <a href="/format/2210.07764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intel Labs at Ego4D Challenge 2022: A Better Baseline for Audio-Visual  Diarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+K">Kyle Min</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Validation report for the Ego4D challenge at ECCV 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item837">[837]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09903" title="Abstract">arXiv:2210.09903</a> (replaced) [<a href="/pdf/2210.09903" title="Download PDF">pdf</a>, <a href="/format/2210.09903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Convex Optimization with Unbounded Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Raunak Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Dean%2C+S">Sarah Dean</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+R">Robert Kleinberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item838">[838]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09943" title="Abstract">arXiv:2210.09943</a> (replaced) [<a href="/pdf/2210.09943" title="Download PDF">pdf</a>, <a href="/format/2210.09943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sukthanker%2C+R+S">Rhea Sanjay Sukthanker</a>, 
<a href="/search/cs?searchtype=author&query=Dooley%2C+S">Samuel Dooley</a>, 
<a href="/search/cs?searchtype=author&query=Dickerson%2C+J+P">John P. Dickerson</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+C">Colin White</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>, 
<a href="/search/cs?searchtype=author&query=Goldblum%2C+M">Micah Goldblum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item839">[839]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11925" title="Abstract">arXiv:2210.11925</a> (replaced) [<a href="/pdf/2210.11925" title="Download PDF">pdf</a>, <a href="/format/2210.11925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian  Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Noble%2C+M">Maxence Noble</a>, 
<a href="/search/stat?searchtype=author&query=De+Bortoli%2C+V">Valentin De Bortoli</a>, 
<a href="/search/stat?searchtype=author&query=Durmus%2C+A">Alain Durmus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item840">[840]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.12770" title="Abstract">arXiv:2210.12770</a> (replaced) [<a href="/pdf/2210.12770" title="Download PDF">pdf</a>, <a href="/format/2210.12770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Value of Pre-trained Language Models for Clinical Named  Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belkadi%2C+S">Samuel Belkadi</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lifeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuping Wu</a>, 
<a href="/search/cs?searchtype=author&query=Nenadic%2C+G">Goran Nenadic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> working paper - Large Language Models, Fine-tuning LLMs, Clinical NLP, Medication Mining, AI for Healthcare
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item841">[841]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.12974" title="Abstract">arXiv:2210.12974</a> (replaced) [<a href="/pdf/2210.12974" title="Download PDF">pdf</a>, <a href="/format/2210.12974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Neuron Disturbing in Fusing Heterogeneous Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Biao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuqin Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item842">[842]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13011" title="Abstract">arXiv:2210.13011</a> (replaced) [<a href="/pdf/2210.13011" title="Download PDF">pdf</a>, <a href="/format/2210.13011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Many-Actions Policy Gradient
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nauman%2C+M">Michal Nauman</a>, 
<a href="/search/cs?searchtype=author&query=Cygan%2C+M">Marek Cygan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML Proceedings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item843">[843]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.14889" title="Abstract">arXiv:2210.14889</a> (replaced) [<a href="/pdf/2210.14889" title="Download PDF">pdf</a>, <a href="/format/2210.14889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perfectly Secure Steganography Using Minimum Entropy Coupling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Witt%2C+C+S">Christian Schroeder de Witt</a>, 
<a href="/search/cs?searchtype=author&query=Sokota%2C+S">Samuel Sokota</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+J+Z">J. Zico Kolter</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>, 
<a href="/search/cs?searchtype=author&query=Strohmeier%2C+M">Martin Strohmeier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item844">[844]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.17122" title="Abstract">arXiv:2210.17122</a> (replaced) [<a href="/pdf/2210.17122" title="Download PDF">pdf</a>, <a href="/format/2210.17122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mining Word Boundaries in Speech as Naturally Annotated Word  Segmentation Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenghua Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shilin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+C">Chen Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhefeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huai%2C+B">Baoxing Huai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> latest version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item845">[845]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04922" title="Abstract">arXiv:2211.04922</a> (replaced) [<a href="/pdf/2211.04922" title="Download PDF">pdf</a>, <a href="/ps/2211.04922" title="Download PostScript">ps</a>, <a href="/format/2211.04922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decomposition of Probability Marginals for Security Games in  Max-Flow/Min-Cut Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matuschke%2C+J">Jannik Matuschke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preliminary version of this work has appeared in the proceedings of IPCO 2023 under the title "Decomposition of Probability Marginals for Security Games in Abstract Networks"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computer Science and Game Theory (cs.GT); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item846">[846]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09253" title="Abstract">arXiv:2211.09253</a> (replaced) [<a href="/pdf/2211.09253" title="Download PDF">pdf</a>, <a href="/format/2211.09253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beurling-Selberg Extremization for Dual-Blind Deconvolution Recovery in  Joint Radar-Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Monsalve%2C+J">Jonathan Monsalve</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+E">Edwin Vargas</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+K+V">Kumar Vijay Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Sadler%2C+B+M">Brian M. Sadler</a>, 
<a href="/search/cs?searchtype=author&query=Arguello%2C+H">Henry Arguello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP); Functional Analysis (math.FA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item847">[847]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09565" title="Abstract">arXiv:2211.09565</a> (replaced) [<a href="/pdf/2211.09565" title="Download PDF">pdf</a>, <a href="/format/2211.09565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Good Practices in Evaluating Transfer Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhengyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Renjue Li</a>, 
<a href="/search/cs?searchtype=author&query=Sicre%2C+R">Ronan Sicre</a>, 
<a href="/search/cs?searchtype=author&query=Amsaleg%2C+L">Laurent Amsaleg</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An extended version can be found at <a href="/abs/2310.11850">arXiv:2310.11850</a>. Code and a list of categorized attacks are available at <a href="https://github.com/ZhengyuZhao/TransferAttackEval">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item848">[848]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09944" title="Abstract">arXiv:2211.09944</a> (replaced) [<a href="/pdf/2211.09944" title="Download PDF">pdf</a>, <a href="/format/2211.09944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MelHuBERT: A simplified HuBERT on Mel spectrograms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tzu-Quan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item849">[849]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13251" title="Abstract">arXiv:2211.13251</a> (replaced) [<a href="/pdf/2211.13251" title="Download PDF">pdf</a>, <a href="/format/2211.13251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CGOF++: Controllable 3D Face Synthesis with Conditional Generative  Occupancy Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Keqiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shangzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhaoyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Quan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). This article is an extension of the NeurIPS'22 paper <a href="/abs/2206.08361">arXiv:2206.08361</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item850">[850]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14485" title="Abstract">arXiv:2211.14485</a> (replaced) [<a href="/pdf/2211.14485" title="Download PDF">pdf</a>, <a href="/format/2211.14485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FastHuman: Reconstructing High-Quality Clothed Human in Minutes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lixiang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Songyou Peng</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Q">Qijun Gan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianke Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on 3D Vision, 3DV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item851">[851]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14646" title="Abstract">arXiv:2211.14646</a> (replaced) [<a href="/pdf/2211.14646" title="Download PDF">pdf</a>, <a href="/format/2211.14646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Improved Input Masking for Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+S">Sriram Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 19 figures. Accepted at ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item852">[852]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15788" title="Abstract">arXiv:2211.15788</a> (replaced) [<a href="/pdf/2211.15788" title="Download PDF">pdf</a>, <a href="/format/2211.15788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Visual Active Search Framework for Geospatial Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Anindya Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Lanier%2C+M">Michael Lanier</a>, 
<a href="/search/cs?searchtype=author&query=Alfeld%2C+S">Scott Alfeld</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiarui Feng</a>, 
<a href="/search/cs?searchtype=author&query=Garnett%2C+R">Roman Garnett</a>, 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+N">Nathan Jacobs</a>, 
<a href="/search/cs?searchtype=author&query=Vorobeychik%2C+Y">Yevgeniy Vorobeychik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024, 24 pages, 18 figures, Code is available at: <a href="https://github.com/anindyasarkarIITH/VAS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item853">[853]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.16314" title="Abstract">arXiv:2211.16314</a> (replaced) [<a href="/pdf/2211.16314" title="Download PDF">pdf</a>, <a href="/format/2211.16314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Intersections and Differences Between Linear Statistical  Shape Models Using Markov Chain Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weiherer%2C+M">Maximilian Weiherer</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+F">Finn Klein</a>, 
<a href="/search/cs?searchtype=author&query=Egger%2C+B">Bernhard Egger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item854">[854]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00124" title="Abstract">arXiv:2212.00124</a> (replaced) [<a href="/pdf/2212.00124" title="Download PDF">pdf</a>, <a href="/format/2212.00124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Risk to Rule Them All: A Risk-Sensitive Perspective on Model-Based  Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rigter%2C+M">Marc Rigter</a>, 
<a href="/search/cs?searchtype=author&query=Lacerda%2C+B">Bruno Lacerda</a>, 
<a href="/search/cs?searchtype=author&query=Hawes%2C+N">Nick Hawes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item855">[855]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03447" title="Abstract">arXiv:2212.03447</a> (replaced) [<a href="/pdf/2212.03447" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integration of Pre-trained Protein Language Models into Geometric Deep  Learning Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lirong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+D">Dragomir Radev</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jinbo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item856">[856]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04607" title="Abstract">arXiv:2212.04607</a> (replaced) [<a href="/pdf/2212.04607" title="Download PDF">pdf</a>, <a href="/format/2212.04607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidence-Conditioned Value Functions for Offline Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Joey Hong</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aviral Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published as a paper in ICLR 2023; 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item857">[857]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04717" title="Abstract">arXiv:2212.04717</a> (replaced) [<a href="/pdf/2212.04717" title="Download PDF">pdf</a>, <a href="/format/2212.04717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Sensitivity of Reward Inference to Misspecified Human Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Joey Hong</a>, 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+K">Kush Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Dragan%2C+A">Anca Dragan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published as a paper in ICLR 2023; 17 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item858">[858]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06681" title="Abstract">arXiv:2212.06681</a> (replaced) [<a href="/pdf/2212.06681" title="Download PDF">pdf</a>, <a href="/format/2212.06681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The two sides of the Environmental Kuznets Curve: a socio-semantic  analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Menezes%2C+T">Telmo Menezes</a>, 
<a href="/search/cs?searchtype=author&query=Pottier%2C+A">Antonin Pottier</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+C">Camille Roth</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> {\OE}conomia, 13-2 (2023) 279-321
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item859">[859]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09114" title="Abstract">arXiv:2212.09114</a> (replaced) [<a href="/pdf/2212.09114" title="Download PDF">pdf</a>, <a href="/format/2212.09114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAPSTONE: Curriculum Sampling for Dense Retrieval with Document  Expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xingwei He</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+A">A-Long Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+A">Anlei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jian Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Yiu%2C+S+M">Siu Ming Yiu</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accetpted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item860">[860]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09508" title="Abstract">arXiv:2212.09508</a> (replaced) [<a href="/pdf/2212.09508" title="Download PDF">pdf</a>, <a href="/ps/2212.09508" title="Download PostScript">ps</a>, <a href="/format/2212.09508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on the smallest eigenvalue of the empirical covariance of causal  Gaussian processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ziemann%2C+I">Ingvar Ziemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item861">[861]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11850" title="Abstract">arXiv:2212.11850</a> (replaced) [<a href="/pdf/2212.11850" title="Download PDF">pdf</a>, <a href="/format/2212.11850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DYST (Did You See That?): An Amplified Covert Channel That Points To  Previously Seen Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wendzel%2C+S">Steffen Wendzel</a>, 
<a href="/search/cs?searchtype=author&query=Schmidbauer%2C+T">Tobias Schmidbauer</a>, 
<a href="/search/cs?searchtype=author&query=Zillien%2C+S">Sebastian Zillien</a>, 
<a href="/search/cs?searchtype=author&query=Keller%2C+J">J&#xf6;rg Keller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, rev 1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item862">[862]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.12474" title="Abstract">arXiv:2212.12474</a> (replaced) [<a href="/pdf/2212.12474" title="Download PDF">pdf</a>, <a href="/format/2212.12474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Informed Gaussian Process Regression Generalizes Linear PDE  Solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pf%C3%B6rtner%2C+M">Marvin Pf&#xf6;rtner</a>, 
<a href="/search/cs?searchtype=author&query=Steinwart%2C+I">Ingo Steinwart</a>, 
<a href="/search/cs?searchtype=author&query=Hennig%2C+P">Philipp Hennig</a>, 
<a href="/search/cs?searchtype=author&query=Wenger%2C+J">Jonathan Wenger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item863">[863]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.12978" title="Abstract">arXiv:2212.12978</a> (replaced) [<a href="/pdf/2212.12978" title="Download PDF">pdf</a>, <a href="/format/2212.12978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Gradient Descent Ascent Method for Nonconvex-Nonconcave  Minimax Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zheng%2C+T">Taoli Zheng</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+L">Linglingzhi Zhu</a>, 
<a href="/search/math?searchtype=author&query=So%2C+A+M">Anthony Man-Cho So</a>, 
<a href="/search/math?searchtype=author&query=Blanchet%2C+J">Jose Blanchet</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jiajin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item864">[864]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.14424" title="Abstract">arXiv:2212.14424</a> (replaced) [<a href="/pdf/2212.14424" title="Download PDF">pdf</a>, <a href="/format/2212.14424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normalizing flow neural networks by JKO scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item865">[865]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.14654" title="Abstract">arXiv:2212.14654</a> (replaced) [<a href="/pdf/2212.14654" title="Download PDF">pdf</a>, <a href="/format/2212.14654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling More Users to Benefit from Near-Field Communications: From  Linear to Circular Array
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zidong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+M">Mingyao Cui</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+L">Linglong Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TWC. In this paper, the rotational symmetry of UCA is leveraged to provide uniform and enlarged near-field regions, enabling more users to benefit from near-field communications. Simulation codes will be provided to reproduce the results in this paper: <a href="http://oa.ee.tsinghua.edu.cn/dailinglong/publications/publications.html">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item866">[866]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00278" title="Abstract">arXiv:2301.00278</a> (replaced) [<a href="/pdf/2301.00278" title="Download PDF">pdf</a>, <a href="/ps/2301.00278" title="Download PostScript">ps</a>, <a href="/format/2301.00278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Isometric path complexity of graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chakraborty%2C+D">Dibyayan Chakraborty</a>, 
<a href="/search/math?searchtype=author&query=Chalopin%2C+J">J&#xe9;r&#xe9;mie Chalopin</a>, 
<a href="/search/math?searchtype=author&query=Foucaud%2C+F">Florent Foucaud</a>, 
<a href="/search/math?searchtype=author&query=Vax%C3%A8s%2C+Y">Yann Vax&#xe8;s</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preliminary version appeared in the proceedings of the MFCS 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item867">[867]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00955" title="Abstract">arXiv:2301.00955</a> (replaced) [<a href="/pdf/2301.00955" title="Download PDF">pdf</a>, <a href="/format/2301.00955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Federated Clustering over Non-IID Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+C">Chong-Yung Chi</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q. S. Quek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item868">[868]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.01597" title="Abstract">arXiv:2301.01597</a> (replaced) [<a href="/pdf/2301.01597" title="Download PDF">pdf</a>, <a href="/format/2301.01597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Problem-Dependent Power of Quantum Neural Networks on Multi-Class  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Du%2C+Y">Yuxuan Du</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yang%2C+Y">Yibo Yang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hsieh%2C+M">Min-Hsiu Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version. Published on PRL
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. Lett. 131, 140601 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item869">[869]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04633" title="Abstract">arXiv:2301.04633</a> (replaced) [<a href="/pdf/2301.04633" title="Download PDF">pdf</a>, <a href="/ps/2301.04633" title="Download PostScript">ps</a>, <a href="/format/2301.04633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Machine Learning Inference with GPUs in ProtoDUNE Data  Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ex?searchtype=author&query=Cai%2C+T">Tejin Cai</a>, 
<a href="/search/hep-ex?searchtype=author&query=Herner%2C+K">Kenneth Herner</a>, 
<a href="/search/hep-ex?searchtype=author&query=Yang%2C+T">Tingjun Yang</a>, 
<a href="/search/hep-ex?searchtype=author&query=Wang%2C+M">Michael Wang</a>, 
<a href="/search/hep-ex?searchtype=author&query=Flechas%2C+M+A">Maria Acosta Flechas</a>, 
<a href="/search/hep-ex?searchtype=author&query=Harris%2C+P">Philip Harris</a>, 
<a href="/search/hep-ex?searchtype=author&query=Holzman%2C+B">Burt Holzman</a>, 
<a href="/search/hep-ex?searchtype=author&query=Pedro%2C+K">Kevin Pedro</a>, 
<a href="/search/hep-ex?searchtype=author&query=Tran%2C+N">Nhan Tran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures, matches accepted version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Comput Softw Big Sci 7, 11 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Experiment (hep-ex)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item870">[870]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05798" title="Abstract">arXiv:2301.05798</a> (replaced) [<a href="/pdf/2301.05798" title="Download PDF">pdf</a>, <a href="/format/2301.05798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regulating For-Hire Autonomous Vehicles for An Equitable Multimodal  Transportation Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gao%2C+J">Jing Gao</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+S">Sen Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computer Science and Game Theory (cs.GT); General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item871">[871]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.07461" title="Abstract">arXiv:2301.07461</a> (replaced) [<a href="/pdf/2301.07461" title="Download PDF">pdf</a>, <a href="/ps/2301.07461" title="Download PostScript">ps</a>, <a href="/format/2301.07461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained optimal control of monotone systems with applications to  battery fast-charging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Drummond%2C+R">Ross Drummond</a>, 
<a href="/search/math?searchtype=author&query=Courtier%2C+N+E">Nicola E. Courtier</a>, 
<a href="/search/math?searchtype=author&query=Howey%2C+D+A">David A. Howey</a>, 
<a href="/search/math?searchtype=author&query=Couto%2C+L+D">Luis D. Couto</a>, 
<a href="/search/math?searchtype=author&query=Guiver%2C+C">Chris Guiver</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item872">[872]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09028" title="Abstract">arXiv:2301.09028</a> (replaced) [<a href="/pdf/2301.09028" title="Download PDF">pdf</a>, <a href="/format/2301.09028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterization and Learning of Causal Graphs with Small Conditioning  Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kocaoglu%2C+M">Murat Kocaoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in NeurIPS'23. 41 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item873">[873]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10625" title="Abstract">arXiv:2301.10625</a> (replaced) [<a href="/pdf/2301.10625" title="Download PDF">pdf</a>, <a href="/format/2301.10625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating the Pitfalls of Active Learning Evaluation: A Systematic  Framework for Meaningful Performance Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%BCth%2C+C+T">Carsten T. L&#xfc;th</a>, 
<a href="/search/cs?searchtype=author&query=Bungert%2C+T+J">Till J. Bungert</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+L">Lukas Klein</a>, 
<a href="/search/cs?searchtype=author&query=Jaeger%2C+P+F">Paul F. Jaeger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item874">[874]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11990" title="Abstract">arXiv:2301.11990</a> (replaced) [<a href="/pdf/2301.11990" title="Download PDF">pdf</a>, <a href="/format/2301.11990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alignment with human representations supports robust few-shot learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sucholutsky%2C+I">Ilia Sucholutsky</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Spotlight at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item875">[875]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12321" title="Abstract">arXiv:2301.12321</a> (replaced) [<a href="/pdf/2301.12321" title="Download PDF">pdf</a>, <a href="/format/2301.12321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Relation Graph: A Unified Framework for Identifying Label Noise  and Outlier Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jang-Hyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H+O">Hyun Oh Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item876">[876]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12466" title="Abstract">arXiv:2301.12466</a> (replaced) [<a href="/pdf/2301.12466" title="Download PDF">pdf</a>, <a href="/format/2301.12466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernelized Cumulants: Beyond Kernel Mean Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bonnier%2C+P">Patric Bonnier</a>, 
<a href="/search/stat?searchtype=author&query=Oberhauser%2C+H">Harald Oberhauser</a>, 
<a href="/search/stat?searchtype=author&query=Szab%C3%B3%2C+Z">Zolt&#xe1;n Szab&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item877">[877]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12549" title="Abstract">arXiv:2301.12549</a> (replaced) [<a href="/pdf/2301.12549" title="Download PDF">pdf</a>, <a href="/format/2301.12549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Deterministic Robustness Certification on ImageNet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">Andy Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Leino%2C+K">Klas Leino</a>, 
<a href="/search/cs?searchtype=author&query=Fredrikson%2C+M">Matt Fredrikson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item878">[878]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13314" title="Abstract">arXiv:2301.13314</a> (replaced) [<a href="/pdf/2301.13314" title="Download PDF">pdf</a>, <a href="/format/2301.13314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oracle Complexity of Single-Loop Switching Subgradient Methods for  Non-Smooth Weakly Convex Functional Constrained Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+Y">Yankun Huang</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+Q">Qihang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work is published in the proceedings of NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item879">[879]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13721" title="Abstract">arXiv:2301.13721</a> (replaced) [<a href="/pdf/2301.13721" title="Download PDF">pdf</a>, <a href="/format/2301.13721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuwang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Y">Yan Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item880">[880]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00294" title="Abstract">arXiv:2302.00294</a> (replaced) [<a href="/pdf/2302.00294" title="Download PDF">pdf</a>, <a href="/format/2302.00294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The geometry of hidden representations of large transformer models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valeriani%2C+L">Lucrezia Valeriani</a>, 
<a href="/search/cs?searchtype=author&query=Doimo%2C+D">Diego Doimo</a>, 
<a href="/search/cs?searchtype=author&query=Cuturello%2C+F">Francesca Cuturello</a>, 
<a href="/search/cs?searchtype=author&query=Laio%2C+A">Alessandro Laio</a>, 
<a href="/search/cs?searchtype=author&query=Ansuini%2C+A">Alessio Ansuini</a>, 
<a href="/search/cs?searchtype=author&query=Cazzaniga%2C+A">Alberto Cazzaniga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item881">[881]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00368" title="Abstract">arXiv:2302.00368</a> (replaced) [<a href="/pdf/2302.00368" title="Download PDF">pdf</a>, <a href="/format/2302.00368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Time Amendment with a Coarse Classifier for Fine-Grained  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+K">Kanishk Jain</a>, 
<a href="/search/cs?searchtype=author&query=Karthik%2C+S">Shyamgopal Karthik</a>, 
<a href="/search/cs?searchtype=author&query=Gandhi%2C+V">Vineet Gandhi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures, 3 tables, Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item882">[882]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00878" title="Abstract">arXiv:2302.00878</a> (replaced) [<a href="/pdf/2302.00878" title="Download PDF">pdf</a>, <a href="/format/2302.00878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Contextual Lasso: Sparse Linear Models via Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Thompson%2C+R">Ryan Thompson</a>, 
<a href="/search/stat?searchtype=author&query=Dezfouli%2C+A">Amir Dezfouli</a>, 
<a href="/search/stat?searchtype=author&query=Kohn%2C+R">Robert Kohn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Advances in Neural Information Processing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item883">[883]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01381" title="Abstract">arXiv:2302.01381</a> (replaced) [<a href="/pdf/2302.01381" title="Download PDF">pdf</a>, <a href="/format/2302.01381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Robustness against Natural Distribution Shifts for Models with  Different Training Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhouxing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Carlini%2C+N">Nicholas Carlini</a>, 
<a href="/search/cs?searchtype=author&query=Balashankar%2C+A">Ananth Balashankar</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+L">Ludwig Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Beutel%2C+A">Alex Beutel</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yao Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item884">[884]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01560" title="Abstract">arXiv:2302.01560</a> (replaced) [<a href="/pdf/2302.01560" title="Download PDF">pdf</a>, <a href="/format/2302.01560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Describe, Explain, Plan and Select: Interactive Planning with Large  Language Models Enables Open-World Multi-Task Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+S">Shaofei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanzhou Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Anji Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yitao Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item885">[885]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01892" title="Abstract">arXiv:2302.01892</a> (replaced) [<a href="/pdf/2302.01892" title="Download PDF">pdf</a>, <a href="/format/2302.01892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonconvex Distributed Feedback Optimization for Aggregative Cooperative  Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carnevale%2C+G">Guido Carnevale</a>, 
<a href="/search/math?searchtype=author&query=Mimmo%2C+N">Nicola Mimmo</a>, 
<a href="/search/math?searchtype=author&query=Notarstefano%2C+G">Giuseppe Notarstefano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item886">[886]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02460" title="Abstract">arXiv:2302.02460</a> (replaced) [<a href="/pdf/2302.02460" title="Download PDF">pdf</a>, <a href="/format/2302.02460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonparametric Density Estimation under Distribution Drift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazzetto%2C+A">Alessio Mazzetto</a>, 
<a href="/search/cs?searchtype=author&query=Upfal%2C+E">Eli Upfal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item887">[887]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02560" title="Abstract">arXiv:2302.02560</a> (replaced) [<a href="/pdf/2302.02560" title="Download PDF">pdf</a>, <a href="/format/2302.02560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Estimation of Exposure Shifts with Neural Networks: Evaluating  the Health Benefits of Stricter Air Quality Standards in the US
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tec%2C+M">Mauricio Tec</a>, 
<a href="/search/cs?searchtype=author&query=Mudele%2C+O">Oladimeji Mudele</a>, 
<a href="/search/cs?searchtype=author&query=Josey%2C+K">Kevin Josey</a>, 
<a href="/search/cs?searchtype=author&query=Dominici%2C+F">Francesca Dominici</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item888">[888]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04610" title="Abstract">arXiv:2302.04610</a> (replaced) [<a href="/pdf/2302.04610" title="Download PDF">pdf</a>, <a href="/format/2302.04610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier-Robust Gromov-Wasserstein for Graph Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lemin Kong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiajin Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jianheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=So%2C+A+M">Anthony Man-Cho So</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item889">[889]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05259" title="Abstract">arXiv:2302.05259</a> (replaced) [<a href="/pdf/2302.05259" title="Download PDF">pdf</a>, <a href="/format/2302.05259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Star-Shaped Denoising Diffusion Probabilistic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Okhotin%2C+A">Andrey Okhotin</a>, 
<a href="/search/stat?searchtype=author&query=Molchanov%2C+D">Dmitry Molchanov</a>, 
<a href="/search/stat?searchtype=author&query=Arkhipkin%2C+V">Vladimir Arkhipkin</a>, 
<a href="/search/stat?searchtype=author&query=Bartosh%2C+G">Grigory Bartosh</a>, 
<a href="/search/stat?searchtype=author&query=Ohanesian%2C+V">Viktor Ohanesian</a>, 
<a href="/search/stat?searchtype=author&query=Alanov%2C+A">Aibek Alanov</a>, 
<a href="/search/stat?searchtype=author&query=Vetrov%2C+D">Dmitry Vetrov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item890">[890]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05666" title="Abstract">arXiv:2302.05666</a> (replaced) [<a href="/pdf/2302.05666" title="Download PDF">pdf</a>, <a href="/format/2302.05666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xuefei Ning</a>, 
<a href="/search/cs?searchtype=author&query=Blaschko%2C+M+B">Matthew B. Blaschko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item891">[891]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05763" title="Abstract">arXiv:2302.05763</a> (replaced) [<a href="/pdf/2302.05763" title="Download PDF">pdf</a>, <a href="/format/2302.05763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Multi-User Activity Recognition through Facilitated Training  Data and Deep Learning for Human-Robot Collaboration Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Semeraro%2C+F">Francesco Semeraro</a>, 
<a href="/search/cs?searchtype=author&query=Carberry%2C+J">Jon Carberry</a>, 
<a href="/search/cs?searchtype=author&query=Cangelosi%2C+A">Angelo Cangelosi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item892">[892]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06005" title="Abstract">arXiv:2302.06005</a> (replaced) [<a href="/pdf/2302.06005" title="Download PDF">pdf</a>, <a href="/format/2302.06005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-optimal learning with average H&#xf6;lder smoothness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanneke%2C+S">Steve Hanneke</a>, 
<a href="/search/cs?searchtype=author&query=Kontorovich%2C+A">Aryeh Kontorovich</a>, 
<a href="/search/cs?searchtype=author&query=Kornowski%2C+G">Guy Kornowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item893">[893]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06032" title="Abstract">arXiv:2302.06032</a> (replaced) [<a href="/pdf/2302.06032" title="Download PDF">pdf</a>, <a href="/ps/2302.06032" title="Download PostScript">ps</a>, <a href="/format/2302.06032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Non-Convex Stochastic Optimization under Generalized  Smoothness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zijian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jagabathula%2C+S">Srikanth Jagabathula</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhengyuan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The whole paper is rewritten with new results in V2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item894">[894]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07225" title="Abstract">arXiv:2302.07225</a> (replaced) [<a href="/pdf/2302.07225" title="Download PDF">pdf</a>, <a href="/format/2302.07225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounding Training Data Reconstruction in DP-SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayes%2C+J">Jamie Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Mahloujifar%2C+S">Saeed Mahloujifar</a>, 
<a href="/search/cs?searchtype=author&query=Balle%2C+B">Borja Balle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> New experiments and comparison with related work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item895">[895]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08577" title="Abstract">arXiv:2302.08577</a> (replaced) [<a href="/pdf/2302.08577" title="Download PDF">pdf</a>, <a href="/format/2302.08577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Keep it Neutral: Using Natural Language Inference to Improve Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mersinias%2C+M">Michail Mersinias</a>, 
<a href="/search/cs?searchtype=author&query=Mahowald%2C+K">Kyle Mahowald</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item896">[896]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08643" title="Abstract">arXiv:2302.08643</a> (replaced) [<a href="/pdf/2302.08643" title="Download PDF">pdf</a>, <a href="/format/2302.08643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Temporal Wavelet Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+T">Duc Thien Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M+D+T">Manh Duc Tuan Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Hy%2C+T+S">Truong Son Hy</a>, 
<a href="/search/cs?searchtype=author&query=Kondor%2C+R">Risi Kondor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2111.01940">arXiv:2111.01940</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item897">[897]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09267" title="Abstract">arXiv:2302.09267</a> (replaced) [<a href="/pdf/2302.09267" title="Download PDF">pdf</a>, <a href="/format/2302.09267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Approximation Approaches to Group Distributionally Robust  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zhen-Hua Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianbao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhi-Hua Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item898">[898]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09880" title="Abstract">arXiv:2302.09880</a> (replaced) [<a href="/pdf/2302.09880" title="Download PDF">pdf</a>, <a href="/format/2302.09880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Unbounded Machine Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurmanji%2C+M">Meghdad Kurmanji</a>, 
<a href="/search/cs?searchtype=author&query=Triantafillou%2C+P">Peter Triantafillou</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+J">Jamie Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Triantafillou%2C+E">Eleni Triantafillou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item899">[899]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10688" title="Abstract">arXiv:2302.10688</a> (replaced) [<a href="/pdf/2302.10688" title="Download PDF">pdf</a>, <a href="/format/2302.10688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Calibrating Diffusion Probabilistic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Min Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shuicheng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhijie Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item900">[900]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10850" title="Abstract">arXiv:2302.10850</a> (replaced) [<a href="/pdf/2302.10850" title="Download PDF">pdf</a>, <a href="/format/2302.10850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Dhawal Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+Y">Yinlam Chow</a>, 
<a href="/search/cs?searchtype=author&query=Tulepbergenov%2C+A">Aza Tulepbergenov</a>, 
<a href="/search/cs?searchtype=author&query=Ghavamzadeh%2C+M">Mohammad Ghavamzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Boutilier%2C+C">Craig Boutilier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item901">[901]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12247" title="Abstract">arXiv:2302.12247</a> (replaced) [<a href="/pdf/2302.12247" title="Download PDF">pdf</a>, <a href="/format/2302.12247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying &amp; Modeling Multimodal Interactions: An Information  Decomposition Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+P+P">Paul Pu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiang Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+C+K">Chun Kai Ling</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+S">Suzanne Nie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Richard Chen</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Allen%2C+N">Nicholas Allen</a>, 
<a href="/search/cs?searchtype=author&query=Auerbach%2C+R">Randy Auerbach</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+F">Faisal Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Morency%2C+L">Louis-Philippe Morency</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Code available at: <a href="https://github.com/pliang279/PID">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item902">[902]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14324" title="Abstract">arXiv:2302.14324</a> (replaced) [<a href="/pdf/2302.14324" title="Download PDF">pdf</a>, <a href="/format/2302.14324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A CS guide to the quantum singular value transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Tang%2C+E">Ewin Tang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tian%2C+K">Kevin Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages; v2 QSVT proofs more self-contained, additional result separating bounded and unbounded polynomial approximations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item903">[903]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14655" title="Abstract">arXiv:2302.14655</a> (replaced) [<a href="/pdf/2302.14655" title="Download PDF">pdf</a>, <a href="/format/2302.14655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multifidelity Approach to Robust Orbit Determination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Foss%C3%A0%2C+A">Alberto Foss&#xe0;</a>, 
<a href="/search/math?searchtype=author&query=Armellin%2C+R">Roberto Armellin</a>, 
<a href="/search/math?searchtype=author&query=Delande%2C+E">Emmanuel Delande</a>, 
<a href="/search/math?searchtype=author&query=Losacco%2C+M">Matteo Losacco</a>, 
<a href="/search/math?searchtype=author&query=Sanfedino%2C+F">Francesco Sanfedino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted for publication in Acta Astronautica
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item904">[904]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00064" title="Abstract">arXiv:2303.00064</a> (replaced) [<a href="/pdf/2303.00064" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WEARDA: Recording Wearable Sensor Data for Human Activity Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Dijk%2C+R+M+K">Richard M.K. van Dijk</a>, 
<a href="/search/cs?searchtype=author&query=Gawehns%2C+D">Daniela Gawehns</a>, 
<a href="/search/cs?searchtype=author&query=van+Leeuwen%2C+M">Matthijs van Leeuwen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted 20 January 2023; Accepted 6 July 2023; Published 26 October 2023 by the Journal of Open Research Software JORS, 11 pages, 5 figures, 3 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> van Dijk RMK, Gawehns D, van Leeuwen M 2023 WEARDA: Recording
  Wearable Sensor Data for Human Activity Monitoring. Journal of Open Research
  Software, 11:13
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item905">[905]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01179" title="Abstract">arXiv:2303.01179</a> (replaced) [<a href="/pdf/2303.01179" title="Download PDF">pdf</a>, <a href="/format/2303.01179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SHAP-IQ: Unified Approximation of any-order Shapley Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fumagalli%2C+F">Fabian Fumagalli</a>, 
<a href="/search/cs?searchtype=author&query=Muschalik%2C+M">Maximilian Muschalik</a>, 
<a href="/search/cs?searchtype=author&query=Kolpaczki%2C+P">Patrick Kolpaczki</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCllermeier%2C+E">Eyke H&#xfc;llermeier</a>, 
<a href="/search/cs?searchtype=author&query=Hammer%2C+B">Barbara Hammer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item906">[906]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01440" title="Abstract">arXiv:2303.01440</a> (replaced) [<a href="/pdf/2303.01440" title="Download PDF">pdf</a>, <a href="/format/2303.01440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Programmatic Imitation Learning from Unlabeled and Noisy Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xin%2C+J">Jimmy Xin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Linus Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jiayi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+K">Kia Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Holtz%2C+J">Jarrett Holtz</a>, 
<a href="/search/cs?searchtype=author&query=Dillig%2C+I">Isil Dillig</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item907">[907]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01621" title="Abstract">arXiv:2303.01621</a> (replaced) [<a href="/pdf/2303.01621" title="Download PDF">pdf</a>, <a href="/format/2303.01621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lamp%2C+J">Josephine Lamp</a>, 
<a href="/search/cs?searchtype=author&query=Derdzinski%2C+M">Mark Derdzinski</a>, 
<a href="/search/cs?searchtype=author&query=Hannemann%2C+C">Christopher Hannemann</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Linden%2C+J">Joost van der Linden</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+D">David Evans</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems 36 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item908">[908]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01870" title="Abstract">arXiv:2303.01870</a> (replaced) [<a href="/pdf/2303.01870" title="Download PDF">pdf</a>, <a href="/format/2303.01870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Adversarial Training for ImageNet: Architectures, Training  and Generalization across Threat Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+N+D">Naman D Singh</a>, 
<a href="/search/cs?searchtype=author&query=Croce%2C+F">Francesco Croce</a>, 
<a href="/search/cs?searchtype=author&query=Hein%2C+M">Matthias Hein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item909">[909]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02014" title="Abstract">arXiv:2303.02014</a> (replaced) [<a href="/pdf/2303.02014" title="Download PDF">pdf</a>, <a href="/format/2303.02014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Summary Statistic Privacy in Data Sharing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zinan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sekar%2C+V">Vyas Sekar</a>, 
<a href="/search/cs?searchtype=author&query=Fanti%2C+G">Giulia Fanti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item910">[910]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02265" title="Abstract">arXiv:2303.02265</a> (replaced) [<a href="/pdf/2303.02265" title="Download PDF">pdf</a>, <a href="/format/2303.02265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Influence Human Behavior with Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Joey Hong</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>, 
<a href="/search/cs?searchtype=author&query=Dragan%2C+A">Anca Dragan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023; 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item911">[911]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03908" title="Abstract">arXiv:2303.03908</a> (replaced) [<a href="/pdf/2303.03908" title="Download PDF">pdf</a>, <a href="/format/2303.03908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Client-specific Property Inference against Secure Aggregation in  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kerkouche%2C+R">Raouf Kerkouche</a>, 
<a href="/search/cs?searchtype=author&query=%C3%81cs%2C+G">Gergely &#xc1;cs</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Workshop on Privacy in the Electronic Society (WPES'23), held in conjunction with CCS'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item912">[912]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03944" title="Abstract">arXiv:2303.03944</a> (replaced) [<a href="/pdf/2303.03944" title="Download PDF">pdf</a>, <a href="/format/2303.03944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Momentum-Based Gradient Methods for Bilevel Optimization with  Nonconvex Lower-Level
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+F">Feihu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In new version of our paper, we relaxed some assumptions, updated our algorithms and added some numerical experiments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item913">[913]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04132" title="Abstract">arXiv:2303.04132</a> (replaced) [<a href="/pdf/2303.04132" title="Download PDF">pdf</a>, <a href="/format/2303.04132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and  the Case of Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Josifoski%2C+M">Martin Josifoski</a>, 
<a href="/search/cs?searchtype=author&query=Sakota%2C+M">Marija Sakota</a>, 
<a href="/search/cs?searchtype=author&query=Peyrard%2C+M">Maxime Peyrard</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item914">[914]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04745" title="Abstract">arXiv:2303.04745</a> (replaced) [<a href="/pdf/2303.04745" title="Download PDF">pdf</a>, <a href="/format/2303.04745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Theory of Correct, Incorrect, and Extrinsic Equivariance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xupeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J+Y">Jung Yeon Park</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+M">Mingxi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+G">Guanang Su</a>, 
<a href="/search/cs?searchtype=author&query=Platt%2C+R">Robert Platt</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item915">[915]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05479" title="Abstract">arXiv:2303.05479</a> (replaced) [<a href="/pdf/2303.05479" title="Download PDF">pdf</a>, <a href="/format/2303.05479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online  Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakamoto%2C+M">Mitsuhiko Nakamoto</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yuexiang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Anikait Singh</a>, 
<a href="/search/cs?searchtype=author&query=Mark%2C+M+S">Max Sobol Mark</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aviral Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. project page: <a href="https://nakamotoo.github.io/Cal-QL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item916">[916]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05660" title="Abstract">arXiv:2303.05660</a> (replaced) [<a href="/pdf/2303.05660" title="Download PDF">pdf</a>, <a href="/format/2303.05660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards better traffic volume estimation: Jointly addressing the  underdetermination and nonequilibrium problems with correlation-adaptive GNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nie%2C+T">Tong Nie</a>, 
<a href="/search/stat?searchtype=author&query=Qin%2C+G">Guoyang Qin</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+Y">Yunpeng Wang</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+J">Jian Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at Transportation Research Part C: Emerging Technologies
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item917">[917]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05679" title="Abstract">arXiv:2303.05679</a> (replaced) [<a href="/pdf/2303.05679" title="Download PDF">pdf</a>, <a href="/format/2303.05679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering with minimum spanning trees: How good can it be?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gagolewski%2C+M">Marek Gagolewski</a>, 
<a href="/search/stat?searchtype=author&query=Cena%2C+A">Anna Cena</a>, 
<a href="/search/stat?searchtype=author&query=Bartoszuk%2C+M">Maciej Bartoszuk</a>, 
<a href="/search/stat?searchtype=author&query=Brzozowski%2C+%C5%81">&#x141;ukasz Brzozowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item918">[918]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06833" title="Abstract">arXiv:2303.06833</a> (replaced) [<a href="/pdf/2303.06833" title="Download PDF">pdf</a>, <a href="/format/2303.06833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-based Planning for Symbolic Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shojaee%2C+P">Parshin Shojaee</a>, 
<a href="/search/cs?searchtype=author&query=Meidani%2C+K">Kazem Meidani</a>, 
<a href="/search/cs?searchtype=author&query=Farimani%2C+A+B">Amir Barati Farimani</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+C+K">Chandan K. Reddy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Project code at: <a href="https://github.com/deep-symbolic-mathematics/TPSR">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item919">[919]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06999" title="Abstract">arXiv:2303.06999</a> (replaced) [<a href="/pdf/2303.06999" title="Download PDF">pdf</a>, <a href="/format/2303.06999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Label Errors in Object Detection Datasets by Loss Inspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schubert%2C+M">Marius Schubert</a>, 
<a href="/search/cs?searchtype=author&query=Riedlinger%2C+T">Tobias Riedlinger</a>, 
<a href="/search/cs?searchtype=author&query=Kahl%2C+K">Karsten Kahl</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C3%B6ll%2C+D">Daniel Kr&#xf6;ll</a>, 
<a href="/search/cs?searchtype=author&query=Schoenen%2C+S">Sebastian Schoenen</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0egvi%C4%87%2C+S">Sini&#x161;a &#x160;egvi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Rottmann%2C+M">Matthias Rottmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item920">[920]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07245" title="Abstract">arXiv:2303.07245</a> (replaced) [<a href="/pdf/2303.07245" title="Download PDF">pdf</a>, <a href="/ps/2303.07245" title="Download PostScript">ps</a>, <a href="/format/2303.07245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concentration without Independence via Information Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esposito%2C+A+R">Amedeo Roberto Esposito</a>, 
<a href="/search/cs?searchtype=author&query=Mondelli%2C+M">Marco Mondelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item921">[921]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08975" title="Abstract">arXiv:2303.08975</a> (replaced) [<a href="/pdf/2303.08975" title="Download PDF">pdf</a>, <a href="/format/2303.08975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HANDLOOM: Learned Tracing of One-Dimensional Objects for Inspection and  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Viswanath%2C+V">Vainavi Viswanath</a>, 
<a href="/search/cs?searchtype=author&query=Shivakumar%2C+K">Kaushik Shivakumar</a>, 
<a href="/search/cs?searchtype=author&query=Ajmera%2C+J">Jainil Ajmera</a>, 
<a href="/search/cs?searchtype=author&query=Parulekar%2C+M">Mallika Parulekar</a>, 
<a href="/search/cs?searchtype=author&query=Kerr%2C+J">Justin Kerr</a>, 
<a href="/search/cs?searchtype=author&query=Ichnowski%2C+J">Jeffrey Ichnowski</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Richard Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Kollar%2C+T">Thomas Kollar</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+K">Ken Goldberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item922">[922]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09387" title="Abstract">arXiv:2303.09387</a> (replaced) [<a href="/pdf/2303.09387" title="Download PDF">pdf</a>, <a href="/format/2303.09387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Manipulation from AI Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carroll%2C+M">Micah Carroll</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Alan Chan</a>, 
<a href="/search/cs?searchtype=author&query=Ashton%2C+H">Henry Ashton</a>, 
<a href="/search/cs?searchtype=author&query=Krueger%2C+D">David Krueger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at EAAMO 2023; The first two authors contributed equally; author order was decided with a coin flip
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item923">[923]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09716" title="Abstract">arXiv:2303.09716</a> (replaced) [<a href="/pdf/2303.09716" title="Download PDF">pdf</a>, <a href="/ps/2303.09716" title="Download PostScript">ps</a>, <a href="/format/2303.09716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum  Markov Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Winnicki%2C+A">Anna Winnicki</a>, 
<a href="/search/cs?searchtype=author&query=Srikant%2C+R">R. Srikant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item924">[924]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10834" title="Abstract">arXiv:2303.10834</a> (replaced) [<a href="/pdf/2303.10834" title="Download PDF">pdf</a>, <a href="/format/2303.10834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object-Centric Slot Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jindong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+F">Fei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gautam Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sungjin Ahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 as a Spotlight paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item925">[925]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12175" title="Abstract">arXiv:2303.12175</a> (replaced) [<a href="/pdf/2303.12175" title="Download PDF">pdf</a>, <a href="/format/2303.12175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Black-box Backdoor Defense via Zero-shot Image Purification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yucheng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuansheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Z">Zihan Guan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ninghao Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item926">[926]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12662" title="Abstract">arXiv:2303.12662</a> (replaced) [<a href="/pdf/2303.12662" title="Download PDF">pdf</a>, <a href="/format/2303.12662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multipolar Acoustic Source Reconstruction from Sparse Far-Field Data  using ALOHA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yukun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wahab%2C+A">Abdul Wahab</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xianchao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item927">[927]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13506" title="Abstract">arXiv:2303.13506</a> (replaced) [<a href="/pdf/2303.13506" title="Download PDF">pdf</a>, <a href="/format/2303.13506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Quantization Model of Neural Scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michaud%2C+E+J">Eric J. Michaud</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Girit%2C+U">Uzay Girit</a>, 
<a href="/search/cs?searchtype=author&query=Tegmark%2C+M">Max Tegmark</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 18 figures, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn)

</div>
</div>
</dd>
<dt><a name="item928">[928]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15149" title="Abstract">arXiv:2303.15149</a> (replaced) [<a href="/pdf/2303.15149" title="Download PDF">pdf</a>, <a href="/format/2303.15149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Can Human Sketches Do for Object Detection?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+P+N">Pinaki Nath Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Bhunia%2C+A+K">Ayan Kumar Bhunia</a>, 
<a href="/search/cs?searchtype=author&query=Sain%2C+A">Aneeshan Sain</a>, 
<a href="/search/cs?searchtype=author&query=Koley%2C+S">Subhadeep Koley</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tao Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yi-Zhe Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Best Paper Finalist (Top 12 Best Papers). Presented in special single-track plenary sessions to all attendees in Computer Vision and Pattern Recognition (CVPR), 2023. Updated an error in Fig.3 (from Softmax to Cross Entropy). Thanks to the community for pointing it out
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item929">[929]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15827" title="Abstract">arXiv:2303.15827</a> (replaced) [<a href="/pdf/2303.15827" title="Download PDF">pdf</a>, <a href="/format/2303.15827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CONFIDE: Contextual Finite Differences Modelling of PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Linial%2C+O">Ori Linial</a>, 
<a href="/search/cs?searchtype=author&query=Avner%2C+O">Orly Avner</a>, 
<a href="/search/cs?searchtype=author&query=Di+Castro%2C+D">Dotan Di Castro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item930">[930]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16698" title="Abstract">arXiv:2303.16698</a> (replaced) [<a href="/pdf/2303.16698" title="Download PDF">pdf</a>, <a href="/format/2303.16698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic inverse optimal control for non-linear partially  observable systems disentangles perceptual uncertainty and behavioral costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Straub%2C+D">Dominik Straub</a>, 
<a href="/search/cs?searchtype=author&query=Schultheis%2C+M">Matthias Schultheis</a>, 
<a href="/search/cs?searchtype=author&query=Koeppl%2C+H">Heinz Koeppl</a>, 
<a href="/search/cs?searchtype=author&query=Rothkopf%2C+C+A">Constantin A. Rothkopf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item931">[931]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16787" title="Abstract">arXiv:2303.16787</a> (replaced) [<a href="/pdf/2303.16787" title="Download PDF">pdf</a>, <a href="/format/2303.16787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dispersion relation reconstruction for 2D Photonic Crystals based on  polynomial interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">Yueqi Wang</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+G">Guanglian Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item932">[932]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16813" title="Abstract">arXiv:2303.16813</a> (replaced) [<a href="/pdf/2303.16813" title="Download PDF">pdf</a>, <a href="/format/2303.16813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal approximation using complex-valued neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Geuchen%2C+P">Paul Geuchen</a>, 
<a href="/search/math?searchtype=author&query=Voigtlaender%2C+F">Felix Voigtlaender</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item933">[933]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17475" title="Abstract">arXiv:2303.17475</a> (replaced) [<a href="/pdf/2303.17475" title="Download PDF">pdf</a>, <a href="/format/2303.17475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient distributed representations beyond negative sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dall%27Amico%2C+L">Lorenzo Dall&#x27;Amico</a>, 
<a href="/search/cs?searchtype=author&query=Belliardo%2C+E+M">Enrico Maria Belliardo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item934">[934]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17503" title="Abstract">arXiv:2303.17503</a> (replaced) [<a href="/pdf/2303.17503" title="Download PDF">pdf</a>, <a href="/format/2303.17503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koyamada%2C+S">Sotetsu Koyamada</a>, 
<a href="/search/cs?searchtype=author&query=Okano%2C+S">Shinri Okano</a>, 
<a href="/search/cs?searchtype=author&query=Nishimori%2C+S">Soichiro Nishimori</a>, 
<a href="/search/cs?searchtype=author&query=Murata%2C+Y">Yu Murata</a>, 
<a href="/search/cs?searchtype=author&query=Habara%2C+K">Keigo Habara</a>, 
<a href="/search/cs?searchtype=author&query=Kita%2C+H">Haruka Kita</a>, 
<a href="/search/cs?searchtype=author&query=Ishii%2C+S">Shin Ishii</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item935">[935]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17531" title="Abstract">arXiv:2303.17531</a> (replaced) [<a href="/pdf/2303.17531" title="Download PDF">pdf</a>, <a href="/format/2303.17531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymmetric Image Retrieval with Cross Model Compatible Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Linial%2C+O">Ori Linial</a>, 
<a href="/search/cs?searchtype=author&query=Shoshan%2C+A">Alon Shoshan</a>, 
<a href="/search/cs?searchtype=author&query=Bhonker%2C+N">Nadav Bhonker</a>, 
<a href="/search/cs?searchtype=author&query=Hirsch%2C+E">Elad Hirsch</a>, 
<a href="/search/cs?searchtype=author&query=Zamir%2C+L">Lior Zamir</a>, 
<a href="/search/cs?searchtype=author&query=Kviatkovsky%2C+I">Igor Kviatkovsky</a>, 
<a href="/search/cs?searchtype=author&query=Medioni%2C+G">Gerard Medioni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item936">[936]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17618" title="Abstract">arXiv:2303.17618</a> (replaced) [<a href="/pdf/2303.17618" title="Download PDF">pdf</a>, <a href="/format/2303.17618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven abstractions via adaptive refinements and a Kantorovich  metric [extended version]
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banse%2C+A">Adrien Banse</a>, 
<a href="/search/cs?searchtype=author&query=Romao%2C+L">Licio Romao</a>, 
<a href="/search/cs?searchtype=author&query=Abate%2C+A">Alessandro Abate</a>, 
<a href="/search/cs?searchtype=author&query=Jungers%2C+R+M">Rapha&#xeb;l M. Jungers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is an extended version of a CDC2023 submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item937">[937]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00083" title="Abstract">arXiv:2304.00083</a> (replaced) [<a href="/pdf/2304.00083" title="Download PDF">pdf</a>, <a href="/format/2304.00083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generative Framework for Low-Cost Result Validation of Outsourced  Machine Learning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Abhinav Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Aguilera%2C+M+A+G">Miguel A. Guirao Aguilera</a>, 
<a href="/search/cs?searchtype=author&query=Tourani%2C+R">Reza Tourani</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+S">Satyajayant Misra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item938">[938]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00200" title="Abstract">arXiv:2304.00200</a> (replaced) [<a href="/pdf/2304.00200" title="Download PDF">pdf</a>, <a href="/ps/2304.00200" title="Download PostScript">ps</a>, <a href="/format/2304.00200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion map particle systems for generative modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+F">Fengyi Li</a>, 
<a href="/search/stat?searchtype=author&query=Marzouk%2C+Y">Youssef Marzouk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item939">[939]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00437" title="Abstract">arXiv:2304.00437</a> (replaced) [<a href="/pdf/2304.00437" title="Download PDF">pdf</a>, <a href="/format/2304.00437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisting high-resolution schemes with van-Albada slope limiter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lu%2C+J">Jingcheng Lu</a>, 
<a href="/search/math?searchtype=author&query=Tadmor%2C+E">Eitan Tadmor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item940">[940]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00686" title="Abstract">arXiv:2304.00686</a> (replaced) [<a href="/pdf/2304.00686" title="Download PDF">pdf</a>, <a href="/format/2304.00686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffuRec: A Diffusion Model for Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Aixin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenliang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item941">[941]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01624" title="Abstract">arXiv:2304.01624</a> (replaced) [<a href="/pdf/2304.01624" title="Download PDF">pdf</a>, <a href="/format/2304.01624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a family of low-rank algorithms for large-scale algebraic Riccati  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bertram%2C+C">Christian Bertram</a>, 
<a href="/search/math?searchtype=author&query=Fa%C3%9Fbender%2C+H">Heike Fa&#xdf;bender</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item942">[942]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01969" title="Abstract">arXiv:2304.01969</a> (replaced) [<a href="/pdf/2304.01969" title="Download PDF">pdf</a>, <a href="/format/2304.01969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEGClass: Extremely Weakly Supervised Text Classification via  Mutually-Enhancing Text Granularities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kargupta%2C+P">Priyanka Kargupta</a>, 
<a href="/search/cs?searchtype=author&query=Komarlu%2C+T">Tanay Komarlu</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Susik Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/pkargupta/MEGClass/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item943">[943]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02084" title="Abstract">arXiv:2304.02084</a> (replaced) [<a href="/pdf/2304.02084" title="Download PDF">pdf</a>, <a href="/format/2304.02084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri  using X-ray CT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parsons%2C+S">Stephen Parsons</a>, 
<a href="/search/cs?searchtype=author&query=Parker%2C+C+S">C. Seth Parker</a>, 
<a href="/search/cs?searchtype=author&query=Chapman%2C+C">Christy Chapman</a>, 
<a href="/search/cs?searchtype=author&query=Hayashida%2C+M">Mami Hayashida</a>, 
<a href="/search/cs?searchtype=author&query=Seales%2C+W+B">W. Brent Seales</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item944">[944]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02599" title="Abstract">arXiv:2304.02599</a> (replaced) [<a href="/pdf/2304.02599" title="Download PDF">pdf</a>, <a href="/format/2304.02599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query lower bounds for log-concave sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chewi%2C+S">Sinho Chewi</a>, 
<a href="/search/math?searchtype=author&query=de+Dios+Pont%2C+J">Jaume de Dios Pont</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jerry Li</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+C">Chen Lu</a>, 
<a href="/search/math?searchtype=author&query=Narayanan%2C+S">Shyam Narayanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item945">[945]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02742" title="Abstract">arXiv:2304.02742</a> (replaced) [<a href="/pdf/2304.02742" title="Download PDF">pdf</a>, <a href="/format/2304.02742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot Medical Image Translation via Frequency-Guided Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yunxiang Li</a>, 
<a href="/search/eess?searchtype=author&query=Shao%2C+H">Hua-Chieh Shao</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+X">Xiao Liang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+L">Liyuan Chen</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+R">Ruiqi Li</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+S">Steve Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">You Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Medical Imaging, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item946">[946]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03271" title="Abstract">arXiv:2304.03271</a> (replaced) [<a href="/pdf/2304.03271" title="Download PDF">pdf</a>, <a href="/format/2304.03271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making AI Less &quot;Thirsty&quot;: Uncovering and Addressing the Secret Water  Footprint of AI Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+A">Mohammad A. Islam</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shaolei Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> New updates include discussion on water withdrawal and water consumption, scope definition for water, and new estimates of GPT-3's water footprint based on Microsoft's new WUE and PUE data. Source codes available at: <a href="https://github.com/Ren-Research/Making-AI-Less-Thirsty">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item947">[947]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03370" title="Abstract">arXiv:2304.03370</a> (replaced) [<a href="/pdf/2304.03370" title="Download PDF">pdf</a>, <a href="/format/2304.03370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reliable learning in challenging environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balcan%2C+M">Maria-Florina Balcan</a>, 
<a href="/search/cs?searchtype=author&query=Hanneke%2C+S">Steve Hanneke</a>, 
<a href="/search/cs?searchtype=author&query=Pukdee%2C+R">Rattana Pukdee</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+D">Dravyansh Sharma</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item948">[948]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03833" title="Abstract">arXiv:2304.03833</a> (replaced) [<a href="/pdf/2304.03833" title="Download PDF">pdf</a>, <a href="/format/2304.03833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Robot Manipulation from Cross-Morphology Demonstration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salhotra%2C+G">Gautam Salhotra</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+I+A">I-Chun Arthur Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sukhatme%2C+G">Gaurav Sukhatme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Conference on Robot Learning (CoRL) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item949">[949]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04421" title="Abstract">arXiv:2304.04421</a> (replaced) [<a href="/pdf/2304.04421" title="Download PDF">pdf</a>, <a href="/format/2304.04421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local-Global Temporal Difference Learning for Satellite Video  Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Q">Qiangqiang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xianyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiang He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liangpei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chia-wen Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TCSVT
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Circuits and Systems for Video Technology,
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item950">[950]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04675" title="Abstract">arXiv:2304.04675</a> (replaced) [<a href="/pdf/2304.04675" title="Download PDF">pdf</a>, <a href="/format/2304.04675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Machine Translation with Large Language Models: Empirical  Results and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenhao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qingxiu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingjing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item951">[951]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06939" title="Abstract">arXiv:2304.06939</a> (replaced) [<a href="/pdf/2304.06939" title="Download PDF">pdf</a>, <a href="/format/2304.06939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with  Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wanrong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Awadalla%2C+A">Anas Awadalla</a>, 
<a href="/search/cs?searchtype=author&query=Gadre%2C+S+Y">Samir Yitzhak Gadre</a>, 
<a href="/search/cs?searchtype=author&query=Dodge%2C+J">Jesse Dodge</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+A">Alex Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+L">Ludwig Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS D&amp;B 2023. Project homepage: <a href="https://github.com/allenai/mmc4">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item952">[952]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07041" title="Abstract">arXiv:2304.07041</a> (replaced) [<a href="/pdf/2304.07041" title="Download PDF">pdf</a>, <a href="/format/2304.07041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Diffusion model for POI recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yifang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongjun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+W">Wei Ju</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Ming Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Transactions on Information Systems (TOIS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item953">[953]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07056" title="Abstract">arXiv:2304.07056</a> (replaced) [<a href="/pdf/2304.07056" title="Download PDF">pdf</a>, <a href="/format/2304.07056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual Quality Assessment of Face Video Compression: A Benchmark and  An Effective Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yixuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+B">Bolin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+B">Baoliang Chen</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+M">Meng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shiqi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+W">Weisi Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item954">[954]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07363" title="Abstract">arXiv:2304.07363</a> (replaced) [<a href="/pdf/2304.07363" title="Download PDF">pdf</a>, <a href="/format/2304.07363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Integrated Cyber-Physical Risk Assessment Framework for Worst-Case  Attacks in Industrial Control Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aftabi%2C+N">Navid Aftabi</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+D">Dan Li</a>, 
<a href="/search/math?searchtype=author&query=D.%2C+P">Ph.D.</a>, 
<a href="/search/math?searchtype=author&query=Sharkey%2C+T">Thomas Sharkey</a>, 
<a href="/search/math?searchtype=author&query=D%2C+P">Ph.D</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item955">[955]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07441" title="Abstract">arXiv:2304.07441</a> (replaced) [<a href="/pdf/2304.07441" title="Download PDF">pdf</a>, <a href="/format/2304.07441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Scalable Massively Parallel Algorithms for Embedded Planar Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi-Jun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D+W">Da Wei Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in SODA24. 55 pages, 9 figures, 1 table. Added section on weighted edit distance and shortened abstract
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item956">[956]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07504" title="Abstract">arXiv:2304.07504</a> (replaced) [<a href="/pdf/2304.07504" title="Download PDF">pdf</a>, <a href="/format/2304.07504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Distributed Optimization under Average Second-order  Similarity: Algorithms and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dachao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yuze Han</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haishan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihua Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready version for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item957">[957]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08315" title="Abstract">arXiv:2304.08315</a> (replaced) [<a href="/pdf/2304.08315" title="Download PDF">pdf</a>, <a href="/format/2304.08315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thorny Roses: Investigating the Dual Use Dilemma in Natural Language  Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaffee%2C+L">Lucie-Aim&#xe9;e Kaffee</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Arnav Arora</a>, 
<a href="/search/cs?searchtype=author&query=Talat%2C+Z">Zeerak Talat</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item958">[958]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10127" title="Abstract">arXiv:2304.10127</a> (replaced) [<a href="/pdf/2304.10127" title="Download PDF">pdf</a>, <a href="/format/2304.10127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Sample Difficulty from Pre-trained Models for Reliable  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+P">Peng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhijie Deng</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yinpeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item959">[959]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10440" title="Abstract">arXiv:2304.10440</a> (replaced) [<a href="/pdf/2304.10440" title="Download PDF">pdf</a>, <a href="/format/2304.10440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sima%2C+C">Chonghao Sima</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bangjun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+P">Peijin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shengyin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+F">Feng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongyang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023 Track on Datasets and Benchmarks | OpenLane-V2 Dataset: <a href="https://github.com/OpenDriveLab/OpenLane-V2">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item960">[960]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10921" title="Abstract">arXiv:2304.10921</a> (replaced) [<a href="/pdf/2304.10921" title="Download PDF">pdf</a>, <a href="/format/2304.10921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-Based Distributed Controller Design Over Directed Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Watanabe%2C+Y">Yuto Watanabe</a>, 
<a href="/search/eess?searchtype=author&query=Sakurama%2C+K">Kazunori Sakurama</a>, 
<a href="/search/eess?searchtype=author&query=Ahn%2C+H">Hyo-Sung Ahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item961">[961]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10932" title="Abstract">arXiv:2304.10932</a> (replaced) [<a href="/pdf/2304.10932" title="Download PDF">pdf</a>, <a href="/format/2304.10932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Dictionaries from Physical-Based Interpolation for Water  Network Leak Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Irofti%2C+P">Paul Irofti</a>, 
<a href="/search/eess?searchtype=author&query=Romero-Ben%2C+L">Luis Romero-Ben</a>, 
<a href="/search/eess?searchtype=author&query=Stoican%2C+F">Florin Stoican</a>, 
<a href="/search/eess?searchtype=author&query=Puig%2C+V">Vicen&#xe7; Puig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item962">[962]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11327" title="Abstract">arXiv:2304.11327</a> (replaced) [<a href="/pdf/2304.11327" title="Download PDF">pdf</a>, <a href="/format/2304.11327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Improving Feature Learning for Out-of-Distribution  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaiwen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">James Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Yongqiang Chen, Wei Huang, and Kaiwen Zhou contributed equally; NeurIPS 2023, 55 pages, 64 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item963">[963]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12032" title="Abstract">arXiv:2304.12032</a> (replaced) [<a href="/pdf/2304.12032" title="Download PDF">pdf</a>, <a href="/format/2304.12032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> lifex-cfd: an open-source computational fluid dynamics solver for  cardiovascular applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Africa%2C+P+C">Pasquale Claudio Africa</a>, 
<a href="/search/physics?searchtype=author&query=Fumagalli%2C+I">Ivan Fumagalli</a>, 
<a href="/search/physics?searchtype=author&query=Bucelli%2C+M">Michele Bucelli</a>, 
<a href="/search/physics?searchtype=author&query=Zingaro%2C+A">Alberto Zingaro</a>, 
<a href="/search/physics?searchtype=author&query=Fedele%2C+M">Marco Fedele</a>, 
<a href="/search/physics?searchtype=author&query=Dede%27%2C+L">Luca Dede&#x27;</a>, 
<a href="/search/physics?searchtype=author&query=Quarteroni%2C+A">Alfio Quarteroni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Mathematical Software (cs.MS); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item964">[964]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12633" title="Abstract">arXiv:2304.12633</a> (replaced) [<a href="/pdf/2304.12633" title="Download PDF">pdf</a>, <a href="/format/2304.12633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PUNR: Pre-training with User Behavior Modeling for News Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+G">Guangyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongtao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Wanhui Qian</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Z">Zhepeng Lv</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Songlin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP23. Github Repo: <a href="https://github.com/ma787639046/punr">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item965">[965]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13060" title="Abstract">arXiv:2304.13060</a> (replaced) [<a href="/pdf/2304.13060" title="Download PDF">pdf</a>, <a href="/format/2304.13060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Injecting structural hints: Using language models to study inductive  biases in language learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papadimitriou%2C+I">Isabel Papadimitriou</a>, 
<a href="/search/cs?searchtype=author&query=Jurafsky%2C+D">Dan Jurafsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item966">[966]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13949" title="Abstract">arXiv:2304.13949</a> (replaced) [<a href="/pdf/2304.13949" title="Download PDF">pdf</a>, <a href="/format/2304.13949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UCF: Uncovering Common Features for Generalizable Deepfake Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiyuan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yanbo Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Baoyuan Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item967">[967]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14997" title="Abstract">arXiv:2304.14997</a> (replaced) [<a href="/pdf/2304.14997" title="Download PDF">pdf</a>, <a href="/format/2304.14997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Automated Circuit Discovery for Mechanistic Interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conmy%2C+A">Arthur Conmy</a>, 
<a href="/search/cs?searchtype=author&query=Mavor-Parker%2C+A+N">Augustine N. Mavor-Parker</a>, 
<a href="/search/cs?searchtype=author&query=Lynch%2C+A">Aengus Lynch</a>, 
<a href="/search/cs?searchtype=author&query=Heimersheim%2C+S">Stefan Heimersheim</a>, 
<a href="/search/cs?searchtype=author&query=Garriga-Alonso%2C+A">Adri&#xe0; Garriga-Alonso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item968">[968]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00418" title="Abstract">arXiv:2305.00418</a> (replaced) [<a href="/pdf/2305.00418" title="Download PDF">pdf</a>, <a href="/format/2305.00418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of Using Large Language Models for Unit Test  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siddiq%2C+M+L">Mohammed Latif Siddiq</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+J+C+S">Joanna C. S. Santos</a>, 
<a href="/search/cs?searchtype=author&query=Tanvir%2C+R+H">Ridwanul Hasan Tanvir</a>, 
<a href="/search/cs?searchtype=author&query=Ulfat%2C+N">Noshin Ulfat</a>, 
<a href="/search/cs?searchtype=author&query=Rifat%2C+F+A">Fahmid Al Rifat</a>, 
<a href="/search/cs?searchtype=author&query=Lopes%2C+V+C">Vinicius Carvalho Lopes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to Journal of Systems and Software; 36 pages, 4 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item969">[969]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00478" title="Abstract">arXiv:2305.00478</a> (replaced) [<a href="/pdf/2305.00478" title="Download PDF">pdf</a>, <a href="/format/2305.00478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Agnostic Fourier Neural Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jafarzadeh%2C+S">Siavash Jafarzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item970">[970]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01323" title="Abstract">arXiv:2305.01323</a> (replaced) [<a href="/pdf/2305.01323" title="Download PDF">pdf</a>, <a href="/format/2305.01323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Turning Flowchart into Dialog: Augmenting Flowchart-grounded  Troubleshooting Dialogs via Synthetic Data Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Haolan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Maruf%2C+S">Sameen Maruf</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Lizhen Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zukerman%2C+I">Ingrid Zukerman</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ALTA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item971">[971]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02109" title="Abstract">arXiv:2305.02109</a> (replaced) [<a href="/pdf/2305.02109" title="Download PDF">pdf</a>, <a href="/format/2305.02109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synergies Between Federated Learning and O-RAN: Towards an Elastic  Virtualized Architecture for Multiple Distributed Machine Learning Services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdisarabshali%2C+P">Payam Abdisarabshali</a>, 
<a href="/search/cs?searchtype=author&query=Accurso%2C+N">Nicholas Accurso</a>, 
<a href="/search/cs?searchtype=author&query=Malandra%2C+F">Filippo Malandra</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+W">Weifeng Su</a>, 
<a href="/search/cs?searchtype=author&query=Hosseinalipour%2C+S">Seyyedali Hosseinalipour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item972">[972]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02252" title="Abstract">arXiv:2305.02252</a> (replaced) [<a href="/pdf/2305.02252" title="Download PDF">pdf</a>, <a href="/ps/2305.02252" title="Download PostScript">ps</a>, <a href="/format/2305.02252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adaptive Algorithm for Learning with Unknown Distribution Drift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazzetto%2C+A">Alessio Mazzetto</a>, 
<a href="/search/cs?searchtype=author&query=Upfal%2C+E">Eli Upfal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version for Camera-ready with minor changes in text for readability, and including a new small section on linear regression
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item973">[973]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02734" title="Abstract">arXiv:2305.02734</a> (replaced) [<a href="/pdf/2305.02734" title="Download PDF">pdf</a>, <a href="/format/2305.02734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly-supervised Micro- and Macro-expression Spotting Based on  Multi-level Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wang-Wang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kai-Fu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hong-Mei Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong-Jie Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item974">[974]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03288" title="Abstract">arXiv:2305.03288</a> (replaced) [<a href="/pdf/2305.03288" title="Download PDF">pdf</a>, <a href="/format/2305.03288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Softmax Gating Function in Gaussian Mixture of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+H">Huy Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+T">TrungTin Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item975">[975]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03510" title="Abstract">arXiv:2305.03510</a> (replaced) [<a href="/pdf/2305.03510" title="Download PDF">pdf</a>, <a href="/format/2305.03510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Efficient Cross-lingual Transfer of Vision and Language Models  via Translation-based Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jialu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+E">Xin Eric Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item976">[976]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03598" title="Abstract">arXiv:2305.03598</a> (replaced) [<a href="/pdf/2305.03598" title="Download PDF">pdf</a>, <a href="/format/2305.03598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial  Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jullien%2C+M">Ma&#xeb;l Jullien</a>, 
<a href="/search/cs?searchtype=author&query=Valentino%2C+M">Marco Valentino</a>, 
<a href="/search/cs?searchtype=author&query=Frost%2C+H">Hannah Frost</a>, 
<a href="/search/cs?searchtype=author&query=O%27Regan%2C+P">Paul O&#x27;Regan</a>, 
<a href="/search/cs?searchtype=author&query=Landers%2C+D">Donal Landers</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+A">Andr&#xe9; Freitas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Camera-ready, 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item977">[977]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04556" title="Abstract">arXiv:2305.04556</a> (replaced) [<a href="/pdf/2305.04556" title="Download PDF">pdf</a>, <a href="/format/2305.04556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Autoregressive Math Word Problem Solver with Unified Tree Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bin%2C+Y">Yi Bin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+M">Mengqun Han</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenhao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H+T">Heng Tao Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item978">[978]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05332" title="Abstract">arXiv:2305.05332</a> (replaced) [<a href="/pdf/2305.05332" title="Download PDF">pdf</a>, <a href="/ps/2305.05332" title="Download PostScript">ps</a>, <a href="/format/2305.05332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Multi-Message Private Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gholami%2C+A">Ali Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/cs?searchtype=author&query=Jahani-Nezhad%2C+T">Tayyebeh Jahani-Nezhad</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hua Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+M">Mingyue Ji</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item979">[979]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05803" title="Abstract">arXiv:2305.05803</a> (replaced) [<a href="/pdf/2305.05803" title="Download PDF">pdf</a>, <a href="/format/2305.05803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly  Supervised Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianle Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+Z">Zheda Mai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruiwen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+W">Wei-lun Chao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tianle Chen and Zheda Mai contributed equally to this work. Accepted to NeurIPS2023 ICBINB Workshop Our code is available at \url{<a href="https://github.com/cskyl/SAM_WSSS">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item980">[980]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06156" title="Abstract">arXiv:2305.06156</a> (replaced) [<a href="/pdf/2305.06156" title="Download PDF">pdf</a>, <a href="/format/2305.06156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Vault: A Comprehensive Multilingual Dataset for Advancing Code  Understanding and Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manh%2C+D+N">Dung Nguyen Manh</a>, 
<a href="/search/cs?searchtype=author&query=Hai%2C+N+L">Nam Le Hai</a>, 
<a href="/search/cs?searchtype=author&query=Dau%2C+A+T+V">Anh T. V. Dau</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A+M">Anh Minh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nghiem%2C+K">Khanh Nghiem</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+N+D+Q">Nghi D. Q. Bui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023, Long Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item981">[981]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06654" title="Abstract">arXiv:2305.06654</a> (replaced) [<a href="/pdf/2305.06654" title="Download PDF">pdf</a>, <a href="/ps/2305.06654" title="Download PostScript">ps</a>, <a href="/format/2305.06654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Privacy-Preserving Coded Computing With Hierarchical Task  Partitioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qicheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+Z">Zhaojun Nan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sheng Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item982">[982]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06773" title="Abstract">arXiv:2305.06773</a> (replaced) [<a href="/pdf/2305.06773" title="Download PDF">pdf</a>, <a href="/format/2305.06773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Better Understanding of the Computer Vision Research Community  in Africa
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Omotayo%2C+A">Abdul-Hakeem Omotayo</a>, 
<a href="/search/cs?searchtype=author&query=Gamal%2C+M">Mai Gamal</a>, 
<a href="/search/cs?searchtype=author&query=Ehab%2C+E">Eman Ehab</a>, 
<a href="/search/cs?searchtype=author&query=Dovonon%2C+G">Gbetondji Dovonon</a>, 
<a href="/search/cs?searchtype=author&query=Akinjobi%2C+Z">Zainab Akinjobi</a>, 
<a href="/search/cs?searchtype=author&query=Lukman%2C+I">Ismaila Lukman</a>, 
<a href="/search/cs?searchtype=author&query=Turki%2C+H">Houcemeddine Turki</a>, 
<a href="/search/cs?searchtype=author&query=Abdien%2C+M">Mahmod Abdien</a>, 
<a href="/search/cs?searchtype=author&query=Tondji%2C+I">Idriss Tondji</a>, 
<a href="/search/cs?searchtype=author&query=Oppong%2C+A">Abigail Oppong</a>, 
<a href="/search/cs?searchtype=author&query=Pimi%2C+Y">Yvan Pimi</a>, 
<a href="/search/cs?searchtype=author&query=Gamal%2C+K">Karim Gamal</a>, 
<a href="/search/cs?searchtype=author&query=Ro%27ya-CV4Africa">Ro&#x27;ya-CV4Africa</a>, 
<a href="/search/cs?searchtype=author&query=Siam%2C+M">Mennatullah Siam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in EAAMO'23 under ACM License. This work is part of our African computer vision grassroots research in Ro'ya - CV4Africa, <a href="https://ro-ya-cv4africa.github.io/homepage/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item983">[983]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06807" title="Abstract">arXiv:2305.06807</a> (replaced) [<a href="/pdf/2305.06807" title="Download PDF">pdf</a>, <a href="/format/2305.06807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Design in Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yue Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+H">Hongyuan Zha</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baoxiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item984">[984]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06908" title="Abstract">arXiv:2305.06908</a> (replaced) [<a href="/pdf/2305.06908" title="Download PDF">pdf</a>, <a href="/format/2305.06908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zhen Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+W">Wei Xue</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qifeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yike Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item985">[985]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07017" title="Abstract">arXiv:2305.07017</a> (replaced) [<a href="/pdf/2305.07017" title="Download PDF">pdf</a>, <a href="/format/2305.07017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Inverse Scaling Law for CLIP Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zeyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Cihang Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item986">[986]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07116" title="Abstract">arXiv:2305.07116</a> (replaced) [<a href="/pdf/2305.07116" title="Download PDF">pdf</a>, <a href="/format/2305.07116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy cost and machine learning accuracy impact of k-anonymisation and  synthetic data techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Reus%2C+P">Pepijn de Reus</a>, 
<a href="/search/cs?searchtype=author&query=Oprescu%2C+A">Ana Oprescu</a>, 
<a href="/search/cs?searchtype=author&query=van+Elsen%2C+K">Koen van Elsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the proceedings (Pages: 57-65) of The International Conference on Information and Communications Technology for Sustainability (ICT4S) 2023 in Rennes, France. 9 pages, 4 figures, 5 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 International Conference on ICT for Sustainability (ICT4S),
  Pages: 57-65
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item987">[987]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08381" title="Abstract">arXiv:2305.08381</a> (replaced) [<a href="/pdf/2305.08381" title="Download PDF">pdf</a>, <a href="/format/2305.08381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-efficient Tuning of Large-scale Multimodal Foundation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haixin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Jianlong Chang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Dian Jin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jinan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qi Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item988">[988]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08420" title="Abstract">arXiv:2305.08420</a> (replaced) [<a href="/pdf/2305.08420" title="Download PDF">pdf</a>, <a href="/format/2305.08420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RelaMiX: Exploring Few-Shot Adaptation in Video-based Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Kunyu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+D">Di Wen</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+D">David Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sarfraz%2C+M+S">M. Saquib Sarfraz</a>, 
<a href="/search/cs?searchtype=author&query=Stiefelhagen%2C+R">Rainer Stiefelhagen</a>, 
<a href="/search/cs?searchtype=author&query=Roitberg%2C+A">Alina Roitberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Benchmarks and source code are made publicly available at <a href="https://github.com/KPeng9510/RelaMiX">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item989">[989]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08528" title="Abstract">arXiv:2305.08528</a> (replaced) [<a href="/pdf/2305.08528" title="Download PDF">pdf</a>, <a href="/format/2305.08528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NICOL: A Neuro-inspired Collaborative Semi-humanoid Robot that Bridges  Social Interaction and Reliable Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kerzel%2C+M">Matthias Kerzel</a>, 
<a href="/search/cs?searchtype=author&query=Allgeuer%2C+P">Philipp Allgeuer</a>, 
<a href="/search/cs?searchtype=author&query=Strahl%2C+E">Erik Strahl</a>, 
<a href="/search/cs?searchtype=author&query=Frick%2C+N">Nicolas Frick</a>, 
<a href="/search/cs?searchtype=author&query=Habekost%2C+J">Jan-Gerrit Habekost</a>, 
<a href="/search/cs?searchtype=author&query=Eppe%2C+M">Manfred Eppe</a>, 
<a href="/search/cs?searchtype=author&query=Wermter%2C+S">Stefan Wermter</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published in IEEE Access 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item990">[990]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09073" title="Abstract">arXiv:2305.09073</a> (replaced) [<a href="/pdf/2305.09073" title="Download PDF">pdf</a>, <a href="/format/2305.09073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consensus and Subjectivity of Skin Tone Annotation for ML Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schumann%2C+C">Candice Schumann</a>, 
<a href="/search/cs?searchtype=author&query=Olanubi%2C+G+O">Gbolahan O. Olanubi</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+A">Auriel Wright</a>, 
<a href="/search/cs?searchtype=author&query=Monk%2C+E">Ellis Monk Jr.</a>, 
<a href="/search/cs?searchtype=author&query=Heldreth%2C+C">Courtney Heldreth</a>, 
<a href="/search/cs?searchtype=author&query=Ricco%2C+S">Susanna Ricco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item991">[991]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09274" title="Abstract">arXiv:2305.09274</a> (replaced) [<a href="/pdf/2305.09274" title="Download PDF">pdf</a>, <a href="/format/2305.09274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReMatching: Low-Resolution Representations for Scalable Shape  Correspondence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maggioli%2C+F">Filippo Maggioli</a>, 
<a href="/search/cs?searchtype=author&query=Baieri%2C+D">Daniele Baieri</a>, 
<a href="/search/cs?searchtype=author&query=Melzi%2C+S">Simone Melzi</a>, 
<a href="/search/cs?searchtype=author&query=Rodol%C3%A0%2C+E">Emanuele Rodol&#xe0;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item992">[992]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09821" title="Abstract">arXiv:2305.09821</a> (replaced) [<a href="/pdf/2305.09821" title="Download PDF">pdf</a>, <a href="/format/2305.09821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-Photon Counting Receivers for Optical Wireless Communications in  Future 6G Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shenjie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chitnis%2C+D">Danial Chitnis</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Haas%2C+H">Harald Haas</a>, 
<a href="/search/cs?searchtype=author&query=Khalighi%2C+M">Mohammad-Ali Khalighi</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+R+K">Robert K. Henderson</a>, 
<a href="/search/cs?searchtype=author&query=Safari%2C+M">Majid Safari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item993">[993]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09956" title="Abstract">arXiv:2305.09956</a> (replaced) [<a href="/pdf/2305.09956" title="Download PDF">pdf</a>, <a href="/ps/2305.09956" title="Download PostScript">ps</a>, <a href="/format/2305.09956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Adversarial Consistency of Surrogate Risks for Binary Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frank%2C+N">Natalie Frank</a>, 
<a href="/search/cs?searchtype=author&query=Niles-Weed%2C+J">Jonathan Niles-Weed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, published in NeurIps 2023. version 2: reorganized Section 4 and added proofs of the approximate complimentary slackness theorems. arXiv admin note: text overlap with <a href="/abs/2206.09099">arXiv:2206.09099</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item994">[994]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10037" title="Abstract">arXiv:2305.10037</a> (replaced) [<a href="/pdf/2305.10037" title="Download PDF">pdf</a>, <a href="/format/2305.10037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Language Models Solve Graph Problems in Natural Language?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Heng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaochuang Han</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item995">[995]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10156" title="Abstract">arXiv:2305.10156</a> (replaced) [<a href="/pdf/2305.10156" title="Download PDF">pdf</a>, <a href="/format/2305.10156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personality Understanding of Fictional Characters during Book Reading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiangnan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shunyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+W">Wenjie Pang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaochen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhou Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item996">[996]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10519" title="Abstract">arXiv:2305.10519</a> (replaced) [<a href="/pdf/2305.10519" title="Download PDF">pdf</a>, <a href="/format/2305.10519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Knowledge Assessment for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qingxiu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingjing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item997">[997]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10626" title="Abstract">arXiv:2305.10626</a> (replaced) [<a href="/pdf/2305.10626" title="Download PDF">pdf</a>, <a href="/format/2305.10626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models Meet World Models: Embodied Experiences Enhance Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jiannan Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+T">Tianhua Tao</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+T">Tianmin Shu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zichao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiting Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item998">[998]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10855" title="Abstract">arXiv:2305.10855</a> (replaced) [<a href="/pdf/2305.10855" title="Download PDF">pdf</a>, <a href="/format/2305.10855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TextDiffuser: Diffusion Models as Text Painters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingye Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yupan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+T">Tengchao Lv</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Lei Cui</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item999">[999]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11132" title="Abstract">arXiv:2305.11132</a> (replaced) [<a href="/pdf/2305.11132" title="Download PDF">pdf</a>, <a href="/format/2305.11132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attacks on Online Learners: a Teacher-Student Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Margiotta%2C+R+G">Riccardo Giuseppe Margiotta</a>, 
<a href="/search/stat?searchtype=author&query=Goldt%2C+S">Sebastian Goldt</a>, 
<a href="/search/stat?searchtype=author&query=Sanguinetti%2C+G">Guido Sanguinetti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Statistical Mechanics (cond-mat.stat-mech); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1000">[1000]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11317" title="Abstract">arXiv:2305.11317</a> (replaced) [<a href="/pdf/2305.11317" title="Download PDF">pdf</a>, <a href="/format/2305.11317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Generative AI: Integrating GPT-k for Efficient Editing in  Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wanrong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yujie Lu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tsu-Jui Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+E">Xin Eric Wang</a>, 
<a href="/search/cs?searchtype=author&query=Eckstein%2C+M">Miguel Eckstein</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1001">[1001]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11362" title="Abstract">arXiv:2305.11362</a> (replaced) [<a href="/e-print/2305.11362" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Online Item Pricing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huh%2C+J+S">Joon Suk Huh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Will be merged into a new work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1002">[1002]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11826" title="Abstract">arXiv:2305.11826</a> (replaced) [<a href="/pdf/2305.11826" title="Download PDF">pdf</a>, <a href="/format/2305.11826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReTAG: Reasoning Aware Table to Analytic Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosal%2C+D">Deepanway Ghosal</a>, 
<a href="/search/cs?searchtype=author&query=Nema%2C+P">Preksha Nema</a>, 
<a href="/search/cs?searchtype=author&query=Raghuveer%2C+A">Aravindan Raghuveer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1003">[1003]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12056" title="Abstract">arXiv:2305.12056</a> (replaced) [<a href="/pdf/2305.12056" title="Download PDF">pdf</a>, <a href="/ps/2305.12056" title="Download PostScript">ps</a>, <a href="/format/2305.12056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform-in-Time Wasserstein Stability Bounds for (Noisy) Stochastic  Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhu%2C+L">Lingjiong Zhu</a>, 
<a href="/search/stat?searchtype=author&query=Gurbuzbalaban%2C+M">Mert Gurbuzbalaban</a>, 
<a href="/search/stat?searchtype=author&query=Raj%2C+A">Anant Raj</a>, 
<a href="/search/stat?searchtype=author&query=Simsekli%2C+U">Umut Simsekli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 pages, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1004">[1004]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12147" title="Abstract">arXiv:2305.12147</a> (replaced) [<a href="/pdf/2305.12147" title="Download PDF">pdf</a>, <a href="/format/2305.12147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LogiCoT: Logical Chain-of-Thought Instruction-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanmeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Z">Zhiyang Teng</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Leyang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qiji Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1005">[1005]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12171" title="Abstract">arXiv:2305.12171</a> (replaced) [<a href="/pdf/2305.12171" title="Download PDF">pdf</a>, <a href="/format/2305.12171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Co-Policy for Synergistic Human-Robot Collaborative Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ng%2C+E">Eley Ng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kennedy%2C+M">Monroe Kennedy III</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE Robotics and Automation Letters (RA-L). 8 pages, 7 figures, 3 tables. Supplementary material at <a href="https://sites.google.com/view/diffusion-co-policy-hrc">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1006">[1006]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12343" title="Abstract">arXiv:2305.12343</a> (replaced) [<a href="/pdf/2305.12343" title="Download PDF">pdf</a>, <a href="/format/2305.12343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy and energy conservation for thermal atmospheric dynamics using  mixed compatible finite elements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ricardo%2C+K">Kieran Ricardo</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+D">David Lee</a>, 
<a href="/search/math?searchtype=author&query=Duru%2C+K">Kenneth Duru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1007">[1007]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12467" title="Abstract">arXiv:2305.12467</a> (replaced) [<a href="/pdf/2305.12467" title="Download PDF">pdf</a>, <a href="/format/2305.12467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Multi-phase Optimization Dynamics and Rich Nonlinear  Behaviors of ReLU Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chao Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 94 pages, NeurIPS 2023 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1008">[1008]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12577" title="Abstract">arXiv:2305.12577</a> (replaced) [<a href="/pdf/2305.12577" title="Download PDF">pdf</a>, <a href="/format/2305.12577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guided Motion Diffusion for Controllable Human Motion Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karunratanakul%2C+K">Korrawe Karunratanakul</a>, 
<a href="/search/cs?searchtype=author&query=Preechakul%2C+K">Konpat Preechakul</a>, 
<a href="/search/cs?searchtype=author&query=Suwajanakorn%2C+S">Supasorn Suwajanakorn</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siyu Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV23. Project page: <a href="https://korrawe.github.io/gmd-project/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1009">[1009]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12580" title="Abstract">arXiv:2305.12580</a> (replaced) [<a href="/pdf/2305.12580" title="Download PDF">pdf</a>, <a href="/format/2305.12580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Bidirectional Decoding: Case Study in Morphological  Inflection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Canby%2C+M+E">Marc E. Canby</a>, 
<a href="/search/cs?searchtype=author&query=Hockenmaier%2C+J">Julia Hockenmaier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1010">[1010]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13797" title="Abstract">arXiv:2305.13797</a> (replaced) [<a href="/pdf/2305.13797" title="Download PDF">pdf</a>, <a href="/format/2305.13797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Assel%2C+H">Hugues Van Assel</a>, 
<a href="/search/cs?searchtype=author&query=Vayer%2C+T">Titouan Vayer</a>, 
<a href="/search/cs?searchtype=author&query=Flamary%2C+R">R&#xe9;mi Flamary</a>, 
<a href="/search/cs?searchtype=author&query=Courty%2C+N">Nicolas Courty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1011">[1011]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13876" title="Abstract">arXiv:2305.13876</a> (replaced) [<a href="/pdf/2305.13876" title="Download PDF">pdf</a>, <a href="/format/2305.13876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross3DVG: Cross-Dataset 3D Visual Grounding on Different RGB-D Scans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miyanishi%2C+T">Taiki Miyanishi</a>, 
<a href="/search/cs?searchtype=author&query=Azuma%2C+D">Daichi Azuma</a>, 
<a href="/search/cs?searchtype=author&query=Kurita%2C+S">Shuhei Kurita</a>, 
<a href="/search/cs?searchtype=author&query=Kawanabe%2C+M">Motoki Kawanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3DV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1012">[1012]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14076" title="Abstract">arXiv:2305.14076</a> (replaced) [<a href="/pdf/2305.14076" title="Download PDF">pdf</a>, <a href="/format/2305.14076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding the Dynamics of Gaussian-Stein Variational  Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+T">Tianle Liu</a>, 
<a href="/search/math?searchtype=author&query=Ghosal%2C+P">Promit Ghosal</a>, 
<a href="/search/math?searchtype=author&query=Balasubramanian%2C+K">Krishnakumar Balasubramanian</a>, 
<a href="/search/math?searchtype=author&query=Pillai%2C+N+S">Natesh S. Pillai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023; 60 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Probability (math.PR); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1013">[1013]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14078" title="Abstract">arXiv:2305.14078</a> (replaced) [<a href="/pdf/2305.14078" title="Download PDF">pdf</a>, <a href="/format/2305.14078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as Commonsense Knowledge for Large-Scale Task  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zirui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+W+S">Wee Sun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+D">David Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1014">[1014]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14152" title="Abstract">arXiv:2305.14152</a> (replaced) [<a href="/pdf/2305.14152" title="Download PDF">pdf</a>, <a href="/format/2305.14152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-Efficient Fine-Tuning of Compressed Large Language Models via  sub-4-bit Integer Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeonghoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+H">Jung Hyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungdong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Joonsuk Park</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+K+M">Kang Min Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+S+J">Se Jung Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongsoo Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023. Camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1015">[1015]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14250" title="Abstract">arXiv:2305.14250</a> (replaced) [<a href="/pdf/2305.14250" title="Download PDF">pdf</a>, <a href="/format/2305.14250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models with Rationality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kassner%2C+N">Nora Kassner</a>, 
<a href="/search/cs?searchtype=author&query=Tafjord%2C+O">Oyvind Tafjord</a>, 
<a href="/search/cs?searchtype=author&query=Sabharwal%2C+A">Ashish Sabharwal</a>, 
<a href="/search/cs?searchtype=author&query=Richardson%2C+K">Kyle Richardson</a>, 
<a href="/search/cs?searchtype=author&query=Schuetze%2C+H">Hinrich Schuetze</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+P">Peter Clark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1016">[1016]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14257" title="Abstract">arXiv:2305.14257</a> (replaced) [<a href="/pdf/2305.14257" title="Download PDF">pdf</a>, <a href="/format/2305.14257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Prompting Assists Large Language Model on Web Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+A">Abishek Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+R">Robert Lo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F+F">Frank F. Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shuyan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings; Natural Language Reasoning and Structured Explanations Workshop at ACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1017">[1017]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14286" title="Abstract">arXiv:2305.14286</a> (replaced) [<a href="/pdf/2305.14286" title="Download PDF">pdf</a>, <a href="/format/2305.14286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Minartz%2C+K">Koen Minartz</a>, 
<a href="/search/cs?searchtype=author&query=Poels%2C+Y">Yoeri Poels</a>, 
<a href="/search/cs?searchtype=author&query=Koop%2C+S">Simon Koop</a>, 
<a href="/search/cs?searchtype=author&query=Menkovski%2C+V">Vlado Menkovski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1018">[1018]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14292" title="Abstract">arXiv:2305.14292</a> (replaced) [<a href="/pdf/2305.14292" title="Download PDF">pdf</a>, <a href="/format/2305.14292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WikiChat: Stopping the Hallucination of Large Language Model Chatbots by  Few-Shot Grounding on Wikipedia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Semnani%2C+S+J">Sina J. Semnani</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+V+Z">Violet Z. Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H+C">Heidi C. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+M+S">Monica S. Lam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1019">[1019]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14442" title="Abstract">arXiv:2305.14442</a> (replaced) [<a href="/pdf/2305.14442" title="Download PDF">pdf</a>, <a href="/format/2305.14442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Preconditioning and Fisher Adaptive Langevin Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Titsias%2C+M+K">Michalis K. Titsias</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item1020">[1020]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14591" title="Abstract">arXiv:2305.14591</a> (replaced) [<a href="/pdf/2305.14591" title="Download PDF">pdf</a>, <a href="/format/2305.14591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kexun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Danqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jingtao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1021">[1021]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14593" title="Abstract">arXiv:2305.14593</a> (replaced) [<a href="/pdf/2305.14593" title="Download PDF">pdf</a>, <a href="/format/2305.14593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discriminative calibration: Check Bayesian computation from simulations  and flexible classifier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yao%2C+Y">Yuling Yao</a>, 
<a href="/search/stat?searchtype=author&query=Domke%2C+J">Justin Domke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item1022">[1022]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14649" title="Abstract">arXiv:2305.14649</a> (replaced) [<a href="/pdf/2305.14649" title="Download PDF">pdf</a>, <a href="/format/2305.14649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Joint Time-frequency Domain Transformer for Multivariate Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yushu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengzhuo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinzhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+H">Hao Jing</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenlai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guangwen Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1023">[1023]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14702" title="Abstract">arXiv:2305.14702</a> (replaced) [<a href="/pdf/2305.14702" title="Download PDF">pdf</a>, <a href="/format/2305.14702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DecipherPref: Analyzing Influential Factors in Human Preference  Judgments via GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yebowen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kaiqiang Song</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Sangwoo Cho</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Foroosh%2C+H">Hassan Foroosh</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1024">[1024]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14772" title="Abstract">arXiv:2305.14772</a> (replaced) [<a href="/pdf/2305.14772" title="Download PDF">pdf</a>, <a href="/format/2305.14772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Question Answering Framework for Decontextualizing User-facing  Snippets from Scientific Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Newman%2C+B">Benjamin Newman</a>, 
<a href="/search/cs?searchtype=author&query=Soldaini%2C+L">Luca Soldaini</a>, 
<a href="/search/cs?searchtype=author&query=Fok%2C+R">Raymond Fok</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+K">Kyle Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 2 figures, 8 tables, EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1025">[1025]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14795" title="Abstract">arXiv:2305.14795</a> (replaced) [<a href="/pdf/2305.14795" title="Download PDF">pdf</a>, <a href="/format/2305.14795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop  Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zexuan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhengxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C+D">Christopher D. Manning</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. Our code and datasets are available at <a href="https://github.com/princeton-nlp/MQuAKE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1026">[1026]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14897" title="Abstract">arXiv:2305.14897</a> (replaced) [<a href="/pdf/2305.14897" title="Download PDF">pdf</a>, <a href="/format/2305.14897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text encoders bottleneck compositionality in contrastive vision-language  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamath%2C+A">Amita Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1027">[1027]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14928" title="Abstract">arXiv:2305.14928</a> (replaced) [<a href="/pdf/2305.14928" title="Download PDF">pdf</a>, <a href="/format/2305.14928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Reliable Misinformation Mitigation: Generalization, Uncertainty,  and GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pelrine%2C+K">Kellin Pelrine</a>, 
<a href="/search/cs?searchtype=author&query=Imouza%2C+A">Anne Imouza</a>, 
<a href="/search/cs?searchtype=author&query=Thibault%2C+C">Camille Thibault</a>, 
<a href="/search/cs?searchtype=author&query=Reksoprodjo%2C+M">Meilina Reksoprodjo</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+C">Caleb Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Christoph%2C+J">Joel Christoph</a>, 
<a href="/search/cs?searchtype=author&query=Rabbany%2C+R">Reihaneh Rabbany</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1028">[1028]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14974" title="Abstract">arXiv:2305.14974</a> (replaced) [<a href="/pdf/2305.14974" title="Download PDF">pdf</a>, <a href="/format/2305.14974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Block-local learning with probabilistic latent representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kappel%2C+D">David Kappel</a>, 
<a href="/search/cs?searchtype=author&query=Nazeer%2C+K+K">Khaleelulla Khan Nazeer</a>, 
<a href="/search/cs?searchtype=author&query=Fokam%2C+C+T">Cabrel Teguemne Fokam</a>, 
<a href="/search/cs?searchtype=author&query=Mayr%2C+C">Christian Mayr</a>, 
<a href="/search/cs?searchtype=author&query=Subramoney%2C+A">Anand Subramoney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 4 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1029">[1029]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15113" title="Abstract">arXiv:2305.15113</a> (replaced) [<a href="/pdf/2305.15113" title="Download PDF">pdf</a>, <a href="/ps/2305.15113" title="Download PostScript">ps</a>, <a href="/format/2305.15113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Mini Review on the utilization of Reinforcement Learning with OPC UA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schindler%2C+S">Simon Schindler</a>, 
<a href="/search/cs?searchtype=author&query=Uray%2C+M">Martin Uray</a>, 
<a href="/search/cs?searchtype=author&query=Huber%2C+S">Stefan Huber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint of Paper submitted to INDIN'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1030">[1030]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15121" title="Abstract">arXiv:2305.15121</a> (replaced) [<a href="/pdf/2305.15121" title="Download PDF">pdf</a>, <a href="/format/2305.15121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Individual Input for Deep Anomaly Detection on Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thimonier%2C+H">Hugo Thimonier</a>, 
<a href="/search/cs?searchtype=author&query=Popineau%2C+F">Fabrice Popineau</a>, 
<a href="/search/cs?searchtype=author&query=Rimmel%2C+A">Arpad Rimmel</a>, 
<a href="/search/cs?searchtype=author&query=Doan%2C+B">Bich-Li&#xea;n Doan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1031">[1031]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15155" title="Abstract">arXiv:2305.15155</a> (replaced) [<a href="/pdf/2305.15155" title="Download PDF">pdf</a>, <a href="/format/2305.15155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Momentum Provably Improves Error Feedback!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fatkhullin%2C+I">Ilyas Fatkhullin</a>, 
<a href="/search/cs?searchtype=author&query=Tyurin%2C+A">Alexander Tyurin</a>, 
<a href="/search/cs?searchtype=author&query=Richt%C3%A1rik%2C+P">Peter Richt&#xe1;rik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1032">[1032]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15267" title="Abstract">arXiv:2305.15267</a> (replaced) [<a href="/pdf/2305.15267" title="Download PDF">pdf</a>, <a href="/format/2305.15267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Energy-Based Normalizing Flow with Score-Matching Objectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chao%2C+C">Chen-Hao Chao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei-Fang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Kira%2C+Z">Zsolt Kira</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chun-Yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023. Code: <a href="https://github.com/chen-hao-chao/ebflow">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1033">[1033]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15349" title="Abstract">arXiv:2305.15349</a> (replaced) [<a href="/pdf/2305.15349" title="Download PDF">pdf</a>, <a href="/format/2305.15349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Convergence of Black-Box Variational Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kyurae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Jisu Oh</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kaiwen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi-An Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gardner%2C+J+R">Jacob R. Gardner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS'23; previous title: "Black-Box Variational Inference Converges"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Optimization and Control (math.OC); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1034">[1034]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15383" title="Abstract">arXiv:2305.15383</a> (replaced) [<a href="/pdf/2305.15383" title="Download PDF">pdf</a>, <a href="/ps/2305.15383" title="Download PostScript">ps</a>, <a href="/format/2305.15383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Minimax Regret for Online Learning with Feedback Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eldowa%2C+K">Khaled Eldowa</a>, 
<a href="/search/cs?searchtype=author&query=Esposito%2C+E">Emmanuel Esposito</a>, 
<a href="/search/cs?searchtype=author&query=Cesari%2C+T">Tommaso Cesari</a>, 
<a href="/search/cs?searchtype=author&query=Cesa-Bianchi%2C+N">Nicol&#xf2; Cesa-Bianchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1035">[1035]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15393" title="Abstract">arXiv:2305.15393</a> (replaced) [<a href="/pdf/2305.15393" title="Download PDF">pdf</a>, <a href="/format/2305.15393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LayoutGPT: Compositional Visual Planning and Generation with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Weixi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wanrong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tsu-jui Fu</a>, 
<a href="/search/cs?searchtype=author&query=Jampani%2C+V">Varun Jampani</a>, 
<a href="/search/cs?searchtype=author&query=Akula%2C+A">Arjun Akula</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuehai He</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+S">Sugato Basu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+E">Xin Eric Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1036">[1036]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15640" title="Abstract">arXiv:2305.15640</a> (replaced) [<a href="/pdf/2305.15640" title="Download PDF">pdf</a>, <a href="/format/2305.15640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Out-of-Distribution Error via Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuzhe Lu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yilong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+R">Runtian Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+A">Andrew Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Ketong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenlin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kolouri%2C+S">Soheil Kolouri</a>, 
<a href="/search/cs?searchtype=author&query=Stepputtis%2C+S">Simon Stepputtis</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+J">Joseph Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Sycara%2C+K">Katia Sycara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1037">[1037]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15759" title="Abstract">arXiv:2305.15759</a> (replaced) [<a href="/pdf/2305.15759" title="Download PDF">pdf</a>, <a href="/format/2305.15759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Latent Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lyu%2C+S">Saiyue Lyu</a>, 
<a href="/search/stat?searchtype=author&query=Vinaroz%2C+M">Margarita Vinaroz</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+M+F">Michael F. Liu</a>, 
<a href="/search/stat?searchtype=author&query=Park%2C+M">Mijung Park</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1038">[1038]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15944" title="Abstract">arXiv:2305.15944</a> (replaced) [<a href="/pdf/2305.15944" title="Download PDF">pdf</a>, <a href="/format/2305.15944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Turn Your Knowledge Graph Embeddings into Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loconte%2C+L">Lorenzo Loconte</a>, 
<a href="/search/cs?searchtype=author&query=Di+Mauro%2C+N">Nicola Di Mauro</a>, 
<a href="/search/cs?searchtype=author&query=Peharz%2C+R">Robert Peharz</a>, 
<a href="/search/cs?searchtype=author&query=Vergari%2C+A">Antonio Vergari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1039">[1039]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16052" title="Abstract">arXiv:2305.16052</a> (replaced) [<a href="/pdf/2305.16052" title="Download PDF">pdf</a>, <a href="/ps/2305.16052" title="Download PostScript">ps</a>, <a href="/format/2305.16052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Data Sharing between Competitors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsoy%2C+N">Nikita Tsoy</a>, 
<a href="/search/cs?searchtype=author&query=Konstantinov%2C+N">Nikola Konstantinov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item1040">[1040]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16186" title="Abstract">arXiv:2305.16186</a> (replaced) [<a href="/pdf/2305.16186" title="Download PDF">pdf</a>, <a href="/format/2305.16186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerated Methods for Riemannian Min-Max Optimization Ensuring Bounded  Geometric Penalties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mart%C3%ADnez-Rubio%2C+D">David Mart&#xed;nez-Rubio</a>, 
<a href="/search/math?searchtype=author&query=Roux%2C+C">Christophe Roux</a>, 
<a href="/search/math?searchtype=author&query=Criscitiello%2C+C">Christopher Criscitiello</a>, 
<a href="/search/math?searchtype=author&query=Pokutta%2C+S">Sebastian Pokutta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> added weakly-convex analysis, and some remarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1041">[1041]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16215" title="Abstract">arXiv:2305.16215</a> (replaced) [<a href="/pdf/2305.16215" title="Download PDF">pdf</a>, <a href="/format/2305.16215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Koopman Kernel Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bevanda%2C+P">Petar Bevanda</a>, 
<a href="/search/cs?searchtype=author&query=Beier%2C+M">Max Beier</a>, 
<a href="/search/cs?searchtype=author&query=Lederer%2C+A">Armin Lederer</a>, 
<a href="/search/cs?searchtype=author&query=Sosnowski%2C+S">Stefan Sosnowski</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCllermeier%2C+E">Eyke H&#xfc;llermeier</a>, 
<a href="/search/cs?searchtype=author&query=Hirche%2C+S">Sandra Hirche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY); Dynamical Systems (math.DS); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1042">[1042]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16261" title="Abstract">arXiv:2305.16261</a> (replaced) [<a href="/pdf/2305.16261" title="Download PDF">pdf</a>, <a href="/format/2305.16261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trans-Dimensional Generative Modeling via Jump Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Campbell%2C+A">Andrew Campbell</a>, 
<a href="/search/stat?searchtype=author&query=Harvey%2C+W">William Harvey</a>, 
<a href="/search/stat?searchtype=author&query=Weilbach%2C+C">Christian Weilbach</a>, 
<a href="/search/stat?searchtype=author&query=De+Bortoli%2C+V">Valentin De Bortoli</a>, 
<a href="/search/stat?searchtype=author&query=Rainforth%2C+T">Tom Rainforth</a>, 
<a href="/search/stat?searchtype=author&query=Doucet%2C+A">Arnaud Doucet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 11 figures, 8 tables; NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1043">[1043]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16272" title="Abstract">arXiv:2305.16272</a> (replaced) [<a href="/pdf/2305.16272" title="Download PDF">pdf</a>, <a href="/format/2305.16272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentivizing Honesty among Competitors in Collaborative Learning and  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dorner%2C+F+E">Florian E. Dorner</a>, 
<a href="/search/cs?searchtype=author&query=Konstantinov%2C+N">Nikola Konstantinov</a>, 
<a href="/search/cs?searchtype=author&query=Pashaliev%2C+G">Georgi Pashaliev</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Camera Ready; 37 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1044">[1044]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16284" title="Abstract">arXiv:2305.16284</a> (replaced) [<a href="/pdf/2305.16284" title="Download PDF">pdf</a>, <a href="/format/2305.16284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent  Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khaled%2C+A">Ahmed Khaled</a>, 
<a href="/search/cs?searchtype=author&query=Mishchenko%2C+K">Konstantin Mishchenko</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Chi Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 1 table, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1045">[1045]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16289" title="Abstract">arXiv:2305.16289</a> (replaced) [<a href="/pdf/2305.16289" title="Download PDF">pdf</a>, <a href="/format/2305.16289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversify Your Vision Datasets with Automatic Diffusion-Based  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dunlap%2C+L">Lisa Dunlap</a>, 
<a href="/search/cs?searchtype=author&query=Umino%2C+A">Alyssa Umino</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiezhi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+E">Joseph E. Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update: replaced Planes dataset with Waterbirds &amp; updated results after bug fix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1046">[1046]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16292" title="Abstract">arXiv:2305.16292</a> (replaced) [<a href="/pdf/2305.16292" title="Download PDF">pdf</a>, <a href="/format/2305.16292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharpness-Aware Minimization Leads to Low-Rank Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andriushchenko%2C+M">Maksym Andriushchenko</a>, 
<a href="/search/cs?searchtype=author&query=Bahri%2C+D">Dara Bahri</a>, 
<a href="/search/cs?searchtype=author&query=Mobahi%2C+H">Hossein Mobahi</a>, 
<a href="/search/cs?searchtype=author&query=Flammarion%2C+N">Nicolas Flammarion</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The camera-ready version (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1047">[1047]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16322" title="Abstract">arXiv:2305.16322</a> (replaced) [<a href="/pdf/2305.16322" title="Download PDF">pdf</a>, <a href="/format/2305.16322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shihao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongdong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yen-Chun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+J">Jianmin Bao</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+S">Shaozhe Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K+K">Kwan-Yee K. Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready, Code is available at <a href="https://github.com/ShihaoZhaoZSH/Uni-ControlNet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1048">[1048]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16380" title="Abstract">arXiv:2305.16380</a> (replaced) [<a href="/pdf/2305.16380" title="Download PDF">pdf</a>, <a href="/format/2305.16380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scan and Snap: Understanding Training Dynamics and Token Composition in  1-layer Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S">Simon Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1049">[1049]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16557" title="Abstract">arXiv:2305.16557</a> (replaced) [<a href="/pdf/2305.16557" title="Download PDF">pdf</a>, <a href="/format/2305.16557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree-Based Diffusion Schr&#xf6;dinger Bridge with Applications to  Wasserstein Barycenters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Noble%2C+M">Maxence Noble</a>, 
<a href="/search/stat?searchtype=author&query=De+Bortoli%2C+V">Valentin De Bortoli</a>, 
<a href="/search/stat?searchtype=author&query=Doucet%2C+A">Arnaud Doucet</a>, 
<a href="/search/stat?searchtype=author&query=Durmus%2C+A">Alain Durmus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item1050">[1050]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16569" title="Abstract">arXiv:2305.16569</a> (replaced) [<a href="/pdf/2305.16569" title="Download PDF">pdf</a>, <a href="/ps/2305.16569" title="Download PostScript">ps</a>, <a href="/format/2305.16569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Value Iteration with Anchoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jongmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+E+K">Ernest K. Ryu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neural Information Processing System 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1051">[1051]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16934" title="Abstract">arXiv:2305.16934</a> (replaced) [<a href="/pdf/2305.16934" title="Download PDF">pdf</a>, <a href="/format/2305.16934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Evaluating Adversarial Robustness of Large Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunqing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chongxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+N">Ngai-Man Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Min Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1052">[1052]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16960" title="Abstract">arXiv:2305.16960</a> (replaced) [<a href="/pdf/2305.16960" title="Download PDF">pdf</a>, <a href="/ps/2305.16960" title="Download PostScript">ps</a>, <a href="/format/2305.16960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Socially Aligned Language Models on Simulated Social  Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruibo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruixin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+C">Chenyan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+A+M">Andrew M. Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+S">Soroush Vosoughi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code, data, and models can be downloaded via <a href="https://github.com/agi-templar/Stable-Alignment">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1053">[1053]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16999" title="Abstract">arXiv:2305.16999</a> (replaced) [<a href="/pdf/2305.16999" title="Download PDF">pdf</a>, <a href="/format/2305.16999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three Towers: Flexible Contrastive Learning with Pretrained Image Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kossen%2C+J">Jannik Kossen</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+M">Mark Collier</a>, 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+B">Basil Mustafa</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+X">Xiaohua Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Beyer%2C+L">Lucas Beyer</a>, 
<a href="/search/cs?searchtype=author&query=Steiner%2C+A">Andreas Steiner</a>, 
<a href="/search/cs?searchtype=author&query=Berent%2C+J">Jesse Berent</a>, 
<a href="/search/cs?searchtype=author&query=Jenatton%2C+R">Rodolphe Jenatton</a>, 
<a href="/search/cs?searchtype=author&query=Kokiopoulou%2C+E">Efi Kokiopoulou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1054">[1054]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17195" title="Abstract">arXiv:2305.17195</a> (replaced) [<a href="/pdf/2305.17195" title="Download PDF">pdf</a>, <a href="/format/2305.17195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring the Future by Imagining the Past
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chandra%2C+K">Kartik Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tony Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tzu-Mao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ragan-Kelley%2C+J">Jonathan Ragan-Kelley</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J">Josh Tenenbaum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Graphics (cs.GR); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1055">[1055]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17214" title="Abstract">arXiv:2305.17214</a> (replaced) [<a href="/pdf/2305.17214" title="Download PDF">pdf</a>, <a href="/format/2305.17214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain  Activities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zijiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shaonan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1056">[1056]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17225" title="Abstract">arXiv:2305.17225</a> (replaced) [<a href="/pdf/2305.17225" title="Download PDF">pdf</a>, <a href="/format/2305.17225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Component Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wendong%2C+L">Liang Wendong</a>, 
<a href="/search/stat?searchtype=author&query=Keki%C4%87%2C+A">Armin Keki&#x107;</a>, 
<a href="/search/stat?searchtype=author&query=von+K%C3%BCgelgen%2C+J">Julius von K&#xfc;gelgen</a>, 
<a href="/search/stat?searchtype=author&query=Buchholz%2C+S">Simon Buchholz</a>, 
<a href="/search/stat?searchtype=author&query=Besserve%2C+M">Michel Besserve</a>, 
<a href="/search/stat?searchtype=author&query=Gresele%2C+L">Luigi Gresele</a>, 
<a href="/search/stat?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1057">[1057]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17432" title="Abstract">arXiv:2305.17432</a> (replaced) [<a href="/pdf/2305.17432" title="Download PDF">pdf</a>, <a href="/format/2305.17432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GMSF: Global Matching Scene Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yushan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Edstedt%2C+J">Johan Edstedt</a>, 
<a href="/search/cs?searchtype=author&query=Wandt%2C+B">Bastian Wandt</a>, 
<a href="/search/cs?searchtype=author&query=Forss%C3%A9n%2C+P">Per-Erik Forss&#xe9;n</a>, 
<a href="/search/cs?searchtype=author&query=Magnusson%2C+M">Maria Magnusson</a>, 
<a href="/search/cs?searchtype=author&query=Felsberg%2C+M">Michael Felsberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1058">[1058]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17570" title="Abstract">arXiv:2305.17570</a> (replaced) [<a href="/pdf/2305.17570" title="Download PDF">pdf</a>, <a href="/format/2305.17570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auditing Fairness by Betting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chugg%2C+B">Ben Chugg</a>, 
<a href="/search/stat?searchtype=author&query=Cortes-Gomez%2C+S">Santiago Cortes-Gomez</a>, 
<a href="/search/stat?searchtype=author&query=Wilder%2C+B">Bryan Wilder</a>, 
<a href="/search/stat?searchtype=author&query=Ramdas%2C+A">Aaditya Ramdas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023. 29 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG); Applications (stat.AP); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1059">[1059]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17588" title="Abstract">arXiv:2305.17588</a> (replaced) [<a href="/pdf/2305.17588" title="Download PDF">pdf</a>, <a href="/format/2305.17588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnosing Transformers: Illuminating Feature Spaces for Clinical  Decision-Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+A+R">Aliyah R. Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Cherapanamjeri%2C+Y">Yeshwanth Cherapanamjeri</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Briton Park</a>, 
<a href="/search/cs?searchtype=author&query=Naumann%2C+T">Tristan Naumann</a>, 
<a href="/search/cs?searchtype=author&query=Odisho%2C+A+Y">Anobel Y. Odisho</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bin Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1060">[1060]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18007" title="Abstract">arXiv:2305.18007</a> (replaced) [<a href="/pdf/2305.18007" title="Download PDF">pdf</a>, <a href="/format/2305.18007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Score Guidance for Text-Driven Image-to-Image Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunsoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Minsoo Kang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bohyung Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1061">[1061]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18262" title="Abstract">arXiv:2305.18262</a> (replaced) [<a href="/pdf/2305.18262" title="Download PDF">pdf</a>, <a href="/format/2305.18262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Confidence: Reliable Models Should Also Consider Atypicality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuksekgonul%2C+M">Mert Yuksekgonul</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Linjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>, 
<a href="/search/cs?searchtype=author&query=Guestrin%2C+C">Carlos Guestrin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1062">[1062]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18390" title="Abstract">arXiv:2305.18390</a> (replaced) [<a href="/pdf/2305.18390" title="Download PDF">pdf</a>, <a href="/format/2305.18390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergent Modularity in Pre-trained Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhiyuan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaojun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaozhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Ruobing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of ACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1063">[1063]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18395" title="Abstract">arXiv:2305.18395</a> (replaced) [<a href="/pdf/2305.18395" title="Download PDF">pdf</a>, <a href="/format/2305.18395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge-Augmented Reasoning Distillation for Small Language Models in  Knowledge-Intensive Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Minki Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seanie Lee</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+J">Jinheon Baek</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1064">[1064]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18399" title="Abstract">arXiv:2305.18399</a> (replaced) [<a href="/pdf/2305.18399" title="Download PDF">pdf</a>, <a href="/format/2305.18399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the impact of activation and normalization in obtaining isometric  embeddings at initialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joudaki%2C+A">Amir Joudaki</a>, 
<a href="/search/cs?searchtype=author&query=Daneshmand%2C+H">Hadi Daneshmand</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+F">Francis Bach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1065">[1065]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18402" title="Abstract">arXiv:2305.18402</a> (replaced) [<a href="/pdf/2305.18402" title="Download PDF">pdf</a>, <a href="/format/2305.18402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Sculpting: Uncovering hierarchically modular task structure in  neural networks through pruning and network analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patil%2C+S+M">Shreyas Malakarjun Patil</a>, 
<a href="/search/cs?searchtype=author&query=Michael%2C+L">Loizos Michael</a>, 
<a href="/search/cs?searchtype=author&query=Dovrolis%2C+C">Constantine Dovrolis</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 37th Conference on Neural Information Processing Systems (NeurIPS
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1066">[1066]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18548" title="Abstract">arXiv:2305.18548</a> (replaced) [<a href="/pdf/2305.18548" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I/O-efficient iterative matrix inversion with photonic integrated  circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minjia Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+C">Chunhui Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wonfor%2C+A">Adrian Wonfor</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Penty%2C+R">Richard Penty</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qixiang Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item1067">[1067]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18765" title="Abstract">arXiv:2305.18765</a> (replaced) [<a href="/pdf/2305.18765" title="Download PDF">pdf</a>, <a href="/ps/2305.18765" title="Download PostScript">ps</a>, <a href="/format/2305.18765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compactness estimates for difference schemes for conservation laws with  discontinuous flux
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Karlsen%2C+K+H">Kenneth H. Karlsen</a>, 
<a href="/search/math?searchtype=author&query=Towers%2C+J+D">John D. Towers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1068">[1068]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19066" title="Abstract">arXiv:2305.19066</a> (replaced) [<a href="/pdf/2305.19066" title="Download PDF">pdf</a>, <a href="/format/2305.19066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nested Diffusion Processes for Anytime Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elata%2C+N">Noam Elata</a>, 
<a href="/search/cs?searchtype=author&query=Kawar%2C+B">Bahjat Kawar</a>, 
<a href="/search/cs?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>, 
<a href="/search/cs?searchtype=author&query=Elad%2C+M">Michael Elad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1069">[1069]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19147" title="Abstract">arXiv:2305.19147</a> (replaced) [<a href="/pdf/2305.19147" title="Download PDF">pdf</a>, <a href="/format/2305.19147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional score-based diffusion models for Bayesian inference in  infinite dimensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Baldassari%2C+L">Lorenzo Baldassari</a>, 
<a href="/search/stat?searchtype=author&query=Siahkoohi%2C+A">Ali Siahkoohi</a>, 
<a href="/search/stat?searchtype=author&query=Garnier%2C+J">Josselin Garnier</a>, 
<a href="/search/stat?searchtype=author&query=Solna%2C+K">Knut Solna</a>, 
<a href="/search/stat?searchtype=author&query=de+Hoop%2C+M+V">Maarten V. de Hoop</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Analysis of PDEs (math.AP); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item1070">[1070]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19164" title="Abstract">arXiv:2305.19164</a> (replaced) [<a href="/pdf/2305.19164" title="Download PDF">pdf</a>, <a href="/format/2305.19164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LANCE: Stress-testing Visual Models by Generating Language-guided  Counterfactual Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+V">Viraj Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Yenamandra%2C+S">Sriram Yenamandra</a>, 
<a href="/search/cs?searchtype=author&query=Chattopadhyay%2C+P">Prithvijit Chattopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+J">Judy Hoffman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera ready. Project webpage: <a href="https://virajprabhu.github.io/lance-web/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1071">[1071]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19185" title="Abstract">arXiv:2305.19185</a> (replaced) [<a href="/pdf/2305.19185" title="Download PDF">pdf</a>, <a href="/format/2305.19185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compression with Bayesian Implicit Neural Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zongyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Flamich%2C+G">Gergely Flamich</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiajun He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhibo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a Spotlight paper in NeurIPS 2023. Updated camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1072">[1072]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19240" title="Abstract">arXiv:2305.19240</a> (replaced) [<a href="/pdf/2305.19240" title="Download PDF">pdf</a>, <a href="/format/2305.19240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NetHack is Hard to Hack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piterbarg%2C+U">Ulyana Piterbarg</a>, 
<a href="/search/cs?searchtype=author&query=Pinto%2C+L">Lerrel Pinto</a>, 
<a href="/search/cs?searchtype=author&query=Fergus%2C+R">Rob Fergus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1073">[1073]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19308" title="Abstract">arXiv:2305.19308</a> (replaced) [<a href="/pdf/2305.19308" title="Download PDF">pdf</a>, <a href="/format/2305.19308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SheetCopilot: Bringing Software Productivity to the Next Level through  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jingran Su</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuntao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoxiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1074">[1074]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19366" title="Abstract">arXiv:2305.19366</a> (replaced) [<a href="/pdf/2305.19366" title="Download PDF">pdf</a>, <a href="/format/2305.19366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Bayesian Inference of Graphical Structure and Parameters with a  Single Generative Flow Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deleu%2C+T">Tristan Deleu</a>, 
<a href="/search/cs?searchtype=author&query=Nishikawa-Toomey%2C+M">Mizu Nishikawa-Toomey</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+J">Jithendaraa Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>, 
<a href="/search/cs?searchtype=author&query=Charlin%2C+L">Laurent Charlin</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1075">[1075]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19562" title="Abstract">arXiv:2305.19562</a> (replaced) [<a href="/pdf/2305.19562" title="Download PDF">pdf</a>, <a href="/format/2305.19562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replicability in Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karbasi%2C+A">Amin Karbasi</a>, 
<a href="/search/cs?searchtype=author&query=Velegkas%2C+G">Grigoris Velegkas</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L+F">Lin F. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Felix Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1076">[1076]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19746" title="Abstract">arXiv:2305.19746</a> (replaced) [<a href="/pdf/2305.19746" title="Download PDF">pdf</a>, <a href="/format/2305.19746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive and Explainable Deployment of Navigation Skills via  Hierarchical Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyowoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seongun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaesik Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2023. First two authors contributed equally. Code at <a href="https://github.com/leekwoon/hrl-nav">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1077">[1077]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19753" title="Abstract">arXiv:2305.19753</a> (replaced) [<a href="/pdf/2305.19753" title="Download PDF">pdf</a>, <a href="/format/2305.19753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Tunnel Effect: Building Data Representations in Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masarczyk%2C+W">Wojciech Masarczyk</a>, 
<a href="/search/cs?searchtype=author&query=Ostaszewski%2C+M">Mateusz Ostaszewski</a>, 
<a href="/search/cs?searchtype=author&query=Imani%2C+E">Ehsan Imani</a>, 
<a href="/search/cs?searchtype=author&query=Pascanu%2C+R">Razvan Pascanu</a>, 
<a href="/search/cs?searchtype=author&query=Mi%C5%82o%C5%9B%2C+P">Piotr Mi&#x142;o&#x15b;</a>, 
<a href="/search/cs?searchtype=author&query=Trzci%C5%84ski%2C+T">Tomasz Trzci&#x144;ski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1078">[1078]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19818" title="Abstract">arXiv:2305.19818</a> (replaced) [<a href="/pdf/2305.19818" title="Download PDF">pdf</a>, <a href="/ps/2305.19818" title="Download PostScript">ps</a>, <a href="/format/2305.19818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectal Harmonics: Bridging Spectral Embedding and Matrix Completion in  Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Munkhoeva%2C+M">Marina Munkhoeva</a>, 
<a href="/search/cs?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1079">[1079]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.20054" title="Abstract">arXiv:2305.20054</a> (replaced) [<a href="/pdf/2305.20054" title="Download PDF">pdf</a>, <a href="/format/2305.20054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UNSSOR: Unsupervised Neural Speech Separation by Leveraging  Over-determined Training Mixtures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhong-Qiu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in Conference on Neural Information Processing Systems (NeurIPS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1080">[1080]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.20065" title="Abstract">arXiv:2305.20065</a> (replaced) [<a href="/pdf/2305.20065" title="Download PDF">pdf</a>, <a href="/format/2305.20065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Exploration for Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiappa%2C+A+S">Alberto Silvio Chiappa</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+A+M">Alessandro Marin Vargas</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+A+Z">Ann Zixiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Mathis%2C+A">Alexander Mathis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/amathislab/lattice">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item1081">[1081]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.20088" title="Abstract">arXiv:2305.20088</a> (replaced) [<a href="/pdf/2305.20088" title="Download PDF">pdf</a>, <a href="/format/2305.20088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving CLIP Training with Language Rewrites
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lijie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+D">Dilip Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Isola%2C+P">Phillip Isola</a>, 
<a href="/search/cs?searchtype=author&query=Katabi%2C+D">Dina Katabi</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonglong Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1082">[1082]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00169" title="Abstract">arXiv:2306.00169</a> (replaced) [<a href="/pdf/2306.00169" title="Download PDF">pdf</a>, <a href="/format/2306.00169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inconsistency, Instability, and Generalization Gap of Deep Neural  Network Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnson%2C+R">Rie Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1083">[1083]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00183" title="Abstract">arXiv:2306.00183</a> (replaced) [<a href="/pdf/2306.00183" title="Download PDF">pdf</a>, <a href="/format/2306.00183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffused Redundancy in Pre-trained Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nanda%2C+V">Vedant Nanda</a>, 
<a href="/search/cs?searchtype=author&query=Speicher%2C+T">Till Speicher</a>, 
<a href="/search/cs?searchtype=author&query=Dickerson%2C+J+P">John P. Dickerson</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>, 
<a href="/search/cs?searchtype=author&query=Gummadi%2C+K+P">Krishna P. Gummadi</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+A">Adrian Weller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1084">[1084]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00219" title="Abstract">arXiv:2306.00219</a> (replaced) [<a href="/pdf/2306.00219" title="Download PDF">pdf</a>, <a href="/format/2306.00219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Brush: A Latent Diffusion Model-based Editing Tool for  AI-generated Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gholami%2C+P">Peyman Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+R">Robert Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1085">[1085]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00335" title="Abstract">arXiv:2306.00335</a> (replaced) [<a href="/pdf/2306.00335" title="Download PDF">pdf</a>, <a href="/ps/2306.00335" title="Download PostScript">ps</a>, <a href="/format/2306.00335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate inference of marginals using the IBIA framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bathla%2C+S">Shivani Bathla</a>, 
<a href="/search/cs?searchtype=author&query=Vasudevan%2C+V">Vinita Vasudevan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1086">[1086]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00354" title="Abstract">arXiv:2306.00354</a> (replaced) [<a href="/pdf/2306.00354" title="Download PDF">pdf</a>, <a href="/format/2306.00354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Negative Transfer in Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Go%2C+H">Hyojun Go</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">JinYoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yunsung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seunghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Shinhyeok Oh</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+H">Hyeongdon Moon</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Seungtaek Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips 2023 camera ready. Project page: \href{<a href="https://gohyojun15.github.io/ANT_diffusion/">this https URL</a>}{url}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1087">[1087]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00412" title="Abstract">arXiv:2306.00412</a> (replaced) [<a href="/pdf/2306.00412" title="Download PDF">pdf</a>, <a href="/ps/2306.00412" title="Download PostScript">ps</a>, <a href="/format/2306.00412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beamforming Design for IRS-and-UAV-Aided Two-Way Amplify-and-Forward  Relay Networks in Maritime IoT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuehui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+F">Feng Shu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuanyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weiping Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shihao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yifan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qiankun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiangzhou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1088">[1088]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00542" title="Abstract">arXiv:2306.00542</a> (replaced) [<a href="/pdf/2306.00542" title="Download PDF">pdf</a>, <a href="/format/2306.00542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonparametric Identifiability of Causal Representations from Unknown  Interventions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=von+K%C3%BCgelgen%2C+J">Julius von K&#xfc;gelgen</a>, 
<a href="/search/stat?searchtype=author&query=Besserve%2C+M">Michel Besserve</a>, 
<a href="/search/stat?searchtype=author&query=Wendong%2C+L">Liang Wendong</a>, 
<a href="/search/stat?searchtype=author&query=Gresele%2C+L">Luigi Gresele</a>, 
<a href="/search/stat?searchtype=author&query=Keki%C4%87%2C+A">Armin Keki&#x107;</a>, 
<a href="/search/stat?searchtype=author&query=Bareinboim%2C+E">Elias Bareinboim</a>, 
<a href="/search/stat?searchtype=author&query=Blei%2C+D+M">David M. Blei</a>, 
<a href="/search/stat?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready version; 36 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1089">[1089]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00904" title="Abstract">arXiv:2306.00904</a> (replaced) [<a href="/pdf/2306.00904" title="Download PDF">pdf</a>, <a href="/format/2306.00904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interaction Measures, Partition Lattices and Kernel Tests for High-Order  Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+Z">Zhaolu Liu</a>, 
<a href="/search/stat?searchtype=author&query=Peach%2C+R+L">Robert L. Peach</a>, 
<a href="/search/stat?searchtype=author&query=Mediano%2C+P+A+M">Pedro A.M. Mediano</a>, 
<a href="/search/stat?searchtype=author&query=Barahona%2C+M">Mauricio Barahona</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item1090">[1090]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00946" title="Abstract">arXiv:2306.00946</a> (replaced) [<a href="/pdf/2306.00946" title="Download PDF">pdf</a>, <a href="/format/2306.00946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exposing Attention Glitches with Flip-Flop Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingbin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ash%2C+J+T">Jordan T. Ash</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+S">Surbhi Goel</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Akshay Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cyril Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: NeurIPS 2023 camera-ready + data release
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1091">[1091]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01646" title="Abstract">arXiv:2306.01646</a> (replaced) [<a href="/pdf/2306.01646" title="Download PDF">pdf</a>, <a href="/format/2306.01646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auditing for Human Expertise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Alur%2C+R">Rohan Alur</a>, 
<a href="/search/stat?searchtype=author&query=Laine%2C+L">Loren Laine</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+D+K">Darrick K. Li</a>, 
<a href="/search/stat?searchtype=author&query=Raghavan%2C+M">Manish Raghavan</a>, 
<a href="/search/stat?searchtype=author&query=Shah%2C+D">Devavrat Shah</a>, 
<a href="/search/stat?searchtype=author&query=Shung%2C+D">Dennis Shung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 10 figures. To appear in the proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1092">[1092]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01693" title="Abstract">arXiv:2306.01693</a> (replaced) [<a href="/pdf/2306.01693" title="Download PDF">pdf</a>, <a href="/format/2306.01693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Grained Human Feedback Gives Better Rewards for Language Model  Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zeqiu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yushi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Dziri%2C+N">Nouha Dziri</a>, 
<a href="/search/cs?searchtype=author&query=Suhr%2C+A">Alane Suhr</a>, 
<a href="/search/cs?searchtype=author&query=Ammanabrolu%2C+P">Prithviraj Ammanabrolu</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Ostendorf%2C+M">Mari Ostendorf</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1093">[1093]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01765" title="Abstract">arXiv:2306.01765</a> (replaced) [<a href="/pdf/2306.01765" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Message in a Bottle -- An Update to the Golden Record
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J+H">Jonathan H. Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Berea%2C+A">Anamaria Berea</a>, 
<a href="/search/cs?searchtype=author&query=Bowden%2C+H">Heather Bowden</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+P">Prithwis Das</a>, 
<a href="/search/cs?searchtype=author&query=Fahy%2C+K+A">Kristen A. Fahy</a>, 
<a href="/search/cs?searchtype=author&query=Ginsberg%2C+J">Joseph Ginsberg</a>, 
<a href="/search/cs?searchtype=author&query=Jew%2C+R">Robert Jew</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kershenbaum%2C+A">Arik Kershenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Kipping%2C+D">David Kipping</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+G">Graham Lau</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+K">Karen Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Lendo%2C+C+I+N">C. Isabel Nunez Lendo</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+P+E">Philip E. Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Searra%2C+N">Nick Searra</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+S+F">Stuart F. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Traphagan%2C+J">John Traphagan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Physics Education (physics.ed-ph); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item1094">[1094]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02014" title="Abstract">arXiv:2306.02014</a> (replaced) [<a href="/pdf/2306.02014" title="Download PDF">pdf</a>, <a href="/format/2306.02014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering the Hidden Dynamics of Video Self-supervised Learning under  Distribution Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+P">Pritam Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Beirami%2C+A">Ahmad Beirami</a>, 
<a href="/search/cs?searchtype=author&query=Etemad%2C+A">Ali Etemad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1095">[1095]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02049" title="Abstract">arXiv:2306.02049</a> (replaced) [<a href="/pdf/2306.02049" title="Download PDF">pdf</a>, <a href="/format/2306.02049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LambdaBeam: Neural Program Search with Higher-Order Functions and  Lambdas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+K">Kensen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hanjun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wen-Ding Li</a>, 
<a href="/search/cs?searchtype=author&query=Ellis%2C+K">Kevin Ellis</a>, 
<a href="/search/cs?searchtype=author&query=Sutton%2C+C">Charles Sutton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item1096">[1096]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02213" title="Abstract">arXiv:2306.02213</a> (replaced) [<a href="/pdf/2306.02213" title="Download PDF">pdf</a>, <a href="/format/2306.02213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in  Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teodorescu%2C+D">Daniela Teodorescu</a>, 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+S+M">Saif M. Mohammad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures. arXiv admin note: substantial text overlap with <a href="/abs/2210.07381">arXiv:2210.07381</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1097">[1097]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02444" title="Abstract">arXiv:2306.02444</a> (replaced) [<a href="/pdf/2306.02444" title="Download PDF">pdf</a>, <a href="/format/2306.02444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Sustainable IoT Connectivity: Vision, Technological Enablers,  Challenges, and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez%2C+O+A">Onel A. L&#xf3;pez</a>, 
<a href="/search/cs?searchtype=author&query=Rosabal%2C+O+M">Osmel M. Rosabal</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz-Guirola%2C+D">David Ruiz-Guirola</a>, 
<a href="/search/cs?searchtype=author&query=Raghuwanshi%2C+P">Prasoon Raghuwanshi</a>, 
<a href="/search/cs?searchtype=author&query=Mikhaylov%2C+K">Konstantin Mikhaylov</a>, 
<a href="/search/cs?searchtype=author&query=Lov%C3%A9n%2C+L">Lauri Lov&#xe9;n</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+S">Sridhar Iyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 figures, 12 tables, submitted to IEEE Open Journal of the Communications Society
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1098">[1098]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02652" title="Abstract">arXiv:2306.02652</a> (replaced) [<a href="/pdf/2306.02652" title="Download PDF">pdf</a>, <a href="/format/2306.02652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Anytime Classification in Early-Exit Architectures by Enforcing  Conditional Monotonicity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jazbec%2C+M">Metod Jazbec</a>, 
<a href="/search/cs?searchtype=author&query=Allingham%2C+J+U">James Urquhart Allingham</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nalisnick%2C+E">Eric Nalisnick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1099">[1099]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02846" title="Abstract">arXiv:2306.02846</a> (replaced) [<a href="/pdf/2306.02846" title="Download PDF">pdf</a>, <a href="/format/2306.02846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Partitioned Learned Bloom Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sato%2C+A">Atsuki Sato</a>, 
<a href="/search/cs?searchtype=author&query=Matsui%2C+Y">Yusuke Matsui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item1100">[1100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02948" title="Abstract">arXiv:2306.02948</a> (replaced) [<a href="/pdf/2306.02948" title="Download PDF">pdf</a>, <a href="/format/2306.02948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning under random distributional shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bansak%2C+K">Kirk Bansak</a>, 
<a href="/search/stat?searchtype=author&query=Paulson%2C+E">Elisabeth Paulson</a>, 
<a href="/search/stat?searchtype=author&query=Rothenh%C3%A4usler%2C+D">Dominik Rothenh&#xe4;usler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1101">[1101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03395" title="Abstract">arXiv:2306.03395</a> (replaced) [<a href="/pdf/2306.03395" title="Download PDF">pdf</a>, <a href="/format/2306.03395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational Technologies for Fashion Recommendation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yujuan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zhihui Lai</a>, 
<a href="/search/cs?searchtype=author&query=Mok%2C+P+Y">P. Y. Mok</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1102">[1102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03552" title="Abstract">arXiv:2306.03552</a> (replaced) [<a href="/pdf/2306.03552" title="Download PDF">pdf</a>, <a href="/format/2306.03552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State Regularized Policy Optimization on Data with Dynamics Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhenghai Xue</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Q">Qingpeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Dong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Peng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bo An</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1103">[1103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03792" title="Abstract">arXiv:2306.03792</a> (replaced) [<a href="/pdf/2306.03792" title="Download PDF">pdf</a>, <a href="/format/2306.03792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FAMO: Fast Adaptive Multitask Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yihao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1104">[1104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03819" title="Abstract">arXiv:2306.03819</a> (replaced) [<a href="/pdf/2306.03819" title="Download PDF">pdf</a>, <a href="/format/2306.03819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEACE: Perfect linear concept erasure in closed form
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belrose%2C+N">Nora Belrose</a>, 
<a href="/search/cs?searchtype=author&query=Schneider-Joseph%2C+D">David Schneider-Joseph</a>, 
<a href="/search/cs?searchtype=author&query=Ravfogel%2C+S">Shauli Ravfogel</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>, 
<a href="/search/cs?searchtype=author&query=Raff%2C+E">Edward Raff</a>, 
<a href="/search/cs?searchtype=author&query=Biderman%2C+S">Stella Biderman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1105">[1105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03899" title="Abstract">arXiv:2306.03899</a> (replaced) [<a href="/pdf/2306.03899" title="Download PDF">pdf</a>, <a href="/ps/2306.03899" title="Download PostScript">ps</a>, <a href="/format/2306.03899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Label-free Scene Understanding by Vision Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Runnan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Youquan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingdong Kong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nenglun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinge Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuexin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1106">[1106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04251" title="Abstract">arXiv:2306.04251</a> (replaced) [<a href="/pdf/2306.04251" title="Download PDF">pdf</a>, <a href="/format/2306.04251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards  Simpler Subnetworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kunin%2C+D">Daniel Kunin</a>, 
<a href="/search/cs?searchtype=author&query=Yamamura%2C+A">Atsushi Yamamura</a>, 
<a href="/search/cs?searchtype=author&query=Ganguli%2C+S">Surya Ganguli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 12 figures, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1107">[1107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04806" title="Abstract">arXiv:2306.04806</a> (replaced) [<a href="/pdf/2306.04806" title="Download PDF">pdf</a>, <a href="/format/2306.04806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Capability Assessment of Sequential Decision-Making Systems  in Stochastic Settings (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Pulkit Verma</a>, 
<a href="/search/cs?searchtype=author&query=Karia%2C+R">Rushang Karia</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Siddharth Srivastava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1108">[1108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04924" title="Abstract">arXiv:2306.04924</a> (replaced) [<a href="/pdf/2306.04924" title="Download PDF">pdf</a>, <a href="/format/2306.04924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Optimality of Communication-Privacy-Utility Tradeoffs in  Distributed Mean Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Isik%2C+B">Berivan Isik</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei-Ning Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ozgur%2C+A">Ayfer Ozgur</a>, 
<a href="/search/cs?searchtype=author&query=Weissman%2C+T">Tsachy Weissman</a>, 
<a href="/search/cs?searchtype=author&query=No%2C+A">Albert No</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at the Conference on Neural Information Processing Systems (NeurIPS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1109">[1109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04949" title="Abstract">arXiv:2306.04949</a> (replaced) [<a href="/pdf/2306.04949" title="Download PDF">pdf</a>, <a href="/format/2306.04949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Learning with Progressive Data Expansion Against Spurious  Correlation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yihe Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Mirzasoleiman%2C+B">Baharan Mirzasoleiman</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures, 11 tables. In NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1110">[1110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05178" title="Abstract">arXiv:2306.05178</a> (replaced) [<a href="/pdf/2306.05178" title="Download PDF">pdf</a>, <a href="/format/2306.05178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yuseung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kunho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunjin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+M">Minhyuk Sung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023. Project page: <a href="https://syncdiffusion.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1111">[1111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05268" title="Abstract">arXiv:2306.05268</a> (replaced) [<a href="/pdf/2306.05268" title="Download PDF">pdf</a>, <a href="/format/2306.05268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+P+P">Paul Pu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Martin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>, 
<a href="/search/cs?searchtype=author&query=Morency%2C+L">Louis-Philippe Morency</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Code available at: <a href="https://github.com/pliang279/FactorCL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1112">[1112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05304" title="Abstract">arXiv:2306.05304</a> (replaced) [<a href="/pdf/2306.05304" title="Download PDF">pdf</a>, <a href="/format/2306.05304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Optimisation of Functions on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xingchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Osselin%2C+P">Pierre Osselin</a>, 
<a href="/search/cs?searchtype=author&query=Kenlay%2C+H">Henry Kenlay</a>, 
<a href="/search/cs?searchtype=author&query=Ru%2C+B">Binxin Ru</a>, 
<a href="/search/cs?searchtype=author&query=Osborne%2C+M+A">Michael A. Osborne</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaowen Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. 11 pages, 11 figures, 1 table (29 pages, 31 figures, 1 table including references and appendices)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1113">[1113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05476" title="Abstract">arXiv:2306.05476</a> (replaced) [<a href="/pdf/2306.05476" title="Download PDF">pdf</a>, <a href="/format/2306.05476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Confidence Induced Class Activation Mapping for MRI Brain Tumor  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu-Jen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yiyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+T">Tsung-Yi Ho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1114">[1114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05587" title="Abstract">arXiv:2306.05587</a> (replaced) [<a href="/pdf/2306.05587" title="Download PDF">pdf</a>, <a href="/format/2306.05587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MC-NN: An End-to-End Multi-Channel Neural Network Approach for  Predicting Influenza A Virus Hosts and Antigenic Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yanhua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wojtczak%2C+D">Dominik Wojtczak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted version submitted to the SN Computer Science; Published in the SN Computer Science 2023; V1: minor updates were made to the Results section
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item1115">[1115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05671" title="Abstract">arXiv:2306.05671</a> (replaced) [<a href="/pdf/2306.05671" title="Download PDF">pdf</a>, <a href="/format/2306.05671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology-Aware Uncertainty for Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Saumya Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yikai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaoling Hu</a>, 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+P">Prateek Prasanna</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023; 22 pages, 14 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1116">[1116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05751" title="Abstract">arXiv:2306.05751</a> (replaced) [<a href="/pdf/2306.05751" title="Download PDF">pdf</a>, <a href="/format/2306.05751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Counterfactual Inference through Quantile Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shaoan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Biwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+B">Bin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1117">[1117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05963" title="Abstract">arXiv:2306.05963</a> (replaced) [<a href="/pdf/2306.05963" title="Download PDF">pdf</a>, <a href="/format/2306.05963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Contextual Perception: How to Generalize to New Backgrounds and  Ambiguous Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+Z">Zhuofan Ying</a>, 
<a href="/search/cs?searchtype=author&query=Hase%2C+P">Peter Hase</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023. 23 pages, 13 figures. Our code is available at <a href="https://github.com/zfying/AdaptiveContext">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1118">[1118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06101" title="Abstract">arXiv:2306.06101</a> (replaced) [<a href="/pdf/2306.06101" title="Download PDF">pdf</a>, <a href="/format/2306.06101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prodigy: An Expeditiously Adaptive Parameter-Free Learner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishchenko%2C+K">Konstantin Mishchenko</a>, 
<a href="/search/cs?searchtype=author&query=Defazio%2C+A">Aaron Defazio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1119">[1119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06155" title="Abstract">arXiv:2306.06155</a> (replaced) [<a href="/pdf/2306.06155" title="Download PDF">pdf</a>, <a href="/format/2306.06155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intensity Profile Projection: A Framework for Continuous-Time  Representation Learning for Dynamic Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Modell%2C+A">Alexander Modell</a>, 
<a href="/search/cs?searchtype=author&query=Gallagher%2C+I">Ian Gallagher</a>, 
<a href="/search/cs?searchtype=author&query=Ceccherini%2C+E">Emma Ceccherini</a>, 
<a href="/search/cs?searchtype=author&query=Whiteley%2C+N">Nick Whiteley</a>, 
<a href="/search/cs?searchtype=author&query=Rubin-Delanchy%2C+P">Patrick Rubin-Delanchy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1120">[1120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06156" title="Abstract">arXiv:2306.06156</a> (replaced) [<a href="/pdf/2306.06156" title="Download PDF">pdf</a>, <a href="/format/2306.06156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PoET: A generative model of protein families as sequences-of-sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Truong%2C+T+F">Timothy F. Truong Jr</a>, 
<a href="/search/q-bio?searchtype=author&query=Bepler%2C+T">Tristan Bepler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1121">[1121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06203" title="Abstract">arXiv:2306.06203</a> (replaced) [<a href="/pdf/2306.06203" title="Download PDF">pdf</a>, <a href="/format/2306.06203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLSL: Feature-level Self-supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Q">Qing Su</a>, 
<a href="/search/cs?searchtype=author&query=Netchaev%2C+A">Anton Netchaev</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hai Li</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shihao Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1122">[1122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06230" title="Abstract">arXiv:2306.06230</a> (replaced) [<a href="/pdf/2306.06230" title="Download PDF">pdf</a>, <a href="/format/2306.06230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Frameworks for Hyper-Connected Social XRI Immersive Metaverse  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+J">Jie Guan</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+A">Alexis Morris</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Network ( Volume: 37, Issue: 4, July/August 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1123">[1123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06250" title="Abstract">arXiv:2306.06250</a> (replaced) [<a href="/pdf/2306.06250" title="Download PDF">pdf</a>, <a href="/format/2306.06250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Apple Tasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harris%2C+K">Keegan Harris</a>, 
<a href="/search/cs?searchtype=author&query=Podimata%2C+C">Chara Podimata</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In the thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1124">[1124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06253" title="Abstract">arXiv:2306.06253</a> (replaced) [<a href="/pdf/2306.06253" title="Download PDF">pdf</a>, <a href="/format/2306.06253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision Stacks: Flexible Reinforcement Learning via Modular Generative  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Siyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Grover%2C+A">Aditya Grover</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published at NeurIPS 2023, project page: <a href="https://siyan-zhao.github.io/decision-stacks/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1125">[1125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06446" title="Abstract">arXiv:2306.06446</a> (replaced) [<a href="/pdf/2306.06446" title="Download PDF">pdf</a>, <a href="/format/2306.06446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient  Vision Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+H">Haoran You</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Huihong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yipin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yingyan">Yingyan</a> (Celine)Lin
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1126">[1126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06482" title="Abstract">arXiv:2306.06482</a> (replaced) [<a href="/pdf/2306.06482" title="Download PDF">pdf</a>, <a href="/format/2306.06482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TensorNet: Cartesian Tensor Representations for Efficient Learning of  Molecular Potentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simeon%2C+G">Guillem Simeon</a>, 
<a href="/search/cs?searchtype=author&query=de+Fabritiis%2C+G">Gianni de Fabritiis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item1127">[1127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06529" title="Abstract">arXiv:2306.06529</a> (replaced) [<a href="/pdf/2306.06529" title="Download PDF">pdf</a>, <a href="/format/2306.06529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Injective Functions for Multisets, Measures and Graphs via a  Finite Witness Theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amir%2C+T">Tal Amir</a>, 
<a href="/search/cs?searchtype=author&query=Gortler%2C+S+J">Steven J. Gortler</a>, 
<a href="/search/cs?searchtype=author&query=Avni%2C+I">Ilai Avni</a>, 
<a href="/search/cs?searchtype=author&query=Ravina%2C+R">Ravina Ravina</a>, 
<a href="/search/cs?searchtype=author&query=Dym%2C+N">Nadav Dym</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1128">[1128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06805" title="Abstract">arXiv:2306.06805</a> (replaced) [<a href="/pdf/2306.06805" title="Download PDF">pdf</a>, <a href="/format/2306.06805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Feature Visualization for Deeper Networks with MAgnitude  Constrained Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fel%2C+T">Thomas Fel</a>, 
<a href="/search/cs?searchtype=author&query=Boissin%2C+T">Thibaut Boissin</a>, 
<a href="/search/cs?searchtype=author&query=Boutin%2C+V">Victor Boutin</a>, 
<a href="/search/cs?searchtype=author&query=Picard%2C+A">Agustin Picard</a>, 
<a href="/search/cs?searchtype=author&query=Novello%2C+P">Paul Novello</a>, 
<a href="/search/cs?searchtype=author&query=Colin%2C+J">Julien Colin</a>, 
<a href="/search/cs?searchtype=author&query=Linsley%2C+D">Drew Linsley</a>, 
<a href="/search/cs?searchtype=author&query=Rousseau%2C+T">Tom Rousseau</a>, 
<a href="/search/cs?searchtype=author&query=Cad%C3%A8ne%2C+R">R&#xe9;mi Cad&#xe8;ne</a>, 
<a href="/search/cs?searchtype=author&query=Gardes%2C+L">Laurent Gardes</a>, 
<a href="/search/cs?searchtype=author&query=Serre%2C+T">Thomas Serre</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Conference on Neural Information Processing Systems (NeurIPS),
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1129">[1129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06901" title="Abstract">arXiv:2306.06901</a> (replaced) [<a href="/pdf/2306.06901" title="Download PDF">pdf</a>, <a href="/format/2306.06901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engaging Engineering Teams Through Moral Imagination: A Bottom-Up  Approach for Responsible Innovation and Ethical Culture Change in Technology  Companies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lange%2C+B">Benjamin Lange</a>, 
<a href="/search/cs?searchtype=author&query=Keeling%2C+G">Geoff Keeling</a>, 
<a href="/search/cs?searchtype=author&query=McCroskery%2C+A">Amanda McCroskery</a>, 
<a href="/search/cs?searchtype=author&query=Zevenbergen%2C+B">Ben Zevenbergen</a>, 
<a href="/search/cs?searchtype=author&query=Blascovich%2C+S">Sandra Blascovich</a>, 
<a href="/search/cs?searchtype=author&query=Pedersen%2C+K">Kyle Pedersen</a>, 
<a href="/search/cs?searchtype=author&query=Lentz%2C+A">Alison Lentz</a>, 
<a href="/search/cs?searchtype=author&query=Arcas%2C+B+A+y">Blaise Aguera y Arcas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item1130">[1130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07004" title="Abstract">arXiv:2306.07004</a> (replaced) [<a href="/pdf/2306.07004" title="Download PDF">pdf</a>, <a href="/format/2306.07004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Occlusion-aware Risk Assessment and Driving Strategy for Autonomous  Vehicles Using Simplified Reachability Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hyunwoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jongseo Choi</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+H">Hyuntai Chin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sang-Hyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+D">Doosan Baek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 11 figures IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED OCTOBER, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1131">[1131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07304" title="Abstract">arXiv:2306.07304</a> (replaced) [<a href="/pdf/2306.07304" title="Download PDF">pdf</a>, <a href="/format/2306.07304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Holistic Approach to Unifying Automatic Concept Extraction and Concept  Importance Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fel%2C+T">Thomas Fel</a>, 
<a href="/search/cs?searchtype=author&query=Boutin%2C+V">Victor Boutin</a>, 
<a href="/search/cs?searchtype=author&query=Moayeri%2C+M">Mazda Moayeri</a>, 
<a href="/search/cs?searchtype=author&query=Cad%C3%A8ne%2C+R">R&#xe9;mi Cad&#xe8;ne</a>, 
<a href="/search/cs?searchtype=author&query=Bethune%2C+L">Louis Bethune</a>, 
<a href="/search/cs?searchtype=author&query=and%C3%A9ol%2C+L">L&#xe9;o and&#xe9;ol</a>, 
<a href="/search/cs?searchtype=author&query=Chalvidal%2C+M">Mathieu Chalvidal</a>, 
<a href="/search/cs?searchtype=author&query=Serre%2C+T">Thomas Serre</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Conference on Neural Information Processing Systems (NeurIPS),
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1132">[1132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07528" title="Abstract">arXiv:2306.07528</a> (replaced) [<a href="/pdf/2306.07528" title="Download PDF">pdf</a>, <a href="/format/2306.07528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Off-Policy Learning to Rank: a Reinforcement Learning  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yi Su</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hui Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yiran Wu</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+R">Rishab Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingyun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huazheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by Neruips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1133">[1133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07929" title="Abstract">arXiv:2306.07929</a> (replaced) [<a href="/pdf/2306.07929" title="Download PDF">pdf</a>, <a href="/format/2306.07929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Are Semi-Parametric Reinforcement Learning Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Danyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Situo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongshen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zihan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1134">[1134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08259" title="Abstract">arXiv:2306.08259</a> (replaced) [<a href="/pdf/2306.08259" title="Download PDF">pdf</a>, <a href="/format/2306.08259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yutong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junfeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Lei Bai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenguang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+R">Roger Zimmermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1135">[1135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08777" title="Abstract">arXiv:2306.08777</a> (replaced) [<a href="/pdf/2306.08777" title="Download PDF">pdf</a>, <a href="/format/2306.08777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without  Data Splitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Biggs%2C+F">Felix Biggs</a>, 
<a href="/search/stat?searchtype=author&query=Schrab%2C+A">Antonin Schrab</a>, 
<a href="/search/stat?searchtype=author&query=Gretton%2C+A">Arthur Gretton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages,8 figures, 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 37th Conference on Neural Information Processing Systems (NeurIPS
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1136">[1136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08943" title="Abstract">arXiv:2306.08943</a> (replaced) [<a href="/pdf/2306.08943" title="Download PDF">pdf</a>, <a href="/format/2306.08943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Fields with Hard Constraints of Arbitrary Differential Order
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+F">Fangcheng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Fogarty%2C+K">Kyle Fogarty</a>, 
<a href="/search/cs?searchtype=author&query=Hanji%2C+P">Param Hanji</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sztrajman%2C+A">Alejandro Sztrajman</a>, 
<a href="/search/cs?searchtype=author&query=Spielberg%2C+A">Andrew Spielberg</a>, 
<a href="/search/cs?searchtype=author&query=Tagliasacchi%2C+A">Andrea Tagliasacchi</a>, 
<a href="/search/cs?searchtype=author&query=Bosilj%2C+P">Petra Bosilj</a>, 
<a href="/search/cs?searchtype=author&query=Oztireli%2C+C">Cengiz Oztireli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1137">[1137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09244" title="Abstract">arXiv:2306.09244</a> (replaced) [<a href="/pdf/2306.09244" title="Download PDF">pdf</a>, <a href="/format/2306.09244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Promptable Surgical Instrument Segmentation with Vision-Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zijian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Alabi%2C+O">Oluwatosin Alabi</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+M">Meng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Vercauteren%2C+T">Tom Vercauteren</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Miaojing Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1138">[1138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09335" title="Abstract">arXiv:2306.09335</a> (replaced) [<a href="/pdf/2306.09335" title="Download PDF">pdf</a>, <a href="/format/2306.09335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class-Conditional Conformal Prediction with Many Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ding%2C+T">Tiffany Ding</a>, 
<a href="/search/stat?searchtype=author&query=Angelopoulos%2C+A+N">Anastasios N. Angelopoulos</a>, 
<a href="/search/stat?searchtype=author&query=Bates%2C+S">Stephen Bates</a>, 
<a href="/search/stat?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/stat?searchtype=author&query=Tibshirani%2C+R+J">Ryan J. Tibshirani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1139">[1139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09526" title="Abstract">arXiv:2306.09526</a> (replaced) [<a href="/pdf/2306.09526" title="Download PDF">pdf</a>, <a href="/format/2306.09526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Residual Q-Learning: Offline and Online Policy Customization without  Value
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenran Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chen Tang</a>, 
<a href="/search/cs?searchtype=author&query=Nishimura%2C+H">Haruki Nishimura</a>, 
<a href="/search/cs?searchtype=author&query=Mercat%2C+J">Jean Mercat</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 37th Conference on Neural Information Processing Systems (NeurIPS 2023). The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1140">[1140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09539" title="Abstract">arXiv:2306.09539</a> (replaced) [<a href="/pdf/2306.09539" title="Download PDF">pdf</a>, <a href="/format/2306.09539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Block-State Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fathi%2C+M">Mahan Fathi</a>, 
<a href="/search/cs?searchtype=author&query=Pilault%2C+J">Jonathan Pilault</a>, 
<a href="/search/cs?searchtype=author&query=Firat%2C+O">Orhan Firat</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+C">Christopher Pal</a>, 
<a href="/search/cs?searchtype=author&query=Bacon%2C+P">Pierre-Luc Bacon</a>, 
<a href="/search/cs?searchtype=author&query=Goroshin%2C+R">Ross Goroshin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS'23 - Thirty-seventh Conference on Neural Information Processing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1141">[1141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09557" title="Abstract">arXiv:2306.09557</a> (replaced) [<a href="/pdf/2306.09557" title="Download PDF">pdf</a>, <a href="/format/2306.09557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAJun: Continuous Adaptive Jumping using a Learned Centroidal Controller
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuxiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guanya Shi</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+X">Xiangyun Meng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tingnan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jie Tan</a>, 
<a href="/search/cs?searchtype=author&query=Boots%2C+B">Byron Boots</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please visit <a href="https://yxyang.github.io/cajun/">this https URL</a> for additional results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1142">[1142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09818" title="Abstract">arXiv:2306.09818</a> (replaced) [<a href="/pdf/2306.09818" title="Download PDF">pdf</a>, <a href="/format/2306.09818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiNeRV: Video Compression with Hierarchical Encoding-based Neural  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kwan%2C+H+M">Ho Man Kwan</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+G">Ge Gao</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Gower%2C+A">Andrew Gower</a>, 
<a href="/search/eess?searchtype=author&query=Bull%2C+D">David Bull</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1143">[1143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10168" title="Abstract">arXiv:2306.10168</a> (replaced) [<a href="/pdf/2306.10168" title="Download PDF">pdf</a>, <a href="/format/2306.10168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Geometry: Comparing the Temporal Structure of Computation in  Neural Circuits with Dynamical Similarity Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Ostrow%2C+M">Mitchell Ostrow</a>, 
<a href="/search/q-bio?searchtype=author&query=Eisen%2C+A">Adam Eisen</a>, 
<a href="/search/q-bio?searchtype=author&query=Kozachkov%2C+L">Leo Kozachkov</a>, 
<a href="/search/q-bio?searchtype=author&query=Fiete%2C+I">Ila Fiete</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item1144">[1144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10512" title="Abstract">arXiv:2306.10512</a> (replaced) [<a href="/pdf/2306.10512" title="Download PDF">pdf</a>, <a href="/format/2306.10512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+Y">Yuting Ning</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weizhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+R">Rui Lv</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhenya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Guanhao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Q">Qingyang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shijin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1145">[1145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10608" title="Abstract">arXiv:2306.10608</a> (replaced) [<a href="/pdf/2306.10608" title="Download PDF">pdf</a>, <a href="/format/2306.10608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STHG: Spatial-Temporal Heterogeneous Graph Learning for Advanced  Audio-Visual Diarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+K">Kyle Min</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Validation report for the Ego4D challenge at CVPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1146">[1146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10759" title="Abstract">arXiv:2306.10759</a> (replaced) [<a href="/pdf/2306.10759" title="Download PDF">pdf</a>, <a href="/format/2306.10759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simplifying and Empowering Transformers for Large-Graph Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qitian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wentao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+F">Fan Nie</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haitian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published at NeurIPS 2023, the codes are available at <a href="https://github.com/qitianwu/SGFormer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1147">[1147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10890" title="Abstract">arXiv:2306.10890</a> (replaced) [<a href="/pdf/2306.10890" title="Download PDF">pdf</a>, <a href="/ps/2306.10890" title="Download PostScript">ps</a>, <a href="/format/2306.10890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QoS-Aware Downlink Beamforming for Joint Transmission in Multi-Cell  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chen-Yen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Kuang-Hao">Kuang-Hao</a> (Stanley)Liu
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1148">[1148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10930" title="Abstract">arXiv:2306.10930</a> (replaced) [<a href="/pdf/2306.10930" title="Download PDF">pdf</a>, <a href="/ps/2306.10930" title="Download PostScript">ps</a>, <a href="/format/2306.10930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Practical Max-Min Fair Resource Allocation Algorithm for  Rate-Splitting Multiple Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Facheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yijie Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1149">[1149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11207" title="Abstract">arXiv:2306.11207</a> (replaced) [<a href="/pdf/2306.11207" title="Download PDF">pdf</a>, <a href="/format/2306.11207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quilt-1M: One Million Image-Text Pairs for Histopathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ikezogwo%2C+W+O">Wisdom Oluchi Ikezogwo</a>, 
<a href="/search/cs?searchtype=author&query=Seyfioglu%2C+M+S">Mehmet Saygin Seyfioglu</a>, 
<a href="/search/cs?searchtype=author&query=Ghezloo%2C+F">Fatemeh Ghezloo</a>, 
<a href="/search/cs?searchtype=author&query=Geva%2C+D+S+C">Dylan Stefan Chan Geva</a>, 
<a href="/search/cs?searchtype=author&query=Mohammed%2C+F+S">Fatwir Sheikh Mohammed</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+P+K">Pavan Kumar Anand</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+L">Linda Shapiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1150">[1150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11589" title="Abstract">arXiv:2306.11589</a> (replaced) [<a href="/pdf/2306.11589" title="Download PDF">pdf</a>, <a href="/format/2306.11589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling from Gaussian Process Posteriors using Stochastic Gradient  Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J+A">Jihao Andreas Lin</a>, 
<a href="/search/cs?searchtype=author&query=Antor%C3%A1n%2C+J">Javier Antor&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Padhy%2C+S">Shreyas Padhy</a>, 
<a href="/search/cs?searchtype=author&query=Janz%2C+D">David Janz</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>, 
<a href="/search/cs?searchtype=author&query=Terenin%2C+A">Alexander Terenin</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1151">[1151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11839" title="Abstract">arXiv:2306.11839</a> (replaced) [<a href="/pdf/2306.11839" title="Download PDF">pdf</a>, <a href="/format/2306.11839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Should I Stop or Should I Go: Early Stopping with Heterogeneous  Populations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Adam%2C+H">Hammaad Adam</a>, 
<a href="/search/stat?searchtype=author&query=Yin%2C+F">Fan Yin</a>, 
<a href="/search/stat?searchtype=author&query=Huibin">Huibin</a> (Mary)Hu, 
<a href="/search/stat?searchtype=author&query=Tenenholtz%2C+N">Neil Tenenholtz</a>, 
<a href="/search/stat?searchtype=author&query=Crawford%2C+L">Lorin Crawford</a>, 
<a href="/search/stat?searchtype=author&query=Mackey%2C+L">Lester Mackey</a>, 
<a href="/search/stat?searchtype=author&query=Koenecke%2C+A">Allison Koenecke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1152">[1152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12371" title="Abstract">arXiv:2306.12371</a> (replaced) [<a href="/pdf/2306.12371" title="Download PDF">pdf</a>, <a href="/format/2306.12371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimistic Active Exploration of Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sukhija%2C+B">Bhavya Sukhija</a>, 
<a href="/search/cs?searchtype=author&query=Treven%2C+L">Lenart Treven</a>, 
<a href="/search/cs?searchtype=author&query=Sancaktar%2C+C">Cansu Sancaktar</a>, 
<a href="/search/cs?searchtype=author&query=Blaes%2C+S">Sebastian Blaes</a>, 
<a href="/search/cs?searchtype=author&query=Coros%2C+S">Stelian Coros</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1153">[1153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12552" title="Abstract">arXiv:2306.12552</a> (replaced) [<a href="/pdf/2306.12552" title="Download PDF">pdf</a>, <a href="/format/2306.12552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SituatedGen: Incorporating Geographical and Temporal Contexts into  Generative Commonsense Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1154">[1154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13092" title="Abstract">arXiv:2306.13092</a> (replaced) [<a href="/pdf/2306.13092" title="Download PDF">pdf</a>, <a href="/format/2306.13092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale  From A New Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zeyuan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqiang Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight. Code at <a href="https://github.com/VILA-Lab/SRe2L">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1155">[1155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13104" title="Abstract">arXiv:2306.13104</a> (replaced) [<a href="/pdf/2306.13104" title="Download PDF">pdf</a>, <a href="/format/2306.13104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual  Prostheses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Granley%2C+J">Jacob Granley</a>, 
<a href="/search/q-bio?searchtype=author&query=Fauvel%2C+T">Tristan Fauvel</a>, 
<a href="/search/q-bio?searchtype=author&query=Chalk%2C+M">Matthew Chalk</a>, 
<a href="/search/q-bio?searchtype=author&query=Beyeler%2C+M">Michael Beyeler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1156">[1156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13111" title="Abstract">arXiv:2306.13111</a> (replaced) [<a href="/pdf/2306.13111" title="Download PDF">pdf</a>, <a href="/ps/2306.13111" title="Download PostScript">ps</a>, <a href="/format/2306.13111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relationships between the Phase Retrieval Problem and Permutation  Invariant Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Balan%2C+R">Radu Balan</a>, 
<a href="/search/math?searchtype=author&query=Tsoukanis%2C+E">Efstratios Tsoukanis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the SampTA 2023 conference, July 2023, Yale University, New Haven, CT
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1157">[1157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13460" title="Abstract">arXiv:2306.13460</a> (replaced) [<a href="/pdf/2306.13460" title="Download PDF">pdf</a>, <a href="/format/2306.13460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Descriptive Image Captioning via Semipermeable Maximum  Likelihood Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+Z">Zihao Yue</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+A">Anwen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qin Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1158">[1158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13631" title="Abstract">arXiv:2306.13631</a> (replaced) [<a href="/pdf/2306.13631" title="Download PDF">pdf</a>, <a href="/format/2306.13631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenMask3D: Open-Vocabulary 3D Instance Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takmaz%2C+A">Ay&#xe7;a Takmaz</a>, 
<a href="/search/cs?searchtype=author&query=Fedele%2C+E">Elisabetta Fedele</a>, 
<a href="/search/cs?searchtype=author&query=Sumner%2C+R+W">Robert W. Sumner</a>, 
<a href="/search/cs?searchtype=author&query=Pollefeys%2C+M">Marc Pollefeys</a>, 
<a href="/search/cs?searchtype=author&query=Tombari%2C+F">Federico Tombari</a>, 
<a href="/search/cs?searchtype=author&query=Engelmann%2C+F">Francis Engelmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Project page: <a href="https://openmask3d.github.io/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1159">[1159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13996" title="Abstract">arXiv:2306.13996</a> (replaced) [<a href="/pdf/2306.13996" title="Download PDF">pdf</a>, <a href="/ps/2306.13996" title="Download PostScript">ps</a>, <a href="/format/2306.13996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation Algorithm for Unrooted Prize-Collecting Forest with  Multiple Components and Its Application on Prize-Collecting Sweep Coverage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+W">Wei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shaojie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1160">[1160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14479" title="Abstract">arXiv:2306.14479</a> (replaced) [<a href="/pdf/2306.14479" title="Download PDF">pdf</a>, <a href="/format/2306.14479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design from Policies: Conservative Test-Time Adaptation for Offline  Policy Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongyin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zifeng Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yachen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Donglin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1161">[1161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14818" title="Abstract">arXiv:2306.14818</a> (replaced) [<a href="/pdf/2306.14818" title="Download PDF">pdf</a>, <a href="/format/2306.14818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Molecular Graph Neural Networks via Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kelvinius%2C+F+E">Filip Ekstr&#xf6;m Kelvinius</a>, 
<a href="/search/cs?searchtype=author&query=Georgiev%2C+D">Dimitar Georgiev</a>, 
<a href="/search/cs?searchtype=author&query=Toshev%2C+A+P">Artur Petrov Toshev</a>, 
<a href="/search/cs?searchtype=author&query=Gasteiger%2C+J">Johannes Gasteiger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a conference paper at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph)

</div>
</div>
</dd>
<dt><a name="item1162">[1162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14898" title="Abstract">arXiv:2306.14898</a> (replaced) [<a href="/pdf/2306.14898" title="Download PDF">pdf</a>, <a href="/format/2306.14898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InterCode: Standardizing and Benchmarking Interactive Coding with  Execution Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">John Yang</a>, 
<a href="/search/cs?searchtype=author&query=Prabhakar%2C+A">Akshara Prabhakar</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+K">Karthik Narasimhan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shunyu Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project site with code and data: <a href="https://intercode-benchmark.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1163">[1163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15156" title="Abstract">arXiv:2306.15156</a> (replaced) [<a href="/pdf/2306.15156" title="Download PDF">pdf</a>, <a href="/format/2306.15156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning non-Markovian Decision-Making from State-only Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+A">Aoyang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Feng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Sirui Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1164">[1164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15515" title="Abstract">arXiv:2306.15515</a> (replaced) [<a href="/pdf/2306.15515" title="Download PDF">pdf</a>, <a href="/format/2306.15515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Abdominal organ segmentation via deep diffeomorphic mesh deformations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bongratz%2C+F">Fabian Bongratz</a>, 
<a href="/search/cs?searchtype=author&query=Rickmann%2C+A">Anne-Marie Rickmann</a>, 
<a href="/search/cs?searchtype=author&query=Wachinger%2C+C">Christian Wachinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Scientific Reports
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1165">[1165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16424" title="Abstract">arXiv:2306.16424</a> (replaced) [<a href="/pdf/2306.16424" title="Download PDF">pdf</a>, <a href="/format/2306.16424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Realistic Synthetic Financial Transactions for Anti-Money Laundering  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altman%2C+E">Erik Altman</a>, 
<a href="/search/cs?searchtype=author&query=Blanu%C5%A1a%2C+J">Jovan Blanu&#x161;a</a>, 
<a href="/search/cs?searchtype=author&query=von+Niederh%C3%A4usern%2C+L">Luc von Niederh&#xe4;usern</a>, 
<a href="/search/cs?searchtype=author&query=Egressy%2C+B">B&#xe9;ni Egressy</a>, 
<a href="/search/cs?searchtype=author&query=Anghel%2C+A">Andreea Anghel</a>, 
<a href="/search/cs?searchtype=author&query=Atasu%2C+K">Kubilay Atasu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Computational Finance (q-fin.CP)

</div>
</div>
</dd>
<dt><a name="item1166">[1166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17046" title="Abstract">arXiv:2306.17046</a> (replaced) [<a href="/pdf/2306.17046" title="Download PDF">pdf</a>, <a href="/format/2306.17046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiking Denoising Diffusion Probabilistic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiahang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hanzhong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Renjing Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1167">[1167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17194" title="Abstract">arXiv:2306.17194</a> (replaced) [<a href="/pdf/2306.17194" title="Download PDF">pdf</a>, <a href="/format/2306.17194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Exploitability of Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+M">Manli Shu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiongxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Geiping%2C+J">Jonas Geiping</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+T">Tom Goldstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready (21 pages, 10 figures)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1168">[1168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17283" title="Abstract">arXiv:2306.17283</a> (replaced) [<a href="/pdf/2306.17283" title="Download PDF">pdf</a>, <a href="/format/2306.17283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neural Separation Algorithm for the Rounded Capacity Inequalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeonah Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinkyoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+C">Changhyun Kwon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1169">[1169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17842" title="Abstract">arXiv:2306.17842</a> (replaced) [<a href="/pdf/2306.17842" title="Download PDF">pdf</a>, <a href="/format/2306.17842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen  LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lijun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiruo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vivek Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Macherey%2C+W">Wolfgang Macherey</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yanping Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+D+A">David A. Ross</a>, 
<a href="/search/cs?searchtype=author&query=Essa%2C+I">Irfan Essa</a>, 
<a href="/search/cs?searchtype=author&query=Bisk%2C+Y">Yonatan Bisk</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+K">Kevin Murphy</a>, 
<a href="/search/cs?searchtype=author&query=Hauptmann%2C+A+G">Alexander G. Hauptmann</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lu Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1170">[1170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00134" title="Abstract">arXiv:2307.00134</a> (replaced) [<a href="/pdf/2307.00134" title="Download PDF">pdf</a>, <a href="/format/2307.00134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization Limits of Graph Neural Networks in Identity Effects  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Inverno%2C+G+A">Giuseppe Alessio D&#x27;Inverno</a>, 
<a href="/search/cs?searchtype=author&query=Brugiapaglia%2C+S">Simone Brugiapaglia</a>, 
<a href="/search/cs?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1171">[1171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00142" title="Abstract">arXiv:2307.00142</a> (replaced) [<a href="/pdf/2307.00142" title="Download PDF">pdf</a>, <a href="/format/2307.00142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark  for Short-Term Load Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emami%2C+P">Patrick Emami</a>, 
<a href="/search/cs?searchtype=author&query=Sahu%2C+A">Abhijeet Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Graf%2C+P">Peter Graf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Datasets &amp; Benchmarks Track. 35 pages. Code available at <a href="https://github.com/NREL/BuildingsBench/">this https URL</a> and data available at <a href="https://data.openei.org/submissions/5859">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1172">[1172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00716" title="Abstract">arXiv:2307.00716</a> (replaced) [<a href="/pdf/2307.00716" title="Download PDF">pdf</a>, <a href="/format/2307.00716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JourneyDB: A Benchmark for Generative Image Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Keqiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Junting Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yuying Ge</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Haodong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoshi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Renrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Aojun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zipeng Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Jifeng Dai</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Limin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1173">[1173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00904" title="Abstract">arXiv:2307.00904</a> (replaced) [<a href="/pdf/2307.00904" title="Download PDF">pdf</a>, <a href="/format/2307.00904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An open-source deep learning algorithm for efficient and fully-automatic  analysis of the choroid in optical coherence tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Burke%2C+J">Jamie Burke</a>, 
<a href="/search/eess?searchtype=author&query=Engelmann%2C+J">Justin Engelmann</a>, 
<a href="/search/eess?searchtype=author&query=Hamid%2C+C">Charlene Hamid</a>, 
<a href="/search/eess?searchtype=author&query=Reid-Schachter%2C+M">Megan Reid-Schachter</a>, 
<a href="/search/eess?searchtype=author&query=Pearson%2C+T">Tom Pearson</a>, 
<a href="/search/eess?searchtype=author&query=Pugh%2C+D">Dan Pugh</a>, 
<a href="/search/eess?searchtype=author&query=Dhaun%2C+N">Neeraj Dhaun</a>, 
<a href="/search/eess?searchtype=author&query=King%2C+S">Stuart King</a>, 
<a href="/search/eess?searchtype=author&query=MacGillivray%2C+T">Tom MacGillivray</a>, 
<a href="/search/eess?searchtype=author&query=Bernabeu%2C+M+O">Miguel O. Bernabeu</a>, 
<a href="/search/eess?searchtype=author&query=Storkey%2C+A">Amos Storkey</a>, 
<a href="/search/eess?searchtype=author&query=MacCormick%2C+I+J+C">Ian J.C. MacCormick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 3 tables. Accepted for publication in ARVO TVST (Association for Research in Vision and Ophthalmology, Translational Vision Science &amp; Technology). The code and model weights for DeepGPET are available here: <a href="https://github.com/jaburke166/deepgpet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item1174">[1174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01166" title="Abstract">arXiv:2307.01166</a> (replaced) [<a href="/pdf/2307.01166" title="Download PDF">pdf</a>, <a href="/format/2307.01166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Distribution Shift of Interacting Agents via Coupled Gradient  Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conger%2C+L">Lauren Conger</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+F">Franca Hoffmann</a>, 
<a href="/search/cs?searchtype=author&query=Mazumdar%2C+E">Eric Mazumdar</a>, 
<a href="/search/cs?searchtype=author&query=Ratliff%2C+L">Lillian Ratliff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item1175">[1175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01217" title="Abstract">arXiv:2307.01217</a> (replaced) [<a href="/pdf/2307.01217" title="Download PDF">pdf</a>, <a href="/format/2307.01217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedCP: Separating Feature Information for Personalized Federated  Learning via Conditional Policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yang Hua</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+T">Tao Song</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhengui Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">Ruhui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+H">Haibing Guan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by KDD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1176">[1176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01357" title="Abstract">arXiv:2307.01357</a> (replaced) [<a href="/pdf/2307.01357" title="Download PDF">pdf</a>, <a href="/format/2307.01357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Principal Component Regression with Applications to Panel Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Anish Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+K">Keegan Harris</a>, 
<a href="/search/cs?searchtype=author&query=Whitehouse%2C+J">Justin Whitehouse</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In the thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Econometrics (econ.EM); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1177">[1177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01426" title="Abstract">arXiv:2307.01426</a> (replaced) [<a href="/pdf/2307.01426" title="Download PDF">pdf</a>, <a href="/format/2307.01426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiyuan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xinhang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Siwei Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Baoyuan Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1178">[1178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01546" title="Abstract">arXiv:2307.01546</a> (replaced) [<a href="/pdf/2307.01546" title="Download PDF">pdf</a>, <a href="/format/2307.01546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretraining Conformer with ASR or ASV for Anti-Spoofing Countermeasure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yikang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nishizaki%2C+H">Hiromitsu Nishizaki</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1179">[1179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01712" title="Abstract">arXiv:2307.01712</a> (replaced) [<a href="/pdf/2307.01712" title="Download PDF">pdf</a>, <a href="/format/2307.01712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuchs&#x27; theorem on linear differential equations in arbitrary  characteristic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=F%C3%BCrnsinn%2C+F">Florian F&#xfc;rnsinn</a>, 
<a href="/search/math?searchtype=author&query=Hauser%2C+H">Herwig Hauser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Symbolic Computation (cs.SC); Commutative Algebra (math.AC); Number Theory (math.NT)

</div>
</div>
</dd>
<dt><a name="item1180">[1180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01770" title="Abstract">arXiv:2307.01770</a> (replaced) [<a href="/pdf/2307.01770" title="Download PDF">pdf</a>, <a href="/format/2307.01770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Mahey%2C+G">Guillaume Mahey</a>, 
<a href="/search/stat?searchtype=author&query=Chapel%2C+L">Laetitia Chapel</a>, 
<a href="/search/stat?searchtype=author&query=Gasso%2C+G">Gilles Gasso</a>, 
<a href="/search/stat?searchtype=author&query=Bonet%2C+C">Cl&#xe9;ment Bonet</a>, 
<a href="/search/stat?searchtype=author&query=Courty%2C+N">Nicolas Courty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main: 10 pages,4 Figures Tables Supplementary: 19 pages, 13 Figures ,1 Table. Sumbitted to Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item1181">[1181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02520" title="Abstract">arXiv:2307.02520</a> (replaced) [<a href="/pdf/2307.02520" title="Download PDF">pdf</a>, <a href="/format/2307.02520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional independence testing under misspecified inductive biases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Polo%2C+F+M">Felipe Maia Polo</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+Y">Yuekai Sun</a>, 
<a href="/search/stat?searchtype=author&query=Banerjee%2C+M">Moulinath Banerjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1182">[1182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03406" title="Abstract">arXiv:2307.03406</a> (replaced) [<a href="/pdf/2307.03406" title="Download PDF">pdf</a>, <a href="/format/2307.03406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Goal-Conditioned Predictive Coding for Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zilai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Ce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chen Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, Project Website: <a href="https://brown-palm.github.io/GCPC/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1183">[1183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03848" title="Abstract">arXiv:2307.03848</a> (replaced) [<a href="/pdf/2307.03848" title="Download PDF">pdf</a>, <a href="/format/2307.03848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Learners for Realizable Regression: PAC Learning and Online  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Attias%2C+I">Idan Attias</a>, 
<a href="/search/cs?searchtype=author&query=Hanneke%2C+S">Steve Hanneke</a>, 
<a href="/search/cs?searchtype=author&query=Kalavasis%2C+A">Alkis Kalavasis</a>, 
<a href="/search/cs?searchtype=author&query=Karbasi%2C+A">Amin Karbasi</a>, 
<a href="/search/cs?searchtype=author&query=Velegkas%2C+G">Grigoris Velegkas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1184">[1184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03913" title="Abstract">arXiv:2307.03913</a> (replaced) [<a href="/pdf/2307.03913" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applying human-centered AI in developing effective human-AI teaming: A  perspective of human-AI joint cognitive systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zaifeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1185">[1185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04284" title="Abstract">arXiv:2307.04284</a> (replaced) [<a href="/pdf/2307.04284" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effects of Network Connectivity and Functional Diversity Distribution on  Human Collective Ideation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yiding Cao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yingjun Dong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minjun Kim</a>, 
<a href="/search/cs?searchtype=author&query=MacLaren%2C+N+G">Neil G. MacLaren</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S">Sriniwas Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Dionne%2C+S+D">Shelley D. Dionne</a>, 
<a href="/search/cs?searchtype=author&query=Yammarino%2C+F+J">Francis J. Yammarino</a>, 
<a href="/search/cs?searchtype=author&query=Sayama%2C+H">Hiroki Sayama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 19 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item1186">[1186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04317" title="Abstract">arXiv:2307.04317</a> (replaced) [<a href="/pdf/2307.04317" title="Download PDF">pdf</a>, <a href="/format/2307.04317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Descriptions are Compressive and Invariant Representations for  Visual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhili Feng</a>, 
<a href="/search/cs?searchtype=author&query=Bair%2C+A">Anna Bair</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+J+Z">J. Zico Kolter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1187">[1187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04333" title="Abstract">arXiv:2307.04333</a> (replaced) [<a href="/pdf/2307.04333" title="Download PDF">pdf</a>, <a href="/format/2307.04333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Adversarial Robustness via Score-Based Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Boya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Weijian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihua Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1188">[1188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04657" title="Abstract">arXiv:2307.04657</a> (replaced) [<a href="/pdf/2307.04657" title="Download PDF">pdf</a>, <a href="/format/2307.04657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BeaverTails: Towards Improved Safety Alignment of LLM via a  Human-Preference Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mickel Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Juntao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+C">Ce Bian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruiyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS Datasets and Benchmarks 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1189">[1189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05194" title="Abstract">arXiv:2307.05194</a> (replaced) [<a href="/pdf/2307.05194" title="Download PDF">pdf</a>, <a href="/format/2307.05194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Statistical Inference through $&#x3b2;$-Divergence  One Posterior Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Jewson%2C+J">Jack Jewson</a>, 
<a href="/search/stat?searchtype=author&query=Ghalebikesabi%2C+S">Sahra Ghalebikesabi</a>, 
<a href="/search/stat?searchtype=author&query=Holmes%2C+C">Chris Holmes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item1190">[1190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05397" title="Abstract">arXiv:2307.05397</a> (replaced) [<a href="/pdf/2307.05397" title="Download PDF">pdf</a>, <a href="/format/2307.05397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Vulnerability of DeepFake Detectors to Attacks Generated by  Denoising Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ivanovska%2C+M">Marija Ivanovska</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0truc%2C+V">Vitomir &#x160;truc</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1191">[1191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06129" title="Abstract">arXiv:2307.06129</a> (replaced) [<a href="/pdf/2307.06129" title="Download PDF">pdf</a>, <a href="/format/2307.06129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Channel Estimation for Beyond Diagonal Reconfigurable Intelligent  Surfaces with Group-Connected Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hongyu Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yumeng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, accepted by CAMSAP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1192">[1192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06290" title="Abstract">arXiv:2307.06290</a> (replaced) [<a href="/pdf/2307.06290" title="Download PDF">pdf</a>, <a href="/format/2307.06290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruction Mining: When Data Mining Meets Large Language Model  Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yihan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yanbin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1193">[1193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06350" title="Abstract">arXiv:2307.06350</a> (replaced) [<a href="/pdf/2307.06350" title="Download PDF">pdf</a>, <a href="/format/2307.06350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional  Text-to-image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaiyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Kaiyue Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xihui Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://karine-h.github.io/T2I-CompBench/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1194">[1194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06579" title="Abstract">arXiv:2307.06579</a> (replaced) [<a href="/pdf/2307.06579" title="Download PDF">pdf</a>, <a href="/ps/2307.06579" title="Download PostScript">ps</a>, <a href="/format/2307.06579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge-Coloring Algorithms for Bounded Degree Multigraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhawan%2C+A">Abhishek Dhawan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 14 figures. arXiv admin note: text overlap with <a href="/abs/2303.05408">arXiv:2303.05408</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item1195">[1195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06775" title="Abstract">arXiv:2307.06775</a> (replaced) [<a href="/pdf/2307.06775" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Site-Agnostic Multimodal Deep Learning Model to Identify  Pro-Eating Disorder Content on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldman%2C+J">Jonathan Feldman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1196">[1196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06957" title="Abstract">arXiv:2307.06957</a> (replaced) [<a href="/pdf/2307.06957" title="Download PDF">pdf</a>, <a href="/format/2307.06957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embracing the chaos: analysis and diagnosis of numerical instability in  variational flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+Z">Zuheng Xu</a>, 
<a href="/search/stat?searchtype=author&query=Campbell%2C+T">Trevor Campbell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item1197">[1197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06985" title="Abstract">arXiv:2307.06985</a> (replaced) [<a href="/pdf/2307.06985" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Populating Generalizable Engineering Design Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siddharth%2C+L">L Siddharth</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jianxi Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Databases (cs.DB); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1198">[1198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07120" title="Abstract">arXiv:2307.07120</a> (replaced) [<a href="/pdf/2307.07120" title="Download PDF">pdf</a>, <a href="/format/2307.07120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Genetic Algorithm for the min-max Multiple Traveling Salesman  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahmoudinazlou%2C+S">Sasan Mahmoudinazlou</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+C">Changhyun Kwon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1199">[1199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07288" title="Abstract">arXiv:2307.07288</a> (replaced) [<a href="/pdf/2307.07288" title="Download PDF">pdf</a>, <a href="/format/2307.07288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Neural Feature Fusion Function for Multispectral and  Hyperspectral Image Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">ShangQi Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">RuoCheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+L">Liang-Jian Deng</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+R">Ran Ran</a>, 
<a href="/search/cs?searchtype=author&query=Vivone%2C+G">Gemine Vivone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1200">[1200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07771" title="Abstract">arXiv:2307.07771</a> (replaced) [<a href="/pdf/2307.07771" title="Download PDF">pdf</a>, <a href="/format/2307.07771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Gradient Boosting for Zero-Inflated Insurance Claims and  Comparative Analysis of CatBoost, XGBoost, and LightGBM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=So%2C+B">Banghee So</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26pages, 6tables, 7figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1201">[1201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08283" title="Abstract">arXiv:2307.08283</a> (replaced) [<a href="/pdf/2307.08283" title="Download PDF">pdf</a>, <a href="/format/2307.08283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity Matters: Rethinking the Latent Space for Generative Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haonan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiacheng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1202">[1202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08452" title="Abstract">arXiv:2307.08452</a> (replaced) [<a href="/pdf/2307.08452" title="Download PDF">pdf</a>, <a href="/format/2307.08452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SBMLtoODEjax: Efficient Simulation and Optimization of Biological  Network Models in JAX
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Etcheverry%2C+M">Mayalen Etcheverry</a>, 
<a href="/search/q-bio?searchtype=author&query=Levin%2C+M">Michael Levin</a>, 
<a href="/search/q-bio?searchtype=author&query=Moulin-Frier%2C+C">Cl&#xe9;ment Moulin-Frier</a>, 
<a href="/search/q-bio?searchtype=author&query=Oudeyer%2C+P">Pierre-Yves Oudeyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item1203">[1203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08657" title="Abstract">arXiv:2307.08657</a> (replaced) [<a href="/pdf/2307.08657" title="Download PDF">pdf</a>, <a href="/format/2307.08657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Image Compression: Generalization, Robustness, and Spectral  Biases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lieberman%2C+K">Kelsey Lieberman</a>, 
<a href="/search/eess?searchtype=author&query=Diffenderfer%2C+J">James Diffenderfer</a>, 
<a href="/search/eess?searchtype=author&query=Godfrey%2C+C">Charles Godfrey</a>, 
<a href="/search/eess?searchtype=author&query=Kailkhura%2C+B">Bhavya Kailkhura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1204">[1204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08763" title="Abstract">arXiv:2307.08763</a> (replaced) [<a href="/pdf/2307.08763" title="Download PDF">pdf</a>, <a href="/format/2307.08763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-Mined Task Graphs for Keystep Recognition in Instructional Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ashutosh%2C+K">Kumar Ashutosh</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+S+K">Santhosh Kumar Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Afouras%2C+T">Triantafyllos Afouras</a>, 
<a href="/search/cs?searchtype=author&query=Grauman%2C+K">Kristen Grauman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1205">[1205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09258" title="Abstract">arXiv:2307.09258</a> (replaced) [<a href="/pdf/2307.09258" title="Download PDF">pdf</a>, <a href="/format/2307.09258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast 2-Approximate All-Pairs Shortest Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dory%2C+M">Michal Dory</a>, 
<a href="/search/cs?searchtype=author&query=Forster%2C+S">Sebastian Forster</a>, 
<a href="/search/cs?searchtype=author&query=Kirkpatrick%2C+Y">Yael Kirkpatrick</a>, 
<a href="/search/cs?searchtype=author&query=Nazari%2C+Y">Yasamin Nazari</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+V+V">Virginia Vassilevska Williams</a>, 
<a href="/search/cs?searchtype=author&query=de+Vos%2C+T">Tijn de Vos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to SODA '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item1206">[1206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10524" title="Abstract">arXiv:2307.10524</a> (replaced) [<a href="/pdf/2307.10524" title="Download PDF">pdf</a>, <a href="/format/2307.10524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with  Q-Value Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tongxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yiheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shaolei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wierman%2C+A">Adam Wierman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1207">[1207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10646" title="Abstract">arXiv:2307.10646</a> (replaced) [<a href="/pdf/2307.10646" title="Download PDF">pdf</a>, <a href="/format/2307.10646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Enhancing Reliability in B5G NTNs with Packet Duplication via  Multi-Connectivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majamaa%2C+M">Mikko Majamaa</a>, 
<a href="/search/cs?searchtype=author&query=Martikainen%2C+H">Henrik Martikainen</a>, 
<a href="/search/cs?searchtype=author&query=Puttonen%2C+J">Jani Puttonen</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4m%C3%A4lainen%2C+T">Timo H&#xe4;m&#xe4;lainen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in 2023 IEEE International Conference on Wireless for Space and Extreme Environments (WiSEE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item1208">[1208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11949" title="Abstract">arXiv:2307.11949</a> (replaced) [<a href="/pdf/2307.11949" title="Download PDF">pdf</a>, <a href="/format/2307.11949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HIQL: Offline Goal-Conditioned RL with Latent States as Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seohong Park</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+D">Dibya Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Eysenbach%2C+B">Benjamin Eysenbach</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1209">[1209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11965" title="Abstract">arXiv:2307.11965</a> (replaced) [<a href="/pdf/2307.11965" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Intelligent Remote Sensing Image Quality Inspection System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yijiong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+K">Kang Ran</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hao Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1210">[1210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12388" title="Abstract">arXiv:2307.12388</a> (replaced) [<a href="/pdf/2307.12388" title="Download PDF">pdf</a>, <a href="/format/2307.12388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-aware Grounded Action Transformation towards Sim-to-Real  Transfer for Traffic Signal Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Da%2C+L">Longchao Da</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Hao Mei</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Romir Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hua Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures. This paper is accepted by IEEE-CDC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1211">[1211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12407" title="Abstract">arXiv:2307.12407</a> (replaced) [<a href="/pdf/2307.12407" title="Download PDF">pdf</a>, <a href="/format/2307.12407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JAX FDM: A differentiable solver for inverse form-finding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pastrana%2C+R">Rafael Pastrana</a>, 
<a href="/search/cs?searchtype=author&query=Oktay%2C+D">Deniz Oktay</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+R+P">Ryan P. Adams</a>, 
<a href="/search/cs?searchtype=author&query=Adriaenssens%2C+S">Sigrid Adriaenssens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/arpastrana/jax_fdm">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 40th International Conference on Machine Learning (2023:
  Differentiable Almost Everything Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item1212">[1212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12975" title="Abstract">arXiv:2307.12975</a> (replaced) [<a href="/pdf/2307.12975" title="Download PDF">pdf</a>, <a href="/ps/2307.12975" title="Download PostScript">ps</a>, <a href="/format/2307.12975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Benefits of Policy Learning from Human Preferences in  Contextual Bandit Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiang Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huazheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minshuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1213">[1213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15367" title="Abstract">arXiv:2307.15367</a> (replaced) [<a href="/pdf/2307.15367" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Transparent Sequence Models with Model-Based Tree Markov Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chan Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei-Chun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jun-Ting Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chih-Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yihuang Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a workshop paper at IEEE MIPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1214">[1214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15421" title="Abstract">arXiv:2307.15421</a> (replaced) [<a href="/pdf/2307.15421" title="Download PDF">pdf</a>, <a href="/format/2307.15421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned  Image Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+R">Ronggang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICML 2023 Neural Compression Workshop. Extension work of our ACMMM 2023 paper MLIC: Multi-Reference Entropy Model for Learned Image Compression
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1215">[1215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15612" title="Abstract">arXiv:2307.15612</a> (replaced) [<a href="/pdf/2307.15612" title="Download PDF">pdf</a>, <a href="/ps/2307.15612" title="Download PostScript">ps</a>, <a href="/format/2307.15612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed Points and Attractors of Reactantless and Inhibitorless Reaction  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ascone%2C+R">Rocco Ascone</a>, 
<a href="/search/cs?searchtype=author&query=Bernardini%2C+G">Giulia Bernardini</a>, 
<a href="/search/cs?searchtype=author&query=Manzoni%2C+L">Luca Manzoni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages. The previous version contained major mistakes. The wrong results are the former Corollaries 9, 10, 15, 17, 23, 25, 25, 32 and 36, Proposition 31 and Remark 30 (all following from a single mistake). We have now corrected all these results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item1216">[1216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15969" title="Abstract">arXiv:2307.15969</a> (replaced) [<a href="/pdf/2307.15969" title="Download PDF">pdf</a>, <a href="/format/2307.15969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Searching The Densest Subgraph And Decomposition With Local  Optimality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yugao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shenghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Wenjie Feng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item1217">[1217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16555" title="Abstract">arXiv:2307.16555</a> (replaced) [<a href="/pdf/2307.16555" title="Download PDF">pdf</a>, <a href="/format/2307.16555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-Guided Spatial Pruning Architecture for Efficient Frame  Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Ri Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xuhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ruian He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shili Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Weimin Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bo Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM Multimedia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1218">[1218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16787" title="Abstract">arXiv:2307.16787</a> (replaced) [<a href="/pdf/2307.16787" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Ethics of AI Value Chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Attard-Frost%2C+B">Blair Attard-Frost</a>, 
<a href="/search/cs?searchtype=author&query=Widder%2C+D+G">David Gray Widder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1219">[1219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00186" title="Abstract">arXiv:2308.00186</a> (replaced) [<a href="/pdf/2308.00186" title="Download PDF">pdf</a>, <a href="/format/2308.00186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Complex Motion Plans using Neural ODEs with Safety and  Stability Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nawaz%2C+F">Farhad Nawaz</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Matni%2C+N">Nikolai Matni</a>, 
<a href="/search/cs?searchtype=author&query=Figueroa%2C+N">Nadia Figueroa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1220">[1220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00949" title="Abstract">arXiv:2308.00949</a> (replaced) [<a href="/pdf/2308.00949" title="Download PDF">pdf</a>, <a href="/format/2308.00949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Instance Segmentation from Semantic Image Segmentation  Masks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yuchen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yuhui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zechao Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Liyong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qiaolin Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages,5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1221">[1221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01328" title="Abstract">arXiv:2308.01328</a> (replaced) [<a href="/pdf/2308.01328" title="Download PDF">pdf</a>, <a href="/format/2308.01328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A vision transformer-based framework for knowledge transfer from  multi-modal to mono-modal lymphoma subtyping models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guetarni%2C+B">Bilel Guetarni</a>, 
<a href="/search/eess?searchtype=author&query=Windal%2C+F">Feryal Windal</a>, 
<a href="/search/eess?searchtype=author&query=Benhabiles%2C+H">Halim Benhabiles</a>, 
<a href="/search/eess?searchtype=author&query=Petit%2C+M">Marianne Petit</a>, 
<a href="/search/eess?searchtype=author&query=Dubois%2C+R">Romain Dubois</a>, 
<a href="/search/eess?searchtype=author&query=Leteurtre%2C+E">Emmanuelle Leteurtre</a>, 
<a href="/search/eess?searchtype=author&query=Collard%2C+D">Dominique Collard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item1222">[1222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01940" title="Abstract">arXiv:2308.01940</a> (replaced) [<a href="/pdf/2308.01940" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TSMD: A Database for Static Color Mesh Quality Assessment Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+J">Joel Jung</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaozhong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1223">[1223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03801" title="Abstract">arXiv:2308.03801</a> (replaced) [<a href="/pdf/2308.03801" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On problematic practice of using normalization in  Self-modeling/Multivariate Curve Resolution (S/MCR)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rajk%C3%B3%2C+R">R&#xf3;bert Rajk&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Meantime changed affiliation, terms; inserting more explanations and codes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1224">[1224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04603" title="Abstract">arXiv:2308.04603</a> (replaced) [<a href="/pdf/2308.04603" title="Download PDF">pdf</a>, <a href="/format/2308.04603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Brief Yet In-Depth Survey of Deep Learning-Based Image Watermarking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+X">Xin Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Arjon Das</a>, 
<a href="/search/cs?searchtype=author&query=Alrasheedi%2C+F">Fahad Alrasheedi</a>, 
<a href="/search/cs?searchtype=author&query=Tanvir%2C+A">Abdullah Tanvir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was accepted for publication by the MDPI Applied Sciences journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1225">[1225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05309" title="Abstract">arXiv:2308.05309</a> (replaced) [<a href="/pdf/2308.05309" title="Download PDF">pdf</a>, <a href="/format/2308.05309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homophily-enhanced Structure Learning for Graph Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+M">Ming Gu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Gaoming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+N">Ning Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiawei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Q">Qiaoyu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Meihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+J">Jiajun Bu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages with 7 figures. Accepted by CIKM'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1226">[1226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05859" title="Abstract">arXiv:2308.05859</a> (replaced) [<a href="/pdf/2308.05859" title="Download PDF">pdf</a>, <a href="/format/2308.05859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posiform Planting: Generating QUBO Instances for Benchmarking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Hahn%2C+G">Georg Hahn</a>, 
<a href="/search/quant-ph?searchtype=author&query=Pelofske%2C+E">Elijah Pelofske</a>, 
<a href="/search/quant-ph?searchtype=author&query=Djidjev%2C+H+N">Hristo N. Djidjev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Emerging Technologies (cs.ET); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item1227">[1227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06027" title="Abstract">arXiv:2308.06027</a> (replaced) [<a href="/pdf/2308.06027" title="Download PDF">pdf</a>, <a href="/format/2308.06027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked-Attention Diffusion Guidance for Spatially Controlling  Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Endo%2C+Y">Yuki Endo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to The Visual Computer, code: <a href="https://github.com/endo-yuki-t/MAG">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1228">[1228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06091" title="Abstract">arXiv:2308.06091</a> (replaced) [<a href="/pdf/2308.06091" title="Download PDF">pdf</a>, <a href="/format/2308.06091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward a Better Understanding of Loss Functions for Collaborative  Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seongmin Park</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+M">Mincheol Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jae-woong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hogun Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jongwuk Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1229">[1229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06483" title="Abstract">arXiv:2308.06483</a> (replaced) [<a href="/pdf/2308.06483" title="Download PDF">pdf</a>, <a href="/format/2308.06483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music  Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yenan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+H">Hiroshi Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE GCCE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1230">[1230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06672" title="Abstract">arXiv:2308.06672</a> (replaced) [<a href="/pdf/2308.06672" title="Download PDF">pdf</a>, <a href="/format/2308.06672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A practical PINN framework for multi-scale problems with multi-magnitude  loss terms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yanzhong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiawei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhiming Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1231">[1231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06828" title="Abstract">arXiv:2308.06828</a> (replaced) [<a href="/pdf/2308.06828" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Ensemble Approach to Question Classification: Integrating Electra  Transformer, GloVe, and LSTM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aburass%2C+S">Sanad Aburass</a>, 
<a href="/search/cs?searchtype=author&query=Dorgham%2C+O">Osama Dorgham</a>, 
<a href="/search/cs?searchtype=author&query=Rumman%2C+M+A">Maha Abu Rumman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1232">[1232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10544" title="Abstract">arXiv:2308.10544</a> (replaced) [<a href="/pdf/2308.10544" title="Download PDF">pdf</a>, <a href="/format/2308.10544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Accelerated Model Training via Bayesian Data Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhijie Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+P">Peng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1233">[1233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10631" title="Abstract">arXiv:2308.10631</a> (replaced) [<a href="/pdf/2308.10631" title="Download PDF">pdf</a>, <a href="/format/2308.10631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PsyMo: A Dataset for Estimating Self-Reported Psychological Traits from  Gait
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cosma%2C+A">Adrian Cosma</a>, 
<a href="/search/cs?searchtype=author&query=Radoi%2C+E">Emilian Radoi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1234">[1234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10814" title="Abstract">arXiv:2308.10814</a> (replaced) [<a href="/pdf/2308.10814" title="Download PDF">pdf</a>, <a href="/format/2308.10814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jumping through Local Minima: Quantization in the Loss Landscape of  Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frumkin%2C+N">Natalia Frumkin</a>, 
<a href="/search/cs?searchtype=author&query=Gope%2C+D">Dibakar Gope</a>, 
<a href="/search/cs?searchtype=author&query=Marculescu%2C+D">Diana Marculescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2211.09643">arXiv:2211.09643</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1235">[1235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11967" title="Abstract">arXiv:2308.11967</a> (replaced) [<a href="/pdf/2308.11967" title="Download PDF">pdf</a>, <a href="/ps/2308.11967" title="Download PostScript">ps</a>, <a href="/format/2308.11967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Duality for Clans: an Extension of Gabriel-Ulmer Duality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Frey%2C+J">Jonas Frey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Logic in Computer Science (cs.LO); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item1236">[1236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12035" title="Abstract">arXiv:2308.12035</a> (replaced) [<a href="/pdf/2308.12035" title="Download PDF">pdf</a>, <a href="/format/2308.12035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RefEgo: Referring Expression Comprehension Dataset from First-Person  Perception of Ego4D
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurita%2C+S">Shuhei Kurita</a>, 
<a href="/search/cs?searchtype=author&query=Katsura%2C+N">Naoki Katsura</a>, 
<a href="/search/cs?searchtype=author&query=Onami%2C+E">Eri Onami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures. ICCV2023. Codes are available at <a href="https://github.com/shuheikurita/RefEgo">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1237">[1237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13259" title="Abstract">arXiv:2308.13259</a> (replaced) [<a href="/pdf/2308.13259" title="Download PDF">pdf</a>, <a href="/format/2308.13259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for  Knowledge-intensive Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Keheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+F">Feiyu Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peiguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Yunsen Xian</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+C">Chuantao Yin</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+W">Wenge Rong</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhang Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1238">[1238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14113" title="Abstract">arXiv:2308.14113</a> (replaced) [<a href="/pdf/2308.14113" title="Download PDF">pdf</a>, <a href="/ps/2308.14113" title="Download PostScript">ps</a>, <a href="/format/2308.14113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic-aware Consistency Network for Cloth-changing Person  Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Peini Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jianbing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoquan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1239">[1239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14847" title="Abstract">arXiv:2308.14847</a> (replaced) [<a href="/pdf/2308.14847" title="Download PDF">pdf</a>, <a href="/format/2308.14847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NSF: Neural Surface Fields for Human Modeling from Monocular Depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yuxuan Xue</a>, 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+B+L">Bharat Lal Bhatnagar</a>, 
<a href="/search/cs?searchtype=author&query=Marin%2C+R">Riccardo Marin</a>, 
<a href="/search/cs?searchtype=author&query=Sarafianos%2C+N">Nikolaos Sarafianos</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuanlu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pons-Moll%2C+G">Gerard Pons-Moll</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+T">Tony Tung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accpted to ICCV 2023; Homepage at: <a href="https://yuxuan-xue.com/nsf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1240">[1240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14864" title="Abstract">arXiv:2308.14864</a> (replaced) [<a href="/pdf/2308.14864" title="Download PDF">pdf</a>, <a href="/format/2308.14864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NAS-X: Neural Adaptive Smoothing via Twisting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lawson%2C+D">Dieterich Lawson</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Michael Li</a>, 
<a href="/search/cs?searchtype=author&query=Linderman%2C+S">Scott Linderman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updating for clarity and adding new baselines
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1241">[1241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16573" title="Abstract">arXiv:2308.16573</a> (replaced) [<a href="/pdf/2308.16573" title="Download PDF">pdf</a>, <a href="/format/2308.16573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-Decoder Consistency via Pseudo-Labels Guided Data Augmentation for  Semi-Supervised Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yuanbin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+H">Hui Tang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+L">Longxuan Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Zong%2C+R">Ruige Zong</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+S">Shun Chen</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+T">Tao Tan</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xinlin Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Tong%2C+T">Tong Tong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1242">[1242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00073" title="Abstract">arXiv:2309.00073</a> (replaced) [<a href="/pdf/2309.00073" title="Download PDF">pdf</a>, <a href="/format/2309.00073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Variational Autoencoder for Tackling Stochasticity in  Multi-Step Regression Stock Price Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Koa%2C+K+J+L">Kelvin J.L. Koa</a>, 
<a href="/search/q-fin?searchtype=author&query=Ma%2C+Y">Yunshan Ma</a>, 
<a href="/search/q-fin?searchtype=author&query=Ng%2C+R">Ritchie Ng</a>, 
<a href="/search/q-fin?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Machine Learning (cs.LG); Computational Finance (q-fin.CP)

</div>
</div>
</dd>
<dt><a name="item1243">[1243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00613" title="Abstract">arXiv:2309.00613</a> (replaced) [<a href="/pdf/2309.00613" title="Download PDF">pdf</a>, <a href="/format/2309.00613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Multi-granular Image Editing using Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joseph%2C+K+J">K J Joseph</a>, 
<a href="/search/cs?searchtype=author&query=Udhayanan%2C+P">Prateksha Udhayanan</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+T">Tripti Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Aishwarya Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Karanam%2C+S">Srikrishna Karanam</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+K">Koustava Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+B+V">Balaji Vasan Srinivasan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1244">[1244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01087" title="Abstract">arXiv:2309.01087</a> (replaced) [<a href="/pdf/2309.01087" title="Download PDF">pdf</a>, <a href="/format/2309.01087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilize to Act: Learning to Coordinate for Bimanual Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grannen%2C+J">Jennifer Grannen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yilin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+B">Brandon Vu</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Robot Learning, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1245">[1245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01479" title="Abstract">arXiv:2309.01479</a> (replaced) [<a href="/pdf/2309.01479" title="Download PDF">pdf</a>, <a href="/format/2309.01479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter and Computation Efficient Transfer Learning for  Vision-Language Pre-trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shubin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoshuai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1246">[1246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01903" title="Abstract">arXiv:2309.01903</a> (replaced) [<a href="/pdf/2309.01903" title="Download PDF">pdf</a>, <a href="/format/2309.01903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Plant Disease Diagnosis with Hard-sample Re-mining  Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cap%2C+Q+H">Quan Huu Cap</a>, 
<a href="/search/cs?searchtype=author&query=Fukuda%2C+A">Atsushi Fukuda</a>, 
<a href="/search/cs?searchtype=author&query=Kagiwada%2C+S">Satoshi Kagiwada</a>, 
<a href="/search/cs?searchtype=author&query=Uga%2C+H">Hiroyuki Uga</a>, 
<a href="/search/cs?searchtype=author&query=Iwasaki%2C+N">Nobusuke Iwasaki</a>, 
<a href="/search/cs?searchtype=author&query=Iyatomi%2C+H">Hitoshi Iyatomi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1247">[1247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02460" title="Abstract">arXiv:2309.02460</a> (replaced) [<a href="/pdf/2309.02460" title="Download PDF">pdf</a>, <a href="/format/2309.02460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Multi-Graph Neural Networks for Illicit Account Detection on  Cryptocurrency Transaction Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zhihao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jieming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiannong Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1248">[1248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02623" title="Abstract">arXiv:2309.02623</a> (replaced) [<a href="/pdf/2309.02623" title="Download PDF">pdf</a>, <a href="/format/2309.02623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Superclustering by finding statistically significant separable groups of  optimal gaussian clusters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berngardt%2C+O+I">Oleg I.Berngardt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1249">[1249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03329" title="Abstract">arXiv:2309.03329</a> (replaced) [<a href="/pdf/2309.03329" title="Download PDF">pdf</a>, <a href="/format/2309.03329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEGANet: Multi-Scale Edge-Guided Attention Network for Weak Boundary  Polyp Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bui%2C+N">Nhat-Tan Bui</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D">Dinh-Hieu Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q">Quang-Thuc Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+M">Minh-Triet Tran</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+N">Ngan Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1250">[1250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03800" title="Abstract">arXiv:2309.03800</a> (replaced) [<a href="/pdf/2309.03800" title="Download PDF">pdf</a>, <a href="/format/2309.03800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and  Luck
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edelman%2C+B+L">Benjamin L. Edelman</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+S">Surbhi Goel</a>, 
<a href="/search/cs?searchtype=author&query=Kakade%2C+S">Sham Kakade</a>, 
<a href="/search/cs?searchtype=author&query=Malach%2C+E">Eran Malach</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cyril Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: NeurIPS 2023 camera-ready updates
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1251">[1251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03875" title="Abstract">arXiv:2309.03875</a> (replaced) [<a href="/pdf/2309.03875" title="Download PDF">pdf</a>, <a href="/format/2309.03875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Sampling Methods for Estimating Social Networks, Population  Percentages, and Totals of People Experiencing Unsheltered Homelessness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almquist%2C+Z+W">Zack W. Almquist</a>, 
<a href="/search/cs?searchtype=author&query=Hazel%2C+A">Ashley Hazel</a>, 
<a href="/search/cs?searchtype=author&query=Kajfasz%2C+O">Owen Kajfasz</a>, 
<a href="/search/cs?searchtype=author&query=Rothfolk%2C+J">Janelle Rothfolk</a>, 
<a href="/search/cs?searchtype=author&query=Guilmette%2C+C">Claire Guilmette</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+M">Mary-Catherine Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Ozeryansky%2C+L">Larisa Ozeryansky</a>, 
<a href="/search/cs?searchtype=author&query=Hagopian%2C+A">Amy Hagopian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1252">[1252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04272" title="Abstract">arXiv:2309.04272</a> (replaced) [<a href="/pdf/2309.04272" title="Download PDF">pdf</a>, <a href="/format/2309.04272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning in Zero-Sum Linear Quadratic Games with Last-Iterate  Convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Jiduan Wu</a>, 
<a href="/search/eess?searchtype=author&query=Barakat%2C+A">Anas Barakat</a>, 
<a href="/search/eess?searchtype=author&query=Fatkhullin%2C+I">Ilyas Fatkhullin</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+N">Niao He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1253">[1253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05375" title="Abstract">arXiv:2309.05375</a> (replaced) [<a href="/pdf/2309.05375" title="Download PDF">pdf</a>, <a href="/format/2309.05375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward a Deeper Understanding: RetNet Viewed through Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoning Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1254">[1254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05994" title="Abstract">arXiv:2309.05994</a> (replaced) [<a href="/pdf/2309.05994" title="Download PDF">pdf</a>, <a href="/format/2309.05994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution  Detection in Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhitong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shipeng Yan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuming He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1255">[1255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07080" title="Abstract">arXiv:2309.07080</a> (replaced) [<a href="/pdf/2309.07080" title="Download PDF">pdf</a>, <a href="/format/2309.07080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Dynamic DAG Learning: Application in Discovering Dynamic  Effective Connectome of Brain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Bagheri%2C+A">Abdolmahdi Bagheri</a>, 
<a href="/search/q-bio?searchtype=author&query=Pasande%2C+M">Mohammad Pasande</a>, 
<a href="/search/q-bio?searchtype=author&query=Bello%2C+K">Kevin Bello</a>, 
<a href="/search/q-bio?searchtype=author&query=Araabi%2C+B+N">Babak Nadjar Araabi</a>, 
<a href="/search/q-bio?searchtype=author&query=Akhondi-Asl%2C+A">Alireza Akhondi-Asl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1256">[1256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07778" title="Abstract">arXiv:2309.07778</a> (replaced) [<a href="/pdf/2309.07778" title="Download PDF">pdf</a>, <a href="/format/2309.07778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Virchow: A Million-Slide Digital Pathology Foundation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Vorontsov%2C+E">Eugene Vorontsov</a>, 
<a href="/search/eess?searchtype=author&query=Bozkurt%2C+A">Alican Bozkurt</a>, 
<a href="/search/eess?searchtype=author&query=Casson%2C+A">Adam Casson</a>, 
<a href="/search/eess?searchtype=author&query=Shaikovski%2C+G">George Shaikovski</a>, 
<a href="/search/eess?searchtype=author&query=Zelechowski%2C+M">Michal Zelechowski</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Siqi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Mathieu%2C+P">Philippe Mathieu</a>, 
<a href="/search/eess?searchtype=author&query=van+Eck%2C+A">Alexander van Eck</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+D">Donghun Lee</a>, 
<a href="/search/eess?searchtype=author&query=Viret%2C+J">Julian Viret</a>, 
<a href="/search/eess?searchtype=author&query=Robert%2C+E">Eric Robert</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y+K">Yi Kan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Kunz%2C+J+D">Jeremy D. Kunz</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+M+C+H">Matthew C. H. Lee</a>, 
<a href="/search/eess?searchtype=author&query=Bernhard%2C+J">Jan Bernhard</a>, 
<a href="/search/eess?searchtype=author&query=Godrich%2C+R+A">Ran A. Godrich</a>, 
<a href="/search/eess?searchtype=author&query=Oakley%2C+G">Gerard Oakley</a>, 
<a href="/search/eess?searchtype=author&query=Millar%2C+E">Ewan Millar</a>, 
<a href="/search/eess?searchtype=author&query=Hanna%2C+M">Matthew Hanna</a>, 
<a href="/search/eess?searchtype=author&query=Retamero%2C+J">Juan Retamero</a>, 
<a href="/search/eess?searchtype=author&query=Moye%2C+W+A">William A. Moye</a>, 
<a href="/search/eess?searchtype=author&query=Yousfi%2C+R">Razik Yousfi</a>, 
<a href="/search/eess?searchtype=author&query=Kanan%2C+C">Christopher Kanan</a>, 
<a href="/search/eess?searchtype=author&query=Klimstra%2C+D">David Klimstra</a>, 
<a href="/search/eess?searchtype=author&query=Rothrock%2C+B">Brandon Rothrock</a>, 
<a href="/search/eess?searchtype=author&query=Fuchs%2C+T+J">Thomas J. Fuchs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Tissues and Organs (q-bio.TO)

</div>
</div>
</dd>
<dt><a name="item1257">[1257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08036" title="Abstract">arXiv:2309.08036</a> (replaced) [<a href="/pdf/2309.08036" title="Download PDF">pdf</a>, <a href="/format/2309.08036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BEA: Revisiting anchor-based object detection DNN using Budding Ensemble  Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qutub%2C+S+S">Syed Sha Qutub</a>, 
<a href="/search/cs?searchtype=author&query=Kose%2C+N">Neslihan Kose</a>, 
<a href="/search/cs?searchtype=author&query=Rosales%2C+R">Rafael Rosales</a>, 
<a href="/search/cs?searchtype=author&query=Paulitsch%2C+M">Michael Paulitsch</a>, 
<a href="/search/cs?searchtype=author&query=Hagn%2C+K">Korbinian Hagn</a>, 
<a href="/search/cs?searchtype=author&query=Geissler%2C+F">Florian Geissler</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Hinz%2C+G">Gereon Hinz</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 pages supplementary material. Accepted at BMVC-2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1258">[1258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08710" title="Abstract">arXiv:2309.08710</a> (replaced) [<a href="/pdf/2309.08710" title="Download PDF">pdf</a>, <a href="/format/2309.08710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustered Multi-Agent Linear Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cherkaoui%2C+H">Hamza Cherkaoui</a>, 
<a href="/search/cs?searchtype=author&query=Barlier%2C+M">Merwan Barlier</a>, 
<a href="/search/cs?searchtype=author&query=Colin%2C+I">Igor Colin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1259">[1259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10094" title="Abstract">arXiv:2309.10094</a> (replaced) [<a href="/pdf/2309.10094" title="Download PDF">pdf</a>, <a href="/format/2309.10094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Formulator: AI-powered Concept-driven Visualization Authoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenglong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Thompson%2C+J">John Thompson</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Bongshin Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1260">[1260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10711" title="Abstract">arXiv:2309.10711</a> (replaced) [<a href="/pdf/2309.10711" title="Download PDF">pdf</a>, <a href="/format/2309.10711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Space Energy-based Model for Fine-grained Open Set Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+W">Wentao Bao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Y">Yu Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Add ack
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1261">[1261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10918" title="Abstract">arXiv:2309.10918</a> (replaced) [<a href="/pdf/2309.10918" title="Download PDF">pdf</a>, <a href="/format/2309.10918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posterior Contraction Rates for Mat&#xe9;rn Gaussian Processes on  Riemannian Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rosa%2C+P">Paul Rosa</a>, 
<a href="/search/stat?searchtype=author&query=Borovitskiy%2C+V">Viacheslav Borovitskiy</a>, 
<a href="/search/stat?searchtype=author&query=Terenin%2C+A">Alexander Terenin</a>, 
<a href="/search/stat?searchtype=author&query=Rousseau%2C+J">Judith Rousseau</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item1262">[1262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10987" title="Abstract">arXiv:2309.10987</a> (replaced) [<a href="/pdf/2309.10987" title="Download PDF">pdf</a>, <a href="/format/2309.10987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpikingNeRF: Making Bio-inspired Neural Networks See through the Real  World
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xingting Yao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qinghao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tielong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+Z">Zitao Mo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zeyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuge%2C+Z">Zhengyang Zhuge</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jian Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1263">[1263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11523" title="Abstract">arXiv:2309.11523</a> (replaced) [<a href="/pdf/2309.11523" title="Download PDF">pdf</a>, <a href="/format/2309.11523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RMT: Retentive Networks Meet Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Q">Qihang Fan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huaibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongmin Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code will be released at <a href="https://github.com/qhfan/RMT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1264">[1264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12269" title="Abstract">arXiv:2309.12269</a> (replaced) [<a href="/pdf/2309.12269" title="Download PDF">pdf</a>, <a href="/format/2309.12269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Cambridge Law Corpus: A Corpus for Legal AI Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%96stling%2C+A">Andreas &#xd6;stling</a>, 
<a href="/search/cs?searchtype=author&query=Sargeant%2C+H">Holli Sargeant</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Huiyuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Bull%2C+L">Ludwig Bull</a>, 
<a href="/search/cs?searchtype=author&query=Terenin%2C+A">Alexander Terenin</a>, 
<a href="/search/cs?searchtype=author&query=Jonsson%2C+L">Leif Jonsson</a>, 
<a href="/search/cs?searchtype=author&query=Magnusson%2C+M">M&#xe5;ns Magnusson</a>, 
<a href="/search/cs?searchtype=author&query=Steffek%2C+F">Felix Steffek</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems, Datasets and
  Benchmarks Track, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item1265">[1265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13446" title="Abstract">arXiv:2309.13446</a> (replaced) [<a href="/pdf/2309.13446" title="Download PDF">pdf</a>, <a href="/format/2309.13446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video Timeline Modeling For News Story Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Meng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingda Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jialu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hanjun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shuiwang Ji</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zheyun Feng</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+B">Boqing Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a spotlight by NeurIPS 2023, Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1266">[1266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13457" title="Abstract">arXiv:2309.13457</a> (replaced) [<a href="/pdf/2309.13457" title="Download PDF">pdf</a>, <a href="/format/2309.13457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric  Super-Resolution with BLASTNet 2.0 Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+W+T">Wai Tong Chung</a>, 
<a href="/search/cs?searchtype=author&query=Akoush%2C+B">Bassem Akoush</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+P">Pushan Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Tamkin%2C+A">Alex Tamkin</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+K+S">Ki Sung Jung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J+H">Jacqueline H. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jack Guo</a>, 
<a href="/search/cs?searchtype=author&query=Brouzet%2C+D">Davy Brouzet</a>, 
<a href="/search/cs?searchtype=author&query=Talei%2C+M">Mohsen Talei</a>, 
<a href="/search/cs?searchtype=author&query=Savard%2C+B">Bruno Savard</a>, 
<a href="/search/cs?searchtype=author&query=Poludnenko%2C+A+Y">Alexei Y. Poludnenko</a>, 
<a href="/search/cs?searchtype=author&query=Ihme%2C+M">Matthias Ihme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Adv. in Neural Information Processing Systems 36 (NeurIPS 2023). Link: <a href="https://nips.cc/virtual/2023/poster/73433">this https URL</a> . 55 pages, 21 figures. Keywords: Super-resolution, 3D, Neural Scaling, Physics-informed Loss, Computational Fluid Dynamics, Partial Differential Equations, Turbulent Reacting Flows, Direct Numerical Simulation, Fluid Mechanics, Combustion, Computer Vision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item1267">[1267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13591" title="Abstract">arXiv:2309.13591</a> (replaced) [<a href="/pdf/2309.13591" title="Download PDF">pdf</a>, <a href="/format/2309.13591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Distributed Learning: Tight Error Bounds and Breakdown Point  under Data Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allouah%2C+Y">Youssef Allouah</a>, 
<a href="/search/cs?searchtype=author&query=Guerraoui%2C+R">Rachid Guerraoui</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+N">Nirupam Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Pinot%2C+R">Rafa&#xeb;l Pinot</a>, 
<a href="/search/cs?searchtype=author&query=Rizk%2C+G">Geovani Rizk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1268">[1268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13944" title="Abstract">arXiv:2309.13944</a> (replaced) [<a href="/pdf/2309.13944" title="Download PDF">pdf</a>, <a href="/format/2309.13944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Training for Graph Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengmei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight. Camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1269">[1269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13985" title="Abstract">arXiv:2309.13985</a> (replaced) [<a href="/pdf/2309.13985" title="Download PDF">pdf</a>, <a href="/format/2309.13985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Driven ML-Based Modelling for Correcting Inverse Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+R">Ruiyuan Kang</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+T">Tingting Mu</a>, 
<a href="/search/cs?searchtype=author&query=Liatsis%2C+P">Panos Liatsis</a>, 
<a href="/search/cs?searchtype=author&query=Kyritsis%2C+D+C">Dimitrios C. Kyritsis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, the paper is accepted by Neurips 2023 as a spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item1270">[1270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14062" title="Abstract">arXiv:2309.14062</a> (replaced) [<a href="/pdf/2309.14062" title="Download PDF">pdf</a>, <a href="/format/2309.14062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FeCAM: Exploiting the Heterogeneity of Class Distributions in  Exemplar-Free Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goswami%2C+D">Dipam Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Twardowski%2C+B">Bart&#x142;omiej Twardowski</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Weijer%2C+J">Joost van de Weijer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1271">[1271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14078" title="Abstract">arXiv:2309.14078</a> (replaced) [<a href="/pdf/2309.14078" title="Download PDF">pdf</a>, <a href="/format/2309.14078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ODE-based Recurrent Model-free Reinforcement Learning for POMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuanle Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Duzhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Liyuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tielin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1272">[1272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14183" title="Abstract">arXiv:2309.14183</a> (replaced) [<a href="/pdf/2309.14183" title="Download PDF">pdf</a>, <a href="/format/2309.14183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Species196: A One-Million Semi-supervised Dataset for Fine-grained  Species Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wei He</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Ying Nie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengcheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023 Track Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1273">[1273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14187" title="Abstract">arXiv:2309.14187</a> (replaced) [<a href="/pdf/2309.14187" title="Download PDF">pdf</a>, <a href="/ps/2309.14187" title="Download PostScript">ps</a>, <a href="/format/2309.14187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two tricks to trivialize higher-indexed families
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tesla Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item1274">[1274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14989" title="Abstract">arXiv:2309.14989</a> (replaced) [<a href="/pdf/2309.14989" title="Download PDF">pdf</a>, <a href="/format/2309.14989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tempo Adaptation in Non-stationary Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuhao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jongmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Ming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lavaei%2C+J">Javad Lavaei</a>, 
<a href="/search/cs?searchtype=author&query=Sojoudi%2C+S">Somayeh Sojoudi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages. To be published in Neural Information Processing Systems (NeurIPS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1275">[1275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15101" title="Abstract">arXiv:2309.15101</a> (replaced) [<a href="/pdf/2309.15101" title="Download PDF">pdf</a>, <a href="/format/2309.15101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Positional Encoding for Multi-Layer Perceptrons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujieda%2C+S">Shin Fujieda</a>, 
<a href="/search/cs?searchtype=author&query=Yoshimura%2C+A">Atsushi Yoshimura</a>, 
<a href="/search/cs?searchtype=author&query=Harada%2C+T">Takahiro Harada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item1276">[1276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15376" title="Abstract">arXiv:2309.15376</a> (replaced) [<a href="/pdf/2309.15376" title="Download PDF">pdf</a>, <a href="/format/2309.15376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ADGym: Design Choices for Deep Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Minqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+C">Chaochuan Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+A">Ao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Songqiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hailiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yue Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. The first three authors contribute equally. Code available at <a href="https://github.com/Minqi824/ADGym">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1277">[1277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15970" title="Abstract">arXiv:2309.15970</a> (replaced) [<a href="/pdf/2309.15970" title="Download PDF">pdf</a>, <a href="/format/2309.15970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Motion Planning via Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+A+T">An T. Le</a>, 
<a href="/search/cs?searchtype=author&query=Chalvatzaki%2C+G">Georgia Chalvatzaki</a>, 
<a href="/search/cs?searchtype=author&query=Biess%2C+A">Armin Biess</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+J">Jan Peters</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at NeurIPS 2023. Project website: <a href="https://sites.google.com/view/sinkhorn-step/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1278">[1278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16342" title="Abstract">arXiv:2309.16342</a> (replaced) [<a href="/pdf/2309.16342" title="Download PDF">pdf</a>, <a href="/format/2309.16342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toshev%2C+A+P">Artur P. Toshev</a>, 
<a href="/search/cs?searchtype=author&query=Galletti%2C+G">Gianluca Galletti</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+F">Fabian Fritz</a>, 
<a href="/search/cs?searchtype=author&query=Adami%2C+S">Stefan Adami</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+N+A">Nikolaus A. Adams</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item1279">[1279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17133" title="Abstract">arXiv:2309.17133</a> (replaced) [<a href="/pdf/2309.17133" title="Download PDF">pdf</a>, <a href="/format/2309.17133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-grained Late-interaction Multi-modal Retrieval for Retrieval  Augmented Visual Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weizhe Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinghong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+J">Jingbiao Mei</a>, 
<a href="/search/cs?searchtype=author&query=Coca%2C+A">Alexandru Coca</a>, 
<a href="/search/cs?searchtype=author&query=Byrne%2C+B">Bill Byrne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NeurIPS 2023. This is the camera-ready version. We fixed some numbers and added more experiments to address reviewers' comments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1280">[1280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17448" title="Abstract">arXiv:2309.17448</a> (replaced) [<a href="/pdf/2309.17448" title="Download PDF">pdf</a>, <a href="/format/2309.17448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhongang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wanqi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Ailing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qingping Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanjun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+H+E">Hui En Pang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Haiyi Mei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Loy%2C+C+C">Chen Change Loy</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Homepage: <a href="https://caizhongang.github.io/projects/SMPLer-X/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1281">[1281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00263" title="Abstract">arXiv:2310.00263</a> (replaced) [<a href="/pdf/2310.00263" title="Download PDF">pdf</a>, <a href="/ps/2310.00263" title="Download PostScript">ps</a>, <a href="/format/2310.00263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIS-Aided Cell-Free Massive MIMO Systems for 6G: Fundamentals, System  Design, and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+E">Enyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+B">Bo Ai</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xuemin Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1282">[1282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00402" title="Abstract">arXiv:2310.00402</a> (replaced) [<a href="/pdf/2310.00402" title="Download PDF">pdf</a>, <a href="/format/2310.00402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiskANN++: Efficient Page-based Search over Isomorphic Mapped Graph  Index using Query-sensitivity Entry Vertex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jiongkang Ni</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaoliang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Can Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiajie Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shihai Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuecang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages including references, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item1283">[1283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00431" title="Abstract">arXiv:2310.00431</a> (replaced) [<a href="/pdf/2310.00431" title="Download PDF">pdf</a>, <a href="/format/2310.00431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResolvNet: A Graph Convolutional Network with multi-scale Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koke%2C+C">Christian Koke</a>, 
<a href="/search/cs?searchtype=author&query=Saroha%2C+A">Abhishek Saroha</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yuesong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Eisenberger%2C+M">Marvin Eisenberger</a>, 
<a href="/search/cs?searchtype=author&query=Cremers%2C+D">Daniel Cremers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1284">[1284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00532" title="Abstract">arXiv:2310.00532</a> (replaced) [<a href="/pdf/2310.00532" title="Download PDF">pdf</a>, <a href="/format/2310.00532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation  and Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lin%2C+L">Licong Lin</a>, 
<a href="/search/math?searchtype=author&query=Ying%2C+M">Mufang Ying</a>, 
<a href="/search/math?searchtype=author&query=Ghosh%2C+S">Suvrojit Ghosh</a>, 
<a href="/search/math?searchtype=author&query=Khamaru%2C+K">Koulik Khamaru</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Cun-Hui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1285">[1285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01139" title="Abstract">arXiv:2310.01139</a> (replaced) [<a href="/pdf/2310.01139" title="Download PDF">pdf</a>, <a href="/ps/2310.01139" title="Download PostScript">ps</a>, <a href="/format/2310.01139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability and Generalization for Minibatch SGD and Local SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yunwen Lei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingrui Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1286">[1286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01288" title="Abstract">arXiv:2310.01288</a> (replaced) [<a href="/pdf/2310.01288" title="Download PDF">pdf</a>, <a href="/format/2310.01288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Tracking with Object Permanence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianzhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Caesar%2C+H">Holger Caesar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1287">[1287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01647" title="Abstract">arXiv:2310.01647</a> (replaced) [<a href="/pdf/2310.01647" title="Download PDF">pdf</a>, <a href="/format/2310.01647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Adaptation of Large Pretrained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+A+K">Arnab Kumar Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Panigrahi%2C+S+S">Siba Smarak Panigrahi</a>, 
<a href="/search/cs?searchtype=author&query=Kaba%2C+S">S&#xe9;kou-Oumar Kaba</a>, 
<a href="/search/cs?searchtype=author&query=Rajeswar%2C+S">Sai Rajeswar</a>, 
<a href="/search/cs?searchtype=author&query=Ravanbakhsh%2C+S">Siamak Ravanbakhsh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures. Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1288">[1288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01690" title="Abstract">arXiv:2310.01690</a> (replaced) [<a href="/pdf/2310.01690" title="Download PDF">pdf</a>, <a href="/format/2310.01690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecasting Tropical Cyclones with Cascaded Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Nath%2C+P">Pritthijit Nath</a>, 
<a href="/search/physics?searchtype=author&query=Shukla%2C+P">Pancham Shukla</a>, 
<a href="/search/physics?searchtype=author&query=Quilodr%C3%A1n-Casas%2C+C">C&#xe9;sar Quilodr&#xe1;n-Casas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1289">[1289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02230" title="Abstract">arXiv:2310.02230</a> (replaced) [<a href="/pdf/2310.02230" title="Download PDF">pdf</a>, <a href="/format/2310.02230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts  in Underspecified Visual Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scimeca%2C+L">Luca Scimeca</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+A">Alexander Rubinstein</a>, 
<a href="/search/cs?searchtype=author&query=Nicolicioiu%2C+A">Armand Nicolicioiu</a>, 
<a href="/search/cs?searchtype=author&query=Teney%2C+D">Damien Teney</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Neural Information Processing Systems(NeurIPS) 2023 - Workshop on Diffusion Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1290">[1290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02299" title="Abstract">arXiv:2310.02299</a> (replaced) [<a href="/pdf/2310.02299" title="Download PDF">pdf</a>, <a href="/format/2310.02299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in  3D Physical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Smidt%2C+T+E">Tess E.Smidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1291">[1291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02807" title="Abstract">arXiv:2310.02807</a> (replaced) [<a href="/pdf/2310.02807" title="Download PDF">pdf</a>, <a href="/format/2310.02807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Instance Generative Framework for MILP Solvers Under Limited Data  Availability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+Z">Zijie Geng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Feng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1292">[1292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03103" title="Abstract">arXiv:2310.03103</a> (replaced) [<a href="/pdf/2310.03103" title="Download PDF">pdf</a>, <a href="/format/2310.03103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Prompt Tuning for Domain-Aware Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Guoyizhe Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Anshul Shah</a>, 
<a href="/search/cs?searchtype=author&query=Chellappa%2C+R">Rama Chellappa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1293">[1293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03274" title="Abstract">arXiv:2310.03274</a> (replaced) [<a href="/pdf/2310.03274" title="Download PDF">pdf</a>, <a href="/format/2310.03274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fragment-based Pretraining and Finetuning on Molecular Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luong%2C+K">Kha-Dinh Luong</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Ambuj Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures, published in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1294">[1294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03906" title="Abstract">arXiv:2310.03906</a> (replaced) [<a href="/pdf/2310.03906" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PyDCM: Custom Data Center Models with Reinforcement Learning for  Sustainability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naug%2C+A">Avisek Naug</a>, 
<a href="/search/cs?searchtype=author&query=Guillen%2C+A">Antonio Guillen</a>, 
<a href="/search/cs?searchtype=author&query=Guti%C3%A9rrez%2C+R+L">Ricardo Luna Guti&#xe9;rrez</a>, 
<a href="/search/cs?searchtype=author&query=Gundecha%2C+V">Vineet Gundecha</a>, 
<a href="/search/cs?searchtype=author&query=Markovikj%2C+D">Dejan Markovikj</a>, 
<a href="/search/cs?searchtype=author&query=Kashyap%2C+L+D">Lekhapriya Dheeraj Kashyap</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+L">Lorenz Krause</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbanpour%2C+S">Sahand Ghorbanpour</a>, 
<a href="/search/cs?searchtype=author&query=Mousavi%2C+S">Sajad Mousavi</a>, 
<a href="/search/cs?searchtype=author&query=Babu%2C+A+R">Ashwin Ramesh Babu</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Soumyendu Sarkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation (BuildSys '23), November 15--16, 2023, Istanbul, Turkey
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1295">[1295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03912" title="Abstract">arXiv:2310.03912</a> (replaced) [<a href="/pdf/2310.03912" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RTDK-BO: High Dimensional Bayesian Optimization with Reinforced  Transformer Deep kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shmakov%2C+A">Alexander Shmakov</a>, 
<a href="/search/cs?searchtype=author&query=Naug%2C+A">Avisek Naug</a>, 
<a href="/search/cs?searchtype=author&query=Gundecha%2C+V">Vineet Gundecha</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbanpour%2C+S">Sahand Ghorbanpour</a>, 
<a href="/search/cs?searchtype=author&query=Gutierrez%2C+R+L">Ricardo Luna Gutierrez</a>, 
<a href="/search/cs?searchtype=author&query=Babu%2C+A+R">Ashwin Ramesh Babu</a>, 
<a href="/search/cs?searchtype=author&query=Guillen%2C+A">Antonio Guillen</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Soumyendu Sarkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1296">[1296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04205" title="Abstract">arXiv:2310.04205</a> (replaced) [<a href="/pdf/2310.04205" title="Download PDF">pdf</a>, <a href="/format/2310.04205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Keyword Augmented Retrieval: Novel framework for Information Retrieval  integrated with speech interface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Purwar%2C+A">Anupam Purwar</a>, 
<a href="/search/cs?searchtype=author&query=Sundar%2C+R">Rahul Sundar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1297">[1297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04678" title="Abstract">arXiv:2310.04678</a> (replaced) [<a href="/pdf/2310.04678" title="Download PDF">pdf</a>, <a href="/format/2310.04678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based  Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Naidu%2C+P">Prudhviraj Naidu</a>, 
<a href="/search/cs?searchtype=author&query=Bergen%2C+L">Leon Bergen</a>, 
<a href="/search/cs?searchtype=author&query=Paturi%2C+R">Ramamohan Paturi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1298">[1298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04986" title="Abstract">arXiv:2310.04986</a> (replaced) [<a href="/pdf/2310.04986" title="Download PDF">pdf</a>, <a href="/format/2310.04986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new economic and financial theory of money
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Glinsky%2C+M+E">Michael E. Glinsky</a>, 
<a href="/search/econ?searchtype=author&query=Sievert%2C+S">Sharon Sievert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages, 35 figures, 158 equations, to be submitted to Journal of Economic Affairs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Artificial Intelligence (cs.AI); Classical Physics (physics.class-ph)

</div>
</div>
</dd>
<dt><a name="item1299">[1299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05123" title="Abstract">arXiv:2310.05123</a> (replaced) [<a href="/pdf/2310.05123" title="Download PDF">pdf</a>, <a href="/format/2310.05123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution-Based Trajectory Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z+J">Zi Jing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Ye Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+K+M">Kai Ming Ting</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1300">[1300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05378" title="Abstract">arXiv:2310.05378</a> (replaced) [<a href="/pdf/2310.05378" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transcending the Attention Paradigm: Representation Learning from  Geospatial Social Media Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DiSanto%2C+N">Nick DiSanto</a>, 
<a href="/search/cs?searchtype=author&query=Corso%2C+A">Anthony Corso</a>, 
<a href="/search/cs?searchtype=author&query=Sanders%2C+B">Benjamin Sanders</a>, 
<a href="/search/cs?searchtype=author&query=Harding%2C+G">Gavin Harding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1301">[1301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05955" title="Abstract">arXiv:2310.05955</a> (replaced) [<a href="/pdf/2310.05955" title="Download PDF">pdf</a>, <a href="/format/2310.05955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Quality-Diversity approaches for constrained optimization  problems with mixed continuous, discrete and categorical variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Brevault%2C+L">Loic Brevault</a>, 
<a href="/search/math?searchtype=author&query=Balesdent%2C+M">Mathieu Balesdent</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1302">[1302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06003" title="Abstract">arXiv:2310.06003</a> (replaced) [<a href="/pdf/2310.06003" title="Download PDF">pdf</a>, <a href="/format/2310.06003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Memory and Communication Cost for Efficient Large Language  Model Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanxiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+L">Lin Ju</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jinjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Youshao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Huan%2C+Z">Zhaoxin Huan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fanzhuang Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Lei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaolu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1303">[1303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06182" title="Abstract">arXiv:2310.06182</a> (replaced) [<a href="/pdf/2310.06182" title="Download PDF">pdf</a>, <a href="/format/2310.06182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jiancong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z+Q">Zhi- Quan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1304">[1304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06793" title="Abstract">arXiv:2310.06793</a> (replaced) [<a href="/pdf/2310.06793" title="Download PDF">pdf</a>, <a href="/ps/2310.06793" title="Download PostScript">ps</a>, <a href="/format/2310.06793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stojanovic%2C+S">Stefan Stojanovic</a>, 
<a href="/search/cs?searchtype=author&query=Jedra%2C+Y">Yassir Jedra</a>, 
<a href="/search/cs?searchtype=author&query=Proutiere%2C+A">Alexandre Proutiere</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1305">[1305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07194" title="Abstract">arXiv:2310.07194</a> (replaced) [<a href="/pdf/2310.07194" title="Download PDF">pdf</a>, <a href="/format/2310.07194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Learning for LDPC Codes to Improve the Error-Floor Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwak%2C+H">Hee-Youl Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+D">Dae-Young Yun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yongjune Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sang-Hyo Kim</a>, 
<a href="/search/cs?searchtype=author&query=No%2C+J">Jong-Seon No</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1306">[1306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07535" title="Abstract">arXiv:2310.07535</a> (replaced) [<a href="/pdf/2310.07535" title="Download PDF">pdf</a>, <a href="/format/2310.07535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Fairness-Accuracy tradeoff with few Test Samples under  Covariate Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Havaldar%2C+S">Shreyas Havaldar</a>, 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+J">Jatin Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Shanmugam%2C+K">Karthikeyan Shanmugam</a>, 
<a href="/search/cs?searchtype=author&query=Nandy%2C+J">Jay Nandy</a>, 
<a href="/search/cs?searchtype=author&query=Raghuveer%2C+A">Aravindan Raghuveer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS Workshop on Algorithmic Fairness through the Lens of Time (AFT 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1307">[1307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08056" title="Abstract">arXiv:2310.08056</a> (replaced) [<a href="/pdf/2310.08056" title="Download PDF">pdf</a>, <a href="/format/2310.08056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Label Proportions: Bootstrapping Supervised Learners via  Belief Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Havaldar%2C+S">Shreyas Havaldar</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+N">Navodita Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Sareen%2C+S">Shubhi Sareen</a>, 
<a href="/search/cs?searchtype=author&query=Shanmugam%2C+K">Karthikeyan Shanmugam</a>, 
<a href="/search/cs?searchtype=author&query=Raghuveer%2C+A">Aravindan Raghuveer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Regulatable ML @ NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1308">[1308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08166" title="Abstract">arXiv:2310.08166</a> (replaced) [<a href="/pdf/2310.08166" title="Download PDF">pdf</a>, <a href="/format/2310.08166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ziya-Visual: Bilingual Large Vision-Language Model via Multi-Task  Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Junyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dixiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaojun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+R">Ruyi Gan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yan Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pingjian Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1309">[1309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08599" title="Abstract">arXiv:2310.08599</a> (replaced) [<a href="/pdf/2310.08599" title="Download PDF">pdf</a>, <a href="/format/2310.08599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Multi-Robot Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bui%2C+H">Hoang-Dung Bui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is my Ph.D. comprehensive exam report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1310">[1310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08767" title="Abstract">arXiv:2310.08767</a> (replaced) [<a href="/pdf/2310.08767" title="Download PDF">pdf</a>, <a href="/format/2310.08767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Fission Gas Release at the Mesoscale using Multiscale DenseNet  Regression with Attention Mechanism and Inception Blocks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Toma%2C+P">Peter Toma</a>, 
<a href="/search/cond-mat?searchtype=author&query=Muntaha%2C+M+A">Md Ali Muntaha</a>, 
<a href="/search/cond-mat?searchtype=author&query=Harley%2C+J+B">Joel B. Harley</a>, 
<a href="/search/cond-mat?searchtype=author&query=Tonks%2C+M+R">Michael R. Tonks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted at Journal of Nuclear Materials, 20 pages, 10 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mesoscale and Nanoscale Physics (cond-mat.mes-hall)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1311">[1311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08773" title="Abstract">arXiv:2310.08773</a> (replaced) [<a href="/pdf/2310.08773" title="Download PDF">pdf</a>, <a href="/format/2310.08773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining the Potential and Pitfalls of ChatGPT in Science and  Engineering Problem-Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K+D">Karen D. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Burkholder%2C+E">Eric Burkholder</a>, 
<a href="/search/cs?searchtype=author&query=Wieman%2C+C">Carl Wieman</a>, 
<a href="/search/cs?searchtype=author&query=Salehi%2C+S">Shima Salehi</a>, 
<a href="/search/cs?searchtype=author&query=Haber%2C+N">Nick Haber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item1312">[1312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08800" title="Abstract">arXiv:2310.08800</a> (replaced) [<a href="/pdf/2310.08800" title="Download PDF">pdf</a>, <a href="/format/2310.08800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DDMT: Denoising Diffusion Mask Transformer Models for Multivariate Time  Series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chaocheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tingyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xuanhui Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1313">[1313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09196" title="Abstract">arXiv:2310.09196</a> (replaced) [<a href="/pdf/2310.09196" title="Download PDF">pdf</a>, <a href="/format/2310.09196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A 4-approximation algorithm for min max correlation clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heidrich%2C+H">Holger Heidrich</a>, 
<a href="/search/cs?searchtype=author&query=Irmai%2C+J">Jannik Irmai</a>, 
<a href="/search/cs?searchtype=author&query=Andres%2C+B">Bjoern Andres</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1314">[1314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09219" title="Abstract">arXiv:2310.09219</a> (replaced) [<a href="/pdf/2310.09219" title="Download PDF">pdf</a>, <a href="/format/2310.09219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Kelly is a Warm Person, Joseph is a Role Model&quot;: Gender Biases in  LLM-Generated Reference Letters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yixin Wan</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+G">George Pu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Garimella%2C+A">Aparna Garimella</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1315">[1315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09450" title="Abstract">arXiv:2310.09450</a> (replaced) [<a href="/pdf/2310.09450" title="Download PDF">pdf</a>, <a href="/format/2310.09450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-intrusive Enforcement of Decentralized Stability Protocol for IBRs  in AC Microgrids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+T">Tong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript has been submitted to IEEE Transactions on Smart Grid
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1316">[1316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09680" title="Abstract">arXiv:2310.09680</a> (replaced) [<a href="/pdf/2310.09680" title="Download PDF">pdf</a>, <a href="/format/2310.09680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Contextual Recognition In Automatic Speech Recognition Systems  By Semantic Lattice Rescoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sudarshan%2C+A">Ankitha Sudarshan</a>, 
<a href="/search/cs?searchtype=author&query=Samuel%2C+V">Vinay Samuel</a>, 
<a href="/search/cs?searchtype=author&query=Patwa%2C+P">Parth Patwa</a>, 
<a href="/search/cs?searchtype=author&query=Amara%2C+I">Ibtihel Amara</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1317">[1317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09751" title="Abstract">arXiv:2310.09751</a> (replaced) [<a href="/pdf/2310.09751" title="Download PDF">pdf</a>, <a href="/format/2310.09751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junfeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+R">Roger Zimmermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1318">[1318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10121" title="Abstract">arXiv:2310.10121</a> (replaced) [<a href="/pdf/2310.10121" title="Download PDF">pdf</a>, <a href="/format/2310.10121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and  Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+A">Andi Han</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+D">Dai Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lequan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Junbin Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1319">[1319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10200" title="Abstract">arXiv:2310.10200</a> (replaced) [<a href="/pdf/2310.10200" title="Download PDF">pdf</a>, <a href="/ps/2310.10200" title="Download PostScript">ps</a>, <a href="/format/2310.10200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Circular External Difference Families: Construction and Non-Existence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wu%2C+H">Huawei Wu</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+J">Jing Yang</a>, 
<a href="/search/math?searchtype=author&query=Feng%2C+K">Keqin Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1320">[1320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10298" title="Abstract">arXiv:2310.10298</a> (replaced) [<a href="/pdf/2310.10298" title="Download PDF">pdf</a>, <a href="/format/2310.10298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Summary-based Whole-program Analysis to Identify Unsafe Memory  Accesses in Rust
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingshen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Criswell%2C+J">John Criswell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1321">[1321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10893" title="Abstract">arXiv:2310.10893</a> (replaced) [<a href="/pdf/2310.10893" title="Download PDF">pdf</a>, <a href="/format/2310.10893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Learning Framework for Cost-Effective TCR-Epitope Binding  Affinity Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+P">Pengfei Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Bang%2C+S">Seojin Bang</a>, 
<a href="/search/q-bio?searchtype=author&query=Lee%2C+H">Heewook Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures, this paper has been accepted for publication in the proceedings of the IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1322">[1322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10942" title="Abstract">arXiv:2310.10942</a> (replaced) [<a href="/pdf/2310.10942" title="Download PDF">pdf</a>, <a href="/format/2310.10942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UNK-VQA: A Dataset and A Probe into Multi-modal Large Models&#x27; Abstention  Ability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yanyang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+F">Fangkai Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liqiang Nie</a>, 
<a href="/search/cs?searchtype=author&query=Kankanhalli%2C+M">Mohan Kankanhalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1323">[1323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11053" title="Abstract">arXiv:2310.11053</a> (replaced) [<a href="/pdf/2310.11053" title="Download PDF">pdf</a>, <a href="/format/2310.11053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denevil: Towards Deciphering and Navigating the Ethical Values of Large  Language Models via Instruction Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+S">Shitong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+X">Xiaoyuan Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tun Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+N">Ning Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1324">[1324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11166" title="Abstract">arXiv:2310.11166</a> (replaced) [<a href="/pdf/2310.11166" title="Download PDF">pdf</a>, <a href="/format/2310.11166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text  Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q">Quoc-Nam Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+T+C">Thang Chau Phan</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Duc-Vu Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Van+Nguyen%2C+K">Kiet Van Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP'2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1325">[1325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11448" title="Abstract">arXiv:2310.11448</a> (replaced) [<a href="/pdf/2310.11448" title="Download PDF">pdf</a>, <a href="/format/2310.11448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4K4D: Real-Time 4D View Synthesis at 4K Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Sida Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haotong Lin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guangzhao He</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiaming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yujun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Hujun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaowei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://zju3dv.github.io/4k4d">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1326">[1326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11638" title="Abstract">arXiv:2310.11638</a> (replaced) [<a href="/pdf/2310.11638" title="Download PDF">pdf</a>, <a href="/format/2310.11638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Assessment of Factual Knowledge in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Linhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Thuy-Trang Vu</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1327">[1327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11651" title="Abstract">arXiv:2310.11651</a> (replaced) [<a href="/pdf/2310.11651" title="Download PDF">pdf</a>, <a href="/format/2310.11651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> US Microelectronics Packaging Ecosystem: Challenges and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Noor%2C+R">Rouhan Noor</a>, 
<a href="/search/eess?searchtype=author&query=Kottur%2C+H+R">Himanandhan Reddy Kottur</a>, 
<a href="/search/eess?searchtype=author&query=Craig%2C+P+J">Patrick J Craig</a>, 
<a href="/search/eess?searchtype=author&query=Biswas%2C+L+K">Liton Kumar Biswas</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+M+S+M">M Shafkat M Khan</a>, 
<a href="/search/eess?searchtype=author&query=Varshney%2C+N">Nitin Varshney</a>, 
<a href="/search/eess?searchtype=author&query=Dalir%2C+H">Hamed Dalir</a>, 
<a href="/search/eess?searchtype=author&query=Ak%C3%A7al%C4%B1%2C+E">Elif Ak&#xe7;al&#x131;</a>, 
<a href="/search/eess?searchtype=author&query=Motlagh%2C+B+G">Bahareh Ghane Motlagh</a>, 
<a href="/search/eess?searchtype=author&query=Woychik%2C+C">Charles Woychik</a>, 
<a href="/search/eess?searchtype=author&query=Yoon%2C+Y">Yong-Kyu Yoon</a>, 
<a href="/search/eess?searchtype=author&query=Asadizanjani%2C+N">Navid Asadizanjani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1328">[1328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11778" title="Abstract">arXiv:2310.11778</a> (replaced) [<a href="/pdf/2310.11778" title="Download PDF">pdf</a>, <a href="/format/2310.11778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Agents for Detecting Implicit Stereotypes in Text-to-image  Models at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+T">Tian Bian</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yian Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tingyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H+M">Helen M. Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bingzhe Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1329">[1329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12182" title="Abstract">arXiv:2310.12182</a> (replaced) [<a href="/pdf/2310.12182" title="Download PDF">pdf</a>, <a href="/format/2310.12182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Block-Wise Mixed-Precision Quantization: Enabling High Efficiency for  Practical ReRAM-based DNN Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xueying Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hanson%2C+E">Edward Hanson</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nansu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qilin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoxuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huanrui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+F">Feng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Pande%2C+P+P">Partha Pratim Pande</a>, 
<a href="/search/cs?searchtype=author&query=Doppa%2C+J+R">Janardhan Rao Doppa</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+K">Krishnendu Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hai Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item1330">[1330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12389" title="Abstract">arXiv:2310.12389</a> (replaced) [<a href="/pdf/2310.12389" title="Download PDF">pdf</a>, <a href="/format/2310.12389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Computing for MIMO Beam Selection Problem: Model and Optical  Experimental Solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chengkang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+S">Shuai Hou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Chunfeng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jingwei Wen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiaqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+C">Chongyu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hai Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+K">Kai Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Globecom 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item1331">[1331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12450" title="Abstract">arXiv:2310.12450</a> (replaced) [<a href="/pdf/2310.12450" title="Download PDF">pdf</a>, <a href="/format/2310.12450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Read-and-Select Framework for Zero-shot Entity Linking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yulin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1332">[1332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12455" title="Abstract">arXiv:2310.12455</a> (replaced) [<a href="/pdf/2310.12455" title="Download PDF">pdf</a>, <a href="/format/2310.12455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto Search Indexer for End-to-End Document Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianchi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Minghui Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haizhen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Feng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1333">[1333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12563" title="Abstract">arXiv:2310.12563</a> (replaced) [<a href="/pdf/2310.12563" title="Download PDF">pdf</a>, <a href="/format/2310.12563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate information maximization for bandit games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Barbier-Chebbah%2C+A">Alex Barbier-Chebbah</a> (IP, CNRS, UPCit&#xe9;), 
<a href="/search/stat?searchtype=author&query=Vestergaard%2C+C+L">Christian L. Vestergaard</a> (IP, CNRS, UPCit&#xe9;), 
<a href="/search/stat?searchtype=author&query=Masson%2C+J">Jean-Baptiste Masson</a> (IP, CNRS, UPCit&#xe9;), 
<a href="/search/stat?searchtype=author&query=Boursier%2C+E">Etienne Boursier</a> (INRIA Saclay)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1334">[1334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12671" title="Abstract">arXiv:2310.12671</a> (replaced) [<a href="/pdf/2310.12671" title="Download PDF">pdf</a>, <a href="/format/2310.12671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural networks for insurance pricing with frequency and severity data:  a benchmark study from data preprocessing to technical tariff
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holvoet%2C+F">Freek Holvoet</a>, 
<a href="/search/cs?searchtype=author&query=Antonio%2C+K">Katrien Antonio</a>, 
<a href="/search/cs?searchtype=author&query=Henckaerts%2C+R">Roel Henckaerts</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Risk Management (q-fin.RM)

</div>
</div>
</dd>
<dt><a name="item1335">[1335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12821" title="Abstract">arXiv:2310.12821</a> (replaced) [<a href="/pdf/2310.12821" title="Download PDF">pdf</a>, <a href="/format/2310.12821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding  with Large Language Model Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xin Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tengxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shengdong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiqiang Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1336">[1336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12860" title="Abstract">arXiv:2310.12860</a> (replaced) [<a href="/pdf/2310.12860" title="Download PDF">pdf</a>, <a href="/format/2310.12860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing LLMs for hate speech detection: strengths and vulnerabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Sarthak Roy</a>, 
<a href="/search/cs?searchtype=author&query=Harshavardhan%2C+A">Ashish Harshavardhan</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+P">Punyajoy Saha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures, 7 tables, accepted to findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1337">[1337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12999" title="Abstract">arXiv:2310.12999</a> (replaced) [<a href="/pdf/2310.12999" title="Download PDF">pdf</a>, <a href="/format/2310.12999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Dynamic Programming for Energy-Efficient Base Station Cell  Switching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Junliang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y+T">Yi Tian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jenkin%2C+M">Michael Jenkin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dudek%2C+G">Gregory Dudek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1338">[1338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13019" title="Abstract">arXiv:2310.13019</a> (replaced) [<a href="/pdf/2310.13019" title="Download PDF">pdf</a>, <a href="/format/2310.13019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class  Manipulation Using DeepFool Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Labib%2C+S+M+F+R">S. M. Fazle Rabby Labib</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+J+J">Joyanta Jyoti Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Manab%2C+M+A">Meem Arafat Manab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1339">[1339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13268" title="Abstract">arXiv:2310.13268</a> (replaced) [<a href="/pdf/2310.13268" title="Download PDF">pdf</a>, <a href="/format/2310.13268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model  Statistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kaiwen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1340">[1340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13388" title="Abstract">arXiv:2310.13388</a> (replaced) [<a href="/pdf/2310.13388" title="Download PDF">pdf</a>, <a href="/format/2310.13388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Music Augmentation and Denoising For Peak-Based Audio Fingerprinting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akesbi%2C+K">Kamil Akesbi</a>, 
<a href="/search/cs?searchtype=author&query=Desblancs%2C+D">Dorian Desblancs</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+B">Benjamin Martin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1341">[1341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13499" title="Abstract">arXiv:2310.13499</a> (replaced) [<a href="/pdf/2310.13499" title="Download PDF">pdf</a>, <a href="/format/2310.13499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DistillCSE: Distilled Contrastive Learning for Sentence Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiahao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wei Shao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lihui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lemao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1342">[1342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13571" title="Abstract">arXiv:2310.13571</a> (replaced) [<a href="/pdf/2310.13571" title="Download PDF">pdf</a>, <a href="/ps/2310.13571" title="Download PostScript">ps</a>, <a href="/format/2310.13571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why Can Large Language Models Generate Correct Chain-of-Thoughts?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tutunov%2C+R">Rasul Tutunov</a>, 
<a href="/search/cs?searchtype=author&query=Grosnit%2C+A">Antoine Grosnit</a>, 
<a href="/search/cs?searchtype=author&query=Ziomek%2C+J">Juliusz Ziomek</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bou-Ammar%2C+H">Haitham Bou-Ammar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1343">[1343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13960" title="Abstract">arXiv:2310.13960</a> (replaced) [<a href="/pdf/2310.13960" title="Download PDF">pdf</a>, <a href="/format/2310.13960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linguistically Motivated Sign Language Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moryossef%2C+A">Amit Moryossef</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zifan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+M">Mathias M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Ebling%2C+S">Sarah Ebling</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1344">[1344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13981" title="Abstract">arXiv:2310.13981</a> (replaced) [<a href="/pdf/2310.13981" title="Download PDF">pdf</a>, <a href="/ps/2310.13981" title="Download PostScript">ps</a>, <a href="/format/2310.13981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Filling the Missing: Exploring Generative AI for Enhanced Federated  Learning over Heterogeneous Mobile Edge Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peichun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+L">Liping Qian</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xuemin Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures. Submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1345">[1345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14009" title="Abstract">arXiv:2310.14009</a> (replaced) [<a href="/pdf/2310.14009" title="Download PDF">pdf</a>, <a href="/format/2310.14009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One is More: Diverse Perspectives within a Single Network for Efficient  DRL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Y">Yiqin Tan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Longbo Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1346">[1346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14017" title="Abstract">arXiv:2310.14017</a> (replaced) [<a href="/pdf/2310.14017" title="Download PDF">pdf</a>, <a href="/format/2310.14017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrast Everything: A Hierarchical Contrastive Framework for Medical  Time-Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yu Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haishuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023; 24pages (13 pages main paper + 11 pages supplementary materials)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1347">[1347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14073" title="Abstract">arXiv:2310.14073</a> (replaced) [<a href="/pdf/2310.14073" title="Download PDF">pdf</a>, <a href="/format/2310.14073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact asymptotic estimation of unknown parameters of regression  equations with additive perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Glushchenko%2C+A">Anton Glushchenko</a>, 
<a href="/search/eess?searchtype=author&query=Lastochkin%2C+K">Konstantin Lastochkin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item1348">[1348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14128" title="Abstract">arXiv:2310.14128</a> (replaced) [<a href="/pdf/2310.14128" title="Download PDF">pdf</a>, <a href="/format/2310.14128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Smooth Sliding Control Applied to UAV Trajectory Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Peixoto%2C+A+J">Alessandro Jacoud Peixoto</a>, 
<a href="/search/eess?searchtype=author&query=Serrantola%2C+W+G">Wenderson G. Serrantola</a>, 
<a href="/search/eess?searchtype=author&query=Lizarralde%2C+F">Fernando Lizarralde</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1349">[1349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14194" title="Abstract">arXiv:2310.14194</a> (replaced) [<a href="/pdf/2310.14194" title="Download PDF">pdf</a>, <a href="/format/2310.14194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distractor-aware Event-based Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yingkai Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Baocai Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiaopeng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1350">[1350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14441" title="Abstract">arXiv:2310.14441</a> (replaced) [<a href="/pdf/2310.14441" title="Download PDF">pdf</a>, <a href="/format/2310.14441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EDGE++: Improved Training and Sampling of EDGE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mingyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaohui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Li-Ping Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1351">[1351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14633" title="Abstract">arXiv:2310.14633</a> (replaced) [<a href="/pdf/2310.14633" title="Download PDF">pdf</a>, <a href="/format/2310.14633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending Input Contexts of Language Models through Training on  Segmented Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karypis%2C+P">Petros Karypis</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1352">[1352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14736" title="Abstract">arXiv:2310.14736</a> (replaced) [<a href="/pdf/2310.14736" title="Download PDF">pdf</a>, <a href="/format/2310.14736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAMCLR: Contrastive pre-training on complex scenes using SAM for view  sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Missaoui%2C+B">Benjamin Missaoui</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chongbin Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 Workshop on SSL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1353">[1353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14784" title="Abstract">arXiv:2310.14784</a> (replaced) [<a href="/pdf/2310.14784" title="Download PDF">pdf</a>, <a href="/format/2310.14784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Imbalance-Aware Federated Learning Approach for Wearable  Healthcare with Autoregressive Ratio Observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Wenhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">He Li</a>, 
<a href="/search/cs?searchtype=author&query=Ota%2C+K">Kaoru Ota</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+M">Mianxiong Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE OJCS in Oct. 2023, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1354">[1354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14816" title="Abstract">arXiv:2310.14816</a> (replaced) [<a href="/pdf/2310.14816" title="Download PDF">pdf</a>, <a href="/format/2310.14816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Multi-Level Replanning TAMP Framework for Dynamic  Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+C">Chengfei Yue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xibin Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1355">[1355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14954" title="Abstract">arXiv:2310.14954</a> (replaced) [<a href="/pdf/2310.14954" title="Download PDF">pdf</a>, <a href="/format/2310.14954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Key Frame Mechanism For Efficient Conformer Based End-to-end Speech  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+P">Peng Fan</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+C">Changhao Shan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Sining Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianwei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript has been accepted by IEEE Signal Processing Letters for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1356">[1356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15080" title="Abstract">arXiv:2310.15080</a> (replaced) [<a href="/pdf/2310.15080" title="Download PDF">pdf</a>, <a href="/format/2310.15080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning of Large Language Models with Parameter-Efficient  Prompt Tuning and Adaptive Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Che%2C+T">Tianshi Che</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Ji Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiaxiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiwen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+V+S">Victor S. Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Huaiyu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+D">Dejing Dou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1357">[1357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15161" title="Abstract">arXiv:2310.15161</a> (replaced) [<a href="/pdf/2310.15161" title="Download PDF">pdf</a>, <a href="/format/2310.15161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAM-Med3D
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Sizheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhongying Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Junlong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianpin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yanzhou Su</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Ziyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yiqing Shen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+B">Bin Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaoting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junjun He</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1358">[1358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15342" title="Abstract">arXiv:2310.15342</a> (replaced) [<a href="/pdf/2310.15342" title="Download PDF">pdf</a>, <a href="/format/2310.15342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Hybrid-grained Feature Interaction Selection for Deep Sparse  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+F">Fuyuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dugang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Weihong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiuqiang He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1359">[1359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15368" title="Abstract">arXiv:2310.15368</a> (replaced) [<a href="/pdf/2310.15368" title="Download PDF">pdf</a>, <a href="/format/2310.15368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Integrated Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barkan%2C+O">Oren Barkan</a>, 
<a href="/search/cs?searchtype=author&query=Elisha%2C+Y">Yehonatan Elisha</a>, 
<a href="/search/cs?searchtype=author&query=Weill%2C+J">Jonathan Weill</a>, 
<a href="/search/cs?searchtype=author&query=Asher%2C+Y">Yuval Asher</a>, 
<a href="/search/cs?searchtype=author&query=Eshel%2C+A">Amit Eshel</a>, 
<a href="/search/cs?searchtype=author&query=Koenigstein%2C+N">Noam Koenigstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1360">[1360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15457" title="Abstract">arXiv:2310.15457</a> (replaced) [<a href="/pdf/2310.15457" title="Download PDF">pdf</a>, <a href="/ps/2310.15457" title="Download PostScript">ps</a>, <a href="/format/2310.15457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Unconditionally Stable Iterative Decoupled Algorithm for  Multiple-Network Poroelasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lei%2C+M">Meng Lei</a>, 
<a href="/search/math?searchtype=author&query=Cai%2C+M">Mingchao Cai</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+F">Feng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1361">[1361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15478" title="Abstract">arXiv:2310.15478</a> (replaced) [<a href="/pdf/2310.15478" title="Download PDF">pdf</a>, <a href="/format/2310.15478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Train Your Neural Control Barrier Function: Learning Safety  Filters for Complex Input-Constrained Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=So%2C+O">Oswin So</a>, 
<a href="/search/math?searchtype=author&query=Serlin%2C+Z">Zachary Serlin</a>, 
<a href="/search/math?searchtype=author&query=Mann%2C+M">Makai Mann</a>, 
<a href="/search/math?searchtype=author&query=Gonzales%2C+J">Jake Gonzales</a>, 
<a href="/search/math?searchtype=author&query=Rutledge%2C+K">Kwesi Rutledge</a>, 
<a href="/search/math?searchtype=author&query=Roy%2C+N">Nicholas Roy</a>, 
<a href="/search/math?searchtype=author&query=Fan%2C+C">Chuchu Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024. Project page can be found at <a href="https://mit-realm.github.io/pncbf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1362">[1362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15495" title="Abstract">arXiv:2310.15495</a> (replaced) [<a href="/pdf/2310.15495" title="Download PDF">pdf</a>, <a href="/format/2310.15495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMG: Automated Efficient Approximate Multiplier Generator for FPGAs via  Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingli Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2023 IEEE International Conference on Field-Programmable Technology (ICFPT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item1363">[1363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15694" title="Abstract">arXiv:2310.15694</a> (replaced) [<a href="/pdf/2310.15694" title="Download PDF">pdf</a>, <a href="/format/2310.15694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COPF: Continual Learning Human Preference through Optimal Policy Fitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yuanzhao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1364">[1364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15699" title="Abstract">arXiv:2310.15699</a> (replaced) [<a href="/pdf/2310.15699" title="Download PDF">pdf</a>, <a href="/format/2310.15699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DACOOP-A: Decentralized Adaptive Cooperative Pursuit via Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dengyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Wei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianjiang Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages; This manuscript has been accepted by IEEE Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1365">[1365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15777" title="Abstract">arXiv:2310.15777</a> (replaced) [<a href="/pdf/2310.15777" title="Download PDF">pdf</a>, <a href="/format/2310.15777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MindLLM: Pre-training Lightweight Large Language Model from Scratch,  Evaluations and Domain Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yizhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huashan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Runheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1366">[1366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15846" title="Abstract">arXiv:2310.15846</a> (replaced) [<a href="/pdf/2310.15846" title="Download PDF">pdf</a>, <a href="/format/2310.15846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Spatial-Temporal Triangulation for Bearing-Only Cooperative  Motion Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Canlun Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+Y">Yize Mi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hanqing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaben Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhiyun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shiyu Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1367">[1367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16105" title="Abstract">arXiv:2310.16105</a> (replaced) [<a href="/pdf/2310.16105" title="Download PDF">pdf</a>, <a href="/format/2310.16105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally Differentially Private Gradient Tracking for Distributed Online  Learning over Directed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziqin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongqiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1368">[1368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16137" title="Abstract">arXiv:2310.16137</a> (replaced) [<a href="/pdf/2310.16137" title="Download PDF">pdf</a>, <a href="/format/2310.16137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Codebook-based Uplink Transmission Enhancement in 5G Advanced: Sub-band  Precoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Liu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Shabara%2C+Y">Yahia Shabara</a>, 
<a href="/search/cs?searchtype=author&query=Cheraghi%2C+P">Parisa Cheraghi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted by IEEE VCC 2023. 5 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1369">[1369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16401" title="Abstract">arXiv:2310.16401</a> (replaced) [<a href="/pdf/2310.16401" title="Download PDF">pdf</a>, <a href="/format/2310.16401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks with a Distribution of Parametrized Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+H">See Hian Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+F">Feng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+K">Kelin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Tay%2C+W+P">Wee Peng Tay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1370">[1370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16606" title="Abstract">arXiv:2310.16606</a> (replaced) [<a href="/pdf/2310.16606" title="Download PDF">pdf</a>, <a href="/ps/2310.16606" title="Download PostScript">ps</a>, <a href="/format/2310.16606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AirFL-Mem: Improving Communication-Learning Trade-Off by Long-Term  Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Haifeng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+H">Hong Xing</a>, 
<a href="/search/cs?searchtype=author&query=Simeone%2C+O">Osvaldo Simeone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, submitted for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1371">[1371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16754" title="Abstract">arXiv:2310.16754</a> (replaced) [<a href="/pdf/2310.16754" title="Download PDF">pdf</a>, <a href="/format/2310.16754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAD -- Contextual Multi-modal Alignment for Dynamic AVQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nadeem%2C+A">Asmar Nadeem</a>, 
<a href="/search/cs?searchtype=author&query=Hilton%2C+A">Adrian Hilton</a>, 
<a href="/search/cs?searchtype=author&query=Dawes%2C+R">Robert Dawes</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+G">Graham Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+A">Armin Mustafa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1372">[1372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16787" title="Abstract">arXiv:2310.16787</a> (replaced) [<a href="/pdf/2310.16787" title="Download PDF">pdf</a>, <a href="/format/2310.16787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing  &amp; Attribution in AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Longpre%2C+S">Shayne Longpre</a>, 
<a href="/search/cs?searchtype=author&query=Mahari%2C+R">Robert Mahari</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Anthony Chen</a>, 
<a href="/search/cs?searchtype=author&query=Obeng-Marnu%2C+N">Naana Obeng-Marnu</a>, 
<a href="/search/cs?searchtype=author&query=Sileo%2C+D">Damien Sileo</a>, 
<a href="/search/cs?searchtype=author&query=Brannon%2C+W">William Brannon</a>, 
<a href="/search/cs?searchtype=author&query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
<a href="/search/cs?searchtype=author&query=Khazam%2C+N">Nathan Khazam</a>, 
<a href="/search/cs?searchtype=author&query=Kabbara%2C+J">Jad Kabbara</a>, 
<a href="/search/cs?searchtype=author&query=Perisetla%2C+K">Kartik Perisetla</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xinyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shippole%2C+E">Enrico Shippole</a>, 
<a href="/search/cs?searchtype=author&query=Bollacker%2C+K">Kurt Bollacker</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongshuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Villa%2C+L">Luis Villa</a>, 
<a href="/search/cs?searchtype=author&query=Pentland%2C+S">Sandy Pentland</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+D">Deb Roy</a>, 
<a href="/search/cs?searchtype=author&query=Hooker%2C+S">Sara Hooker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages (18 main), 6 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1373">[1373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16809" title="Abstract">arXiv:2310.16809</a> (replaced) [<a href="/pdf/2310.16809" title="Download PDF">pdf</a>, <a href="/format/2310.16809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and  In-depth Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yongxin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Dezhi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Wenhui Liao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zening Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chongyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Lianwen Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1374">[1374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16831" title="Abstract">arXiv:2310.16831</a> (replaced) [<a href="/pdf/2310.16831" title="Download PDF">pdf</a>, <a href="/format/2310.16831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PERF: Panoramic Neural Radiance Field from a Single Panorama
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guangcong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaoxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Loy%2C+C+C">Chen Change Loy</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://perf-project.github.io/">this https URL</a> , Code: <a href="https://github.com/perf-project/PeRF">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1375">[1375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16837" title="Abstract">arXiv:2310.16837</a> (replaced) [<a href="/e-print/2310.16837" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RDBench: ML Benchmark for Relational Databases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zizhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+L">Lutong Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">He Wen</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tao Feng</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+J">Jiaxuan You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Withdrawn by the authors to avoid conflict of interests
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1376">[1376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16936" title="Abstract">arXiv:2310.16936</a> (replaced) [<a href="/pdf/2310.16936" title="Download PDF">pdf</a>, <a href="/format/2310.16936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnosing Alzheimer&#x27;s Disease using Early-Late Multimodal Data Fusion  with Jacobian Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+Y">Yasmine Mustafa</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tie Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in Proceedings of 2023 IEEE Healthcom, December 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1377">[1377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16999" title="Abstract">arXiv:2310.16999</a> (replaced) [<a href="/pdf/2310.16999" title="Download PDF">pdf</a>, <a href="/format/2310.16999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust, but Verify: Robust Image Segmentation using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaman%2C+F+A">Fahim Ahmed Zaman</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaodong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sonka%2C+M">Milan Sonka</a>, 
<a href="/search/cs?searchtype=author&query=Mudumbai%2C+R">Raghuraman Mudumbai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 Pages, 8 Figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1378">[1378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17015" title="Abstract">arXiv:2310.17015</a> (replaced) [<a href="/pdf/2310.17015" title="Download PDF">pdf</a>, <a href="/format/2310.17015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation for Emotion Detection in Small Imbalanced Text Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koufakou%2C+A">Anna Koufakou</a>, 
<a href="/search/cs?searchtype=author&query=Grisales%2C+D">Diego Grisales</a>, 
<a href="/search/cs?searchtype=author&query=de+jesus%2C+R+C">Ragy Costa de jesus</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+O">Oscar Fox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the Proceedings of the 22nd IEEE International Conference on Machine Learning Applications (ICMLA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1379">[1379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17185" title="Abstract">arXiv:2310.17185</a> (replaced) [<a href="/pdf/2310.17185" title="Download PDF">pdf</a>, <a href="/format/2310.17185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive importance sampling for Deep Ritz
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaoliang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuancheng Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1380">[1380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17228" title="Abstract">arXiv:2310.17228</a> (replaced) [<a href="/pdf/2310.17228" title="Download PDF">pdf</a>, <a href="/format/2310.17228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khatry%2C+A">Anirudh Khatry</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Priyanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Vu Le</a>, 
<a href="/search/cs?searchtype=author&query=Singha%2C+A">Ananya Singha</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mukul Singh</a>, 
<a href="/search/cs?searchtype=author&query=Verbruggen%2C+G">Gust Verbruggen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for EMNLP-Findings, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1381">[1381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17488" title="Abstract">arXiv:2310.17488</a> (replaced) [<a href="/pdf/2310.17488" title="Download PDF">pdf</a>, <a href="/format/2310.17488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightLM: A Lightweight Deep and Narrow Language Model for Generative  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+K">Kai Mei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1382">[1382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17493" title="Abstract">arXiv:2310.17493</a> (replaced) [<a href="/pdf/2310.17493" title="Download PDF">pdf</a>, <a href="/format/2310.17493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Graph Network for Complex Activity Detection in Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Teeti%2C+I">Izzeddin Teeti</a>, 
<a href="/search/cs?searchtype=author&query=Bradley%2C+A">Andrew Bradley</a>, 
<a href="/search/cs?searchtype=author&query=Elhoseiny%2C+M">Mohamed Elhoseiny</a>, 
<a href="/search/cs?searchtype=author&query=Cuzzolin%2C+F">Fabio Cuzzolin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1383">[1383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17496" title="Abstract">arXiv:2310.17496</a> (replaced) [<a href="/pdf/2310.17496" title="Download PDF">pdf</a>, <a href="/format/2310.17496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Interference Induced by Data Training Loops in A/B Tests: A  Weighted Training Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Si%2C+N">Nian Si</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Econometrics (econ.EM)

</div>
</div>
</dd>
<dt><a name="item1384">[1384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17796" title="Abstract">arXiv:2310.17796</a> (replaced) [<a href="/pdf/2310.17796" title="Download PDF">pdf</a>, <a href="/format/2310.17796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ControlLLM: Augment Language Models with Tools by Searching on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaoyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zeqiang Lai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangwei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+E">Erfei Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xizhou Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lewei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Jifeng Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1385">[1385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17952" title="Abstract">arXiv:2310.17952</a> (replaced) [<a href="/pdf/2310.17952" title="Download PDF">pdf</a>, <a href="/format/2310.17952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shape-centered Representation Learning for Visible-Infrared Person  Re-identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+J">Jiaxu Leng</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+J">Ji Gan</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+M">Mengjingcheng Mo</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinbo Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1386">[1386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17966" title="Abstract">arXiv:2310.17966</a> (replaced) [<a href="/pdf/2310.17966" title="Download PDF">pdf</a>, <a href="/format/2310.17966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qisen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jiawei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M+G">Matthieu Gaetan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Liwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+N">Ning Jia</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shiji Song</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight. 24 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1387">[1387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17976" title="Abstract">arXiv:2310.17976</a> (replaced) [<a href="/pdf/2310.17976" title="Download PDF">pdf</a>, <a href="/format/2310.17976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Role-Playing Chatbots Capture the Character Personalities?  Assessing Personality Traits for Role-Playing Chatbots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Q">Quan Tu</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+Y">Yaying Fei</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+Z">Ziang Leng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A Personality Traits Test Over ChatHaruhi
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1388">[1388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18021" title="Abstract">arXiv:2310.18021</a> (replaced) [<a href="/pdf/2310.18021" title="Download PDF">pdf</a>, <a href="/format/2310.18021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FormalGeo: The First Step Toward Human-like IMO-level Geometric  Automated Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaokai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+N">Na Zhu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yiming He</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jia Zou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qike Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaoxiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yanjun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+C">Chenyang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhe Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+D">Dengfeng Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fangzhen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yiwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Cheng Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhenbing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shaorong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiangfeng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+T">Tuo Leng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1389">[1389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18075" title="Abstract">arXiv:2310.18075</a> (replaced) [<a href="/pdf/2310.18075" title="Download PDF">pdf</a>, <a href="/format/2310.18075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xiaoyu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liangyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Na Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+W">Wei Zou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kaijiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+M">Ming Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1390">[1390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18168" title="Abstract">arXiv:2310.18168</a> (replaced) [<a href="/pdf/2310.18168" title="Download PDF">pdf</a>, <a href="/format/2310.18168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personas as a Way to Model Truthfulness in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joshi%2C+N">Nitish Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Rando%2C+J">Javier Rando</a>, 
<a href="/search/cs?searchtype=author&query=Saparov%2C+A">Abulhair Saparov</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N">Najoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">He He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1391">[1391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18235" title="Abstract">arXiv:2310.18235</a> (replaced) [<a href="/pdf/2310.18235" title="Download PDF">pdf</a>, <a href="/format/2310.18235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Davidsonian Scene Graph: Improving Reliability in Fine-grained  Evaluation for Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaemin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yushi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+R">Roopal Garg</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+P">Peter Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Baldridge%2C+J">Jason Baldridge</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Pont-Tuset%2C+J">Jordi Pont-Tuset</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Su Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://google.github.io/dsg">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1392">[1392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18237" title="Abstract">arXiv:2310.18237</a> (replaced) [<a href="/e-print/2310.18237" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI Model for Artistic Style Transfer Using Convolutional  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miah%2C+J">Jonayet Miah</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+D+M">Duc M Cao</a>, 
<a href="/search/cs?searchtype=author&query=Sayed%2C+M+A">Md Abu Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Haque%2C+M+S">Md. Sabbirul Haque</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Incorrectly Input
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1393">[1393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18288" title="Abstract">arXiv:2310.18288</a> (replaced) [<a href="/pdf/2310.18288" title="Download PDF">pdf</a>, <a href="/format/2310.18288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sustainable Concrete via Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ament%2C+S">Sebastian Ament</a>, 
<a href="/search/cs?searchtype=author&query=Witte%2C+A">Andrew Witte</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+N">Nishant Garg</a>, 
<a href="/search/cs?searchtype=author&query=Kusuma%2C+J">Julius Kusuma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item1394">[1394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18297" title="Abstract">arXiv:2310.18297</a> (replaced) [<a href="/pdf/2310.18297" title="Download PDF">pdf</a>, <a href="/format/2310.18297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Clustering Conditioned on Text Criteria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+S">Sehyun Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaeseung Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minkyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaewoong Cho</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+E+K">Ernest K. Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kangwook Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item684">Cross-lists</a></li>
<li><a href="#item762">Replacements</a></li>
</ul>
<small>[ total of 1394 entries:  <b>1-1394</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
