<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Fri 13 Oct 23  to  Mon 16 Oct 23, announced Tue, 17 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item631">Cross-lists</a></li>
<li><a href="#item703">Replacements</a></li>
</ul>
<small>[ total of 1138 entries:  <b>1-1138</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Tue, 17 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09297" title="Abstract">arXiv:2310.09297</a> [<a href="/pdf/2310.09297" title="Download PDF">pdf</a>, <a href="/format/2310.09297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding AI Cognition: A Neural Module for Inference Inspired by  Human Memory Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xiangyu Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jie Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+P">Piao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruizheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhicheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">How humans and machines make sense of current inputs for relation reasoning
and question-answering while putting the perceived information into context of
our past memories, has been a challenging conundrum in cognitive science and
artificial intelligence. Inspired by human brain's memory system and cognitive
architectures, we propose a PMI framework that consists of perception, memory
and inference components. Notably, the memory module comprises working and
long-term memory, with the latter endowed with a higher-order structure to
retain more accumulated knowledge and experiences. Through a differentiable
competitive write access, current perceptions update working memory, which is
later merged with long-term memory via outer product associations, averting
memory overflow and minimizing information conflicts. In the inference module,
relevant information is retrieved from two separate memory origins and
associatively integrated to attain a more comprehensive and precise
interpretation of current perceptions. We exploratively apply our PMI to
improve prevailing Transformers and CNN models on question-answering tasks like
bAbI-20k and Sort-of-CLEVR datasets, as well as relation calculation and image
classification tasks, and in each case, our PMI enhancements consistently
outshine their original counterparts significantly. Visualization analyses
reveal that memory consolidation, along with the interaction and integration of
information from diverse memory sources, substantially contributes to the model
effectiveness on inference tasks.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09298" title="Abstract">arXiv:2310.09298</a> [<a href="/pdf/2310.09298" title="Download PDF">pdf</a>, <a href="/format/2310.09298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency  for Grayscale Image-based Network Intrusion Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+I">Irfan Khan</a>, 
<a href="/search/cs?searchtype=author&query=Farrukh%2C+Y+A">Yasir Ali Farrukh</a>, 
<a href="/search/cs?searchtype=author&query=Wali%2C+S">Syed Wali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the ever-evolving realm of network security, the swift and accurate
identification of diverse attack classes within network traffic is of paramount
importance. This paper introduces "ByteStack-ID," a pioneering approach
tailored for packet-level intrusion detection. At its core, ByteStack-ID
leverages grayscale images generated from the frequency distributions of
payload data, a groundbreaking technique that greatly enhances the model's
ability to discern intricate data patterns. Notably, our approach is
exclusively grounded in packet-level information, a departure from conventional
Network Intrusion Detection Systems (NIDS) that predominantly rely on
flow-based data. While building upon the fundamental concept of stacking
methodology, ByteStack-ID diverges from traditional stacking approaches. It
seamlessly integrates additional meta learner layers into the concatenated base
learners, creating a highly optimized, unified model. Empirical results
unequivocally confirm the outstanding effectiveness of the ByteStack-ID
framework, consistently outperforming baseline models and state-of-the-art
approaches across pivotal performance metrics, including precision, recall, and
F1-score. Impressively, our proposed approach achieves an exceptional 81\%
macro F1-score in multiclass classification tasks. In a landscape marked by the
continuous evolution of network threats, ByteStack-ID emerges as a robust and
versatile security solution, relying solely on packet-level information
extracted from network traffic data.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09299" title="Abstract">arXiv:2310.09299</a> [<a href="/pdf/2310.09299" title="Download PDF">pdf</a>, <a href="/format/2310.09299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Twin Assisted Deep Reinforcement Learning for Online  Optimization of Network Slicing Admission Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+Z">Zhenyu Tao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+X">Xiaohu You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">The proliferation of diverse network services in 5G and beyond networks has
led to the emergence of network slicing technologies. Among these, admission
control plays a crucial role in achieving specific optimization goals through
the selective acceptance of service requests. Although Deep Reinforcement
Learning (DRL) forms the foundation in many admission control approaches for
its effectiveness and flexibility, the initial instability of DRL models
hinders their practical deployment in real-world networks. In this work, we
propose a digital twin (DT) assisted DRL solution to address this issue.
Specifically, we first formulate the admission decision-making process as a
semi-Markov decision process, which is subsequently simplified into an
equivalent discrete-time Markov decision process to facilitate the
implementation of DRL methods. The DT is established through supervised
learning and employed to assist the training phase of the DRL model. Extensive
simulations show that the DT-assisted DRL model increased resource utilization
by over 40\% compared to the directly trained state-of-the-art Dueling-DQN and
over 20\% compared to our directly trained DRL model during initial training.
This improvement is achieved while preserving the model's capacity to optimize
the long-term rewards.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09306" title="Abstract">arXiv:2310.09306</a> [<a href="/pdf/2310.09306" title="Download PDF">pdf</a>, <a href="/ps/2310.09306" title="Download PostScript">ps</a>, <a href="/format/2310.09306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multirotor Newton-Euler and Euler-Lagrange Modeling Equivalence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Martini%2C+S">Simone Martini</a>, 
<a href="/search/eess?searchtype=author&query=Valavanis%2C+K+P">Kimon P. Valavanis</a>, 
<a href="/search/eess?searchtype=author&query=Stefanovic%2C+M">Margareta Stefanovic</a>, 
<a href="/search/eess?searchtype=author&query=Rutherford%2C+M+J">Matthew J. Rutherford</a>, 
<a href="/search/eess?searchtype=author&query=Rizzo%2C+A">Alessandro Rizzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Early version of submitted work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We propose a revised Euler Lagrange multirotor model that guarantees the
equivalence with the Newton Euler (N-E) modeling formulations. First, we show
that the literature quadrotor/multirotor model derived from the Euler Lagrange
(E-L) equations does not lead to an equivalence when compared to the N-E one.
Then we introduce the revised E-L (r-E-L) for multirotor attitude dynamics and
proceed with the analytical proof of equivalence to the N-E model. We verify
the results through simulation studies and show improved stability when
performing feedback linearization control with the r-E-L model compared to the
literature E-L.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09314" title="Abstract">arXiv:2310.09314</a> [<a href="/pdf/2310.09314" title="Download PDF">pdf</a>, <a href="/format/2310.09314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eliciting Model Steering Interactions from Users via Data and Visual  Design Probes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crisan%2C+A">Anamaria Crisan</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+M">Maddie Shang</a>, 
<a href="/search/cs?searchtype=author&query=Brochu%2C+E">Eric Brochu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Domain experts increasingly use automated data science tools to incorporate
machine learning (ML) models in their work but struggle to "debug" these models
when they are incorrect. For these experts, semantic interactions can provide
an accessible avenue to guide and refine ML models without having to
programmatically dive into its technical details. In this research, we conduct
an elicitation study using data and visual design probes to examine if and how
experts with a spectrum of ML expertise use semantic interactions to update a
simple classification model. We use our design probes to facilitate an
interactive dialogue with 20 participants and codify their interactions as a
set of target-interaction pairs. Interestingly, our findings revealed that many
targets of semantic interactions do not directly map to ML model parameters,
but instead aim to augment the data a model uses for training. We also identify
reasons that participants would hesitate to interact with ML models, including
burdens of cognitive load and concerns of injecting bias. Unexpectedly
participants also saw the value of using semantic interactions to work
collaboratively with members of their team. Participants with less ML expertise
found this to be a useful mechanism for communicating their concerns to ML
experts. This was an especially important observation, as our study also shows
the different needs that correspond to diverse ML expertise. Collectively, we
demonstrate that design probes are effective tools for proactively gathering
the affordances that should be offered in an interactive machine learning
system.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09318" title="Abstract">arXiv:2310.09318</a> [<a href="/pdf/2310.09318" title="Download PDF">pdf</a>, <a href="/format/2310.09318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Role of Morphogenetic Competency on Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shreesha%2C+L">Lakshwin Shreesha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Master's thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Adaptation and Self-Organizing Systems (nlin.AO); Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">The relationship between intelligence and evolution is bidirectional: while
evolution can help evolve intelligences, the degree of intelligence itself can
impact evolution (Baldwin, 1896). In the field of Evolutionary Computation, the
inverse relationship (impact of intelligence on evolution) is approached from
the perspective of organism level behaviour (Hinton, 1996). We extend these
ideas to the developmental (cellular morphogenetic) level in the context of an
expanded view of intelligence as not only the ability of a system to navigate
the three-dimensional world, but also as the ability to navigate other
arbitrary spaces (transcriptional, anatomical, physiological, etc.). Here, we
specifically focus on the intelligence of a minimal model of a system
navigating anatomical morphospace, and assess how the degree and manner of
problem solving competency during morphogenesis effects evolutionary dynamics.
To this end, we evolve populations of artificial embryos using a standard
genetic algorithm in silico. Artificial embryos were cellular collectives given
the capacity to undergo morphogenetic rearrangement (e.g., regulative
development) prior to selection within an evolutionary cycle. Results from our
model indicates that morphogenetic competency significantly alters evolutionary
dynamics, with evolution preferring to improve anatomical intelligence rather
than perfect the structural genes. These observations hint that evolution in
the natural world may be leveraging the problem solving competencies of cells
at multiple scales to boost evolvability and robustness to novel conditions. We
discuss implications of our results for the Developmental Biology and
Artificial Life communities.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09319" title="Abstract">arXiv:2310.09319</a> [<a href="/pdf/2310.09319" title="Download PDF">pdf</a>, <a href="/format/2310.09319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological Data Analysis in smart manufacturing processes -- A survey  on the state of the art
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uray%2C+M">Martin Uray</a>, 
<a href="/search/cs?searchtype=author&query=Giunti%2C+B">Barbara Giunti</a>, 
<a href="/search/cs?searchtype=author&query=Kerber%2C+M">Michael Kerber</a>, 
<a href="/search/cs?searchtype=author&query=Huber%2C+S">Stefan Huber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint still under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Algebraic Topology (math.AT); Applications (stat.AP)

</div>
<p class="mathjax">Topological Data Analysis (TDA) is a mathematical method using techniques
from topology for the analysis of complex, multi-dimensional data that has been
widely and successfully applied in several fields such as medicine, material
science, biology, and others. This survey summarizes the state of the art of
TDA in yet another application area: industrial manufacturing and production in
the context of Industry 4.0. We perform a rigorous and reproducible literature
search of applications of TDA on the setting of industrial production and
manufacturing. The resulting works are clustered and analyzed based on their
application area within the manufacturing process and their input data type. We
highlight the key benefits of TDA and their tools in this area and describe its
challenges, as well as future potential. Finally, we discuss which TDA methods
are underutilized in (the specific area of) industry and the identified types
of application, with the goal of prompting more research in this profitable
area of application.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09336" title="Abstract">arXiv:2310.09336</a> [<a href="/pdf/2310.09336" title="Download PDF">pdf</a>, <a href="/format/2310.09336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Abilities Emerge Multiplicatively: Exploring Diffusion  Models on a Synthetic Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okawa%2C+M">Maya Okawa</a>, 
<a href="/search/cs?searchtype=author&query=Lubana%2C+E+S">Ekdeep Singh Lubana</a>, 
<a href="/search/cs?searchtype=author&query=Dick%2C+R+P">Robert P. Dick</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+H">Hidenori Tanaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Modern generative models exhibit unprecedented capabilities to generate
extremely realistic data. However, given the inherent compositionality of the
real world, reliable use of these models in practical applications requires
that they exhibit the capability to compose a novel set of concepts to generate
outputs not seen in the training data set. Prior work demonstrates that recent
diffusion models do exhibit intriguing compositional generalization abilities,
but also fail unpredictably. Motivated by this, we perform a controlled study
for understanding compositional generalization in conditional diffusion models
in a synthetic setting, varying different attributes of the training data and
measuring the model's ability to generate samples out-of-distribution. Our
results show: (i) the order in which the ability to generate samples from a
concept and compose them emerges is governed by the structure of the underlying
data-generating process; (ii) performance on compositional tasks exhibits a
sudden ``emergence'' due to multiplicative reliance on the performance of
constituent tasks, partially explaining emergent phenomena seen in generative
models; and (iii) composing concepts with lower frequency in the training data
to generate out-of-distribution samples requires considerably more optimization
steps compared to generating in-distribution samples. Overall, our study lays a
foundation for understanding capabilities and compositionality in generative
models from a data-centric perspective.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09338" title="Abstract">arXiv:2310.09338</a> [<a href="/pdf/2310.09338" title="Download PDF">pdf</a>, <a href="/format/2310.09338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Quantification using Generative Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunsheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present the Incremental Generative Monte Carlo (IGMC) method, designed to
measure uncertainty in deep neural networks using deep generative approaches.
IGMC iteratively trains generative models, adding their output to the dataset,
to compute the posterior distribution of the expectation of a random variable.
We provide a theoretical guarantee of the convergence rate of IGMC relative to
the sample size and sampling depth. Due to its compatibility with deep
generative approaches, IGMC is adaptable to both neural network classification
and regression tasks. We empirically study the behavior of IGMC on the MNIST
digit classification task.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09340" title="Abstract">arXiv:2310.09340</a> [<a href="/pdf/2310.09340" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geo-knowledge-guided GPT models improve the extraction of location  descriptions from disaster-related social media messages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yingjie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+G">Gengchen Mai</a>, 
<a href="/search/cs?searchtype=author&query=Cundy%2C+C">Chris Cundy</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+K">Kristy Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+N">Ni Lao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lakhanpal%2C+G">Gaurish Lakhanpal</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R+Z">Ryan Zhenqi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Joseph%2C+K">Kenneth Joseph</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Geographical Information Science, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Social media messages posted by people during natural disasters often contain
important location descriptions, such as the locations of victims. Recent
research has shown that many of these location descriptions go beyond simple
place names, such as city names and street names, and are difficult to extract
using typical named entity recognition (NER) tools. While advanced machine
learning models could be trained, they require large labeled training datasets
that can be time-consuming and labor-intensive to create. In this work, we
propose a method that fuses geo-knowledge of location descriptions and a
Generative Pre-trained Transformer (GPT) model, such as ChatGPT and GPT-4. The
result is a geo-knowledge-guided GPT model that can accurately extract location
descriptions from disaster-related social media messages. Also, only 22
training examples encoding geo-knowledge are used in our method. We conduct
experiments to compare this method with nine alternative approaches on a
dataset of tweets from Hurricane Harvey. Our method demonstrates an over 40%
improvement over typically used NER approaches. The experiment results also
show that geo-knowledge is indispensable for guiding the behavior of GPT
models. The extracted location descriptions can help disaster responders reach
victims more quickly and may even save lives.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09341" title="Abstract">arXiv:2310.09341</a> [<a href="/pdf/2310.09341" title="Download PDF">pdf</a>, <a href="/format/2310.09341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing the cold start problem in privacy preserving content-based  recommender systems using hypercube graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tuval%2C+N">Noa Tuval</a>, 
<a href="/search/cs?searchtype=author&query=Hertz%2C+A">Alain Hertz</a>, 
<a href="/search/cs?searchtype=author&query=Kuflik%2C+T">Tsvi Kuflik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 6 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">The initial interaction of a user with a recommender system is problematic
because, in such a so-called cold start situation, the recommender system has
very little information about the user, if any. Moreover, in collaborative
filtering, users need to share their preferences with the service provider by
rating items while in content-based filtering there is no need for such
information sharing. We have recently shown that a content-based model that
uses hypercube graphs can determine user preferences with a very limited number
of ratings while better preserving user privacy. In this paper, we confirm
these findings on the basis of experiments with more than 1,000 users in the
restaurant and movie domains. We show that the proposed method outperforms
standard machine learning algorithms when the number of available ratings is at
most 10, which often happens, and is competitive with larger training sets. In
addition, training is simple and does not require large computational efforts.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09342" title="Abstract">arXiv:2310.09342</a> [<a href="/pdf/2310.09342" title="Download PDF">pdf</a>, <a href="/format/2310.09342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ranking LLM-Generated Loop Invariants for Program Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Saikat Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Lahiri%2C+S+K">Shuvendu K. Lahiri</a>, 
<a href="/search/cs?searchtype=author&query=Fakhoury%2C+S">Sarah Fakhoury</a>, 
<a href="/search/cs?searchtype=author&query=Musuvathi%2C+M">Madanlal Musuvathi</a>, 
<a href="/search/cs?searchtype=author&query=Lal%2C+A">Akash Lal</a>, 
<a href="/search/cs?searchtype=author&query=Rastogi%2C+A">Aseem Rastogi</a>, 
<a href="/search/cs?searchtype=author&query=Senthilnathan%2C+A">Aditya Senthilnathan</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rahul Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Swamy%2C+N">Nikhil Swamy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP-Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Software Engineering (cs.SE)

</div>
<p class="mathjax">Synthesizing inductive loop invariants is fundamental to automating program
verification. In this work, we observe that Large Language Models (such as
gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of
programs in a 0-shot setting, yet require several samples to generate the
correct invariants. This can lead to a large number of calls to a program
verifier to establish an invariant. To address this issue, we propose a {\it
re-ranking} approach for the generated results of LLMs. We have designed a
ranker that can distinguish between correct inductive invariants and incorrect
attempts based on the problem definition. The ranker is optimized as a
contrastive ranker. Experimental results demonstrate that this re-ranking
mechanism significantly improves the ranking of correct invariants among the
generated candidates, leading to a notable reduction in the number of calls to
a verifier.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09343" title="Abstract">arXiv:2310.09343</a> [<a href="/pdf/2310.09343" title="Download PDF">pdf</a>, <a href="/format/2310.09343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dialogue Chain-of-Thought Distillation for Commonsense-aware  Conversational Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chae%2C+H">Hyungjoo Chae</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yongho Song</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+K+T">Kai Tzu-iunn Ong</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taeyoon Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minjin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongha Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yeo%2C+J">Jinyoung Yeo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 8 figures, Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Human-like chatbots necessitate the use of commonsense reasoning in order to
effectively comprehend and respond to implicit information present within
conversations. Achieving such coherence and informativeness in responses,
however, is a non-trivial task. Even for large language models (LLMs), the task
of identifying and aggregating key evidence within a single hop presents a
substantial challenge. This complexity arises because such evidence is
scattered across multiple turns in a conversation, thus necessitating
integration over multiple hops. Hence, our focus is to facilitate such
multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought
(CoT) reasoning. To this end, we propose a knowledge distillation framework
that leverages LLMs as unreliable teachers and selectively distills consistent
and helpful rationales via alignment filters. We further present DOCTOR, a
DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for
response generation. We conduct extensive experiments to show that enhancing
dialogue agents with high-quality rationales from DOCTOR significantly improves
the quality of their responses.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09346" title="Abstract">arXiv:2310.09346</a> [<a href="/pdf/2310.09346" title="Download PDF">pdf</a>, <a href="/format/2310.09346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HaptiCharger: Robotic Charging of Electric Vehicles Based on Human  Haptic Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alyoune%2C+O">Oussama Alyoune</a>, 
<a href="/search/cs?searchtype=author&query=Cabrera%2C+M+A">Miguel Altamirano Cabrera</a>, 
<a href="/search/cs?searchtype=author&query=Tsetserukou%2C+D">Dzmitry Tsetserukou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript accepted to IEEE ROBIO 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The growing demand for electric vehicles requires the development of
automated car charging methods. At the moment, the process of charging an
electric car is completely manual, and that requires physical effort to
accomplish the task, which is not suitable for people with disabilities.
Typically, the effort in the automation of the charging task research is
focused on detecting the position and orientation of the socket, which resulted
in a relatively high accuracy, 5 mm, and 10 degrees. However, this accuracy is
not enough to complete the charging process. In this work, we focus on
designing a novel methodology for robust robotic plug-in and plug-out based on
human haptics to overcome the error in the orientation of the socket.
Participants were invited to perform the charging task, and their cognitive
capabilities were recognized by measuring the applied forces along with the
movements of the charger. Eventually, an algorithm was developed based on the
human's best strategies to be applied to a robotic arm.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09347" title="Abstract">arXiv:2310.09347</a> [<a href="/pdf/2310.09347" title="Download PDF">pdf</a>, <a href="/format/2310.09347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Apple Maturity and Damage Assessment: A Lightweight Detection  Model with GAN and Attention Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yufei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Manzhou Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qin Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This study proposes a method based on lightweight convolutional neural
networks (CNN) and generative adversarial networks (GAN) for apple ripeness and
damage level detection tasks. Initially, a lightweight CNN model is designed by
optimizing the model's depth and width, as well as employing advanced model
compression techniques, successfully reducing the model's parameter and
computational requirements, thus enhancing real-time performance in practical
applications. Simultaneously, attention mechanisms are introduced, dynamically
adjusting the importance of different feature layers to improve the performance
in object detection tasks. To address the issues of sample imbalance and
insufficient sample size, GANs are used to generate realistic apple images,
expanding the training dataset and enhancing the model's recognition capability
when faced with apples of varying ripeness and damage levels. Furthermore, by
applying the object detection network for damage location annotation on damaged
apples, the accuracy of damage level detection is improved, providing a more
precise basis for decision-making. Experimental results show that in apple
ripeness grading detection, the proposed model achieves 95.6\%, 93.8\%, 95.0\%,
and 56.5 in precision, recall, accuracy, and FPS, respectively. In apple damage
level detection, the proposed model reaches 95.3\%, 93.7\%, and 94.5\% in
precision, recall, and mAP, respectively. In both tasks, the proposed method
outperforms other mainstream models, demonstrating the excellent performance
and high practical value of the proposed method in apple ripeness and damage
level detection tasks.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09350" title="Abstract">arXiv:2310.09350</a> [<a href="/pdf/2310.09350" title="Download PDF">pdf</a>, <a href="/format/2310.09350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Domain Adaption for Neural Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dominguez%2C+C">Carlos Dominguez</a>, 
<a href="/search/cs?searchtype=author&query=Campos%2C+J+A">Jon Ander Campos</a>, 
<a href="/search/cs?searchtype=author&query=Agirre%2C+E">Eneko Agirre</a>, 
<a href="/search/cs?searchtype=author&query=Azkune%2C+G">Gorka Azkune</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Neural information retrieval requires costly annotated data for each target
domain to be competitive. Synthetic annotation by query generation using Large
Language Models or rule-based string manipulation has been proposed as an
alternative, but their relative merits have not been analysed. In this paper,
we compare both methods head-to-head using the same neural IR architecture. We
focus on the BEIR benchmark, which includes test datasets from several domains
with no training data, and explore two scenarios: zero-shot, where the
supervised system is trained in a large out-of-domain dataset (MS-MARCO); and
unsupervised domain adaptation, where, in addition to MS-MARCO, the system is
fine-tuned in synthetic data from the target domain. Our results indicate that
Large Language Models outperform rule-based methods in all scenarios by a large
margin, and, more importantly, that unsupervised domain adaptation is effective
compared to applying a supervised IR system in a zero-shot fashion. In addition
we explore several sizes of open Large Language Models to generate synthetic
data and find that a medium-sized model suffices. Code and models are publicly
available for reproducibility.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09357" title="Abstract">arXiv:2310.09357</a> [<a href="/pdf/2310.09357" title="Download PDF">pdf</a>, <a href="/ps/2310.09357" title="Download PostScript">ps</a>, <a href="/format/2310.09357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computational Approach to Style in American Poetry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaplan%2C+D+M">David M. Kaplan</a>, 
<a href="/search/cs?searchtype=author&query=Blei%2C+D+M">David M. Blei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted manuscript; see doi for version of record
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Seventh IEEE International Conference on Data Mining (ICDM 2007),
  Omaha, NE, USA, 2007, pp. 553-558
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We develop a quantitative method to assess the style of American poems and to
visualize a collection of poems in relation to one another. Qualitative poetry
criticism helped guide our development of metrics that analyze various
orthographic, syntactic, and phonemic features. These features are used to
discover comprehensive stylistic information from a poem's multi-layered latent
structure, and to compute distances between poems in this space. Visualizations
provide ready access to the analytical components. We demonstrate our method on
several collections of poetry, showing that it better delineates poetry style
than the traditional word-occurrence features that are used in typical text
analysis algorithms. Our method has potential applications to academic research
of texts, to research of the intuitive personal response to poetry, and to
making recommendations to readers based on their favorite poems.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09358" title="Abstract">arXiv:2310.09358</a> [<a href="/pdf/2310.09358" title="Download PDF">pdf</a>, <a href="/format/2310.09358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When are Bandits Robust to Misspecification?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+D">Debangshu Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Gopalan%2C+A">Aditya Gopalan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Parametric feature-based reward models are widely employed by algorithms for
decision making settings such as bandits and contextual bandits. The typical
assumption under which they are analysed is realizability, i.e., that the true
rewards of actions are perfectly explained by some parametric model in the
class. We are, however, interested in the situation where the true rewards are
(potentially significantly) misspecified with respect to the model class. For
parameterized bandits and contextual bandits, we identify sufficient
conditions, depending on the problem instance and model class, under which
classic algorithms such as $\epsilon$-greedy and LinUCB enjoy sublinear (in the
time horizon) regret guarantees under even grossly misspecified rewards. This
is in contrast to existing worst-case results for misspecified bandits which
show regret bounds that scale linearly with time, and shows that there can be a
nontrivially large set of bandit instances that are robust to misspecification.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09360" title="Abstract">arXiv:2310.09360</a> [<a href="/pdf/2310.09360" title="Download PDF">pdf</a>, <a href="/format/2310.09360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Verification of ReLU Neural Control Barrier Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongchao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junlin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Vorobeychik%2C+Y">Yevgeniy Vorobeychik</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+A">Andrew Clark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Control Barrier Functions (CBFs) are a popular approach for safe control of
nonlinear systems. In CBF-based control, the desired safety properties of the
system are mapped to nonnegativity of a CBF, and the control input is chosen to
ensure that the CBF remains nonnegative for all time. Recently, machine
learning methods that represent CBFs as neural networks (neural control barrier
functions, or NCBFs) have shown great promise due to the universal
representability of neural networks. However, verifying that a learned CBF
guarantees safety remains a challenging research problem. This paper presents
novel exact conditions and algorithms for verifying safety of feedforward NCBFs
with ReLU activation functions. The key challenge in doing so is that, due to
the piecewise linearity of the ReLU function, the NCBF will be
nondifferentiable at certain points, thus invalidating traditional safety
verification methods that assume a smooth barrier function. We resolve this
issue by leveraging a generalization of Nagumo's theorem for proving invariance
of sets with nonsmooth boundaries to derive necessary and sufficient conditions
for safety. Based on this condition, we propose an algorithm for safety
verification of NCBFs that first decomposes the NCBF into piecewise linear
segments and then solves a nonlinear program to verify safety of each segment
as well as the intersections of the linear segments. We mitigate the complexity
by only considering the boundary of the safe region and by pruning the segments
with Interval Bound Propagation (IBP) and linear relaxation. We evaluate our
approach through numerical studies with comparison to state-of-the-art
SMT-based methods. Our code is available at
https://github.com/HongchaoZhang-HZ/exactverif-reluncbf-nips23.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09361" title="Abstract">arXiv:2310.09361</a> [<a href="/pdf/2310.09361" title="Download PDF">pdf</a>, <a href="/format/2310.09361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Certifying $\ell_p$ Robustness Still Worthwhile?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mangal%2C+R">Ravi Mangal</a>, 
<a href="/search/cs?searchtype=author&query=Leino%2C+K">Klas Leino</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Weicheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Pasareanu%2C+C">Corina Pasareanu</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+A">Anupam Datta</a>, 
<a href="/search/cs?searchtype=author&query=Fredrikson%2C+M">Matt Fredrikson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Over the years, researchers have developed myriad attacks that exploit the
ubiquity of adversarial examples, as well as defenses that aim to guard against
the security vulnerabilities posed by such attacks. Of particular interest to
this paper are defenses that provide provable guarantees against the class of
$\ell_p$-bounded attacks. Certified defenses have made significant progress,
taking robustness certification from toy models and datasets to large-scale
problems like ImageNet classification. While this is undoubtedly an interesting
academic problem, as the field has matured, its impact in practice remains
unclear, thus we find it useful to revisit the motivation for continuing this
line of research. There are three layers to this inquiry, which we address in
this paper: (1) why do we care about robustness research? (2) why do we care
about the $\ell_p$-bounded threat model? And (3) why do we care about
certification as opposed to empirical defenses? In brief, we take the position
that local robustness certification indeed confers practical value to the field
of machine learning. We focus especially on the latter two questions from
above. With respect to the first of the two, we argue that the $\ell_p$-bounded
threat model acts as a minimal requirement for safe application of models in
security-critical domains, while at the same time, evidence has mounted
suggesting that local robustness may lead to downstream external benefits not
immediately related to robustness. As for the second, we argue that (i)
certification provides a resolution to the cat-and-mouse game of adversarial
attacks; and furthermore, that (ii) perhaps contrary to popular belief, there
may not exist a fundamental trade-off between accuracy, robustness, and
certifiability, while moreover, certified training techniques constitute a
particularly promising way for learning robust models.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09362" title="Abstract">arXiv:2310.09362</a> [<a href="/pdf/2310.09362" title="Download PDF">pdf</a>, <a href="/format/2310.09362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment  Technique
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elahimanesh%2C+S">Sina Elahimanesh</a>, 
<a href="/search/cs?searchtype=author&query=Salehi%2C+S">Shayan Salehi</a>, 
<a href="/search/cs?searchtype=author&query=Movahed%2C+S+Z">Sara Zahedi Movahed</a>, 
<a href="/search/cs?searchtype=author&query=Alazraki%2C+L">Lisa Alazraki</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Ruoyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Edalat%2C+A">Abbas Edalat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the wake of the post-pandemic era, marked by social isolation and surging
rates of depression and anxiety, conversational agents based on digital
psychotherapy can play an influential role compared to traditional therapy
sessions. In this work, we develop a voice-capable chatbot in Farsi to guide
users through Self-Attachment (SAT), a novel, self-administered, holistic
psychological technique based on attachment theory. Our chatbot uses a dynamic
array of rule-based and classification-based modules to comprehend user input
throughout the conversation and navigates a dialogue flowchart accordingly,
recommending appropriate SAT exercises that depend on the user's emotional and
mental state. In particular, we collect a dataset of over 6,000 utterances and
develop a novel sentiment-analysis module that classifies user sentiment into
12 classes, with accuracy above 92%. To keep the conversation novel and
engaging, the chatbot's responses are retrieved from a large dataset of
utterances created with the aid of Farsi GPT-2 and a reinforcement learning
approach, thus requiring minimal human annotation. Our chatbot also offers a
question-answering module, called SAT Teacher, to answer users' questions about
the principles of Self-Attachment. Finally, we design a cross-platform
application as the bot's user interface. We evaluate our platform in a ten-day
human study with N=52 volunteers from the non-clinical population, who have had
over 2,000 dialogues in total with the chatbot. The results indicate that the
platform was engaging to most users (75%), 72% felt better after the
interactions, and 74% were satisfied with the SAT Teacher's performance.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09370" title="Abstract">arXiv:2310.09370</a> [<a href="/pdf/2310.09370" title="Download PDF">pdf</a>, <a href="/ps/2310.09370" title="Download PostScript">ps</a>, <a href="/format/2310.09370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-optimal Differentially Private Client Selection in Federated  Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+S+E">Syed Eqbal Alam</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+D">Dhirendra Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+S">Shrisha Rao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the proceedings of the 59th Annual Allerton Conference on Communication, Control, and Computing, September 2023, Monticello, Illinois, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We develop an iterative differentially private algorithm for client selection
in federated settings. We consider a federated network wherein clients
coordinate with a central server to complete a task; however, the clients
decide whether to participate or not at a time step based on their preferences
-- local computation and probabilistic intent. The algorithm does not require
client-to-client information exchange. The developed algorithm provides
near-optimal values to the clients over long-term average participation with a
certain differential privacy guarantee. Finally, we present the experimental
results to check the algorithm's efficacy.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09373" title="Abstract">arXiv:2310.09373</a> [<a href="/pdf/2310.09373" title="Download PDF">pdf</a>, <a href="/format/2310.09373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying and examining machine learning biases on Adult dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Girhepuje%2C+S">Sahil Girhepuje</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This research delves into the reduction of machine learning model bias
through Ensemble Learning. Our rigorous methodology comprehensively assesses
bias across various categorical variables, ultimately revealing a pronounced
gender attribute bias. The empirical evidence unveils a substantial
gender-based wage prediction disparity: wages predicted for males, initially at
\$902.91, significantly decrease to \$774.31 when the gender attribute is
alternated to females. Notably, Kullback-Leibler divergence scores point to
gender bias, with values exceeding 0.13, predominantly within tree-based
models. Employing Ensemble Learning elucidates the quest for fairness and
transparency. Intriguingly, our findings reveal that the stacked model aligns
with individual models, confirming the resilience of model bias. This study
underscores ethical considerations and advocates the implementation of hybrid
models for a data-driven society marked by impartiality and inclusivity.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09381" title="Abstract">arXiv:2310.09381</a> [<a href="/pdf/2310.09381" title="Download PDF">pdf</a>, <a href="/ps/2310.09381" title="Download PostScript">ps</a>, <a href="/format/2310.09381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Local Fourier Analysis for Additive Schwarz Smoothers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=de+la+Riva%2C+%C3%81+P">&#xc1;lvaro P&#xe9; de la Riva</a>, 
<a href="/search/math?searchtype=author&query=Rodrigo%2C+C">Carmen Rodrigo</a>, 
<a href="/search/math?searchtype=author&query=Gaspar%2C+F+J">Francisco J. Gaspar</a>, 
<a href="/search/math?searchtype=author&query=Adler%2C+J+H">James H. Adler</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+X">Xiaozhe Hu</a>, 
<a href="/search/math?searchtype=author&query=Zikatanov%2C+L">Ludmil Zikatanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this work, a local Fourier analysis is presented to study the convergence
of multigrid methods based on additive Schwarz smoothers. This analysis is
presented as a general framework which allows us to study these smoothers for
any type of discretization and problem. The presented framework is crucial in
practice since it allows one to know a priori the answer to questions such as
what is the size of the patch to use within these relaxations, the size of the
overlapping, or even the optimal values for the weights involved in the
smoother. Results are shown for a class of additive and restricted additive
Schwarz relaxations used within a multigrid framework applied to high-order
finite-element discretizations and saddle point problems, which are two of the
contexts in which these type of relaxations are widely used.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09382" title="Abstract">arXiv:2310.09382</a> [<a href="/pdf/2310.09382" title="Download PDF">pdf</a>, <a href="/format/2310.09382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalil%2C+A">Ahmed Khalil</a>, 
<a href="/search/cs?searchtype=author&query=Piechocki%2C+R">Robert Piechocki</a>, 
<a href="/search/cs?searchtype=author&query=Santos-Rodriguez%2C+R">Raul Santos-Rodriguez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper we introduce learnable lattice vector quantization and
demonstrate its effectiveness for learning discrete representations. Our
method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with
lattice-based discretization. The learnable lattice imposes a structure over
all discrete embeddings, acting as a deterrent against codebook collapse,
leading to high codebook utilization. Compared to VQ-VAE, our method obtains
lower reconstruction errors under the same training conditions, trains in a
fraction of the time, and with a constant number of parameters (equal to the
embedding dimension $D$), making it a very scalable approach. We demonstrate
these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09383" title="Abstract">arXiv:2310.09383</a> [<a href="/pdf/2310.09383" title="Download PDF">pdf</a>, <a href="/format/2310.09383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Symbolic Reasoning into Neural Generative Models for Design  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacobson%2C+M+J">Maxwell Joseph Jacobson</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yexiang Xue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Design generation requires tight integration of neural and symbolic
reasoning, as good design must meet explicit user needs and honor implicit
rules for aesthetics, utility, and convenience. Current automated design tools
driven by neural networks produce appealing designs, but cannot satisfy user
specifications and utility requirements. Symbolic reasoning tools, such as
constraint programming, cannot perceive low-level visual information in images
or capture subtle aspects such as aesthetics. We introduce the Spatial
Reasoning Integrated Generator (SPRING) for design generation. SPRING embeds a
neural and symbolic integrated spatial reasoning module inside the deep
generative network. The spatial reasoning module decides the locations of
objects to be generated in the form of bounding boxes, which are predicted by a
recurrent neural network and filtered by symbolic constraint satisfaction.
Embedding symbolic reasoning into neural generation guarantees that the output
of SPRING satisfies user requirements. Furthermore, SPRING offers
interpretability, allowing users to visualize and diagnose the generation
process through the bounding boxes. SPRING is also adept at managing novel user
specifications not encountered during its training, thanks to its proficiency
in zero-shot constraint transfer. Quantitative evaluations and a human study
reveal that SPRING outperforms baseline generative models, excelling in
delivering high design quality and better meeting user specifications.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09385" title="Abstract">arXiv:2310.09385</a> [<a href="/pdf/2310.09385" title="Download PDF">pdf</a>, <a href="/format/2310.09385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PIM-GPT: A Hybrid Process-in-Memory Accelerator for Autoregressive  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuting Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W+D">Wei D. Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Decoder-only Transformer models such as GPT have demonstrated superior
performance in text generation, by autoregressively predicting the next token.
However, the performance of GPT is bounded by low compute-to-memory-ratio and
high memory access. Throughput-oriented architectures such as GPUs target
parallel processing rather than sequential token generation, and are not
efficient for GPT acceleration, particularly on-device inference applications.
Process-in-memory (PIM) architectures can significantly reduce data movement
and provide high computation parallelism, and are promising candidates to
accelerate GPT inference.
<br />In this work, we propose PIM-GPT that aims to achieve high throughput, high
energy efficiency and end-to-end acceleration of GPT inference. PIM-GPT
leverages DRAM-based PIM solutions to perform multiply-accumulate (MAC)
operations on the DRAM chips, greatly reducing data movement. A compact
application-specific integrated chip (ASIC) is designed and synthesized to
initiate instructions to PIM chips and support data communication along with
necessary arithmetic computations. At the software level, the mapping scheme is
designed to maximize data locality and computation parallelism by partitioning
a matrix among DRAM channels and banks to utilize all in-bank computation
resources concurrently. We develop an event-driven clock-cycle accurate
simulator to validate the efficacy of the proposed PIM-GPT architecture.
Overall, PIM-GPT achieves 41$-$137$\times$, 631$-$1074$\times$ speedup and
339$-$1085$\times$, 890$-$1632$\times$ energy efficiency over GPU and CPU
baseline, respectively, on 8 GPT models with up to 1.4 billion parameters.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09392" title="Abstract">arXiv:2310.09392</a> [<a href="/pdf/2310.09392" title="Download PDF">pdf</a>, <a href="/format/2310.09392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Estimation of Maximum Vertical Velocity from Radar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chase%2C+R+J">Randy J. Chase</a>, 
<a href="/search/cs?searchtype=author&query=McGovern%2C+A">Amy McGovern</a>, 
<a href="/search/cs?searchtype=author&query=Homeyer%2C+C">Cameron Homeyer</a>, 
<a href="/search/cs?searchtype=author&query=Marinescu%2C+P">Peter Marinescu</a>, 
<a href="/search/cs?searchtype=author&query=Potvin%2C+C">Corey Potvin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Despite being the source region of severe weather hazards, the quantification
of the fast current of upward moving air (i.e., updraft) remains unavailable
for operational forecasting. Updraft proxies, like overshooting top area from
satellite images, have been linked to severe weather hazards but only relate to
a limited portion of the total storm updraft. This study investigates if a
machine learning model, namely U-Nets, can skillfully retrieve maximum vertical
velocity and its areal extent from 3-dimensional (3D) gridded radar
reflectivity alone. The machine learning model is trained using simulated radar
reflectivity and vertical velocity from the National Severe Storm Laboratory's
convection permitting Warn on Forecast System (WoFS). A parametric regression
technique using the Sinh-arcsinh-normal (SHASH) distribution is adapted to run
with UNets, allowing for both deterministic and probabilistic predictions of
maximum vertical velocity. The best models after hyperparameter search provided
less than 50% root mean squared error, a coefficient of determination greater
than 0.65 and an intersection over union (IoU) of more than 0.45 on the
independent test set composed of WoFS data. Beyond the WoFS analysis, a case
study was conducted using real radar data and corresponding dual-Doppler
analyses of vertical velocity within a supercell. The U-Net consistently
underestimates the dual-Doppler updraft speed estimates by 50%. Meanwhile, the
area of the 5 and 10 m s-1 updraft cores show an IoU of 0.25. While the above
statistics are not exceptional, the machine learning model enables quick
distillation of 3D radar data that is related to the maximum vertical velocity
which could be useful in assessing a storm's severe potential.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09394" title="Abstract">arXiv:2310.09394</a> [<a href="/pdf/2310.09394" title="Download PDF">pdf</a>, <a href="/format/2310.09394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantics Alignment via Split Learning for Resilient Multi-User Semantic  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jinhyuk Choi</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jihong Park</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+S">Seung-Woo Ko</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jinho Choi</a>, 
<a href="/search/cs?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seong-Lyun Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, 1 table, submitted to the IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Recent studies on semantic communication commonly rely on neural network (NN)
based transceivers such as deep joint source and channel coding (DeepJSCC).
Unlike traditional transceivers, these neural transceivers are trainable using
actual source data and channels, enabling them to extract and communicate
semantics. On the flip side, each neural transceiver is inherently biased
towards specific source data and channels, making different transceivers
difficult to understand intended semantics, particularly upon their initial
encounter. To align semantics over multiple neural transceivers, we propose a
distributed learning based solution, which leverages split learning (SL) and
partial NN fine-tuning techniques. In this method, referred to as SL with layer
freezing (SLF), each encoder downloads a misaligned decoder, and locally
fine-tunes a fraction of these encoder-decoder NN layers. By adjusting this
fraction, SLF controls computing and communication costs. Simulation results
confirm the effectiveness of SLF in aligning semantics under different source
data and channel dissimilarities, in terms of classification accuracy,
reconstruction errors, and recovery time for comprehending intended semantics
from misalignment.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09395" title="Abstract">arXiv:2310.09395</a> [<a href="/pdf/2310.09395" title="Download PDF">pdf</a>, <a href="/format/2310.09395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Medial Skeletal Diagram: A Generalized Medial Axis Approach for Compact  3D Shape Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Minghao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bohan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Matusik%2C+W">Wojciech Matusik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 28 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">We propose the Medial Skeletal Diagram, a novel skeletal representation that
tackles the prevailing issues around compactness and reconstruction accuracy in
existing skeletal representations. Our approach augments the continuous
elements in the medial axis representation to effectively shift the complexity
away from discrete elements. To that end, we introduce generalized enveloping
primitives, an enhancement of the standard primitives in medial axis, which
ensures efficient coverage of intricate local features of the input shape and
substantially reduces the number of discrete elements required. Moreover, we
present a computational framework that constructs a medial skeletal diagram
from an arbitrary closed manifold mesh. Our optimization pipeline ensures that
the resulting medial skeletal diagram comprehensively covers the input shape
with the fewest primitives. Additionally, each optimized primitive undergoes a
post-refinement process to guarantee an accurate match with the source mesh in
both geometry and tessellation. We validate our approach on a comprehensive
benchmark of 100 shapes, demonstrating its compactness of the discrete elements
and superior reconstruction accuracy across a variety of cases. Furthermore, we
exemplify the versatility of our representation in downstream applications such
as shape optimization, shape generation, mesh decomposition, mesh alignment,
mesh compression, and user-interactive design.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09397" title="Abstract">arXiv:2310.09397</a> [<a href="/pdf/2310.09397" title="Download PDF">pdf</a>, <a href="/format/2310.09397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifiability of Product of Experts Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gordon%2C+S+L">Spencer L. Gordon</a>, 
<a href="/search/cs?searchtype=author&query=Kant%2C+M">Manav Kant</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+E">Eric Ma</a>, 
<a href="/search/cs?searchtype=author&query=Schulman%2C+L+J">Leonard J. Schulman</a>, 
<a href="/search/cs?searchtype=author&query=Staicu%2C+A">Andrei Staicu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Algebraic Geometry (math.AG); Statistics Theory (math.ST)

</div>
<p class="mathjax">Product of experts (PoE) are layered networks in which the value at each node
is an AND (or product) of the values (possibly negated) at its inputs. These
were introduced as a neural network architecture that can efficiently learn to
generate high-dimensional data which satisfy many low-dimensional constraints
-- thereby allowing each individual expert to perform a simple task. PoEs have
found a variety of applications in learning.
<br />We study the problem of identifiability of a product of experts model having
a layer of binary latent variables, and a layer of binary observables that are
iid conditional on the latents. The previous best upper bound on the number of
observables needed to identify the model was exponential in the number of
parameters. We show: (a) When the latents are uniformly distributed, the model
is identifiable with a number of observables equal to the number of parameters
(and hence best possible). (b) In the more general case of arbitrarily
distributed latents, the model is identifiable for a number of observables that
is still linear in the number of parameters (and within a factor of two of
best-possible). The proofs rely on root interlacing phenomena for some special
three-term recurrences.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09398" title="Abstract">arXiv:2310.09398</a> [<a href="/pdf/2310.09398" title="Download PDF">pdf</a>, <a href="/format/2310.09398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An In-Depth Examination of Requirements for Disclosure Risk Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jarmin%2C+R+S">Ron S. Jarmin</a>, 
<a href="/search/cs?searchtype=author&query=Abowd%2C+J+M">John M. Abowd</a>, 
<a href="/search/cs?searchtype=author&query=Ashmead%2C+R">Robert Ashmead</a>, 
<a href="/search/cs?searchtype=author&query=Cumings-Menon%2C+R">Ryan Cumings-Menon</a>, 
<a href="/search/cs?searchtype=author&query=Goldschlag%2C+N">Nathan Goldschlag</a>, 
<a href="/search/cs?searchtype=author&query=Hawes%2C+M+B">Michael B. Hawes</a>, 
<a href="/search/cs?searchtype=author&query=Keller%2C+S+A">Sallie Ann Keller</a>, 
<a href="/search/cs?searchtype=author&query=Kifer%2C+D">Daniel Kifer</a>, 
<a href="/search/cs?searchtype=author&query=Leclerc%2C+P">Philip Leclerc</a>, 
<a href="/search/cs?searchtype=author&query=Reiter%2C+J+P">Jerome P. Reiter</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez%2C+R+A">Rolando A. Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Schmutte%2C+I">Ian Schmutte</a>, 
<a href="/search/cs?searchtype=author&query=Velkoff%2C+V+A">Victoria A. Velkoff</a>, 
<a href="/search/cs?searchtype=author&query=Zhuravlev%2C+P">Pavel Zhuravlev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> PNAS, October 13, 2023, Vol. 120, No. 43
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Econometrics (econ.EM); Methodology (stat.ME)

</div>
<p class="mathjax">The use of formal privacy to protect the confidentiality of responses in the
2020 Decennial Census of Population and Housing has triggered renewed interest
and debate over how to measure the disclosure risks and societal benefits of
the published data products. Following long-established precedent in economics
and statistics, we argue that any proposal for quantifying disclosure risk
should be based on pre-specified, objective criteria. Such criteria should be
used to compare methodologies to identify those with the most desirable
properties. We illustrate this approach, using simple desiderata, to evaluate
the absolute disclosure risk framework, the counterfactual framework underlying
differential privacy, and prior-to-posterior comparisons. We conclude that
satisfying all the desiderata is impossible, but counterfactual comparisons
satisfy the most while absolute disclosure risk satisfies the fewest.
Furthermore, we explain that many of the criticisms levied against differential
privacy would be levied against any technology that is not equivalent to
direct, unrestricted access to confidential data. Thus, more research is
needed, but in the near-term, the counterfactual approach appears best-suited
for privacy-utility analysis.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09400" title="Abstract">arXiv:2310.09400</a> [<a href="/pdf/2310.09400" title="Download PDF">pdf</a>, <a href="/format/2310.09400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Contextualization: Bridging the Gap between Collaborative  Filtering and Pre-trained Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaolong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingdai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yueqing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Traditional recommender systems have heavily relied on identity
representations (IDs) to model users and items, while the ascendancy of
pre-trained language model (PLM) encoders has enriched the modeling of
contextual item descriptions. However, PLMs, although effective in addressing
few-shot, zero-shot, or unified modeling scenarios, often neglect the crucial
collaborative filtering signal. This neglect gives rise to two pressing
challenges: (1) Collaborative Contextualization, the seamless integration of
collaborative signals with contextual representations. (2) the imperative to
bridge the representation gap between ID-based representations and contextual
representations while preserving their contextual semantics. In this paper, we
propose CollabContext, a novel model that adeptly combines collaborative
filtering signals with contextual representations and aligns these
representations within the contextual space, preserving essential contextual
semantics. Experimental results across three real-world datasets demonstrate
substantial improvements. Leveraging collaborative contextualization,
CollabContext can also be effectively applied to cold-start scenarios,
achieving remarkable enhancements in recommendation performance. The code is
available after the conference accepts the paper.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09401" title="Abstract">arXiv:2310.09401</a> [<a href="/pdf/2310.09401" title="Download PDF">pdf</a>, <a href="/format/2310.09401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CIDER: Category-Guided Intent Disentanglement for Accurate Personalized  News Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+Y">Yunyong Ko</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+S">Seongeun Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sang-Wook Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Personalized news recommendation aims to assist users in finding news
articles that align with their interests, which plays a pivotal role in
mitigating users' information overload problem. Although many recent works have
been studied for better user and news representations, the following challenges
have been rarely studied: (C1) How to precisely comprehend a range of intents
coupled within a news article? and (C2) How to differentiate news articles with
varying post-read preferences in users' click history? To tackle both
challenges together, in this paper, we propose a novel personalized news
recommendation framework (CIDER) that employs (1) category-guided intent
disentanglement for (C1) and (2) consistency-based news representation for
(C2). Furthermore, we incorporate a category prediction into the training
process of CIDER as an auxiliary task, which provides supplementary supervisory
signals to enhance intent disentanglement. Extensive experiments on two
real-world datasets reveal that (1) CIDER provides consistent performance
improvements over seven state-of-the-art news recommendation methods and (2)
the proposed strategies significantly improve the model accuracy of CIDER.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09411" title="Abstract">arXiv:2310.09411</a> [<a href="/pdf/2310.09411" title="Download PDF">pdf</a>, <a href="/format/2310.09411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surveying the Landscape of Text Summarization with Deep Learning: A  Comprehensive Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guanghua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weili Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, deep learning has revolutionized natural language processing
(NLP) by enabling the development of models that can learn complex
representations of language data, leading to significant improvements in
performance across a wide range of NLP tasks. Deep learning models for NLP
typically use large amounts of data to train deep neural networks, allowing
them to learn the patterns and relationships in language data. This is in
contrast to traditional NLP approaches, which rely on hand-engineered features
and rules to perform NLP tasks. The ability of deep neural networks to learn
hierarchical representations of language data, handle variable-length input
sequences, and perform well on large datasets makes them well-suited for NLP
applications. Driven by the exponential growth of textual data and the
increasing demand for condensed, coherent, and informative summaries, text
summarization has been a critical research area in the field of NLP. Applying
deep learning to text summarization refers to the use of deep neural networks
to perform text summarization tasks. In this survey, we begin with a review of
fashionable text summarization tasks in recent years, including extractive,
abstractive, multi-document, and so on. Next, we discuss most deep
learning-based models and their experimental results on these tasks. The paper
also covers datasets and data representation for summarization tasks. Finally,
we delve into the opportunities and challenges associated with summarization
tasks and their corresponding methodologies, aiming to inspire future research
efforts to advance the field further. A goal of our survey is to explain how
these methods differ in their requirements as understanding them is essential
for choosing a technique suited for a specific setting.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09412" title="Abstract">arXiv:2310.09412</a> [<a href="/pdf/2310.09412" title="Download PDF">pdf</a>, <a href="/format/2310.09412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Reinforcement Learning for Optimizing Pump Sustainability in  Real-World Water Distribution Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+H">Harsh Patel</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lamb%2C+A+P">Alexander P Lamb</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jieliang Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This article addresses the pump-scheduling optimization problem to enhance
real-time control of real-world water distribution networks (WDNs). Our primary
objectives are to adhere to physical operational constraints while reducing
energy consumption and operational costs. Traditional optimization techniques,
such as evolution-based and genetic algorithms, often fall short due to their
lack of convergence guarantees. Conversely, reinforcement learning (RL) stands
out for its adaptability to uncertainties and reduced inference time, enabling
real-time responsiveness. However, the effective implementation of RL is
contingent on building accurate simulation models for WDNs, and prior
applications have been limited by errors in simulation training data. These
errors can potentially cause the RL agent to learn misleading patterns and
actions and recommend suboptimal operational strategies. To overcome these
challenges, we present an improved "hybrid RL" methodology. This method
integrates the benefits of RL while anchoring it in historical data, which
serves as a baseline to incrementally introduce optimal control
recommendations. By leveraging operational data as a foundation for the agent's
actions, we enhance the explainability of the agent's actions, foster more
robust recommendations, and minimize error. Our findings demonstrate that the
hybrid RL agent can significantly improve sustainability, operational
efficiency, and dynamically adapt to emerging scenarios in real-world WDNs.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09413" title="Abstract">arXiv:2310.09413</a> [<a href="/pdf/2310.09413" title="Download PDF">pdf</a>, <a href="/format/2310.09413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZeroSwap: Data-driven Optimal Market Making in DeFi
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nadkarni%2C+V">Viraj Nadkarni</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiachen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Rana%2C+R">Ranvir Rana</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Chi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Sanjeev Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Viswanath%2C+P">Pramod Viswanath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Automated Market Makers (AMMs) are major centers of matching liquidity supply
and demand in Decentralized Finance. Their functioning relies primarily on the
presence of liquidity providers (LPs) incentivized to invest their assets into
a liquidity pool. However, the prices at which a pooled asset is traded is
often more stale than the prices on centralized and more liquid exchanges. This
leads to the LPs suffering losses to arbitrage. This problem is addressed by
adapting market prices to trader behavior, captured via the classical market
microstructure model of Glosten and Milgrom. In this paper, we propose the
first optimal Bayesian and the first model-free data-driven algorithm to
optimally track the external price of the asset. The notion of optimality that
we use enforces a zero-profit condition on the prices of the market maker,
hence the name ZeroSwap. This ensures that the market maker balances losses to
informed traders with profits from noise traders. The key property of our
approach is the ability to estimate the external market price without the need
for price oracles or loss oracles. Our theoretical guarantees on the
performance of both these algorithms, ensuring the stability and convergence of
their price recommendations, are of independent interest in the theory of
reinforcement learning. We empirically demonstrate the robustness of our
algorithms to changing market conditions.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09417" title="Abstract">arXiv:2310.09417</a> [<a href="/pdf/2310.09417" title="Download PDF">pdf</a>, <a href="/format/2310.09417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Parallelizable Algorithms for Interpolative Decompositions via  Partially Pivoted LU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pearce%2C+K+J">Katherine J. Pearce</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/math?searchtype=author&query=Dong%2C+Y">Yijun Dong</a>, 
<a href="/search/math?searchtype=author&query=Martinsson%2C+P">Per-Gunnar Martinsson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Interpolative and CUR decompositions involve "natural bases" of row and
column subsets, or skeletons, of a given matrix that approximately span its row
and column spaces. These low-rank decompositions preserve properties such as
sparsity or non-negativity, and are easily interpretable in the context of the
original data. For large-scale problems, randomized sketching to sample the row
or column spaces with a random matrix can serve as an effective initial step in
skeleton selection to reduce computational cost. A by now well-established
approach has been to extract a randomized sketch, followed by column-pivoted QR
CPQR) on the sketch matrix. This manuscript describes an alternative approach
where CPQR is replaced by LU with partial pivoting (LUPP). While LUPP by itself
is not rank-revealing, it is demonstrated that when used in a randomized
setting, LUPP not only reveals the numerical rank, but also allows the
estimation of the residual error as the factorization is built. The resulting
algorithm is both adaptive and parallelizable, and attains much higher
practical speed due to the lower communication requirements of LUPP over CPQR.
The method has been implemented for both CPUs and GPUs, and the resulting
software has been made publicly available.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09420" title="Abstract">arXiv:2310.09420</a> [<a href="/pdf/2310.09420" title="Download PDF">pdf</a>, <a href="/ps/2310.09420" title="Download PostScript">ps</a>, <a href="/format/2310.09420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the convergence of discrete dynamic unbalanced transport models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/math?searchtype=author&query=Zou%2C+J">Jun Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> this is the second part of <a href="/abs/2011.05845">arXiv:2011.05845</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">A generalized unbalanced optimal transport distance ${\rm WB}_{\Lambda}$ on
matrix-valued measures $\mathcal{M}(\Omega,\mathbb{S}_+^n)$ was defined in
[<a href="/abs/2011.05845">arXiv:2011.05845</a>] \`{a} la Benamou-Brenier, which extends the
Kantorovich-Bures and the Wasserstein-Fisher-Rao distances. In this work, we
investigate the convergence properties of the discrete transport problems
associated with ${\rm WB}_{\Lambda}$. We first present a convergence framework
for abstract discretization. Then, we propose a specific discretization scheme
that aligns with this framework, under the assumption that the initial and
final distributions are absolutely continuous with respect to the Lebesgue
measure. Moreover, thanks to the static formulation, we show that such an
assumption can be removed for the Wasserstein-Fisher-Rao distance.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09423" title="Abstract">arXiv:2310.09423</a> [<a href="/pdf/2310.09423" title="Download PDF">pdf</a>, <a href="/format/2310.09423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QUIC is not Quick Enough over Fast Internet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xumiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shuowei Jin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yi He</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+A">Ahmad Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z+M">Z. Morley Mao</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+F">Feng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhi-Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">QUIC is expected to be a game-changer in improving web application
performance. In this paper, we conduct a systematic examination of QUIC's
performance over high-speed networks. We find that over fast Internet, the
UDP+QUIC+HTTP/3 stack suffers a data rate reduction of up to 45.2% compared to
the TCP+TLS+HTTP/2 counterpart. Moreover, the performance gap between QUIC and
HTTP/2 grows as the underlying bandwidth increases. We observe this issue on
lightweight data transfer clients and major web browsers (Chrome, Edge,
Firefox, Opera), on different hosts (desktop, mobile), and over diverse
networks (wired broadband, cellular). It affects not only file transfers, but
also various applications such as video streaming (up to 9.8% video bitrate
reduction) and web browsing. Through rigorous packet trace analysis and kernel-
and user-space profiling, we identify the root cause to be high receiver-side
processing overhead, in particular, excessive data packets and QUIC's
user-space ACKs. We make concrete recommendations for mitigating the observed
performance issues.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09424" title="Abstract">arXiv:2310.09424</a> [<a href="/pdf/2310.09424" title="Download PDF">pdf</a>, <a href="/format/2310.09424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALM: Speech-augmented Language Model with In-context Learning for  Speech Recognition and Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhehuai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">He Huang</a>, 
<a href="/search/cs?searchtype=author&query=Andrusenko%2C+A">Andrei Andrusenko</a>, 
<a href="/search/cs?searchtype=author&query=Hrinchuk%2C+O">Oleksii Hrinchuk</a>, 
<a href="/search/cs?searchtype=author&query=Puvvada%2C+K+C">Krishna C. Puvvada</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jason Li</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Subhankar Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Balam%2C+J">Jagadeesh Balam</a>, 
<a href="/search/cs?searchtype=author&query=Ginsburg%2C+B">Boris Ginsburg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submit to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We present a novel Speech Augmented Language Model (SALM) with {\em
multitask} and {\em in-context} learning capabilities. SALM comprises a frozen
text LLM, a audio encoder, a modality adapter module, and LoRA layers to
accommodate speech input and associated task instructions. The unified SALM not
only achieves performance on par with task-specific Conformer baselines for
Automatic Speech Recognition (ASR) and Speech Translation (AST), but also
exhibits zero-shot in-context learning capabilities, demonstrated through
keyword-boosting task for ASR and AST. Moreover, {\em speech supervised
in-context training} is proposed to bridge the gap between LLM training and
downstream speech tasks, which further boosts the in-context learning ability
of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09426" title="Abstract">arXiv:2310.09426</a> [<a href="/pdf/2310.09426" title="Download PDF">pdf</a>, <a href="/format/2310.09426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Reinforcement Learning for Optimizing Production Bidding  Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korenkevych%2C+D">Dmytro Korenkevych</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+F">Frank Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Balakir%2C+A">Artsiom Balakir</a>, 
<a href="/search/cs?searchtype=author&query=Nikulkov%2C+A">Alex Nikulkov</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Lingnan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Cen%2C+Z">Zhihao Cen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zuobing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zheqing Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The online advertising market, with its thousands of auctions run per second,
presents a daunting challenge for advertisers who wish to optimize their spend
under a budget constraint. Thus, advertising platforms typically provide
automated agents to their customers, which act on their behalf to bid for
impression opportunities in real time at scale. Because these proxy agents are
owned by the platform but use advertiser funds to operate, there is a strong
practical need to balance reliability and explainability of the agent with
optimizing power. We propose a generalizable approach to optimizing bidding
policies in production environments by learning from real data using offline
reinforcement learning. This approach can be used to optimize any
differentiable base policy (practically, a heuristic policy based on principles
which the advertiser can easily understand), and only requires data generated
by the base policy itself. We use a hybrid agent architecture that combines
arbitrary base policies with deep neural networks, where only the optimized
base policy parameters are eventually deployed, and the neural network part is
discarded after training. We demonstrate that such an architecture achieves
statistically significant performance gains in both simulated and at-scale
production bidding environments. Our approach does not incur additional
infrastructure, safety, or explainability costs, as it directly optimizes
parameters of existing production routines without replacing them with black
box-style models like neural networks.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09430" title="Abstract">arXiv:2310.09430</a> [<a href="/pdf/2310.09430" title="Download PDF">pdf</a>, <a href="/ps/2310.09430" title="Download PostScript">ps</a>, <a href="/format/2310.09430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Evaluation of Large Language Models on Out-of-Distribution  Logical Reasoning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+Q">Qiming Bao</a>, 
<a href="/search/cs?searchtype=author&query=Gendron%2C+G">Gael Gendron</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+A+Y">Alex Yuxuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+N">Neset Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Witbrock%2C+M">Michael Witbrock</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiamou Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the LLM@IJCAI 2023 non-archival symposium
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly
advanced the performance of artificial systems on various natural language
processing tasks to human-like levels. However, their generalisation and
robustness to perform logical reasoning remain under-evaluated. To probe this
ability, we propose three new logical reasoning datasets named "ReClor-plus",
"LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with
randomly shuffled options, the second with the correct choices replaced by
"none of the other options are correct", and a combination of the previous two
subsets. We carry out experiments on these datasets with both discriminative
and generative LLMs and show that these simple tricks greatly hinder the
performance of the language models. Despite their superior performance on the
original publicly available datasets, we find that all models struggle to
answer our newly constructed datasets. We show that introducing task variations
by perturbing a sizable training set can markedly improve the model's
generalisation and robustness in logical reasoning tasks. Moreover, applying
logic-driven data augmentation for fine-tuning, combined with prompting can
enhance the generalisation performance of both discriminative large language
models and generative large language models. These results offer insights into
assessing and improving the generalisation and robustness of large language
models for logical reasoning tasks. We make our source code and data publicly
available
\url{https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning}.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09431" title="Abstract">arXiv:2310.09431</a> [<a href="/pdf/2310.09431" title="Download PDF">pdf</a>, <a href="/ps/2310.09431" title="Download PostScript">ps</a>, <a href="/format/2310.09431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Superiorized Regularization of Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gibali%2C+A">Aviv Gibali</a>, 
<a href="/search/math?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Inverse problems are characterized by their inherent non-uniqueness and
sensitivity with respect to data perturbations. Their stable solution requires
the application of regularization methods including variational and iterative
regularization methods. Superiorization is a heuristic approach that can steer
basic iterative algorithms to have small value of certain regularization
functional while keeping the algorithms simplicity and computational efforts,
but is able to account for additional prior information. In this note, we
combine the superiorization methodology with iterative regularization methods
and show that the superiorized version of the scheme yields again a
regularization method, however accounting for different prior information.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09432" title="Abstract">arXiv:2310.09432</a> [<a href="/pdf/2310.09432" title="Download PDF">pdf</a>, <a href="/ps/2310.09432" title="Download PostScript">ps</a>, <a href="/format/2310.09432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing BERT-Based Visual Question Answering through Keyword-Driven  Sentence Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Napolitano%2C+D">Davide Napolitano</a>, 
<a href="/search/cs?searchtype=author&query=Vaiani%2C+L">Lorenzo Vaiani</a>, 
<a href="/search/cs?searchtype=author&query=Cagliero%2C+L">Luca Cagliero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is the technical research paper of CIKM 2023 DocIU challenges. The authors received the CIKM 2023 DocIU Winner Award, sponsored by Google, Microsoft, and the Centre for data-driven geoscience
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The Document-based Visual Question Answering competition addresses the
automatic detection of parent-child relationships between elements in
multi-page documents. The goal is to identify the document elements that answer
a specific question posed in natural language. This paper describes the
PoliTo's approach to addressing this task, in particular, our best solution
explores a text-only approach, leveraging an ad hoc sampling strategy.
Specifically, our approach leverages the Masked Language Modeling technique to
fine-tune a BERT model, focusing on sentences containing sensitive keywords
that also occur in the questions, such as references to tables or images.
Thanks to the effectiveness of this approach, we are able to achieve high
performance compared to baselines, demonstrating how our solution contributes
positively to this task.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09434" title="Abstract">arXiv:2310.09434</a> [<a href="/pdf/2310.09434" title="Download PDF">pdf</a>, <a href="/format/2310.09434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning nonlinear integral operators via Recurrent Neural Networks and  its application in solving Integro-Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bassi%2C+H">Hardeep Bassi</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuanran Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Senwei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jia Yin</a>, 
<a href="/search/cs?searchtype=author&query=Reeves%2C+C+C">Cian C. Reeves</a>, 
<a href="/search/cs?searchtype=author&query=Vlcek%2C+V">Vojtech Vlcek</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">In this paper, we propose using LSTM-RNNs (Long Short-Term Memory-Recurrent
Neural Networks) to learn and represent nonlinear integral operators that
appear in nonlinear integro-differential equations (IDEs). The LSTM-RNN
representation of the nonlinear integral operator allows us to turn a system of
nonlinear integro-differential equations into a system of ordinary differential
equations for which many efficient solvers are available. Furthermore, because
the use of LSTM-RNN representation of the nonlinear integral operator in an IDE
eliminates the need to perform a numerical integration in each numerical time
evolution step, the overall temporal cost of the LSTM-RNN-based IDE solver can
be reduced to $O(n_T)$ from $O(n_T^2)$ if a $n_T$-step trajectory is to be
computed. We illustrate the efficiency and robustness of this LSTM-RNN-based
numerical IDE solver with a model problem. Additionally, we highlight the
generalizability of the learned integral operator by applying it to IDEs driven
by different external forces. As a practical application, we show how this
methodology can effectively solve the Dyson's equation for quantum many-body
systems.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09435" title="Abstract">arXiv:2310.09435</a> [<a href="/pdf/2310.09435" title="Download PDF">pdf</a>, <a href="/format/2310.09435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Implementing Autonomous Supply Chains: a Multi-Agent System Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mak%2C+S">Stephen Mak</a>, 
<a href="/search/cs?searchtype=author&query=Minaricova%2C+M">Maria Minaricova</a>, 
<a href="/search/cs?searchtype=author&query=Brintrup%2C+A">Alexandra Brintrup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper includes 30 pages and 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Trade restrictions, the COVID-19 pandemic, and geopolitical conflicts has
significantly exposed vulnerabilities within traditional global supply chains.
These events underscore the need for organisations to establish more resilient
and flexible supply chains. To address these challenges, the concept of the
autonomous supply chain (ASC), characterised by predictive and
self-decision-making capabilities, has recently emerged as promising solution.
However, research on ASCs is relatively limited, with no existing studies on
their implementations. This paper aims to address this gap by presenting an
implementation of ASC using a multi-agent approach. It proposes a methodology
for the analysis and design of such an agent-based ASC system (A2SC). This
paper provides a concrete case study, the autonomous meat supply chain, which
showcases the practical implementation of the A2SC system using the proposed
methodology. Additionally, a system architecture and a toolkit for developing
A2SC systems are presented. Despite with limitations, this paper demonstrates a
promising approach for implementing an effective ASC system.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09436" title="Abstract">arXiv:2310.09436</a> [<a href="/pdf/2310.09436" title="Download PDF">pdf</a>, <a href="/format/2310.09436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sub-network Discovery and Soft-masking for Continual Learning of Mixed  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ke%2C+Z">Zixuan Ke</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wenhan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/ZixuanKe/PyContinual">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023 (findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Continual learning (CL) has two main objectives: preventing catastrophic
forgetting (CF) and encouraging knowledge transfer (KT). The existing
literature mainly focused on overcoming CF. Some work has also been done on KT
when the tasks are similar. To our knowledge, only one method has been proposed
to learn a sequence of mixed tasks. However, these techniques still suffer from
CF and/or limited KT. This paper proposes a new CL method to achieve both. It
overcomes CF by isolating the knowledge of each task via discovering a
subnetwork for it. A soft-masking mechanism is also proposed to preserve the
previous knowledge and to enable the new task to leverage the past knowledge to
achieve KT. Experiments using classification, generation, information
extraction, and their mixture (i.e., heterogeneous tasks) show that the
proposed method consistently outperforms strong baselines.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09438" title="Abstract">arXiv:2310.09438</a> [<a href="/pdf/2310.09438" title="Download PDF">pdf</a>, <a href="/format/2310.09438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relaxed data-consistency for limited bandwidth photoacoustic tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Obmann%2C+D">Daniel Obmann</a>, 
<a href="/search/math?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We study the effect of using weaker forms of data-fidelity terms in
generalized Tikhonov regularization accounting for model uncertainties. We show
that relaxed data-consistency conditions can be beneficial for integrating
available prior knowledge.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09440" title="Abstract">arXiv:2310.09440</a> [<a href="/pdf/2310.09440" title="Download PDF">pdf</a>, <a href="/format/2310.09440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Target Variable Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clark%2C+J">Jessica Clark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">How does the formulation of a target variable affect performance within the
ML pipeline? The experiments in this study examine numeric targets that have
been binarized by comparing against a threshold. We compare the predictive
performance of regression models trained to predict the numeric targets vs.
classifiers trained to predict their binarized counterparts. Specifically, we
make this comparison at every point of a randomized hyperparameter optimization
search to understand the effect of computational resource budget on the
tradeoff between the two. We find that regression requires significantly more
computational effort to converge upon the optimal performance, and is more
sensitive to both randomness and heuristic choices in the training process.
Although classification can and does benefit from systematic hyperparameter
tuning and model selection, the improvements are much less than for regression.
This work comprises the first systematic comparison of regression and
classification within the framework of computational resource requirements. Our
findings contribute to calls for greater replicability and efficiency within
the ML pipeline for the sake of building more sustainable and robust AI
systems.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09441" title="Abstract">arXiv:2310.09441</a> [<a href="/pdf/2310.09441" title="Download PDF">pdf</a>, <a href="/format/2310.09441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEMTRACK: A Deep Learning-Based Approach to Microrobot Tracking in Dense  and Low-Contrast Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sawhney%2C+M">Medha Sawhney</a>, 
<a href="/search/cs?searchtype=author&query=Karmarkar%2C+B">Bhas Karmarkar</a>, 
<a href="/search/cs?searchtype=author&query=Leaman%2C+E+J">Eric J. Leaman</a>, 
<a href="/search/cs?searchtype=author&query=Daw%2C+A">Arka Daw</a>, 
<a href="/search/cs?searchtype=author&query=Karpatne%2C+A">Anuj Karpatne</a>, 
<a href="/search/cs?searchtype=author&query=Behkam%2C+B">Bahareh Behkam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Biological Physics (physics.bio-ph); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Tracking microrobots is challenging, considering their minute size and high
speed. As the field progresses towards developing microrobots for biomedical
applications and conducting mechanistic studies in physiologically relevant
media (e.g., collagen), this challenge is exacerbated by the dense surrounding
environments with feature size and shape comparable to microrobots. Herein, we
report Motion Enhanced Multi-level Tracker (MEMTrack), a robust pipeline for
detecting and tracking microrobots using synthetic motion features, deep
learning-based object detection, and a modified Simple Online and Real-time
Tracking (SORT) algorithm with interpolation for tracking. Our object detection
approach combines different models based on the object's motion pattern. We
trained and validated our model using bacterial micro-motors in collagen
(tissue phantom) and tested it in collagen and aqueous media. We demonstrate
that MEMTrack accurately tracks even the most challenging bacteria missed by
skilled human annotators, achieving precision and recall of 77% and 48% in
collagen and 94% and 35% in liquid media, respectively. Moreover, we show that
MEMTrack can quantify average bacteria speed with no statistically significant
difference from the laboriously-produced manual tracking data. MEMTrack
represents a significant contribution to microrobot localization and tracking,
and opens the potential for vision-based deep learning approaches to microrobot
control in dense and low-contrast settings. All source code for training and
testing MEMTrack and reproducing the results of the paper have been made
publicly available https://github.com/sawhney-medha/MEMTrack.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09442" title="Abstract">arXiv:2310.09442</a> [<a href="/pdf/2310.09442" title="Download PDF">pdf</a>, <a href="/format/2310.09442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q">Quan Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In the context of legged robots, adaptive behavior involves adaptive
balancing and adaptive swing foot reflection. While adaptive balancing
counteracts perturbations to the robot, adaptive swing foot reflection helps
the robot to navigate intricate terrains without foot entrapment. In this
paper, we manage to bring both aspects of adaptive behavior to quadruped
locomotion by combining RL and MPC while improving the robustness and agility
of blind legged locomotion. This integration leverages MPC's strength in
predictive capabilities and RL's adeptness in drawing from past experiences.
Unlike traditional locomotion controls that separate stance foot control and
swing foot trajectory, our innovative approach unifies them, addressing their
lack of synchronization. At the heart of our contribution is the synthesis of
stance foot control with swing foot reflection, improving agility and
robustness in locomotion with adaptive behavior. A hallmark of our approach is
robust blind stair climbing through swing foot reflection. Moreover, we
intentionally designed the learning module as a general plugin for different
robot platforms. We trained the policy and implemented our approach on the
Unitree A1 robot, achieving impressive results: a peak turn rate of 8.5 rad/s,
a peak running speed of 3 m/s, and steering at a speed of 2.5 m/s. Remarkably,
this framework also allows the robot to maintain stable locomotion while
bearing an unexpected load of 10 kg, or 83\% of its body mass. We further
demonstrate the generalizability and robustness of the same policy where it
realizes zero-shot transfer to different robot platforms like Go1 and AlienGo
robots for load carrying. Code is made available for the use of the research
community at https://github.com/DRCL-USC/RL_augmented_MPC.git
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09443" title="Abstract">arXiv:2310.09443</a> [<a href="/pdf/2310.09443" title="Download PDF">pdf</a>, <a href="/format/2310.09443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G10: Enabling An Efficient Unified GPU Memory and Storage Architecture  with Smart Tensor Migrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y+E">Yirui Eric Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yuqi Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jian Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted at The 56th IEEE/ACM International Symposium on Microarchitecture (MICRO'23). *Co-primary authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">To break the GPU memory wall for scaling deep learning workloads, a variety
of architecture and system techniques have been proposed recently. Their
typical approaches include memory extension with flash memory and direct
storage access. However, these techniques still suffer from suboptimal
performance and introduce complexity to the GPU memory management, making them
hard to meet the scalability requirement of deep learning workloads today. In
this paper, we present a unified GPU memory and storage architecture named G10
driven by the fact that the tensor behaviors of deep learning workloads are
highly predictable. G10 integrates the host memory, GPU memory, and flash
memory into a unified memory space, to scale the GPU memory capacity while
enabling transparent data migrations. Based on this unified GPU memory and
storage architecture, G10 utilizes compiler techniques to characterize the
tensor behaviors in deep learning workloads. Therefore, it can schedule data
migrations in advance by considering the available bandwidth of flash memory
and host memory. The cooperative mechanism between deep learning compilers and
the unified memory architecture enables G10 to hide data transfer overheads in
a transparent manner. We implement G10 based on an open-source GPU simulator.
Our experiments demonstrate that G10 outperforms state-of-the-art GPU memory
solutions by up to 1.75$\times$, without code modifications to deep learning
workloads. With the smart data migration mechanism, G10 can reach 90.3\% of the
performance of the ideal case assuming unlimited GPU memory.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09444" title="Abstract">arXiv:2310.09444</a> [<a href="/pdf/2310.09444" title="Download PDF">pdf</a>, <a href="/format/2310.09444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Heterogeneity in Medical Federated learning via Vision  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darzi%2C+E">Erfan Darzi</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yiqing Shen</a>, 
<a href="/search/cs?searchtype=author&query=Sijtsema%2C+N+M">Nanna M. Sijtsema</a>, 
<a href="/search/cs?searchtype=author&query=van+Ooijen%2C+P+M+A">P.M.A van Ooijen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Optimization-based regularization methods have been effective in addressing
the challenges posed by data heterogeneity in medical federated learning,
particularly in improving the performance of underrepresented clients. However,
these methods often lead to lower overall model accuracy and slower convergence
rates. In this paper, we demonstrate that using Vision Transformers can
substantially improve the performance of underrepresented clients without a
significant trade-off in overall accuracy. This improvement is attributed to
the Vision transformer's ability to capture long-range dependencies within the
input data.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09447" title="Abstract">arXiv:2310.09447</a> [<a href="/pdf/2310.09447" title="Download PDF">pdf</a>, <a href="/format/2310.09447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling and resolution in sparse view photoacoustic tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>, 
<a href="/search/math?searchtype=author&query=Obmann%2C+D">Daniel Obmann</a>, 
<a href="/search/math?searchtype=author&query=Felbermayer%2C+K">Karoline Felbermayer</a>, 
<a href="/search/math?searchtype=author&query=Hinterleitner%2C+F">Florian Hinterleitner</a>, 
<a href="/search/math?searchtype=author&query=Burgholzer%2C+P">Peter Burgholzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">We investigate resolution in photoacoustic tomography (PAT). Using Shannon
theory, we investigate the theoretical resolution limit of sparse view PAT
theoretically, and empirically demonstrate that all reconstruction methods used
exceed this limit.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09448" title="Abstract">arXiv:2310.09448</a> [<a href="/pdf/2310.09448" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An integrated and flexible ultrasonic device for the continuous bladder  volume monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Toymus%2C+A+T">Alp Timucin Toymus</a>, 
<a href="/search/eess?searchtype=author&query=Yener%2C+U+C">Umut Can Yener</a>, 
<a href="/search/eess?searchtype=author&query=Bardakci%2C+E">Emine Bardakci</a>, 
<a href="/search/eess?searchtype=author&query=Temel%2C+O+D">Ozgur Deniz Temel</a>, 
<a href="/search/eess?searchtype=author&query=Koseoglu%2C+E">Ersin Koseoglu</a>, 
<a href="/search/eess?searchtype=author&query=Akcoren%2C+D">Dincay Akcoren</a>, 
<a href="/search/eess?searchtype=author&query=Eminoglu%2C+B">Burak Eminoglu</a>, 
<a href="/search/eess?searchtype=author&query=Ali%2C+M">Mohsin Ali</a>, 
<a href="/search/eess?searchtype=author&query=Tarcan%2C+T">Tufan Tarcan</a>, 
<a href="/search/eess?searchtype=author&query=Beker%2C+L">Levent Beker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Bladder volume measurement is critical for early detection and management of
lower urinary tract dysfunctions. The current gold standard is invasive, and
alternative technologies either require trained personnel or do not offer
medical grade information. Here, we report an integrated wearable ultrasonic
bladder volume monitoring (UBVM) device for accurate and autonomous continuous
monitoring of the bladder volume. The device incorporates flexible and
air-backed ultrasonic transducers and miniaturized control electronics with
wireless data transmission capability. We demonstrated the real-life
application of the device on healthy volunteers with various bladder shapes and
sizes with high accuracy. Apart from the lower urinary tract dysfunctions, the
proposed technology could also be adapted for various wearable ultrasonic
applications.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09449" title="Abstract">arXiv:2310.09449</a> [<a href="/pdf/2310.09449" title="Download PDF">pdf</a>, <a href="/format/2310.09449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pairwise Similarity Learning is SimPLE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yandong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+A">Adrian Weller</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+M+J">Michael J. Black</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ICCV 2023 (Project page: <a href="https://simple.is.tue.mpg.de/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we focus on a general yet important learning problem, pairwise
similarity learning (PSL). PSL subsumes a wide range of important applications,
such as open-set face recognition, speaker verification, image retrieval and
person re-identification. The goal of PSL is to learn a pairwise similarity
function assigning a higher similarity score to positive pairs (i.e., a pair of
samples with the same label) than to negative pairs (i.e., a pair of samples
with different label). We start by identifying a key desideratum for PSL, and
then discuss how existing methods can achieve this desideratum. We then propose
a surprisingly simple proxy-free method, called SimPLE, which requires neither
feature/proxy normalization nor angular margin and yet is able to generalize
well in open-set recognition. We apply the proposed method to three challenging
PSL tasks: open-set face recognition, image retrieval and speaker verification.
Comprehensive experimental results on large-scale benchmarks show that our
method performs significantly better than current state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09450" title="Abstract">arXiv:2310.09450</a> [<a href="/pdf/2310.09450" title="Download PDF">pdf</a>, <a href="/format/2310.09450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-intrusive Enforcement of Decentralized Stability Protocol for IBRs  in AC Microgrids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+T">Tong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript has been submitted to IEEE Transactions on Power Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents decentralized, passivity-based stability protocol for
inverter-based resources (IBRs) in AC microgrids and a non-intrusive approach
that enforces the protocol. By "non-intrusive" we mean that the approach does
not require reprogramming IBRs' controllers to enforce the stability protocol.
Implementing the approach only requires very minimal information of IBR
dynamics, and sharing such information with the non-IBR-manufacturer parties
does not cause any concerns on intellectual property privacy. Enforcing the
protocol allows for plug-and-play operation of IBRs, while maintaining
microgrid stability. The proposed method is tested by simulating two networked
microgrids with tie lines and two IBRs modeled in the electromagnetic transient
(EMT) time scale. Simulations show that oscillations with increasing amplitudes
can occur, when two stable AC microgrids are networked. Simulations also
suggest that the proposed approach can mitigate such a system-level symptom by
changing less than 2 percent of energy produced by IBRs.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09452" title="Abstract">arXiv:2310.09452</a> [<a href="/pdf/2310.09452" title="Download PDF">pdf</a>, <a href="/format/2310.09452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Aware Analyses and Algorithms for Interpolative Decompositions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Armstrong%2C+R">Robin Armstrong</a>, 
<a href="/search/math?searchtype=author&query=Buzali%2C+A">Alex Buzali</a>, 
<a href="/search/math?searchtype=author&query=Damle%2C+A">Anil Damle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Low-rank approximation is a task of critical importance in modern science,
engineering, and statistics. Many low-rank approximation algorithms, such as
the randomized singular value decomposition (RSVD), project their input matrix
into a subspace approximating the span of its leading singular vectors. Other
algorithms compress their input into a small subset of representative rows or
columns, leading to a so-called interpolative decomposition. This paper
investigates how the accuracy of interpolative decompositions is affected by
the structural properties of the input matrix being operated on, including how
these properties affect the performance comparison between interpolative
decompositions and RSVD. We also introduce a novel method of interpolative
decomposition in the form of the randomized Golub-Klema-Stewart (RGKS)
algorithm, which combines RSVD with a pivoting strategy for column subset
selection. Through numerical experiments, we find that matrix structures
including singular subspace geometry and singular spectrum decay play a
significant role in determining the performance comparison between these
different algorithms. We also prove inequalities which bound the error of a
general interpolative decomposition in terms of these matrix structures.
Lastly, we develop forms of these bounds specialized to RGKS while considering
how randomization affects the approximation error of this algorithm.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09453" title="Abstract">arXiv:2310.09453</a> [<a href="/pdf/2310.09453" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effects of Same-Race Mentorship Preferences on Academic Performance and  Survival
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Meijun Liu</a> (1), 
<a href="/search/cs?searchtype=author&query=Bu%2C+Y">Yi Bu</a> (2), 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Daifeng Li</a> (3), 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Ying Ding</a> (4), 
<a href="/search/cs?searchtype=author&query=Acuna%2C+D+E">Daniel E. Acuna</a> (5) ((1) Institute for Global Public Policy, Fudan University, (2) Department of Information Management, Peking University, (3) School of Information Management, Sun Yat-sen University, (4) School of Information, University of Texas at Austin, (5) Department of Computer Science, University of Colorado at Boulder)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 88 pages (31 main text, 45 SI, 12 extended data)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Same-race mentorship preference refers to mentors or mentees forming
connections significantly influenced by a shared race. Although racial
diversity in science has been well-studied and linked to favorable outcomes,
the extent and effects of same-race mentorship preferences remain largely
underexplored. Here, we analyze 465,355 mentor-mentee pairs from more than 60
research areas over the last 70 years to investigate the effect of same-race
mentorship preferences on mentees' academic performance and survival. We use
causal inference and statistical matching to measure same-race mentorship
preferences while accounting for racial demographic variations across
institutions, time periods, and research fields. Our findings reveal a
pervasive same-race mentorship propensity across races, fields, and
universities of varying research intensity. We observe an increase in same-race
mentorship propensity over the years, further reinforced inter-generationally
within a mentorship lineage. This propensity is more pronounced for minorities
(Asians, Blacks, and Hispanics). Our results reveal that mentees under the
supervision of mentors with high same-race propensity experience significantly
lower productivity, impact, and collaboration reach during and after training,
ultimately leading to a 27.6% reduced likelihood of remaining in academia. In
contrast, a mentorship approach devoid of racial propensity appears to offer
the best prospects for academic performance and persistence. These findings
underscore the importance of mentorship diversity for academic success and shed
light on factors contributing to minority underrepresentation in science.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09454" title="Abstract">arXiv:2310.09454</a> [<a href="/pdf/2310.09454" title="Download PDF">pdf</a>, <a href="/format/2310.09454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LgTS: Dynamic Task Sampling using LLM-generated sub-goals for  Reinforcement Learning Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shukla%2C+Y">Yash Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wenchang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Sarathy%2C+V">Vasanth Sarathy</a>, 
<a href="/search/cs?searchtype=author&query=Velasquez%2C+A">Alvaro Velasquez</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+R">Robert Wright</a>, 
<a href="/search/cs?searchtype=author&query=Sinapov%2C+J">Jivko Sinapov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advancements in reasoning abilities of Large Language Models (LLM) has
promoted their usage in problems that require high-level planning for robots
and artificial agents. However, current techniques that utilize LLMs for such
planning tasks make certain key assumptions such as, access to datasets that
permit finetuning, meticulously engineered prompts that only provide relevant
and essential information to the LLM, and most importantly, a deterministic
approach to allow execution of the LLM responses either in the form of existing
policies or plan operators. In this work, we propose LgTS (LLM-guided
Teacher-Student learning), a novel approach that explores the planning
abilities of LLMs to provide a graphical representation of the sub-goals to a
reinforcement learning (RL) agent that does not have access to the transition
dynamics of the environment. The RL agent uses Teacher-Student learning
algorithm to learn a set of successful policies for reaching the goal state
from the start state while simultaneously minimizing the number of
environmental interactions. Unlike previous methods that utilize LLMs, our
approach does not assume access to a propreitary or a fine-tuned LLM, nor does
it require pre-trained policies that achieve the sub-goals proposed by the LLM.
Through experiments on a gridworld based DoorKey domain and a search-and-rescue
inspired domain, we show that generating a graphical structure of sub-goals
helps in learning policies for the LLM proposed sub-goals and the
Teacher-Student learning algorithm minimizes the number of environment
interactions when the transition dynamics are unknown.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09458" title="Abstract">arXiv:2310.09458</a> [<a href="/pdf/2310.09458" title="Download PDF">pdf</a>, <a href="/format/2310.09458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PaintHuman: Towards High-fidelity Text-to-3D Human Texturing via  Denoised Score Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jianhui Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Loy%2C+C+C">Chen Change Loy</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+W">Weidong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wayne Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in zero-shot text-to-3D human generation, which employ the
human model prior (eg, SMPL) or Score Distillation Sampling (SDS) with
pre-trained text-to-image diffusion models, have been groundbreaking. However,
SDS may provide inaccurate gradient directions under the weak diffusion
guidance, as it tends to produce over-smoothed results and generate body
textures that are inconsistent with the detailed mesh geometry. Therefore,
directly leverage existing strategies for high-fidelity text-to-3D human
texturing is challenging. In this work, we propose a model called PaintHuman to
addresses the challenges from two aspects. We first propose a novel score
function, Denoised Score Distillation (DSD), which directly modifies the SDS by
introducing negative gradient components to iteratively correct the gradient
direction and generate high-quality textures. In addition, we use the depth map
as a geometric guidance to ensure the texture is semantically aligned to human
mesh surfaces. To guarantee the quality of rendered results, we employ
geometry-aware networks to predict surface materials and render realistic human
textures. Extensive experiments, benchmarked against state-of-the-art methods,
validate the efficacy of our approach.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09461" title="Abstract">arXiv:2310.09461</a> [<a href="/pdf/2310.09461" title="Download PDF">pdf</a>, <a href="/format/2310.09461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAC: ModAlity Calibration for Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yutian Lei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Dong Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM); Robotics (cs.RO)

</div>
<p class="mathjax">The flourishing success of Deep Neural Networks(DNNs) on RGB-input perception
tasks has opened unbounded possibilities for non-RGB-input perception tasks,
such as object detection from wireless signals, lidar scans, and infrared
images. Compared to the matured development pipeline of RGB-input (source
modality) models, developing non-RGB-input (target-modality) models from
scratch poses excessive challenges in the modality-specific network
design/training tricks and labor in the target-modality annotation. In this
paper, we propose ModAlity Calibration (MAC), an efficient pipeline for
calibrating target-modality inputs to the DNN object detection models developed
on the RGB (source) modality. We compose a target-modality-input model by
adding a small calibrator module ahead of a source-modality model and introduce
MAC training techniques to impose dense supervision on the calibrator. By
leveraging (1) prior knowledge synthesized from the source-modality model and
(2) paired {target, source} data with zero manual annotations, our
target-modality models reach comparable or better metrics than baseline models
that require 100% manual annotations. We demonstrate the effectiveness of MAC
by composing the WiFi-input, Lidar-input, and Thermal-Infrared-input models
upon the pre-trained RGB-input models respectively.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09462" title="Abstract">arXiv:2310.09462</a> [<a href="/pdf/2310.09462" title="Download PDF">pdf</a>, <a href="/format/2310.09462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Empowering Reinforcement Learning Agents with Causal  Analysis: Enhancing Automated Cryptocurrency Trading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amirzadeh%2C+R">Rasoul Amirzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Thiruvady%2C+D">Dhananjay Thiruvady</a>, 
<a href="/search/cs?searchtype=author&query=Nazari%2C+A">Asef Nazari</a>, 
<a href="/search/cs?searchtype=author&query=Ee%2C+M+S">Mong Shan Ee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite advances in artificial intelligence-enhanced trading methods,
developing a profitable automated trading system remains challenging in the
rapidly evolving cryptocurrency market. This study aims to address these
challenges by developing a reinforcement learning-based automated trading
system for five popular altcoins~(cryptocurrencies other than Bitcoin): Binance
Coin, Ethereum, Litecoin, Ripple, and Tether. To this end, we present
CausalReinforceNet, a framework framed as a decision support system. Designed
as the foundational architecture of the trading system, the CausalReinforceNet
framework enhances the capabilities of the reinforcement learning agent through
causal analysis. Within this framework, we use Bayesian networks in the feature
engineering process to identify the most relevant features with causal
relationships that influence cryptocurrency price movements. Additionally, we
incorporate probabilistic price direction signals from dynamic Bayesian
networks to enhance our reinforcement learning agent's decision-making. Due to
the high volatility of the cryptocurrency market, we design our framework to
adopt a conservative approach that limits sell and buy position sizes to manage
risk. We develop two agents using the CausalReinforceNet framework, each based
on distinct reinforcement learning algorithms. The results indicate that our
framework substantially surpasses the Buy-and-Hold benchmark strategy in
profitability. Additionally, both agents generated notable returns on
investment for Binance Coin and Ethereum.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09463" title="Abstract">arXiv:2310.09463</a> [<a href="/pdf/2310.09463" title="Download PDF">pdf</a>, <a href="/format/2310.09463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HIO-SDF: Hierarchical Incremental Online Signed Distance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vasilopoulos%2C+V">Vasileios Vasilopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Suveer Garg</a>, 
<a href="/search/cs?searchtype=author&query=Huh%2C+J">Jinwook Huh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Bhoram Lee</a>, 
<a href="/search/cs?searchtype=author&query=Isler%2C+V">Volkan Isler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A good representation of a large, complex mobile robot workspace must be
space-efficient yet capable of encoding relevant geometric details. When
exploring unknown environments, it needs to be updatable incrementally in an
online fashion. We introduce HIO-SDF, a new method that represents the
environment as a Signed Distance Field (SDF). State of the art representations
of SDFs are based on either neural networks or voxel grids. Neural networks are
capable of representing the SDF continuously. However, they are hard to update
incrementally as neural networks tend to forget previously observed parts of
the environment unless an extensive sensor history is stored for training.
Voxel-based representations do not have this problem but they are not
space-efficient especially in large environments with fine details. HIO-SDF
combines the advantages of these representations using a hierarchical approach
which employs a coarse voxel grid that captures the observed parts of the
environment together with high-resolution local information to train a neural
network. HIO-SDF achieves a 46% lower mean global SDF error across all test
scenes than a state of the art continuous representation, and a 30% lower error
than a discrete representation at the same resolution as our coarse global SDF
grid.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09466" title="Abstract">arXiv:2310.09466</a> [<a href="/pdf/2310.09466" title="Download PDF">pdf</a>, <a href="/ps/2310.09466" title="Download PostScript">ps</a>, <a href="/format/2310.09466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Anti-jamming Communications with DMA-Based Reconfigurable  Heterogeneous Array
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaizhi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Liang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaoling Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In the future commercial and military communication systems, anti-jamming
remains a critical issue. Existing homogeneous or heterogeneous arrays with a
limited degrees of freedom (DoF) and high consumption are unable to meet the
requirements of communication in rapidly changing and intense jamming
environments. To address these challenges, we propose a reconfigurable
heterogeneous array (RHA) architecture based on dynamic metasurface antenna
(DMA), which will increase the DoF and further improve anti-jamming
capabilities. We propose a two-step anti-jamming scheme based on RHA, where the
multipaths are estimated by an atomic norm minimization (ANM) based scheme, and
then the received signal-to-interference-plus-noise ratio (SINR) is maximized
by jointly designing the phase shift of each DMA element and the weights of the
array elements. To solve the challenging non-convex discrete fractional problem
along with the estimation error in the direction of arrival (DoA) and channel
state information (CSI), we propose a robust alternative algorithm based on the
S-procedure to solve the lower-bound SINR maximization problem. Simulation
results demonstrate that the proposed RHA architecture and corresponding
schemes have superior performance in terms of jamming immunity and robustness.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09469" title="Abstract">arXiv:2310.09469</a> [<a href="/pdf/2310.09469" title="Download PDF">pdf</a>, <a href="/format/2310.09469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards More Accurate Diffusion Model Acceleration with A Timestep  Aligner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Mengfei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yujun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+C">Changsong Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+R">Ran Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Deli Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong-jin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A diffusion model, which is formulated to produce an image using thousands of
denoising steps, usually suffers from a slow inference speed. Existing
acceleration algorithms simplify the sampling by skipping most steps yet
exhibit considerable performance degradation. By viewing the generation of
diffusion models as a discretized integrating process, we argue that the
quality drop is partly caused by applying an inaccurate integral direction to a
timestep interval. To rectify this issue, we propose a timestep aligner that
helps find a more accurate integral direction for a particular interval at the
minimum cost. Specifically, at each denoising step, we replace the original
parameterization by conditioning the network on a new timestep, which is
obtained by aligning the sampling distribution to the real distribution.
Extensive experiments show that our plug-in design can be trained efficiently
and boost the inference performance of various state-of-the-art acceleration
methods, especially when there are few denoising steps. For example, when using
10 denoising steps on the popular LSUN Bedroom dataset, we improve the FID of
DDIM from 9.65 to 6.07, simply by adopting our method for a more appropriate
set of timesteps. Code will be made publicly available.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09470" title="Abstract">arXiv:2310.09470</a> [<a href="/pdf/2310.09470" title="Download PDF">pdf</a>, <a href="/format/2310.09470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Aware Ergodic Search: Continuous Exploration for Multi-Agent  Systems with Battery Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seewald%2C+A">Adam Seewald</a>, 
<a href="/search/cs?searchtype=author&query=Lerch%2C+C+J">Cameron J. Lerch</a>, 
<a href="/search/cs?searchtype=author&query=Chanc%C3%A1n%2C+M">Marvin Chanc&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Dollar%2C+A+M">Aaron M. Dollar</a>, 
<a href="/search/cs?searchtype=author&query=Abraham%2C+I">Ian Abraham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, submitted to ICRA'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Autonomous exploration without interruption is important in scenarios such as
search and rescue and precision agriculture, where consistent presence is
needed to detect events over large areas. Ergodic search already derives
continuous coverage trajectories in these scenarios so that a robot spends more
time in areas with high information density. However, existing literature on
ergodic search does not consider the robot's energy constraints, limiting how
long a robot can explore. In fact, if the robots are battery-powered, it is
physically not possible to continuously explore on a single battery charge. Our
paper tackles this challenge by integrating ergodic search methods with
energy-aware coverage. We trade off battery usage and coverage quality,
maintaining uninterrupted exploration of a given space by at least one agent.
Our approach derives an abstract battery model for future state-of-charge
estimation and extends canonical ergodic search to ergodic search under battery
constraints. Empirical data from simulations and real-world experiments
demonstrate the effectiveness of our energy-aware ergodic search, which ensures
continuous and uninterrupted exploration and guarantees spatial coverage.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09471" title="Abstract">arXiv:2310.09471</a> [<a href="/pdf/2310.09471" title="Download PDF">pdf</a>, <a href="/format/2310.09471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plug-and-Play Feature Generation for Few-Shot Medical Image  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qianyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Huifang Du</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xing Jia</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shuyong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Y">Yan Teng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haofen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages,5 figures, Accepted to BIBM2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Few-shot learning (FSL) presents immense potential in enhancing model
generalization and practicality for medical image classification with limited
training data; however, it still faces the challenge of severe overfitting in
classifier training due to distribution bias caused by the scarce training
samples. To address the issue, we propose MedMFG, a flexible and lightweight
plug-and-play method designed to generate sufficient class-distinctive features
from limited samples. Specifically, MedMFG first re-represents the limited
prototypes to assign higher weights for more important information features.
Then, the prototypes are variationally generated into abundant effective
features. Finally, the generated features and prototypes are together to train
a more generalized classifier. Experiments demonstrate that MedMFG outperforms
the previous state-of-the-art methods on cross-domain benchmarks involving the
transition from natural images to medical images, as well as medical images
with different lesions. Notably, our method achieves over 10% performance
improvement compared to several baselines. Fusion experiments further validate
the adaptability of MedMFG, as it seamlessly integrates into various backbones
and baselines, consistently yielding improvements of over 2.9% across all
results.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09473" title="Abstract">arXiv:2310.09473</a> [<a href="/pdf/2310.09473" title="Download PDF">pdf</a>, <a href="/format/2310.09473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can CNNs Accurately Classify Human Emotions? A Deep-Learning Facial  Expression Recognition Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+A+J">Ashley Jisue Hong</a>, 
<a href="/search/cs?searchtype=author&query=DiStefano%2C+D">David DiStefano</a>, 
<a href="/search/cs?searchtype=author&query=Dua%2C+S">Sejal Dua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Emotional Artificial Intelligences are currently one of the most anticipated
developments of AI. If successful, these AIs will be classified as one of the
most complex, intelligent nonhuman entities as they will possess sentience, the
primary factor that distinguishes living humans and mechanical machines. For
AIs to be classified as "emotional," they should be able to empathize with
others and classify their emotions because without such abilities they cannot
normally interact with humans. This study investigates the CNN model's ability
to recognize and classify human facial expressions (positive, neutral,
negative). The CNN model made for this study is programmed in Python and
trained with preprocessed data from the Chicago Face Database. The model is
intentionally designed with less complexity to further investigate its ability.
We hypothesized that the model will perform better than chance (33.3%) in
classifying each emotion class of input data. The model accuracy was tested
with novel images. Accuracy was summarized in a percentage report, comparative
plot, and confusion matrix. Results of this study supported the hypothesis as
the model had 75% accuracy over 10,000 images (data), highlighting the
possibility of AIs that accurately analyze human emotions and the prospect of
viable Emotional AIs.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09474" title="Abstract">arXiv:2310.09474</a> [<a href="/pdf/2310.09474" title="Download PDF">pdf</a>, <a href="/format/2310.09474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extremum seeking in the presence of large delays via time-delay approach  to averaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+X">Xuefei Yang</a>, 
<a href="/search/eess?searchtype=author&query=Fridman%2C+E">Emilia Fridman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we study gradient-based classical extremum seeking (ES) for
uncertain n-dimensional (nD) static quadratic maps in the presence of known
large constant distinct input delays and large output constant delay with a
small time-varying uncertainty. This uncertainty may appear due to
network-based measurements. We present a quantitative analysis via a time-delay
approach to averaging. We assume that the Hessian has a nominal known part and
norm-bounded uncertainty, the extremum point belongs to a known box, whereas
the extremum value to a known interval. By using the orthogonal transformation,
we first transform the original static quadratic map into a new one with the
Hessian containing a nominal diagonal part. We apply further a time-delay
transformation to the resulting ES system and arrive at a time-delay system,
which is a perturbation of a linear time-delay system with constant
coefficients. Given large delays, we choose appropriate gains to guarantee
stability of this linear system. To find a lower bound on the dither frequency
for practical stability, we employ variation of constants formula and exploit
the delay-dependent positivity of the fundamental solutions of the linear
system with their tight exponential bounds. Sampled-data ES in the presence of
large distinct input delays is also presented. Explicit conditions in terms of
simple scalar inequalities depending on tuning parameters and delay bounds are
established to guarantee the practical stability of the ES control systems. We
show that given any large delays and initial box, by choosing appropriate gains
we can achieve practical stability for fast enough dithers and small enough
uncertainties.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09478" title="Abstract">arXiv:2310.09478</a> [<a href="/pdf/2310.09478" title="Download PDF">pdf</a>, <a href="/format/2310.09478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiniGPT-v2: large language model as a unified interface for  vision-language multi-task learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Deyao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiaoqian Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zechun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengchuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamoorthi%2C+R">Raghuraman Krishnamoorthi</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+V">Vikas Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yunyang Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Elhoseiny%2C+M">Mohamed Elhoseiny</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large language models have shown their remarkable capabilities as a general
interface for various language-related applications. Motivated by this, we
target to build a unified interface for completing many vision-language tasks
including image description, visual question answering, and visual grounding,
among others. The challenge is to use a single model for performing diverse
vision-language tasks effectively with simple multi-modal instructions. Towards
this objective, we introduce MiniGPT-v2, a model that can be treated as a
unified interface for better handling various vision-language tasks. We propose
using unique identifiers for different tasks when training the model. These
identifiers enable our model to better distinguish each task instruction
effortlessly and also improve the model learning efficiency for each task.
After the three-stage training, the experimental results show that MiniGPT-v2
achieves strong performance on many visual question-answering and visual
grounding benchmarks compared to other vision-language generalist models. Our
model and codes are available at https://minigpt-v2.github.io/
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09479" title="Abstract">arXiv:2310.09479</a> [<a href="/pdf/2310.09479" title="Download PDF">pdf</a>, <a href="/format/2310.09479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified High-binding Watermark for Unconditional Image Generation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">Ruinan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Y">Yu-an Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shangbo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yajie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep learning techniques have implemented many unconditional image generation
(UIG) models, such as GAN, Diffusion model, etc. The extremely realistic images
(also known as AI-Generated Content, AIGC for short) produced by these models
bring urgent needs for intellectual property protection such as data
traceability and copyright certification. An attacker can steal the output
images of the target model and use them as part of the training data to train a
private surrogate UIG model. The implementation mechanisms of UIG models are
diverse and complex, and there is no unified and effective protection and
verification method at present. To address these issues, we propose a two-stage
unified watermark verification mechanism with high-binding effects for such
models. In the first stage, we use an encoder to invisibly write the watermark
image into the output images of the original AIGC tool, and reversely extract
the watermark image through the corresponding decoder. In the second stage, we
design the decoder fine-tuning process, and the fine-tuned decoder can make
correct judgments on whether the suspicious model steals the original AIGC tool
data. Experiments demonstrate our method can complete the verification work
with almost zero false positive rate under the condition of only using the
model output images. Moreover, the proposed method can achieve data steal
verification across different types of UIG models, which further increases the
practicality of the method.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09480" title="Abstract">arXiv:2310.09480</a> [<a href="/pdf/2310.09480" title="Download PDF">pdf</a>, <a href="/format/2310.09480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesis of Event-triggered Controllers for SIRS Epidemic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ding%2C+L">Lichen Ding</a>, 
<a href="/search/eess?searchtype=author&query=Hashimoto%2C+K">Kazumune Hashimoto</a>, 
<a href="/search/eess?searchtype=author&query=Takai%2C+S">Shigemasa Takai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted for publication in Nonlinear Analysis: Hybrid Systems (NAHS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we investigate the problem of mitigating epidemics by applying
an event-triggered control strategy. We consider a
susceptible-infected-removed-susceptible (SIRS) model, which builds upon the
foundational SIR model by accounting for reinfection cases. The event-triggered
control strategy is formulated based on the condition in which the control
input (e.g., the level of public measures) is updated only when the fraction of
the infected subjects changes over a designed threshold. To synthesize the
event-triggered controller, we leverage the notion of a symbolic model, which
represents an abstracted expression of the transition system associated with
the SIRS model under the event-triggered control strategy. %The symbolic model
is constructed based on an approximate alternating simulation relation. Then,
by employing safety and reachability games, two event-triggered controllers are
synthesized to ensure a desired specification, which is more sophisticated than
the one given in previous works. The effectiveness of the proposed approach is
illustrated through numerical simulations.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09483" title="Abstract">arXiv:2310.09483</a> [<a href="/pdf/2310.09483" title="Download PDF">pdf</a>, <a href="/ps/2310.09483" title="Download PostScript">ps</a>, <a href="/format/2310.09483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sorting and Selection in Rounds with Adversarial Comparisons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trevisan%2C+C">Christopher Trevisan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Symposium on Discrete Algorithms (SODA) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We continue the study of selection and sorting of $n$ numbers under the
adversarial comparator model, where comparisons can be adversarially tampered
with if the arguments are sufficiently close.
<br />We derive a randomized sorting algorithm that does $O(n \log^2 n)$
comparisons and gives a correct answer with high probability, addressing an
open problem of Ajtai, Feldman, Hassadim, and Nelson [AFHN15]. Our algorithm
also implies a selection algorithm that does $O(n \log n)$ comparisons and
gives a correct answer with high probability. Both of these results are a
$\log$ factor away from the naive lower bound. [AFHN15] shows an
$\Omega(n^{1+\varepsilon})$ lower bound for both sorting and selection in the
deterministic case, so our results also prove a discrepancy between what is
possible with deterministic and randomized algorithms in this setting.
<br />We also consider both sorting and selection in rounds, exploring the tradeoff
between accuracy, number of comparisons, and number of rounds. Using results
from sorting networks, we give general algorithms for sorting in $d$ rounds
where the number of comparisons increases with $d$ and the accuracy decreases
with $d$. Using these algorithms, we derive selection algorithms in $d+O(\log
d)$ rounds that use the same number of comparisons as the corresponding sorting
algorithm, but have a constant accuracy. Notably, this gives selection
algorithms in $d$ rounds that use $n^{1 + o(1)}$ comparisons and have constant
accuracy for all $d = \omega(1)$, which still beats the deterministic lower
bound of $\Omega(n^{1+\varepsilon})$.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09484" title="Abstract">arXiv:2310.09484</a> [<a href="/pdf/2310.09484" title="Download PDF">pdf</a>, <a href="/format/2310.09484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Design Space of Diffusion Autoencoders for Face Morphing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blasingame%2C+Z">Zander Blasingame</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chen Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Initial pre-print. arXiv admin note: text overlap with <a href="/abs/2301.04218">arXiv:2301.04218</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Face morphs created by Diffusion Autoencoders are a recent innovation and the
design space of such an approach has not been well explored. We explore three
axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM
solver, and 3) partial sampling through small amounts of added noise.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09485" title="Abstract">arXiv:2310.09485</a> [<a href="/pdf/2310.09485" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applying Bayesian Ridge Regression AI Modeling in Virus Severity  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pal%2C+J">Jai Pal</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+B">Bryan Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 Pages, 7 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Artificial intelligence (AI) is a powerful tool for reshaping healthcare
systems. In healthcare, AI is invaluable for its capacity to manage vast
amounts of data, which can lead to more accurate and speedy diagnoses,
ultimately easing the workload on healthcare professionals. As a result, AI has
proven itself to be a power tool across various industries, simplifying complex
tasks and pattern recognition that would otherwise be overwhelming for humans
or traditional computer algorithms. In this paper, we review the strengths and
weaknesses of Bayesian Ridge Regression, an AI model that can be used to bring
cutting edge virus analysis to healthcare professionals around the world. The
model's accuracy assessment revealed promising results, with room for
improvement primarily related to data organization. In addition, the severity
index serves as a valuable tool to gain a broad overview of patient care needs,
aligning with healthcare professionals' preference for broader categorizations.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09486" title="Abstract">arXiv:2310.09486</a> [<a href="/pdf/2310.09486" title="Download PDF">pdf</a>, <a href="/format/2310.09486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mirage: Model-Agnostic Graph Distillation for Graph Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M">Mridul Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Manchanda%2C+S">Sahil Manchanda</a>, 
<a href="/search/cs?searchtype=author&query=Ranu%2C+S">Sayan Ranu</a>, 
<a href="/search/cs?searchtype=author&query=Kodamana%2C+H">Hariprasad Kodamana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">GNNs, like other deep learning models, are data and computation hungry. There
is a pressing need to scale training of GNNs on large datasets to enable their
usage on low-resource environments. Graph distillation is an effort in that
direction with the aim to construct a smaller synthetic training set from the
original training data without significantly compromising model performance.
While initial efforts are promising, this work is motivated by two key
observations: (1) Existing graph distillation algorithms themselves rely on
training with the full dataset, which undermines the very premise of graph
distillation. (2) The distillation process is specific to the target GNN
architecture and hyper-parameters and thus not robust to changes in the
modeling pipeline. We circumvent these limitations by designing a distillation
algorithm called Mirage for graph classification. Mirage is built on the
insight that a message-passing GNN decomposes the input graph into a multiset
of computation trees. Furthermore, the frequency distribution of computation
trees is often skewed in nature, enabling us to condense this data into a
concise distilled summary. By compressing the computation data itself, as
opposed to emulating gradient flows on the original training set-a prevalent
approach to date-Mirage transforms into an unsupervised and
architecture-agnostic distillation algorithm. Extensive benchmarking on
real-world datasets underscores Mirage's superiority, showcasing enhanced
generalization accuracy, data compression, and distillation efficiency when
compared to state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09492" title="Abstract">arXiv:2310.09492</a> [<a href="/pdf/2310.09492" title="Download PDF">pdf</a>, <a href="/format/2310.09492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perception Reinforcement Using Auxiliary Learning Feature Fusion: A  Modified Yolov8 for Head Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiezhou Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guankun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weixiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+X">Xiaopin Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yibin Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">ZongZe Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Head detection provides distribution information of pedestrian, which is
crucial for scene statistical analysis, traffic management, and risk assessment
and early warning. However, scene complexity and large-scale variation in the
real world make accurate detection more difficult. Therefore, we present a
modified Yolov8 which improves head detection performance through reinforcing
target perception. An Auxiliary Learning Feature Fusion (ALFF) module comprised
of LSTM and convolutional blocks is used as the auxiliary task to help the
model perceive targets. In addition, we introduce Noise Calibration into
Distribution Focal Loss to facilitate model fitting and improve the accuracy of
detection. Considering the requirements of high accuracy and speed for the head
detection task, our method is adapted with two kinds of backbone, namely
Yolov8n and Yolov8m. The results demonstrate the superior performance of our
approach in improving detection accuracy and robustness.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09494" title="Abstract">arXiv:2310.09494</a> [<a href="/pdf/2310.09494" title="Download PDF">pdf</a>, <a href="/format/2310.09494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational analyses of linguistic features with schizophrenic and  autistic traits along with formal thought disorders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saga%2C+T">Takeshi Saga</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+H">Hiroki Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+S">Satoshi Nakamura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a revised version of the ICMI2023 paper with the same title
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 25th International Conference on Multimodal
  Interaction (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">[See full abstract in the pdf] Formal Thought Disorder (FTD), which is a
group of symptoms in cognition that affects language and thought, can be
observed through language. FTD is seen across such developmental or psychiatric
disorders as Autism Spectrum Disorder (ASD) or Schizophrenia, and its related
Schizotypal Personality Disorder (SPD). This paper collected a Japanese
audio-report dataset with score labels related to ASD and SPD through a
crowd-sourcing service from the general population. We measured language
characteristics with the 2nd edition of the Social Responsiveness Scale (SRS2)
and the Schizotypal Personality Questionnaire (SPQ), including an odd speech
subscale from SPQ to quantify the FTD symptoms. We investigated the following
four research questions through machine-learning-based score predictions: (RQ1)
How are schizotypal and autistic measures correlated? (RQ2) What is the most
suitable task to elicit FTD symptoms? (RQ3) Does the length of speech affect
the elicitation of FTD symptoms? (RQ4) Which features are critical for
capturing FTD symptoms? We confirmed that an FTD-related subscale, odd speech,
was significantly correlated with both the total SPQ and SRS scores, although
they themselves were not correlated significantly. Our regression analysis
indicated that longer speech about a negative memory elicited more FTD
symptoms. The ablation study confirmed the importance of function words and
both the abstract and temporal features for FTD-related odd speech estimation.
In contrast, content words were effective only in the SRS predictions, and
content words were effective only in the SPQ predictions, a result that implies
the differences between SPD-like and ASD-like symptoms. Data and programs used
in this paper can be found here:
https://sites.google.com/view/sagatake/resource.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09495" title="Abstract">arXiv:2310.09495</a> [<a href="/pdf/2310.09495" title="Download PDF">pdf</a>, <a href="/format/2310.09495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning In-between Imagery Dynamics via Physical Latent Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jihun Han</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yoonsang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Gelb%2C+A">Anne Gelb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">We present a framework designed to learn the underlying dynamics between two
images observed at consecutive time steps. The complex nature of image data and
the lack of temporal information pose significant challenges in capturing the
unique evolving patterns. Our proposed method focuses on estimating the
intermediary stages of image evolution, allowing for interpretability through
latent dynamics while preserving spatial correlations with the image. By
incorporating a latent variable that follows a physical model expressed in
partial differential equations (PDEs), our approach ensures the
interpretability of the learned model and provides insight into corresponding
image dynamics. We demonstrate the robustness and effectiveness of our learning
framework through a series of numerical tests using geoscientific imagery data.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09497" title="Abstract">arXiv:2310.09497</a> [<a href="/pdf/2310.09497" title="Download PDF">pdf</a>, <a href="/format/2310.09497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking  with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+S">Shengyao Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+H">Honglei Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Koopman%2C+B">Bevan Koopman</a>, 
<a href="/search/cs?searchtype=author&query=Zuccon%2C+G">Guido Zuccon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) demonstrate impressive effectiveness in
zero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting
approaches have been proposed for LLM-based zero-shot ranking. Our study begins
by thoroughly evaluating these existing approaches within a consistent
experimental framework, considering factors like model size, token consumption,
latency, among others. This first-of-its-kind comparative evaluation of these
approaches allows us to identify the trade-offs between effectiveness and
efficiency inherent in each approach. We find that while Pointwise approaches
score high on efficiency, they suffer from poor effectiveness. Conversely,
Pairwise approaches demonstrate superior effectiveness but incur high
computational overhead. To further enhance the efficiency of LLM-based
zero-shot ranking, we propose a novel Setwise prompting approach. Our approach
reduces the number of LLM inferences and the amount of prompt token consumption
during the ranking procedure, significantly improving the efficiency of
LLM-based zero-shot ranking. We test our method using the TREC DL datasets and
the BEIR zero-shot document ranking benchmark. The empirical results indicate
that our approach considerably reduces computational costs while also retaining
high zero-shot ranking effectiveness.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09499" title="Abstract">arXiv:2310.09499</a> [<a href="/pdf/2310.09499" title="Download PDF">pdf</a>, <a href="/format/2310.09499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+H">Hang Shao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yanmin Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Various Large Language Models(LLMs) from the Generative Pretrained
Transformer~(GPT) family have achieved outstanding performances in a wide range
of text generation tasks. However, the enormous model sizes have hindered their
practical use in real-world applications due to high inference latency.
Therefore, improving the efficiencies of LLMs through quantization, pruning,
and other means has been a key issue in LLM studies. In this work, we propose a
method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs
to at least 50\% sparsity without the need of any retraining. It allocates
sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced
error while maintaining the overall sparsity level. The advantages of the
proposed method exhibit even more when the sparsity is extremely high.
Furthermore, our method is compatible with quantization, enabling further
compression of LLMs.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09501" title="Abstract">arXiv:2310.09501</a> [<a href="/pdf/2310.09501" title="Download PDF">pdf</a>, <a href="/format/2310.09501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DepNeCTI: Dependency-based Nested Compound Type Identification for  Sanskrit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sandhan%2C+J">Jivnesh Sandhan</a>, 
<a href="/search/cs?searchtype=author&query=Narsupalli%2C+Y">Yaswanth Narsupalli</a>, 
<a href="/search/cs?searchtype=author&query=Muppirala%2C+S">Sreevatsa Muppirala</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+S">Sriram Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Satuluri%2C+P">Pavankumar Satuluri</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Amba Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+P">Pawan Goyal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Pages, Camera-ready version accepted at EMNLP23 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multi-component compounding is a prevalent phenomenon in Sanskrit, and
understanding the implicit structure of a compound's components is crucial for
deciphering its meaning. Earlier approaches in Sanskrit have focused on binary
compounds and neglected the multi-component compound setting. This work
introduces the novel task of nested compound type identification (NeCTI), which
aims to identify nested spans of a multi-component compound and decode the
implicit semantic relations between them. To the best of our knowledge, this is
the first attempt in the field of lexical semantics to propose this task.
<br />We present 2 newly annotated datasets including an out-of-domain dataset for
this task. We also benchmark these datasets by exploring the efficacy of the
standard problem formulations such as nested named entity recognition,
constituency parsing and seq2seq, etc. We present a novel framework named
DepNeCTI: Dependency-based Nested Compound Type Identifier that surpasses the
performance of the best baseline with an average absolute improvement of 13.1
points F1-score in terms of Labeled Span Score (LSS) and a 5-fold enhancement
in inference efficiency. In line with the previous findings in the binary
Sanskrit compound identification task, context provides benefits for the NeCTI
task. The codebase and datasets are publicly available at:
https://github.com/yaswanth-iitkgp/DepNeCTI
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09502" title="Abstract">arXiv:2310.09502</a> [<a href="/pdf/2310.09502" title="Download PDF">pdf</a>, <a href="/ps/2310.09502" title="Download PostScript">ps</a>, <a href="/format/2310.09502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Nonlinear Adaptive Control for Unmanned Aerial Systems Operating  under Dynamic Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lamb%2C+Z">Zachary Lamb</a>, 
<a href="/search/eess?searchtype=author&query=Bell%2C+Z+I">Zachary I. Bell</a>, 
<a href="/search/eess?searchtype=author&query=Longmire%2C+M">Matthew Longmire</a>, 
<a href="/search/eess?searchtype=author&query=Paquet%2C+J">Jared Paquet</a>, 
<a href="/search/eess?searchtype=author&query=Ganesh%2C+P">Prashant Ganesh</a>, 
<a href="/search/eess?searchtype=author&query=Sanfelice%2C+R">Ricardo Sanfelice</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Recent literature in the field of machine learning (ML) control has shown
promising theoretical results for a Deep Neural Network (DNN) based Nonlinear
Adaptive Controller (DNAC) capable of achieving trajectory tracking for
nonlinear systems. Expanding on this work, this paper applies DNAC to the
Attitude Control System (ACS) of a quadrotor and shows improvement to attitude
control performance under disturbed flying conditions where the model
uncertainty is high. Moreover, these results are noteworthy for ML control
because they were achieved with no prior training data and an arbitrary system
dynamics initialization; simply put, the controller presented in this paper is
practically modelless, yet yields the ability to force trajectory tracking for
nonlinear systems while rejecting significant undesirable model disturbances
learned through a DNN. The combination of ML techniques to learn a system's
dynamics and the Lyapunov analysis required to provide stability guarantees
leads to a controller with applications in safety-critical systems that may
undergo uncertain model changes, as is the case for most aerial systems.
Experimental findings are analyzed in the final section of this paper, and DNAC
is shown to outperform the trajectory tracking capabilities of PID, MRAC, and
the recently developed Deep Model Reference Adaptive Control (DMRAC) schemes.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09503" title="Abstract">arXiv:2310.09503</a> [<a href="/pdf/2310.09503" title="Download PDF">pdf</a>, <a href="/format/2310.09503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JM3D &amp; JM3D-LLM: Elevating 3D Representation with Joint Multi-modal Cues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiayi Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Changli Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yiwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoshuai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The rising importance of 3D representation learning, pivotal in computer
vision, autonomous driving, and robotics, is evident. However, a prevailing
trend, which straightforwardly resorted to transferring 2D alignment strategies
to the 3D domain, encounters three distinct challenges: (1) Information
Degradation: This arises from the alignment of 3D data with mere single-view 2D
images and generic texts, neglecting the need for multi-view images and
detailed subcategory texts. (2) Insufficient Synergy: These strategies align 3D
representations to image and text features individually, hampering the overall
optimization for 3D models. (3) Underutilization: The fine-grained information
inherent in the learned representations is often not fully exploited,
indicating a potential loss in detail. To address these issues, we introduce
JM3D, a comprehensive approach integrating point cloud, text, and image. Key
contributions include the Structured Multimodal Organizer (SMO), enriching
vision-language representation with multiple views and hierarchical text, and
the Joint Multi-modal Alignment (JMA), combining language understanding with
visual representation. Our advanced model, JM3D-LLM, marries 3D representation
with large language models via efficient fine-tuning. Evaluations on ModelNet40
and ScanObjectNN establish JM3D's superiority. The superior performance of
JM3D-LLM further underscores the effectiveness of our representation transfer
approach. Our code and models are available at https://github.com/Mr-Neko/JM3D.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09504" title="Abstract">arXiv:2310.09504</a> [<a href="/pdf/2310.09504" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Node Dissimilarity Index for Complex Network Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meghanathan%2C+N">Natarajan Meghanathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">We propose a principal component analysis (PCA)-based approach to quantify
(the node dissimilarity index, NDI) the extent of dissimilarity among nodes in
a network with respect to values incurred for a suite of node-level metrics
(like centrality metrics). We subject the dataset (n nodes and their values
incurred for four commonly studied centrality metrics: degree, eigenvector,
betweenness and closeness) to PCA and retain the m ( &lt;= 4) principal components
(with variance &gt;= 1.0). We construct an n-node dissimilarity matrix whose
entries are the absolute difference (if m = 1) or Euclidean distance (if M &gt; 1)
of the principal component coordinates of the corresponding nodes. We compute
NDI (&gt;= 1.0) to be the ratio of the principal Eigenvalue of the node
dissimilarity matrix and average of entries in the node dissimilarity matrix.
The larger the NDI, the greater the dissimilarity among the node-level metrics
(centrality metrics) values considered for analysis.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09505" title="Abstract">arXiv:2310.09505</a> [<a href="/pdf/2310.09505" title="Download PDF">pdf</a>, <a href="/format/2310.09505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Test-Time Adaptation for Acoustic Foundation Models in  Open-World Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongfu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hengguan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution
shifts during inference, especially in visual recognition tasks. However, while
acoustic models face similar challenges due to distribution shifts in test-time
speech, TTA techniques specifically designed for acoustic modeling in the
context of open-world data shifts remain scarce. This gap is further
exacerbated when considering the unique characteristics of acoustic foundation
models: 1) they are primarily built on transformer architectures with layer
normalization and 2) they deal with test-time speech data of varying lengths in
a non-stationary manner. These aspects make the direct application of
vision-focused TTA methods, which are mostly reliant on batch normalization and
assume independent samples, infeasible. In this paper, we delve into TTA for
pre-trained acoustic models facing open-world data shifts. We find that noisy,
high-entropy speech frames, often non-silent, carry key semantic content.
Traditional TTA methods might inadvertently filter out this information using
potentially flawed heuristics. In response, we introduce a heuristic-free,
learning-based adaptation enriched by confidence enhancement. Noting that
speech signals' short-term consistency, we also apply consistency
regularization during test-time optimization. Our experiments on synthetic and
real-world datasets affirm our method's superiority over existing baselines.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09506" title="Abstract">arXiv:2310.09506</a> [<a href="/pdf/2310.09506" title="Download PDF">pdf</a>, <a href="/format/2310.09506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Semantic Communication Protocols for 6G: From Protocol Learning  to Language-Oriented Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jihong Park</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+S">Seung-Woo Ko</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jinho Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seong-Lyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 13 figures, submitted to IEEE BITS the Information Theory Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The forthcoming 6G systems are expected to address a wide range of
non-stationary tasks. This poses challenges to traditional medium access
control (MAC) protocols that are static and predefined. In response,
data-driven MAC protocols have recently emerged, offering ability to tailor
their signaling messages for specific tasks. This article presents a novel
categorization of these data-driven MAC protocols into three levels: Level 1
MAC. task-oriented neural protocols constructed using multi-agent deep
reinforcement learning (MADRL); Level 2 MAC. neural network-oriented symbolic
protocols developed by converting Level 1 MAC outputs into explicit symbols;
and Level 3 MAC. language-oriented semantic protocols harnessing large language
models (LLMs) and generative models. With this categorization, we aim to
explore the opportunities and challenges of each level by delving into their
foundational techniques. Drawing from information theory and associated
principles as well as selected case studies, this study provides insights into
the trajectory of data-driven MAC protocols and sheds light on future research
directions.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09507" title="Abstract">arXiv:2310.09507</a> [<a href="/pdf/2310.09507" title="Download PDF">pdf</a>, <a href="/format/2310.09507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust  Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">DongAo Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jiaxuan Pang</a>, 
<a href="/search/cs?searchtype=author&query=Gotway%2C+M+B">Michael B. Gotway</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jianming Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Best Paper Award Runner-Up at Medical Image Computing and Computer Assisted Intervention (MICCAI) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep learning nowadays offers expert-level and sometimes even
super-expert-level performance, but achieving such performance demands massive
annotated data for training (e.g., Google's proprietary CXR Foundation Model
(CXR-FM) was trained on 821,544 labeled and mostly private chest X-rays
(CXRs)). Numerous datasets are publicly available in medical imaging but
individually small and heterogeneous in expert labels. We envision a powerful
and robust foundation model that can be trained by aggregating numerous small
public datasets. To realize this vision, we have developed Ark, a framework
that accrues and reuses knowledge from heterogeneous expert annotations in
various datasets. As a proof of concept, we have trained two Ark models on
335,484 and 704,363 CXRs, respectively, by merging several datasets including
ChestX-ray14, CheXpert, MIMIC-II, and VinDr-CXR, evaluated them on a wide range
of imaging tasks covering both classification and segmentation via fine-tuning,
linear-probing, and gender-bias analysis, and demonstrated our Ark's superior
and robust performance over the SOTA fully/self-supervised baselines and
Google's proprietary CXR-FM. This enhanced performance is attributed to our
simple yet powerful observation that aggregating numerous public datasets
diversifies patient populations and accrues knowledge from diverse experts,
yielding unprecedented performance yet saving annotation cost. With all codes
and pretrained models released at GitHub.com/JLiangLab/Ark, we hope that Ark
exerts an important impact on open science, as accruing and reusing knowledge
from expert annotations in public datasets can potentially surpass the
performance of proprietary models trained on unusually large data, inspiring
many more researchers worldwide to share codes and datasets to build open
foundation models, accelerate open science, and democratize deep learning for
medical imaging.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09508" title="Abstract">arXiv:2310.09508</a> [<a href="/pdf/2310.09508" title="Download PDF">pdf</a>, <a href="/format/2310.09508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Findability: A Novel Measure of Information Accessibility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Aman Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Mall%2C+P+R">Priyanshu Raj Mall</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+D">Dwaipayan Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The overwhelming volume of data generated and indexed by search engines poses
a significant challenge in retrieving documents from the index efficiently and
effectively. Even with a well-crafted query, several relevant documents often
get buried among a multitude of competing documents, resulting in reduced
accessibility or `findability' of the desired document. Consequently, it is
crucial to develop a robust methodology for assessing this dimension of
Information Retrieval (IR) system performance. While previous studies have
focused on measuring document accessibility disregarding user queries and
document relevance, there exists no metric to quantify the findability of a
document within a given IR system without resorting to manual labor. This paper
aims to address this gap by defining and deriving a metric to evaluate the
findability of documents as perceived by end-users. Through experiments, we
demonstrate the varying impact of different retrieval models and collections on
the findability of documents. Furthermore, we establish the findability measure
as an independent metric distinct from retrievability, an accessibility measure
introduced in prior literature.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09509" title="Abstract">arXiv:2310.09509</a> [<a href="/pdf/2310.09509" title="Download PDF">pdf</a>, <a href="/ps/2310.09509" title="Download PostScript">ps</a>, <a href="/format/2310.09509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical simulation to the time fractional Vakhnenko Parkes equation  for modeling the propagation of high frequency waves in relaxation medium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Das%2C+G">Gayatri Das</a>, 
<a href="/search/math?searchtype=author&query=Ray%2C+S+S">S. Saha Ray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">This article is concerned with solving the time fractional Vakhnenko Parkes
equation using the reproducing kernels. Reproducing kernel theory, the normal
basis, some important Hilbert spaces, homogenization of constraints, and the
orthogonalization process are the main tools of this technique. The main
advantage of reproducing kernel method is it is truly meshless. The solutions
obtained by the implementation reproducing kernels Hilbert space method on the
time-fractional Vakhnenko Parkes equation is in the form of a series. The
obtained solution converges to the exact solution uniquely. It is observed that
the implemented method is highly effective. The effectiveness of reproducing
kernel Hilbert space method is presented through the tables and graphs. The
perfectness of this method is tested by taking different error norms and the
order of convergence of the errors.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09510" title="Abstract">arXiv:2310.09510</a> [<a href="/pdf/2310.09510" title="Download PDF">pdf</a>, <a href="/format/2310.09510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survey on Security Attacks in Connected and Autonomous Vehicular Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+S+M+M">S M Mostaq Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Banik%2C+S">Shampa Banik</a>, 
<a href="/search/cs?searchtype=author&query=Banik%2C+T">Trapa Banik</a>, 
<a href="/search/cs?searchtype=author&query=Shibli%2C+A+M">Ashfak Md Shibli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure, 1 table, Conference info: 2023 IEEE International Conference on Computing (ICOCO)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Connected and autonomous vehicles, also known as CAVs, are a general trend in
the evolution of the automotive industry that can be utilized to make
transportation safer, improve the number of mobility options available, user
costs will go down and new jobs will be created. However, as our society grows
more automated and networked, criminal actors will have additional
opportunities to conduct a variety of attacks, putting CAV security in danger.
By providing a brief review of the state of cyber security in the CAVs
environment, this study aims to draw attention to the issues and concerns
associated with security. The first thing it does is categorize the multiple
cybersecurity threats and weaknesses in the context of CAVs into three groups:
attacks on the vehicles network, attacks on the Internet at large, and other
attacks. This is done in accordance with the various communication networks and
targets under attack. Next, it considers the possibility of cyber attacks to be
an additional form of threat posed by the environment of CAVs. After that, it
details the most uptodate defense tactics for securing CAVs and analyzes how
effective they are. In addition, it draws some conclusions about the various
cyber security and safety requirements of CAVs that are now available, which is
beneficial for the use of CAVs in the real world. At the end, we discussed some
implications on Adversary Attacks on Autonomous Vehicles. In conclusion, a
number of difficulties and unsolved issues for future research are analyzed and
explored.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09511" title="Abstract">arXiv:2310.09511</a> [<a href="/pdf/2310.09511" title="Download PDF">pdf</a>, <a href="/format/2310.09511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Parameter Identification of Generalized Non-cooperative Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianguo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+J">Jinlong Lei</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+H">Hongsheng Qi</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yiguang Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">This work studies the parameter identification problem of a generalized
non-cooperative game, where each player's cost function is influenced by an
observable signal and some unknown parameters. We consider the scenario where
equilibrium of the game at some observable signals can be observed with noises,
whereas our goal is to identify the unknown parameters with the observed data.
Assuming that the observable signals and the corresponding noise-corrupted
equilibriums are acquired sequentially, we construct this parameter
identification problem as online optimization and introduce a novel online
parameter identification algorithm. To be specific, we construct a regularized
loss function that balances conservativeness and correctiveness, where the
conservativeness term ensures that the new estimates do not deviate
significantly from the current estimates, while the correctiveness term is
captured by the Karush-Kuhn-Tucker conditions. We then prove that when the
players' cost functions are linear with respect to the unknown parameters and
the learning rate of the online parameter identification algorithm satisfies
\mu_k \propto 1/\sqrt{k}, along with other assumptions, the regret bound of the
proposed algorithm is O(\sqrt{K}). Finally, we conduct numerical simulations on
a Nash-Cournot problem to demonstrate that the performance of the online
identification algorithm is comparable to that of the offline setting.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09512" title="Abstract">arXiv:2310.09512</a> [<a href="/pdf/2310.09512" title="Download PDF">pdf</a>, <a href="/format/2310.09512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attentive Multi-Layer Perceptron for Non-autoregressive Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shuyang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiangtao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a conference paper at research track in 2023 ECML-PKDD
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Autoregressive~(AR) generation almost dominates sequence generation for its
efficacy. Recently, non-autoregressive~(NAR) generation gains increasing
popularity for its efficiency and growing efficacy. However, its efficiency is
still bottlenecked by quadratic complexity in sequence lengths, which is
prohibitive for scaling to long sequence generation and few works have been
done to mitigate this problem. In this paper, we propose a novel MLP variant,
\textbf{A}ttentive \textbf{M}ulti-\textbf{L}ayer \textbf{P}erceptron~(AMLP), to
produce a generation model with linear time and space complexity. Different
from classic MLP with static and learnable projection matrices, AMLP leverages
adaptive projections computed from inputs in an attentive mode. The
sample-aware adaptive projections enable communications among tokens in a
sequence, and model the measurement between the query and key space.
Furthermore, we marry AMLP with popular NAR models, deriving a highly efficient
NAR-AMLP architecture with linear time and space complexity. Empirical results
show that such marriage architecture surpasses competitive efficient NAR
models, by a significant margin on text-to-speech synthesis and machine
translation. We also test AMLP's self- and cross-attention ability separately
with extensive ablation experiments, and find them comparable or even superior
to the other efficient models. The efficiency analysis further shows that AMLP
extremely reduces the memory cost against vanilla non-autoregressive models for
long sequences.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09516" title="Abstract">arXiv:2310.09516</a> [<a href="/pdf/2310.09516" title="Download PDF">pdf</a>, <a href="/format/2310.09516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Link Prediction via GNN Layers Induced by Negative Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiannian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Q">Quan Gan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wipf%2C+D">David Wipf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Graph neural networks (GNNs) for link prediction can loosely be divided into
two broad categories. First, \emph{node-wise} architectures pre-compute
individual embeddings for each node that are later combined by a simple decoder
to make predictions. While extremely efficient at inference time (since node
embeddings are only computed once and repeatedly reused), model expressiveness
is limited such that isomorphic nodes contributing to candidate edges may not
be distinguishable, compromising accuracy. In contrast, \emph{edge-wise}
methods rely on the formation of edge-specific subgraph embeddings to enrich
the representation of pair-wise relationships, disambiguating isomorphic nodes
to improve accuracy, but with the cost of increased model complexity. To better
navigate this trade-off, we propose a novel GNN architecture whereby the
\emph{forward pass} explicitly depends on \emph{both} positive (as is typical)
and negative (unique to our approach) edges to inform more flexible, yet still
cheap node-wise embeddings. This is achieved by recasting the embeddings
themselves as minimizers of a forward-pass-specific energy function (distinct
from the actual training loss) that favors separation of positive and negative
samples. As demonstrated by extensive empirical evaluations, the resulting
architecture retains the inference speed of node-wise models, while producing
competitive accuracy with edge-wise alternatives.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09517" title="Abstract">arXiv:2310.09517</a> [<a href="/pdf/2310.09517" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OBSUM: An object-based spatial unmixing model for spatiotemporal fusion  of remote sensing images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Houcai Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+D">Dingqi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Bruzzone%2C+L">Lorenzo Bruzzone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Spatiotemporal fusion aims to improve both the spatial and temporal
resolution of remote sensing images, thus facilitating time-series analysis at
a fine spatial scale. However, there are several important issues that limit
the application of current spatiotemporal fusion methods. First, most
spatiotemporal fusion methods are based on pixel-level computation, which
neglects the valuable object-level information of the land surface. Moreover,
many existing methods cannot accurately retrieve strong temporal changes
between the available high-resolution image at base date and the predicted one.
This study proposes an Object-Based Spatial Unmixing Model (OBSUM), which
incorporates object-based image analysis and spatial unmixing, to overcome the
two abovementioned problems. OBSUM consists of one preprocessing step and three
fusion steps, i.e., object-level unmixing, object-level residual compensation,
and pixel-level residual compensation. OBSUM can be applied using only one fine
image at the base date and one coarse image at the prediction date, without the
need of a coarse image at the base date. The performance of OBSUM was compared
with five representative spatiotemporal fusion methods. The experimental
results demonstrated that OBSUM outperformed other methods in terms of both
accuracy indices and visual effects over time-series. Furthermore, OBSUM also
achieved satisfactory results in two typical remote sensing applications.
Therefore, it has great potential to generate accurate and high-resolution
time-series observations for supporting various remote sensing applications.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09518" title="Abstract">arXiv:2310.09518</a> [<a href="/pdf/2310.09518" title="Download PDF">pdf</a>, <a href="/format/2310.09518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruction Tuning with Human Curriculum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+B+W">Bruce W. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hyunsoo Cho</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+K+M">Kang Min Yoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The dominant paradigm for instruction tuning is the random-shuffled training
of maximally diverse instruction-response pairs. This paper explores the
potential benefits of applying a structured cognitive learning approach to
instruction tuning in contemporary large language models like ChatGPT and
GPT-4. Unlike the previous conventional randomized instruction dataset, we
propose a highly structured synthetic dataset that mimics the progressive and
organized nature of human education. We curate our dataset by aligning it with
educational frameworks, incorporating meta information including its topic and
cognitive rigor level for each sample. Our dataset covers comprehensive
fine-grained topics spanning diverse educational stages (from middle school to
graduate school) with various questions for each topic to enhance conceptual
depth using Bloom's taxonomy-a classification framework distinguishing various
levels of human cognition for each concept. The results demonstrate that this
cognitive rigorous training approach yields significant performance
enhancements - +3.06 on the MMLU benchmark and an additional +1.28 on AI2
Reasoning Challenge (hard set) - compared to conventional randomized training,
all while avoiding additional computational costs. This research highlights the
potential of leveraging human learning principles to enhance the capabilities
of language models in comprehending and responding to complex instructions and
tasks.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09519" title="Abstract">arXiv:2310.09519</a> [<a href="/pdf/2310.09519" title="Download PDF">pdf</a>, <a href="/ps/2310.09519" title="Download PostScript">ps</a>, <a href="/format/2310.09519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crowd Modeling and Control via Cooperative Adaptive Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zirui Wan</a>, 
<a href="/search/cs?searchtype=author&query=Vlaski%2C+S">Stefan Vlaski</a>, 
<a href="/search/cs?searchtype=author&query=Sanei%2C+S">Saeid Sanei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">This paper introduces a crowd modeling and motion control approach that
employs diffusion adaptation within an adaptive network. In the network, nodes
collaboratively address specific estimation problems while simultaneously
moving as agents governed by certain motion control mechanisms. Our research
delves into the behaviors of agents when they encounter spatial constraints.
Within this framework, agents pursue several objectives, such as target
tracking, coherent motion, and obstacle evasion. Throughout their navigation,
they demonstrate a nature of self-organization and self-adjustment that drives
them to maintain certain social distances with each other, and adaptively
adjust their behaviors in response to the environmental changes. Our findings
suggest a promising approach to mitigate the spread of viral pandemics and
averting stampedes.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09520" title="Abstract">arXiv:2310.09520</a> [<a href="/pdf/2310.09520" title="Download PDF">pdf</a>, <a href="/format/2310.09520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward-Augmented Decoding: Efficient Controlled Text Generation With a  Unidirectional Reward Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Haikang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Raffel%2C+C">Colin Raffel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While large language models have proven effective in a huge range of
downstream applications, they often generate text that is problematic or lacks
a desired attribute. In this paper, we introduce Reward-Augmented Decoding
(RAD), a text generation procedure that uses a small unidirectional reward
model to encourage a language model to generate text that has certain
properties. Specifically, RAD uses the reward model to score generations as
they are produced and rescales sampling probabilities to favor high-reward
tokens. By using a unidirectional reward model, RAD can cache activations from
prior generation steps to decrease computational overhead. Through experiments
on generating non-toxic and sentiment-controlled text, we demonstrate that RAD
performs best among methods that change only the generation procedure and
matches the performance of state-of-the-art methods that involve re-training
the language model. We further validate that RAD is effective on very large
language models while incurring a minimal computational overhead.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09522" title="Abstract">arXiv:2310.09522</a> [<a href="/pdf/2310.09522" title="Download PDF">pdf</a>, <a href="/format/2310.09522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Prediction of Full-Ocean Depth SSP by Hierarchical LSTM: An  Experimental Result
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiajun Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
<p class="mathjax">SSP distribution is an important parameter for underwater positioning,
navigation and timing (PNT) because it affects the propagation mode of
underwater acoustic signals. To accurate predict future sound speed
distribution, we propose a hierarchical long short--term memory (H--LSTM)
neural network for future sound speed prediction, which explore the
distribution pattern of sound velocity in the time dimension. To verify the
feasibility and effectiveness, we conducted both simulations and real
experiments. The ocean experiment was held in the South China Sea in April,
2023. Results show that the accuracy of the proposed method outperforms the
state--of--the--art methods.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09525" title="Abstract">arXiv:2310.09525</a> [<a href="/pdf/2310.09525" title="Download PDF">pdf</a>, <a href="/format/2310.09525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TS-ENAS:Two-Stage Evolution for Cell-based Network Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Juan Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shenghong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yizhang Xia</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weiwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zeping Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jinhua Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Neural network architecture search provides a solution to the automatic
design of network structures. However, it is difficult to search the whole
network architecture directly. Although using stacked cells to search neural
network architectures is an effective way to reduce the complexity of
searching, these methods do not able find the global optimal neural network
structure since the number of layers, cells and connection methods is fixed. In
this paper, we propose a Two-Stage Evolution for cell-based Network
Architecture Search(TS-ENAS), including one-stage searching based on stacked
cells and second-stage adjusting these cells. In our algorithm, a new
cell-based search space and an effective two-stage encoding method are designed
to represent cells and neural network structures. In addition, a cell-based
weight inheritance strategy is designed to initialize the weight of the
network, which significantly reduces the running time of the algorithm. The
proposed methods are extensively tested and compared on four image
classification dataset, Fashion-MNIST, CIFAR10, CIFAR100 and ImageNet and
compared with 22 state-of-the-art algorithms including hand-designed networks
and NAS networks. The experimental results show that TS-ENAS can more
effectively find the neural network architecture with comparative performance.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09527" title="Abstract">arXiv:2310.09527</a> [<a href="/pdf/2310.09527" title="Download PDF">pdf</a>, <a href="/ps/2310.09527" title="Download PostScript">ps</a>, <a href="/format/2310.09527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A discontinuous plane wave neural network method for Helmholtz equation  and time-harmonic Maxwell&#x27;s equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yuan%2C+L">Long Yuan</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+Q">Qiya Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper we propose a {\it discontinuous} plane wave neural network
(DPWNN) method with $hp-$refinement for approximately solving Helmholtz
equation and time-harmonic Maxwell equations. In this method, we define a
quadratic functional as in the plane wave least square (PWLS) method with
$h-$refinement and introduce new discretization sets spanned by element-wise
neural network functions with a single hidden layer, where the activation
function on each element is chosen as a complex-valued exponential function
like the plane wave function. The desired approximate solution is recursively
generated by iteratively solving the minimization problem associated with the
functional and the sets described above, which is defined by a sequence of
approximate minimizers of the underlying residual functionals, where plane wave
direction angles and activation coefficients are alternatively computed by
iterative algorithms. For the proposed DPWNN method, the plane wave directions
are adaptively determined in the iterative process, which is different from
that in the standard PWLS method (where the plane wave directions are
preliminarily given). Numerical experiments will confirm that this DPWNN method
can generate approximate solutions with higher accuracy than the PWLS method.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09528" title="Abstract">arXiv:2310.09528</a> [<a href="/pdf/2310.09528" title="Download PDF">pdf</a>, <a href="/format/2310.09528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+W">Woojin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kookjin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Rim%2C+D">Donsub Rim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+N">Noseong Park</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">In various engineering and applied science applications, repetitive numerical
simulations of partial differential equations (PDEs) for varying input
parameters are often required (e.g., aircraft shape optimization over many
design parameters) and solvers are required to perform rapid execution. In this
study, we suggest a path that potentially opens up a possibility for
physics-informed neural networks (PINNs), emerging deep-learning-based solvers,
to be considered as one such solver. Although PINNs have pioneered a proper
integration of deep-learning and scientific computing, they require repetitive
time-consuming training of neural networks, which is not suitable for
many-query scenarios. To address this issue, we propose a lightweight low-rank
PINNs containing only hundreds of model parameters and an associated
hypernetwork-based meta-learning algorithm, which allows efficient
approximation of solutions of PDEs for varying ranges of PDE input parameters.
Moreover, we show that the proposed method is effective in overcoming a
challenging issue, known as "failure modes" of PINNs.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09532" title="Abstract">arXiv:2310.09532</a> [<a href="/pdf/2310.09532" title="Download PDF">pdf</a>, <a href="/ps/2310.09532" title="Download PostScript">ps</a>, <a href="/format/2310.09532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Open Repository of Performance Portability of Applications,  Benchmarks and Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marowka%2C+A">Ami Marowka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The adoption of heterogeneous computing systems based on diverse
architectures to achieve exascale computing power has worsened the performance
portability problem of scientific applications that were designed to run on
these platforms.
<br />To cope with the challenges posed by supercomputing, new performance
portability frameworks have been developed alongside advanced methods and
metrics to evaluate the performance portability of heterogeneous applications.
However, many studies have shown that the new methods and metrics do not
produce coherent results which yield clear conclusions that are required for
designing the hardware and software architectures of tomorrow's supercomputing
systems.
<br />We outline a proposal to establish an open repository of performance
portability of applications, benchmarks and models which will be standardized,
objective, and based on strict operating and reporting guidelines. Such
guidelines will ensure a fair, comparable and meaningful measure of the
performance portability while the requirement for a detailed disclosure of the
obtained results and the configuration settings will ensure the reproducibility
of the reported results.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09533" title="Abstract">arXiv:2310.09533</a> [<a href="/pdf/2310.09533" title="Download PDF">pdf</a>, <a href="/format/2310.09533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards End-to-End Unsupervised Saliency Detection with Self-Supervised  Top-Down Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yicheng Song</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shuyong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+H">Haozhe Xing</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yiting Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised salient object detection aims to detect salient objects without
using supervision signals eliminating the tedious task of manually labeling
salient objects. To improve training efficiency, end-to-end methods for USOD
have been proposed as a promising alternative. However, current solutions rely
heavily on noisy handcraft labels and fail to mine rich semantic information
from deep features. In this paper, we propose a self-supervised end-to-end
salient object detection framework via top-down context. Specifically,
motivated by contrastive learning, we exploit the self-localization from the
deepest feature to construct the location maps which are then leveraged to
learn the most instructive segmentation guidance. Further considering the lack
of detailed information in deepest features, we exploit the detail-boosting
refiner module to enrich the location labels with details. Moreover, we observe
that due to lack of supervision, current unsupervised saliency models tend to
detect non-salient objects that are salient in some other samples of
corresponding scenarios. To address this widespread issue, we design a novel
Unsupervised Non-Salient Suppression (UNSS) method developing the ability to
ignore non-salient objects. Extensive experiments on benchmark datasets
demonstrate that our method achieves leading performance among the recent
end-to-end methods and most of the multi-stage solutions. The code is
available.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09536" title="Abstract">arXiv:2310.09536</a> [<a href="/pdf/2310.09536" title="Download PDF">pdf</a>, <a href="/format/2310.09536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CarExpert: Leveraging Large Language Models for In-Car Conversational  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rony%2C+M+R+A+H">Md Rashad Al Hasan Rony</a>, 
<a href="/search/cs?searchtype=author&query=Suess%2C+C">Christian Suess</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+S+R">Sinchana Ramakanth Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Sudhi%2C+V">Viju Sudhi</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Julia Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Vogel%2C+M">Maximilian Vogel</a>, 
<a href="/search/cs?searchtype=author&query=Teucher%2C+R">Roman Teucher</a>, 
<a href="/search/cs?searchtype=author&query=Friedl%2C+K+E">Ken E. Friedl</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+S">Soumya Sahoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted into EMNLP 2023 (industry track), corresponding Author: Md Rashad Al Hasan Rony
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated remarkable performance by
following natural language instructions without fine-tuning them on
domain-specific tasks and data. However, leveraging LLMs for domain-specific
question answering suffers from severe limitations. The generated answer tends
to hallucinate due to the training data collection time (when using
off-the-shelf), complex user utterance and wrong retrieval (in
retrieval-augmented generation). Furthermore, due to the lack of awareness
about the domain and expected output, such LLMs may generate unexpected and
unsafe answers that are not tailored to the target domain. In this paper, we
propose CarExpert, an in-car retrieval-augmented conversational
question-answering system leveraging LLMs for different tasks. Specifically,
CarExpert employs LLMs to control the input, provide domain-specific documents
to the extractive and generative answering components, and controls the output
to ensure safe and domain-specific answers. A comprehensive empirical
evaluation exhibits that CarExpert outperforms state-of-the-art LLMs in
generating natural, safe and car-specific answers.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09542" title="Abstract">arXiv:2310.09542</a> [<a href="/pdf/2310.09542" title="Download PDF">pdf</a>, <a href="/format/2310.09542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Treewidth Boundedness Problem for an Inductive Separation Logic of  Relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bozga%2C+M">Marius Bozga</a>, 
<a href="/search/cs?searchtype=author&query=Bueri%2C+L">Lucas Bueri</a>, 
<a href="/search/cs?searchtype=author&query=Iosif%2C+R">Radu Iosif</a>, 
<a href="/search/cs?searchtype=author&query=Zuleger%2C+F">Florian Zuleger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">The treewidth boundedness problem for a logic asks for the existence of an
upper bound on the treewidth of the models of a given formula in that logic.
This problem is found to be undecidable for first order logic. We consider a
generalization of Separation Logic over relational signatures, interpreted over
standard relational structures, and describe an algorithm for the treewidth
boundedness problem in the context of this logic.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09543" title="Abstract">arXiv:2310.09543</a> [<a href="/pdf/2310.09543" title="Download PDF">pdf</a>, <a href="/format/2310.09543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking the Sim-to-Real Gap in Cloth Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blanco-Mulero%2C+D">David Blanco-Mulero</a>, 
<a href="/search/cs?searchtype=author&query=Barbany%2C+O">Oriol Barbany</a>, 
<a href="/search/cs?searchtype=author&query=Alcan%2C+G">Gokhan Alcan</a>, 
<a href="/search/cs?searchtype=author&query=Colom%C3%A9%2C+A">Adri&#xe0; Colom&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Torras%2C+C">Carme Torras</a>, 
<a href="/search/cs?searchtype=author&query=Kyrki%2C+V">Ville Kyrki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Robotics and Automation Letters. 8 pages, 6 figures. Supplementary material available at <a href="https://sites.google.com/view/cloth-sim2real-benchmark">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Realistic physics engines play a crucial role for learning to manipulate
deformable objects such as garments in simulation. By doing so, researchers can
circumvent challenges such as sensing the deformation of the object in the
real-world. In spite of the extensive use of simulations for this task, few
works have evaluated the reality gap between deformable object simulators and
real-world data. We present a benchmark dataset to evaluate the sim-to-real gap
in cloth manipulation. The dataset is collected by performing a dynamic cloth
manipulation task involving contact with a rigid table. We use the dataset to
evaluate the reality gap, computational time, and simulation stability of four
popular deformable object simulators: MuJoCo, Bullet, Flex, and SOFA.
Additionally, we discuss the benefits and drawbacks of each simulator. The
benchmark dataset is open-source. Supplementary material, videos, and code, can
be found at https://sites.google.com/view/cloth-sim2real-benchmark.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09549" title="Abstract">arXiv:2310.09549</a> [<a href="/pdf/2310.09549" title="Download PDF">pdf</a>, <a href="/format/2310.09549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scene Text Recognition Models Explainability Using Local Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ty%2C+M+V">Mark Vincent Ty</a>, 
<a href="/search/cs?searchtype=author&query=Atienza%2C+R">Rowel Atienza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICIP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ty, Mark Vincent, and Rowel Atienza. "Scene Text Recognition
  Models Explainability Using Local Features." 2023 IEEE International
  Conference on Image Processing (ICIP). IEEE, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Explainable AI (XAI) is the study on how humans can be able to understand the
cause of a model's prediction. In this work, the problem of interest is Scene
Text Recognition (STR) Explainability, using XAI to understand the cause of an
STR model's prediction. Recent XAI literatures on STR only provide a simple
analysis and do not fully explore other XAI methods. In this study, we
specifically work on data explainability frameworks, called attribution-based
methods, that explain the important parts of an input data in deep learning
models. However, integrating them into STR produces inconsistent and
ineffective explanations, because they only explain the model in the global
context. To solve this problem, we propose a new method, STRExp, to take into
consideration the local explanations, i.e. the individual character prediction
explanations. This is then benchmarked across different attribution-based
methods on different STR datasets and evaluated across different STR models.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09550" title="Abstract">arXiv:2310.09550</a> [<a href="/pdf/2310.09550" title="Download PDF">pdf</a>, <a href="/format/2310.09550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Model Comprehend Ancient Chinese? A Preliminary Test  on ACLUE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haonan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at RANLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have showcased remarkable capabilities in
understanding and generating language. However, their ability in comprehending
ancient languages, particularly ancient Chinese, remains largely unexplored. To
bridge this gap, we present ACLUE, an evaluation benchmark designed to assess
the capability of language models in comprehending ancient Chinese. ACLUE
consists of 15 tasks cover a range of skills, spanning phonetic, lexical,
syntactic, semantic, inference and knowledge. Through the evaluation of eight
state-of-the-art LLMs, we observed a noticeable disparity in their performance
between modern Chinese and ancient Chinese. Among the assessed models, ChatGLM2
demonstrates the most remarkable performance, achieving an average score of
37.4%. We have made our code and data public available.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09554" title="Abstract">arXiv:2310.09554</a> [<a href="/pdf/2310.09554" title="Download PDF">pdf</a>, <a href="/format/2310.09554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural network scoring for efficient computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waltsburger%2C+H">Hugo Waltsburger</a>, 
<a href="/search/cs?searchtype=author&query=Libessart%2C+E">Erwan Libessart</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Chengfang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Kolar%2C+A">Anthony Kolar</a>, 
<a href="/search/cs?searchtype=author&query=Guinvarc%27h%2C+R">Regis Guinvarc&#x27;h</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2023 IEEE International Symposium on Circuits
  and Systems (ISCAS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Much work has been dedicated to estimating and optimizing workloads in
high-performance computing (HPC) and deep learning. However, researchers have
typically relied on few metrics to assess the efficiency of those techniques.
Most notably, the accuracy, the loss of the prediction, and the computational
time with regard to GPUs or/and CPUs characteristics. It is rare to see figures
for power consumption, partly due to the difficulty of obtaining accurate power
readings. In this paper, we introduce a composite score that aims to
characterize the trade-off between accuracy and power consumption measured
during the inference of neural networks. For this purpose, we present a new
open-source tool allowing researchers to consider more metrics: granular power
consumption, but also RAM/CPU/GPU utilization, as well as storage, and network
input/output (I/O). To our best knowledge, it is the first fit test for neural
architectures on hardware architectures. This is made possible thanks to
reproducible power efficiency measurements. We applied this procedure to
state-of-the-art neural network architectures on miscellaneous hardware. One of
the main applications and novelties is the measurement of algorithmic power
efficiency. The objective is to allow researchers to grasp their algorithms'
efficiencies better. This methodology was developed to explore trade-offs
between energy usage and accuracy in neural networks. It is also useful when
fitting hardware for a specific task or to compare two architectures more
accurately, with architecture exploration in mind.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09560" title="Abstract">arXiv:2310.09560</a> [<a href="/pdf/2310.09560" title="Download PDF">pdf</a>, <a href="/format/2310.09560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UNIQA: A Unified Framework for Both Full-Reference and No-Reference  Image Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yun%2C+Y+K">Yi Ke Yun</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weisi Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The human visual system (HVS) is effective at distinguishing low-quality
images due to its ability to sense the distortion level and the resulting
semantic impact. Prior research focuses on developing dedicated networks based
on the presence and absence of pristine images, respectively, and this results
in limited application scope and potential performance inconsistency when
switching from NR to FR IQA. In addition, most methods heavily rely on spatial
distortion modeling through difference maps or weighted features, and this may
not be able to well capture the correlations between distortion and the
semantic impact it causes. To this end, we aim to design a unified network for
both Full-Reference (FR) and No-Reference (NR) IQA via semantic impact
modeling. Specifically, we employ an encoder to extract multi-level features
from input images. Then a Hierarchical Self-Attention (HSA) module is proposed
as a universal adapter for both FR and NR inputs to model the spatial
distortion level at each encoder stage. Furthermore, considering that
distortions contaminate encoder stages and damage image semantic meaning
differently, a Cross-Scale Cross-Attention (CSCA) module is proposed to examine
correlations between distortion at shallow stages and deep ones. By adopting
HSA and CSCA, the proposed network can effectively perform both FR and NR IQA.
Extensive experiments demonstrate that the proposed simple network is effective
and outperforms the relevant state-of-the-art FR and NR methods on four
synthetic-distorted datasets and three authentic-distorted datasets.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09561" title="Abstract">arXiv:2310.09561</a> [<a href="/pdf/2310.09561" title="Download PDF">pdf</a>, <a href="/ps/2310.09561" title="Download PostScript">ps</a>, <a href="/format/2310.09561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Network approaches for single-cell data: A recent overview
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazaros%2C+K">Konstantinos Lazaros</a>, 
<a href="/search/cs?searchtype=author&query=Koumadorakis%2C+D+E">Dimitris E. Koumadorakis</a>, 
<a href="/search/cs?searchtype=author&query=Vlamos%2C+P">Panagiotis Vlamos</a>, 
<a href="/search/cs?searchtype=author&query=Vrahatis%2C+A+G">Aristidis G. Vrahatis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Genomics (q-bio.GN)

</div>
<p class="mathjax">Graph Neural Networks (GNN) are reshaping our understanding of biomedicine
and diseases by revealing the deep connections among genes and cells. As both
algorithmic and biomedical technologies have advanced significantly, we're
entering a transformative phase of personalized medicine. While pioneering
tools like Graph Attention Networks (GAT) and Graph Convolutional Neural
Networks (Graph CNN) are advancing graph-based learning, the rise of
single-cell sequencing techniques is reshaping our insights on cellular
diversity and function. Numerous studies have combined GNNs with single-cell
data, showing promising results. In this work, we highlight the GNN
methodologies tailored for single-cell data over the recent years. We outline
the diverse range of graph deep learning architectures that center on GAT
methodologies. Furthermore, we underscore the several objectives of GNN
strategies in single-cell data contexts, ranging from cell-type annotation,
data integration and imputation, gene regulatory network reconstruction,
clustering and many others. This review anticipates a future where GNNs become
central to single-cell analysis efforts, particularly as vast omics datasets
are continuously generated and the interconnectedness of cells and genes
enhances our depth of knowledge in biomedicine.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09562" title="Abstract">arXiv:2310.09562</a> [<a href="/pdf/2310.09562" title="Download PDF">pdf</a>, <a href="/format/2310.09562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does CLIP&#x27;s Generalization Performance Mainly Stem from High Train-Test  Similarity?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mayilvahanan%2C+P">Prasanna Mayilvahanan</a>, 
<a href="/search/cs?searchtype=author&query=Wiedemer%2C+T">Thadd&#xe4;us Wiedemer</a>, 
<a href="/search/cs?searchtype=author&query=Rusak%2C+E">Evgenia Rusak</a>, 
<a href="/search/cs?searchtype=author&query=Bethge%2C+M">Matthias Bethge</a>, 
<a href="/search/cs?searchtype=author&query=Brendel%2C+W">Wieland Brendel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Foundation models like CLIP are trained on hundreds of millions of samples
and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows
stellar zero-shot and few-shot capabilities on a wide range of
out-of-distribution (OOD) benchmarks, which prior works attribute mainly to
today's large and comprehensive training dataset (like LAION). However, it is
questionable how meaningful terms like out-of-distribution generalization are
for CLIP as it seems likely that web-scale datasets like LAION simply contain
many samples that are similar to common OOD benchmarks originally designed for
ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that
replicate ImageNet's train-test similarity with respect to common OOD
benchmarks. While we observe a performance drop on some benchmarks,
surprisingly, CLIP's overall performance remains high. This shows that high
train-test similarity is insufficient to explain CLIP's OOD performance, and
other properties of the training data must drive CLIP to learn more
generalizable representations. Additionally, by pruning data points that are
dissimilar to the OOD benchmarks, we uncover a 100M split of LAION
($\frac{1}{4}$th of its original size) on which CLIP can be trained to match
its original OOD performance.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09563" title="Abstract">arXiv:2310.09563</a> [<a href="/pdf/2310.09563" title="Download PDF">pdf</a>, <a href="/format/2310.09563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Unified Representations for Multi-Resolution Face Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hulingxiao He</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Wu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yidian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shilong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Wen Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hanqing Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to BMVC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work, we propose Branch-to-Trunk network (BTNet), a representation
learning method for multi-resolution face recognition. It consists of a trunk
network (TNet), namely a unified encoder, and multiple branch networks (BNets),
namely resolution adapters. As per the input, a resolution-specific BNet is
used and the output are implanted as feature maps in the feature pyramid of
TNet, at a layer with the same resolution. The discriminability of tiny faces
is significantly improved, as the interpolation error introduced by rescaling,
especially up-sampling, is mitigated on the inputs. With branch distillation
and backward-compatible training, BTNet transfers discriminative
high-resolution information to multiple branches while guaranteeing
representation compatibility. Our experiments demonstrate strong performance on
face recognition benchmarks, both for multi-resolution identity matching and
feature aggregation, with much less computation amount and parameter storage.
We establish new state-of-the-art on the challenging QMUL-SurvFace 1: N face
identification task. Our code is available at
https://github.com/StevenSmith2000/BTNet.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09566" title="Abstract">arXiv:2310.09566</a> [<a href="/pdf/2310.09566" title="Download PDF">pdf</a>, <a href="/format/2310.09566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy stable discontinuous Galerkin schemes for two-fluid relativistic  plasma flow equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bhoriya%2C+D">Deepak Bhoriya</a>, 
<a href="/search/math?searchtype=author&query=Biswas%2C+B">Biswarup Biswas</a>, 
<a href="/search/math?searchtype=author&query=Kumar%2C+H">Harish Kumar</a>, 
<a href="/search/math?searchtype=author&query=Chandrashekhar%2C+P">Praveen Chandrashekhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Journal of Scientific Computing (JOMP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">This article proposes entropy stable discontinuous Galerkin schemes (DG) for
two-fluid relativistic plasma flow equations. These equations couple the flow
of relativistic fluids via electromagnetic quantities evolved using Maxwell's
equations. The proposed schemes are based on the Gauss-Lobatto quadrature rule,
which has the summation by parts (SBP) property. We exploit the structure of
the equations having the flux with three independent parts coupled via
nonlinear source terms. We design entropy stable DG schemes for each flux part,
coupled with the fact that the source terms do not affect entropy, resulting in
an entropy stable scheme for the complete system. The proposed schemes are then
tested on various test problems in one and two dimensions to demonstrate their
accuracy and stability.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09567" title="Abstract">arXiv:2310.09567</a> [<a href="/pdf/2310.09567" title="Download PDF">pdf</a>, <a href="/format/2310.09567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic alignment in cone-beam tomography via fan-beam symmetry and  variable projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guerrero%2C+P">Patricio Guerrero</a>, 
<a href="/search/math?searchtype=author&query=Bellens%2C+S">Simon Bellens</a>, 
<a href="/search/math?searchtype=author&query=Santander%2C+R">Ricardo Santander</a>, 
<a href="/search/math?searchtype=author&query=Dewulf%2C+W">Wim Dewulf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work is concerned with cone-beam computed tomography with circular
source trajectory, where the reconstruction inverse problem requires an
accurate knowledge of source, detector and rotational axis relative positions
and orientations. We address this problem as a preceding step of the
reconstruction process directly from the acquired projections. The method
estimates both the detector shift (orthogonal to focal and rotational axes) and
the in-plane detector rotation, relative to source and rotational axis. The
obtained algorithm is based on a fan-beam symmetry condition and the variable
projection optimization approach with a low computational cost. Therefore, the
alignment problem for fan-beam tomography is addressed as well. The methods are
validated with simulated and real industrial tomographic data with code
examples available for both fan- and cone-beam geometries.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09568" title="Abstract">arXiv:2310.09568</a> [<a href="/pdf/2310.09568" title="Download PDF">pdf</a>, <a href="/format/2310.09568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wafer-scale Computing: Advancements, Challenges, and Future Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xinhan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huizheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhen He</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xingmao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiahao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qize Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+S">Sihan Guan</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiahao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+H">Haoran Shang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xinru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Shaojun Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+S">Shouyi Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Nowadays, artificial intelligence (AI) technology with large models plays an
increasingly important role in both academia and industry. It also brings a
rapidly increasing demand for the computing power of the hardware. As the
computing demand for AI continues to grow, the growth of hardware computing
power has failed to keep up. This has become a significant factor restricting
the development of AI. The augmentation of hardware computing power is mainly
propelled by the escalation of transistor density and chip area. However, the
former is impeded by the termination of the Moore's Law and Dennard scaling,
and the latter is significantly restricted by the challenge of disrupting the
legacy fabrication equipment and process.
<br />In recent years, advanced packaging technologies that have gradually matured
are increasingly used to implement bigger chips that integrate multiple
chiplets, while still providing interconnections with chip-level density and
bandwidth. Compared to conventional high-performance computing paradigms such
as multi-accelerator and datacenter-scale computing, Wafer-scale Computing
shows remarkable advantages in communication bandwidth, integration density,
and programmability potential. Not surprisingly, disruptive Wafer-scale
Computing also brings unprecedented design challenges for hardware
architecture, design-system-technology co-optimization, power and cooling
systems, and compiler tool chain. At present, there are no comprehensive
surveys summarizing the current state and design insights of Wafer-scale
Computing. This paper aims to take the first step to help academia and industry
review existing wafer-scale chips and essential technologies in a one-stop
manner. So that people can conveniently grasp the basic knowledge and key
points, understand the achievements and shortcomings of existing research, and
contribute to this promising research direction.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09570" title="Abstract">arXiv:2310.09570</a> [<a href="/pdf/2310.09570" title="Download PDF">pdf</a>, <a href="/format/2310.09570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Efficient Multi-Codec Bitrate-Ladder Estimation for Adaptive  Video Streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Menon%2C+V+V">Vignesh V Menon</a>, 
<a href="/search/cs?searchtype=author&query=Farahani%2C+R">Reza Farahani</a>, 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+P+T">Prajit T Rajendran</a>, 
<a href="/search/cs?searchtype=author&query=Afzal%2C+S">Samira Afzal</a>, 
<a href="/search/cs?searchtype=author&query=Schoeffmann%2C+K">Klaus Schoeffmann</a>, 
<a href="/search/cs?searchtype=author&query=Timmerer%2C+C">Christian Timmerer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE International Conference on Visual Communications and Image Processing (VCIP), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">With the emergence of multiple modern video codecs, streaming service
providers are forced to encode, store, and transmit bitrate ladders of multiple
codecs separately, consequently suffering from additional energy costs for
encoding, storage, and transmission. To tackle this issue, we introduce an
online energy-efficient Multi-Codec Bitrate ladder Estimation scheme (MCBE) for
adaptive video streaming applications. In MCBE, quality representations within
the bitrate ladder of new-generation codecs (e.g., High Efficiency Video Coding
(HEVC), Alliance for Open Media Video 1 (AV1)) that lie below the predicted
rate-distortion curve of the Advanced Video Coding (AVC) codec are removed.
Moreover, perceptual redundancy between representations of the bitrate ladders
of the considered codecs is also minimized based on a Just Noticeable
Difference (JND) threshold. Therefore, random forest-based models predict the
VMAF score of bitrate ladder representations of each codec. In a live streaming
session where all clients support the decoding of AVC, HEVC, and AV1, MCBE
achieves impressive results, reducing cumulative encoding energy by 56.45%,
storage energy usage by 94.99%, and transmission energy usage by 77.61%
(considering a JND of six VMAF points). These energy reductions are in
comparison to a baseline bitrate ladder encoding based on current industry
practice.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09571" title="Abstract">arXiv:2310.09571</a> [<a href="/pdf/2310.09571" title="Download PDF">pdf</a>, <a href="/format/2310.09571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Feasibility of Cross-Language Detection of Malicious Packages in  npm and PyPI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ladisa%2C+P">Piergiorgio Ladisa</a>, 
<a href="/search/cs?searchtype=author&query=Ponta%2C+S+E">Serena Elisa Ponta</a>, 
<a href="/search/cs?searchtype=author&query=Ronzoni%2C+N">Nicola Ronzoni</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+M">Matias Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Barais%2C+O">Olivier Barais</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of Annual Computer Security Applications Conference (ACSAC '23), December 4--8, 2023, Austin, TX, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Current software supply chains heavily rely on open-source packages hosted in
public repositories. Given the popularity of ecosystems like npm and PyPI,
malicious users started to spread malware by publishing open-source packages
containing malicious code. Recent works apply machine learning techniques to
detect malicious packages in the npm ecosystem. However, the scarcity of
samples poses a challenge to the application of machine learning techniques in
other ecosystems. Despite the differences between JavaScript and Python, the
open-source software supply chain attacks targeting such languages show
noticeable similarities (e.g., use of installation scripts, obfuscated strings,
URLs).
<br />In this paper, we present a novel approach that involves a set of
language-independent features and the training of models capable of detecting
malicious packages in npm and PyPI by capturing their commonalities. This
methodology allows us to train models on a diverse dataset encompassing
multiple languages, thereby overcoming the challenge of limited sample
availability. We evaluate the models both in a controlled experiment (where
labels of data are known) and in the wild by scanning newly uploaded packages
for both npm and PyPI for 10 days.
<br />We find that our approach successfully detects malicious packages for both
npm and PyPI. Over an analysis of 31,292 packages, we reported 58 previously
unknown malicious packages (38 for npm and 20 for PyPI), which were
consequently removed from the respective repositories.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09573" title="Abstract">arXiv:2310.09573</a> [<a href="/pdf/2310.09573" title="Download PDF">pdf</a>, <a href="/format/2310.09573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Detoxifying Language Models via Toxification Reversal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leong%2C+C+T">Chak Tou Leong</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiashuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language model detoxification aims to minimize the risk of generating
offensive or harmful content in pretrained language models (PLMs) for safer
deployment. Existing methods can be roughly categorized as finetuning-based and
decoding-based. However, the former is often resource-intensive, while the
latter relies on additional components and potentially compromises the
generation fluency. In this paper, we propose a more lightweight approach that
enables the PLM itself to achieve "self-detoxification". Our method is built
upon the observation that prepending a negative steering prompt can effectively
induce PLMs to generate toxic content. At the same time, we are inspired by the
recent research in the interpretability field, which formulates the evolving
contextualized representations within the PLM as an information stream
facilitated by the attention layers. Drawing on this idea, we devise a method
to identify the toxification direction from the normal generation process to
the one prompted with the negative prefix, and then steer the generation to the
reversed direction by manipulating the information movement within the
attention layers. Experimental results show that our approach, without any
fine-tuning or extra components, can achieve comparable performance with
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09574" title="Abstract">arXiv:2310.09574</a> [<a href="/pdf/2310.09574" title="Download PDF">pdf</a>, <a href="/format/2310.09574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduced Policy Optimization for Continuous Control with Hard Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shutong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yali Du</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Ye Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent advances in constrained reinforcement learning (RL) have endowed
reinforcement learning with certain safety guarantees. However, deploying
existing constrained RL algorithms in continuous control tasks with general
hard constraints remains challenging, particularly in those situations with
non-convex hard constraints. Inspired by the generalized reduced gradient (GRG)
algorithm, a classical constrained optimization technique, we propose a reduced
policy optimization (RPO) algorithm that combines RL with GRG to address
general hard constraints. RPO partitions actions into basic actions and
nonbasic actions following the GRG method and outputs the basic actions via a
policy network. Subsequently, RPO calculates the nonbasic actions by solving
equations based on equality constraints using the obtained basic actions. The
policy network is then updated by implicitly differentiating nonbasic actions
with respect to basic actions. Additionally, we introduce an action projection
procedure based on the reduced gradient and apply a modified Lagrangian
relaxation technique to ensure inequality constraints are satisfied. To the
best of our knowledge, RPO is the first attempt that introduces GRG to RL as a
way of efficiently handling both equality and inequality hard constraints. It
is worth noting that there is currently a lack of RL environments with complex
hard constraints, which motivates us to develop three new benchmarks: two
robotics manipulation tasks and a smart grid operation control task. With these
benchmarks, RPO achieves better performance than previous constrained RL
algorithms in terms of both cumulative reward and constraint violation. We
believe RPO, along with the new benchmarks, will open up new opportunities for
applying RL to real-world problems with complex constraints.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09575" title="Abstract">arXiv:2310.09575</a> [<a href="/pdf/2310.09575" title="Download PDF">pdf</a>, <a href="/format/2310.09575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Common Challenges of Deep Reinforcement Learning Applications  Development: An Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morovati%2C+M+M">Mohammad Mehdi Morovati</a>, 
<a href="/search/cs?searchtype=author&query=Tambon%2C+F">Florian Tambon</a>, 
<a href="/search/cs?searchtype=author&query=Taraghi%2C+M">Mina Taraghi</a>, 
<a href="/search/cs?searchtype=author&query=Nikanjam%2C+A">Amin Nikanjam</a>, 
<a href="/search/cs?searchtype=author&query=Khomh%2C+F">Foutse Khomh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Empirical Software Engineering journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Machine Learning (ML) is increasingly being adopted in different industries.
Deep Reinforcement Learning (DRL) is a subdomain of ML used to produce
intelligent agents. Despite recent developments in DRL technology, the main
challenges that developers face in the development of DRL applications are
still unknown. To fill this gap, in this paper, we conduct a large-scale
empirical study of 927 DRL-related posts extracted from Stack Overflow, the
most popular Q&amp;A platform in the software community. Through the process of
labeling and categorizing extracted posts, we created a taxonomy of common
challenges encountered in the development of DRL applications, along with their
corresponding popularity levels. This taxonomy has been validated through a
survey involving 59 DRL developers. Results show that at least 45% of
developers experienced 18 of the 21 challenges identified in the taxonomy. The
most frequent source of difficulty during the development of DRL applications
are Comprehension, API usage, and Design problems, while Parallel processing,
and DRL libraries/frameworks are classified as the most difficult challenges to
address, with respect to the time required to receive an accepted answer. We
hope that the research community will leverage this taxonomy to develop
efficient strategies to address the identified challenges and improve the
quality of DRL applications.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09578" title="Abstract">arXiv:2310.09578</a> [<a href="/pdf/2310.09578" title="Download PDF">pdf</a>, <a href="/format/2310.09578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Index Tracking via Topological Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goel%2C+A">Anubha Goel</a>, 
<a href="/search/cs?searchtype=author&query=Pasricha%2C+P">Puneet Pasricha</a>, 
<a href="/search/cs?searchtype=author&query=Kanniainen%2C+J">Juho Kanniainen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Portfolio Management (q-fin.PM)

</div>
<p class="mathjax">In this research, we introduce a novel methodology for the index tracking
problem with sparse portfolios by leveraging topological data analysis (TDA).
Utilizing persistence homology to measure the riskiness of assets, we introduce
a topological method for data-driven learning of the parameters for
regularization terms. Specifically, the Vietoris-Rips filtration method is
utilized to capture the intricate topological features of asset movements,
providing a robust framework for portfolio tracking. Our approach has the
advantage of accommodating both $\ell_1$ and $\ell_2$ penalty terms without the
requirement for expensive estimation procedures. We empirically validate the
performance of our methodology against state-of-the-art sparse index tracking
techniques, such as Elastic-Net and SLOPE, using a dataset that covers 23 years
of S&amp;P500 index and its constituent data. Our out-of-sample results show that
this computationally efficient technique surpasses conventional methods across
risk metrics, risk-adjusted performance, and trading expenses in varied market
conditions. Furthermore, in turbulent markets, it not only maintains but also
enhances tracking performance.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09580" title="Abstract">arXiv:2310.09580</a> [<a href="/pdf/2310.09580" title="Download PDF">pdf</a>, <a href="/format/2310.09580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where to Decide? Centralized vs. Distributed Vehicle Assignment for  Platoon Formation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heinovski%2C+J">Julian Heinovski</a>, 
<a href="/search/cs?searchtype=author&query=Dressler%2C+F">Falko Dressler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Platooning is a promising cooperative driving application for future
intelligent transportation systems. In order to assign vehicles to platoons,
some algorithm for platoon formation is required. Such vehicle-to-platoon
assignments have to be computed on-demand, e.g., when vehicles join or leave
the freeways. In order to get best results from platooning, individual
properties of involved vehicles have to be considered during the assignment
computation. In this paper, we explore the computation of vehicle-to-platoon
assignments as an optimization problem based on similarity between vehicles. We
define the similarity and, vice versa, the deviation among vehicles based on
the desired driving speed of vehicles and their position on the road. We create
three approaches to solve this assignment problem: centralized solver,
centralized greedy, and distributed greedy, using a Mixed Integer Programming
solver and greedy heuristics, respectively. Conceptually, the approaches differ
in both knowledge about vehicles as well as methodology. We perform a
large-scale simulation study using PlaFoSim to compare all approaches. While
the distributed greedy approach seems to have disadvantages due to the limited
local knowledge, it performs as good as the centralized solver approach across
most metrics. Both outperform the centralized greedy approach, which suffers
from synchronization and greedy selection effects.Since the centralized solver
approach assumes global knowledge and requires a complex Mixed Integer
Programming solver to compute vehicle-to-platoon assignments, we consider the
distributed greedy approach to have the best performance among all presented
approaches.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09583" title="Abstract">arXiv:2310.09583</a> [<a href="/pdf/2310.09583" title="Download PDF">pdf</a>, <a href="/format/2310.09583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Sides of The Same Coin: Bridging Deep Equilibrium Models and Neural  ODEs via Homotopy Continuation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shutong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+T">Tianyu Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Ye Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep Equilibrium Models (DEQs) and Neural Ordinary Differential Equations
(Neural ODEs) are two branches of implicit models that have achieved remarkable
success owing to their superior performance and low memory consumption. While
both are implicit models, DEQs and Neural ODEs are derived from different
mathematical formulations. Inspired by homotopy continuation, we establish a
connection between these two models and illustrate that they are actually two
sides of the same coin. Homotopy continuation is a classical method of solving
nonlinear equations based on a corresponding ODE. Given this connection, we
proposed a new implicit model called HomoODE that inherits the property of high
accuracy from DEQs and the property of stability from Neural ODEs. Unlike DEQs,
which explicitly solve an equilibrium-point-finding problem via Newton's
methods in the forward pass, HomoODE solves the equilibrium-point-finding
problem implicitly using a modified Neural ODE via homotopy continuation.
Further, we developed an acceleration method for HomoODE with a shared
learnable initial point. It is worth noting that our model also provides a
better understanding of why Augmented Neural ODEs work as long as the augmented
part is regarded as the equilibrium point to find. Comprehensive experiments
with several image classification tasks demonstrate that HomoODE surpasses
existing implicit models in terms of both accuracy and memory consumption.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09586" title="Abstract">arXiv:2310.09586</a> [<a href="/pdf/2310.09586" title="Download PDF">pdf</a>, <a href="/format/2310.09586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causality and Independence Enhancement for Biased Node Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guoxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+F">Fangda Guo</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qinglang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jiangli Shao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Most existing methods that address out-of-distribution (OOD) generalization
for node classification on graphs primarily focus on a specific type of data
biases, such as label selection bias or structural bias. However, anticipating
the type of bias in advance is extremely challenging, and designing models
solely for one specific type may not necessarily improve overall generalization
performance. Moreover, limited research has focused on the impact of mixed
biases, which are more prevalent and demanding in real-world scenarios. To
address these limitations, we propose a novel Causality and Independence
Enhancement (CIE) framework, applicable to various graph neural networks
(GNNs). Our approach estimates causal and spurious features at the node
representation level and mitigates the influence of spurious correlations
through the backdoor adjustment. Meanwhile, independence constraint is
introduced to improve the discriminability and stability of causal and spurious
features in complex biased environments. Essentially, CIE eliminates different
types of data biases from a unified perspective, without the need to design
separate methods for each bias as before. To evaluate the performance under
specific types of data biases, mixed biases, and low-resource scenarios, we
conducted comprehensive experiments on five publicly available datasets.
Experimental results demonstrate that our approach CIE not only significantly
enhances the performance of GNNs but outperforms state-of-the-art debiased node
classification methods.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09589" title="Abstract">arXiv:2310.09589</a> [<a href="/pdf/2310.09589" title="Download PDF">pdf</a>, <a href="/format/2310.09589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Airborne Sense and Detect of Drones using LiDAR and adapted PointPillars  DNN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manduhu%2C+M">Manduhu Manduhu</a>, 
<a href="/search/cs?searchtype=author&query=Dow%2C+A">Alexander Dow</a>, 
<a href="/search/cs?searchtype=author&query=Trslic%2C+P">Petar Trslic</a>, 
<a href="/search/cs?searchtype=author&query=Dooly%2C+G">Gerard Dooly</a>, 
<a href="/search/cs?searchtype=author&query=Blanck%2C+B">Benjamin Blanck</a>, 
<a href="/search/cs?searchtype=author&query=Riordan%2C+J">James Riordan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The safe operation of drone swarms beyond visual line of sight requires
multiple safeguards to mitigate the risk of collision between drones flying in
hyper localised scenarios. Cooperative navigation and flight coordination
strategies that rely on pre-planned trajectories and require constant network
connectivity are brittle to failure. Drone embedded sense and detect offers a
comprehensive mode of separation between drones for deconfliction and collision
avoidance. This paper presents the first airborne LiDAR based solution for
drone-swarm detection and localisation using 3D deep learning. It adapts and
embeds the PointPillars deep learning neural network on the drone. To collect
training data of close-quarter multi drone operations and safety critical
scenarios, a scenario Digital Twin is used to augment real datasets with high
fidelity synthetic data. The method has been validated in real-world tests. The
trained model achieves over 80% recall and 96% precision when tested on real
datasets. By incorporating a detection-by-tracking algorithm the system can
reliably monitor the separation distance of multiple drones in challenging
environments.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09590" title="Abstract">arXiv:2310.09590</a> [<a href="/pdf/2310.09590" title="Download PDF">pdf</a>, <a href="/format/2310.09590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Math Word Problems with Reexamination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bin%2C+Y">Yi Bin</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenhao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yujuan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Math word problem (MWP) solving aims to understand the descriptive math
problem and calculate the result, for which previous efforts are mostly devoted
to upgrade different technical modules. This paper brings a different
perspective of \textit{reexamination process} during training by introducing a
pseudo-dual task to enhance the MWP solving. We propose a pseudo-dual (PseDual)
learning scheme to model such process, which is model-agnostic thus can be
adapted to any existing MWP solvers. The pseudo-dual task is specifically
defined as filling the numbers in the expression back into the original word
problem with numbers masked. To facilitate the effective joint learning of the
two tasks, we further design a scheduled fusion strategy for the number
infilling task, which smoothly switches the input from the ground-truth math
expressions to the predicted ones. Our pseudo-dual learning scheme has been
tested and proven effective when being equipped in several representative MWP
solvers through empirical studies. \textit{The codes and trained models are
available at:} \url{https://github.com/steven640pixel/PsedualMWP}.
\end{abstract}
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09593" title="Abstract">arXiv:2310.09593</a> [<a href="/pdf/2310.09593" title="Download PDF">pdf</a>, <a href="/format/2310.09593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-aware Session-based Recommendation with Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">JianXiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 10 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Session-based recommendation (SBR) is a task that aims to predict items based
on anonymous sequences of user behaviors in a session. While there are methods
that leverage rich context information in sessions for SBR, most of them have
the following limitations: 1) they fail to distinguish the item-item edge types
when constructing the global graph for exploiting cross-session contexts; 2)
they learn a fixed embedding vector for each item, which lacks the flexibility
to reflect the variation of user interests across sessions; 3) they generally
use the one-hot encoded vector of the target item as the hard label to predict,
thus failing to capture the true user preference. To solve these issues, we
propose CARES, a novel context-aware session-based recommendation model with
graph neural networks, which utilizes different types of contexts in sessions
to capture user interests. Specifically, we first construct a multi-relation
cross-session graph to connect items according to intra- and cross-session
item-level contexts. Further, to encode the variation of user interests, we
design personalized item representations. Finally, we employ a label
collaboration strategy for generating soft user preference distribution as
labels. Experiments on three benchmark datasets demonstrate that CARES
consistently outperforms state-of-the-art models in terms of P@20 and MRR@20.
Our data and codes are publicly available at
https://github.com/brilliantZhang/CARES.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09596" title="Abstract">arXiv:2310.09596</a> [<a href="/pdf/2310.09596" title="Download PDF">pdf</a>, <a href="/format/2310.09596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RethinkingTMSC: An Empirical Study for Target-Oriented Multimodal  Sentiment Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Junjie Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Junfeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, Target-oriented Multimodal Sentiment Classification (TMSC) has
gained significant attention among scholars. However, current multimodal models
have reached a performance bottleneck. To investigate the causes of this
problem, we perform extensive empirical evaluation and in-depth analysis of the
datasets to answer the following questions: Q1: Are the modalities equally
important for TMSC? Q2: Which multimodal fusion modules are more effective? Q3:
Do existing datasets adequately support the research? Our experiments and
analyses reveal that the current TMSC systems primarily rely on the textual
modality, as most of targets' sentiments can be determined solely by text.
Consequently, we point out several directions to work on for the TMSC task in
terms of model design and dataset construction. The code and data can be found
in https://github.com/Junjie-Ye/RethinkingTMSC.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09599" title="Abstract">arXiv:2310.09599</a> [<a href="/pdf/2310.09599" title="Download PDF">pdf</a>, <a href="/ps/2310.09599" title="Download PostScript">ps</a>, <a href="/format/2310.09599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient two-grid fourth-order compact difference scheme with  variable-step BDF2 method for the semilinear parabolic equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+B">Bingyin Zhang</a>, 
<a href="/search/math?searchtype=author&query=Fu%2C+H">Hongfei Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Due to the lack of corresponding analysis on appropriate mapping operator
between two grids, high-order two-grid difference algorithms are rarely
studied. In this paper, we firstly discuss the boundedness of a local bi-cubic
Lagrange interpolation operator. And then, taking the semilinear parabolic
equation as an example, we first construct a variable-step high-order nonlinear
difference algorithm using compact difference technique in space and the
second-order backward differentiation formula (BDF2) with variable temporal
stepsize in time. With the help of discrete orthogonal convolution (DOC)
kernels and a cut-off numerical technique, the unique solvability and
corresponding error estimates of the high-order nonlinear difference scheme are
established under assumptions that the temporal stepsize ratio satisfies rk &lt;
4.8645 and the maximum temporal stepsize satisfies tau = o(h^1/2 ). Then, an
efficient two-grid high-order difference algorithm is developed by combining a
small-scale variable-step high-order nonlinear difference algorithm on the
coarse grid and a large-scale variable-step high-order linearized difference
algorithm on the fine grid, in which the constructed piecewise bi-cubic
Lagrange interpolation mapping operator is adopted to project the coarse-grid
solution to the fine grid. Under the same temporal stepsize ratio restriction
rk &lt; 4.8645 and a weaker maximum temporal stepsize condition tau = o(H^1.2 ),
optimal fourth-order in space and second-order in time error estimates of the
two-grid difference scheme is established if the coarse-fine grid stepsizes
satisfy H = O(h^4/7). Finally, several numerical experiments are carried out to
demonstrate the effectiveness and efficiency of the proposed scheme.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09600" title="Abstract">arXiv:2310.09600</a> [<a href="/pdf/2310.09600" title="Download PDF">pdf</a>, <a href="/format/2310.09600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hawkeye: A PyTorch-based Library for Fine-Grained Image Recognition with  Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiabei He</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiu-Shen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Ye Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM OSC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fine-Grained Image Recognition (FGIR) is a fundamental and challenging task
in computer vision and multimedia that plays a crucial role in Intellectual
Economy and Industrial Internet applications. However, the absence of a unified
open-source software library covering various paradigms in FGIR poses a
significant challenge for researchers and practitioners in the field. To
address this gap, we present Hawkeye, a PyTorch-based library for FGIR with
deep learning. Hawkeye is designed with a modular architecture, emphasizing
high-quality code and human-readable configuration, providing a comprehensive
solution for FGIR tasks. In Hawkeye, we have implemented 16 state-of-the-art
fine-grained methods, covering 6 different paradigms, enabling users to explore
various approaches for FGIR. To the best of our knowledge, Hawkeye represents
the first open-source PyTorch-based library dedicated to FGIR. It is publicly
available at https://github.com/Hawkeye-FineGrained/Hawkeye/, providing
researchers and practitioners with a powerful tool to advance their research
and development in the field of FGIR.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09604" title="Abstract">arXiv:2310.09604</a> [<a href="/pdf/2310.09604" title="Download PDF">pdf</a>, <a href="/format/2310.09604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Hierarchical Features with Joint Latent Space Energy-Based  Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiali Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tian Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper studies the fundamental problem of multi-layer generator models in
learning hierarchical representations. The multi-layer generator model that
consists of multiple layers of latent variables organized in a top-down
architecture tends to learn multiple levels of data abstraction. However, such
multi-layer latent variables are typically parameterized to be Gaussian, which
can be less informative in capturing complex abstractions, resulting in limited
success in hierarchical representation learning. On the other hand, the
energy-based (EBM) prior is known to be expressive in capturing the data
regularities, but it often lacks the hierarchical structure to capture
different levels of hierarchical representations. In this paper, we propose a
joint latent space EBM prior model with multi-layer latent variables for
effective hierarchical representation learning. We develop a variational joint
learning scheme that seamlessly integrates an inference model for efficient
inference. Our experiments demonstrate that the proposed joint EBM prior is
effective and expressive in capturing hierarchical representations and
modelling data distribution.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09605" title="Abstract">arXiv:2310.09605</a> [<a href="/pdf/2310.09605" title="Download PDF">pdf</a>, <a href="/format/2310.09605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Penetrative AI: Making LLMs Comprehend the Physical World
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huatao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Liying Han</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mo Li</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+M">Mani Srivastava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent developments in Large Language Models (LLMs) have demonstrated their
remarkable capabilities across a range of tasks. Questions, however, persist
about the nature of LLMs and their potential to integrate common-sense human
knowledge when performing tasks involving information about the real physical
world. This paper delves into these questions by exploring how LLMs can be
extended to interact with and reason about the physical world through IoT
sensors and actuators, a concept that we term "\textit{Penetrative AI}". The
paper explores such an extension at two levels of LLMs' ability to penetrate
into the physical world via the processing of sensory signals. Our preliminary
findings indicate that LLMs, with ChatGPT being the representative example in
our exploration, have considerable and unique proficiency in employing the
knowledge they learned during training for interpreting IoT sensor data and
reasoning over them about tasks in the physical realm. Not only this opens up
new applications for LLMs beyond traditional text-based tasks, but also enables
new ways of incorporating human knowledge in cyber-physical systems.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09607" title="Abstract">arXiv:2310.09607</a> [<a href="/pdf/2310.09607" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of the Penetration of 5G Wireless RF-EMF on Human Skin
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Annalakshmi%2C+N">N.Annalakshmi</a>, 
<a href="/search/cs?searchtype=author&query=Umarani%2C+S">S.Umarani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">New wireless mobile technology has been released every ten years, improving
previous generations' facilities. Even though 5G supports many services on
demand nowadays, its higher radiation increases the queries about the safety of
humans and other living things. The health effects related to EMF of 5G is
still under in discussion. Number of health organizations such as ICNIRP, FCC
and WHO have specified the 5G safety guidelines and suggested the minimum
distance between UE and BS. But this is unsafe for humans while using high
frequencies. The unique position and functions of the skin are not considered
by the most recent revisions to the ICNIRP exposure limits. Although 5G
radiation has an impact on biological matter, a connection between
physiological consequences and health issues is not being investigated. It is
time to establish a task force to clarify the consequences of 5G radiation on
the skin and overall health, and to change exposure restrictions as necessary.
This paper investigates the human Electro Magnetic Field (EMF) exposure from 5G
wireless communication. It analyzes the impact of the penetration level of EMF
in human skin at 28GHz using the Specific Absorption Rate metric and discusses
the reduction of EMF exposure level of 5G.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09609" title="Abstract">arXiv:2310.09609</a> [<a href="/pdf/2310.09609" title="Download PDF">pdf</a>, <a href="/format/2310.09609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Intelligent Network Management: Leveraging AI for Network  Service Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K+N">Khuong N. Nguyen</a> (1), 
<a href="/search/cs?searchtype=author&query=Sehgal%2C+A">Abhishek Sehgal</a> (1), 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuming Zhu</a> (1), 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Junsu Choi</a> (2), 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanbo Chen</a> (1), 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a> (1), 
<a href="/search/cs?searchtype=author&query=Ng%2C+B+L">Boon Loong Ng</a> (1), 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Charlie Zhang</a> (1) ((1) Standards and Mobility Innovation Laboratory - Samsung Research America, (2) Samsung Electronics Co., Ltd)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">As the complexity and scale of modern computer networks continue to increase,
there has emerged an urgent need for precise traffic analysis, which plays a
pivotal role in cutting-edge wireless connectivity technologies. This study
focuses on leveraging Machine Learning methodologies to create an advanced
network traffic classification system. We introduce a novel data-driven
approach that excels in identifying various network service types in real-time,
by analyzing patterns within the network traffic. Our method organizes similar
kinds of network traffic into distinct categories, referred to as network
services, based on latency requirement. Furthermore, it decomposes the network
traffic stream into multiple, smaller traffic flows, with each flow uniquely
carrying a specific service. Our ML models are trained on a dataset comprised
of labeled examples representing different network service types collected on
various Wi-Fi network conditions. Upon evaluation, our system demonstrates a
remarkable accuracy in distinguishing the network services. These results
emphasize the substantial promise of integrating Artificial Intelligence in
wireless technologies. Such an approach encourages more efficient energy
consumption, enhances Quality of Service assurance, and optimizes the
allocation of network resources, thus laying a solid groundwork for the
development of advanced intelligent networks.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09611" title="Abstract">arXiv:2310.09611</a> [<a href="/pdf/2310.09611" title="Download PDF">pdf</a>, <a href="/format/2310.09611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VizAbility: Multimodal Accessible Data Visualization with Keyboard  Navigation and Conversational Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorniak%2C+J">Joshua Gorniak</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Gwon%2C+S">Stephen Gwon</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Donglai Wei</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N+W">Nam Wook Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Data visualization serves as a crucial tool for communicating important
information in our society. Yet, as visualizations grow more complex, they
become less accessible to individuals with visual impairments. Traditional
accessibility approaches like alternative text and data tables often fall short
of capturing the full potential of data visualization. To bridge this gap, we
introduce VizAbility, a novel multimodal accessible system that combines
keyboard navigation with conventional interaction, enabling individuals with
visual impairments to actively engage with and explore data visualizations.
VizAbility utilizes an LLM-based pipeline, seamlessly integrating data, chart
structures, user locality, and web-based information to provide comprehensive
answers. Our quantitative evaluation validates the LLM-based
question-and-answer pipeline, and a user study involving six participants
underscores the promising potential of VizAbility's multimodal approach. We
discuss how current visualization tools can integrate VizAbility to enhance the
accessibility of data visualizations online.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09612" title="Abstract">arXiv:2310.09612</a> [<a href="/pdf/2310.09612" title="Download PDF">pdf</a>, <a href="/format/2310.09612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Neural Networks Can Learn Generalizable Same-Different Visual  Relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tartaglini%2C+A+R">Alexa R. Tartaglini</a>, 
<a href="/search/cs?searchtype=author&query=Feucht%2C+S">Sheridan Feucht</a>, 
<a href="/search/cs?searchtype=author&query=Lepori%2C+M+A">Michael A. Lepori</a>, 
<a href="/search/cs?searchtype=author&query=Vong%2C+W+K">Wai Keen Vong</a>, 
<a href="/search/cs?searchtype=author&query=Lovering%2C+C">Charles Lovering</a>, 
<a href="/search/cs?searchtype=author&query=Lake%2C+B+M">Brenden M. Lake</a>, 
<a href="/search/cs?searchtype=author&query=Pavlick%2C+E">Ellie Pavlick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although deep neural networks can achieve human-level performance on many
object recognition benchmarks, prior work suggests that these same models fail
to learn simple abstract relations, such as determining whether two objects are
the same or different. Much of this prior work focuses on training
convolutional neural networks to classify images of two same or two different
abstract shapes, testing generalization on within-distribution stimuli. In this
article, we comprehensively study whether deep neural networks can acquire and
generalize same-different relations both within and out-of-distribution using a
variety of architectures, forms of pretraining, and fine-tuning datasets. We
find that certain pretrained transformers can learn a same-different relation
that generalizes with near perfect accuracy to out-of-distribution stimuli.
Furthermore, we find that fine-tuning on abstract shapes that lack texture or
color provides the strongest out-of-distribution generalization. Our results
suggest that, with the right approach, deep neural networks can learn
generalizable same-different visual relations.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09613" title="Abstract">arXiv:2310.09613</a> [<a href="/pdf/2310.09613" title="Download PDF">pdf</a>, <a href="/ps/2310.09613" title="Download PostScript">ps</a>, <a href="/format/2310.09613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combinatorial Group Testing in Presence of Deletions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gandikota%2C+V">Venkata Gandikota</a>, 
<a href="/search/cs?searchtype=author&query=Polyanskii%2C+N">Nikita Polyanskii</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haodong Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, no figures, 6 Algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The study in group testing aims to develop strategies to identify a small set
of defective items among a large population using a few pooled tests. The
established techniques have been highly beneficial in a broad spectrum of
applications ranging from channel communication to identifying
COVID-19-infected individuals efficiently. Despite significant research on
group testing and its variants since the 1940s, testing strategies robust to
deletion noise have yet to be studied. Many practical systems exhibit deletion
errors, for instance, in wireless communication and data storage systems. Such
deletions of test outcomes lead to asynchrony between the tests, which the
current group testing strategies cannot handle. In this work, we initiate the
study of non-adaptive group testing strategies resilient to deletion noise. We
characterize the necessary and sufficient conditions to successfully identify
the defective items even after the adversarial deletion of certain test
outputs. We also provide constructions of testing matrices along with an
efficient recovery algorithm.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09614" title="Abstract">arXiv:2310.09614</a> [<a href="/pdf/2310.09614" title="Download PDF">pdf</a>, <a href="/format/2310.09614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lost in Visualization: Investigating the Knowledge Gap between  Visualization Research and Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+N+W">Nam Wook Kim</a>, 
<a href="/search/cs?searchtype=author&query=Myers%2C+G">Grace Myers</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jinhan Choi</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y">Yoonsuh Cho</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+C">Changhoon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yea-Seul Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Empirical research on perception and cognition has laid the foundation for
visualization design, often yielding useful design guidelines for
practitioners. However, it remains uncertain how well practitioners stay
informed about the latest findings in visualization research. In this paper, we
employed a mixed-method approach to explore the knowledge gap between
visualization research and real-world design guidelines. We initially collected
existing design guidelines from various sources and empirical studies from
major publishing venues, analyzing their alignment and uncovering missing links
and contradictory knowledge. Subsequently, we conducted surveys and interviews
with practitioners and researchers to gain further insights into their
experiences and attitudes towards design guidelines and empirical studies, and
their views on the knowledge gap between research and practice. Our findings
highlight the similarities and differences in their perspectives and propose
strategies to bridge the divide in visualization design knowledge.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09615" title="Abstract">arXiv:2310.09615</a> [<a href="/pdf/2310.09615" title="Download PDF">pdf</a>, <a href="/format/2310.09615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STORM: Efficient Stochastic Transformer based World Models for  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weipu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yetian Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, model-based reinforcement learning algorithms have demonstrated
remarkable efficacy in visual input environments. These approaches begin by
constructing a parameterized simulation world model of the real environment
through self-supervised learning. By leveraging the imagination of the world
model, the agent's policy is enhanced without the constraints of sampling from
the real environment. The performance of these algorithms heavily relies on the
sequence modeling and generation capabilities of the world model. However,
constructing a perfectly accurate model of a complex unknown environment is
nearly impossible. Discrepancies between the model and reality may cause the
agent to pursue virtual goals, resulting in subpar performance in the real
environment. Introducing random noise into model-based reinforcement learning
has been proven beneficial. In this work, we introduce Stochastic
Transformer-based wORld Model (STORM), an efficient world model architecture
that combines the strong sequence modeling and generation capabilities of
Transformers with the stochastic nature of variational autoencoders. STORM
achieves a mean human performance of $126.7\%$ on the Atari $100$k benchmark,
setting a new record among state-of-the-art methods that do not employ
lookahead search techniques. Moreover, training an agent with $1.85$ hours of
real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics
card requires only $4.3$ hours, showcasing improved efficiency compared to
previous methodologies.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09617" title="Abstract">arXiv:2310.09617</a> [<a href="/pdf/2310.09617" title="Download PDF">pdf</a>, <a href="/format/2310.09617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Good is ChatGPT in Giving Advice on Your Visualization Design?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+N+W">Nam Wook Kim</a>, 
<a href="/search/cs?searchtype=author&query=Myers%2C+G">Grace Myers</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+B">Benjamin Bach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Data visualization practitioners often lack formal training, resulting in a
knowledge gap in visualization design best practices. Large-language models
like ChatGPT, with their vast internet-scale training data, offer
transformative potential in addressing this gap. To explore this potential, we
adopted a mixed-method approach. Initially, we analyzed the VisGuide forum, a
repository of data visualization questions, by comparing ChatGPT-generated
responses to human replies. Subsequently, our user study delved into
practitioners' reactions and attitudes toward ChatGPT as a visualization
assistant. Participants, who brought their visualizations and questions,
received feedback from both human experts and ChatGPT in a randomized order.
They filled out experience surveys and shared deeper insights through
post-interviews. The results highlight the unique advantages and disadvantages
of ChatGPT, such as its ability to quickly provide a wide range of design
options based on a broad knowledge base, while also revealing its limitations
in terms of depth and critical thinking capabilities.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09618" title="Abstract">arXiv:2310.09618</a> [<a href="/pdf/2310.09618" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moral consensus and divergence in partisan language use
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rim%2C+N">Nakwon Rim</a>, 
<a href="/search/cs?searchtype=author&query=Berman%2C+M+G">Marc G. Berman</a>, 
<a href="/search/cs?searchtype=author&query=Leong%2C+Y+C">Yuan Chang Leong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Polarization has increased substantially in political discourse, contributing
to a widening partisan divide. In this paper, we analyzed large-scale,
real-world language use in Reddit communities (294,476,146 comments) and in
news outlets (6,749,781 articles) to uncover psychological dimensions along
which partisan language is divided. Using word embedding models that captured
semantic associations based on co-occurrences of words in vast textual corpora,
we identified patterns of affective polarization present in natural political
discourse. We then probed the semantic associations of words related to seven
political topics (e.g., abortion, immigration) along the dimensions of morality
(moral-to-immoral), threat (threatening-to-safe), and valence
(pleasant-to-unpleasant). Across both Reddit communities and news outlets, we
identified a small but systematic divergence in the moral associations of words
between text sources with different partisan leanings. Moral associations of
words were highly correlated between conservative and liberal text sources
(average $\rho$ = 0.96), but the differences remained reliable to enable us to
distinguish text sources along partisan lines with above 85% classification
accuracy. These findings underscore that despite a shared moral understanding
across the political spectrum, there are consistent differences that shape
partisan language and potentially exacerbate political polarization. Our
results, drawn from both informal interactions on social media and curated
narratives in news outlets, indicate that these trends are widespread.
Leveraging advanced computational techniques, this research offers a fresh
perspective that complements traditional methods in political attitudes.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09619" title="Abstract">arXiv:2310.09619</a> [<a href="/pdf/2310.09619" title="Download PDF">pdf</a>, <a href="/format/2310.09619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Expression Tree Decoding Strategy for Mathematical Equation  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yongliang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Nong%2C+Q">Qingpeng Nong</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z+T+Y">Zeqi Tan Yanna Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Weiming Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Generating mathematical equations from natural language requires an accurate
understanding of the relations among math expressions. Existing approaches can
be broadly categorized into token-level and expression-level generation. The
former treats equations as a mathematical language, sequentially generating
math tokens. Expression-level methods generate each expression one by one.
However, each expression represents a solving step, and there naturally exist
parallel or dependent relations between these steps, which are ignored by
current sequential methods. Therefore, we integrate tree structure into the
expression-level generation and advocate an expression tree decoding strategy.
To generate a tree with expression as its node, we employ a layer-wise parallel
decoding strategy: we decode multiple independent expressions (leaf nodes) in
parallel at each layer and repeat parallel decoding layer by layer to
sequentially generate these parent node expressions that depend on others.
Besides, a bipartite matching algorithm is adopted to align multiple
predictions with annotations for each layer. Experiments show our method
outperforms other baselines, especially for these equations with complex
structures.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09620" title="Abstract">arXiv:2310.09620</a> [<a href="/pdf/2310.09620" title="Download PDF">pdf</a>, <a href="/format/2310.09620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning for Urban Air Quality Analytics: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jindong Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weijia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The increasing air pollution poses an urgent global concern with far-reaching
consequences, such as premature mortality and reduced crop yield, which
significantly impact various aspects of our daily lives. Accurate and timely
analysis of air pollution is crucial for understanding its underlying
mechanisms and implementing necessary precautions to mitigate potential
socio-economic losses. Traditional analytical methodologies, such as
atmospheric modeling, heavily rely on domain expertise and often make
simplified assumptions that may not be applicable to complex air pollution
problems. In contrast, Machine Learning (ML) models are able to capture the
intrinsic physical and chemical rules by automatically learning from a large
amount of historical observational data, showing great promise in various air
quality analytical tasks. In this article, we present a comprehensive survey of
ML-based air quality analytics, following a roadmap spanning from data
acquisition to pre-processing, and encompassing various analytical tasks such
as pollution pattern mining, air quality inference, and forecasting. Moreover,
we offer a systematic categorization and summary of existing methodologies and
applications, while also providing a list of publicly available air quality
datasets to ease the research in this direction. Finally, we identify several
promising future research directions. This survey can serve as a valuable
resource for professionals seeking suitable solutions for their specific
challenges and advancing their research at the cutting edge.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09621" title="Abstract">arXiv:2310.09621</a> [<a href="/pdf/2310.09621" title="Download PDF">pdf</a>, <a href="/format/2310.09621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prime Match: A Privacy-Preserving Inventory Matching System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Polychroniadou%2C+A">Antigoni Polychroniadou</a>, 
<a href="/search/cs?searchtype=author&query=Asharov%2C+G">Gilad Asharov</a>, 
<a href="/search/cs?searchtype=author&query=Diamond%2C+B">Benjamin Diamond</a>, 
<a href="/search/cs?searchtype=author&query=Balch%2C+T">Tucker Balch</a>, 
<a href="/search/cs?searchtype=author&query=Buehler%2C+H">Hans Buehler</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+R">Richard Hua</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Suwen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Gimler%2C+G">Greg Gimler</a>, 
<a href="/search/cs?searchtype=author&query=Veloso%2C+M">Manuela Veloso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 7 figures, USENIX Security 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Prime match: A privacy-preserving inventory matching system. In
  Joseph A. Calandrino and Carmela Troncoso, editors, 32nd USENIX Security
  Symposium, USENIX Security 2023, Anaheim, CA, USA, August 9-11, 2023. USENIX
  Association, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; General Economics (econ.GN); Trading and Market Microstructure (q-fin.TR)

</div>
<p class="mathjax">Inventory matching is a standard mechanism/auction for trading financial
stocks by which buyers and sellers can be paired. In the financial world, banks
often undertake the task of finding such matches between their clients. The
related stocks can be traded without adversely impacting the market price for
either client. If matches between clients are found, the bank can offer the
trade at advantageous rates. If no match is found, the parties have to buy or
sell the stock in the public market, which introduces additional costs. A
problem with the process as it is presently conducted is that the involved
parties must share their order to buy or sell a particular stock, along with
the intended quantity (number of shares), to the bank. Clients worry that if
this information were to leak somehow, then other market participants would
become aware of their intentions and thus cause the price to move adversely
against them before their transaction finalizes. We provide a solution, Prime
Match, that enables clients to match their orders efficiently with reduced
market impact while maintaining privacy. In the case where there are no
matches, no information is revealed. Our main cryptographic innovation is a
two-round secure linear comparison protocol for computing the minimum between
two quantities without preprocessing and with malicious security, which can be
of independent interest. We report benchmarks of our Prime Match system, which
runs in production and is adopted by J.P. Morgan. The system is designed
utilizing a star topology network, which provides clients with a centralized
node (the bank) as an alternative to the idealized assumption of point-to-point
connections, which would be impractical and undesired for the clients to
implement in reality. Prime Match is the first secure multiparty computation
solution running live in the traditional financial world.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09623" title="Abstract">arXiv:2310.09623</a> [<a href="/pdf/2310.09623" title="Download PDF">pdf</a>, <a href="/format/2310.09623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Digital Language Coherence Marker for Monitoring Dementia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gkoumas%2C+D">Dimitris Gkoumas</a>, 
<a href="/search/cs?searchtype=author&query=Tsakalidis%2C+A">Adam Tsakalidis</a>, 
<a href="/search/cs?searchtype=author&query=Liakata%2C+M">Maria Liakata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It has been accepted to appear at EMNLP23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The use of spontaneous language to derive appropriate digital markers has
become an emergent, promising and non-intrusive method to diagnose and monitor
dementia. Here we propose methods to capture language coherence as a
cost-effective, human-interpretable digital marker for monitoring cognitive
changes in people with dementia. We introduce a novel task to learn the
temporal logical consistency of utterances in short transcribed narratives and
investigate a range of neural approaches. We compare such language coherence
patterns between people with dementia and healthy controls and conduct a
longitudinal evaluation against three clinical bio-markers to investigate the
reliability of our proposed digital coherence marker. The coherence marker
shows a significant difference between people with mild cognitive impairment,
those with Alzheimer's Disease and healthy controls. Moreover our analysis
shows high association between the coherence marker and the clinical
bio-markers as well as generalisability potential to other related conditions.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09624" title="Abstract">arXiv:2310.09624</a> [<a href="/pdf/2310.09624" title="Download PDF">pdf</a>, <a href="/format/2310.09624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASSERT: Automated Safety Scenario Red Teaming for Evaluating the  Robustness of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+A">Alex Mei</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+S">Sharon Levy</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Findings of the 2023 Conference on Empirical Methods in Natural Language Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">As large language models are integrated into society, robustness toward a
suite of prompts is increasingly important to maintain reliability in a
high-variance environment.Robustness evaluations must comprehensively
encapsulate the various settings in which a user may invoke an intelligent
system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,
consisting of three methods -- semantically aligned augmentation, target
bootstrapping, and adversarial knowledge injection. For robust safety
evaluation, we apply these methods in the critical domain of AI safety to
algorithmically generate a test suite of prompts covering diverse robustness
settings -- semantic equivalence, related scenarios, and adversarial. We
partition our prompts into four safety domains for a fine-grained analysis of
how the domain affects model performance. Despite dedicated safeguards in
existing state-of-the-art models, we find statistically significant performance
differences of up to 11% in absolute classification accuracy among semantically
related scenarios and error rates of up to 19% absolute error in zero-shot
adversarial settings, raising concerns for users' physical safety.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09626" title="Abstract">arXiv:2310.09626</a> [<a href="/pdf/2310.09626" title="Download PDF">pdf</a>, <a href="/ps/2310.09626" title="Download PostScript">ps</a>, <a href="/format/2310.09626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Current and Future Challenges in Humanoid Robotics -- An Empirical  Investigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paetzel-Pr%C3%BCsmann%2C+M">Maike Paetzel-Pr&#xfc;smann</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+A">Alessandra Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Keijsers%2C+M">Merel Keijsers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The goal of RoboCup is to make research in the area of robotics measurable
over time, and grow a community that works together to solve increasingly
difficult challenges over the years. The most ambitious of these challenges it
to be able to play against the human world champions in soccer in 2050. To
better understand what members of the RoboCup community believes to be the
state of the art and the main challenges in the next decade and towards the
2050 game, we developed a survey and distributed it to members of different
experience level and background within the community. We present data from 39
responses. Results highlighted that locomotion, awareness and decision-making,
and robustness of robots are among those considered of high importance for the
community, while human-robot interaction and natural language processing and
generation are rated of low in importance and difficulty.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09627" title="Abstract">arXiv:2310.09627</a> [<a href="/pdf/2310.09627" title="Download PDF">pdf</a>, <a href="/format/2310.09627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Moving Objects Using a Novel Optical-Flow-Based  Range-Independent Invariant
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raviv%2C+D">Daniel Raviv</a>, 
<a href="/search/cs?searchtype=author&query=Yepes%2C+J+D">Juan D. Yepes</a>, 
<a href="/search/cs?searchtype=author&query=Gowda%2C+A">Ayush Gowda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">This paper focuses on a novel approach for detecting moving objects during
camera motion. We present an optical-flow-based transformation that yields a
consistent 2D invariant image output regardless of time instants, range of
points in 3D, and the speed of the camera. In other words, this transformation
generates a lookup image that remains invariant despite the changing projection
of the 3D scene and camera motion. In the new domain, projections of 3D points
that deviate from the values of the predefined lookup image can be clearly
identified as moving relative to the stationary 3D environment, making them
seamlessly detectable. The method does not require prior knowledge of the
direction of motion or speed of the camera, nor does it necessitate 3D point
range information. It is well-suited for real-time parallel processing,
rendering it highly practical for implementation. We have validated the
effectiveness of the new domain through simulations and experiments,
demonstrating its robustness in scenarios involving rectilinear camera motion,
both in simulations and with real-world data. This approach introduces new ways
for moving objects detection during camera motion, and also lays the foundation
for future research in the context of moving object detection during
six-degrees-of-freedom camera motion.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09628" title="Abstract">arXiv:2310.09628</a> [<a href="/pdf/2310.09628" title="Download PDF">pdf</a>, <a href="/format/2310.09628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Battery Diagnosis and Prognosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altinpulluk%2C+N+B">Nur Banu Altinpulluk</a>, 
<a href="/search/cs?searchtype=author&query=Altinpulluk%2C+D">Deniz Altinpulluk</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+P">Paritosh Ramanan</a>, 
<a href="/search/cs?searchtype=author&query=Paulson%2C+N">Noah Paulson</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+F">Feng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Babinec%2C+S">Susan Babinec</a>, 
<a href="/search/cs?searchtype=author&query=Yildirim%2C+M">Murat Yildirim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Battery diagnosis, prognosis and health management models play a critical
role in the integration of battery systems in energy and mobility fields.
However, large-scale deployment of these models is hindered by a myriad of
challenges centered around data ownership, privacy, communication, and
processing. State-of-the-art battery diagnosis and prognosis methods require
centralized collection of data, which further aggravates these challenges. Here
we propose a federated battery prognosis model, which distributes the
processing of battery standard current-voltage-time-usage data in a
privacy-preserving manner. Instead of exchanging raw standard
current-voltage-time-usage data, our model communicates only the model
parameters, thus reducing communication load and preserving data
confidentiality. The proposed model offers a paradigm shift in battery health
management through privacy-preserving distributed methods for battery data
processing and remaining lifetime prediction.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09629" title="Abstract">arXiv:2310.09629</a> [<a href="/pdf/2310.09629" title="Download PDF">pdf</a>, <a href="/format/2310.09629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Online Replanning with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Siyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengdi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yikang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+W">Wei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+C">Chuang Gan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Diffusion models have risen as a promising approach to data-driven planning,
and have demonstrated impressive robotic control, reinforcement learning, and
video planning performance. Given an effective planner, an important question
to consider is replanning -- when given plans should be regenerated due to both
action execution error and external environment changes. Direct plan execution,
without replanning, is problematic as errors from individual actions rapidly
accumulate and environments are partially observable and stochastic.
Simultaneously, replanning at each timestep incurs a substantial computational
cost, and may prevent successful task execution, as different generated plans
prevent consistent progress to any particular goal. In this paper, we explore
how we may effectively replan with diffusion models. We propose a principled
approach to determine when to replan, based on the diffusion model's estimated
likelihood of existing generated plans. We further present an approach to
replan existing trajectories to ensure that new plans follow the same goal
state as the original trajectory, which may efficiently bootstrap off
previously generated plans. We illustrate how a combination of our proposed
additions significantly improves the performance of diffusion planners leading
to 38\% gains over past diffusion planning approaches on Maze2D, and further
enables the handling of stochastic and long-horizon robotic control tasks.
Videos can be found on the anonymous website:
\url{https://vis-www.cs.umass.edu/replandiffuser/}.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09630" title="Abstract">arXiv:2310.09630</a> [<a href="/pdf/2310.09630" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Traffic Sign Detection: A Case Study in a Santa Clara Suburban  Neighborhood
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loghashankar%2C+H">Harish Loghashankar</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Hieu Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This research project aims to develop a real-time traffic sign detection
system using the YOLOv5 architecture and deploy it for efficient traffic sign
recognition during a drive in a suburban neighborhood. The project's primary
objectives are to train the YOLOv5 model on a diverse dataset of traffic sign
images and deploy the model on a suitable hardware platform capable of
real-time inference. The project will involve collecting a comprehensive
dataset of traffic sign images. By leveraging the trained YOLOv5 model, the
system will detect and classify traffic signs from a real-time camera on a
dashboard inside a vehicle. The performance of the deployed system will be
evaluated based on its accuracy in detecting traffic signs, real-time
processing speed, and overall reliability. During a case study in a suburban
neighborhood, the system demonstrated a notable 96% accuracy in detecting
traffic signs. This research's findings have the potential to improve road
safety and traffic management by providing timely and accurate real-time
information about traffic signs and can pave the way for further research into
autonomous driving.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09631" title="Abstract">arXiv:2310.09631</a> [<a href="/pdf/2310.09631" title="Download PDF">pdf</a>, <a href="/format/2310.09631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Landslide Topology Uncovers Failure Movements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rana%2C+K">Kamal Rana</a>, 
<a href="/search/cs?searchtype=author&query=Bhuyan%2C+K">Kushanav Bhuyan</a>, 
<a href="/search/cs?searchtype=author&query=Ferrer%2C+J+V">Joaquin Vicente Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Cotton%2C+F">Fabrice Cotton</a>, 
<a href="/search/cs?searchtype=author&query=Ozturk%2C+U">Ugur Ozturk</a>, 
<a href="/search/cs?searchtype=author&query=Catani%2C+F">Filippo Catani</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+N">Nishant Malik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The death toll and monetary damages from landslides continue to rise despite
advancements in predictive modeling. The predictive capability of these models
is limited as landslide databases used in training and assessing the models
often have crucial information missing, such as underlying failure types. Here,
we present an approach for identifying failure types based on their movements,
e.g., slides and flows by leveraging 3D landslide topology. We observe
topological proxies reveal prevalent signatures of mass movement mechanics
embedded in the landslide's morphology or shape, such as detecting coupled
movement styles within complex landslides. We find identical failure types
exhibit similar topological properties, and by using them as predictors, we can
identify failure types in historic and event-specific landslide databases
(including multi-temporal) from various geomorphological and climatic contexts
such as Italy, the US Pacific Northwest region, Denmark, Turkey, and China with
80 to 94 % accuracy. To demonstrate the real-world application of the method,
we implement it in two undocumented datasets from China and publicly release
the datasets. These new insights can considerably improve the performance of
landslide predictive models and impact assessments. Moreover, our work
introduces a new paradigm for studying landslide shapes to understand
underlying processes through the lens of landslide topology.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09632" title="Abstract">arXiv:2310.09632</a> [<a href="/pdf/2310.09632" title="Download PDF">pdf</a>, <a href="/format/2310.09632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-based Mapping of Space Using Visual Motion Invariants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yepes%2C+J+D">Juan D. Yepes</a>, 
<a href="/search/cs?searchtype=author&query=Raviv%2C+D">Daniel Raviv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">This paper focuses on visual motion-based invariants that result in a
representation of 3D points in which the stationary environment remains
invariant, ensuring shape constancy. This is achieved even as the images
undergo constant change due to camera motion. Nonlinear functions of measurable
optical flow, which are related to geometric 3D invariants, are utilized to
create a novel representation. We refer to the resulting optical flow-based
invariants as 'Time-Clearance' and the well-known 'Time-to-Contact' (TTC).
Since these invariants remain constant over time, it becomes straightforward to
detect moving points that do not adhere to the expected constancy. We present
simulations of a camera moving relative to a 3D object, snapshots of its
projected images captured by a rectilinearly moving camera, and the object as
it appears unchanged in the new domain over time. In addition, Unity-based
simulations demonstrate color-coded transformations of a projected 3D scene,
illustrating how moving objects can be readily identified. This representation
is straightforward, relying on simple optical flow functions. It requires only
one camera, and there is no need to determine the magnitude of the camera's
velocity vector. Furthermore, the representation is pixel-based, making it
suitable for parallel processing.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09633" title="Abstract">arXiv:2310.09633</a> [<a href="/pdf/2310.09633" title="Download PDF">pdf</a>, <a href="/format/2310.09633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dimma: Semi-supervised Low Light Image Enhancement with Adaptive Dimming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koz%C5%82owski%2C+W">Wojciech Koz&#x142;owski</a>, 
<a href="/search/cs?searchtype=author&query=Szachniewicz%2C+M">Micha&#x142; Szachniewicz</a>, 
<a href="/search/cs?searchtype=author&query=Stypu%C5%82kowski%2C+M">Micha&#x142; Stypu&#x142;kowski</a>, 
<a href="/search/cs?searchtype=author&query=Zi%C4%99ba%2C+M">Maciej Zi&#x119;ba</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Enhancing low-light images while maintaining natural colors is a challenging
problem due to camera processing variations and limited access to photos with
ground-truth lighting conditions. The latter is a crucial factor for supervised
methods that achieve good results on paired datasets but do not handle
out-of-domain data well. On the other hand, unsupervised methods, while able to
generalize, often yield lower-quality enhancements. To fill this gap, we
propose Dimma, a semi-supervised approach that aligns with any camera by
utilizing a small set of image pairs to replicate scenes captured under extreme
lighting conditions taken by that specific camera. We achieve that by
introducing a convolutional mixture density network that generates distorted
colors of the scene based on the illumination differences. Additionally, our
approach enables accurate grading of the dimming factor, which provides a wide
range of control and flexibility in adjusting the brightness levels during the
low-light image enhancement process. To further improve the quality of our
results, we introduce an architecture based on a conditional UNet. The
lightness value provided by the user serves as the conditional input to
generate images with the desired lightness. Our approach using only few image
pairs achieves competitive results compared to fully supervised methods.
Moreover, when trained on the full dataset, our model surpasses
state-of-the-art methods in some metrics and closely approaches them in others.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09634" title="Abstract">arXiv:2310.09634</a> [<a href="/pdf/2310.09634" title="Download PDF">pdf</a>, <a href="/format/2310.09634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An End-to-End System for Reproducibility Assessment of Source Code  Repositories via Their Readmes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akdeniz%2C+E+K">Ey&#xfc;p Kaan Akdeniz</a>, 
<a href="/search/cs?searchtype=author&query=Tekir%2C+S">Selma Tekir</a>, 
<a href="/search/cs?searchtype=author&query=Hinnawi%2C+M+N+A+A">Malik Nizar Asad Al Hinnawi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Increased reproducibility of machine learning research has been a driving
force for dramatic improvements in learning performances. The scientific
community further fosters this effort by including reproducibility ratings in
reviewer forms and considering them as a crucial factor for the overall
evaluation of papers. Accompanying source code is not sufficient to make a work
reproducible. The shared codes should meet the ML reproducibility checklist as
well. This work aims to support reproducibility evaluations of papers with
source codes. We propose an end-to-end system that operates on the Readme file
of the source code repositories. The system checks the compliance of a given
Readme to a template proposed by a widely used platform for sharing source
codes of research. Our system generates scores based on a custom function to
combine section scores. We also train a hierarchical transformer model to
assign a class label to a given Readme. The experimental results show that the
section similarity-based system performs better than the hierarchical
transformer. Moreover, it has an advantage regarding explainability since one
can directly relate the score to the sections of Readme files.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09636" title="Abstract">arXiv:2310.09636</a> [<a href="/pdf/2310.09636" title="Download PDF">pdf</a>, <a href="/format/2310.09636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Adversarial Training for Text-to-Speech Synthesis Based on  Raw Phonetic Input and Explicit Prosody Modelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boros%2C+T">Tiberiu Boros</a>, 
<a href="/search/cs?searchtype=author&query=Dumitrescu%2C+S+D">Stefan Daniel Dumitrescu</a>, 
<a href="/search/cs?searchtype=author&query=Mironica%2C+I">Ionut Mironica</a>, 
<a href="/search/cs?searchtype=author&query=Chivereanu%2C+R">Radu Chivereanu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We describe an end-to-end speech synthesis system that uses generative
adversarial training. We train our Vocoder for raw phoneme-to-audio conversion,
using explicit phonetic, pitch and duration modeling. We experiment with
several pre-trained models for contextualized and decontextualized word
embeddings and we introduce a new method for highly expressive character voice
matching, based on discreet style tokens.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09638" title="Abstract">arXiv:2310.09638</a> [<a href="/pdf/2310.09638" title="Download PDF">pdf</a>, <a href="/ps/2310.09638" title="Download PostScript">ps</a>, <a href="/format/2310.09638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Combinatorial Approximation Algorithm for Correlation Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ostovari%2C+M">Mojtaba Ostovari</a>, 
<a href="/search/cs?searchtype=author&query=Zarei%2C+A">Alireza Zarei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">This article introduces a quick and simple combinatorial approximation
algorithm for the Weighted correlation clustering problem. In this problem, we
have a set of vertices and two difference and similarity weight values for each
pair of vertices, and the goal is to cluster the vertices with minimum total
intra-cluster difference weights plus inter-cluster similarity weights. Our
algorithm's approximation factor is 3 when an instance of this problem
satisfies probability constraints (the best-known was 5). If the instance
satisfies triangle inequality in addition to probability constraints, the
approximation factor is 1.6 (the best-known was 2).
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09639" title="Abstract">arXiv:2310.09639</a> [<a href="/pdf/2310.09639" title="Download PDF">pdf</a>, <a href="/ps/2310.09639" title="Download PostScript">ps</a>, <a href="/format/2310.09639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPZero: Dimension-Independent and Differentially Private Zeroth-Order  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Thekumparampil%2C+K+K">Kiran Koshy Thekumparampil</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sewoong Oh</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+N">Niao He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">The widespread practice of fine-tuning pretrained large language models
(LLMs) on domain-specific data faces two major challenges in memory and
privacy. First, as the size of LLMs continue to grow, encompassing billions of
parameters, the memory demands of gradient-based training methods via
backpropagation become prohibitively high. Second, given the tendency of LLMs
to memorize and disclose sensitive training data, the privacy of fine-tuning
data must be respected. To this end, we explore the potential of zeroth-order
methods in differentially private optimization for fine-tuning LLMs.
Zeroth-order methods, which rely solely on forward passes, substantially reduce
memory consumption during training. However, directly combining them with
standard differential privacy mechanism poses dimension-dependent complexity.
To bridge the gap, we introduce DPZero, a novel differentially private
zeroth-order algorithm with nearly dimension-independent rates. Our theoretical
analysis reveals that its complexity hinges primarily on the problem's
intrinsic dimension and exhibits only a logarithmic dependence on the ambient
dimension. This renders DPZero a highly practical option for real-world LLMs
deployments.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09642" title="Abstract">arXiv:2310.09642</a> [<a href="/pdf/2310.09642" title="Download PDF">pdf</a>, <a href="/format/2310.09642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot Imitation from Video Demonstration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chereddy%2C+V+S+T">Venkat Surya Teja Chereddy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents an attempt to replicate the robot imitation work
conducted by Sermanet et al., with a specific focus on the experiments
involving robot joint position prediction. While the original study utilized
human poses to predict robot joint positions, this project aimed to achieve
robot-to-robot imitation due to the challenges of obtaining human-to-robot
translation data. The primary objective was to provide a neural network with
robot images and have it predict end-effector positions through regression. The
paper discusses the implementation process, including data collection using the
open-source RoboSuite, where a Python module was developed to capture
randomized action data for four different robots. Challenges in data
collection, such as oscillations and limited action variety, were addressed
through domain randomization. Results show high testing error and
unsatisfactory imitation due to overfitting, necessitating improvements in the
project.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09647" title="Abstract">arXiv:2310.09647</a> [<a href="/pdf/2310.09647" title="Download PDF">pdf</a>, <a href="/format/2310.09647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point-DynRF: Point-based Dynamic Radiance Fields from a Monocular Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Byeongjun Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Changick Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Dynamic radiance fields have emerged as a promising approach for generating
novel views from a monocular video. However, previous methods enforce the
geometric consistency to dynamic radiance fields only between adjacent input
frames, making it difficult to represent the global scene geometry and
degenerates at the viewpoint that is spatio-temporally distant from the input
camera trajectory. To solve this problem, we introduce point-based dynamic
radiance fields (\textbf{Point-DynRF}), a novel framework where the global
geometric information and the volume rendering process are trained by neural
point clouds and dynamic radiance fields, respectively. Specifically, we
reconstruct neural point clouds directly from geometric proxies and optimize
both radiance fields and the geometric proxies using our proposed losses,
allowing them to complement each other. We validate the effectiveness of our
method with experiments on the NVIDIA Dynamic Scenes Dataset and several
causally captured monocular video clips.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09650" title="Abstract">arXiv:2310.09650</a> [<a href="/pdf/2310.09650" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Federated Learning in Healthcare: a review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thrasher%2C+J">Jacob Thrasher</a>, 
<a href="/search/cs?searchtype=author&query=Devkota%2C+A">Alina Devkota</a>, 
<a href="/search/cs?searchtype=author&query=Siwakotai%2C+P">Prasiddha Siwakotai</a>, 
<a href="/search/cs?searchtype=author&query=Chivukula%2C+R">Rohit Chivukula</a>, 
<a href="/search/cs?searchtype=author&query=Poudel%2C+P">Pranav Poudel</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chaunbo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+B">Binod Bhattarai</a>, 
<a href="/search/cs?searchtype=author&query=Gyawali%2C+P">Prashnna Gyawali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in multimodal machine learning have empowered the
development of accurate and robust AI systems in the medical domain, especially
within centralized database systems. Simultaneously, Federated Learning (FL)
has progressed, providing a decentralized mechanism where data need not be
consolidated, thereby enhancing the privacy and security of sensitive
healthcare data. The integration of these two concepts supports the ongoing
progress of multimodal learning in healthcare while ensuring the security and
privacy of patient records within local data-holding agencies. This paper
offers a concise overview of the significance of FL in healthcare and outlines
the current state-of-the-art approaches to Multimodal Federated Learning (MMFL)
within the healthcare domain. It comprehensively examines the existing
challenges in the field, shedding light on the limitations of present models.
Finally, the paper outlines potential directions for future advancements in the
field, aiming to bridge the gap between cutting-edge AI technology and the
imperative need for patient data privacy in healthcare applications.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09651" title="Abstract">arXiv:2310.09651</a> [<a href="/pdf/2310.09651" title="Download PDF">pdf</a>, <a href="/format/2310.09651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lexical Entrainment for Conversational Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhengxiang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+P">Procheta Sen</a>, 
<a href="/search/cs?searchtype=author&query=Lipani%2C+A">Aldo Lipani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Conversational agents have become ubiquitous in assisting with daily tasks,
and are expected to possess human-like features. One such feature is lexical
entrainment (LE), a phenomenon in which speakers in human-human conversations
tend to naturally and subconsciously align their lexical choices with those of
their interlocutors, leading to more successful and engaging conversations. As
an example, if a digital assistant replies 'Your appointment for Jinling Noodle
Pub is at 7 pm' to the question 'When is my reservation for Jinling Noodle Bar
today?', it may feel as though the assistant is trying to correct the speaker,
whereas a response of 'Your reservation for Jinling Noodle Bar is at 7 pm'
would likely be perceived as more positive. This highlights the importance of
LE in establishing a shared terminology for maximum clarity and reducing
ambiguity in conversations. However, we demonstrate in this work that current
response generation models do not adequately address this crucial humanlike
phenomenon. To address this, we propose a new dataset, named MULTIWOZ-ENTR, and
a measure for LE for conversational systems. Additionally, we suggest a way to
explicitly integrate LE into conversational systems with two new tasks, a LE
extraction task and a LE generation task. We also present two baseline
approaches for the LE extraction task, which aim to detect LE expressions from
dialogue contexts.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09652" title="Abstract">arXiv:2310.09652</a> [<a href="/pdf/2310.09652" title="Download PDF">pdf</a>, <a href="/format/2310.09652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BufferSearch: Generating Black-Box Adversarial Texts With Lower Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+W">Wenjie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yitao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zhehua Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Xuan%2C+Q">Qi Xuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianyi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Machine learning security has recently become a prominent topic in the
natural language processing (NLP) area. The existing black-box adversarial
attack suffers prohibitively from the high model querying complexity, resulting
in easily being captured by anti-attack monitors. Meanwhile, how to eliminate
redundant model queries is rarely explored. In this paper, we propose a
query-efficient approach BufferSearch to effectively attack general intelligent
NLP systems with the minimal number of querying requests. In general,
BufferSearch makes use of historical information and conducts statistical test
to avoid incurring model queries frequently. Numerically, we demonstrate the
effectiveness of BufferSearch on various benchmark text-classification
experiments by achieving the competitive attacking performance but with a
significant reduction of query quantity. Furthermore, BufferSearch performs
multiple times better than competitors within restricted query budget. Our work
establishes a strong benchmark for the future study of query-efficiency in NLP
adversarial attacks.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09653" title="Abstract">arXiv:2310.09653</a> [<a href="/pdf/2310.09653" title="Download PDF">pdf</a>, <a href="/format/2310.09653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SelfVC: Voice Conversion With Iterative Refinement using Self  Transformations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neekhara%2C+P">Paarth Neekhara</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+S">Shehzeen Hussain</a>, 
<a href="/search/cs?searchtype=author&query=Valle%2C+R">Rafael Valle</a>, 
<a href="/search/cs?searchtype=author&query=Ginsburg%2C+B">Boris Ginsburg</a>, 
<a href="/search/cs?searchtype=author&query=Ranjan%2C+R">Rishabh Ranjan</a>, 
<a href="/search/cs?searchtype=author&query=Dubnov%2C+S">Shlomo Dubnov</a>, 
<a href="/search/cs?searchtype=author&query=Koushanfar%2C+F">Farinaz Koushanfar</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We propose SelfVC, a training strategy to iteratively improve a voice
conversion model with self-synthesized examples. Previous efforts on voice
conversion focus on explicitly disentangling speech representations to
separately encode speaker characteristics and linguistic content. However,
disentangling speech representations to capture such attributes using
task-specific loss terms can lead to information loss by discarding finer
nuances of the original signal. In this work, instead of explicitly
disentangling attributes with loss terms, we present a framework to train a
controllable voice conversion model on entangled speech representations derived
from self-supervised learning and speaker verification models. First, we
develop techniques to derive prosodic information from the audio signal and SSL
representations to train predictive submodules in the synthesis model. Next, we
propose a training strategy to iteratively improve the synthesis model for
voice conversion, by creating a challenging training objective using
self-synthesized examples. In this training approach, the current state of the
synthesis model is used to generate voice-converted variations of an utterance,
which serve as inputs for the reconstruction task, ensuring a continuous and
purposeful refinement of the model. We demonstrate that incorporating such
self-synthesized examples during training improves the speaker similarity of
generated speech as compared to a baseline voice conversion model trained
solely on heuristically perturbed inputs. SelfVC is trained without any text
and is applicable to a range of tasks such as zero-shot voice conversion,
cross-lingual voice conversion, and controllable speech synthesis with pitch
and pace modifications. SelfVC achieves state-of-the-art results in zero-shot
voice conversion on metrics evaluating naturalness, speaker similarity, and
intelligibility of synthesized audio.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09655" title="Abstract">arXiv:2310.09655</a> [<a href="/pdf/2310.09655" title="Download PDF">pdf</a>, <a href="/format/2310.09655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory Tracking for Tilted Hexarotors with Concurrent Attitude  Regulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Perin%2C+M">Marco Perin</a>, 
<a href="/search/eess?searchtype=author&query=Bertoni%2C+M">Massimiliano Bertoni</a>, 
<a href="/search/eess?searchtype=author&query=Michieletto%2C+G">Giulia Michieletto</a>, 
<a href="/search/eess?searchtype=author&query=Oboe%2C+R">Roberto Oboe</a>, 
<a href="/search/eess?searchtype=author&query=Cenedese%2C+A">Angelo Cenedese</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Tilted hexarotors embody a technology that remains partially unexploited in
terms of its potential, especially concerning precise and concurrent position
and attitude control. Focusing on these aerial platforms, we propose two
control architectures that can tackle the trajectory tracking task, ensuring
also the attitude regulation: one is designed resting on the differential
flatness property of the system, which is investigated in the paper, and the
other is a hierarchical nonlinear controller. We comparatively discuss the
performance of the two control schemes, in terms of the accuracy of both the
tracking control action and the attitude regulation, the input effort, and the
robustness in the presence of disturbances. Numerical results reveal both the
robustness of the hierarchical approach in the case of external disturbance and
the accuracy of the differential flatness-based controller in unwindy
conditions.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09656" title="Abstract">arXiv:2310.09656</a> [<a href="/pdf/2310.09656" title="Download PDF">pdf</a>, <a href="/format/2310.09656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent  Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiani Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+B">Balasubramaniam Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhengyuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+X">Xiao Qin</a>, 
<a href="/search/cs?searchtype=author&query=Faloutsos%2C+C">Christos Faloutsos</a>, 
<a href="/search/cs?searchtype=author&query=Rangwala%2C+H">Huzefa Rangwala</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent advances in tabular data generation have greatly enhanced synthetic
data quality. However, extending diffusion models to tabular data is
challenging due to the intricately varied distributions and a blend of data
types of tabular data. This paper introduces TABSYN, a methodology that
synthesizes tabular data by leveraging a diffusion model within a variational
autoencoder (VAE) crafted latent space. The key advantages of the proposed
TABSYN include (1) Generality: the ability to handle a broad spectrum of data
types by converting them into a single unified space and explicitly capture
inter-column relations; (2) Quality: optimizing the distribution of latent
embeddings to enhance the subsequent training of diffusion models, which helps
generate high-quality synthetic data, (3) Speed: much fewer number of reverse
steps and faster synthesis speed than existing diffusion-based methods.
Extensive experiments on six datasets with five metrics demonstrate that TABSYN
outperforms existing methods. Specifically, it reduces the error rates by 86%
and 67% for column-wise distribution and pair-wise column correlation
estimations compared with the most competitive baselines.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09657" title="Abstract">arXiv:2310.09657</a> [<a href="/pdf/2310.09657" title="Download PDF">pdf</a>, <a href="/format/2310.09657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology-guided Hypergraph Transformer Network: Unveiling Structural  Insights for Improved Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saifuddin%2C+K+M">Khaled Mohammed Saifuddin</a>, 
<a href="/search/cs?searchtype=author&query=Aktas%2C+M+E">Mehmet Emin Aktas</a>, 
<a href="/search/cs?searchtype=author&query=Akbas%2C+E">Esra Akbas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Hypergraphs, with their capacity to depict high-order relationships, have
emerged as a significant extension of traditional graphs. Although Graph Neural
Networks (GNNs) have remarkable performance in graph representation learning,
their extension to hypergraphs encounters challenges due to their intricate
structures. Furthermore, current hypergraph transformers, a special variant of
GNN, utilize semantic feature-based self-attention, ignoring topological
attributes of nodes and hyperedges. To address these challenges, we propose a
Topology-guided Hypergraph Transformer Network (THTN). In this model, we first
formulate a hypergraph from a graph while retaining its structural essence to
learn higher-order relations within the graph. Then, we design a simple yet
effective structural and spatial encoding module to incorporate the topological
and spatial information of the nodes into their representation. Further, we
present a structure-aware self-attention mechanism that discovers the important
nodes and hyperedges from both semantic and structural viewpoints. By
leveraging these two modules, THTN crafts an improved node representation,
capturing both local and global topological expressions. Extensive experiments
conducted on node classification tasks demonstrate that the performance of the
proposed model consistently exceeds that of the existing approaches.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09658" title="Abstract">arXiv:2310.09658</a> [<a href="/pdf/2310.09658" title="Download PDF">pdf</a>, <a href="/format/2310.09658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generalized Extensive-Form Fictitious Play Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schulze%2C+T+P">Tim P. Schulze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a simple extensive-form algorithm for finding equilibria of
two-player, zero-sum games. The algorithm is realization equivalent to a
generalized form of Fictitious Play. We compare its performance to that of a
similar extensive-form fictitious play algorithm and a counter-factual regret
minimization algorithm. All three algorithms share the same advantages over
normal-form fictitious play in terms of reducing storage requirements and
computational complexity. The new algorithm is intuitive and straightforward to
implement, making it an appealing option for those looking for a quick and easy
game solving tool.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09659" title="Abstract">arXiv:2310.09659</a> [<a href="/pdf/2310.09659" title="Download PDF">pdf</a>, <a href="/format/2310.09659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HAPS in the Non-Terrestrial Network Nexus: Prospective Architectures and  Performance Insights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+Z">Zhengying Lou</a>, 
<a href="/search/cs?searchtype=author&query=Belmekki%2C+B+E+Y">Baha Eddine Youcef Belmekki</a>, 
<a href="/search/cs?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">High altitude platform stations (HAPS) have recently emerged as a new key
stratospheric player in non-terrestrial networks (NTN) alongside satellites and
low-altitude platforms. In this paper, we present the main communication links
between HAPS and other NTN platforms, their advantages, and their challenges.
Then, prospective network architectures in which HAPS plays an indispensable
role in the future NTNs are presented such as ad-hoc, cell-free, and integrated
access and backhaul. To showcase the importance of HAPS in the NTN, we provide
comprehensive performance insights when using HAPS in the prospective
architectures with the most suitable communication link. The insights show the
HAPS' ability to interconnect the NTN nexus as well as their versatility by
incorporating different metrics into the analysis such as routing latency,
energy efficiency, coverage probability, and channel capacity. Depending on the
architecture, HAPS will play different roles in NTN, such as a UAV network
center, satellite relay, and ground network extension. Finally, the performance
gain provided by HAPS usage in NTN is further highlighted by comparing the
results when no HAPS are used.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09660" title="Abstract">arXiv:2310.09660</a> [<a href="/pdf/2310.09660" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Damping Effect of Inner Control Loops for Grid-Forming VSCs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xiongfei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Jin%2C+Z">Zheming Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents an analytical approach to explore the damping effect of
inner loops on grid-forming converters. First, an impedance model is proposed
to characterize the behaviors of inner loops, thereby illustrating their
influence on output impedance shaping. Then, based on the impedance
representation, the complex torque coefficient method is employed to assess the
contribution of inner loops to system damping. The interactions among inner
loops, outer loops, and the ac grid are analyzed. It reveals that inner loops
shape the electrical damping torque coefficient and consequently influence both
synchronous and sub-synchronous oscillation modes. The virtual admittance and
current control-based inner-loop scheme is employed to illustrate the proposed
analytical approach. The case study comprises the analysis of impedance
profiles, the analysis of damping torque contributed by inner loops under
various grid strengths, and the comparison between dq-frame and
{\alpha}\b{eta}-frame realizations of inner loops. Finally, simulation and
experimental tests collaborate with theoretical approaches and findings.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09661" title="Abstract">arXiv:2310.09661</a> [<a href="/pdf/2310.09661" title="Download PDF">pdf</a>, <a href="/format/2310.09661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Legend at ArAIEval Shared Task: Persuasion Technique Detection using a  Language-Agnostic Text Representation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ojo%2C+O+E">Olumide E. Ojo</a>, 
<a href="/search/cs?searchtype=author&query=Adebanji%2C+O+O">Olaronke O. Adebanji</a>, 
<a href="/search/cs?searchtype=author&query=Calvo%2C+H">Hiram Calvo</a>, 
<a href="/search/cs?searchtype=author&query=Dieke%2C+D+O">Damian O. Dieke</a>, 
<a href="/search/cs?searchtype=author&query=Ojo%2C+O+E">Olumuyiwa E. Ojo</a>, 
<a href="/search/cs?searchtype=author&query=Akinsanya%2C+S+E">Seye E. Akinsanya</a>, 
<a href="/search/cs?searchtype=author&query=Abiola%2C+T+O">Tolulope O. Abiola</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+A">Anna Feldman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we share our best performing submission to the Arabic AI Tasks
Evaluation Challenge (ArAIEval) at ArabicNLP 2023. Our focus was on Task 1,
which involves identifying persuasion techniques in excerpts from tweets and
news articles. The persuasion technique in Arabic texts was detected using a
training loop with XLM-RoBERTa, a language-agnostic text representation model.
This approach proved to be potent, leveraging fine-tuning of a multilingual
language model. In our evaluation of the test set, we achieved a micro F1 score
of 0.64 for subtask A of the competition.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09663" title="Abstract">arXiv:2310.09663</a> [<a href="/pdf/2310.09663" title="Download PDF">pdf</a>, <a href="/format/2310.09663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VBFT: Veloce Byzantine Fault Tolerant Consensus for Blockchains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jalalzai%2C+M+M">Mohammad M. Jalalzai</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Lemieux%2C+V">Victoria Lemieux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2109.14604">arXiv:2109.14604</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Low latency is one of the most desirable features of partially synchronous
Byzantine consensus protocols. Existing low-latency protocols have achieved
consensus with just two communication steps by reducing the maximum number of
faults the protocol can tolerate (from $f = \frac{n-1}{3}$ to $f =
\frac{n+1}{5}$), \textcolor{black}{by relaxing protocol safety guarantees}, or
by using trusted hardware like Trusted Execution Environment. Furthermore,
these two-step protocols don't support rotating primary and low-cost view
change (leader replacement), which are important features of many blockchain
use cases. In this paper, we propose a protocol called VBFT which achieves
consensus in just two communication steps without scarifying desirable
features. In particular, VBFT tolerates $f = \frac{n-1}{3}$ faults (which is
the best possible), guarantees strong safety for honest primaries, and requires
no trusted hardware. Moreover, VBFT supports primary rotation and low-cost view
change, thereby improving prior art on multiple axes.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09665" title="Abstract">arXiv:2310.09665</a> [<a href="/pdf/2310.09665" title="Download PDF">pdf</a>, <a href="/format/2310.09665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Blockchain-empowered Multi-Aggregator Federated Learning Architecture  in Edge Computing with Deep Reinforcement Learning Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weili Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated learning (FL) is emerging as a sought-after distributed machine
learning architecture, offering the advantage of model training without direct
exposure of raw data. With advancements in network infrastructure, FL has been
seamlessly integrated into edge computing. However, the limited resources on
edge devices introduce security vulnerabilities to FL in the context. While
blockchain technology promises to bolster security, practical deployment on
resource-constrained edge devices remains a challenge. Moreover, the
exploration of FL with multiple aggregators in edge computing is still new in
the literature. Addressing these gaps, we introduce the Blockchain-empowered
Heterogeneous Multi-Aggregator Federated Learning Architecture (BMA-FL). We
design a novel light-weight Byzantine consensus mechanism, namely PBCM, to
enable secure and fast model aggregation and synchronization in BMA-FL. We also
dive into the heterogeneity problem in BMA-FL that the aggregators are
associated with varied number of connected trainers with Non-IID data
distributions and diverse training speed. We proposed a multi-agent deep
reinforcement learning algorithm to help aggregators decide the best training
strategies. The experiments on real-word datasets demonstrate the efficiency of
BMA-FL to achieve better models faster than baselines, showing the efficacy of
PBCM and proposed deep reinforcement learning algorithm.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09667" title="Abstract">arXiv:2310.09667</a> [<a href="/pdf/2310.09667" title="Download PDF">pdf</a>, <a href="/ps/2310.09667" title="Download PostScript">ps</a>, <a href="/format/2310.09667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge-InversionNet: Enabling Efficient Inference of InversionNet on Edge  Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhepeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Putla%2C+I">Isaacshubhanand Putla</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weiwen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Youzuo Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP); Geophysics (physics.geo-ph)

</div>
<p class="mathjax">Seismic full waveform inversion (FWI) is a widely used technique in
geophysics for inferring subsurface structures from seismic data. And
InversionNet is one of the most successful data-driven machine learning models
that is applied to seismic FWI. However, the high computing costs to run
InversionNet have made it challenging to be efficiently deployed on edge
devices that are usually resource-constrained. Therefore, we propose to employ
the structured pruning algorithm to get a lightweight version of InversionNet,
which can make an efficient inference on edge devices. And we also made a
prototype with Raspberry Pi to run the lightweight InversionNet. Experimental
results show that the pruned InversionNet can achieve up to 98.2 % reduction in
computing resources with moderate model performance degradation.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09668" title="Abstract">arXiv:2310.09668</a> [<a href="/pdf/2310.09668" title="Download PDF">pdf</a>, <a href="/format/2310.09668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Testers&#x27; Biases: Guiding Model Testing with Knowledge Bases using  LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenyang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Rustogi%2C+R">Rishabh Rustogi</a>, 
<a href="/search/cs?searchtype=author&query=Brower-Sinning%2C+R">Rachel Brower-Sinning</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+G+A">Grace A. Lewis</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4stner%2C+C">Christian K&#xe4;stner</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongshuang Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Current model testing work has mostly focused on creating test cases.
Identifying what to test is a step that is largely ignored and poorly
supported. We propose Weaver, an interactive tool that supports requirements
elicitation for guiding model testing. Weaver uses large language models to
generate knowledge bases and recommends concepts from them interactively,
allowing testers to elicit requirements for further testing. Weaver provides
rich external knowledge to testers and encourages testers to systematically
explore diverse concepts beyond their own biases. In a user study, we show that
both NLP experts and non-experts identified more, as well as more diverse
concepts worth testing when using Weaver. Collectively, they found more than
200 failing test cases for stance detection with zero-shot ChatGPT. Our case
studies further show that Weaver can help practitioners test models in
real-world settings, where developers define more nuanced application scenarios
(e.g., code understanding and transcript summarization) using LLMs.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09669" title="Abstract">arXiv:2310.09669</a> [<a href="/pdf/2310.09669" title="Download PDF">pdf</a>, <a href="/format/2310.09669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework For Automated Dissection Along Tissue Boundary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+K">Ki-Hwan Oh</a>, 
<a href="/search/cs?searchtype=author&query=Borgioli%2C+L">Leonardo Borgioli</a>, 
<a href="/search/cs?searchtype=author&query=Zefran%2C+M">Milos Zefran</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liaohai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Giulianotti%2C+P+C">Pier Cristoforo Giulianotti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robotic surgery promises enhanced precision and adaptability over traditional
surgical methods. It also offers the possibility of automating surgical
interventions, resulting in reduced stress on the surgeon, better surgical
outcomes, and lower costs. Cholecystectomy, the removal of the gallbladder,
serves as an ideal model procedure for automation due to its distinct and
well-contrasted anatomical features between the gallbladder and liver, along
with standardized surgical maneuvers. Dissection is a frequently used subtask
in cholecystectomy where the surgeon delivers the energy on the hook to detach
the gallbladder from the liver. Hence, dissection along tissue boundaries is a
good candidate for surgical automation. For the da Vinci surgical robot to
perform the same procedure as a surgeon automatically, it needs to have the
ability to (1) recognize and distinguish between the two different tissues
(e.g. the liver and the gallbladder), (2) understand where the boundary between
the two tissues is located in the 3D workspace, (3) locate the instrument tip
relative to the boundary in the 3D space using visual feedback, and (4) move
the instrument along the boundary. This paper presents a novel framework that
addresses these challenges through AI-assisted image processing and
vision-based robot control. We also present the ex-vivo evaluation of the
automated procedure on chicken and pork liver specimens that demonstrates the
effectiveness of the proposed framework.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09671" title="Abstract">arXiv:2310.09671</a> [<a href="/pdf/2310.09671" title="Download PDF">pdf</a>, <a href="/format/2310.09671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPGA Implementation of OTFS Modulation for 6G Communication Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Isik%2C+M">Murat Isik</a>, 
<a href="/search/cs?searchtype=author&query=Nkomo%2C+M">Malvin Nkomo</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Anup Das</a>, 
<a href="/search/cs?searchtype=author&query=Dandekar%2C+K+R">Kapil R. Dandekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE Future Networks, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Sixth-generation (6G) communication systems are poised to accommodate high
data-rate wireless communication services in highly dynamic channels, with
applications including high-speed trains, unmanned aerial vehicles, and
intelligent transportation systems. Orthogonal frequency-division multiplexing
(OFDM) modulation suffers from performance degradation in such high-mobility
applications due to high Doppler spread in the channel. The recently proposed
Orthogonal Time Frequency Space (OTFS) modulation scheme outperforms OFDM in
terms of supporting a higher transmitter (Tx) and receiver (Rx) user velocity.
Additionally, the highly-dynamic time-frequency (TF) channel has little effect
on OTFS modulated signals, which enables the realization of low-complexity
pre-processing architectures for implementing massive-multiple input multiple
outputs (MIMO) based OTFS systems. However, while OTFS has received attention
in the literature from a theory and simulation perspective, there has been
comparatively little work on real-time FPGA implementation of OTFS waveforms.
Thus, in this paper, we first present a mathematical overview of OTFS
modulation and then describe an FPGA implementation of OTFS implementation on
hardware. Power, area, and timing analysis of the implemented design on a Zynq
UltraScale+ RFSoC FPGA are provided for benchmarking purposes.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09672" title="Abstract">arXiv:2310.09672</a> [<a href="/pdf/2310.09672" title="Download PDF">pdf</a>, <a href="/format/2310.09672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Semi-Structured Automatic ICD Coding via Tree-based Contrastive  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+C+K">Chandan K. Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Ping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+Y">Yue Ning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Automatic coding of International Classification of Diseases (ICD) is a
multi-label text categorization task that involves extracting disease or
procedure codes from clinical notes. Despite the application of
state-of-the-art natural language processing (NLP) techniques, there are still
challenges including limited availability of data due to privacy constraints
and the high variability of clinical notes caused by different writing habits
of medical professionals and various pathological features of patients. In this
work, we investigate the semi-structured nature of clinical notes and propose
an automatic algorithm to segment them into sections. To address the
variability issues in existing ICD coding models with limited data, we
introduce a contrastive pre-training approach on sections using a soft
multi-label similarity metric based on tree edit distance. Additionally, we
design a masked section training strategy to enable ICD coding models to locate
sections related to ICD codes. Extensive experimental results demonstrate that
our proposed training strategies effectively enhance the performance of
existing ICD coding methods.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09675" title="Abstract">arXiv:2310.09675</a> [<a href="/pdf/2310.09675" title="Download PDF">pdf</a>, <a href="/format/2310.09675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Model-Agnostic Multi-Group Equivariant Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baltaji%2C+R">Razan Baltaji</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+S">Sourya Basu</a>, 
<a href="/search/cs?searchtype=author&query=Varshney%2C+L+R">Lav R. Varshney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Constructing model-agnostic group equivariant networks, such as equitune
(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be
computationally expensive for large product groups. We address this by
providing efficient model-agnostic equivariant designs for two related
problems: one where the network has multiple inputs each with potentially
different groups acting on them, and another where there is a single input but
the group acting on it is a large product group. For the first design, we
initially consider a linear model and characterize the entire equivariant space
that satisfies this constraint. This characterization gives rise to a novel
fusion layer between different channels that satisfies an invariance-symmetry
(IS) constraint, which we call an IS layer. We then extend this design beyond
linear models, similar to equitune, consisting of equivariant and IS layers. We
also show that the IS layer is a universal approximator of invariant-symmetric
functions. Inspired by the first design, we use the notion of the IS property
to design a second efficient model-agnostic equivariant design for large
product groups acting on a single input. For the first design, we provide
experiments on multi-image classification where each view is transformed
independently with transformations such as rotations. We find equivariant
models are robust to such transformations and perform competitively otherwise.
For the second design, we consider three applications: language
compositionality on the SCAN dataset to product groups; fairness in natural
language generation from GPT-2 to address intersectionality; and robust
zero-shot image classification with CLIP. Overall, our methods are simple and
general, competitive with equitune and its variants, while also being
computationally more efficient.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09676" title="Abstract">arXiv:2310.09676</a> [<a href="/pdf/2310.09676" title="Download PDF">pdf</a>, <a href="/format/2310.09676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mastering Robot Manipulation with Multimodal Prompts through Pretraining  and Multi-task Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiachen Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Q">Qiaozi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Johnston%2C+M">Michael Johnston</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiaofeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuehai He</a>, 
<a href="/search/cs?searchtype=author&query=Shakiah%2C+S">Suhaila Shakiah</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hangjie Shi</a>, 
<a href="/search/cs?searchtype=author&query=Ghanadan%2C+R">Reza Ghanadan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prompt-based learning has been demonstrated as a compelling paradigm
contributing to large language models' tremendous success (LLMs). Inspired by
their success in language tasks, existing research has leveraged LLMs in
embodied instruction following and task planning. However, not much attention
has been paid to embodied tasks with multimodal prompts, combining vision
signals with text descriptions. This type of task poses a major challenge to
robots' capability to understand the interconnection and complementarity
between vision and language signals. In this work, we introduce an effective
framework that learns a policy to perform robot manipulation with multimodal
prompts from multi-task expert trajectories. Our methods consist of a two-stage
training pipeline that performs inverse dynamics pretraining and multi-task
finetuning. To facilitate multimodal understanding, we design our multimodal
prompt encoder by augmenting a pretrained LM with a residual connection to the
visual input and model the dependencies among action dimensions. Empirically,
we evaluate the efficacy of our method on the VIMA-BENCH and establish a new
state-of-the-art (10% improvement in success rate). Moreover, we demonstrate
that our model exhibits remarkable in-context learning ability.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09678" title="Abstract">arXiv:2310.09678</a> [<a href="/pdf/2310.09678" title="Download PDF">pdf</a>, <a href="/format/2310.09678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree Containment Above Minimum Degree is FPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fomin%2C+F+V">Fedor V. Fomin</a>, 
<a href="/search/cs?searchtype=author&query=Golovach%2C+P+A">Petr A. Golovach</a>, 
<a href="/search/cs?searchtype=author&query=Sagunov%2C+D">Danil Sagunov</a>, 
<a href="/search/cs?searchtype=author&query=Simonov%2C+K">Kirill Simonov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">According to the classic Chv{\'{a}}tal's Lemma from 1977, a graph of minimum
degree $\delta(G)$ contains every tree on $\delta(G)+1$ vertices.
<br />Our main result is the following algorithmic "extension" of Chv\'{a}tal's
Lemma: For any $n$-vertex graph $G$, integer $k$, and a tree $T$ on at most
$\delta(G)+k$ vertices, deciding whether $G$ contains a subgraph isomorphic to
$T$, can be done in time $f(k)\cdot n^{\mathcal{O}(1)}$ for some function $f$
of $k$ only.
<br />The proof of our main result is based on an interplay between extremal graph
theory and parameterized algorithms.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09679" title="Abstract">arXiv:2310.09679</a> [<a href="/pdf/2310.09679" title="Download PDF">pdf</a>, <a href="/format/2310.09679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Do Deep Saliency Models Learn about Visual Attention?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Ming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent years, deep saliency models have made significant progress in
predicting human visual attention. However, the mechanisms behind their success
remain largely unexplained due to the opaque nature of deep neural networks. In
this paper, we present a novel analytic framework that sheds light on the
implicit features learned by saliency models and provides principled
interpretation and quantification of their contributions to saliency
prediction. Our approach decomposes these implicit features into interpretable
bases that are explicitly aligned with semantic attributes and reformulates
saliency prediction as a weighted combination of probability maps connecting
the bases and saliency. By applying our framework, we conduct extensive
analyses from various perspectives, including the positive and negative weights
of semantics, the impact of training data and architectural designs, the
progressive influences of fine-tuning, and common failure patterns of
state-of-the-art deep saliency models. Additionally, we demonstrate the
effectiveness of our framework by exploring visual attention characteristics in
various application scenarios, such as the atypical attention of people with
autism spectrum disorder, attention to emotion-eliciting stimuli, and attention
evolution over time. Our code is publicly available at
\url{https://github.com/szzexpoi/saliency_analysis}.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09680" title="Abstract">arXiv:2310.09680</a> [<a href="/pdf/2310.09680" title="Download PDF">pdf</a>, <a href="/format/2310.09680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Contextual Recognition In Automatic Speech Recognition Systems  By Semantic Lattice Rescoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sudarshan%2C+A">Ankitha Sudarshan</a>, 
<a href="/search/cs?searchtype=author&query=Samuel%2C+V">Vinay Samuel</a>, 
<a href="/search/cs?searchtype=author&query=Patwa%2C+P">Parth Patwa</a>, 
<a href="/search/cs?searchtype=author&query=Amara%2C+I">Ibtihel Amara</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Automatic Speech Recognition (ASR) has witnessed a profound research
interest. Recent breakthroughs have given ASR systems different prospects such
as faithfully transcribing spoken language, which is a pivotal advancement in
building conversational agents. However, there is still an imminent challenge
of accurately discerning context-dependent words and phrases. In this work, we
propose a novel approach for enhancing contextual recognition within ASR
systems via semantic lattice processing leveraging the power of deep learning
models in accurately delivering spot-on transcriptions across a wide variety of
vocabularies and speaking styles. Our solution consists of using Hidden Markov
Models and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks
(DNN) models integrating both language and acoustic modeling for better
accuracy. We infused our network with the use of a transformer-based model to
properly rescore the word lattice achieving remarkable capabilities with a
palpable reduction in Word Error Rate (WER). We demonstrate the effectiveness
of our proposed framework on the LibriSpeech dataset with empirical analyses.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09681" title="Abstract">arXiv:2310.09681</a> [<a href="/pdf/2310.09681" title="Download PDF">pdf</a>, <a href="/format/2310.09681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Region Multi-Agent Formation Control With Velocity Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rai%2C+A">Ayush Rai</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+S">Shaoshuai Mou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper provides a solution to the problem of safe region formation
control with reference velocity tracking for a second-order multi-agent system
without velocity measurements. Safe region formation control is a control
problem where the agents are expected to attain the desired formation while
reaching the target region and simultaneously ensuring collision and obstacle
avoidance. To tackle this control problem, we break it down into two distinct
objectives: safety and region formation control, to provide a completely
distributed algorithm. Region formation control is modeled as a high-level
abstract objective, whereas safety and actuator saturation are modeled as a
low-level objective designed independently, without any knowledge of the
former, and being minimally invasive. Our approach incorporates connectivity
preservation, actuator saturation, safety considerations, and lack of velocity
measurement from other agents with second-order system dynamics which are
important constraints in practical applications. Both internal safety for
collision avoidance among agents and external safety for avoiding unsafe
regions are ensured using exponential control barrier functions. We provide
theoretical results for asymptotic convergence and numerical simulation to show
the approach's effectiveness.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09685" title="Abstract">arXiv:2310.09685</a> [<a href="/pdf/2310.09685" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative artificial intelligence for de novo protein design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Winnifrith%2C+A">Adam Winnifrith</a>, 
<a href="/search/cs?searchtype=author&query=Outeiral%2C+C">Carlos Outeiral</a>, 
<a href="/search/cs?searchtype=author&query=Hie%2C+B">Brian Hie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Engineering new molecules with desirable functions and properties has the
potential to extend our ability to engineer proteins beyond what nature has so
far evolved. Advances in the so-called "de novo" design problem have recently
been brought forward by developments in artificial intelligence. Generative
architectures, such as language models and diffusion processes, seem adept at
generating novel, yet realistic proteins that display desirable properties and
perform specified functions. State-of-the-art design protocols now achieve
experimental success rates nearing 20%, thus widening the access to de novo
designed proteins. Despite extensive progress, there are clear field-wide
challenges, for example in determining the best in silico metrics to prioritise
designs for experimental testing, and in designing proteins that can undergo
large conformational changes or be regulated by post-translational
modifications and other cellular processes. With an increase in the number of
models being developed, this review provides a framework to understand how
these tools fit into the overall process of de novo protein design. Throughout,
we highlight the power of incorporating biochemical knowledge to improve
performance and interpretability.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09686" title="Abstract">arXiv:2310.09686</a> [<a href="/pdf/2310.09686" title="Download PDF">pdf</a>, <a href="/format/2310.09686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Column Generation by Reinforcement Learning-Based  Hyper-Heuristic for Vehicle Routing and Scheduling Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lindong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Column generation (CG) is a vital method to solve large-scale problems by
dynamically generating variables. It has extensive applications in common
combinatorial optimization, such as vehicle routing and scheduling problems,
where each iteration step requires solving an NP-hard constrained shortest path
problem. Although some heuristic methods for acceleration already exist, they
are not versatile enough to solve different problems. In this work, we propose
a reinforcement learning-based hyper-heuristic framework, dubbed RLHH, to
enhance the performance of CG. RLHH is a selection module embedded in CG to
accelerate convergence and get better integer solutions. In each CG iteration,
the RL agent selects a low-level heuristic to construct a reduced network only
containing the edges with a greater chance of being part of the optimal
solution. In addition, we specify RLHH to solve two typical combinatorial
optimization problems: Vehicle Routing Problem with Time Windows (VRPTW) and
Bus Driver Scheduling Problem (BDSP). The total cost can be reduced by up to
27.9\% in VRPTW and 15.4\% in BDSP compared to the best lower-level heuristic
in our tested scenarios, within equivalent or even less computational time. The
proposed RLHH is the first RL-based CG method that outperforms traditional
approaches in terms of solution quality, which can promote the application of
CG in combinatorial optimization.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09687" title="Abstract">arXiv:2310.09687</a> [<a href="/pdf/2310.09687" title="Download PDF">pdf</a>, <a href="/format/2310.09687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Collaborative Filtering is not Collaborative: Unfairness of PCA for  Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">David Liu</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+J">Jackie Baek</a>, 
<a href="/search/cs?searchtype=author&query=Eliassi-Rad%2C+T">Tina Eliassi-Rad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">We study the fairness of dimensionality reduction methods for
recommendations. We focus on the established method of principal component
analysis (PCA), which identifies latent components and produces a low-rank
approximation via the leading components while discarding the trailing
components. Prior works have defined notions of "fair PCA"; however, these
definitions do not answer the following question: what makes PCA unfair? We
identify two underlying mechanisms of PCA that induce unfairness at the item
level. The first negatively impacts less popular items, due to the fact that
less popular items rely on trailing latent components to recover their values.
The second negatively impacts the highly popular items, since the leading PCA
components specialize in individual popular items instead of capturing
similarities between items. To address these issues, we develop a
polynomial-time algorithm, Item-Weighted PCA, a modification of PCA that uses
item-specific weights in the objective. On a stylized class of matrices, we
prove that Item-Weighted PCA using a specific set of weights minimizes a
popularity-normalized error metric. Our evaluations on real-world datasets show
that Item-Weighted PCA not only improves overall recommendation quality by up
to $0.1$ item-level AUC-ROC but also improves on both popular and less popular
items.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09688" title="Abstract">arXiv:2310.09688</a> [<a href="/pdf/2310.09688" title="Download PDF">pdf</a>, <a href="/format/2310.09688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recursively-Constrained Partially Observable Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ho%2C+Q+H">Qi Heng Ho</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+T">Tyler Becker</a>, 
<a href="/search/cs?searchtype=author&query=Kraske%2C+B">Ben Kraske</a>, 
<a href="/search/cs?searchtype=author&query=Laouar%2C+Z">Zakariya Laouar</a>, 
<a href="/search/cs?searchtype=author&query=Feather%2C+M">Martin Feather</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+F">Federico Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Lahijanian%2C+M">Morteza Lahijanian</a>, 
<a href="/search/cs?searchtype=author&query=Sunberg%2C+Z+N">Zachary N. Sunberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In many problems, it is desirable to optimize an objective function while
imposing constraints on some other aspect of the problem. A Constrained
Partially Observable Markov Decision Process (C-POMDP) allows modelling of such
problems while subject to transition uncertainty and partial observability.
Typically, the constraints in C-POMDPs enforce a threshold on expected
cumulative costs starting from an initial state distribution. In this work, we
first show that optimal C-POMDP policies may violate Bellman's principle of
optimality and thus may exhibit pathological behaviors, which can be
undesirable for many applications. To address this drawback, we introduce a new
formulation, the Recursively-Constrained POMDP (RC-POMDP), that imposes
additional history dependent cost constraints on the C-POMDP. We show that,
unlike C-POMDPs, RC-POMDPs always have deterministic optimal policies, and that
optimal policies obey Bellman's principle of optimality. We also present a
point-based dynamic programming algorithm that synthesizes optimal policies for
RC-POMDPs. In our evaluations, we show that policies for RC-POMDPs produce more
desirable behavior than policies for C-POMDPs and demonstrate the efficacy of
our algorithm across a set of benchmark problems.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09689" title="Abstract">arXiv:2310.09689</a> [<a href="/pdf/2310.09689" title="Download PDF">pdf</a>, <a href="/format/2310.09689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Partially Supervised Reinforcement Learning Framework for Visual  Active Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Anindya Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+N">Nathan Jacobs</a>, 
<a href="/search/cs?searchtype=author&query=Vorobeychik%2C+Y">Yevgeniy Vorobeychik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 20 figures, Accepted to NeurIPS 2023, Code is available at <a href="https://github.com/anindyasarkarIITH/PSRL_VAS/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Visual active search (VAS) has been proposed as a modeling framework in which
visual cues are used to guide exploration, with the goal of identifying regions
of interest in a large geospatial area. Its potential applications include
identifying hot spots of rare wildlife poaching activity, search-and-rescue
scenarios, identifying illegal trafficking of weapons, drugs, or people, and
many others. State of the art approaches to VAS include applications of deep
reinforcement learning (DRL), which yield end-to-end search policies, and
traditional active search, which combines predictions with custom algorithmic
approaches. While the DRL framework has been shown to greatly outperform
traditional active search in such domains, its end-to-end nature does not make
full use of supervised information attained either during training, or during
actual search, a significant limitation if search tasks differ significantly
from those in the training distribution. We propose an approach that combines
the strength of both DRL and conventional active search by decomposing the
search policy into a prediction module, which produces a geospatial
distribution of regions of interest based on task embedding and search history,
and a search module, which takes the predictions and search history as input
and outputs the search distribution. We develop a novel meta-learning approach
for jointly learning the resulting combined policy that can make effective use
of supervised information obtained both at training and decision time. Our
extensive experiments demonstrate that the proposed representation and
meta-learning frameworks significantly outperform state of the art in visual
active search on several problem domains.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09690" title="Abstract">arXiv:2310.09690</a> [<a href="/pdf/2310.09690" title="Download PDF">pdf</a>, <a href="/format/2310.09690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Configuration Validation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lian%2C+X">Xinyu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yinfang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Runxiang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+P">Parth Thakkar</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianyin Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Operating Systems (cs.OS)

</div>
<p class="mathjax">Misconfigurations are the major causes of software failures. Existing
configuration validation techniques rely on manually written rules or test
cases, which are expensive to implement and maintain, and are hard to be
comprehensive. Leveraging machine learning (ML) and natural language processing
(NLP) for configuration validation is considered a promising direction, but has
been facing challenges such as the need of not only large-scale configuration
data, but also system-specific features and models which are hard to
generalize. Recent advances in Large Language Models (LLMs) show the promises
to address some of the long-lasting limitations of ML/NLP-based configuration
validation techniques. In this paper, we present an exploratory analysis on the
feasibility and effectiveness of using LLMs like GPT and Codex for
configuration validation. Specifically, we take a first step to empirically
evaluate LLMs as configuration validators without additional fine-tuning or
code generation. We develop a generic LLM-based validation framework, named
Ciri, which integrates different LLMs. Ciri devises effective prompt
engineering with few-shot learning based on both valid configuration and
misconfiguration data. Ciri also validates and aggregates the outputs of LLMs
to generate validation results, coping with known hallucination and
nondeterminism of LLMs. We evaluate the validation effectiveness of Ciri on
five popular LLMs using configuration data of six mature, widely deployed
open-source systems. Our analysis (1) confirms the potential of using LLMs for
configuration validation, (2) understands the design space of LLMbased
validators like Ciri, especially in terms of prompt engineering with few-shot
learning, and (3) reveals open challenges such as ineffectiveness in detecting
certain types of misconfigurations and biases to popular configuration
parameters.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09691" title="Abstract">arXiv:2310.09691</a> [<a href="/pdf/2310.09691" title="Download PDF">pdf</a>, <a href="/format/2310.09691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DentiBot: System Design and 6-DoF Hybrid Position/Force Control for  Robot-Assisted Endodontic Treatment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao-Fang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+Y">Yi-Ching Ho</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cheng-Wei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Robotic technologies are becoming increasingly popular in dentistry due to
the high level of precision required in delicate dental procedures. Most dental
robots available today are designed for implant surgery, helping dentists to
accurately place implants in the desired position and depth. In this paper, we
introduce the DentiBot, the first robot specifically designed for dental
endodontic treatment. The DentiBot is equipped with a force and torque sensor,
as well as a string-based Patient Tracking Module, allowing for real-time
monitoring of endodontic file contact and patient movement. We propose a 6-DoF
hybrid position/force controller that enables autonomous adjustment of the
surgical path and compensation for patient movement, while also providing
protection against endodontic file fracture. In addition, a file flexibility
model is incorporated to compensate for file bending. Pre-clinical evaluations
performed on acrylic root canal models and resin teeth confirm the feasibility
of the DentiBot in assisting endodontic treatment.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09692" title="Abstract">arXiv:2310.09692</a> [<a href="/pdf/2310.09692" title="Download PDF">pdf</a>, <a href="/format/2310.09692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spike-based Neuromorphic Computing for Next-Generation Computer Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+S">Md Sakib Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Schuman%2C+C+D">Catherine D. Schuman</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+T">Tauhidur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Rose%2C+G+S">Garrett S. Rose</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pending to be published as a book chapter in the book 'Computer Vision: Challenges, Trends, and Opportunities' from CRC Press
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Neuromorphic Computing promises orders of magnitude improvement in energy
efficiency compared to traditional von Neumann computing paradigm. The goal is
to develop an adaptive, fault-tolerant, low-footprint, fast, low-energy
intelligent system by learning and emulating brain functionality which can be
realized through innovation in different abstraction layers including material,
device, circuit, architecture and algorithm. As the energy consumption in
complex vision tasks keep increasing exponentially due to larger data set and
resource-constrained edge devices become increasingly ubiquitous, spike-based
neuromorphic computing approaches can be viable alternative to deep
convolutional neural network that is dominating the vision field today. In this
book chapter, we introduce neuromorphic computing, outline a few representative
examples from different layers of the design stack (devices, circuits and
algorithms) and conclude with a few exciting applications and future research
directions that seem promising for computer vision in the near future.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09693" title="Abstract">arXiv:2310.09693</a> [<a href="/pdf/2310.09693" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Influence of Acceleration and Deceleration Capability on Machine Tool  Feed System Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+D">Dongsheng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xuesong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+T">Tingting Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">With the increasing demand for high speed and high precision machining of
machine tools, the problem of which factors of feed system ultimately determine
the performance of machine tools is becoming more and more prominent. At
present, the feed system is designed mainly by limiting the load inertia ratio.
This design method ignores the match between electromechanical system, motion
process and control, and cannot guarantee the optimal performance of the feed
system. And it is also difficult to intuitively explain the relationship
between the inertia ratio and the dynamic performance of the system. Based on
the analysis of the relationship between the structural parameters and the
dynamic performance of the feed system, the viewpoint that the acceleration and
deceleration capacity ultimately determine the performance of the feed system
is put forward in this paper, and the theoretical root of the traditional
design based on the inertia ratio is given. The simulation and experiment show
that if the acceleration and deceleration capacity is too small, there will not
be enough acceleration ability to follow the movement instruction of the
system, resulting in the system performance decline. However, if the
acceleration and deceleration capacity is too large, the system stability will
be reduced, which can explain the traditional design principle of the machine
tool that the inertia ratio should not be too large or too small. This study
provides a clear theoretical basis for machine tool design.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09696" title="Abstract">arXiv:2310.09696</a> [<a href="/pdf/2310.09696" title="Download PDF">pdf</a>, <a href="/format/2310.09696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive Evidence Refinement for Open-domain Multimodal Retrieval  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+A">Anran Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xingjiao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Luwei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tianlong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Cheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Pre-trained multimodal models have achieved significant success in
retrieval-based question answering. However, current multimodal retrieval
question-answering models face two main challenges. Firstly, utilizing
compressed evidence features as input to the model results in the loss of
fine-grained information within the evidence. Secondly, a gap exists between
the feature extraction of evidence and the question, which hinders the model
from effectively extracting critical features from the evidence based on the
given question. We propose a two-stage framework for evidence retrieval and
question-answering to alleviate these issues. First and foremost, we propose a
progressive evidence refinement strategy for selecting crucial evidence. This
strategy employs an iterative evidence retrieval approach to uncover the
logical sequence among the evidence pieces. It incorporates two rounds of
filtering to optimize the solution space, thus further ensuring temporal
efficiency. Subsequently, we introduce a semi-supervised contrastive learning
training strategy based on negative samples to expand the scope of the question
domain, allowing for a more thorough exploration of latent knowledge within
known samples. Finally, in order to mitigate the loss of fine-grained
information, we devise a multi-turn retrieval and question-answering strategy
to handle multimodal inputs. This strategy involves incorporating multimodal
evidence directly into the model as part of the historical dialogue and
question. Meanwhile, we leverage a cross-modal attention mechanism to capture
the underlying connections between the evidence and the question, and the
answer is generated through a decoding generation approach. We validate the
model's effectiveness through extensive experiments, achieving outstanding
performance on WebQA and MultimodelQA benchmark tests.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09699" title="Abstract">arXiv:2310.09699</a> [<a href="/pdf/2310.09699" title="Download PDF">pdf</a>, <a href="/format/2310.09699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Max-Min Fair Resource Allocations Quickly on Large Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Namyar%2C+P">Pooria Namyar</a>, 
<a href="/search/cs?searchtype=author&query=Arzani%2C+B">Behnaz Arzani</a>, 
<a href="/search/cs?searchtype=author&query=Kandula%2C+S">Srikanth Kandula</a>, 
<a href="/search/cs?searchtype=author&query=Segarra%2C+S">Santiago Segarra</a>, 
<a href="/search/cs?searchtype=author&query=Crankshaw%2C+D">Daniel Crankshaw</a>, 
<a href="/search/cs?searchtype=author&query=Krishnaswamy%2C+U">Umesh Krishnaswamy</a>, 
<a href="/search/cs?searchtype=author&query=Govindan%2C+R">Ramesh Govindan</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+H">Himanshu Raj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to USENIX NSDI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We consider the max-min fair resource allocation problem. The best-known
solutions use either a sequence of optimizations or waterfilling, which only
applies to a narrow set of cases. These solutions have become a practical
bottleneck in WAN traffic engineering and cluster scheduling, especially at
larger problem sizes. We improve both approaches: (1) we show how to convert
the optimization sequence into a single fast optimization, and (2) we
generalize waterfilling to the multi-path case. We empirically show our new
algorithms Pareto-dominate prior techniques: they produce faster, fairer, and
more efficient allocations. Some of our allocators also have theoretical
guarantees: they trade off a bounded amount of unfairness for faster
allocation. We have deployed our allocators in Azure's WAN traffic engineering
pipeline, where we preserve solution quality and achieve a roughly $3\times$
speedup.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09700" title="Abstract">arXiv:2310.09700</a> [<a href="/pdf/2310.09700" title="Download PDF">pdf</a>, <a href="/format/2310.09700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mmWave Enabled Connected Autonomous Vehicles: A Use Case with V2V  Cooperative Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mollah%2C+M+B">Muhammad Baqer Mollah</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Honggang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Karim%2C+M+A">Mohammad Ataul Karim</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Hua Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Network, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Social and Information Networks (cs.SI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Connected and autonomous vehicles (CAVs) will revolutionize tomorrow's
intelligent transportation systems, being considered promising to improve
transportation safety, traffic efficiency, and mobility. In fact, envisioned
use cases of CAVs demand very high throughput, lower latency, highly reliable
communications, and precise positioning capabilities. The availability of a
large spectrum at millimeter-wave (mmWave) band potentially promotes new
specifications in spectrum technologies capable of supporting such service
requirements. In this article, we specifically focus on how mmWave
communications are being approached in vehicular standardization activities,
CAVs use cases and deployment challenges in realizing the future fully
connected settings. Finally, we also present a detailed performance assessment
on mmWave-enabled vehicle-to-vehicle (V2V) cooperative perception as an example
case study to show the impact of different configurations.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09705" title="Abstract">arXiv:2310.09705</a> [<a href="/pdf/2310.09705" title="Download PDF">pdf</a>, <a href="/format/2310.09705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SGA: A Graph Augmentation Method for Signed Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+S">Shuyan Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xianda Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kaiqi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiamou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+D">Dong Hao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Signed Graph Neural Networks (SGNNs) are vital for analyzing complex patterns
in real-world signed graphs containing positive and negative links. However,
three key challenges hinder current SGNN-based signed graph representation
learning: sparsity in signed graphs leaves latent structures undiscovered,
unbalanced triangles pose representation difficulties for SGNN models, and
real-world signed graph datasets often lack supplementary information like node
labels and features. These constraints limit the potential of SGNN-based
representation learning. We address these issues with data augmentation
techniques. Despite many graph data augmentation methods existing for unsigned
graphs, none are tailored for signed graphs. Our paper introduces the novel
Signed Graph Augmentation framework (SGA), comprising three main components.
First, we employ the SGNN model to encode the signed graph, extracting latent
structural information for candidate augmentation structures. Second, we
evaluate these candidate samples (edges) and select the most beneficial ones
for modifying the original training set. Third, we propose a novel augmentation
perspective that assigns varying training difficulty to training samples,
enabling the design of a new training strategy. Extensive experiments on six
real-world datasets (Bitcoin-alpha, Bitcoin-otc, Epinions, Slashdot, Wiki-elec,
and Wiki-RfA) demonstrate that SGA significantly improves performance across
multiple benchmarks. Our method outperforms baselines by up to 22.2% in AUC for
SGCN on Wiki-RfA, 33.3% in F1-binary, 48.8% in F1-micro, and 36.3% in F1-macro
for GAT on Bitcoin-alpha in link sign prediction.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09706" title="Abstract">arXiv:2310.09706</a> [<a href="/pdf/2310.09706" title="Download PDF">pdf</a>, <a href="/format/2310.09706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaptSSR: Pre-training User Model with Augmentation-Adaptive  Self-Supervised Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuren Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chao Song</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+M">Min Hou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuqing Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zhihao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zaixi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S+L">Sanshi Lei Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">User modeling, which aims to capture users' characteristics or interests,
heavily relies on task-specific labeled data and suffers from the data sparsity
issue. Several recent studies tackled this problem by pre-training the user
model on massive user behavior sequences with a contrastive learning task.
Generally, these methods assume different views of the same behavior sequence
constructed via data augmentation are semantically consistent, i.e., reflecting
similar characteristics or interests of the user, and thus maximizing their
agreement in the feature space. However, due to the diverse interests and heavy
noise in user behaviors, existing augmentation methods tend to lose certain
characteristics of the user or introduce noisy interests. Thus, forcing the
user model to directly maximize the similarity between the augmented views may
result in a negative transfer. To this end, we propose to replace the
contrastive learning task with a new pretext task: Augmentation-Adaptive
Self-Supervised Ranking (AdaptSSR), which alleviates the requirement of
semantic consistency between the augmented views while pre-training a
discriminative user model. Specifically, we adopt a multiple pairwise ranking
loss which trains the user model to capture the similarity orders between the
implicitly augmented view, the explicitly augmented view, and views from other
users. We further employ an in-batch hard negative sampling strategy to
facilitate model training. Moreover, considering the distinct impacts of data
augmentation on different behavior sequences, we design an
augmentation-adaptive fusion mechanism to automatically adjust the similarity
order constraint applied to each sample based on the estimated similarity
between the augmented views. Extensive experiments on both public and
industrial datasets with six downstream tasks verify the effectiveness of
AdaptSSR.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09709" title="Abstract">arXiv:2310.09709</a> [<a href="/pdf/2310.09709" title="Download PDF">pdf</a>, <a href="/format/2310.09709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Advances in Body Composition Assessment with ShapedNet: A Single  Image Deep Regression Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nascimento%2C+N+M+M">Navar Medeiros M. Nascimento</a>, 
<a href="/search/cs?searchtype=author&query=de+Sousa+Junior%2C+P+C">Pedro Cavalcante de Sousa Junior</a>, 
<a href="/search/cs?searchtype=author&query=Nunes%2C+P+Y+R">Pedro Yuri Rodrigues Nunes</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+S+P+P">Suane Pires Pinheiro da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Loureiro%2C+L+L">Luiz Lannes Loureiro</a>, 
<a href="/search/cs?searchtype=author&query=Bittencourt%2C+V+Z">Victor Zaban Bittencourt</a>, 
<a href="/search/cs?searchtype=author&query=Junior%2C+V+L+M+C">Valden Luis Matos Capistrano Junior</a>, 
<a href="/search/cs?searchtype=author&query=Filho%2C+P+P+R">Pedro Pedrosa Rebou&#xe7;as Filho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprinted version in October 2023. The paper is under consideration at Pattern Recognition Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a novel technique called ShapedNet to enhance body composition
assessment. This method employs a deep neural network capable of estimating
Body Fat Percentage (BFP), performing individual identification, and enabling
localization using a single photograph. The accuracy of ShapedNet is validated
through comprehensive comparisons against the gold standard method, Dual-Energy
X-ray Absorptiometry (DXA), utilizing 1273 healthy adults spanning various
ages, sexes, and BFP levels. The results demonstrate that ShapedNet outperforms
in 19.5% state of the art computer vision-based approaches for body fat
estimation, achieving a Mean Absolute Percentage Error (MAPE) of 4.91% and Mean
Absolute Error (MAE) of 1.42. The study evaluates both gender-based and
Gender-neutral approaches, with the latter showcasing superior performance. The
method estimates BFP with 95% confidence within an error margin of 4.01% to
5.81%. This research advances multi-task learning and body composition
assessment theory through ShapedNet.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09711" title="Abstract">arXiv:2310.09711</a> [<a href="/pdf/2310.09711" title="Download PDF">pdf</a>, <a href="/format/2310.09711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LOVECon: Text-driven Training-Free Long Video Editing with ControlNet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+Z">Zhenyi Liao</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhijie Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Leveraging pre-trained conditional diffusion models for video editing without
further tuning has gained increasing attention due to its promise in film
production, advertising, etc. Yet, seminal works in this line fall short in
generation length, temporal coherence, or fidelity to the source video. This
paper aims to bridge the gap, establishing a simple and effective baseline for
training-free diffusion model-based long video editing. As suggested by prior
arts, we build the pipeline upon ControlNet, which excels at various image
editing tasks based on text prompts. To break down the length constraints
caused by limited computational memory, we split the long video into
consecutive windows and develop a novel cross-window attention mechanism to
ensure the consistency of global style and maximize the smoothness among
windows. To achieve more accurate control, we extract the information from the
source video via DDIM inversion and integrate the outcomes into the latent
states of the generations. We also incorporate a video frame interpolation
model to mitigate the frame-level flickering issue. Extensive empirical studies
verify the superior efficacy of our method over competing baselines across
scenarios, including the replacement of the attributes of foreground objects,
style transfer, and background replacement. In particular, our method manages
to edit videos with up to 128 frames according to user requirements. Code is
available at https://github.com/zhijie-group/LOVECon.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09714" title="Abstract">arXiv:2310.09714</a> [<a href="/pdf/2310.09714" title="Download PDF">pdf</a>, <a href="/format/2310.09714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Task Performance of Learned Simplified Models via  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bui%2C+H">Hien Bui</a>, 
<a href="/search/cs?searchtype=author&query=Posa%2C+M">Michael Posa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In contact-rich tasks, the hybrid, multi-modal nature of contact dynamics
poses great challenges in model representation, planning, and control. Recent
efforts have attempted to address these challenges via data-driven methods,
learning dynamical models in combination with model predictive control. Those
methods, while effective, rely solely on minimizing forward prediction errors
to hope for better task performance with MPC controllers. This weak correlation
can result in data inefficiency as well as limitations to overall performance.
In response, we propose a novel strategy: using a policy gradient algorithm to
find a simplified dynamics model that explicitly maximizes task performance.
Specifically, we parameterize the stochastic policy as the perturbed output of
the MPC controller, thus, the learned model representation can directly
associate with the policy or task performance. We apply the proposed method to
contact-rich tasks where a three-fingered robotic hand manipulates previously
unknown objects. Our method significantly enhances task success rate by up to
15% in manipulating diverse objects compared to the existing method while
sustaining data efficiency. Our method can solve some tasks with success rates
of 70% or higher using under 30 minutes of data. All videos and codes are
available at https://sites.google.com/view/lcs-rl.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09715" title="Abstract">arXiv:2310.09715</a> [<a href="/pdf/2310.09715" title="Download PDF">pdf</a>, <a href="/format/2310.09715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NTT-PIM: Row-Centric Architecture and Mapping for Efficient  Number-Theoretic Transform on PIM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaewoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sugil Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jongeun Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in DAC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Recently DRAM-based PIMs (processing-in-memories) with unmodified cell arrays
have demonstrated impressive performance for accelerating AI applications.
However, due to the very restrictive hardware constraints, PIM remains an
accelerator for simple functions only. In this paper we propose NTT-PIM, which
is based on the same principles such as no modification of cell arrays and very
restrictive area budget, but shows state-of-the-art performance for a very
complex application such as NTT, thanks to features optimized for the
application's characteristics, such as in-place update and pipelining via
multiple buffers. Our experimental results demonstrate that our NTT-PIM can
outperform previous best PIM-based NTT accelerators in terms of runtime by 1.7
~ 17 times while having negligible area and power overhead.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09716" title="Abstract">arXiv:2310.09716</a> [<a href="/pdf/2310.09716" title="Download PDF">pdf</a>, <a href="/format/2310.09716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Conversational Search: Large Language Model-Aided Informative  Query Rewriting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+F">Fanghua Ye</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shenghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Yilmaz%2C+E">Emine Yilmaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, accepted to EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Query rewriting plays a vital role in enhancing conversational search by
transforming context-dependent user queries into standalone forms. Existing
approaches primarily leverage human-rewritten queries as labels to train query
rewriting models. However, human rewrites may lack sufficient information for
optimal retrieval performance. To overcome this limitation, we propose
utilizing large language models (LLMs) as query rewriters, enabling the
generation of informative query rewrites through well-designed instructions. We
define four essential properties for well-formed rewrites and incorporate all
of them into the instruction. In addition, we introduce the role of rewrite
editors for LLMs when initial query rewrites are available, forming a
``rewrite-then-edit'' process. Furthermore, we propose distilling the rewriting
capabilities of LLMs into smaller models to reduce rewriting latency. Our
experimental evaluation on the QReCC dataset demonstrates that informative
query rewrites can yield substantially improved retrieval performance compared
to human rewrites, especially with sparse retrievers.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09718" title="Abstract">arXiv:2310.09718</a> [<a href="/pdf/2310.09718" title="Download PDF">pdf</a>, <a href="/format/2310.09718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Effective Multi-View Subspace Clustering for Large-scale  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuxiu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ren Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Gongguan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Caiming Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent multi-view subspace clustering achieves impressive results utilizing
deep networks, where the self-expressive correlation is typically modeled by a
fully connected (FC) layer. However, they still suffer from two limitations: i)
it is under-explored to extract a unified representation from multiple views
that simultaneously satisfy minimal sufficiency and discriminability. ii) the
parameter scale of the FC layer is quadratic to the number of samples,
resulting in high time and memory costs that significantly degrade their
feasibility in large-scale datasets. In light of this, we propose a novel deep
framework termed Efficient and Effective Large-scale Multi-View Subspace
Clustering (E$^2$LMVSC). Specifically, to enhance the quality of the unified
representation, a soft clustering assignment similarity constraint is devised
for explicitly decoupling consistent, complementary, and superfluous
information across multi-view data. Then, following information bottleneck
theory, a sufficient yet minimal unified feature representation is obtained.
Moreover, E$^2$LMVSC employs the maximal coding rate reduction principle to
promote intra-cluster aggregation and inter-cluster separability within the
unified representation. Finally, the self-expressive coefficients are learned
by a Relation-Metric Net instead of a parameterized FC layer for greater
efficiency. Extensive experiments show that E$^2$LMVSC yields comparable
results to existing methods and achieves state-of-the-art clustering
performance in large-scale multi-view datasets.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09720" title="Abstract">arXiv:2310.09720</a> [<a href="/pdf/2310.09720" title="Download PDF">pdf</a>, <a href="/format/2310.09720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence  Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhuofeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Vydiswaran%2C+V+V">VG Vinod Vydiswaran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of Findings EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we propose a hierarchical contrastive learning framework,
HiCL, which considers local segment-level and global sequence-level
relationships to improve training efficiency and effectiveness. Traditional
methods typically encode a sequence in its entirety for contrast with others,
often neglecting local representation learning, leading to challenges in
generalizing to shorter texts. Conversely, HiCL improves its effectiveness by
dividing the sequence into several segments and employing both local and global
contrastive learning to model segment-level and sequence-level relationships.
Further, considering the quadratic time complexity of transformers over input
tokens, HiCL boosts training efficiency by first encoding short segments and
then aggregating them to obtain the sequence representation. Extensive
experiments show that HiCL enhances the prior top-performing SNCSE model across
seven extensively evaluated STS tasks, with an average increase of +0.2%
observed on BERT-large and +0.44% on RoBERTa-large.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09723" title="Abstract">arXiv:2310.09723</a> [<a href="/pdf/2310.09723" title="Download PDF">pdf</a>, <a href="/format/2310.09723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A generalization of the achievable rate of a MISO system using Bode-Fano  wideband matching theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+N">Nitish Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Castellanos%2C+M+R">Miguel R. Castellanos</a>, 
<a href="/search/cs?searchtype=author&query=Khosravirad%2C+S+R">Saeed R. Khosravirad</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jinfeng Du</a>, 
<a href="/search/cs?searchtype=author&query=Viswanathan%2C+H">Harish Viswanathan</a>, 
<a href="/search/cs?searchtype=author&query=Heath%2C+R+W">Robert W. Heath Jr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Impedance-matching networks affect power transfer from the radio frequency
(RF) chains to the antennas. Their design impacts the signal to noise ratio
(SNR) and the achievable rate. In this paper, we maximize the
information-theoretic achievable rate of a multiple-input-single-output (MISO)
system with wideband matching constraints. Using a multiport circuit theory
approach with frequency-selective scattering parameters, we propose a general
framework for optimizing the MISO achievable rate that incorporates Bode-Fano
wideband matching theory. We express the solution to the achievable rate
optimization problem in terms of the optimized transmission coefficient and the
Lagrangian parameters corresponding to the Bode-Fano inequality constraints. We
apply this framework to a single electric Chu's antenna and an array of two
electric Chu's antennas. We compare the optimized achievable rate obtained
numerically with other benchmarks like the ideal achievable rate computed by
disregarding matching constraints and the achievable rate obtained by using
sub-optimal matching strategies like conjugate matching and frequency-flat
transmission. We also propose a practical methodology to approximate the
achievable rate bound by using the optimal transmission coefficient to derive a
physically realizable matching network through the ADS software.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09725" title="Abstract">arXiv:2310.09725</a> [<a href="/pdf/2310.09725" title="Download PDF">pdf</a>, <a href="/format/2310.09725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KGQUIZ: Evaluating the Generalization of Encoded Knowledge in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yuyang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Balachandran%2C+V">Vidhisha Balachandran</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+S">Shiqi Lou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) demonstrate remarkable performance on
knowledge-intensive tasks, suggesting that real-world knowledge is encoded in
their model parameters. However, besides explorations on a few probing tasks in
limited knowledge domains, it is not well understood how to evaluate LLMs'
knowledge systematically and how well their knowledge abilities generalize,
across a spectrum of knowledge domains and progressively complex task formats.
To this end, we propose KGQuiz, a knowledge-intensive benchmark to
comprehensively investigate the knowledge generalization abilities of LLMs.
KGQuiz is a scalable framework constructed from triplet-based knowledge, which
covers three knowledge domains and consists of five tasks with increasing
complexity: true-or-false, multiple-choice QA, blank filling, factual editing,
and open-ended knowledge generation. To gain a better understanding of LLMs'
knowledge abilities and their generalization, we evaluate 10 open-source and
black-box LLMs on the KGQuiz benchmark across the five knowledge-intensive
tasks and knowledge domains. Extensive experiments demonstrate that LLMs
achieve impressive performance in straightforward knowledge QA tasks, while
settings and contexts requiring more complex reasoning or employing
domain-specific facts still present significant challenges. We envision KGQuiz
as a testbed to analyze such nuanced variations in performance across domains
and task formats, and ultimately to understand, evaluate, and improve LLMs'
knowledge abilities across a wide spectrum of knowledge domains and tasks.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09726" title="Abstract">arXiv:2310.09726</a> [<a href="/pdf/2310.09726" title="Download PDF">pdf</a>, <a href="/format/2310.09726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuseSR: Super Resolution for Real-time Rendering through Efficient  Multi-resolution Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zhihua Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jingsen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yuxin Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuankun Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yuchi Huo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanlin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Hujun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by SIGGRAPH Asia 2023. Project page: <a href="https://isaac-paradox.github.io/FuseSR/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The workload of real-time rendering is steeply increasing as the demand for
high resolution, high refresh rates, and high realism rises, overwhelming most
graphics cards. To mitigate this problem, one of the most popular solutions is
to render images at a low resolution to reduce rendering overhead, and then
manage to accurately upsample the low-resolution rendered image to the target
resolution, a.k.a. super-resolution techniques. Most existing methods focus on
exploiting information from low-resolution inputs, such as historical frames.
The absence of high frequency details in those LR inputs makes them hard to
recover fine details in their high-resolution predictions. In this paper, we
propose an efficient and effective super-resolution method that predicts
high-quality upsampled reconstructions utilizing low-cost high-resolution
auxiliary G-Buffers as additional input. With LR images and HR G-buffers as
input, the network requires to align and fuse features at multi resolution
levels. We introduce an efficient and effective H-Net architecture to solve
this problem and significantly reduce rendering overhead without noticeable
quality deterioration. Experiments show that our method is able to produce
temporally consistent reconstructions in $4 \times 4$ and even challenging $8
\times 8$ upsampling cases at 4K resolution with real-time performance, with
substantially improved quality and significant performance boost compared to
existing works.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09727" title="Abstract">arXiv:2310.09727</a> [<a href="/pdf/2310.09727" title="Download PDF">pdf</a>, <a href="/format/2310.09727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Fast Convergence of Independent Natural Policy Gradient for  Markov Potential Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Youbang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Ruida Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P+R">P. R. Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Shahrampour%2C+S">Shahin Shahrampour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Will appear in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This work studies an independent natural policy gradient (NPG) algorithm for
the multi-agent reinforcement learning problem in Markov potential games. It is
shown that, under mild technical assumptions and the introduction of the
suboptimality gap, the independent NPG method with an oracle providing exact
policy evaluation asymptotically reaches an $\epsilon$-Nash Equilibrium (NE)
within $\mathcal{O}(1/\epsilon)$ iterations. This improves upon the previous
best result of $\mathcal{O}(1/\epsilon^2)$ iterations and is of the same order,
$\mathcal{O}(1/\epsilon)$, that is achievable for the single-agent case.
Empirical results for a synthetic potential game and a congestion game are
presented to verify the theoretical bounds.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09728" title="Abstract">arXiv:2310.09728</a> [<a href="/pdf/2310.09728" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SVM based Multiclass Classifier for Gait phase Classification using  Shank IMU Sensor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%2C+A+K+G">Aswadh Khumar G S</a>, 
<a href="/search/cs?searchtype=author&query=JK%2C+B+K">Barath Kumar JK</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In this study, a gait phase classification method based on SVM multiclass
classification is introduced, with a focus on the precise identification of the
stance and swing phases, which are further subdivided into seven phases. Data
from individual IMU sensors, such as Shank Acceleration X, Y, Z, Shank Gyro X,
and Knee Angles, are used as features in this classification model. The
suggested technique successfully classifies the various gait phases with a
significant accuracy of about 90.3%. Gait phase classification is crucial,
especially in the domains of exoskeletons and prosthetics, where accurate
identification of gait phases enables seamless integration with assistive
equipment, improving mobility, stability, and energy economy. This study
extends the study of gait and offers an effective method for correctly
identifying gait phases from Shank IMU sensor data, with potential applications
in biomechanical research, exoskeletons, rehabilitation, and prosthetics.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09729" title="Abstract">arXiv:2310.09729</a> [<a href="/pdf/2310.09729" title="Download PDF">pdf</a>, <a href="/format/2310.09729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Synthetic Data Meets Ensemble Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haoyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Azizan%2C+N">Navid Azizan</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Akash Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">When machine learning models are trained on synthetic data and then deployed
on real data, there is often a performance drop due to the distribution shift
between synthetic and real data. In this paper, we introduce a new ensemble
strategy for training downstream models, with the goal of enhancing their
performance when used on real data. We generate multiple synthetic datasets by
applying a differential privacy (DP) mechanism several times in parallel and
then ensemble the downstream models trained on these datasets. While each
synthetic dataset might deviate more from the real data distribution, they
collectively increase sample diversity. This may enhance the robustness of
downstream models against distribution shifts. Our extensive experiments reveal
that while ensembling does not enhance downstream performance (compared with
training a single model) for models trained on synthetic data generated by
marginal-based or workload-based DP mechanisms, our proposed ensemble strategy
does improve the performance for models trained using GAN-based DP mechanisms
in terms of both accuracy and calibration of downstream models.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09731" title="Abstract">arXiv:2310.09731</a> [<a href="/pdf/2310.09731" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Gait Modelling of Lower Limb Dynamics : A Mathematical Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=JK%2C+B+K">Barath Kumar JK</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+A+K+G">Aswadh Khumar G S</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 3 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper focuses on the analysis of human gait cycle dynamics and presents
a mathematical model to determine the torque exerted on the lower limb joints
throughout the complete gait cycle, including its various phases. The study
involved a healthy subject who participated in a series of initial walking
experiments. The development of a mathematical model that accurately represents
the natural motion of the human lower limb has garnered significant attention
in the field of lower limb prosthetics design. In this study, the researchers
incorporated the functional relationship between the limb joints and the end
effector of the lower extremity. This knowledge is crucial for rehabilitation
purposes as it helps in understanding the connectivity of joints, links, and
the overall body orientation required to effectively control the motion of the
actuators. When analysing physical activities, measurements of human strength
play a crucial role. Traditionally, these measurements have focused on
determining the maximum voluntary torque at a single joint angle and angular
velocity. However, it is important to consider that the available strength
varies significantly with joint position and velocity.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09733" title="Abstract">arXiv:2310.09733</a> [<a href="/pdf/2310.09733" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Intelligent Algorithms for Gait Phase Classification in Lower  Limb Robotic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=JK%2C+B+K">Barath Kumar JK</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+A+K+G">Aswadh Khumar G S</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 Pages,28 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Accurate and rapid detection of gait phases is of utmost importance in
achieving optimal performance of powered lower-limb prostheses and
exoskeletons. With the increasing versatility and complexity of these robotic
systems, there is a growing need to enhance the performance of gait detection
algorithms. The development of reliable and functional gait detection
algorithms holds the potential to enhance precision, stability, and safety in
prosthetic devices and other rehabilitation technologies. In this systematic
review, we delve into the extensive body of research and development in the
domain of gait event detection methods, with a specific focus on their
application to prosthetic devices. Our review critically assesses various
proposed methods, aiming to identify the most effective approaches for gait
phase classification in lower limb robotic systems. Through a comprehensive
comparative analysis, we highlight the strengths and weaknesses of different
algorithms, shedding light on their performance characteristics, applicability,
and potential for further improvements. This comprehensive review was conducted
by screening two databases, namely IEEE and Scopus. The search was limited to
204 papers published from 2010 to 2023. A total of 6 papers that focused on
Heuristic, Thresholding, and Amplitude Zero Crossing involved techniques were
identified and included in the review. 33.3% of implemented Algorithms used
kinematic parameters such as joint angles, joint linear and angular velocity,
and joint angular acceleration. This study purely focuses on threshold-based
algorithms and thus paper focusing on other gait phase detection methods were
excluded.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09734" title="Abstract">arXiv:2310.09734</a> [<a href="/pdf/2310.09734" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Behaviors of QCA Inverter due to Cell Displacement and Temperature  Variation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Angshuman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Sur%2C+S">Surajit Sur</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+C">Chiradeep Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Sukla%2C+A+S">Aninda Sankar Sukla</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+R">Ratna Chakrabarty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proc. of 2015 2nd International Conference on Nanotechnology (ICNT 2015)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">Quantum dot Cellular Automata (QCA) is the emerging area in the field of
nanotechnology. Inverter is a fundamental logic primitive in QCA. Molecular,
semiconductor, magnetic, and metallic QCA are main methodology in the
fabrication of quantum cell. While all types of QCA work on room temperature,
metallic one is not suitable in normal temperature. So temperature plays a
significant role in QCA circuit. In this paper, the effect of temperature in
two-cell conventional inverter and recently proposed three-cell high polarized
inverter has been discussed. The polarization and Kink energy of QCA circuit is
influenced due to the change of distance between two cells. This paper clearly
mentioned the variation of polarization and kink energy of QCA inverter due to
cell displacement. Finally this paper makes a comparison between the
conventional two-cell inverter and recently proposed three-cell inverter. The
simulation tool QCADesigner has been used to study the effects of QCA
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09735" title="Abstract">arXiv:2310.09735</a> [<a href="/pdf/2310.09735" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Smart Algorithms for Gait Phases Detection in Lower Limb  Prosthesis: A Comprehensive Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=JK%2C+B+K">Barath Kumar JK</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+A+K+G">Aswadh Khumar G S</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages,14 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Over the past few years, the division of gait phases has emerged as a complex
area of research that carries significant importance for various applications
in the field of gait technologies. The accurate partitioning of gait phases
plays a crucial role in advancing these applications. Researchers have been
exploring a range of sensors that can be employed to provide data for
algorithms involved in gait phase partitioning. These sensors can be broadly
categorized into two types: wearable and non-wearable, each offering unique
advantages and capabilities. In our study aimed at examining the current
approaches to gait analysis and detection specifically designed for
implementation in ambulatory rehabilitation systems, we conducted a
comprehensive meta-analysis of existing research studies. Our analysis revealed
a diverse range of sensors and sensor combinations that demonstrate the ability
to analyze gait patterns in ambulatory settings. These sensor options vary from
basic force-based binary switches to more intricate setups incorporating
multiple inertial sensors and sophisticated algorithms. The findings highlight
the wide spectrum of available technologies and methodologies used in gait
analysis for ambulatory applications. To conduct an extensive review, we
systematically examined two prominent databases, IEEE and Scopus, with the aim
of identifying relevant studies pertaining to gait analysis. The search
criteria were limited to 189 papers published between 1999 and 2023. From this
pool, we identified and included five papers that specifically focused on
various techniques including Thresholding, Quasi-static method, adaptive
classifier, and SVM-based approaches. These selected papers provided valuable
insights for our review.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09736" title="Abstract">arXiv:2310.09736</a> [<a href="/pdf/2310.09736" title="Download PDF">pdf</a>, <a href="/format/2310.09736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-Specific Language Model Post-Training for Indonesian Financial  NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maharani%2C+N+P+I">Ni Putu Intan Maharani</a>, 
<a href="/search/cs?searchtype=author&query=Yustiawan%2C+Y">Yoga Yustiawan</a>, 
<a href="/search/cs?searchtype=author&query=Rochim%2C+F+C">Fauzy Caesar Rochim</a>, 
<a href="/search/cs?searchtype=author&query=Purwarianti%2C+A">Ayu Purwarianti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICEEI 2023 (International Conference on Electrical Engineering and Informatics 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">BERT and IndoBERT have achieved impressive performance in several NLP tasks.
There has been several investigation on its adaption in specialized domains
especially for English language. We focus on financial domain and Indonesian
language, where we perform post-training on pre-trained IndoBERT for financial
domain using a small scale of Indonesian financial corpus. In this paper, we
construct an Indonesian self-supervised financial corpus, Indonesian financial
sentiment analysis dataset, Indonesian financial topic classification dataset,
and release a family of BERT models for financial NLP. We also evaluate the
effectiveness of domain-specific post-training on sentiment analysis and topic
classification tasks. Our findings indicate that the post-training increases
the effectiveness of a language model when it is fine-tuned to domain-specific
downstream tasks.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09739" title="Abstract">arXiv:2310.09739</a> [<a href="/pdf/2310.09739" title="Download PDF">pdf</a>, <a href="/format/2310.09739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AugUndo: Scaling Up Augmentations for Unsupervised Depth Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yangchao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T+Y">Tian Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hyoungseob Park</a>, 
<a href="/search/cs?searchtype=author&query=Soatto%2C+S">Stefano Soatto</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+D">Dong Lao</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+A">Alex Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised depth completion methods are trained by minimizing sparse depth
and image reconstruction error. Block artifacts from resampling, intensity
saturation, and occlusions are amongst the many undesirable by-products of
common data augmentation schemes that affect image reconstruction quality, and
thus the training signal. Hence, typical augmentations on images that are
viewed as essential to training pipelines in other vision tasks have seen
limited use beyond small image intensity changes and flipping. The sparse depth
modality have seen even less as intensity transformations alter the scale of
the 3D scene, and geometric transformations may decimate the sparse points
during resampling. We propose a method that unlocks a wide range of
previously-infeasible geometric augmentations for unsupervised depth
completion. This is achieved by reversing, or "undo"-ing, geometric
transformations to the coordinates of the output depth, warping the depth map
back to the original reference frame. This enables computing the reconstruction
losses using the original images and sparse depth maps, eliminating the
pitfalls of naive loss computation on the augmented inputs. This simple yet
effective strategy allows us to scale up augmentations to boost performance. We
demonstrate our method on indoor (VOID) and outdoor (KITTI) datasets where we
improve upon three existing methods by an average of 10.4\% across both
datasets.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09742" title="Abstract">arXiv:2310.09742</a> [<a href="/pdf/2310.09742" title="Download PDF">pdf</a>, <a href="/format/2310.09742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Bill of Materials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boucher%2C+N">Nicholas Boucher</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+R">Ross Anderson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Ensuring the security of software supply chains requires reliable
identification of upstream dependencies. We present the Automatic Bill of
Materials, or ABOM, a technique for embedding dependency metadata in binaries
at compile time. Rather than relying on developers to explicitly enumerate
dependency names and versions, ABOM embeds a hash of each distinct input source
code file into the binary emitted by a compiler. Hashes are stored in
Compressed Bloom Filters, highly space-efficient probabilistic data structures,
which enable querying for the presence of dependencies without the possibility
of false negatives. If leveraged across the ecosystem, ABOMs provide a
zero-touch, backwards-compatible, drop-in solution for fast supply chain attack
detection in real-world, language-independent software.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09743" title="Abstract">arXiv:2310.09743</a> [<a href="/pdf/2310.09743" title="Download PDF">pdf</a>, <a href="/format/2310.09743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding Modular Reconfigurable Robots: A Survey on Mechanisms and  Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+G">Guanqi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Y">Yuxiao Tu</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+T+L">Tin Lun Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The intrinsic modularity and reconfigurability of modular reconfigurable
robots (MRR) confer advantages such as versatility, fault tolerance, and
economic efficacy, thereby showcasing considerable potential across diverse
applications. The continuous evolution of the technology landscape and the
emergence of diverse conceptual designs have generated multiple MRR categories,
each described by its respective morphology or capability characteristics,
leading to some ambiguity in the taxonomy. This paper conducts a comprehensive
survey encompassing the entirety of MRR hardware and design, spanning from the
inception in 1985 to 2023. This paper introduces an innovative, unified
conceptual framework for understanding MRR hardware, which encompasses three
pivotal elements: connectors, actuators, and homogeneity. Through the
utilization of this trilateral framework, this paper provide an intuitive
understanding of the diverse spectrum of MRR hardware iterations while
systematically deciphering and classifying the entire range, offering a more
structured perspective. This survey elucidates the fundamental attributes
characterizing MRRs and their compositional aspects, providinig insights into
their design, technology, functionality, and categorization. Augmented by the
proposed trilateral framework, this paper also elaborates on the trajectory of
evolution, prevailing trends, principal challenges, and potential prospects
within the field of MRRs.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09744" title="Abstract">arXiv:2310.09744</a> [<a href="/pdf/2310.09744" title="Download PDF">pdf</a>, <a href="/format/2310.09744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explore the Effect of Data Selection on Poison Efficiency in Backdoor  Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+P">Pengfei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yueqi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)

</div>
<p class="mathjax">As the number of parameters in Deep Neural Networks (DNNs) scales, the thirst
for training data also increases. To save costs, it has become common for users
and enterprises to delegate time-consuming data collection to third parties.
Unfortunately, recent research has shown that this practice raises the risk of
DNNs being exposed to backdoor attacks. Specifically, an attacker can
maliciously control the behavior of a trained model by poisoning a small
portion of the training data. In this study, we focus on improving the
poisoning efficiency of backdoor attacks from the sample selection perspective.
The existing attack methods construct such poisoned samples by randomly
selecting some clean data from the benign set and then embedding a trigger into
them. However, this random selection strategy ignores that each sample may
contribute differently to the backdoor injection, thereby reducing the
poisoning efficiency. To address the above problem, a new selection strategy
named Improved Filtering and Updating Strategy (FUS++) is proposed.
Specifically, we adopt the forgetting events of the samples to indicate the
contribution of different poisoned samples and use the curvature of the loss
surface to analyses the effectiveness of this phenomenon. Accordingly, we
combine forgetting events and curvature of different samples to conduct a
simple yet efficient sample selection strategy. The experimental results on
image classification (CIFAR-10, CIFAR-100, ImageNet-10), text classification
(AG News), audio classification (ESC-50), and age regression (Facial Age)
consistently demonstrate the effectiveness of the proposed strategy: the attack
performance using FUS++ is significantly higher than that using random
selection for the same poisoning ratio.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09747" title="Abstract">arXiv:2310.09747</a> [<a href="/pdf/2310.09747" title="Download PDF">pdf</a>, <a href="/format/2310.09747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Staged Depthwise Correlation and Feature Fusion for Siamese Object  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">Dianbo Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jianqiang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Ziyan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yamane%2C+S">Satoshi Yamane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in 2023 International Joint Conference on Neural Networks (IJCNN)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work, we propose a novel staged depthwise correlation and feature
fusion network, named DCFFNet, to further optimize the feature extraction for
visual tracking. We build our deep tracker upon a siamese network architecture,
which is offline trained from scratch on multiple large-scale datasets in an
end-to-end manner. The model contains a core component, that is, depthwise
correlation and feature fusion module (correlation-fusion module), which
facilitates model to learn a set of optimal weights for a specific object by
utilizing ensembles of multi-level features from lower and higher layers and
multi-channel semantics on the same layer. We combine the modified ResNet-50
with the proposed correlation-fusion layer to constitute the feature extractor
of our model. In training process, we find the training of model become more
stable, that benifits from the correlation-fusion module. For comprehensive
evaluations of performance, we implement our tracker on the popular benchmarks,
including OTB100, VOT2018 and LaSOT. Extensive experiment results demonstrate
that our proposed method achieves favorably competitive performance against
many leading trackers in terms of accuracy and precision, while satisfying the
real-time requirements of applications.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09748" title="Abstract">arXiv:2310.09748</a> [<a href="/pdf/2310.09748" title="Download PDF">pdf</a>, <a href="/format/2310.09748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model-Aware In-Context Learning for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Ge Li</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chongyang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huangzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhi Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have shown impressive in-context learning (ICL)
ability in code generation. LLMs take a prompt consisting of requirement-code
examples and a new requirement as input, and output new programs. Existing
studies have found that ICL is highly dominated by the examples and thus arises
research on example selection. However, existing approaches randomly select
examples or only consider the textual similarity of requirements to retrieve,
leading to sub-optimal performance. In this paper, we propose a novel
learning-based selection approach named LAIL (LLM-Aware In-context Learning)
for code generation. Given a candidate example, we exploit LLMs themselves to
estimate it by considering the generation probabilities of ground-truth
programs given a requirement and the example. We then label candidate examples
as positive or negative through the probability feedback. Based on the labeled
data, we import a contrastive learning objective to train an effective
retriever that acquires the preference of LLMs in code generation. We apply
LAIL to three LLMs and evaluate it on three representative datasets (e.g.,
MBJP, MBPP, and MBCPP). LATA outperforms the state-of-the-art baselines by
11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5 in
terms of Pass@1, respectively.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09749" title="Abstract">arXiv:2310.09749</a> [<a href="/pdf/2310.09749" title="Download PDF">pdf</a>, <a href="/format/2310.09749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Torch to Projector: Fundamental Tradeoff of Integrated Sensing and  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yifeng Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Weijie Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yuanhao Cui</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures, submitted to IEEE BITS the Information Theory Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Sensing and communications (S&amp;C) have been historically developed in
parallel. In recent decade, they have been evolving from separation to
integration, giving rise to the integrated sensing and communications (ISAC)
paradigm, that has been recognized as one of the six key 6G usage scenarios.
Despite the plethora of research works dedicated to ISAC signal processing, the
fundamental performance limits of S&amp;C remain widely unexplored in an ISAC
system. In this tutorial paper, we attempt to summarize the recent research
findings in characterizing the performance boundary of ISAC systems and the
resulting S&amp;C tradeoff from an information-theoretical viewpoint. We begin with
a folklore "torch metaphor" that depicts the resource competition mechanism of
S&amp;C. Then, we elaborate on the fundamental capacity-distortion (C-D) theory,
indicating the incompleteness of this metaphor. Towards that end, we further
elaborate on the S&amp;C tradeoff by discussing a special case within the C-D
framework, namely the Cramer-Rao bound (CRB)-rate region. In particular, S&amp;C
have preference discrepancies over both the subspace occupied by the
transmitted signal and the adopted codebook, leading to a "projector metaphor"
complementary to the ISAC torch analogy. We also present two practical design
examples by leveraging the lessons learned from fundamental theories. Finally,
we conclude the paper by identifying a number of open challenges.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09751" title="Abstract">arXiv:2310.09751</a> [<a href="/pdf/2310.09751" title="Download PDF">pdf</a>, <a href="/format/2310.09751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junfeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+R">Roger Zimmermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Multivariate time series forecasting plays a pivotal role in contemporary web
technologies. In contrast to conventional methods that involve creating
dedicated models for specific time series application domains, this research
advocates for a unified model paradigm that transcends domain boundaries.
However, learning an effective cross-domain model presents the following
challenges. First, various domains exhibit disparities in data characteristics,
e.g., the number of variables, posing hurdles for existing models that impose
inflexible constraints on these factors. Second, the model may encounter
difficulties in distinguishing data from various domains, leading to suboptimal
performance in our assessments. Third, the diverse convergence rates of time
series domains can also result in compromised empirical performance. To address
these issues, we propose UniTime for effective cross-domain time series
learning. Concretely, UniTime can flexibly adapt to data with varying
characteristics. It also uses domain instructions and a Language-TS Transformer
to offer identification information and align two modalities. In addition,
UniTime employs masking to alleviate domain convergence speed imbalance issues.
Our extensive experiments demonstrate the effectiveness of UniTime in advancing
state-of-the-art forecasting performance and zero-shot transferability.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09753" title="Abstract">arXiv:2310.09753</a> [<a href="/pdf/2310.09753" title="Download PDF">pdf</a>, <a href="/format/2310.09753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When can transformers reason with abstract symbols?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boix-Adsera%2C+E">Enric Boix-Adsera</a>, 
<a href="/search/cs?searchtype=author&query=Saremi%2C+O">Omid Saremi</a>, 
<a href="/search/cs?searchtype=author&query=Abbe%2C+E">Emmanuel Abbe</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+S">Samy Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Littwin%2C+E">Etai Littwin</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+J">Joshua Susskind</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We investigate the capabilities of transformer large language models (LLMs)
on relational reasoning tasks involving abstract symbols. Such tasks have long
been studied in the neuroscience literature as fundamental building blocks for
more complex abilities in programming, mathematics, and verbal reasoning. For
(i) regression tasks, we prove that transformers generalize when trained, but
require astonishingly large quantities of training data. For (ii)
next-token-prediction tasks with symbolic labels, we show an "inverse scaling
law": transformers fail to generalize as their embedding dimension increases.
For both settings (i) and (ii), we propose subtle transformer modifications
which can reduce the amount of data needed by adding two trainable parameters
per head.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09754" title="Abstract">arXiv:2310.09754</a> [<a href="/pdf/2310.09754" title="Download PDF">pdf</a>, <a href="/format/2310.09754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huanhuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weizhi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yifan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liuji Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Fact verification aims to automatically probe the veracity of a claim based
on several pieces of evidence. Existing works are always engaging in the
accuracy improvement, let alone the explainability, a critical capability of
fact verification system. Constructing an explainable fact verification system
in a complex multi-hop scenario is consistently impeded by the absence of a
relevant high-quality dataset. Previous dataset either suffer from excessive
simplification or fail to incorporate essential considerations for
explainability. To address this, we present EX-FEVER, a pioneering dataset for
multi-hop explainable fact verification. With over 60,000 claims involving
2-hop and 3-hop reasoning, each is created by summarizing and modifying
information from hyperlinked Wikipedia documents. Each instance is accompanied
by a veracity label and an explanation that outlines the reasoning path
supporting the veracity classification. Additionally, we demonstrate a novel
baseline system on our EX-FEVER dataset, showcasing document retrieval,
explanation generation, and claim verification and observe that existing fact
verification models trained on previous datasets struggle to perform well on
our dataset. Furthermore, we highlight the potential of utilizing Large
Language Models in the fact verification task. We hope our dataset could make a
significant contribution by providing ample opportunities to explore the
integration of natural language explanations in the domain of fact
verification.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09755" title="Abstract">arXiv:2310.09755</a> [<a href="/pdf/2310.09755" title="Download PDF">pdf</a>, <a href="/format/2310.09755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Segmentation: Road Network Generation with Multi-Modal LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasal%2C+S">Sumedh Rasal</a>, 
<a href="/search/cs?searchtype=author&query=Boddhu%2C+S+K">Sanjay Kumar Boddhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces an innovative approach to road network generation
through the utilization of a multi-modal Large Language Model (LLM). Our model
is specifically designed to process aerial images of road layouts and produce
detailed, navigable road networks within the input images. The core innovation
of our system lies in the unique training methodology employed for the large
language model to generate road networks as its output. This approach draws
inspiration from the BLIP-2 architecture <a href="/abs/2301.12597">arXiv:2301.12597</a>, leveraging
pre-trained frozen image encoders and large language models to create a
versatile multi-modal LLM.
<br />Our work also offers an alternative to the reasoning segmentation method
proposed in the LISA paper <a href="/abs/2308.00692">arXiv:2308.00692</a>. By training the large language
model with our approach, the necessity for generating binary segmentation
masks, as suggested in the LISA paper <a href="/abs/2308.00692">arXiv:2308.00692</a>, is effectively
eliminated. Experimental results underscore the efficacy of our multi-modal LLM
in providing precise and valuable navigational guidance. This research
represents a significant stride in bolstering autonomous navigation systems,
especially in road network scenarios, where accurate guidance is of paramount
importance.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09756" title="Abstract">arXiv:2310.09756</a> [<a href="/pdf/2310.09756" title="Download PDF">pdf</a>, <a href="/format/2310.09756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Benchmarks for Asian Facial Recognition Tasks: Face Classification  with Large Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Jinwoo Seo</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Soora Choi</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+E">Eungyeom Ha</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Beomjune Kim</a>, 
<a href="/search/cs?searchtype=author&query=Na%2C+D">Dongbin Na</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The face classification system is an important tool for recognizing personal
identity properly. This paper introduces a new Large-Scale Korean Influencer
Dataset named KoIn. Our presented dataset contains many real-world photos of
Korean celebrities in various environments that might contain stage lighting,
backup dancers, and background objects. These various images can be useful for
training classification models classifying K-influencers. Most of the images in
our proposed dataset have been collected from social network services (SNS)
such as Instagram. Our dataset, KoIn, contains over 100,000 K-influencer photos
from over 100 Korean celebrity classes. Moreover, our dataset provides
additional hard case samples such as images including human faces with masks
and hats. We note that the hard case samples are greatly useful in evaluating
the robustness of the classification systems. We have extensively conducted
several experiments utilizing various classification models to validate the
effectiveness of our proposed dataset. Specifically, we demonstrate that recent
state-of-the-art (SOTA) foundation architectures show decent classification
performance when trained on our proposed dataset. In this paper, we also
analyze the robustness performance against hard case samples of large-scale
foundation models when we fine-tune the foundation models on the normal cases
of the proposed dataset, KoIn. Our presented dataset and codes will be publicly
available at https://github.com/dukong1/KoIn_Benchmark_Dataset.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09757" title="Abstract">arXiv:2310.09757</a> [<a href="/pdf/2310.09757" title="Download PDF">pdf</a>, <a href="/format/2310.09757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoEmo Vision Transformer: Integrating Cross-Attention and Movement  Vectors in 3D Pose Estimation for HRI Emotion Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+D+C">David C. Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tianma Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongji Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+R">Raghav Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C">Casey Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Song Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kitts%2C+C+A">Christopher A. Kitts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE/RSJ International Conference on Intelligent Robots (IROS), Detroit, Michigan
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/RSJ International Conference on
  Intelligent Robots (IROS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Emotion detection presents challenges to intelligent human-robot interaction
(HRI). Foundational deep learning techniques used in emotion detection are
limited by information-constrained datasets or models that lack the necessary
complexity to learn interactions between input data elements, such as the the
variance of human emotions across different contexts. In the current effort, we
introduce 1) MoEmo (Motion to Emotion), a cross-attention vision transformer
(ViT) for human emotion detection within robotics systems based on 3D human
pose estimations across various contexts, and 2) a data set that offers
full-body videos of human movement and corresponding emotion labels based on
human gestures and environmental contexts. Compared to existing approaches, our
method effectively leverages the subtle connections between movement vectors of
gestures and environmental contexts through the use of cross-attention on the
extracted movement vectors of full-body human gestures/poses and feature maps
of environmental contexts. We implement a cross-attention fusion model to
combine movement vectors and environment contexts into a joint representation
to derive emotion estimation. Leveraging our Naturalistic Motion Database, we
train the MoEmo system to jointly analyze motion and context, yielding emotion
detection that outperforms the current state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09759" title="Abstract">arXiv:2310.09759</a> [<a href="/pdf/2310.09759" title="Download PDF">pdf</a>, <a href="/format/2310.09759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prototype-oriented Unsupervised Change Detection for Disaster Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+Y">Youngtack Oh</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minseok Seo</a>, 
<a href="/search/cs?searchtype=author&query=Ki%2C+D">Doyi Ki</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Junghoon Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4page, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Climate change has led to an increased frequency of natural disasters such as
floods and cyclones. This emphasizes the importance of effective disaster
monitoring. In response, the remote sensing community has explored change
detection methods. These methods are primarily categorized into supervised
techniques, which yield precise results but come with high labeling costs, and
unsupervised techniques, which eliminate the need for labeling but involve
intricate hyperparameter tuning. To address these challenges, we propose a
novel unsupervised change detection method named Prototype-oriented
Unsupervised Change Detection for Disaster Management (PUCD). PUCD captures
changes by comparing features from pre-event, post-event, and
prototype-oriented change synthesis images via a foundational model, and
refines results using the Segment Anything Model (SAM). Although PUCD is an
unsupervised change detection, it does not require complex hyperparameter
tuning. We evaluate PUCD framework on the LEVIR-Extension dataset and the
disaster dataset and it achieves state-of-the-art performance compared to other
methods on the LEVIR-Extension dataset.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09760" title="Abstract">arXiv:2310.09760</a> [<a href="/pdf/2310.09760" title="Download PDF">pdf</a>, <a href="/format/2310.09760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Augmentation with Controlled Diffusion for Weakly-Supervised  Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wangyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+T">Tianhong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaowei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jimin Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Weakly-supervised semantic segmentation (WSSS), which aims to train
segmentation models solely using image-level labels, has achieved significant
attention. Existing methods primarily focus on generating high-quality pseudo
labels using available images and their image-level labels. However, the
quality of pseudo labels degrades significantly when the size of available
dataset is limited. Thus, in this paper, we tackle this problem from a
different view by introducing a novel approach called Image Augmentation with
Controlled Diffusion (IACD). This framework effectively augments existing
labeled datasets by generating diverse images through controlled diffusion,
where the available images and image-level labels are served as the controlling
information. Moreover, we also propose a high-quality image selection strategy
to mitigate the potential noise introduced by the randomness of diffusion
models. In the experiments, our proposed IACD approach clearly surpasses
existing state-of-the-art methods. This effect is more obvious when the amount
of available data is small, demonstrating the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09761" title="Abstract">arXiv:2310.09761</a> [<a href="/pdf/2310.09761" title="Download PDF">pdf</a>, <a href="/format/2310.09761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAPro: Webly Supervised Learning with Cross-Modality Aligned Prototypes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yulei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yunhang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chaoyou Fu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yun Gu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Webly supervised learning has attracted increasing attention for its
effectiveness in exploring publicly accessible data at scale without manual
annotation. However, most existing methods of learning with web datasets are
faced with challenges from label noise, and they have limited assumptions on
clean samples under various noise. For instance, web images retrieved with
queries of tiger cat (a cat species) and drumstick (a musical instrument) are
almost dominated by images of tigers and chickens, which exacerbates the
challenge of fine-grained visual concept learning. In this case, exploiting
both web images and their associated texts is a requisite solution to combat
real-world noise. In this paper, we propose Cross-modality Aligned Prototypes
(CAPro), a unified prototypical contrastive learning framework to learn visual
representations with correct semantics. For one thing, we leverage textual
prototypes, which stem from the distinct concept definition of classes, to
select clean images by text matching and thus disambiguate the formation of
visual prototypes. For another, to handle missing and mismatched noisy texts,
we resort to the visual feature space to complete and enhance individual texts
and thereafter improve text matching. Such semantically aligned visual
prototypes are further polished up with high-quality samples, and engaged in
both cluster regularization and noise removal. Besides, we propose collective
bootstrapping to encourage smoother and wiser label reference from
appearance-similar instances in a manner of dictionary look-up. Extensive
experiments on WebVision1k and NUS-WIDE (Web) demonstrate that CAPro well
handles realistic noise under both single-label and multi-label scenarios.
CAPro achieves new state-of-the-art performance and exhibits robustness to
open-set recognition. Codes are available at https://github.com/yuleiqin/capro.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09762" title="Abstract">arXiv:2310.09762</a> [<a href="/pdf/2310.09762" title="Download PDF">pdf</a>, <a href="/format/2310.09762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversifying the Mixture-of-Experts Representation for Language Models  with Orthogonal Optimizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Boan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Keqin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Dazhao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The Mixture of Experts (MoE) has emerged as a highly successful technique in
deep learning, based on the principle of divide-and-conquer to maximize model
capacity without significant additional computational cost. Even in the era of
large-scale language models (LLMs), MoE continues to play a crucial role, as
some researchers have indicated that GPT-4 adopts the MoE structure to ensure
diverse inference results. However, MoE is susceptible to performance
degeneracy, particularly evident in the issues of imbalance and homogeneous
representation among experts. While previous studies have extensively addressed
the problem of imbalance, the challenge of homogeneous representation remains
unresolved. In this study, we shed light on the homogeneous representation
problem, wherein experts in the MoE fail to specialize and lack diversity,
leading to frustratingly high similarities in their representations (up to 99%
in a well-performed MoE model). This problem restricts the expressive power of
the MoE and, we argue, contradicts its original intention. To tackle this
issue, we propose a straightforward yet highly effective solution: OMoE, an
orthogonal expert optimizer. Additionally, we introduce an alternating training
strategy that encourages each expert to update in a direction orthogonal to the
subspace spanned by other experts. Our algorithm facilitates MoE training in
two key ways: firstly, it explicitly enhances representation diversity, and
secondly, it implicitly fosters interaction between experts during orthogonal
weights computation. Through extensive experiments, we demonstrate that our
proposed optimization algorithm significantly improves the performance of
fine-tuning the MoE model on the GLUE benchmark, SuperGLUE benchmark,
question-answering task, and name entity recognition tasks.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09764" title="Abstract">arXiv:2310.09764</a> [<a href="/pdf/2310.09764" title="Download PDF">pdf</a>, <a href="/format/2310.09764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DropMix: Better Graph Contrastive Learning with Harder Negative Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yueqi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While generating better negative samples for contrastive learning has been
widely studied in the areas of CV and NLP, very few work has focused on
graph-structured data. Recently, Mixup has been introduced to synthesize hard
negative samples in graph contrastive learning (GCL). However, due to the
unsupervised learning nature of GCL, without the help of soft labels, directly
mixing representations of samples could inadvertently lead to the information
loss of the original hard negative and further adversely affect the quality of
the newly generated harder negative. To address the problem, in this paper, we
propose a novel method DropMix to synthesize harder negative samples, which
consists of two main steps. Specifically, we first select some hard negative
samples by measuring their hardness from both local and global views in the
graph simultaneously. After that, we mix hard negatives only on partial
representation dimensions to generate harder ones and decrease the information
loss caused by Mixup. We conduct extensive experiments to verify the
effectiveness of DropMix on six benchmark datasets. Our results show that our
method can lead to better GCL performance. Our data and codes are publicly
available at https://github.com/Mayueq/DropMix-Code.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09765" title="Abstract">arXiv:2310.09765</a> [<a href="/pdf/2310.09765" title="Download PDF">pdf</a>, <a href="/format/2310.09765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Access to Justice for the Indian Population: A Benchmark for  Evaluating Translation of Legal Text to Indian Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahapatra%2C+S">Sayan Mahapatra</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+D">Debtanu Datta</a>, 
<a href="/search/cs?searchtype=author&query=Soni%2C+S">Shubham Soni</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+A">Adrijit Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Saptarshi Ghosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Most legal text in the Indian judiciary is written in complex English due to
historical reasons. However, only about 10% of the Indian population is
comfortable in reading English. Hence legal text needs to be made available in
various Indian languages, possibly by translating the available legal text from
English. Though there has been a lot of research on translation to and between
Indian languages, to our knowledge, there has not been much prior work on such
translation in the legal domain. In this work, we construct the first
high-quality legal parallel corpus containing aligned text units in English and
nine Indian languages, that includes several low-resource languages. We also
benchmark the performance of a wide variety of Machine Translation (MT) systems
over this corpus, including commercial MT systems, open-source MT systems and
Large Language Models. Through a comprehensive survey by Law practitioners, we
check how satisfied they are with the translations by some of these MT systems,
and how well automatic MT evaluation metrics agree with the opinions of Law
practitioners.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09767" title="Abstract">arXiv:2310.09767</a> [<a href="/pdf/2310.09767" title="Download PDF">pdf</a>, <a href="/format/2310.09767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VLIS: Unimodal Language Models Guide Multimodal Language Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+J">Jiwan Chung</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multimodal language generation, which leverages the synergy of language and
vision, is a rapidly expanding field. However, existing vision-language models
face challenges in tasks that require complex linguistic understanding. To
address this issue, we introduce Visual-Language models as Importance Sampling
weights (VLIS), a novel framework that combines the visual conditioning
capability of vision-language models with the language understanding of
unimodal text-only language models without further training. It extracts
pointwise mutual information of each image and text from a visual-language
model and uses the value as an importance sampling weight to adjust the token
likelihood from a text-only model. VLIS improves vision-language models on
diverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and
ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning,
and ROCStories). Our results suggest that VLIS represents a promising new
direction for multimodal language generation.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09769" title="Abstract">arXiv:2310.09769</a> [<a href="/pdf/2310.09769" title="Download PDF">pdf</a>, <a href="/ps/2310.09769" title="Download PostScript">ps</a>, <a href="/format/2310.09769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cell-Free Massive MIMO Surveillance Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mobini%2C+Z">Zahra Mobini</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+H+Q">Hien Quoc Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Matthaiou%2C+M">Michail Matthaiou</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted for publication in 2023 IEEE Global Communications Conference (GLOBECOM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Wireless surveillance, in which untrusted communications links are
proactively monitored by legitimate agencies, has started to garner a lot of
interest for enhancing the national security. In this paper, we propose a new
cell-free massive multiple-input multiple-output (CF-mMIMO) wireless
surveillance system, where a large number of distributed multi-antenna aided
legitimate monitoring nodes (MNs) embark on either observing or jamming
untrusted communication links. To facilitate concurrent observing and jamming,
a subset of the MNs is selected for monitoring the untrusted transmitters
(UTs), while the remaining MNs are selected for jamming the untrusted receivers
(URs). We analyze the performance of CF-mMIMO wireless surveillance and derive
a closed-form expression for the monitoring success probability of MNs. We then
propose a greedy algorithm for the observing vs, jamming mode assignment of
MNs, followed by the conception of a jamming transmit power allocation
algorithm for maximizing the minimum monitoring success probability concerning
all the UT and UR pairs based on the associated long-term channel state
information knowledge. In conclusion, our proposed CF-mMIMO system is capable
of significantly improving the performance of the MNs compared to that of the
state-of-the-art baseline. In scenarios of a mediocre number of MNs, our
proposed scheme provides an 11-fold improvement in the minimum monitoring
success probability compared to its co-located mMIMO benchmarker.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09772" title="Abstract">arXiv:2310.09772</a> [<a href="/pdf/2310.09772" title="Download PDF">pdf</a>, <a href="/format/2310.09772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Graph Meaning Representations through Decoupling Contextual  Representation Learning and Structural Information Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Li Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dingyi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Hong Qu</a>, 
<a href="/search/cs?searchtype=author&query=Hershcovich%2C+D">Daniel Hershcovich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the field of natural language understanding, the intersection of neural
models and graph meaning representations (GMRs) remains a compelling area of
research. Despite the growing interest, a critical gap persists in
understanding the exact influence of GMRs, particularly concerning relation
extraction tasks. Addressing this, we introduce DAGNN-plus, a simple and
parameter-efficient neural architecture designed to decouple contextual
representation learning from structural information propagation. Coupled with
various sequence encoders and GMRs, this architecture provides a foundation for
systematic experimentation on two English and two Chinese datasets. Our
empirical analysis utilizes four different graph formalisms and nine parsers.
The results yield a nuanced understanding of GMRs, showing improvements in
three out of the four datasets, particularly favoring English over Chinese due
to highly accurate parsers. Interestingly, GMRs appear less effective in
literary-domain datasets compared to general-domain datasets. These findings
lay the groundwork for better-informed design of GMRs and parsers to improve
relation classification, which is expected to tangibly impact the future
trajectory of natural language understanding research.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09773" title="Abstract">arXiv:2310.09773</a> [<a href="/pdf/2310.09773" title="Download PDF">pdf</a>, <a href="/format/2310.09773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RSVP: Customer Intent Detection via Agent Response Contrastive and  Generative Pre-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yu-Chien Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei-Yao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yen%2C+A">An-Zi Yen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wen-Chih Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The dialogue systems in customer services have been developed with neural
models to provide users with precise answers and round-the-clock support in
task-oriented conversations by detecting customer intents based on their
utterances. Existing intent detection approaches have highly relied on
adaptively pre-training language models with large-scale datasets, yet the
predominant cost of data collection may hinder their superiority. In addition,
they neglect the information within the conversational responses of the agents,
which have a lower collection cost, but are significant to customer intent as
agents must tailor their replies based on the customers' intent. In this paper,
we propose RSVP, a self-supervised framework dedicated to task-oriented
dialogues, which utilizes agent responses for pre-training in a two-stage
manner. Specifically, we introduce two pre-training tasks to incorporate the
relations of utterance-response pairs: 1) Response Retrieval by selecting a
correct response from a batch of candidates, and 2) Response Generation by
mimicking agents to generate the response to a given utterance. Our benchmark
results for two real-world customer service datasets show that RSVP
significantly outperforms the state-of-the-art baselines by 4.95% for accuracy,
3.4% for MRR@3, and 2.75% for MRR@5 on average. Extensive case studies are
investigated to show the validity of incorporating agent responses into the
pre-training stage.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09774" title="Abstract">arXiv:2310.09774</a> [<a href="/pdf/2310.09774" title="Download PDF">pdf</a>, <a href="/format/2310.09774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Worst-Case Analysis is Maximum-A-Posteriori Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongjun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The worst-case resource usage of a program can provide useful information for
many software-engineering tasks, such as performance optimization and
algorithmic-complexity-vulnerability discovery. This paper presents a generic,
adaptive, and sound fuzzing framework, called DSE-SMC, for estimating
worst-case resource usage. DSE-SMC is generic because it is black-box as long
as the user provides an interface for retrieving resource-usage information on
a given input; adaptive because it automatically balances between exploration
and exploitation of candidate inputs; and sound because it is guaranteed to
converge to the true resource-usage distribution of the analyzed program.
<br />DSE-SMC is built upon a key observation: resource accumulation in a program
is isomorphic to the soft-conditioning mechanism in Bayesian probabilistic
programming; thus, worst-case resource analysis is isomorphic to the
maximum-a-posteriori-estimation problem of Bayesian statistics. DSE-SMC
incorporates sequential Monte Carlo (SMC) -- a generic framework for Bayesian
inference -- with adaptive evolutionary fuzzing algorithms, in a sound manner,
i.e., DSE-SMC asymptotically converges to the posterior distribution induced by
resource-usage behavior of the analyzed program. Experimental evaluation on
Java applications demonstrates that DSE-SMC is significantly more effective
than existing black-box fuzzing methods for worst-case analysis.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09776" title="Abstract">arXiv:2310.09776</a> [<a href="/pdf/2310.09776" title="Download PDF">pdf</a>, <a href="/format/2310.09776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CBARF: Cascaded Bundle-Adjusting Neural Radiance Fields from Imperfect  Camera Poses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Hongyu Fu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lincheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing volumetric neural rendering techniques, such as Neural Radiance
Fields (NeRF), face limitations in synthesizing high-quality novel views when
the camera poses of input images are imperfect. To address this issue, we
propose a novel 3D reconstruction framework that enables simultaneous
optimization of camera poses, dubbed CBARF (Cascaded Bundle-Adjusting NeRF).In
a nutshell, our framework optimizes camera poses in a coarse-to-fine manner and
then reconstructs scenes based on the rectified poses. It is observed that the
initialization of camera poses has a significant impact on the performance of
bundle-adjustment (BA). Therefore, we cascade multiple BA modules at different
scales to progressively improve the camera poses. Meanwhile, we develop a
neighbor-replacement strategy to further optimize the results of BA in each
stage. In this step, we introduce a novel criterion to effectively identify
poorly estimated camera poses. Then we replace them with the poses of
neighboring cameras, thus further eliminating the impact of inaccurate camera
poses. Once camera poses have been optimized, we employ a density voxel grid to
generate high-quality 3D reconstructed scenes and images in novel views.
Experimental results demonstrate that our CBARF model achieves state-of-the-art
performance in both pose optimization and novel view synthesis, especially in
the existence of large camera pose noise.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09778" title="Abstract">arXiv:2310.09778</a> [<a href="/pdf/2310.09778" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Urban Big Data for Informed Business Location Decisions: A  Case Study of Starbucks in Tianhe District, Guangzhou City
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yan Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+D">Danni Chang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xuan Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">With the development of the information age, cities provide a large amount of
data that can be analyzed and utilized to facilitate the decision-making
process. Urban big data and analytics are particularly valuable in the analysis
of business location decisions, providing insight and supporting informed
choices. By examining data relating to commercial locations, it becomes
possible to analyze various spatial characteristics and derive the feasibility
of different locations. This analytical approach contributes to effective
decision-making and the formulation of robust location strategies. To
illustrate this, the study focuses on Starbucks cafes in the Tianhe District of
Guangzhou City, China. Utilizing data visualization maps, the spatial
distribution characteristics and influencing factors of Starbucks locations are
analyzed. By examining the geographical coordinates of Starbucks, main
distribution characteristics are identified. Through this analysis, it explores
the factors influencing the spatial layout of commercial store locations, using
Starbucks as a case study. The findings offer valuable insights into the
management of industrial layout and the location strategies of commercial
businesses in urban environments, opening avenues for further research and
development in this field.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09779" title="Abstract">arXiv:2310.09779</a> [<a href="/pdf/2310.09779" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Correlation between Urban Microclimate Simulation and  Urban Morphology: A Case Study in Yeongdeungpo-gu, Seoul
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yan Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+D">Danni Chang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jieli Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Different social backgrounds and planning policies give rise to diverse urban
morphologies. These morphologies influence urban microclimate factors and
contribute to the formation of unique local microclimates, particularly in
terms of outdoor temperature. In recent times, the heat island effect has
gained increasing significance during the summer season. Therefore, this study
aims to explore the correlation between urban microclimate simulation and urban
morphology within the context of the heat island effect. Specifically, we
investigate how the outside temperature varies across different types of
residential buildings in Yeongdeungpo-gu, Seoul, South Korea, during the summer
period. We compare temperature conditions using a multi-dimensional system of
building clusters' morphological indices and employ ENVI-met software for
simulation purposes. The results of the urban microclimate simulation are
comprehensively analyzed, revealing a significant finding: high-rise
residential buildings exhibit considerably higher outdoor temperatures compared
to low-rise residential buildings. Furthermore, the presence of open spaces
plays a crucial role in mitigating high neighborhood temperatures. By deriving
insights from these findings, we aim to provide valuable conclusions to support
city managers in making informed decisions.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09780" title="Abstract">arXiv:2310.09780</a> [<a href="/pdf/2310.09780" title="Download PDF">pdf</a>, <a href="/format/2310.09780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Notes on Applicability of Explainable AI Methods to Machine Learning  Models Using Features Extracted by Persistent Homology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hama%2C+N">Naofumi Hama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Data analysis that uses the output of topological data analysis as input for
machine learning algorithms has been the subject of extensive research. This
approach offers a means of capturing the global structure of data. Persistent
homology (PH), a common methodology within the field of TDA, has found
wide-ranging applications in machine learning. One of the key reasons for the
success of the PH-ML pipeline lies in the deterministic nature of feature
extraction conducted through PH. The ability to achieve satisfactory levels of
accuracy with relatively simple downstream machine learning models, when
processing these extracted features, underlines the pipeline's superior
interpretability. However, it must be noted that this interpretation has
encountered issues. Specifically, it fails to accurately reflect the feasible
parameter region in the data generation process, and the physical or chemical
constraints that restrict this process. Against this backdrop, we explore the
potential application of explainable AI methodologies to this PH-ML pipeline.
We apply this approach to the specific problem of predicting gas adsorption in
metal-organic frameworks and demonstrate that it can yield suggestive results.
The codes to reproduce our results are available at
https://github.com/naofumihama/xai_ph_ml
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09781" title="Abstract">arXiv:2310.09781</a> [<a href="/pdf/2310.09781" title="Download PDF">pdf</a>, <a href="/format/2310.09781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Negative Sampling with Adaptive Denoising Mixup for Knowledge Graph  Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangnan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhen Yao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ISWC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Knowledge graph embedding (KGE) aims to map entities and relations of a
knowledge graph (KG) into a low-dimensional and dense vector space via
contrasting the positive and negative triples. In the training process of KGEs,
negative sampling is essential to find high-quality negative triples since KGs
only contain positive triples. Most existing negative sampling methods assume
that non-existent triples with high scores are high-quality negative triples.
However, negative triples sampled by these methods are likely to contain noise.
Specifically, they ignore that non-existent triples with high scores might also
be true facts due to the incompleteness of KGs, which are usually called false
negative triples. To alleviate the above issue, we propose an easily pluggable
denoising mixup method called DeMix, which generates high-quality triples by
refining sampled negative triples in a self-supervised manner. Given a sampled
unlabeled triple, DeMix firstly classifies it into a marginal pseudo-negative
triple or a negative triple based on the judgment of the KGE model itself.
Secondly, it selects an appropriate mixup partner for the current triple to
synthesize a partially positive or a harder negative triple. Experimental
results on the knowledge graph completion task show that the proposed DeMix is
superior to other negative sampling techniques, ensuring corresponding KGEs a
faster convergence and better link prediction results.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09786" title="Abstract">arXiv:2310.09786</a> [<a href="/pdf/2310.09786" title="Download PDF">pdf</a>, <a href="/format/2310.09786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Working with XR in Public: Effects on Users and Bystanders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biener%2C+V">Verena Biener</a>, 
<a href="/search/cs?searchtype=author&query=Kalamkar%2C+S">Snehanjali Kalamkar</a>, 
<a href="/search/cs?searchtype=author&query=Dudley%2C+J+J">John J Dudley</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinghui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Kristensson%2C+P+O">Per Ola Kristensson</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+J">J&#xf6;rg M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Grubert%2C+J">Jens Grubert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Recent commercial off-the-shelf virtual and augmented reality devices have
been promoted as tools for knowledge work and research findings show how this
kind of work can benefit from the affordances of extended reality (XR). One
major advantage that XR can provide is the enlarged display space that can be
used to display virtual screens which is a feature already readily available in
many commercial devices. This could be especially helpful in mobile contexts,
in which users might not have access to their optimal physical work setup. Such
situations often occur in a public setting, for example when working on a train
while traveling to a business meeting. At the same time, the use of XR devices
is still uncommon in public, which might impact both users and bystanders.
Hence, there is a need to better understand the implications of using XR
devices for work in public both on the user itself, as well as on bystanders.
We report the results of a study in a university cafeteria in which
participants used three different systems. In one setup they only used a laptop
with a single screen, in a second setup, they combined the laptop with an
optical see-through AR headset, and in the third, they combined the laptop with
an immersive VR headset. In addition, we also collected 231 responses from
bystanders through a questionnaire. The combined results indicate that (1)
users feel safer if they can see their physical surroundings; (2) current use
of XR in public makes users stand out; and (3) prior XR experience can
influence how users feel when using XR in public.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09787" title="Abstract">arXiv:2310.09787</a> [<a href="/pdf/2310.09787" title="Download PDF">pdf</a>, <a href="/format/2310.09787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Link Prediction for New Nodes in Temporal Graph Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaobo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinhu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhanheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Ying He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Modelling temporal networks for dynamic link prediction of new nodes has many
real-world applications, such as providing relevant item recommendations to new
customers in recommender systems and suggesting appropriate posts to new users
on social platforms. Unlike old nodes, new nodes have few historical links,
which poses a challenge for the dynamic link prediction task. Most existing
dynamic models treat all nodes equally and are not specialized for new nodes,
resulting in suboptimal performances. In this paper, we consider dynamic link
prediction of new nodes as a few-shot problem and propose a novel model based
on the meta-learning principle to effectively mitigate this problem.
Specifically, we develop a temporal encoder with a node-level span memory to
obtain a new node embedding, and then we use a predictor to determine whether
the new node generates a link. To overcome the few-shot challenge, we
incorporate the encoder-predictor into the meta-learning paradigm, which can
learn two types of implicit information during the formation of the temporal
network through span adaptation and node adaptation. The acquired implicit
information can serve as model initialisation and facilitate rapid adaptation
to new nodes through a fine-tuning process on just a few links. Experiments on
three publicly available datasets demonstrate the superior performance of our
model compared to existing state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09789" title="Abstract">arXiv:2310.09789</a> [<a href="/pdf/2310.09789" title="Download PDF">pdf</a>, <a href="/format/2310.09789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLrce: Efficient Federated Learning with Relationship-based Client  Selection and Early-Stopping Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Ziru Niu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+A+K">A. Kai Qin</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+T">Tao Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arxiv preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning (FL) achieves great popularity in broad areas as a
powerful interface to offer intelligent services to customers while maintaining
data privacy. Nevertheless, FL faces communication and computation bottlenecks
due to limited bandwidth and resource constraints of edge devices. To
comprehensively address the bottlenecks, the technique of dropout is
introduced, where resource-constrained edge devices are allowed to
collaboratively train a subset of the global model parameters. However, dropout
impedes the learning efficiency of FL under unbalanced local data
distributions. As a result, FL requires more rounds to achieve appropriate
accuracy, consuming more communication and computation resources. In this
paper, we present FLrce, an efficient FL framework with a relationship-based
client selection and early-stopping strategy. FLrce accelerates the FL process
by selecting clients with more significant effects, enabling the global model
to converge to a high accuracy in fewer rounds. FLrce also leverages an early
stopping mechanism to terminate FL in advance to save communication and
computation resources. Experiment results show that FLrce increases the
communication and computation efficiency by 6% to 73.9% and 20% to 79.5%,
respectively, while maintaining competitive accuracy.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09791" title="Abstract">arXiv:2310.09791</a> [<a href="/pdf/2310.09791" title="Download PDF">pdf</a>, <a href="/format/2310.09791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto-LfD: Towards Closing the Loop for Learning from Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shaokang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yijin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yanlong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Over the past few years, there have been numerous works towards advancing the
generalization capability of robots, among which learning from demonstrations
(LfD) has drawn much attention by virtue of its user-friendly and
data-efficient nature. While many LfD solutions have been reported, a key
question has not been properly addressed: how can we evaluate the
generalization performance of LfD? For instance, when a robot draws a letter
that needs to pass through new desired points, how does it ensure the new
trajectory maintains a similar shape to the demonstration? This question
becomes more relevant when a new task is significantly far from the
demonstrated region. To tackle this issue, a user often resorts to manual
tuning of the hyperparameters of an LfD approach until a satisfactory
trajectory is attained. In this paper, we aim to provide closed-loop evaluative
feedback for LfD and optimize LfD in an automatic fashion. Specifically, we
consider dynamical movement primitives (DMP) and kernelized movement primitives
(KMP) as examples and develop a generic optimization framework capable of
measuring the generalization performance of DMP and KMP and auto-optimizing
their hyperparameters without any human inputs. Evaluations including a
peg-in-hole task and a pushing task on a real robot evidence the applicability
of our framework.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09792" title="Abstract">arXiv:2310.09792</a> [<a href="/pdf/2310.09792" title="Download PDF">pdf</a>, <a href="/format/2310.09792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCME: A Self-Contrastive Method for Data-free and Query-Limited Model  Extraction Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Renyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+K">Kwok-Yan Lam</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Previous studies have revealed that artificial intelligence (AI) systems are
vulnerable to adversarial attacks. Among them, model extraction attacks fool
the target model by generating adversarial examples on a substitute model. The
core of such an attack is training a substitute model as similar to the target
model as possible, where the simulation process can be categorized in a
data-dependent and data-free manner. Compared with the data-dependent method,
the data-free one has been proven to be more practical in the real world since
it trains the substitute model with synthesized data. However, the distribution
of these fake data lacks diversity and cannot detect the decision boundary of
the target model well, resulting in the dissatisfactory simulation effect.
Besides, these data-free techniques need a vast number of queries to train the
substitute model, increasing the time and computing consumption and the risk of
exposure. To solve the aforementioned problems, in this paper, we propose a
novel data-free model extraction method named SCME (Self-Contrastive Model
Extraction), which considers both the inter- and intra-class diversity in
synthesizing fake data. In addition, SCME introduces the Mixup operation to
augment the fake data, which can explore the target model's decision boundary
effectively and improve the simulating capacity. Extensive experiments show
that the proposed method can yield diversified fake data. Moreover, our method
has shown superiority in many different attack settings under the query-limited
scenario, especially for untargeted attacks, the SCME outperforms SOTA methods
by 11.43\% on average for five baseline datasets.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09793" title="Abstract">arXiv:2310.09793</a> [<a href="/pdf/2310.09793" title="Download PDF">pdf</a>, <a href="/format/2310.09793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Detection of Cat Facial Landmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martvel%2C+G">George Martvel</a>, 
<a href="/search/cs?searchtype=author&query=Shimshoni%2C+I">Ilan Shimshoni</a>, 
<a href="/search/cs?searchtype=author&query=Zamansky%2C+A">Anna Zamansky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2305.04232">arXiv:2305.04232</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The field of animal affective computing is rapidly emerging, and analysis of
facial expressions is a crucial aspect. One of the most significant challenges
that researchers in the field currently face is the scarcity of high-quality,
comprehensive datasets that allow the development of models for facial
expressions analysis. One of the possible approaches is the utilisation of
facial landmarks, which has been shown for humans and animals. In this paper we
present a novel dataset of cat facial images annotated with bounding boxes and
48 facial landmarks grounded in cat facial anatomy. We also introduce a
landmark detection convolution neural network-based model which uses a
magnifying ensembe method. Our model shows excellent performance on cat faces
and is generalizable to human facial landmark detection.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09795" title="Abstract">arXiv:2310.09795</a> [<a href="/pdf/2310.09795" title="Download PDF">pdf</a>, <a href="/format/2310.09795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AFLOW: Developing Adversarial Examples under Extremely Noise-limited  Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Renyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Extensive studies have demonstrated that deep neural networks (DNNs) are
vulnerable to adversarial attacks. Despite the significant progress in the
attack success rate that has been made recently, the adversarial noise
generated by most of the existing attack methods is still too conspicuous to
the human eyes and proved to be easily detected by defense mechanisms.
Resulting that these malicious examples cannot contribute to exploring the
vulnerabilities of existing DNNs sufficiently. Thus, to better reveal the
defects of DNNs and further help enhance their robustness under noise-limited
situations, a new inconspicuous adversarial examples generation method is
exactly needed to be proposed. To bridge this gap, we propose a novel Normalize
Flow-based end-to-end attack framework, called AFLOW, to synthesize
imperceptible adversarial examples under strict constraints. Specifically,
rather than the noise-adding manner, AFLOW directly perturbs the hidden
representation of the corresponding image to craft the desired adversarial
examples. Compared with existing methods, extensive experiments on three
benchmark datasets show that the adversarial examples built by AFLOW exhibit
superiority in imperceptibility, image quality and attack capability. Even on
robust models, AFLOW can still achieve higher attack results than previous
methods.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09797" title="Abstract">arXiv:2310.09797</a> [<a href="/pdf/2310.09797" title="Download PDF">pdf</a>, <a href="/format/2310.09797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Number Representation Systems Library Supporting New Representations  Based on Morris Tapered Floating-point with Hidden Exponent Bit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ciocirlan%2C+S">Stefan-Dan Ciocirlan</a>, 
<a href="/search/cs?searchtype=author&query=Loghin%2C+D">Dumitrel Loghin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The introduction of posit reopened the debate about the utility of IEEE754 in
specific domains. In this context, we propose a high-level language (Scala)
library that aims to reduce the effort of designing and testing new number
representation systems (NRSs). The library's efficiency is tested with three
new NRSs derived from Morris Tapered Floating-Point by adding a hidden exponent
bit. We call these NRSs MorrisHEB, MorrisBiasHEB, and MorrisUnaryHEB,
respectively. We show that they offer a better dynamic range, better decimal
accuracy for unary operations, more exact results for addition (37.61% in the
case of MorrisUnaryHEB), and better average decimal accuracy for inexact
results on binary operations than posit and IEEE754. Going through existing
benchmarks in the literature, and favorable/unfavorable examples for
IEEE754/posit, we show that these new NRSs produce similar (less than one
decimal accuracy difference) or even better results than IEEE754 and posit.
Given the entire spectrum of results, there are arguments for MorrisBiasHEB to
be used as a replacement for IEEE754 in general computations. MorrisUnaryHEB
has a more populated ``golden zone'' (+13.6%) and a better dynamic range (149X)
than posit, making it a candidate for machine learning computations.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09800" title="Abstract">arXiv:2310.09800</a> [<a href="/pdf/2310.09800" title="Download PDF">pdf</a>, <a href="/format/2310.09800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Inversion Attacks on Homogeneous and Heterogeneous Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Renyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+P">Peiyuan Si</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recently, Graph Neural Networks (GNNs), including Homogeneous Graph Neural
Networks (HomoGNNs) and Heterogeneous Graph Neural Networks (HeteGNNs), have
made remarkable progress in many physical scenarios, especially in
communication applications. Despite achieving great success, the privacy issue
of such models has also received considerable attention. Previous studies have
shown that given a well-fitted target GNN, the attacker can reconstruct the
sensitive training graph of this model via model inversion attacks, leading to
significant privacy worries for the AI service provider. We advocate that the
vulnerability comes from the target GNN itself and the prior knowledge about
the shared properties in real-world graphs. Inspired by this, we propose a
novel model inversion attack method on HomoGNNs and HeteGNNs, namely HomoGMI
and HeteGMI. Specifically, HomoGMI and HeteGMI are gradient-descent-based
optimization methods that aim to maximize the cross-entropy loss on the target
GNN and the $1^{st}$ and $2^{nd}$-order proximities on the reconstructed graph.
Notably, to the best of our knowledge, HeteGMI is the first attempt to perform
model inversion attacks on HeteGNNs. Extensive experiments on multiple
benchmarks demonstrate that the proposed method can achieve better performance
than the competitors.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09802" title="Abstract">arXiv:2310.09802</a> [<a href="/pdf/2310.09802" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploitation Business: Leveraging Information Asymmetry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+K">Kwangseob Ahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, No figure, looking for publication journal, independent researcher
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">This paper investigates the "Exploitation Business" model, which capitalizes
on information asymmetry to exploit vulnerable populations. It focuses on
businesses targeting non-experts or fraudsters who capitalize on information
asymmetry to sell their products or services to desperate individuals. This
phenomenon is also described as "profit-making activities based on
informational exploitation," which thrives on individuals' limited access to
information, lack of expertise, and Fear of Missing Out (FOMO).
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09806" title="Abstract">arXiv:2310.09806</a> [<a href="/pdf/2310.09806" title="Download PDF">pdf</a>, <a href="/format/2310.09806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LSH (Locality-Sensitive Hashing) Be Replaced by Neural Network?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Renyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xing Chu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jing He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">With the rapid development of GPU (Graphics Processing Unit) technologies and
neural networks, we can explore more appropriate data structures and
algorithms. Recent progress shows that neural networks can partly replace
traditional data structures. In this paper, we proposed a novel DNN (Deep
Neural Network)-based learned locality-sensitive hashing, called LLSH, to
efficiently and flexibly map high-dimensional data to low-dimensional space.
LLSH replaces the traditional LSH (Locality-sensitive Hashing) function
families with parallel multi-layer neural networks, which reduces the time and
memory consumption and guarantees query accuracy simultaneously. The proposed
LLSH demonstrate the feasibility of replacing the hash index with
learning-based neural networks and open a new door for developers to design and
configure data organization more accurately to improve information-searching
performance. Extensive experiments on different types of datasets show the
superiority of the proposed method in query accuracy, time consumption, and
memory usage.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09810" title="Abstract">arXiv:2310.09810</a> [<a href="/pdf/2310.09810" title="Download PDF">pdf</a>, <a href="/format/2310.09810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT for Vulnerability Detection, Classification, and Repair: How Far  Are We?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+M">Michael Fu</a>, 
<a href="/search/cs?searchtype=author&query=Tantithamthavorn%2C+C">Chakkrit Tantithamthavorn</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V">Van Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Trung Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 30th Asia-Pacific Software Engineering Conference (APSEC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large language models (LLMs) like ChatGPT (i.e., gpt-3.5-turbo and gpt-4)
exhibited remarkable advancement in a range of software engineering tasks
associated with source code such as code review and code generation. In this
paper, we undertake a comprehensive study by instructing ChatGPT for four
prevalent vulnerability tasks: function and line-level vulnerability
prediction, vulnerability classification, severity estimation, and
vulnerability repair. We compare ChatGPT with state-of-the-art language models
designed for software vulnerability purposes. Through an empirical assessment
employing extensive real-world datasets featuring over 190,000 C/C++ functions,
we found that ChatGPT achieves limited performance, trailing behind other
language models in vulnerability contexts by a significant margin. The
experimental outcomes highlight the challenging nature of vulnerability
prediction tasks, requiring domain-specific expertise. Despite ChatGPT's
substantial model scale, exceeding that of source code-pre-trained language
models (e.g., CodeBERT) by a factor of 14,000, the process of fine-tuning
remains imperative for ChatGPT to generalize for vulnerability prediction
tasks. We publish the studied dataset, experimental prompts for ChatGPT, and
experimental results at https://github.com/awsm-research/ChatGPT4Vul.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09813" title="Abstract">arXiv:2310.09813</a> [<a href="/pdf/2310.09813" title="Download PDF">pdf</a>, <a href="/format/2310.09813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parking Problem by Oblivious Mobile Robots in Infinite Grids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+A">Abhinav Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyaya%2C+K">Krishnendu Mukhopadhyaya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In this paper, the parking problem of a swarm of mobile robots has been
studied. The robots are deployed at the nodes of an infinite grid, which has a
subset of prefixed nodes marked as parking nodes. Each parking node p_i has a
capacity of k_i which is given as input and equals the maximum number of robots
a parking node can accommodate. As a solution to the parking problem, robots
need to partition themselves into groups so that each parking node contains a
number of robots that are equal to the capacity of the node in the final
configuration. It is assumed that the number of robots in the initial
configuration represents the sum of the capacities of the parking nodes. The
robots are assumed to be autonomous, anonymous, homogeneous, identical and
oblivious. They operate under an asynchronous scheduler. They neither have any
agreement on the coordinate axes nor do they agree on a common chirality. All
the initial configurations for which the problem is unsolvable have been
identified. A deterministic distributed algorithm has been proposed for the
remaining configurations, ensuring the solvability of the problem.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09817" title="Abstract">arXiv:2310.09817</a> [<a href="/pdf/2310.09817" title="Download PDF">pdf</a>, <a href="/format/2310.09817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OAAFormer: Robust and Efficient Point Cloud Registration Through  Overlapping-Aware Attention in Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Junjie Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qiujie Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuangmin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+S">Shiqing Xin</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+C">Changhe Tu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the domain of point cloud registration, the coarse-to-fine feature
matching paradigm has received substantial attention owing to its impressive
performance. This paradigm involves a two-step process: first, the extraction
of multi-level features, and subsequently, the propagation of correspondences
from coarse to fine levels. Nonetheless, this paradigm exhibits two notable
limitations.Firstly, the utilization of the Dual Softmax operation has the
potential to promote one-to-one correspondences between superpoints,
inadvertently excluding valuable correspondences. This propensity arises from
the fact that a source superpoint typically maintains associations with
multiple target superpoints. Secondly, it is imperative to closely examine the
overlapping areas between point clouds, as only correspondences within these
regions decisively determine the actual transformation. Based on these
considerations, we propose {\em OAAFormer} to enhance correspondence quality.
On one hand, we introduce a soft matching mechanism, facilitating the
propagation of potentially valuable correspondences from coarse to fine levels.
Additionally, we integrate an overlapping region detection module to minimize
mismatches to the greatest extent possible. Furthermore, we introduce a
region-wise attention module with linear complexity during the fine-level
matching phase, designed to enhance the discriminative capabilities of the
extracted features. Tests on the challenging 3DLoMatch benchmark demonstrate
that our approach leads to a substantial increase of about 7\% in the inlier
ratio, as well as an enhancement of 2-4\% in registration recall. =
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09819" title="Abstract">arXiv:2310.09819</a> [<a href="/pdf/2310.09819" title="Download PDF">pdf</a>, <a href="/format/2310.09819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing K-means for Big Data: A Comparative Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mussabayev%2C+R">Ravil Mussabayev</a>, 
<a href="/search/cs?searchtype=author&query=Mussabayev%2C+R">Rustam Mussabayev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper presents a comparative analysis of different optimization
techniques for the K-means algorithm in the context of big data. K-means is a
widely used clustering algorithm, but it can suffer from scalability issues
when dealing with large datasets. The paper explores different approaches to
overcome these issues, including parallelization, approximation, and sampling
methods. The authors evaluate the performance of these techniques on various
benchmark datasets and compare them in terms of speed, quality of clustering,
and scalability according to the LIMA dominance criterion. The results show
that different techniques are more suitable for different types of datasets and
provide insights into the trade-offs between speed and accuracy in K-means
clustering for big data. Overall, the paper offers a comprehensive guide for
practitioners and researchers on how to optimize K-means for big data
applications.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09820" title="Abstract">arXiv:2310.09820</a> [<a href="/pdf/2310.09820" title="Download PDF">pdf</a>, <a href="/format/2310.09820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Reliability of Large Language Model Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weixuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Haddow%2C+B">Barry Haddow</a>, 
<a href="/search/cs?searchtype=author&query=Birch%2C+A">Alexandra Birch</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wei Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have been treated as knowledge bases due to
their strong performance in knowledge probing tasks. LLMs are typically
evaluated using accuracy, yet this metric does not capture the vulnerability of
LLMs to hallucination-inducing factors like prompt and context variability. How
do we evaluate the capabilities of LLMs to consistently produce factually
correct answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe
(MONITOR), a novel metric designed to directly measure LLMs' factual
reliability. MONITOR computes the distance between the probability
distributions of a valid output and its counterparts produced by the same LLM
probing the same fact using different styles of prompts and
contexts.Experiments on a comprehensive range of 12 LLMs demonstrate the
effectiveness of MONITOR in evaluating the factual reliability of LLMs while
maintaining a low computational overhead. In addition, we release the FKTC
(Factual Knowledge Test Corpus) test set, containing 210,158 prompts in total
to foster research along this line (https://github.com/Vicky-Wil/MONITOR).
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09821" title="Abstract">arXiv:2310.09821</a> [<a href="/pdf/2310.09821" title="Download PDF">pdf</a>, <a href="/format/2310.09821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LICO: Explainable Models with Language-Image Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yiming Lei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zilong Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+H">Hongming Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Interpreting the decisions of deep learning models has been actively studied
since the explosion of deep neural networks. One of the most convincing
interpretation approaches is salience-based visual interpretation, such as
Grad-CAM, where the generation of attention maps depends merely on categorical
labels. Although existing interpretation methods can provide explainable
decision clues, they often yield partial correspondence between image and
saliency maps due to the limited discriminative information from one-hot
labels. This paper develops a Language-Image COnsistency model for explainable
image classification, termed LICO, by correlating learnable linguistic prompts
with corresponding visual features in a coarse-to-fine manner. Specifically, we
first establish a coarse global manifold structure alignment by minimizing the
distance between the distributions of image and language features. We then
achieve fine-grained saliency maps by applying optimal transport (OT) theory to
assign local feature maps with class-specific prompts. Extensive experimental
results on eight benchmark datasets demonstrate that the proposed LICO achieves
a significant improvement in generating more explainable attention maps in
conjunction with existing interpretation methods such as Grad-CAM. Remarkably,
LICO improves the classification performance of existing models without
introducing any computational overhead during inference. Source code is made
available at https://github.com/ymLeiFDU/LICO.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09822" title="Abstract">arXiv:2310.09822</a> [<a href="/pdf/2310.09822" title="Download PDF">pdf</a>, <a href="/format/2310.09822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Turn Passive to Active: A Survey on Active Intellectual Property  Protection of Deep Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Mingfu Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L+Y">Leo Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yushu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiqiang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The intellectual property protection of deep learning (DL) models has
attracted increasing serious concerns. Many works on intellectual property
protection for Deep Neural Networks (DNN) models have been proposed. The vast
majority of existing work uses DNN watermarking to verify the ownership of the
model after piracy occurs, which is referred to as passive verification. On the
contrary, we focus on a new type of intellectual property protection method
named active copyright protection, which refers to active authorization control
and user identity management of the DNN model. As of now, there is relatively
limited research in the field of active DNN copyright protection. In this
review, we attempt to clearly elaborate on the connotation, attributes, and
requirements of active DNN copyright protection, provide evaluation methods and
metrics for active copyright protection, review and analyze existing work on
active DL model intellectual property protection, discuss potential attacks
that active DL model copyright protection techniques may face, and provide
challenges and future directions for active DL model intellectual property
protection. This review is helpful to systematically introduce the new field of
active DNN copyright protection and provide reference and foundation for
subsequent work.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09824" title="Abstract">arXiv:2310.09824</a> [<a href="/pdf/2310.09824" title="Download PDF">pdf</a>, <a href="/format/2310.09824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overconstrained Robotic Limb with Energy-Efficient, Omni-directional  Locomotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ronghan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jiayi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shihao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Bangchao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haoran Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jia Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chaoyang Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 13 figures, 2 tables, submitted to IEEE TRO
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper studies the design, modeling, and control of a novel quadruped,
featuring overconstrained robotic limbs employing the Bennett linkage for
motion and power transmission. The modular limb design allows the robot to
morph into reptile- or mammal-inspired forms. In contrast to the prevailing
focus on planar limbs, this research delves into the classical overconstrained
linkages, which have strong theoretical foundations in advanced kinematics but
limited engineering applications. The study showcases the morphological
superiority of overconstrained robotic limbs that can transform into planar or
spherical limbs, exemplifying the Bennett linkage. By conducting kinematic and
dynamic modeling, we apply model predictive control to simulate a range of
locomotion tasks, revealing that overconstrained limbs outperform planar
designs in omni-directional tasks like forward trotting, lateral trotting, and
turning on the spot when considering foothold distances. These findings
highlight the biological distinctions in limb design between reptiles and
mammals and represent the first documented instance of overconstrained robotic
limbs outperforming planar designs in dynamic locomotion.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09827" title="Abstract">arXiv:2310.09827</a> [<a href="/pdf/2310.09827" title="Download PDF">pdf</a>, <a href="/format/2310.09827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VFLAIR: A Research Library and Benchmark for Vertical Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+T">Tianyuan Zou</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Z">Zixuan Gu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yu He</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+H">Hideaki Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+G">Guangnan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya-Qin Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 12 figures, 12 tabels
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Vertical Federated Learning (VFL) has emerged as a collaborative training
paradigm that allows participants with different features of the same group of
users to accomplish cooperative training without exposing their raw data or
model parameters. VFL has gained significant attention for its research
potential and real-world applications in recent years, but still faces
substantial challenges, such as in defending various kinds of data inference
and backdoor attacks. Moreover, most of existing VFL projects are
industry-facing and not easily used for keeping track of the current research
progress. To address this need, we present an extensible and lightweight VFL
framework VFLAIR (available at https://github.com/FLAIR-THU/VFLAIR), which
supports VFL training with a variety of models, datasets and protocols, along
with standardized modules for comprehensive evaluations of attacks and defense
strategies. We also benchmark 11 attacks and 8 defenses performance under
different communication and model partition settings and draw concrete insights
and recommendations on the choice of defense strategies for different practical
VFL deployment scenario.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09828" title="Abstract">arXiv:2310.09828</a> [<a href="/pdf/2310.09828" title="Download PDF">pdf</a>, <a href="/format/2310.09828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Top-K Pooling with Patch Contrastive Learning for Weakly-Supervised  Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wangyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+T">Tianhong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaowei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jimin Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels
has gained significant attention due to cost-effectiveness. Recently, Vision
Transformer (ViT) based methods without class activation map (CAM) have shown
greater capability in generating reliable pseudo labels than previous methods
using CAM. However, the current ViT-based methods utilize max pooling to select
the patch with the highest prediction score to map the patch-level
classification to the image-level one, which may affect the quality of pseudo
labels due to the inaccurate classification of the patches. In this paper, we
introduce a novel ViT-based WSSS method named top-K pooling with patch
contrastive learning (TKP-PCL), which employs a top-K pooling layer to
alleviate the limitations of previous max pooling selection. A patch
contrastive error (PCE) is also proposed to enhance the patch embeddings to
further improve the final results. The experimental results show that our
approach is very efficient and outperforms other state-of-the-art WSSS methods
on the PASCAL VOC 2012 dataset.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09831" title="Abstract">arXiv:2310.09831</a> [<a href="/pdf/2310.09831" title="Download PDF">pdf</a>, <a href="/format/2310.09831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAGIC: Detecting Advanced Persistent Threats via Masked Graph  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+Z">Zian Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yun Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+Y">Yuhong Nan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinjing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+M">Mi Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 13 figures, to appear in the 33rd USENIX Security Symposium (USENIX Security 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Advance Persistent Threats (APTs), adopted by most delicate attackers, are
becoming increasing common and pose great threat to various enterprises and
institutions. Data provenance analysis on provenance graphs has emerged as a
common approach in APT detection. However, previous works have exhibited
several shortcomings: (1) requiring attack-containing data and a priori
knowledge of APTs, (2) failing in extracting the rich contextual information
buried within provenance graphs and (3) becoming impracticable due to their
prohibitive computation overhead and memory consumption.
<br />In this paper, we introduce MAGIC, a novel and flexible self-supervised APT
detection approach capable of performing multi-granularity detection under
different level of supervision. MAGIC leverages masked graph representation
learning to model benign system entities and behaviors, performing efficient
deep feature extraction and structure abstraction on provenance graphs. By
ferreting out anomalous system behaviors via outlier detection methods, MAGIC
is able to perform both system entity level and batched log level APT
detection. MAGIC is specially designed to handle concept drift with a model
adaption mechanism and successfully applies to universal conditions and
detection scenarios. We evaluate MAGIC on three widely-used datasets, including
both real-world and simulated attacks. Evaluation results indicate that MAGIC
achieves promising detection results in all scenarios and shows enormous
advantage over state-of-the-art APT detection approaches in performance
overhead.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09832" title="Abstract">arXiv:2310.09832</a> [<a href="/pdf/2310.09832" title="Download PDF">pdf</a>, <a href="/format/2310.09832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Merging Experts into One: Improving Computational Efficiency of Mixture  of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shwai He</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+R">Run-Ze Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Scaling the size of language models usually leads to remarkable advancements
in NLP tasks. But it often comes with a price of growing computational cost.
Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a
small subset of parameters (e.g., one expert) for each input, its computation
escalates significantly if increasing the number of activated experts, limiting
its practical utility. Can we retain the advantages of adding more experts
without substantially increasing the computational costs? In this paper, we
first demonstrate the superiority of selecting multiple experts and then
propose a computation-efficient approach called \textbf{\texttt{Merging Experts
into One}} (MEO), which reduces the computation cost to that of a single
expert. Extensive experiments show that MEO significantly improves
computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G
(MEO). Moreover, we propose a token-level attention block that further enhances
the efficiency and performance of token-level MEO, e.g., 83.3\% (MEO) vs.
82.6\% (vanilla MoE) average score on the GLUE benchmark. Our code will be
released upon acceptance. Code will be released at:
\url{https://github.com/Shwai-He/MEO}.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09833" title="Abstract">arXiv:2310.09833</a> [<a href="/pdf/2310.09833" title="Download PDF">pdf</a>, <a href="/format/2310.09833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by  Mutual Information Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Simin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruixiao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+P">Pu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiakai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aishan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+W">Weifeng Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Robust multi-agent reinforcement learning (MARL) necessitates resilience to
uncertain or worst-case actions by unknown allies. Existing max-min
optimization techniques in robust MARL seek to enhance resilience by training
agents against worst-case adversaries, but this becomes intractable as the
number of agents grows, leading to exponentially increasing worst-case
scenarios. Attempts to simplify this complexity often yield overly pessimistic
policies, inadequate robustness across scenarios and high computational
demands. Unlike these approaches, humans naturally learn adaptive and resilient
behaviors without the necessity of preparing for every conceivable worst-case
scenario. Motivated by this, we propose MIR2, which trains policy in routine
scenarios and minimize Mutual Information as Robust Regularization.
Theoretically, we frame robustness as an inference problem and prove that
minimizing mutual information between histories and actions implicitly
maximizes a lower bound on robustness under certain assumptions. Further
analysis reveals that our proposed approach prevents agents from overreacting
to others through an information bottleneck and aligns the policy with a robust
action prior. Empirically, our MIR2 displays even greater resilience against
worst-case adversaries than max-min optimization in StarCraft II, Multi-agent
Mujoco and rendezvous. Our superiority is consistent when deployed in
challenging real-world robot swarm control scenario. See code and demo videos
in Supplementary Materials.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09834" title="Abstract">arXiv:2310.09834</a> [<a href="/pdf/2310.09834" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Flow Recovery from Packet Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kenyon%2C+A">Anthony Kenyon</a>, 
<a href="/search/cs?searchtype=author&query=Elizondo%2C+D">David Elizondo</a>, 
<a href="/search/cs?searchtype=author&query=Deka%2C+L">Lipika Deka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, Submitting to Elsevier's Computer Networks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Typical event datasets such as those used in network intrusion detection
comprise hundreds of thousands, sometimes millions, of discrete packet events.
These datasets tend to be high dimensional, stateful, and time-series in
nature, holding complex local and temporal feature associations. Packet data
can be abstracted into lower dimensional summary data, such as packet flow
records, where some of the temporal complexities of packet data can be
mitigated, and smaller well-engineered feature subsets can be created. This
data can be invaluable as training data for machine learning and cyber threat
detection techniques. Data can be collected in real-time, or from historical
packet trace archives. In this paper we focus on how flow records and summary
metadata can be extracted from packet data with high accuracy and robustness.
We identify limitations in current methods, how they may impact datasets, and
how these flaws may impact learning models. Finally, we propose methods to
improve the state of the art and introduce proof of concept tools to support
this work.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09835" title="Abstract">arXiv:2310.09835</a> [<a href="/pdf/2310.09835" title="Download PDF">pdf</a>, <a href="/format/2310.09835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure and Robust Communications for Cislunar Space Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cetin%2C+S+G">Selen Gecgel Cetin</a>, 
<a href="/search/math?searchtype=author&query=Kurt%2C+G+K">Gunes Karabulut Kurt</a>, 
<a href="/search/math?searchtype=author&query=Vazquez-Castro%2C+A">Angeles Vazquez-Castro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">There is no doubt that the Moon has become the center of interest for
commercial and international actors. Over the past decade, the number of
planned long-term missions has increased dramatically. This makes the
establishment of cislunar space networks (CSNs) crucial to orchestrate
uninterrupted communications between the Moon and Earth. However, there are
numerous challenges, unknowns, and uncertainties associated with cislunar
communications that may pose various risks to lunar missions. In this study, we
aim to address these challenges for cislunar communications by proposing a
machine learning-based cislunar space domain awareness (SDA) capability that
enables robust and secure communications. To this end, we first propose a
detailed channel model for selected cislunar scenarios. Secondly, we propose
two types of interference that could model anomalies that occur in cislunar
space and are so far known only to a limited extent. Finally, we discuss our
cislunar SDA to work in conjunction with the spacecraft communication system.
Our proposed cislunar SDA, involving heuristic learning capabilities with
machine learning algorithms, detects interference models with over 96%
accuracy. The results demonstrate the promising performance of our cislunar SDA
approach for secure and robust cislunar communication.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09838" title="Abstract">arXiv:2310.09838</a> [<a href="/pdf/2310.09838" title="Download PDF">pdf</a>, <a href="/format/2310.09838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining How a Neural Network Play the Go Game and Let People Learn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huilin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Huijie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Quanshi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The AI model has surpassed human players in the game of Go, and it is widely
believed that the AI model has encoded new knowledge about the Go game beyond
human players. In this way, explaining the knowledge encoded by the AI model
and using it to teach human players represent a promising-yet-challenging issue
in explainable AI. To this end, mathematical supports are required to ensure
that human players can learn accurate and verifiable knowledge, rather than
specious intuitive analysis. Thus, in this paper, we extract interaction
primitives between stones encoded by the value network for the Go game, so as
to enable people to learn from the value network. Experiments show the
effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09843" title="Abstract">arXiv:2310.09843</a> [<a href="/pdf/2310.09843" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoCoFormer: A controllable feature-rich polyphonic music generation  method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiuyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+T">Tengfei Niu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingping Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper explores the modeling method of polyphonic music sequence. Due to
the great potential of Transformer models in music generation, controllable
music generation is receiving more attention. In the task of polyphonic music,
current controllable generation research focuses on controlling the generation
of chords, but lacks precise adjustment for the controllable generation of
choral music textures. This paper proposed Condition Choir Transformer
(CoCoFormer) which controls the output of the model by controlling the chord
and rhythm inputs at a fine-grained level. In this paper, the self-supervised
method improves the loss function and performs joint training through
conditional control input and unconditional input training. In order to
alleviate the lack of diversity on generated samples caused by the teacher
forcing training, this paper added an adversarial training method. CoCoFormer
enhances model performance with explicit and implicit inputs to chords and
rhythms. In this paper, the experiments proves that CoCoFormer has reached the
current better level than current models. On the premise of specifying the
polyphonic music texture, the same melody can also be generated in a variety of
ways.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09846" title="Abstract">arXiv:2310.09846</a> [<a href="/pdf/2310.09846" title="Download PDF">pdf</a>, <a href="/format/2310.09846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing Few-Shot Named Entity Recognizers to Unseen Domains with  Type-Related Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziqi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhumin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+P">Pengjie Ren</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Few-shot named entity recognition (NER) has shown remarkable progress in
identifying entities in low-resource domains. However, few-shot NER methods
still struggle with out-of-domain (OOD) examples due to their reliance on
manual labeling for the target domain. To address this limitation, recent
studies enable generalization to an unseen target domain with only a few
labeled examples using data augmentation techniques. Two important challenges
remain: First, augmentation is limited to the training data, resulting in
minimal overlap between the generated data and OOD examples. Second, knowledge
transfer is implicit and insufficient, severely hindering model
generalizability and the integration of knowledge from the source domain. In
this paper, we propose a framework, prompt learning with type-related features
(PLTR), to address these challenges. To identify useful knowledge in the source
domain and enhance knowledge transfer, PLTR automatically extracts entity
type-related features (TRFs) based on mutual information criteria. To bridge
the gap between training and OOD data, PLTR generates a unique prompt for each
unseen example by selecting relevant TRFs. We show that PLTR achieves
significant performance improvements on in-domain and cross-domain datasets.
The use of PLTR facilitates model adaptation and increases representation
similarities between the source and unseen domains.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09847" title="Abstract">arXiv:2310.09847</a> [<a href="/pdf/2310.09847" title="Download PDF">pdf</a>, <a href="/format/2310.09847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XRMDN: A Recurrent Mixture Density Networks-based Architecture for  Short-Term Probabilistic Demand Forecasting in Mobility-on-Demand Systems  with High Volatility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoming Li</a>, 
<a href="/search/cs?searchtype=author&query=Normandin-Taillon%2C+H">Hubert Normandin-Taillon</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 14 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In real Mobility-on-Demand (MoD) systems, demand is subject to high and
dynamic volatility, which is difficult to predict by conventional time-series
forecasting approaches. Most existing forecasting approaches yield the point
value as the prediction result, which ignores the uncertainty that exists in
the forecasting result. This will lead to the forecasting result severely
deviating from the true demand value due to the high volatility existing in
demand. To fill the gap, we propose an extended recurrent mixture density
network (XRMDN), which extends the weight and mean neural networks to recurrent
neural networks. The recurrent neurons for mean and variance can capture the
trend of the historical data-series data, which enables a better forecasting
result in dynamic and high volatility. We conduct comprehensive experiments on
one taxi trip record and one bike-sharing real MoD data set to validate the
performance of XRMDN. Specifically, we compare our model to three types of
benchmark models, including statistical, machine learning, and deep learning
models on three evaluation metrics. The validation results show that XRMDN
outperforms the three groups of benchmark models in terms of the evaluation
metrics. Most importantly, XRMDN substantially improves the forecasting
accuracy with the demands in strong volatility. Last but not least, this
probabilistic demand forecasting model contributes not only to the demand
prediction in MoD systems but also to other optimization application problems,
especially optimization under uncertainty, in MoD applications.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09848" title="Abstract">arXiv:2310.09848</a> [<a href="/pdf/2310.09848" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Stance Classification with Quantified Moral Foundations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+P">Prasanta Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+L+Z">Liang Ze Wong</a>, 
<a href="/search/cs?searchtype=author&query=Loh%2C+B+S">Brandon Siyuan Loh</a>, 
<a href="/search/cs?searchtype=author&query=Simons%2C+J+J+P">Joseph J. P. Simons</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+J">Jisun An</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study enhances stance detection on social media by incorporating deeper
psychological attributes, specifically individuals' moral foundations. These
theoretically-derived dimensions aim to provide a comprehensive profile of an
individual's moral concerns which, in recent work, has been linked to behaviour
in a range of domains, including society, politics, health, and the
environment. In this paper, we investigate how moral foundation dimensions can
contribute to predicting an individual's stance on a given target. Specifically
we incorporate moral foundation features extracted from text, along with
message semantic features, to classify stances at both message- and user-levels
across a range of targets and models. Our preliminary results suggest that
encoding moral foundations can enhance the performance of stance detection
tasks and help illuminate the associations between specific moral foundations
and online stances on target topics. The results highlight the importance of
considering deeper psychological attributes in stance analysis and underscores
the role of moral foundations in guiding online social behavior.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09852" title="Abstract">arXiv:2310.09852</a> [<a href="/pdf/2310.09852" title="Download PDF">pdf</a>, <a href="/format/2310.09852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alpha Elimination: Using Deep Reinforcement Learning to Reduce Fill-In  during Sparse Matrix Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+A">Arpan Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Pawan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to ECML 2023, Research Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A large number of computational and scientific methods commonly require
decomposing a sparse matrix into triangular factors as LU decomposition. A
common problem faced during this decomposition is that even though the given
matrix may be very sparse, the decomposition may lead to a denser triangular
factors due to fill-in. A significant fill-in may lead to prohibitively larger
computational costs and memory requirement during decomposition as well as
during the solve phase. To this end, several heuristic sparse matrix reordering
methods have been proposed to reduce fill-in before the decomposition. However,
finding an optimal reordering algorithm that leads to minimal fill-in during
such decomposition is known to be a NP-hard problem. A reinforcement learning
based approach is proposed for this problem. The sparse matrix reordering
problem is formulated as a single player game. More specifically, Monte-Carlo
tree search in combination with neural network is used as a decision making
algorithm to search for the best move in our game. The proposed method,
alphaElimination is found to produce significantly lesser non-zeros in the LU
decomposition as compared to existing state-of-the-art heuristic algorithms
with little to no increase in overall running time of the algorithm. The code
for the project will be publicly available
here\footnote{\url{https://github.com/misterpawan/alphaEliminationPaper}}.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09853" title="Abstract">arXiv:2310.09853</a> [<a href="/pdf/2310.09853" title="Download PDF">pdf</a>, <a href="/format/2310.09853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MERTech: Instrument Playing Technique Detection Using Self-Supervised  Pretrained Model With Multi-Task Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dichucheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yinghao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Weixing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Q">Qiuqiang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yulun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+M">Mingjin Che</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Benetos%2C+E">Emmanouil Benetos</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Instrument playing techniques (IPTs) constitute a pivotal component of
musical expression. However, the development of automatic IPT detection methods
suffers from limited labeled data and inherent class imbalance issues. In this
paper, we propose to apply a self-supervised learning model pre-trained on
large-scale unlabeled music data and finetune it on IPT detection tasks. This
approach addresses data scarcity and class imbalance challenges. Recognizing
the significance of pitch in capturing the nuances of IPTs and the importance
of onset in locating IPT events, we investigate multi-task finetuning with
pitch and onset detection as auxiliary tasks. Additionally, we apply a
post-processing approach for event-level prediction, where an IPT activation
initiates an event only if the onset output confirms an onset in that frame.
Our method outperforms prior approaches in both frame-level and event-level
metrics across multiple IPT benchmark datasets. Further experiments demonstrate
the efficacy of multi-task finetuning on each IPT class.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09854" title="Abstract">arXiv:2310.09854</a> [<a href="/pdf/2310.09854" title="Download PDF">pdf</a>, <a href="/ps/2310.09854" title="Download PostScript">ps</a>, <a href="/format/2310.09854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> let data talk: data-regularized operator learning theory for inverse  problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+K">Ke Chen</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+C">Chunmei Wang</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+H">Haizhao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Regularization plays a pivotal role in integrating prior information into
inverse problems. While many deep learning methods have been proposed to solve
inverse problems, determining where to apply regularization remains a crucial
consideration. Typical methods regularize neural networks via architecture,
wherein neural network functions parametrize the parameter of interest or the
regularization term. We introduce a novel approach, denoted as the
"data-regularized operator learning" (DaROL) method, designed to address PDE
inverse problems. The DaROL method trains a neural network on data, regularized
through common techniques such as Tikhonov variational methods and Bayesian
inference. The DaROL method offers flexibility across different frameworks,
faster inverse problem-solving, and a simpler structure that separates
regularization and neural network training. We demonstrate that training a
neural network on the regularized data is equivalent to supervised learning for
a regularized inverse map. Furthermore, we provide sufficient conditions for
the smoothness of such a regularized inverse map and estimate the learning
error in terms of neural network size and the number of training samples.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09856" title="Abstract">arXiv:2310.09856</a> [<a href="/pdf/2310.09856" title="Download PDF">pdf</a>, <a href="/format/2310.09856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pseudo-differential integral autoencoder network for inverse PDE  operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+K">Ke Chen</a>, 
<a href="/search/math?searchtype=author&query=Lai%2C+J">Jasen Lai</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+C">Chunmei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Partial differential equations (PDEs) play a foundational role in modeling
physical phenomena. This study addresses the challenging task of determining
variable coefficients within PDEs from measurement data. We introduce a novel
neural network, "pseudo-differential IAEnet" (pd-IAEnet), which draws
inspiration from pseudo-differential operators. pd-IAEnet achieves
significantly enhanced computational speed and accuracy with fewer parameters
compared to conventional models. Extensive benchmark evaluations are conducted
across a range of inverse problems, including Electrical Impedance Tomography
(EIT), optical tomography, and seismic imaging, consistently demonstrating
pd-IAEnet's superior accuracy. Notably, pd-IAEnet exhibits robustness in the
presence of measurement noise, a critical characteristic for real-world
applications. An exceptional feature is its discretization invariance, enabling
effective training on data from diverse discretization schemes while
maintaining accuracy on different meshes. In summary, pd-IAEnet offers a potent
and efficient solution for addressing inverse PDE problems, contributing to
improved computational efficiency, robustness, and adaptability to a wide array
of data sources.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09858" title="Abstract">arXiv:2310.09858</a> [<a href="/pdf/2310.09858" title="Download PDF">pdf</a>, <a href="/format/2310.09858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Reinforcement Learning for Resource Allocation in V2X Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kaidi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shenglong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G+Y">Geoffrey Ye Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to TWC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Resource allocation significantly impacts the performance of
vehicle-to-everything (V2X) networks. Most existing algorithms for resource
allocation are based on optimization or machine learning (e.g., reinforcement
learning). In this paper, we explore resource allocation in a V2X network under
the framework of federated reinforcement learning (FRL). On one hand, the usage
of RL overcomes many challenges from the model-based optimization schemes. On
the other hand, federated learning (FL) enables agents to deal with a number of
practical issues, such as privacy, communication overhead, and exploration
efficiency. The framework of FRL is then implemented by the inexact alternative
direction method of multipliers (ADMM), where subproblems are solved
approximately using policy gradients and accelerated by an adaptive step size
calculated from their second moments. The developed algorithm, PASM, is proven
to be convergent under mild conditions and has a nice numerical performance
compared with some baseline methods for solving the resource allocation problem
in a V2X network.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09861" title="Abstract">arXiv:2310.09861</a> [<a href="/pdf/2310.09861" title="Download PDF">pdf</a>, <a href="/ps/2310.09861" title="Download PostScript">ps</a>, <a href="/format/2310.09861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stacked Intelligent Metasurface Performs a 2D DFT in the Wave Domain for  DOA Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+J">Jiancheng An</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=Di+Renzo%2C+M">Marco Di Renzo</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, submitted to IEEE ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Staked intelligent metasurface (SIM) based techniques are developed to
perform two-dimensional (2D) direction-of-arrival (DOA) estimation. In contrast
to the conventional designs, an advanced SIM in front of the receiving array
automatically performs the 2D discrete Fourier transform (DFT) as the incident
waves propagate through it. To arrange for the SIM to carry out this task, we
design a gradient descent algorithm for iteratively updating the phase shift of
each meta-atom in the SIM to minimize the fitting error between the SIM's
response and the 2D DFT matrix. To further improve the DOA estimation accuracy,
we configure the phase shifts in the input layer of SIM to generate a set of 2D
DFT matrices having orthogonal spatial frequency bins. Extensive numerical
simulations verify the capability of a well-trained SIM to perform 2D DFT.
Specifically, it is demonstrated that the SIM having an optical computational
speed achieves an MSE of $10^{-4}$ in 2D DOA estimation.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09866" title="Abstract">arXiv:2310.09866</a> [<a href="/pdf/2310.09866" title="Download PDF">pdf</a>, <a href="/format/2310.09866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Multi-Objective Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haibo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chaosheng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Momma%2C+M">Michinari Momma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In recent years, multi-objective optimization (MOO) emerges as a foundational
problem underpinning many multi-agent multi-task learning applications.
However, existing algorithms in MOO literature remain limited to centralized
learning settings, which do not satisfy the distributed nature and data privacy
needs of such multi-agent multi-task learning applications. This motivates us
to propose a new federated multi-objective learning (FMOL) framework with
multiple clients distributively and collaboratively solving an MOO problem
while keeping their training data private. Notably, our FMOL framework allows a
different set of objective functions across different clients to support a wide
range of applications, which advances and generalizes the MOO formulation to
the federated learning paradigm for the first time. For this FMOL framework, we
propose two new federated multi-objective optimization (FMOO) algorithms called
federated multi-gradient descent averaging (FMGDA) and federated stochastic
multi-gradient descent averaging (FSMGDA). Both algorithms allow local updates
to significantly reduce communication costs, while achieving the {\em same}
convergence rates as those of the their algorithmic counterparts in the
single-objective federated learning. Our extensive experiments also corroborate
the efficacy of our proposed FMOO algorithms.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09871" title="Abstract">arXiv:2310.09871</a> [<a href="/pdf/2310.09871" title="Download PDF">pdf</a>, <a href="/format/2310.09871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivariate sensitivity adaptive polynomial chaos expansion for vector  valued surrogate modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loukrezis%2C+D">Dimitrios Loukrezis</a>, 
<a href="/search/cs?searchtype=author&query=Diehl%2C+E">Eric Diehl</a>, 
<a href="/search/cs?searchtype=author&query=De+Gersem%2C+H">Herbert De Gersem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 6 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">This work develops a novel basis adaptive method for constructing anisotropic
polynomial chaos expansions of multidimensional (vector valued) model
responses. The suggested method selects the most influential expansion terms
adaptively, based on multivariate sensitivity analysis metrics that can be
estimated by post-processing the polynomial chaos expansion. It is able to
construct anisotropic polynomial chaos expansions and mitigate complexities due
to high dimensional model inputs, while simultaneously allowing approximations
of model responses with sizes up to the order of thousands. The vector valued
polynomial chaos surrogate model is computed at a much smaller computational
cost compared to existing sparse and/or basis adaptive polynomial chaos
expansion methods, which must be applied element-wise over the multidimensional
response and produce one polynomial basis per response element. Contrarily, a
common basis is obtained with the suggested method, leading to major savings in
terms of computational demand, while also achieving comparable approximation
accuracy. Numerical investigations on different engineering test cases
illustrate the advantages of the multivariate sensitivity adaptive polynomial
chaos expansion method over competing approaches.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09872" title="Abstract">arXiv:2310.09872</a> [<a href="/pdf/2310.09872" title="Download PDF">pdf</a>, <a href="/format/2310.09872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empower Text-Attributed Graphs Learning with Large Language Models  (LLMs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jianxiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuxiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+C">Chenghua Gong</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jiaqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuecang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Text-attributed graphs have recently garnered significant attention due to
their wide range of applications in web domains. Existing methodologies employ
word embedding models for acquiring text representations as node features,
which are subsequently fed into Graph Neural Networks (GNNs) for training.
Recently, the advent of Large Language Models (LLMs) has introduced their
powerful capabilities in information retrieval and text generation, which can
greatly enhance the text attributes of graph data. Furthermore, the acquisition
and labeling of extensive datasets are both costly and time-consuming
endeavors. Consequently, few-shot learning has emerged as a crucial problem in
the context of graph learning tasks. In order to tackle this challenge, we
propose a lightweight paradigm called ENG, which adopts a plug-and-play
approach to empower text-attributed graphs through node generation using LLMs.
Specifically, we utilize LLMs to extract semantic information from the labels
and generate samples that belong to these categories as exemplars.
Subsequently, we employ an edge predictor to capture the structural information
inherent in the raw dataset and integrate the newly generated samples into the
original graph. This approach harnesses LLMs for enhancing class-level
information and seamlessly introduces labeled nodes and edges without modifying
the raw dataset, thereby facilitating the node classification task in few-shot
scenarios. Extensive experiments demonstrate the outstanding performance of our
proposed paradigm, particularly in low-shot scenarios. For instance, in the
1-shot setting of the ogbn-arxiv dataset, ENG achieves a 76% improvement over
the baseline model.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09873" title="Abstract">arXiv:2310.09873</a> [<a href="/pdf/2310.09873" title="Download PDF">pdf</a>, <a href="/format/2310.09873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning for Reduced-order Models of Legged Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu-Ming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+H">Hien Bui</a>, 
<a href="/search/cs?searchtype=author&query=Posa%2C+M">Michael Posa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Model-based approaches for planning and control for bipedal locomotion have a
long history of success. It can provide stability and safety guarantees while
being effective in accomplishing many locomotion tasks. Model-free
reinforcement learning, on the other hand, has gained much popularity in recent
years due to computational advancements. It can achieve high performance in
specific tasks, but it lacks physical interpretability and flexibility in
re-purposing the policy for a different set of tasks. For instance, we can
initially train a neural network (NN) policy using velocity commands as inputs.
However, to handle new task commands like desired hand or footstep locations at
a desired walking velocity, we must retrain a new NN policy. In this work, we
attempt to bridge the gap between these two bodies of work on a bipedal
platform. We formulate a model-based reinforcement learning problem to learn a
reduced-order model (ROM) within a model predictive control (MPC). Results show
a 49% improvement in viable task region size and a 21% reduction in motor
torque cost. All videos and code are available at
https://sites.google.com/view/ymchen/research/rl-for-roms.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09874" title="Abstract">arXiv:2310.09874</a> [<a href="/pdf/2310.09874" title="Download PDF">pdf</a>, <a href="/format/2310.09874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset  Condensation for Content-Based Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiahao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qijiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hengchang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengcai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiao-Ming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Ke Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Modern techniques in Content-based Recommendation (CBR) leverage item content
information to provide personalized services to users, but suffer from
resource-intensive training on large datasets. To address this issue, we
explore the dataset condensation for textual CBR in this paper. The goal of
dataset condensation is to synthesize a small yet informative dataset, upon
which models can achieve performance comparable to those trained on large
datasets. While existing condensation approaches are tailored to classification
tasks for continuous data like images or embeddings, direct application of them
to CBR has limitations. To bridge this gap, we investigate efficient dataset
condensation for content-based recommendation. Inspired by the remarkable
abilities of large language models (LLMs) in text comprehension and generation,
we leverage LLMs to empower the generation of textual content during
condensation. To handle the interaction data involving both users and items, we
devise a dual-level condensation method: content-level and user-level. At
content-level, we utilize LLMs to condense all contents of an item into a new
informative title. At user-level, we design a clustering-based synthesis
module, where we first utilize LLMs to extract user interests. Then, the user
interests and user embeddings are incorporated to condense users and generate
interactions for condensed users. Notably, the condensation paradigm of this
method is forward and free from iterative optimization on the synthesized
dataset. Extensive empirical findings from our study, conducted on three
authentic datasets, substantiate the efficacy of the proposed method.
Particularly, we are able to approximate up to 97% of the original performance
while reducing the dataset size by 95% (i.e., on dataset MIND).
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09875" title="Abstract">arXiv:2310.09875</a> [<a href="/pdf/2310.09875" title="Download PDF">pdf</a>, <a href="/format/2310.09875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Free as a Bird: Event-based Dynamic Sense-and-Avoid for Ornithopter  Robot Flight
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez-G%C3%B3mez%2C+J+P">J.P. Rodr&#xed;guez-G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Tapia%2C+R">R. Tapia</a>, 
<a href="/search/cs?searchtype=author&query=Guzm%C3%A1n%2C+M+M">M.M. Guzm&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Dios%2C+J+R+M">J.R. Mart&#xed;nez-de Dios</a>, 
<a href="/search/cs?searchtype=author&query=Ollero%2C+A">A. Ollero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures, Journal paper. For associated video, see "this http URL" <a href="https://www.youtube.com/watch?v=cBMcw5jRnfU">this https URL</a>&amp;list=PL-Kzs2T7Hx3K-IDKsgUwPUnzHmk8Pcsek
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Robotics and Automation Letters Volume: 7, Issue: 2, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Autonomous flight of flapping-wing robots is a major challenge for robot
perception. Most of the previous sense-and-avoid works have studied the problem
of obstacle avoidance for flapping-wing robots considering only static
obstacles. This paper presents a fully onboard dynamic sense-and-avoid scheme
for large-scale ornithopters using event cameras. These sensors trigger pixel
information due to changes of illumination in the scene such as those produced
by dynamic objects. The method performs event-by-event processing in low-cost
hardware such as those onboard small aerial vehicles. The proposed scheme
detects obstacles and evaluates possible collisions with the robot body. The
onboard controller actuates over the horizontal and vertical tail deflections
to execute the avoidance maneuver. The scheme is validated in both indoor and
outdoor scenarios using obstacles of different shapes and sizes. To the best of
the authors' knowledge, this is the first event-based method for dynamic
obstacle avoidance in a flapping-wing robot.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09876" title="Abstract">arXiv:2310.09876</a> [<a href="/pdf/2310.09876" title="Download PDF">pdf</a>, <a href="/format/2310.09876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounding and Filling: A Fast and Flexible Framework for Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Bo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zixuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianbing Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NLPCC2023 Best Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Most image captioning models following an autoregressive manner suffer from
significant inference latency. Several models adopted a non-autoregressive
manner to speed up the process. However, the vanilla non-autoregressive manner
results in subpar performance, since it generates all words simultaneously,
which fails to capture the relationships between words in a description. The
semi-autoregressive manner employs a partially parallel method to preserve
performance, but it sacrifices inference speed. In this paper, we introduce a
fast and flexible framework for image captioning called BoFiCap based on
bounding and filling techniques. The BoFiCap model leverages the inherent
characteristics of image captioning tasks to pre-define bounding boxes for
image regions and their relationships. Subsequently, the BoFiCap model fills
corresponding words in each box using two-generation manners. Leveraging the
box hints, our filling process allows each word to better perceive other words.
Additionally, our model offers flexible image description generation: 1) by
employing different generation manners based on speed or performance
requirements, 2) producing varied sentences based on user-specified boxes.
Experimental evaluations on the MS-COCO benchmark dataset demonstrate that our
framework in a non-autoregressive manner achieves the state-of-the-art on
task-specific metric CIDEr (125.6) while speeding up 9.22x than the baseline
model with an autoregressive manner; in a semi-autoregressive manner, our
method reaches 128.4 on CIDEr while a 3.69x speedup. Our code and data is
available at https://github.com/ChangxinWang/BoFiCap.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09877" title="Abstract">arXiv:2310.09877</a> [<a href="/pdf/2310.09877" title="Download PDF">pdf</a>, <a href="/format/2310.09877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical inference using machine learning and classical techniques  based on accumulated local effects (ALE)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okoli%2C+C">Chitu Okoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Accumulated Local Effects (ALE) is a model-agnostic approach for global
explanations of the results of black-box machine learning (ML) algorithms.
There are at least three challenges with conducting statistical inference based
on ALE: ensuring the reliability of ALE analyses, especially in the context of
small datasets; intuitively characterizing a variable's overall effect in ML;
and making robust inferences from ML data analysis. In response, we introduce
innovative tools and techniques for statistical inference using ALE,
establishing bootstrapped confidence intervals tailored to dataset size and
introducing ALE effect size measures that intuitively indicate effects on both
the outcome variable scale and a normalized scale. Furthermore, we demonstrate
how to use these tools to draw reliable statistical inferences, reflecting the
flexible patterns ALE adeptly highlights, with implementations available in the
'ale' package in R. This work propels the discourse on ALE and its
applicability in ML and statistical analysis forward, offering practical
solutions to prevailing challenges in the field.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09881" title="Abstract">arXiv:2310.09881</a> [<a href="/pdf/2310.09881" title="Download PDF">pdf</a>, <a href="/format/2310.09881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning with Iterative Demonstration Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Chengwei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aston Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dagar%2C+A">Anirudh Dagar</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wenming Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Spurred by advancements in scale, large language models (LLMs) have
demonstrated strong few-shot learning ability via in-context learning (ICL).
However, the performance of ICL has been shown to be highly sensitive to the
selection of few-shot demonstrations. Selecting the most suitable examples as
context remains an ongoing challenge and an open problem. Existing literature
has highlighted the importance of selecting examples that are diverse or
semantically similar to the test sample while ignoring the fact that the
optimal selection dimension, i.e., diversity or similarity, is task-specific.
Leveraging the merits of both dimensions, we propose Iterative Demonstration
Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT),
IDS iteratively selects examples that are diverse but still strongly correlated
with the test sample as ICL demonstrations. Specifically, IDS applies
Zero-shot-CoT to the test sample before demonstration selection. The output
reasoning path is then used to choose demonstrations that are prepended to the
test sample for inference. The generated answer is accompanied by its
corresponding reasoning path for extracting a new set of demonstrations in the
next iteration. After several iterations, IDS adopts majority voting to obtain
the final result. Through extensive experiments on tasks including commonsense
reasoning, question answering, topic classification, and sentiment analysis, we
demonstrate that IDS can consistently outperform existing ICL demonstration
selection methods.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09883" title="Abstract">arXiv:2310.09883</a> [<a href="/pdf/2310.09883" title="Download PDF">pdf</a>, <a href="/format/2310.09883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Object Goal Visual Navigation With Class-Independent  Relationship Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinting Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shizhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=LU%2C+Y">Yue LU</a>, 
<a href="/search/cs?searchtype=author&query=Dan%2C+K">Kerry Dan</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+L">Lingyan Ran</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanning Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This paper investigates the zero-shot object goal visual navigation problem.
In the object goal visual navigation task, the agent needs to locate navigation
targets from its egocentric visual input. "Zero-shot" means that the target the
agent needs to find is not trained during the training phase. To address the
issue of coupling navigation ability with target features during training, we
propose the Class-Independent Relationship Network (CIRN). This method combines
target detection information with the relative semantic similarity between the
target and the navigation target, and constructs a brand new state
representation based on similarity ranking, this state representation does not
include target feature or environment feature, effectively decoupling the
agent's navigation ability from target features. And a Graph Convolutional
Network (GCN) is employed to learn the relationships between different objects
based on their similarities. During testing, our approach demonstrates strong
generalization capabilities, including zero-shot navigation tasks with
different targets and environments. Through extensive experiments in the
AI2-THOR virtual environment, our method outperforms the current
state-of-the-art approaches in the zero-shot object goal visual navigation
task. Furthermore, we conducted experiments in more challenging cross-target
and cross-scene settings, which further validate the robustness and
generalization ability of our method. Our code is available at:
https://github.com/SmartAndCleverRobot/ICRA-CIRN.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09884" title="Abstract">arXiv:2310.09884</a> [<a href="/pdf/2310.09884" title="Download PDF">pdf</a>, <a href="/format/2310.09884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unmasking the Web of Deceit: Uncovering Coordinated Activity to Expose  Information Operations on Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>, 
<a href="/search/cs?searchtype=author&query=Pant%C3%A8%2C+V">Valeria Pant&#xe8;</a>, 
<a href="/search/cs?searchtype=author&query=Burghardt%2C+K">Keith Burghardt</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Social media platforms, particularly Twitter, have become pivotal arenas for
influence campaigns, often orchestrated by state-sponsored information
operations (IOs). This paper delves into the detection of key players driving
IOs by employing similarity graphs constructed from behavioral pattern data. We
unveil that well-known, yet underutilized network properties can help
accurately identify coordinated IO drivers. Drawing from a comprehensive
dataset of 49 million tweets from six countries, which includes multiple
verified IOs, our study reveals that traditional network filtering techniques
do not consistently pinpoint IO drivers across campaigns. We first propose a
framework based on node pruning that emerges superior, particularly when
combining multiple behavioral indicators across different networks. Then, we
introduce a supervised machine learning model that harnesses a vector
representation of the fused similarity network. This model, which boasts a
precision exceeding 0.95, adeptly classifies IO drivers on a global scale and
reliably forecasts their temporal engagements. Our findings are crucial in the
fight against deceptive influence campaigns on social media, helping us better
understand and detect them.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09886" title="Abstract">arXiv:2310.09886</a> [<a href="/pdf/2310.09886" title="Download PDF">pdf</a>, <a href="/format/2310.09886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lifelong Sequence Generation with Dynamic Module Expansion and  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Chengwei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Lifelong sequence generation (LSG), a problem in continual learning, aims to
continually train a model on a sequence of generation tasks to learn constantly
emerging new generation patterns while avoiding the forgetting of previous
knowledge. Existing LSG methods mainly focus on maintaining old knowledge while
paying little attention to knowledge transfer across tasks. In contrast, humans
can better learn new tasks by leveraging previously acquired knowledge from
similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic
Module Expansion and Adaptation (DMEA), which enables the model to dynamically
determine the architecture for acquiring new knowledge based on task
correlation and select the most similar previous tasks to facilitate adaptation
to new tasks. In addition, as the learning process can easily be biased towards
the current task which might cause more severe forgetting of previously learned
knowledge, we propose dynamic gradient scaling to balance the learning of the
current task and replayed tasks. With extensive experiments, we demonstrate
that DMEA can consistently outperform existing methods in different LSG
settings.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09889" title="Abstract">arXiv:2310.09889</a> [<a href="/pdf/2310.09889" title="Download PDF">pdf</a>, <a href="/format/2310.09889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Capacity Region of Information Theoretic Secure Aggregation with  Uncoded Groupwise Keys
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hua Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+M">Mingyue Ji</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+T">Tiebin Mi</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper considers the secure aggregation problem for federated learning
under an information theoretic cryptographic formulation, where distributed
training nodes (referred to as users) train models based on their own local
data and a curious-but-honest server aggregates the trained models without
retrieving other information about users' local data. Secure aggregation
generally contains two phases, namely key sharing phase and model aggregation
phase. Due to the common effect of user dropouts in federated learning, the
model aggregation phase should contain two rounds, where in the first round the
users transmit masked models and, in the second round, according to the
identity of surviving users after the first round, these surviving users
transmit some further messages to help the server decrypt the sum of users'
trained models. The objective of the considered information theoretic
formulation is to characterize the capacity region of the communication rates
in the two rounds from the users to the server in the model aggregation phase,
assuming that key sharing has already been performed offline in prior. In this
context, Zhao and Sun completely characterized the capacity region under the
assumption that the keys can be arbitrary random variables. More recently, an
additional constraint, known as "uncoded groupwise keys," has been introduced.
This constraint entails the presence of multiple independent keys within the
system, with each key being shared by precisely S users. The capacity region
for the information-theoretic secure aggregation problem with uncoded groupwise
keys was established in our recent work subject to the condition S &gt; K - U,
where K is the number of total users and U is the designed minimum number of
surviving users. In this paper we fully characterize of the the capacity region
for this problem by proposing a new converse bound and an achievable scheme.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09890" title="Abstract">arXiv:2310.09890</a> [<a href="/pdf/2310.09890" title="Download PDF">pdf</a>, <a href="/format/2310.09890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-Based Methods for Discrete Optimization in Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+E">Eric Lei</a>, 
<a href="/search/cs?searchtype=author&query=Adibi%2C+A">Arman Adibi</a>, 
<a href="/search/cs?searchtype=author&query=Hassani%2C+H">Hamed Hassani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Discrete optimization problems often arise in deep learning tasks, despite
the fact that neural networks typically operate on continuous data. One class
of these problems involve objective functions which depend on neural networks,
but optimization variables which are discrete. Although the discrete
optimization literature provides efficient algorithms, they are still
impractical in these settings due to the high cost of an objective function
evaluation, which involves a neural network forward-pass. In particular, they
require $O(n)$ complexity per iteration, but real data such as point clouds
have values of $n$ in thousands or more. In this paper, we investigate a
score-based approximation framework to solve such problems. This framework uses
a score function as a proxy for the marginal gain of the objective, leveraging
embeddings of the discrete variables and speed of auto-differentiation
frameworks to compute backward-passes in parallel. We experimentally
demonstrate, in adversarial set classification tasks, that our method achieves
a superior trade-off in terms of speed and solution quality compared to
heuristic methods.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09891" title="Abstract">arXiv:2310.09891</a> [<a href="/pdf/2310.09891" title="Download PDF">pdf</a>, <a href="/format/2310.09891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Deep Learning Models Resistant to Transfer-based Adversarial  Attacks via Data-centric Robust Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yulong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenhao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiang Ji</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qiwei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongshan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhibo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Transfer-based adversarial attacks raise a severe threat to real-world deep
learning systems since they do not require access to target models. Adversarial
training (AT), which is recognized as the strongest defense against white-box
attacks, has also guaranteed high robustness to (black-box) transfer-based
attacks. However, AT suffers from heavy computational overhead since it
optimizes the adversarial examples during the whole training process. In this
paper, we demonstrate that such heavy optimization is not necessary for AT
against transfer-based attacks. Instead, a one-shot adversarial augmentation
prior to training is sufficient, and we name this new defense paradigm
Data-centric Robust Learning (DRL). Our experimental results show that DRL
outperforms widely-used AT techniques (e.g., PGD-AT, TRADES, EAT, and FAT) in
terms of black-box robustness and even surpasses the top-1 defense on
RobustBench when combined with diverse data augmentations and loss
regularizations. We also identify other benefits of DRL, for instance, the
model generalization capability and robust fairness.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09892" title="Abstract">arXiv:2310.09892</a> [<a href="/pdf/2310.09892" title="Download PDF">pdf</a>, <a href="/format/2310.09892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Perception using Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+S">Siming He</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C+D">Christopher D. Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+D">Dexter Ong</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y+S">Yifei Simon Shao</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+P">Pratik Chaudhari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We study active perception from first principles to argue that an autonomous
agent performing active perception should maximize the mutual information that
past observations posses about future ones. Doing so requires (a) a
representation of the scene that summarizes past observations and the ability
to update this representation to incorporate new observations (state estimation
and mapping), (b) the ability to synthesize new observations of the scene (a
generative model), and (c) the ability to select control trajectories that
maximize predictive information (planning). This motivates a neural radiance
field (NeRF)-like representation which captures photometric, geometric and
semantic properties of the scene grounded. This representation is well-suited
to synthesizing new observations from different viewpoints. And thereby, a
sampling-based planner can be used to calculate the predictive information from
synthetic observations along dynamically-feasible trajectories. We use active
perception for exploring cluttered indoor environments and employ a notion of
semantic uncertainty to check for the successful completion of an exploration
task. We demonstrate these ideas via simulation in realistic 3D indoor
environments.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09893" title="Abstract">arXiv:2310.09893</a> [<a href="/pdf/2310.09893" title="Download PDF">pdf</a>, <a href="/format/2310.09893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Contact-Implicit Model Predictive Control with Online Residual  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei-Cheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Aydinoglu%2C+A">Alp Aydinoglu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wanxin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Posa%2C+M">Michael Posa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Wei-Cheng Huang and Alp Aydinoglu contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The hybrid nature of multi-contact robotic systems, due to making and
breaking contact with the environment, creates significant challenges for
high-quality control. Existing model-based methods typically rely on either
good prior knowledge of the multi-contact model or require significant offline
model tuning effort, thus resulting in low adaptability and robustness. In this
paper, we propose a real-time adaptive multi-contact model predictive control
framework, which enables online adaption of the hybrid multi-contact model and
continuous improvement of the control performance for contact-rich tasks. This
framework includes an adaption module, which continuously learns a residual of
the hybrid model to minimize the gap between the prior model and reality, and a
real-time multi-contact MPC controller. We demonstrated the effectiveness of
the framework in synthetic examples, and applied it on hardware to solve
contact-rich manipulation tasks, where a robot uses its end-effector to roll
different unknown objects on a table to track given paths. The hardware
experiments show that with a rough prior model, the multi-contact MPC
controller adapts itself on-the-fly with an adaption rate around 20 Hz and
successfully manipulates previously unknown objects with non-smooth surface
geometries.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09897" title="Abstract">arXiv:2310.09897</a> [<a href="/pdf/2310.09897" title="Download PDF">pdf</a>, <a href="/format/2310.09897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reformulating NLP tasks to Capture Longitudinal Manifestation of  Language Disorders in People with Dementia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gkoumas%2C+D">Dimitris Gkoumas</a>, 
<a href="/search/cs?searchtype=author&query=Purver%2C+M">Matthew Purver</a>, 
<a href="/search/cs?searchtype=author&query=Liakata%2C+M">Maria Liakata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It has been accepted to appear at EMNLP23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dementia is associated with language disorders which impede communication.
Here, we automatically learn linguistic disorder patterns by making use of a
moderately-sized pre-trained language model and forcing it to focus on
reformulated natural language processing (NLP) tasks and associated linguistic
patterns. Our experiments show that NLP tasks that encapsulate contextual
information and enhance the gradient signal with linguistic patterns benefit
performance. We then use the probability estimates from the best model to
construct digital linguistic markers measuring the overall quality in
communication and the intensity of a variety of language disorders. We
investigate how the digital markers characterize dementia speech from a
longitudinal perspective. We find that our proposed communication marker is
able to robustly and reliably characterize the language of people with
dementia, outperforming existing linguistic approaches; and shows external
validity via significant correlation with clinical markers of behaviour.
Finally, our proposed linguistic disorder markers provide useful insights into
gradual language impairment associated with disease progression.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09899" title="Abstract">arXiv:2310.09899</a> [<a href="/pdf/2310.09899" title="Download PDF">pdf</a>, <a href="/format/2310.09899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizable whole-body global manipulation of deformable linear  objects by dual-arm robot in 3-D constrained environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mingrui Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+K">Kangchen Lv</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yongpeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://mingrui-yu.github.io/DLO_planning_2">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Constrained environments are common in practical applications of manipulating
deformable linear objects (DLOs), where movements of both DLOs and robots
should be constrained. This task is high-dimensional and highly constrained
owing to the highly deformable DLOs, dual-arm robots with high degrees of
freedom, and 3-D complex environments, which render global planning
challenging. Furthermore, accurate DLO models needed by planning are often
unavailable owing to their strong nonlinearity and diversity, resulting in
unreliable planned paths. This article focuses on the global moving and shaping
of DLOs in constrained environments by dual-arm robots. The main objectives are
1) to efficiently and accurately accomplish this task, and 2) to achieve
generalizable and robust manipulation of various DLOs. To this end, we propose
a complementary framework with whole-body planning and control using
appropriate DLO model representations. First, a global planner is proposed to
efficiently find feasible solutions based on a simplified DLO energy model,
which considers the full system states and all constraints to plan more
reliable paths. Then, a closed-loop manipulation scheme is proposed to
compensate for the modeling errors and enhance the robustness and accuracy,
which incorporates a model predictive controller that real-time adjusts the
robot motion based on an adaptive DLO motion model. The key novelty is that our
framework can efficiently solve the high-dimensional problem subject to
multiple constraints and generalize to various DLOs without elaborate model
identifications. Experiments demonstrate that our framework can accomplish
considerably more complicated tasks than existing works, with significantly
higher efficiency, generalizability, and reliability.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09909" title="Abstract">arXiv:2310.09909</a> [<a href="/pdf/2310.09909" title="Download PDF">pdf</a>, <a href="/format/2310.09909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for  Multimodal Medical Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chaoyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+J">Jiayu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qiaoyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weike Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weixiong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoman Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Driven by the large foundation models, the development of artificial
intelligence has witnessed tremendous progress lately, leading to a surge of
general interest from the public. In this study, we aim to assess the
performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm
of multimodal medical diagnosis. Our evaluation encompasses 17 human body
systems, including Central Nervous System, Head and Neck, Cardiac, Chest,
Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology,
Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma,
Pediatrics, with images taken from 8 modalities used in daily clinic routine,
e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI),
Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA),
Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on
multiple clinical tasks with or without patent history provided, including
imaging modality and anatomy recognition, disease diagnosis, report generation,
disease localisation.
<br />Our observation shows that, while GPT-4V demonstrates proficiency in
distinguishing between medical image modalities and anatomy, it faces
significant challenges in disease diagnosis and generating comprehensive
reports. These findings underscore that while large multimodal models have made
significant advancements in computer vision and natural language processing, it
remains far from being used to effectively support real-world medical
applications and clinical decision-making.
<br />All images used in this report can be found in
https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09912" title="Abstract">arXiv:2310.09912</a> [<a href="/pdf/2310.09912" title="Download PDF">pdf</a>, <a href="/format/2310.09912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Discovery of Interpretable Directions in h-space of  Pre-trained Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zijian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L+L+Z">Luping Liu. Zhijie Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yichen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhou Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose the first unsupervised and learning-based method to identify
interpretable directions in the h-space of pre-trained diffusion models. Our
method is derived from an existing technique that operates on the GAN latent
space. In a nutshell, we employ a shift control module for pre-trained
diffusion models to manipulate a sample into a shifted version of itself,
followed by a reconstructor to reproduce both the type and the strength of the
manipulation. By jointly optimizing them, the model will spontaneously discover
disentangled and interpretable directions. To prevent the discovery of
meaningless and destructive directions, we employ a discriminator to maintain
the fidelity of shifted sample. Due to the iterative generative process of
diffusion models, our training requires a substantial amount of GPU VRAM to
store numerous intermediate tensors for back-propagating gradient. To address
this issue, we first propose a general VRAM-efficient training algorithm based
on gradient checkpointing technique to back-propagate any gradient through the
whole generative process, with acceptable occupancy of VRAM and sacrifice of
training efficiency. Compared with existing related works on diffusion models,
our method inherently identifies global and scalable directions, without
necessitating any other complicated procedures. Extensive experiments on
various datasets demonstrate the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09913" title="Abstract">arXiv:2310.09913</a> [<a href="/pdf/2310.09913" title="Download PDF">pdf</a>, <a href="/format/2310.09913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Sustained And Coordinated Rhythmic Deformations With SMA For  Controller-Free Locomotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Ziyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Suyi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This study presents a modular, electronics-free, and fully onboard control
and actuation approach for SMA-based soft robots to achieve locomotion tasks.
This approach exploits the nonlinear mechanics of compliant curved beams and
carefully designed mechanical control circuits to create and synchronize
rhythmic deformation cycles, mimicking the central pattern generators (CPG)
prevalent in animal locomotions. More specifically, the study elucidates a new
strategy to amplify the actuation performance of the shape memory coil actuator
by coupling it to a carefully designed, mono-stable curve beam with a
snap-through buckling behavior. Such SMA-curved beam assembly is integrated
with an entirely mechanical circuit featuring a slider mechanism. This circuit
can automatically cut off and supply current to the SMA according to its
deformation status, generating a self-sustained rhythmic deformation cycle
using a simple DC power supply. Finally, this study presents a new strategy to
coordinate (synchronize) two rhythmic deformation cycles from two robotic
modules to achieve efficient crawling locomotion but still use a single DC
power. This work represents a significant step towards fully autonomous,
electronics-free SMA-based locomotion robots with fully onboard actuation and
control.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09916" title="Abstract">arXiv:2310.09916</a> [<a href="/pdf/2310.09916" title="Download PDF">pdf</a>, <a href="/format/2310.09916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Socially reactive navigation models for mobile robots in dynamic  environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+R">Ricarte Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+P">Plinio Moreno</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The objective of this work is to expand upon previous works, considering
socially acceptable behaviours within robot navigation and interaction, and
allow a robot to closely approach static and dynamic individuals or groups. The
space models developed in this dissertation are adaptive, that is, capable of
changing over time to accommodate the changing circumstances often existent
within a social environment. The space model's parameters' adaptation occurs
with the end goal of enabling a close interaction between humans and robots and
is thus capable of taking into account not only the arrangement of the groups,
but also the basic characteristics of the robot itself. This work also further
develops a preexisting approach pose estimation algorithm in order to better
guarantee the safety and comfort of the humans involved in the interaction, by
taking into account basic human sensibilities. The algorithms are integrated
into ROS's navigation system through the use of the $costmap2d$ and the
$move\_base$ packages. The space model adaptation is tested via comparative
evaluation against previous algorithms through the use of datasets. The entire
navigation system is then evaluated through both simulations (static and
dynamic) and real life situations (static). These experiments demonstrate that
the developed space model and approach pose estimation algorithms are capable
of enabling a robot to closely approach individual humans and groups, while
maintaining considerations for their comfort and sensibilities.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09917" title="Abstract">arXiv:2310.09917</a> [<a href="/pdf/2310.09917" title="Download PDF">pdf</a>, <a href="/format/2310.09917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical study of pretrained multilingual language models for zero-shot  cross-lingual generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chirkova%2C+N">Nadezhda Chirkova</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Sheng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Nikoulina%2C+V">Vassilina Nikoulina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Zero-shot cross-lingual generation assumes finetuning the multilingual
pretrained language model (mPLM) on a generation task in one language and then
using it to make predictions for this task in other languages. Previous works
notice a frequent problem of generation in a wrong language and propose
approaches to address it, usually using mT5 as a backbone model. In this work,
we test alternative mPLMs, such as mBART and NLLB, considering full finetuning
and parameter-efficient finetuning with adapters. We find that mBART with
adapters performs similarly to mT5 of the same size, and NLLB can be
competitive in some cases. We also underline the importance of tuning learning
rate used for finetuning, which helps to alleviate the problem of generation in
the wrong language.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09920" title="Abstract">arXiv:2310.09920</a> [<a href="/pdf/2310.09920" title="Download PDF">pdf</a>, <a href="/format/2310.09920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BONES: Near-Optimal Neural-Enhanced Video Streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Lingdong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+S">Simran Singh</a>, 
<a href="/search/eess?searchtype=author&query=Chakareski%2C+J">Jacob Chakareski</a>, 
<a href="/search/eess?searchtype=author&query=Hajiesmaili%2C+M">Mohammad Hajiesmaili</a>, 
<a href="/search/eess?searchtype=author&query=Sitaraman%2C+R+K">Ramesh K. Sitaraman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Accessing high-quality video content can be challenging due to insufficient
and unstable network bandwidth. Recent advances in neural enhancement have
shown promising results in improving the quality of degraded videos through
deep learning. Neural-Enhanced Streaming (NES) incorporates this new approach
into video streaming, allowing users to download low-quality video segments and
then enhance them to obtain high-quality content without violating the playback
of the video stream. We introduce BONES, an NES control algorithm that jointly
manages the network and computational resources to maximize the quality of
experience (QoE) of the user. BONES formulates NES as a Lyapunov optimization
problem and solves it in an online manner with near-optimal performance, making
it the first NES algorithm to provide a theoretical performance guarantee. Our
comprehensive experimental results indicate that BONES increases QoE by 4% to
13% over state-of-the-art algorithms, demonstrating its potential to enhance
the video streaming experience for users. Our code and data will be released to
the public.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09924" title="Abstract">arXiv:2310.09924</a> [<a href="/pdf/2310.09924" title="Download PDF">pdf</a>, <a href="/format/2310.09924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning with Explicit Context Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Munguia-Galeano%2C+F">Francisco Munguia-Galeano</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+A">Ah-Hwee Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Ze Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript accepted for publication as regular paper in IEEE Transactions on Neural Networks and Learning Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Reinforcement learning (RL) has shown an outstanding capability for solving
complex computational problems. However, most RL algorithms lack an explicit
method that would allow learning from contextual information. Humans use
context to identify patterns and relations among elements in the environment,
along with how to avoid making wrong actions. On the other hand, what may seem
like an obviously wrong decision from a human perspective could take hundreds
of steps for an RL agent to learn to avoid. This paper proposes a framework for
discrete environments called Iota explicit context representation (IECR). The
framework involves representing each state using contextual key frames (CKFs),
which can then be used to extract a function that represents the affordances of
the state; in addition, two loss functions are introduced with respect to the
affordances of the state. The novelty of the IECR framework lies in its
capacity to extract contextual information from the environment and learn from
the CKFs' representation. We validate the framework by developing four new
algorithms that learn using context: Iota deep Q-network (IDQN), Iota double
deep Q-network (IDDQN), Iota dueling deep Q-network (IDuDQN), and Iota dueling
double deep Q-network (IDDDQN). Furthermore, we evaluate the framework and the
new algorithms in five discrete environments. We show that all the algorithms,
which use contextual information, converge in around 40,000 training steps of
the neural networks, significantly outperforming their state-of-the-art
equivalents.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09925" title="Abstract">arXiv:2310.09925</a> [<a href="/pdf/2310.09925" title="Download PDF">pdf</a>, <a href="/format/2310.09925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homophone Disambiguation Reveals Patterns of Context Mixing in Speech  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohebbi%2C+H">Hosein Mohebbi</a>, 
<a href="/search/cs?searchtype=author&query=Chrupa%C5%82a%2C+G">Grzegorz Chrupa&#x142;a</a>, 
<a href="/search/cs?searchtype=author&query=Zuidema%2C+W">Willem Zuidema</a>, 
<a href="/search/cs?searchtype=author&query=Alishahi%2C+A">Afra Alishahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Transformers have become a key architecture in speech processing, but our
understanding of how they build up representations of acoustic and linguistic
structure is limited. In this study, we address this gap by investigating how
measures of 'context-mixing' developed for text models can be adapted and
applied to models of spoken language. We identify a linguistic phenomenon that
is ideal for such a case study: homophony in French (e.g. livre vs livres),
where a speech recognition model has to attend to syntactic cues such as
determiners and pronouns in order to disambiguate spoken words with identical
pronunciations and transcribe them while respecting grammatical agreement. We
perform a series of controlled experiments and probing analyses on
Transformer-based speech models. Our findings reveal that representations in
encoder-only models effectively incorporate these cues to identify the correct
transcription, whereas encoders in encoder-decoder models mainly relegate the
task of capturing contextual dependencies to decoder modules.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09926" title="Abstract">arXiv:2310.09926</a> [<a href="/pdf/2310.09926" title="Download PDF">pdf</a>, <a href="/format/2310.09926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Uncertainty in Multimodal Foundation Models using Public  Internet Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Shiladitya Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongbo Wei</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Laan%2C+L">Lars van der Laan</a>, 
<a href="/search/cs?searchtype=author&query=Alaa%2C+A+M">Ahmed M. Alaa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Foundation models are trained on vast amounts of data at scale using
self-supervised learning, enabling adaptation to a wide range of downstream
tasks. At test time, these models exhibit zero-shot capabilities through which
they can classify previously unseen (user-specified) categories. In this paper,
we address the problem of quantifying uncertainty in these zero-shot
predictions. We propose a heuristic approach for uncertainty estimation in
zero-shot settings using conformal prediction with web data. Given a set of
classes at test time, we conduct zero-shot classification with CLIP-style
models using a prompt template, e.g., "an image of a &lt;category&gt;", and use the
same template as a search query to source calibration data from the open web.
Given a web-based calibration set, we apply conformal prediction with a novel
conformity score that accounts for potential errors in retrieved web data. We
evaluate the utility of our proposed method in Biomedical foundation models;
our preliminary results show that web-based conformal prediction sets achieve
the target coverage with satisfactory efficiency on a variety of biomedical
datasets.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09929" title="Abstract">arXiv:2310.09929</a> [<a href="/pdf/2310.09929" title="Download PDF">pdf</a>, <a href="/format/2310.09929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Scientific Names for Zero-Shot Species Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parashar%2C+S">Shubham Parashar</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhiqiu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanan Li</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+S">Shu Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as
CLIP can recognize images of common objects in a zero-shot fashion. However, it
is underexplored how to use CLIP for zero-shot recognition of highly
specialized concepts, e.g., species of birds, plants, and animals, for which
their scientific names are written in Latin or Greek. Indeed, CLIP performs
poorly for zero-shot species recognition with prompts that use scientific
names, e.g., "a photo of Lepus Timidus" (which is a scientific name in Latin).
Because these names are usually not included in CLIP's training set. To improve
performance, prior works propose to use large-language models (LLMs) to
generate descriptions (e.g., of species color and shape) and additionally use
them in prompts. We find that they bring only marginal gains. Differently, we
are motivated to translate scientific names (e.g., Lepus Timidus) to common
English names (e.g., mountain hare) and use such in the prompts. We find that
common names are more likely to be included in CLIP's training set, and
prompting them achieves 2$\sim$5 times higher accuracy on benchmarking datasets
of fine-grained species recognition.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09930" title="Abstract">arXiv:2310.09930</a> [<a href="/pdf/2310.09930" title="Download PDF">pdf</a>, <a href="/format/2310.09930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FiLM: Fill-in Language Models for Any-Order Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tianxiao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+R">Ruoqi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Harchaoui%2C+Z">Zaid Harchaoui</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language models have become the backbone of today's AI systems. However,
their predominant left-to-right generation limits the use of bidirectional
context, which is essential for tasks that involve filling text in the middle.
We propose the Fill-in Language Model (FiLM), a new language modeling approach
that allows for flexible generation at any position without adhering to a
specific generation order. Its training extends the masked language modeling
objective by adopting varying mask probabilities sampled from the Beta
distribution to enhance the generative capabilities of FiLM. During inference,
FiLM can seamlessly insert missing phrases, sentences, or paragraphs, ensuring
that the outputs are fluent and are coherent with the surrounding context. In
both automatic and human evaluations, FiLM outperforms existing infilling
methods that rely on left-to-right language models trained on rearranged text
segments. FiLM is easy to implement and can be either trained from scratch or
fine-tuned from a left-to-right language model. Notably, as the model size
grows, FiLM's perplexity approaches that of strong left-to-right language
models of similar sizes, indicating FiLM's scalability and potential as a large
language model.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09932" title="Abstract">arXiv:2310.09932</a> [<a href="/pdf/2310.09932" title="Download PDF">pdf</a>, <a href="/format/2310.09932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Reading Between the Heat&quot;: Co-Teaching Body Thermal Signatures for  Non-intrusive Stress Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+H">Harshit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bergen-Cico%2C+D">Dessa Bergen-Cico</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+T">Tauhidur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Salekin%2C+A">Asif Salekin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Stress impacts our physical and mental health as well as our social life. A
passive and contactless indoor stress monitoring system can unlock numerous
important applications such as workplace productivity assessment, smart homes,
and personalized mental health monitoring. While the thermal signatures from a
user's body captured by a thermal camera can provide important information
about the "fight-flight" response of the sympathetic and parasympathetic
nervous system, relying solely on thermal imaging for training a stress
prediction model often lead to overfitting and consequently a suboptimal
performance. This paper addresses this challenge by introducing ThermaStrain, a
novel co-teaching framework that achieves high-stress prediction performance by
transferring knowledge from the wearable modality to the contactless thermal
modality. During training, ThermaStrain incorporates a wearable electrodermal
activity (EDA) sensor to generate stress-indicative representations from
thermal videos, emulating stress-indicative representations from a wearable EDA
sensor. During testing, only thermal sensing is used, and stress-indicative
patterns from thermal data and emulated EDA representations are extracted to
improve stress assessment. The study collected a comprehensive dataset with
thermal video and EDA data under various stress conditions and distances.
ThermaStrain achieves an F1 score of 0.8293 in binary stress classification,
outperforming the thermal-only baseline approach by over 9%. Extensive
evaluations highlight ThermaStrain's effectiveness in recognizing
stress-indicative attributes, its adaptability across distances and stress
scenarios, real-time executability on edge platforms, its applicability to
multi-individual sensing, ability to function on limited visibility and
unfamiliar conditions, and the advantages of its co-teaching approach.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09933" title="Abstract">arXiv:2310.09933</a> [<a href="/pdf/2310.09933" title="Download PDF">pdf</a>, <a href="/format/2310.09933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantitative Stability Conditions for Grid-Forming Converters With  Complex Droop Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=He%2C+X">Xiuqiang He</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+L">Linbin Huang</a>, 
<a href="/search/eess?searchtype=author&query=Suboti%C4%87%2C+I">Irina Suboti&#x107;</a>, 
<a href="/search/eess?searchtype=author&query=H%C3%A4berle%2C+V">Verena H&#xe4;berle</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we study analytically the transient stability of
grid-connected distributed generation systems with grid-forming (GFM) complex
droop control, also known as dispatchable virtual oscillator control (dVOC). We
prove theoretically that complex droop control, as a state-of-the-art GFM
control, always possesses steady-state equilibria whereas classical p/f and q/v
droop control does not. We provide quantitative conditions for complex droop
control maintaining transient stability (global asymptotic stability) under
grid disturbances, which is beyond the well-established local (non-global)
stability for classical droop control. For the transient instability of complex
droop control, we reveal that the unstable trajectories are bounded,
manifesting as limit cycle oscillations. Moreover, we extend our stability
results from second-order GFM control dynamics to full-order system dynamics
that additionally encompass both circuit electromagnetic transients and
inner-loop dynamics. Our theoretical results contribute an insightful
understanding of the transient stability and instability of complex droop
control and offer practical guidelines for parameter tuning and stability
guarantees.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09935" title="Abstract">arXiv:2310.09935</a> [<a href="/pdf/2310.09935" title="Download PDF">pdf</a>, <a href="/format/2310.09935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Passivity and Decentralized Stability Conditions for Grid-Forming  Converters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=He%2C+X">Xiuqiang He</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We prove that the popular grid-forming control, i.e., dispatchable virtual
oscillator control (dVOC), also termed complex droop control, exhibits
output-feedback passivity in its large-signal model, featuring an explicit and
physically meaningful passivity index. Using this passivity property, we derive
decentralized stability conditions for the transient stability of dVOC in
multi-converter grid-connected systems, beyond prior small-signal stability
results. The decentralized conditions are of practical significance,
particularly for ensuring the transient stability of renewable power plants
under grid disturbances.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09943" title="Abstract">arXiv:2310.09943</a> [<a href="/pdf/2310.09943" title="Download PDF">pdf</a>, <a href="/format/2310.09943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Robustness of Visual Representations for Object Assembly Task  Requiring Spatio-Geometrical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ku%2C+C">Chahyon Ku</a> (1), 
<a href="/search/cs?searchtype=author&query=Winge%2C+C">Carl Winge</a> (1), 
<a href="/search/cs?searchtype=author&query=Diaz%2C+R">Ryan Diaz</a> (1), 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Wentao Yuan</a> (2), 
<a href="/search/cs?searchtype=author&query=Desingh%2C+K">Karthik Desingh</a> (1) ((1) University of Minnesota, (2) University of Washington)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper primarily focuses on evaluating and benchmarking the robustness of
visual representations in the context of object assembly tasks. Specifically,
it investigates the alignment and insertion of objects with geometrical
extrusions and intrusions, commonly referred to as a peg-in-hole task. The
accuracy required to detect and orient the peg and the hole geometry in SE(3)
space for successful assembly poses significant challenges. Addressing this, we
employ a general framework in visuomotor policy learning that utilizes visual
pretraining models as vision encoders. Our study investigates the robustness of
this framework when applied to a dual-arm manipulation setup, specifically to
the grasp variations. Our quantitative analysis shows that existing pretrained
models fail to capture the essential visual features necessary for this task.
However, a visual encoder trained from scratch consistently outperforms the
frozen pretrained models. Moreover, we discuss rotation representations and
associated loss functions that substantially improve policy learning. We
present a novel task scenario designed to evaluate the progress in visuomotor
policy learning, with a specific focus on improving the robustness of intricate
assembly tasks that require both geometrical and spatial reasoning. Videos,
additional experiments, dataset, and code are available at
https://bit.ly/geometric-peg-in-hole .
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09946" title="Abstract">arXiv:2310.09946</a> [<a href="/pdf/2310.09946" title="Download PDF">pdf</a>, <a href="/format/2310.09946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UvA-MT&#x27;s Participation in the WMT23 General Translation Shared Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+S">Shaomu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Stap%2C+D">David Stap</a>, 
<a href="/search/cs?searchtype=author&query=Araabi%2C+A">Ali Araabi</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by the WMT2023 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper describes the UvA-MT's submission to the WMT 2023 shared task on
general machine translation. We participate in the constrained track in two
directions: English &lt;-&gt; Hebrew. In this competition, we show that by using one
model to handle bidirectional tasks, as a minimal setting of Multilingual
Machine Translation (MMT), it is possible to achieve comparable results with
that of traditional bilingual translation for both directions. By including
effective strategies, like back-translation, re-parameterized embedding table,
and task-oriented fine-tuning, we obtained competitive final results in the
automatic evaluation for both English -&gt; Hebrew and Hebrew -&gt; English
directions.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09949" title="Abstract">arXiv:2310.09949</a> [<a href="/pdf/2310.09949" title="Download PDF">pdf</a>, <a href="/format/2310.09949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chameleon: a Heterogeneous and Disaggregated Accelerator System for  Retrieval-Augmented Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zeller%2C+M">Marco Zeller</a>, 
<a href="/search/cs?searchtype=author&query=Waleffe%2C+R">Roger Waleffe</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>, 
<a href="/search/cs?searchtype=author&query=Alonso%2C+G">Gustavo Alonso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Computation and Language (cs.CL)

</div>
<p class="mathjax">A Retrieval-Augmented Language Model (RALM) augments a generative language
model by retrieving context-specific knowledge from an external database. This
strategy facilitates impressive text generation quality even with smaller
models, thus reducing orders of magnitude of computational demands. However,
RALMs introduce unique system design challenges due to (a) the diverse workload
characteristics between LM inference and retrieval and (b) the various system
requirements and bottlenecks for different RALM configurations such as model
sizes, database sizes, and retrieval frequencies. We propose Chameleon, a
heterogeneous accelerator system that integrates both LM and retrieval
accelerators in a disaggregated architecture. The heterogeneity ensures
efficient acceleration of both LM inference and retrieval, while the
accelerator disaggregation enables the system to independently scale both types
of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype
implements retrieval accelerators on FPGAs and assigns LM inference to GPUs,
with a CPU server orchestrating these accelerators over the network. Compared
to CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x
speedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon
exhibits up to 2.16x reduction in latency and 3.18x speedup in throughput
compared to the hybrid CPU-GPU architecture. These promising results pave the
way for bringing accelerator heterogeneity and disaggregation into future RALM
systems.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09951" title="Abstract">arXiv:2310.09951</a> [<a href="/pdf/2310.09951" title="Download PDF">pdf</a>, <a href="/format/2310.09951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open RAN meets Semantic Communications: A Synergistic Match for Open,  Intelligent, and Knowledge-Driven 6G
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peizheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Aijaz%2C+A">Adnan Aijaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figrues. This paper has been accepted by the IEEE Conference on Standards for Communications and Networking (IEEE CSCN 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Improving sustainability, enhancing spectral and energy efficiency, and
bringing in-network intelligence and reasoning are the driving forces for 6G.
In this context, semantic communications (SemCom) and open radio access
networks (Open RAN) are emerging as focal points of research. SemCom is widely
viewed as a disruptive paradigm that creates the possibility of
knowledge-driven networks. On the other hand, Open RAN paves the way for open
and programmable networks with native intelligence. This paper investigates the
synergies between SemCom and Open RAN and introduces the concept of
semantic-aware Open RAN. It presents the main architectural components and
functionalities along with some of the key applications of semantic-aware Open
RAN. It also conducts demonstration and evaluation of a remote WiFi
localization built on semantic-aware Open RAN. Finally, it highlights some open
challenges for semantic-aware Open RAN.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09952" title="Abstract">arXiv:2310.09952</a> [<a href="/pdf/2310.09952" title="Download PDF">pdf</a>, <a href="/format/2310.09952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeking Next Layer Neurons&#x27; Attention for Error-Backpropagation-Like  Training in a Multi-Agent Network Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moakhar%2C+A+S">Arshia Soltani Moakhar</a>, 
<a href="/search/cs?searchtype=author&query=Azizmalayeri%2C+M">Mohammad Azizmalayeri</a>, 
<a href="/search/cs?searchtype=author&query=Mirzaei%2C+H">Hossein Mirzaei</a>, 
<a href="/search/cs?searchtype=author&query=Manzuri%2C+M+T">Mohammad Taghi Manzuri</a>, 
<a href="/search/cs?searchtype=author&query=Rohban%2C+M+H">Mohammad Hossein Rohban</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Despite considerable theoretical progress in the training of neural networks
viewed as a multi-agent system of neurons, particularly concerning biological
plausibility and decentralized training, their applicability to real-world
problems remains limited due to scalability issues. In contrast,
error-backpropagation has demonstrated its effectiveness for training deep
networks in practice. In this study, we propose a local objective for neurons
that, when pursued by neurons individually, align them to exhibit similarities
to error-backpropagation in terms of efficiency and scalability during
training. For this purpose, we examine a neural network comprising
decentralized, self-interested neurons seeking to maximize their local
objective -- attention from subsequent layer neurons -- and identify the
optimal strategy for neurons. We also analyze the relationship between this
strategy and backpropagation, establishing conditions under which the derived
strategy is equivalent to error-backpropagation. Lastly, we demonstrate the
learning capacity of these multi-agent neural networks through experiments on
three datasets and showcase their superior performance relative to
error-backpropagation in a catastrophic forgetting benchmark.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09956" title="Abstract">arXiv:2310.09956</a> [<a href="/pdf/2310.09956" title="Download PDF">pdf</a>, <a href="/ps/2310.09956" title="Download PostScript">ps</a>, <a href="/format/2310.09956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tabletop Transparent Scene Reconstruction via Epipolar-Guided Optical  Flow with Monocular Depth Completion Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaotong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zheming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhuo Deng</a>, 
<a href="/search/cs?searchtype=author&query=Ghasemalizadeh%2C+O">Omid Ghasemalizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Min Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+C">Cheng-Hao Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+A">Arnie Sen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE-RAS Humanoids 2023 paper, 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Reconstructing transparent objects using affordable RGB-D cameras is a
persistent challenge in robotic perception due to inconsistent appearances
across views in the RGB domain and inaccurate depth readings in each
single-view. We introduce a two-stage pipeline for reconstructing transparent
objects tailored for mobile platforms. In the first stage, off-the-shelf
monocular object segmentation and depth completion networks are leveraged to
predict the depth of transparent objects, furnishing single-view shape prior.
Subsequently, we propose Epipolar-guided Optical Flow (EOF) to fuse several
single-view shape priors from the first stage to a cross-view consistent 3D
reconstruction given camera poses estimated from opaque part of the scene. Our
key innovation lies in EOF which employs boundary-sensitive sampling and
epipolar-line constraints into optical flow to accurately establish 2D
correspondences across multiple views on transparent objects. Quantitative
evaluations demonstrate that our pipeline significantly outperforms baseline
methods in 3D reconstruction quality, paving the way for more adept robotic
perception and interaction with transparent objects.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09961" title="Abstract">arXiv:2310.09961</a> [<a href="/pdf/2310.09961" title="Download PDF">pdf</a>, <a href="/format/2310.09961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical Evaluation of Asymmetric Shapley Values for Root-Cause  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kelen%2C+D+M">Domokos M. Kelen</a>, 
<a href="/search/cs?searchtype=author&query=Petreczky%2C+M">Mih&#xe1;ly Petreczky</a>, 
<a href="/search/cs?searchtype=author&query=Kersch%2C+P">P&#xe9;ter Kersch</a>, 
<a href="/search/cs?searchtype=author&query=Bencz%C3%BAr%2C+A+A">Andr&#xe1;s A. Bencz&#xfa;r</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, to be published in IEEE ICDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">In this work, we examine Asymmetric Shapley Values (ASV), a variant of the
popular SHAP additive local explanation method. ASV proposes a way to improve
model explanations incorporating known causal relations between variables, and
is also considered as a way to test for unfair discrimination in model
predictions. Unexplored in previous literature, relaxing symmetry in Shapley
values can have counter-intuitive consequences for model explanation. To better
understand the method, we first show how local contributions correspond to
global contributions of variance reduction. Using variance, we demonstrate
multiple cases where ASV yields counter-intuitive attributions, arguably
producing incorrect results for root-cause analysis. Second, we identify
generalized additive models (GAM) as a restricted class for which ASV exhibits
desirable properties. We support our arguments by proving multiple theoretical
results about the method. Finally, we demonstrate the use of asymmetric
attributions on multiple real-world datasets, comparing the results with and
without restricted model families using gradient boosting and deep learning
models.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09965" title="Abstract">arXiv:2310.09965</a> [<a href="/pdf/2310.09965" title="Download PDF">pdf</a>, <a href="/format/2310.09965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProteusNeRF: Fast Lightweight NeRF Editing using 3D-Aware Image Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Binglun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dutt%2C+N+S">Niladri Shekhar Dutt</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+N+J">Niloy J. Mitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Neural Radiance Fields (NeRFs) have recently emerged as a popular option for
photo-realistic object capture due to their ability to faithfully capture
high-fidelity volumetric content even from handheld video input. Although much
research has been devoted to efficient optimization leading to real-time
training and rendering, options for interactive editing NeRFs remain limited.
We present a very simple but effective neural network architecture that is fast
and efficient while maintaining a low memory footprint. This architecture can
be incrementally guided through user-friendly image-based edits. Our
representation allows straightforward object selection via semantic feature
distillation at the training stage. More importantly, we propose a local
3D-aware image context to facilitate view-consistent image editing that can
then be distilled into fine-tuned NeRFs, via geometric and appearance
adjustments. We evaluate our setup on a variety of examples to demonstrate
appearance and geometric edits and report 10-30x speedup over concurrent work
focusing on text-guided NeRF editing. Video results can be seen on our project
webpage at https://proteusnerf.github.io.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09969" title="Abstract">arXiv:2310.09969</a> [<a href="/pdf/2310.09969" title="Download PDF">pdf</a>, <a href="/format/2310.09969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Socially Acceptable Bipedal Navigation: A Signal-Temporal-Logic- Driven  Approach for Safe Locomotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shamsah%2C+A">Abdulaziz Shamsah</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Ye Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Social navigation for bipedal robots remains relatively unexplored due to the
highly complex, nonlinear dynamics of bipedal locomotion. This study presents a
preliminary exploration of social navigation for bipedal robots in a human
crowded environment. We propose a social path planner that ensures the
locomotion safety of the bipedal robot while navigating under a social norm.
The proposed planner leverages a conditional variational autoencoder
architecture and learns from human crowd datasets to produce a socially
acceptable path plan. Robot-specific locomotion safety is formally enforced by
incorporating signal temporal logic specifications during the learning process.
We demonstrate the integration of the social path planner with a model
predictive controller and a low-level passivity controller to enable
comprehensive full-body joint control of Digit in a dynamic simulation.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09971" title="Abstract">arXiv:2310.09971</a> [<a href="/pdf/2310.09971" title="Download PDF">pdf</a>, <a href="/format/2310.09971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grigsby%2C+J">Jake Grigsby</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Linxi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses
sequence models to tackle the challenges of generalization, long-term memory,
and meta-learning. Recent works have shown that off-policy learning can make
in-context RL with recurrent policies viable. Nonetheless, these approaches
require extensive tuning and limit scalability by creating key bottlenecks in
agents' memory capacity, planning horizon, and model size. AMAGO revisits and
redesigns the off-policy in-context approach to successfully train
long-sequence Transformers over entire rollouts in parallel with end-to-end RL.
Our agent is uniquely scalable and applicable to a wide range of problems. We
demonstrate its strong performance empirically in meta-RL and long-term memory
domains. AMAGO's focus on sparse rewards and off-policy data also allows
in-context learning to extend to goal-conditioned problems with challenging
exploration. When combined with a novel hindsight relabeling scheme, AMAGO can
solve a previously difficult category of open-world domains, where agents
complete many possible instructions in procedurally generated environments. We
evaluate our agent on three goal-conditioned domains and study how its
individual improvements connect to create a generalist policy.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09974" title="Abstract">arXiv:2310.09974</a> [<a href="/pdf/2310.09974" title="Download PDF">pdf</a>, <a href="/format/2310.09974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithmic Contract Design for Crowdsourced Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frangias%2C+K">Kiriaki Frangias</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+A">Andrew Lin</a>, 
<a href="/search/cs?searchtype=author&query=Vitercik%2C+E">Ellen Vitercik</a>, 
<a href="/search/cs?searchtype=author&query=Zampetakis%2C+M">Manolis Zampetakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Ranking is fundamental to many areas, such as search engine and recommender
system optimization, as well as peer grading. Crowdsourcing, which is often
used for these tasks, requires proper incentivization to ensure accurate
inputs. In this work, we draw on the field of contract theory from Economics to
propose a novel mechanism that enables a principal to accurately rank a set of
items by incentivizing agents to provide pairwise comparisons of the items. Our
mechanism implements these incentives by verifying a subset of each agent's
comparisons with a ground-truth ordering, a task we assume to be costly. The
agent is compensated (for example, monetarily or with class credit) based on
the accuracy of these comparisons. Our mechanism achieves the following
guarantees: (1) it only requires the principal to verify $O(\log s)$
comparisons, where $s$ is the total number of agents, and (2) it provably
achieves higher total utility for principal compared to ranking the items
herself with no crowdsourcing.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09977" title="Abstract">arXiv:2310.09977</a> [<a href="/pdf/2310.09977" title="Download PDF">pdf</a>, <a href="/format/2310.09977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ABACuS: All-Bank Activation Counters for Scalable and Low Overhead  RowHammer Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olgun%2C+A">Ataberk Olgun</a>, 
<a href="/search/cs?searchtype=author&query=Tugrul%2C+Y+C">Yahya Can Tugrul</a>, 
<a href="/search/cs?searchtype=author&query=Bostanci%2C+N">Nisa Bostanci</a>, 
<a href="/search/cs?searchtype=author&query=Yuksel%2C+I+E">Ismail Emir Yuksel</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haocong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Rhyner%2C+S">Steve Rhyner</a>, 
<a href="/search/cs?searchtype=author&query=Yaglikci%2C+A+G">Abdullah Giray Yaglikci</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+G+F">Geraldo F. Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in USENIX Security '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">We introduce ABACuS, a new low-cost hardware-counter-based RowHammer
mitigation technique that performance-, energy-, and area-efficiently scales
with worsening RowHammer vulnerability. We observe that both benign workloads
and RowHammer attacks tend to access DRAM rows with the same row address in
multiple DRAM banks at around the same time. Based on this observation,
ABACuS's key idea is to use a single shared row activation counter to track
activations to the rows with the same row address in all DRAM banks. Unlike
state-of-the-art RowHammer mitigation mechanisms that implement a separate row
activation counter for each DRAM bank, ABACuS implements fewer counters (e.g.,
only one) to track an equal number of aggressor rows.
<br />Our evaluations show that ABACuS securely prevents RowHammer bitflips at low
performance/energy overhead and low area cost. We compare ABACuS to four
state-of-the-art mitigation mechanisms. At a near-future RowHammer threshold of
1000, ABACuS incurs only 0.58% (0.77%) performance and 1.66% (2.12%) DRAM
energy overheads, averaged across 62 single-core (8-core) workloads, requiring
only 9.47 KiB of storage per DRAM rank. At the RowHammer threshold of 1000, the
best prior low-area-cost mitigation mechanism incurs 1.80% higher average
performance overhead than ABACuS, while ABACuS requires 2.50X smaller chip area
to implement. At a future RowHammer threshold of 125, ABACuS performs very
similarly to (within 0.38% of the performance of) the best prior performance-
and energy-efficient RowHammer mitigation mechanism while requiring 22.72X
smaller chip area. ABACuS is freely and openly available at
https://github.com/CMU-SAFARI/ABACuS.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09978" title="Abstract">arXiv:2310.09978</a> [<a href="/pdf/2310.09978" title="Download PDF">pdf</a>, <a href="/format/2310.09978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chinese Painting Style Transfer Using Deep Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weijian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Y">Yanyang Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Artistic style transfer aims to modify the style of the image while
preserving its content. Style transfer using deep learning models has been
widely studied since 2015, and most of the applications are focused on specific
artists like Van Gogh, Monet, Cezanne. There are few researches and
applications on traditional Chinese painting style transfer. In this paper, we
will study and leverage different state-of-the-art deep generative models for
Chinese painting style transfer and evaluate the performance both qualitatively
and quantitatively. In addition, we propose our own algorithm that combines
several style transfer models for our task. Specifically, we will transfer two
main types of traditional Chinese painting style, known as "Gong-bi" and
"Shui-mo" (to modern images like nature objects, portraits and landscapes.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09982" title="Abstract">arXiv:2310.09982</a> [<a href="/pdf/2310.09982" title="Download PDF">pdf</a>, <a href="/format/2310.09982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AP$n$P: A Less-constrained P$n$P Solver for Pose Estimation with Unknown  Anisotropic Scaling or Focal Lengths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jiaxin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Leutenegger%2C+S">Stefan Leutenegger</a>, 
<a href="/search/cs?searchtype=author&query=Kneip%2C+L">Laurent Kneip</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Perspective-$n$-Point (P$n$P) stands as a fundamental algorithm for pose
estimation in various applications. In this paper, we present a new approach to
the P$n$P problem with relaxed constraints, eliminating the need for precise 3D
coordinates or complete calibration data. We refer to it as AP$n$P due to its
ability to handle unknown anisotropic scaling factors of 3D coordinates or
alternatively two distinct focal lengths in addition to the conventional rigid
pose. Through algebraic manipulations and a novel parametrization, both cases
are brought into similar forms that distinguish themselves primarily by the
order of a rotation and an anisotropic scaling operation. AP$n$P furthermore
brings down both cases to an identical polynomial problem, which is solved
using the Gr\"obner basis approach. Experimental results on both simulated and
real datasets demonstrate the effectiveness of AP$n$P, providing a more
flexible and practical solution to several pose estimation tasks.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09983" title="Abstract">arXiv:2310.09983</a> [<a href="/pdf/2310.09983" title="Download PDF">pdf</a>, <a href="/format/2310.09983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Farzi Data: Autoregressive Data Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sachdeva%2C+N">Noveen Sachdeva</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zexue He</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wang-Cheng Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jianmo Ni</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D+Z">Derek Zhiyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review. 23 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">We study data distillation for auto-regressive machine learning tasks, where
the input and output have a strict left-to-right causal structure. More
specifically, we propose Farzi, which summarizes an event sequence dataset into
a small number of synthetic sequences -- Farzi Data -- which are optimized to
maintain (if not improve) model performance compared to training on the full
dataset. Under the hood, Farzi conducts memory-efficient data distillation by
(i) deriving efficient reverse-mode differentiation of the Adam optimizer by
leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional
discrete event-space into a latent-space which provably promotes implicit
regularization. Empirically, for sequential recommendation and language
modeling tasks, we are able to achieve 98-120% of downstream full-data
performance when training state-of-the-art models on Farzi Data of size as
little as 0.1% of the original dataset. Notably, being able to train better
models with significantly less data sheds light on the design of future large
auto-regressive models, and opens up new opportunities to further scale up
model and data sizes.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09985" title="Abstract">arXiv:2310.09985</a> [<a href="/pdf/2310.09985" title="Download PDF">pdf</a>, <a href="/format/2310.09985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting for Discovery: Flexible Sense-Making for AI Art-Making with  Dreamsheets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almeda%2C+S+G">Shm Garanganao Almeda</a>, 
<a href="/search/cs?searchtype=author&query=Zamfirescu-Pereira%2C+J+D">J.D. Zamfirescu-Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K+W">Kyu Won Kim</a>, 
<a href="/search/cs?searchtype=author&query=Rathnam%2C+P+M">Pradeep Mani Rathnam</a>, 
<a href="/search/cs?searchtype=author&query=Hartmann%2C+B">Bjoern Hartmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 14 figures, currently under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Design space exploration (DSE) for Text-to-Image (TTI) models entails
navigating a vast, opaque space of possible image outputs, through a
commensurately vast input space of hyperparameters and prompt text. Minor
adjustments to prompt input can surface unexpectedly disparate images. How can
interfaces support end-users in reliably steering prompt-space explorations
towards interesting results? Our design probe, DreamSheets, supports
exploration strategies with LLM-based functions for assisted prompt
construction and simultaneous display of generated results, hosted in a
spreadsheet interface. The flexible layout and novel generative functions
enable experimentation with user-defined workflows. Two studies, a preliminary
lab study and a longitudinal study with five expert artists, revealed a set of
strategies participants use to tackle the challenges of TTI design space
exploration, and the interface features required to support them - like using
text-generation to define local "axes" of exploration. We distill these
insights into a UI mockup to guide future interfaces.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09986" title="Abstract">arXiv:2310.09986</a> [<a href="/pdf/2310.09986" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Statistical Learning of Branch and Bound for Vehicle Routing  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naguib%2C+A">Andrew Naguib</a>, 
<a href="/search/cs?searchtype=author&query=Yousef%2C+W+A">Waleed A. Yousef</a>, 
<a href="/search/cs?searchtype=author&query=Traor%C3%A9%2C+I">Issa Traor&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Mamun%2C+M">Mohammed Mamun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">Recently, machine learning of the branch and bound algorithm has shown
promise in approximating competent solutions to NP-hard problems. In this
paper, we utilize and comprehensively compare the outcomes of three neural
networks--graph convolutional neural network (GCNN), GraphSAGE, and graph
attention network (GAT)--to solve the capacitated vehicle routing problem. We
train these neural networks to emulate the decision-making process of the
computationally expensive Strong Branching strategy. The neural networks are
trained on six instances with distinct topologies from the CVRPLIB and
evaluated on eight additional instances. Moreover, we reduced the minimum
number of vehicles required to solve a CVRP instance to a bin-packing problem,
which was addressed in a similar manner. Through rigorous experimentation, we
found that this approach can match or improve upon the performance of the
branch and bound algorithm with the Strong Branching strategy while requiring
significantly less computational time. The source code that corresponds to our
research findings and methodology is readily accessible and available for
reference at the following web address:
\href{https://isotlaboratory.github.io/ml4vrp/}{https://isotlaboratory.github.io/ml4vrp/}.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09987" title="Abstract">arXiv:2310.09987</a> [<a href="/pdf/2310.09987" title="Download PDF">pdf</a>, <a href="/format/2310.09987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Disruption via Continuous Batch Removal: The Case of Sicilian  Mafia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+M">Mingshan Jia</a>, 
<a href="/search/cs?searchtype=author&query=De+Meo%2C+P">Pasquale De Meo</a>, 
<a href="/search/cs?searchtype=author&query=Gabrys%2C+B">Bogdan Gabrys</a>, 
<a href="/search/cs?searchtype=author&query=Musial%2C+K">Katarzyna Musial</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Network disruption is pivotal in understanding the robustness and
vulnerability of complex networks, which is instrumental in devising strategies
for infrastructure protection, epidemic control, cybersecurity, and combating
crime. In this paper, with a particular focus on disrupting criminal networks,
we proposed to impose a within-the-largest-connected-component constraint in a
continuous batch removal disruption process. Through a series of experiments on
a recently released Sicilian Mafia network, we revealed that the constraint
would enhance degree-based methods while weakening betweenness-based
approaches. Moreover, based on the findings from the experiments using various
disruption strategies, we propose a structurally-filtered greedy disruption
strategy that integrates the effectiveness of greedy-like methods with the
efficiency of structural-metric-based approaches. The proposed strategy
significantly outperforms the longstanding state-of-the-art method of
betweenness centrality while maintaining the same time complexity.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09988" title="Abstract">arXiv:2310.09988</a> [<a href="/pdf/2310.09988" title="Download PDF">pdf</a>, <a href="/format/2310.09988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalization of CTC-based End-to-End Speech Recognition Using  Pronunciation-Driven Subword Tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Z">Zhihong Lei</a>, 
<a href="/search/cs?searchtype=author&query=Pusateri%2C+E">Ernest Pusateri</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shiyi Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Leo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingbin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+T">Tim Ng</a>, 
<a href="/search/cs?searchtype=author&query=Travadi%2C+R">Ruchir Travadi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Youyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hannemann%2C+M">Mirko Hannemann</a>, 
<a href="/search/cs?searchtype=author&query=Siu%2C+M">Man-Hung Siu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhen Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent advances in deep learning and automatic speech recognition have
improved the accuracy of end-to-end speech recognition systems, but recognition
of personal content such as contact names remains a challenge. In this work, we
describe our personalization solution for an end-to-end speech recognition
system based on connectionist temporal classification. Building on previous
work, we present a novel method for generating additional subword tokenizations
for personal entities from their pronunciations. We show that using this
technique in combination with two established techniques, contextual biasing
and wordpiece prior normalization, we are able to achieve personal named entity
accuracy on par with a competitive hybrid system.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09991" title="Abstract">arXiv:2310.09991</a> [<a href="/pdf/2310.09991" title="Download PDF">pdf</a>, <a href="/format/2310.09991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applications of Machine Learning in Biopharmaceutical Process  Development and Manufacturing: Current Trends, Challenges, and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khuat%2C+T+T">Thanh Tung Khuat</a>, 
<a href="/search/cs?searchtype=author&query=Bassett%2C+R">Robert Bassett</a>, 
<a href="/search/cs?searchtype=author&query=Otte%2C+E">Ellen Otte</a>, 
<a href="/search/cs?searchtype=author&query=Grevis-James%2C+A">Alistair Grevis-James</a>, 
<a href="/search/cs?searchtype=author&query=Gabrys%2C+B">Bogdan Gabrys</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 155 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While machine learning (ML) has made significant contributions to the
biopharmaceutical field, its applications are still in the early stages in
terms of providing direct support for quality-by-design based development and
manufacturing of biopharmaceuticals, hindering the enormous potential for
bioprocesses automation from their development to manufacturing. However, the
adoption of ML-based models instead of conventional multivariate data analysis
methods is significantly increasing due to the accumulation of large-scale
production data. This trend is primarily driven by the real-time monitoring of
process variables and quality attributes of biopharmaceutical products through
the implementation of advanced process analytical technologies. Given the
complexity and multidimensionality of a bioproduct design, bioprocess
development, and product manufacturing data, ML-based approaches are
increasingly being employed to achieve accurate, flexible, and high-performing
predictive models to address the problems of analytics, monitoring, and control
within the biopharma field. This paper aims to provide a comprehensive review
of the current applications of ML solutions in a bioproduct design, monitoring,
control, and optimisation of upstream, downstream, and product formulation
processes. Finally, this paper thoroughly discusses the main challenges related
to the bioprocesses themselves, process data, and the use of machine learning
models in biopharmaceutical process development and manufacturing. Moreover, it
offers further insights into the adoption of innovative machine learning
methods and novel trends in the development of new digital biopharma solutions.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09994" title="Abstract">arXiv:2310.09994</a> [<a href="/pdf/2310.09994" title="Download PDF">pdf</a>, <a href="/format/2310.09994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Graph and Attention Based Hyperspectral Image Classification  Methods for Remote Sensing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vats%2C+A">Aryan Vats</a>, 
<a href="/search/cs?searchtype=author&query=Suri%2C+M">Manan Suri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The use of Deep Learning techniques for classification in Hyperspectral
Imaging (HSI) is rapidly growing and achieving improved performances. Due to
the nature of the data captured by sensors that produce HSI images, a common
issue is the dimensionality of the bands that may or may not contribute to the
label class distinction. Due to the widespread nature of class labels,
Principal Component Analysis is a common method used for reducing the
dimensionality. However,there may exist methods that incorporate all bands of
the Hyperspectral image with the help of the Attention mechanism. Furthermore,
to yield better spectral spatial feature extraction, recent methods have also
explored the usage of Graph Convolution Networks and their unique ability to
use node features in prediction, which is akin to the pixel spectral makeup. In
this survey we present a comprehensive summary of Graph based and Attention
based methods to perform Hyperspectral Image Classification for remote sensing
and aerial HSI images. We also summarize relevant datasets on which these
techniques have been evaluated and benchmark the processing techniques.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09997" title="Abstract">arXiv:2310.09997</a> [<a href="/pdf/2310.09997" title="Download PDF">pdf</a>, <a href="/format/2310.09997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiralerspong%2C+T">Thomas Jiralerspong</a>, 
<a href="/search/cs?searchtype=author&query=Kondrup%2C+F">Flemming Kondrup</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>, 
<a href="/search/cs?searchtype=author&query=Khetarpal%2C+K">Khimya Khetarpal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">The ability to plan at many different levels of abstraction enables agents to
envision the long-term repercussions of their decisions and thus enables
sample-efficient learning. This becomes particularly beneficial in complex
environments from high-dimensional state space such as pixels, where the goal
is distant and the reward sparse. We introduce Forecaster, a deep hierarchical
reinforcement learning approach which plans over high-level goals leveraging a
temporally abstract world model. Forecaster learns an abstract model of its
environment by modelling the transitions dynamics at an abstract level and
training a world model on such transition. It then uses this world model to
choose optimal high-level goals through a tree-search planning procedure. It
additionally trains a low-level policy that learns to reach those goals. Our
method not only captures building world models with longer horizons, but also,
planning with such models in downstream tasks. We empirically demonstrate
Forecaster's potential in both single-task learning and generalization to new
tasks in the AntMaze domain.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10001" title="Abstract">arXiv:2310.10001</a> [<a href="/pdf/2310.10001" title="Download PDF">pdf</a>, <a href="/format/2310.10001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auditing Targeted Political Advertising on Social Media During the 2021  German Election
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%A4r%2C+D">Dominik B&#xe4;r</a>, 
<a href="/search/cs?searchtype=author&query=Pierri%2C+F">Francesco Pierri</a>, 
<a href="/search/cs?searchtype=author&query=De+Francisci+Morales%2C+G">Gianmarco De Francisci Morales</a>, 
<a href="/search/cs?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Both authors contributed equally to this research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Political advertising on social media has become a central element in
election campaigns. However, granular information about political advertising
on social media was previously unavailable, thus raising concerns regarding
fairness, accountability, and transparency in electoral processes. In this
paper, we analyze targeted political advertising on social media using a
unique, large-scale dataset of over 80000 political ads from Meta during the
2021 German federal election, with more than 1.1 billion impressions. For each
political ad, our dataset records granular information about targeting
strategies, spending, and actual impressions. We then study (i) the prevalence
of targeted ads across the political spectrum; (ii) the discrepancies between
targeted and actual audiences due to algorithmic distribution; and (iii) what
makes an efficient targeting strategy on social media. We find that targeted
ads are prevalent across the entire political spectrum, with considerable
differences in strategies and efficiency between the political left and right.
Furthermore, there are significant discrepancies between the targeted and
actual audience, which vary across parties. Notably, the efficiency of
political ads (as measured by impressions per EUR) is particularly high when
ads are targeted at a broad audience, or published by far-right parties - which
raises important fairness concerns. Overall, our work contributes to a better
understanding of targeted political advertising on social media and informs
policymakers about the design of effective regulatory frameworks to promote
fairness, accountability, and transparency.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10008" title="Abstract">arXiv:2310.10008</a> [<a href="/pdf/2310.10008" title="Download PDF">pdf</a>, <a href="/format/2310.10008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Unified and Effective Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+K">Kaixiong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xiaohan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+F">Fangrui Lv</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiangyu Yue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Website: <a href="https://invictus717.github.io/Generalization/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose $\textbf{UniDG}$, a novel and $\textbf{Uni}$fied framework for
$\textbf{D}$omain $\textbf{G}$eneralization that is capable of significantly
enhancing the out-of-distribution generalization performance of foundation
models regardless of their architectures. The core idea of UniDG is to finetune
models during the inference stage, which saves the cost of iterative training.
Specifically, we encourage models to learn the distribution of test data in an
unsupervised manner and impose a penalty regarding the updating step of model
parameters. The penalty term can effectively reduce the catastrophic forgetting
issue as we would like to maximally preserve the valuable knowledge in the
original model. Empirically, across 12 visual backbones, including CNN-, MLP-,
and Transformer-based models, ranging from 1.89M to 303M parameters, UniDG
shows an average accuracy improvement of +5.4% on DomainBed. These performance
results demonstrate the superiority and versatility of UniDG. The code is
publicly available at https://github.com/invictus717/UniDG
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10010" title="Abstract">arXiv:2310.10010</a> [<a href="/pdf/2310.10010" title="Download PDF">pdf</a>, <a href="/format/2310.10010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Black-box Targeted Adversarial Attack on Segment Anything (SAM)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sheng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoning Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep recognition models are widely vulnerable to adversarial examples, which
change the model output by adding quasi-imperceptible perturbation to the image
input. Recently, Segment Anything Model (SAM) has emerged to become a popular
foundation model in computer vision due to its impressive generalization to
unseen data and tasks. Realizing flexible attacks on SAM is beneficial for
understanding the robustness of SAM in the adversarial context. To this end,
this work aims to achieve a targeted adversarial attack (TAA) on SAM.
Specifically, under a certain prompt, the goal is to make the predicted mask of
an adversarial example resemble that of a given target image. The task of TAA
on SAM has been realized in a recent arXiv work in the white-box setup by
assuming access to prompt and model, which is thus less practical. To address
the issue of prompt dependence, we propose a simple yet effective approach by
only attacking the image encoder. Moreover, we propose a novel regularization
loss to enhance the cross-model transferability by increasing the feature
dominance of adversarial images over random natural images. Extensive
experiments verify the effectiveness of our proposed simple techniques to
conduct a successful black-box TAA on SAM.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10012" title="Abstract">arXiv:2310.10012</a> [<a href="/pdf/2310.10012" title="Download PDF">pdf</a>, <a href="/format/2310.10012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion  Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y">Yu-Lin Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chia-Yi Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chulin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chih-Hsun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jia-You Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chia-Mu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chun-Ying Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion
(SD), have recently demonstrated exceptional capabilities for generating
high-quality content. However, this progress has raised several concerns of
potential misuse, particularly in creating copyrighted, prohibited, and
restricted content, or NSFW (not safe for work) images. While efforts have been
made to mitigate such problems, either by implementing a safety filter at the
evaluation stage or by fine-tuning models to eliminate undesirable concepts or
styles, the effectiveness of these safety measures in dealing with a wide range
of prompts remains largely unexplored. In this work, we aim to investigate
these safety mechanisms by proposing one novel concept retrieval algorithm for
evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I
diffusion models, where the whole evaluation can be prepared in advance without
prior knowledge of the target model. Specifically, Ring-A-Bell first performs
concept extraction to obtain holistic representations for sensitive and
inappropriate concepts. Subsequently, by leveraging the extracted concept,
Ring-A-Bell automatically identifies problematic prompts for diffusion models
with the corresponding generation of inappropriate content, allowing the user
to assess the reliability of deployed safety mechanisms. Finally, we
empirically validate our method by testing online services such as Midjourney
and various methods of concept removal. Our results show that Ring-A-Bell, by
manipulating safe prompting benchmarks, can transform prompts that were
originally regarded as safe to evade existing safety mechanisms, thus revealing
the defects of the so-called safety mechanisms which could practically lead to
the generation of harmful contents.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10015" title="Abstract">arXiv:2310.10015</a> [<a href="/pdf/2310.10015" title="Download PDF">pdf</a>, <a href="/format/2310.10015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Reality of the Situation: A Survey of Situated Analytics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Sungbok Shin</a>, 
<a href="/search/cs?searchtype=author&query=Batch%2C+A">Andrea Batch</a>, 
<a href="/search/cs?searchtype=author&query=Butcher%2C+P+W+S">Peter W. S. Butcher</a>, 
<a href="/search/cs?searchtype=author&query=Ritsos%2C+P+D">Panagiotis D. Ritsos</a>, 
<a href="/search/cs?searchtype=author&query=Elmqvist%2C+N">Niklas Elmqvist</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The advent of low cost, accessible, and high performance augmented reality
(AR) has shed light on a situated form of analytics where in-situ
visualizations embedded in the real world can facilitate sensemaking based on
the user's physical location. In this work, we identify prior literature in
this emerging field with a focus on situated analytics. After collecting 47
relevant situated analytics systems, we classify them using a taxonomy of three
dimensions: situating triggers, view situatedness, and data depiction. We then
identify four archetypical patterns in our classification using an ensemble
cluster analysis. We also assess the level which these systems support the
sensemaking process. Finally, we discuss insights and design guidelines that we
learned from our analysis.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10016" title="Abstract">arXiv:2310.10016</a> [<a href="/pdf/2310.10016" title="Download PDF">pdf</a>, <a href="/ps/2310.10016" title="Download PostScript">ps</a>, <a href="/format/2310.10016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Scalable Cross-Chain Messaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chervinski%2C+J+O">Joao Otavio Chervinski</a>, 
<a href="/search/cs?searchtype=author&query=Kreutz%2C+D">Diego Kreutz</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiangshan Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Blockchains were originally designed as closed execution environments and
lack the ability to communicate directly with external systems. To overcome
this limitation, many blockchains employ relayers, external applications
capable of transporting data between different blockchains. Typically, the
process of relaying data is permissionless and multiple independent relayers
work concurrently to transport the same information between two blockchains.
While this model increases the reliability of data delivery by providing
redundancy, it also introduces challenges that have not been previously
discussed. In this work, we bridge this gap by discussing the shortcomings of
permissionless cross-chain relaying systems and identifying three issues that
adversely impact their performance, scalability and security. We take the first
step towards addressing issues that hinder performance and scalability by
proposing a novel protocol to enable coordination among independent relayers.
Additionally, we provide an in-depth discussion about the trade-offs associated
with the design of relayer coordination protocols for permissionless settings.
Through this work we provide a foundation for improving cross-chain relaying
services.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10021" title="Abstract">arXiv:2310.10021</a> [<a href="/pdf/2310.10021" title="Download PDF">pdf</a>, <a href="/format/2310.10021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bootstrap Your Own Skills: Learning to Solve New Tasks with Large  Language Model Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jesse Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiahui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pertsch%2C+K">Karl Pertsch</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Minsuk Chang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shao-Hua Sun</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+J+J">Joseph J. Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoRL 2023 (Oral); 24 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose BOSS, an approach that automatically learns to solve new
long-horizon, complex, and meaningful tasks by growing a learned skill library
with minimal supervision. Prior work in reinforcement learning require expert
supervision, in the form of demonstrations or rich reward functions, to learn
long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills)
learns to accomplish new tasks by performing "skill bootstrapping," where an
agent with a set of primitive skills interacts with the environment to practice
new skills without receiving reward feedback for tasks outside of the initial
skill set. This bootstrapping phase is guided by large language models (LLMs)
that inform the agent of meaningful skills to chain together. Through this
process, BOSS builds a wide range of complex and useful behaviors from a basic
set of primitive skills. We demonstrate through experiments in realistic
household environments that agents trained with our LLM-guided bootstrapping
procedure outperform those trained with naive bootstrapping as well as prior
unsupervised skill acquisition methods on zero-shot execution of unseen,
long-horizon tasks in new environments. Website at clvrai.com/boss.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10022" title="Abstract">arXiv:2310.10022</a> [<a href="/pdf/2310.10022" title="Download PDF">pdf</a>, <a href="/format/2310.10022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of Super-resolution on Low-resolution  Micro-expression Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Ling Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingpei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaohua Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wenming Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Q">Qirong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Guoying Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Micro-expression recognition (MER) in low-resolution (LR) scenarios presents
an important and complex challenge, particularly for practical applications
such as group MER in crowded environments. Despite considerable advancements in
super-resolution techniques for enhancing the quality of LR images and videos,
few study has focused on investigate super-resolution for improving LR MER. The
scarcity of investigation can be attributed to the inherent difficulty in
capturing the subtle motions of micro-expressions, even in original-resolution
MER samples, which becomes even more challenging in LR samples due to the loss
of distinctive features. Furthermore, a lack of systematic benchmarking and
thorough analysis of super-resolution-assisted MER methods has been noted. This
paper tackles these issues by conducting a series of benchmark experiments that
integrate both super-resolution (SR) and MER methods, guided by an in-depth
literature survey. Specifically, we employ seven cutting-edge state-of-the-art
(SOTA) MER techniques and evaluate their performance on samples generated from
13 SOTA SR techniques, thereby addressing the problem of super-resolution in
MER. Through our empirical study, we uncover the primary challenges associated
with SR-assisted MER and identify avenues to tackle these challenges by
leveraging recent advancements in both SR and MER methodologies. Our analysis
provides insights for progressing toward more efficient SR-assisted MER.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10023" title="Abstract">arXiv:2310.10023</a> [<a href="/pdf/2310.10023" title="Download PDF">pdf</a>, <a href="/format/2310.10023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D-BBS: Global Localization for 3D Point Cloud Scan Matching Using  Branch-and-Bound Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aoki%2C+K">Koki Aoki</a>, 
<a href="/search/cs?searchtype=author&query=Koide%2C+K">Kenji Koide</a>, 
<a href="/search/cs?searchtype=author&query=Oishi%2C+S">Shuji Oishi</a>, 
<a href="/search/cs?searchtype=author&query=Yokozuka%2C+M">Masashi Yokozuka</a>, 
<a href="/search/cs?searchtype=author&query=Banno%2C+A">Atsuhiko Banno</a>, 
<a href="/search/cs?searchtype=author&query=Meguro%2C+J">Junichi Meguro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents an accurate and fast 3D global localization method,
3D-BBS, that extends the existing branch-and-bound (BnB)-based 2D scan matching
(BBS) algorithm. To reduce memory consumption, we utilize a sparse hash table
for storing hierarchical 3D voxel maps. To improve the processing cost of BBS
in 3D space, we propose an efficient roto-translational space branching and
best-first search strategy. Furthermore, we devise a batched BnB algorithm to
fully leverage GPU parallel processing. Through experiments in simulated and
real environments, we demonstrated that the 3D-BBS enabled accurate global
localization with only a 3D LiDAR scan and a 3D pre-built map. This method
required only 878 msec on average to perform global localization and
outperformed state-of-the-art feature-matching-based global localization
methods in terms of accuracy and processing speed.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10025" title="Abstract">arXiv:2310.10025</a> [<a href="/pdf/2310.10025" title="Download PDF">pdf</a>, <a href="/format/2310.10025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-Scale Interest Extraction Framework with Self-Supervision for  Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liangliang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongzhan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jinshan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guang Chen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 26th European Conference on Artificial Intelligence ECAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">In the sequential recommendation task, the recommender generally learns
multiple embeddings from a user's historical behaviors, to catch the diverse
interests of the user. Nevertheless, the existing approaches just extract each
interest independently for the corresponding sub-sequence while ignoring the
global correlation of the entire interaction sequence, which may fail to
capture the user's inherent preference for the potential interests
generalization and unavoidably make the recommended items homogeneous with the
historical behaviors. In this paper, we propose a novel Dual-Scale Interest
Extraction framework (DSIE) to precisely estimate the user's current interests.
Specifically, DSIE explicitly models the user's inherent preference with
contrastive learning by attending over his/her entire interaction sequence at
the global scale and catches the user's diverse interests in a fine granularity
at the local scale. Moreover, we develop a novel interest aggregation module to
integrate the multi-interests according to the inherent preference to generate
the user's current interests for the next-item prediction. Experiments
conducted on three real-world benchmark datasets demonstrate that DSIE
outperforms the state-of-the-art models in terms of recommendation preciseness
and novelty.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10027" title="Abstract">arXiv:2310.10027</a> [<a href="/pdf/2310.10027" title="Download PDF">pdf</a>, <a href="/format/2310.10027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoomDesigner: Encoding Anchor-latents for Style-consistent and  Shape-compatible Indoor Scene Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiqun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zibo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+S">Sixun Dong</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shenghua Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3DV 2024. project: <a href="https://github.com/zhao-yiqun/RoomDesigner">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Indoor scene generation aims at creating shape-compatible, style-consistent
furniture arrangements within a spatially reasonable layout. However, most
existing approaches primarily focus on generating plausible furniture layouts
without incorporating specific details related to individual furniture pieces.
To address this limitation, we propose a two-stage model integrating shape
priors into the indoor scene generation by encoding furniture as anchor latent
representations. In the first stage, we employ discrete vector quantization to
encode furniture pieces as anchor-latents. Based on the anchor-latents
representation, the shape and location information of the furniture was
characterized by a concatenation of location, size, orientation, class, and our
anchor latent. In the second stage, we leverage a transformer model to predict
indoor scenes autoregressively. Thanks to incorporating the proposed
anchor-latents representations, our generative model produces shape-compatible
and style-consistent furniture arrangements and synthesis furniture in diverse
shapes. Furthermore, our method facilitates various human interaction
applications, such as style-consistent scene completion, object mismatch
correction, and controllable object-level editing. Experimental results on the
3D-Front dataset demonstrate that our approach can generate more consistent and
compatible indoor scenes compared to existing methods, even without shape
retrieval. Additionally, extensive ablation studies confirm the effectiveness
of our design choices in the indoor scene generation model.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10029" title="Abstract">arXiv:2310.10029</a> [<a href="/pdf/2310.10029" title="Download PDF">pdf</a>, <a href="/format/2310.10029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Human Motion Compensation Framework for a Supernumerary Robotic Arm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a> (1, 2 and 3), 
<a href="/search/cs?searchtype=author&query=Balatti%2C+P">Pietro Balatti</a> (3), 
<a href="/search/cs?searchtype=author&query=Leonori%2C+M">Mattia Leonori</a> (3), 
<a href="/search/cs?searchtype=author&query=Ajoudani%2C+A">Arash Ajoudani</a> (3) ((1) State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China (2) Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China (3) Human-Robot Interfaces and Interaction Lab, Istituto Italiano di Tecnologia, Genoa, Italy)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures, accepted by 2023 IEEE-RAS International Conference on Humanoid Robots (Humanoids)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Supernumerary robotic arms (SRAs) can be used as the third arm to complement
and augment the abilities of human users. The user carrying a SRA forms a
connected kinodynamic chain, which can be viewed as a special class of
floating-base robot systems. However, unlike general floating-base robot
systems, human users are the bases of SRAs and they have their subjective
behaviors/motions. This implies that human body motions can unintentionally
affect the SRA's end-effector movements. To address this challenge, we propose
a framework to compensate for the human whole-body motions that interfere with
the SRA's end-effector trajectories. The SRA system in this study consists of a
6-degree-of-freedom lightweight arm and a wearable interface. The wearable
interface allows users to adjust the installation position of the SRA to fit
different body shapes. An inertial measurement unit (IMU)-based sensory
interface can provide the body skeleton motion feedback of the human user in
real time. By simplifying the floating-base kinematics model, we design an
effective motion planner by reconstructing the Jacobian matrix of the SRA.
Under the proposed framework, the performance of the reconstructed Jacobian
method is assessed by comparing the results obtained with the classical
nullspace-based method through two sets of experiments.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10030" title="Abstract">arXiv:2310.10030</a> [<a href="/pdf/2310.10030" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unraveling Fundamental Properties of Power System Resilience Curves  using Unsupervised Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Mostafavi%2C+A">Ali Mostafavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The standard model of infrastructure resilience, the resilience triangle, has
been the primary way of characterizing and quantifying infrastructure
resilience. However, the theoretical model merely provides a one-size-fits-all
framework for all infrastructure systems. Most of the existing studies examine
the characteristics of infrastructure resilience curves based on analytical
models constructed upon simulated system performance. Limited empirical studies
hindered our ability to fully understand and predict resilience characteristics
in infrastructure systems. To address this gap, this study examined over 200
resilience curves related to power outages in three major extreme weather
events. Using unsupervised machine learning, we examined different curve
archetypes, as well as the fundamental properties of each resilience curve
archetype. The results show two primary archetypes for power system resilience
curves, triangular, and trapezoidal curves. Triangular curves characterize
resilience behavior based on 1. critical functionality threshold, 2. critical
functionality recovery rate, and 3. recovery pivot point. Trapezoidal
archetypes explain resilience curves based on 1. duration of sustained function
loss and 2. constant recovery rate. The longer the duration of sustained
function loss, the slower the constant rate of recovery. The findings of this
study provide novel perspectives enabling better understanding and prediction
of resilience performance of power system infrastructures.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10033" title="Abstract">arXiv:2310.10033</a> [<a href="/pdf/2310.10033" title="Download PDF">pdf</a>, <a href="/format/2310.10033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Unfolding Network for Image Compressed Sensing by Content-adaptive  Gradient Updating and Deformation-invariant Non-local Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+W">Wenxue Cui</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiaopeng Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Debin Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 13 figures. Accepted by IEEE Transactions on Multimedia (TMM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Inspired by certain optimization solvers, the deep unfolding network (DUN)
has attracted much attention in recent years for image compressed sensing (CS).
However, there still exist the following two issues: 1) In existing DUNs, most
hyperparameters are usually content independent, which greatly limits their
adaptability for different input contents. 2) In each iteration, a plain
convolutional neural network is usually adopted, which weakens the perception
of wider context prior and therefore depresses the expressive ability. In this
paper, inspired by the traditional Proximal Gradient Descent (PGD) algorithm, a
novel DUN for image compressed sensing (dubbed DUN-CSNet) is proposed to solve
the above two issues. Specifically, for the first issue, a novel content
adaptive gradient descent network is proposed, in which a well-designed step
size generation sub-network is developed to dynamically allocate the
corresponding step sizes for different textures of input image by generating a
content-aware step size map, realizing a content-adaptive gradient updating.
For the second issue, considering the fact that many similar patches exist in
an image but have undergone a deformation, a novel deformation-invariant
non-local proximal mapping network is developed, which can adaptively build the
long-range dependencies between the nonlocal patches by deformation-invariant
non-local modeling, leading to a wider perception on context priors. Extensive
experiments manifest that the proposed DUN-CSNet outperforms existing
state-of-the-art CS methods by large margins.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10035" title="Abstract">arXiv:2310.10035</a> [<a href="/pdf/2310.10035" title="Download PDF">pdf</a>, <a href="/format/2310.10035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical Study of Zero-Shot NER with ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tingyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongwei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) exhibited powerful capability in various natural
language processing tasks. This work focuses on exploring LLM performance on
zero-shot information extraction, with a focus on the ChatGPT and named entity
recognition (NER) task. Inspired by the remarkable reasoning capability of LLM
on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods
to NER and propose reasoning strategies tailored for NER. First, we explore a
decomposed question-answering paradigm by breaking down the NER task into
simpler subproblems by labels. Second, we propose syntactic augmentation to
stimulate the model's intermediate thinking in two ways: syntactic prompting,
which encourages the model to analyze the syntactic structure itself, and tool
augmentation, which provides the model with the syntactic information generated
by a parsing tool. Besides, we adapt self-consistency to NER by proposing a
two-stage majority voting strategy, which first votes for the most consistent
mentions, then the most consistent types. The proposed methods achieve
remarkable improvements for zero-shot NER across seven benchmarks, including
Chinese and English datasets, and on both domain-specific and general-domain
scenarios. In addition, we present a comprehensive analysis of the error types
with suggestions for optimization directions. We also verify the effectiveness
of the proposed methods on the few-shot setting and other LLMs.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10036" title="Abstract">arXiv:2310.10036</a> [<a href="/pdf/2310.10036" title="Download PDF">pdf</a>, <a href="/format/2310.10036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evading Detection Actively: Toward Anti-Forensics against Forgery  Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+L">Long Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shenghai Luo</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+S">Shunquan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Han Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bin Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiwu Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Anti-forensics seeks to eliminate or conceal traces of tampering artifacts.
Typically, anti-forensic methods are designed to deceive binary detectors and
persuade them to misjudge the authenticity of an image. However, to the best of
our knowledge, no attempts have been made to deceive forgery detectors at the
pixel level and mis-locate forged regions. Traditional adversarial attack
methods cannot be directly used against forgery localization due to the
following defects: 1) they tend to just naively induce the target forensic
models to flip their pixel-level pristine or forged decisions; 2) their
anti-forensics performance tends to be severely degraded when faced with the
unseen forensic models; 3) they lose validity once the target forensic models
are retrained with the anti-forensics images generated by them. To tackle the
three defects, we propose SEAR (Self-supErvised Anti-foRensics), a novel
self-supervised and adversarial training algorithm that effectively trains
deep-learning anti-forensic models against forgery localization. SEAR sets a
pretext task to reconstruct perturbation for self-supervised learning. In
adversarial training, SEAR employs a forgery localization model as a supervisor
to explore tampering features and constructs a deep-learning concealer to erase
corresponding traces. We have conducted largescale experiments across diverse
datasets. The experimental results demonstrate that, through the combination of
self-supervised learning and adversarial learning, SEAR successfully deceives
the state-of-the-art forgery localization methods, as well as tackle the three
defects regarding traditional adversarial attack methods mentioned above.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10038" title="Abstract">arXiv:2310.10038</a> [<a href="/pdf/2310.10038" title="Download PDF">pdf</a>, <a href="/format/2310.10038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart City Transportation: Deep Learning Ensemble Approach for Traffic  Accident Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adewopo%2C+V">Victor Adewopo</a>, 
<a href="/search/cs?searchtype=author&query=Elsayed%2C+N">Nelly Elsayed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The dynamic and unpredictable nature of road traffic necessitates effective
accident detection methods for enhancing safety and streamlining traffic
management in smart cities. This paper offers a comprehensive exploration study
of prevailing accident detection techniques, shedding light on the nuances of
other state-of-the-art methodologies while providing a detailed overview of
distinct traffic accident types like rear-end collisions, T-bone collisions,
and frontal impact accidents. Our novel approach introduces the I3D-CONVLSTM2D
model architecture, a lightweight solution tailored explicitly for accident
detection in smart city traffic surveillance systems by integrating RGB frames
with optical flow information. Our experimental study's empirical analysis
underscores our approach's efficacy, with the I3D-CONVLSTM2D RGB + Optical-Flow
(Trainable) model outperforming its counterparts, achieving an impressive 87\%
Mean Average Precision (MAP). Our findings further elaborate on the challenges
posed by data imbalances, particularly when working with a limited number of
datasets, road structures, and traffic scenarios. Ultimately, our research
illuminates the path towards a sophisticated vision-based accident detection
system primed for real-time integration into edge IoT devices within smart
urban infrastructures.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10039" title="Abstract">arXiv:2310.10039</a> [<a href="/pdf/2310.10039" title="Download PDF">pdf</a>, <a href="/format/2310.10039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TpopT: Efficient Trainable Template Optimization on Low-Dimensional  Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jingkai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X+R">Xinyu Rain Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jimmy Wang</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A1rka%2C+Z">Zsuzsanna M&#xe1;rka</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A1rka%2C+S">Szabolcs M&#xe1;rka</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+J">John Wright</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In scientific and engineering scenarios, a recurring task is the detection of
low-dimensional families of signals or patterns. A classic family of
approaches, exemplified by template matching, aims to cover the search space
with a dense template bank. While simple and highly interpretable, it suffers
from poor computational efficiency due to unfavorable scaling in the signal
space dimensionality. In this work, we study TpopT (TemPlate OPTimization) as
an alternative scalable framework for detecting low-dimensional families of
signals which maintains high interpretability. We provide a theoretical
analysis of the convergence of Riemannian gradient descent for TpopT, and prove
that it has a superior dimension scaling to covering. We also propose a
practical TpopT framework for nonparametric signal sets, which incorporates
techniques of embedding and kernel interpolation, and is further configurable
into a trainable network architecture by unrolled optimization. The proposed
trainable TpopT exhibits significantly improved efficiency-accuracy tradeoffs
for gravitational wave detection, where matched filtering is currently a method
of choice. We further illustrate the general applicability of this approach
with experiments on handwritten digit data.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10041" title="Abstract">arXiv:2310.10041</a> [<a href="/pdf/2310.10041" title="Download PDF">pdf</a>, <a href="/ps/2310.10041" title="Download PostScript">ps</a>, <a href="/format/2310.10041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolution quadratures based on block generalized Adams methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+L">Ling Liu</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+J">Junjie Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages , 11 figures , 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper studies an extension of the classical convolution quadrature, a
well-known numerical method for calculation of convolution integrals. In
contrast to the existing counterpart, which uses the linear multistep formula
or Runge-Kutta method, we employ the block generalized Adams method to
discretize the underlying initial value problem. Similar to the convolution
quadrature method based on the linear multistep formula, the proposed method
can also be implemented on an equispaced grid. In addition, the proposed
high-order method is as stable as the convolution quadrature based on the
Runge-Kutta method, which indicates that it can accurately solve a wide range
of problems without becoming unstable. We provide a detailed convergence
analysis for the proposed convolution quadrature method and numerically
illustrate our theoretical findings for convolution integrals with smooth and
weakly singular kernels.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10045" title="Abstract">arXiv:2310.10045</a> [<a href="/pdf/2310.10045" title="Download PDF">pdf</a>, <a href="/format/2310.10045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetrical SyncMap for Imbalanced General Chunking Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Heng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+D+V">Danilo Vasconcellos Vargas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 19 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Heng Zhang, Danilo Vasconcellos Vargas, Symmetrical SyncMap for
  imbalanced general chunking problems, Physica D: Nonlinear Phenomena, Volume
  456, 2023, 133923, ISSN 0167-2789
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Recently, SyncMap pioneered an approach to learn complex structures from
sequences as well as adapt to any changes in underlying structures. This is
achieved by using only nonlinear dynamical equations inspired by neuron group
behaviors, i.e., without loss functions. Here we propose Symmetrical SyncMap
that goes beyond the original work to show how to create dynamical equations
and attractor-repeller points which are stable over the long run, even dealing
with imbalanced continual general chunking problems (CGCPs). The main idea is
to apply equal updates from negative and positive feedback loops by symmetrical
activation. We then introduce the concept of memory window to allow for more
positive updates. Our algorithm surpasses or ties other unsupervised
state-of-the-art baselines in all 12 imbalanced CGCPs with various
difficulties, including dynamically changing ones. To verify its performance in
real-world scenarios, we conduct experiments on several well-studied structure
learning problems. The proposed method surpasses substantially other methods in
3 out of 4 scenarios, suggesting that symmetrical activation plays a critical
role in uncovering topological structures and even hierarchies encoded in
temporal data.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10046" title="Abstract">arXiv:2310.10046</a> [<a href="/pdf/2310.10046" title="Download PDF">pdf</a>, <a href="/format/2310.10046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRANSOM: An Efficient Fault-Tolerant System for Training LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Baodong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingping Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kangyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yongqiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tieyao Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shigang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) represented by chartGPT have achieved profound
applications and breakthroughs in various fields. This demonstrates that LLMs
with hundreds of billions or trillions of parameters will continue to transform
our daily lives. However, training LLMs with super-large-scale parameters
requires even larger and high-performance GPU clusters and continuous training
periods lasting for months. Due to the inevitable hardware and software
failures in large clusters, maintaining large-scale training sessions lasting
more than a week has become extremely challenging. A significant amount of time
is spent on tasks such as checkpoint saving and recovery, task restart
submissions, and task anomaly checks, greatly reducing the efficiency of
effective training. To address these issues, a novel fault-tolerant large model
training system has been proposed, which we named TRANSOM. In this work, we
have designed three key components: the Training pipeline Automatic Fault
Tolerance and Recovery Mechanism (TOL), the Training Task Multi-dimensional
Metric Automatic Anomaly Detection System (TEE), and the Training Checkpoint
Asynchronous Access Automatic Fault Tolerance and Recovery Technology (TCE).
Our preliminary results indicate that TRANSOM significantly accelerates the
efficiency of large-scale LLMs training on clusters. For instance, the
pre-training time for GPT-3 with 175B parameters has been reduced by 28%, and
the checkpoint storage and recovery performance has improved by a factor of 20.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10047" title="Abstract">arXiv:2310.10047</a> [<a href="/pdf/2310.10047" title="Download PDF">pdf</a>, <a href="/format/2310.10047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Large Language Model Fine-tuning for Solving Math Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Avi Singh</a>, 
<a href="/search/cs?searchtype=author&query=Freeman%2C+C+D">C. Daniel Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Co-Reyes%2C+J+D">John D. Co-Reyes</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P+J">Peter J. Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite their success in many natural language tasks, solving math problems
remains a significant challenge for large language models (LLMs). A large gap
exists between LLMs' pass-at-one and pass-at-N performance in solving math
problems, suggesting LLMs might be close to finding correct solutions,
motivating our exploration of fine-tuning methods to unlock LLMs' performance.
Using the challenging MATH dataset, we investigate three fine-tuning
strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed
solution for a given math problem; (2) solution-cluster re-ranking, where the
LLM is fine-tuned as a solution verifier/evaluator to choose among generated
candidate solution clusters; (3) multi-task sequential fine-tuning, which
integrates both solution generation and evaluation tasks together efficiently
to enhance the LLM performance. With these methods, we present a thorough
empirical study on a series of PaLM 2 models and find: (1) The quality and
style of the step-by-step solutions used for fine-tuning can make a significant
impact on the model performance; (2) While solution re-ranking and majority
voting are both effective for improving the model performance when used
separately, they can also be used together for an even greater performance
boost; (3) Multi-task fine-tuning that sequentially separates the solution
generation and evaluation tasks can offer improved performance compared with
the solution fine-tuning baseline. Guided by these insights, we design a
fine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset
with fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the
few-shot performance of pre-trained PaLM 2-L model with majority voting.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10049" title="Abstract">arXiv:2310.10049</a> [<a href="/pdf/2310.10049" title="Download PDF">pdf</a>, <a href="/format/2310.10049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FATE-LLM: A Industrial Grade Federated Learning Framework for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+T">Tao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yan Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+G">Guoqiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weijing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wenbin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lixin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qiang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have
exhibited remarkable performances across various tasks in recent years.
However, LLMs face two main challenges in real-world applications. One
challenge is that training LLMs consumes vast computing resources, preventing
LLMs from being adopted by small and medium-sized enterprises with limited
computing resources. Another is that training LLM requires a large amount of
high-quality data, which are often scattered among enterprises. To address
these challenges, we propose FATE-LLM, an industrial-grade federated learning
framework for large language models. FATE-LLM (1) facilitates federated
learning for large language models (coined FedLLM); (2) promotes efficient
training of FedLLM using parameter-efficient fine-tuning methods; (3) protects
the intellectual property of LLMs; (4) preserves data privacy during training
and inference through privacy-preserving mechanisms. We release the code of
FATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research
of FedLLM and enable a broad range of industrial applications.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10050" title="Abstract">arXiv:2310.10050</a> [<a href="/pdf/2310.10050" title="Download PDF">pdf</a>, <a href="/format/2310.10050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EfficientOCR: An Extensible, Open-Source Package for Efficiently  Digitizing World Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bryan%2C+T">Tom Bryan</a>, 
<a href="/search/cs?searchtype=author&query=Carlson%2C+J">Jacob Carlson</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Abhishek Arora</a>, 
<a href="/search/cs?searchtype=author&query=Dell%2C+M">Melissa Dell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); General Economics (econ.GN)

</div>
<p class="mathjax">Billions of public domain documents remain trapped in hard copy or lack an
accurate digitization. Modern natural language processing methods cannot be
used to index, retrieve, and summarize their texts; conduct computational
textual analyses; or extract information for statistical analyses, and these
texts cannot be incorporated into language model training. Given the diversity
and sheer quantity of public domain texts, liberating them at scale requires
optical character recognition (OCR) that is accurate, extremely cheap to
deploy, and sample-efficient to customize to novel collections, languages, and
character sets. Existing OCR engines, largely designed for small-scale
commercial applications in high resource languages, often fall short of these
requirements. EffOCR (EfficientOCR), a novel open-source OCR package, meets
both the computational and sample efficiency requirements for liberating texts
at scale by abandoning the sequence-to-sequence architecture typically used for
OCR, which takes representations from a learned vision model as inputs to a
learned language model. Instead, EffOCR models OCR as a character or word-level
image retrieval problem. EffOCR is cheap and sample efficient to train, as the
model only needs to learn characters' visual appearance and not how they are
used in sequence to form language. Models in the EffOCR model zoo can be
deployed off-the-shelf with only a few lines of code. Importantly, EffOCR also
allows for easy, sample efficient customization with a simple model training
interface and minimal labeling requirements due to its sample efficiency. We
illustrate the utility of EffOCR by cheaply and accurately digitizing 20
million historical U.S. newspaper scans, evaluating zero-shot performance on
randomly selected documents from the U.S. National Archives, and accurately
digitizing Japanese documents for which all other OCR solutions failed.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10051" title="Abstract">arXiv:2310.10051</a> [<a href="/pdf/2310.10051" title="Download PDF">pdf</a>, <a href="/format/2310.10051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EAR-Net: Pursuing End-to-End Absolute Rotations from Multi-View Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuzhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qiulei Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Absolute rotation estimation is an important topic in 3D computer vision.
Existing works in literature generally employ a multi-stage (at least
two-stage) estimation strategy where multiple independent operations (feature
matching, two-view rotation estimation, and rotation averaging) are implemented
sequentially. However, such a multi-stage strategy inevitably leads to the
accumulation of the errors caused by each involved operation, and degrades its
final estimation on global rotations accordingly. To address this problem, we
propose an End-to-end method for estimating Absolution Rotations from
multi-view images based on deep neural Networks, called EAR-Net. The proposed
EAR-Net consists of an epipolar confidence graph construction module and a
confidence-aware rotation averaging module. The epipolar confidence graph
construction module is explored to simultaneously predict pairwise relative
rotations among the input images and their corresponding confidences, resulting
in a weighted graph (called epipolar confidence graph). Based on this graph,
the confidence-aware rotation averaging module, which is differentiable, is
explored to predict the absolute rotations. Thanks to the introduced
confidences of the relative rotations, the proposed EAR-Net could effectively
handle outlier cases. Experimental results on three public datasets demonstrate
that EAR-Net outperforms the state-of-the-art methods by a large margin in
terms of accuracy and speed.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10053" title="Abstract">arXiv:2310.10053</a> [<a href="/pdf/2310.10053" title="Download PDF">pdf</a>, <a href="/ps/2310.10053" title="Download PostScript">ps</a>, <a href="/format/2310.10053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Low-Cost Approximate Multiplier for FPGAs using Dynamic  Reconfiguration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vakili%2C+S">Shervin Vakili</a>, 
<a href="/search/cs?searchtype=author&query=Vaziri%2C+M">Mobin Vaziri</a>, 
<a href="/search/cs?searchtype=author&query=Zarei%2C+A">Amirhossein Zarei</a>, 
<a href="/search/cs?searchtype=author&query=Langlois%2C+J+M+P">J.M. Pierre Langlois</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Multipliers are widely-used arithmetic operators in digital signal processing
and machine learning circuits. Due to their relatively high complexity, they
can have high latency and be a significant source of power consumption. One
strategy to alleviate these limitations is to use approximate computing. This
paper thus introduces an original FPGA-based approximate multiplier
specifically optimized for machine learning computations. It utilizes
dynamically reconfigurable lookup table (LUT) primitives in AMD-Xilinx
technology to realize the core part of the computations. The paper provides an
in-depth analysis of the hardware architecture, implementation outcomes, and
accuracy evaluations of the multiplier proposed in INT8 precision.
Implementation results on an AMD-Xilinx Kintex Ultrascale+ FPGA demonstrate
remarkable savings of 64% and 67% in LUT utilization for signed multiplication
and multiply-and-accumulation configurations, respectively, when compared to
the standard Xilinx multiplier core. Accuracy measurements on four popular deep
learning (DL) benchmarks indicate a minimal average accuracy decrease of less
than 0.29% during post-training deployment, with the maximum reduction staying
less than 0.33%. The source code of this work is available on GitHub.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10054" title="Abstract">arXiv:2310.10054</a> [<a href="/pdf/2310.10054" title="Download PDF">pdf</a>, <a href="/format/2310.10054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NASH: A Simple Unified Framework of Structured Pruning for Accelerating  Encoder-Decoder Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Jongwoo Ko</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seungjoon Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yujin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sumyeong Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+D">Du-Seong Chang</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+E">Euijai Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of the Association for Computational Linguistics: EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Structured pruning methods have proven effective in reducing the model size
and accelerating inference speed in various network architectures such as
Transformers. Despite the versatility of encoder-decoder models in numerous NLP
tasks, the structured pruning methods on such models are relatively less
explored compared to encoder-only models. In this study, we investigate the
behavior of the structured pruning of the encoder-decoder models in the
decoupled pruning perspective of the encoder and decoder component,
respectively. Our findings highlight two insights: (1) the number of decoder
layers is the dominant factor of inference speed, and (2) low sparsity in the
pruned encoder network enhances generation quality. Motivated by these
findings, we propose a simple and effective framework, NASH, that narrows the
encoder and shortens the decoder networks of encoder-decoder models. Extensive
experiments on diverse generation and inference tasks validate the
effectiveness of our method in both speedup and output quality.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10056" title="Abstract">arXiv:2310.10056</a> [<a href="/pdf/2310.10056" title="Download PDF">pdf</a>, <a href="/format/2310.10056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Conservative Objective Models for Data-Driven Crystal Structure  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+H">Han Qi</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xinyang Geng</a>, 
<a href="/search/cs?searchtype=author&query=Rando%2C+S">Stefano Rando</a>, 
<a href="/search/cs?searchtype=author&query=Ohama%2C+I">Iku Ohama</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aviral Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In computational chemistry, crystal structure prediction (CSP) is an
optimization problem that involves discovering the lowest energy stable crystal
structure for a given chemical formula. This problem is challenging as it
requires discovering globally optimal designs with the lowest energies on
complex manifolds. One approach to tackle this problem involves building
simulators based on density functional theory (DFT) followed by running search
in simulation, but these simulators are painfully slow. In this paper, we study
present and study an alternate, data-driven approach to crystal structure
prediction: instead of directly searching for the most stable structures in
simulation, we train a surrogate model of the crystal formation energy from a
database of existing crystal structures, and then optimize this model with
respect to the parameters of the crystal structure. This surrogate model is
trained to be conservative so as to prevent exploitation of its errors by the
optimizer. To handle optimization in the non-Euclidean space of crystal
structures, we first utilize a state-of-the-art graph diffusion auto-encoder
(CD-VAE) to convert a crystal structure into a vector-based search space and
then optimize a conservative surrogate model of the crystal energy, trained on
top of this vector representation. We show that our approach, dubbed LCOMs
(latent conservative objective models), performs comparably to the best current
approaches in terms of success rate of structure prediction, while also
drastically reducing computational cost.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10059" title="Abstract">arXiv:2310.10059</a> [<a href="/pdf/2310.10059" title="Download PDF">pdf</a>, <a href="/format/2310.10059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flow Dynamics Correction for Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Koniusz%2C+P">Piotr Koniusz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Various research studies indicate that action recognition performance highly
depends on the types of motions being extracted and how accurate the human
actions are represented. In this paper, we investigate different optical flow,
and features extracted from these optical flow that capturing both short-term
and long-term motion dynamics. We perform power normalization on the magnitude
component of optical flow for flow dynamics correction to boost subtle or
dampen sudden motions. We show that existing action recognition models which
rely on optical flow are able to get performance boosted with our corrected
optical flow. To further improve performance, we integrate our corrected flow
dynamics into popular models through a simple hallucination step by selecting
only the best performing optical flow features, and we show that by
'translating' the CNN feature maps into these optical flow features with
different scales of motions leads to the new state-of-the-art performance on
several benchmarks including HMDB-51, YUP++, fine-grained action recognition on
MPII Cooking Activities, and large-scale Charades.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10060" title="Abstract">arXiv:2310.10060</a> [<a href="/pdf/2310.10060" title="Download PDF">pdf</a>, <a href="/format/2310.10060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation for Time-Series Classification: a Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zijun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingbo Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianhua Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Data Augmentation (DA) for Time Series Classification (TSC) is a common
technique in machine learning to increase the number of training samples, which
enhances model performance, enriches the dataset variety, and helps mitigate
overfitting. Nonetheless, this technique is currently faced with challenges
characterized by incomplete reviews, ambiguous taxonomies, insufficient
evaluations, and user-unfriendly tools. This study undertakes a detailed
exploration of DA for TSC. We first conducted a thorough review of the
developments in the field of DA for TSC over the past 10 years since existing
surveys on DA for TSC are not comprehensive enough. Our efforts encompassed
gathering more than 60 distinct DA techniques from a pool over 100 research
papers. This endeavor culminated in the creation of an innovative taxonomy
exclusively tailored to DA within the TSC domain. The taxonomy organizes
methods into five main categories: Transformation-Based, Pattern-Based,
Generative, Decomposition-Based, and Automated Data Augmentation. This
classification serves as a sturdy reference for researchers when choosing
methods. In addition, since there is a lack of comprehensive and detailed
evaluations of popular data augmentation methods, we conduct a comprehensive
assessment. More than 15 DA methods were tested on 8 UCR time-series datasets
using the ResNet and deploying a multi-metric evaluation strategy that includes
Accuracy, Method Ranking, and Residual Analysis, the outcome was a baseline
accuracy of 88.94 +- 11.83%. Findings highlighted the variable effectiveness of
DA methods, for instance, methods like Permutation enhanced performance while
Rotation decreased accuracy. Dataset properties also profoundly influence DA
efficacy, we give users accurate and practical advice based on our experimental
results to guide them in choosing the most appropriate DA methods for different
data characteristics.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10061" title="Abstract">arXiv:2310.10061</a> [<a href="/pdf/2310.10061" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A computational model of serial and parallel processing in visual search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heaton%2C+R+F">Rachel F. Heaton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 116 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Symbolic Computation (cs.SC)

</div>
<p class="mathjax">The following is a dissertation aimed at understanding what the various
phenomena in visual search teach us about the nature of human visual
representations and processes. I first review some of the major empirical
findings in the study of visual search. I next present a theory of visual
search in terms of what I believe these findings suggest about the
representations and processes underlying ventral visual processing. These
principles are instantiated in a computational model called CASPER (Concurrent
Attention: Serial and Parallel Evaluation with Relations), originally developed
by Hummel, that I have adapted to account for a range of phenomena in visual
search. I then describe an extension of the CASPER model to account for our
ability to search for visual items defined not simply by the features composing
those items but by the spatial relations among those features. Seven
experiments (four main experiments and three replications) are described that
test CASPER's predictions about relational search. Finally, I evaluate the fit
between CASPER's predictions and the empirical findings and show with three
additional simulations that CASPER can account for negative acceleration in
search functions for relational stimuli if one postulates that the visual
system is leveraging an emergent feature that bypasses relational processing.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10062" title="Abstract">arXiv:2310.10062</a> [<a href="/pdf/2310.10062" title="Download PDF">pdf</a>, <a href="/format/2310.10062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Evaluation of Tool-Assisted Generation Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacovi%2C+A">Alon Jacovi</a>, 
<a href="/search/cs?searchtype=author&query=Caciularu%2C+A">Avi Caciularu</a>, 
<a href="/search/cs?searchtype=author&query=Herzig%2C+J">Jonathan Herzig</a>, 
<a href="/search/cs?searchtype=author&query=Aharoni%2C+R">Roee Aharoni</a>, 
<a href="/search/cs?searchtype=author&query=Bohnet%2C+B">Bernd Bohnet</a>, 
<a href="/search/cs?searchtype=author&query=Geva%2C+M">Mor Geva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A growing area of research investigates augmenting language models with tools
(e.g., search engines, calculators) to overcome their shortcomings (e.g.,
missing or incorrect knowledge, incorrect logical inferences). Various few-shot
tool-usage strategies have been proposed. However, there is no systematic and
fair comparison across different strategies, or between these strategies and
strong baselines that do not leverage tools. We conduct an extensive empirical
analysis, finding that (1) across various datasets, example difficulty levels,
and models, strong no-tool baselines are competitive to tool-assisted
strategies, implying that effectively using tools with in-context
demonstrations is a difficult unsolved problem; (2) for knowledge-retrieval
tasks, strategies that *refine* incorrect outputs with tools outperform
strategies that retrieve relevant information *ahead of* or *during
generation*; (3) tool-assisted strategies are expensive in the number of tokens
they require to work -- incurring additional costs by orders of magnitude --
which does not translate into significant improvement in performance. Overall,
our findings suggest that few-shot tool integration is still an open challenge,
emphasizing the need for comprehensive evaluations of future strategies to
accurately assess their *benefits* and *costs*.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10064" title="Abstract">arXiv:2310.10064</a> [<a href="/pdf/2310.10064" title="Download PDF">pdf</a>, <a href="/format/2310.10064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Graph Filters for Spectral GNNs via Newton Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Junjie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+E">Enyan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Dongsheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Spectral Graph Neural Networks (GNNs) are gaining attention because they can
surpass the limitations of message-passing GNNs by learning spectral filters
that capture essential frequency information in graph data through task
supervision. However, previous research suggests that the choice of filter
frequency is tied to the graph's homophily level, a connection that hasn't been
thoroughly explored in existing spectral GNNs. To address this gap, the study
conducts both theoretical and empirical analyses, revealing that low-frequency
filters have a positive correlation with homophily, while high-frequency
filters have a negative correlation. This leads to the introduction of a
shape-aware regularization technique applied to a Newton Interpolation-based
spectral filter, enabling the customization of polynomial spectral filters that
align with desired homophily levels. Extensive experiments demonstrate that
NewtonNet successfully achieves the desired filter shapes and exhibits superior
performance on both homophilous and heterophilous datasets.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10065" title="Abstract">arXiv:2310.10065</a> [<a href="/pdf/2310.10065" title="Download PDF">pdf</a>, <a href="/format/2310.10065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging BRC-20 to Ethereum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Guangsheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In this paper, we design, implement, and (partially-) evaluate a lightweight
bridge (as a type of middleware) to connect the Bitcoin and Ethereum networks
that were heterogeneously uncontactable before. Inspired by the recently
introduced Bitcoin Request Comment (BRC-20) standard, we leverage the
flexibility of Bitcoin inscriptions by embedding editable operations within
each satoshi and mapping them to programmable Ethereum smart contracts. A user
can initialize his/her requests from the Bitcoin network, subsequently
triggering corresponding actions on the Ethereum network. We validate the
lightweight nature of our solution and its ability to facilitate secure and
seamless interactions between two heterogeneous ecosystems.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10068" title="Abstract">arXiv:2310.10068</a> [<a href="/pdf/2310.10068" title="Download PDF">pdf</a>, <a href="/format/2310.10068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizable Person Search on Open-world User-Generated Video Content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guanshuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yichao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fufu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Q">Qiong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jie Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shouhong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaokang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Person search is a challenging task that involves detecting and retrieving
individuals from a large set of un-cropped scene images. Existing person search
applications are mostly trained and deployed in the same-origin scenarios.
However, collecting and annotating training samples for each scene is often
difficult due to the limitation of resources and the labor cost. Moreover,
large-scale intra-domain data for training are generally not legally available
for common developers, due to the regulation of privacy and public security.
Leveraging easily accessible large-scale User Generated Video Contents
(\emph{i.e.} UGC videos) to train person search models can fit the open-world
distribution, but still suffering a performance gap from the domain difference
to surveillance scenes. In this work, we explore enhancing the out-of-domain
generalization capabilities of person search models, and propose a
generalizable framework on both feature-level and data-level generalization to
facilitate downstream tasks in arbitrary scenarios. Specifically, we focus on
learning domain-invariant representations for both detection and ReID by
introducing a multi-task prototype-based domain-specific batch normalization,
and a channel-wise ID-relevant feature decorrelation strategy. We also identify
and address typical sources of noise in open-world training frames, including
inaccurate bounding boxes, the omission of identity labels, and the absence of
cross-camera data. Our framework achieves promising performance on two
challenging person search benchmarks without using any human annotation or
samples from the target domain.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10070" title="Abstract">arXiv:2310.10070</a> [<a href="/pdf/2310.10070" title="Download PDF">pdf</a>, <a href="/format/2310.10070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GreatSplicing: A Semantically Rich Splicing Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+X">Xiuli Bi</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiaming Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In existing splicing forgery datasets, the insufficient semantic variety of
spliced regions causes a problem that trained detection models overfit semantic
features rather than splicing traces. Meanwhile, because of the absence of a
reasonable dataset, different detection methods proposed cannot reach a
consensus on experimental settings. To address these urgent issues,
GreatSplicing, an manually created splicing dataset with considerable amount
and high quality, is proposed in this paper. GreatSplicing comprises 5,000
spliced images and covers spliced regions with 335 distinct semantic
categories, allowing neural networks to grasp splicing traces better. Extensive
experiments demonstrate that models trained on GreatSplicing exhibit minimal
misidentification rates and superior cross-dataset detection capabilities
compared to existing datasets. Furthermore, GreatSplicing is available for all
research purposes and could be downloaded from www.greatsplicing.net.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10071" title="Abstract">arXiv:2310.10071</a> [<a href="/pdf/2310.10071" title="Download PDF">pdf</a>, <a href="/format/2310.10071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kou%2C+Y">Yutong Kou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bing Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Weiming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures, Accepted by NeurIPS 2023 as a Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, the transformer has enabled the speed-oriented trackers to approach
state-of-the-art (SOTA) performance with high-speed thanks to the smaller input
size or the lighter feature extraction backbone, though they still
substantially lag behind their corresponding performance-oriented versions. In
this paper, we demonstrate that it is possible to narrow or even close this gap
while achieving high tracking speed based on the smaller input size. To this
end, we non-uniformly resize the cropped image to have a smaller input size
while the resolution of the area where the target is more likely to appear is
higher and vice versa. This enables us to solve the dilemma of attending to a
larger visual field while retaining more raw information for the target despite
a smaller input size. Our formulation for the non-uniform resizing can be
efficiently solved through quadratic programming (QP) and naturally integrated
into most of the crop-based local trackers. Comprehensive experiments on five
challenging datasets based on two kinds of transformer trackers, \ie, OSTrack
and TransT, demonstrate consistent improvements over them. In particular,
applying our method to the speed-oriented version of OSTrack even outperforms
its performance-oriented counterpart by 0.6% AUC on TNL2K, while running 50%
faster and saving over 55% MACs. Codes and models are available at
https://github.com/Kou-99/ZoomTrack.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10072" title="Abstract">arXiv:2310.10072</a> [<a href="/pdf/2310.10072" title="Download PDF">pdf</a>, <a href="/format/2310.10072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning ChatGPT for Automatic Scoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Latif%2C+E">Ehsan Latif</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+X">Xiaoming Zhai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Computers and Education: Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for
automatically scoring student written constructed responses using example
assessment tasks in science education. Recent studies on OpenAI's generative
model GPT-3.5 proved its superiority in predicting the natural language with
high accuracy and human-like responses. GPT-3.5 has been trained over enormous
online language materials such as journals and Wikipedia; therefore, more than
direct usage of pre-trained GPT-3.5 is required for automatic scoring as
students utilize a different language than trained material. These imply that a
domain-specific model, fine-tuned over data for specific tasks, can enhance
model performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks
with a diverse dataset of middle-school and high-school student responses and
expert scoring. The six tasks comprise two multi-label and four multi-class
assessment tasks. We compare the performance of fine-tuned GPT-3.5 with the
fine-tuned state-of-the-art Google's generated language model, BERT. The
results show that in-domain training corpora constructed from science questions
and responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5
shows a remarkable average increase (9.1%) in automatic scoring accuracy (mean
= 9.15, SD = 0.042) for the six tasks, p =0.001 &lt; 0.05. Specifically, for
multi-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5
achieved significantly higher scoring accuracy than BERT across all the labels,
with the second item achieving a 7.1% increase. The average scoring increase
for the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our
study confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring
of student responses on domain-specific data in education with high accuracy.
We have released fine-tuned models for public use and community engagement.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10073" title="Abstract">arXiv:2310.10073</a> [<a href="/pdf/2310.10073" title="Download PDF">pdf</a>, <a href="/format/2310.10073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expression Domain Translation Network for Cross-domain Head Reenactment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+T">Taewoong Kang</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Jeongsik Oh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaeseong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sunghyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite the remarkable advancements in head reenactment, the existing methods
face challenges in cross-domain head reenactment, which aims to transfer human
motions to domains outside the human, including cartoon characters. It is still
difficult to extract motion from out-of-domain images due to the distinct
appearances, such as large eyes. Recently, previous work introduced a
large-scale anime dataset called AnimeCeleb and a cross-domain head reenactment
model, including an optimization-based mapping function to translate the human
domain's expressions to the anime domain. However, we found that the mapping
function, which relies on a subset of expressions, imposes limitations on the
mapping of various expressions. To solve this challenge, we introduce a novel
expression domain translation network that transforms human expressions into
anime expressions. Specifically, to maintain the geometric consistency of
expressions between the input and output of the expression domain translation
network, we employ a 3D geometric-aware loss function that reduces the
distances between the vertices in the 3D mesh of the human and anime. By doing
so, it forces high-fidelity and one-to-one mapping with respect to two
cross-expression domains. Our method outperforms existing methods in both
qualitative and quantitative analysis, marking a significant advancement in the
field of cross-domain head reenactment.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10074" title="Abstract">arXiv:2310.10074</a> [<a href="/pdf/2310.10074" title="Download PDF">pdf</a>, <a href="/format/2310.10074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoTTA: Robust Test-Time Adaptation on Noisy Data Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+T">Taesik Gong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yewon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T">Taeckyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chottananurak%2C+S">Sorn Chottananurak</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sung-Ju Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Test-time adaptation (TTA) aims to address distributional shifts between
training and testing data using only unlabeled test data streams for continual
model adaptation. However, most TTA methods assume benign test streams, while
test samples could be unexpectedly diverse in the wild. For instance, an unseen
object or noise could appear in autonomous driving. This leads to a new threat
to existing TTA algorithms; we found that prior TTA algorithms suffer from
those noisy test samples as they blindly adapt to incoming samples. To address
this problem, we present Screening-out Test-Time Adaptation (SoTTA), a novel
TTA algorithm that is robust to noisy samples. The key enabler of SoTTA is
two-fold: (i) input-wise robustness via high-confidence uniform-class sampling
that effectively filters out the impact of noisy samples and (ii)
parameter-wise robustness via entropy-sharpness minimization that improves the
robustness of model parameters against large gradients from noisy samples. Our
evaluation with standard TTA benchmarks with various noisy scenarios shows that
our method outperforms state-of-the-art TTA methods under the presence of noisy
samples and achieves comparable accuracy to those methods without noisy
samples. The source code is available at https://github.com/taeckyung/SoTTA .
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10076" title="Abstract">arXiv:2310.10076</a> [<a href="/pdf/2310.10076" title="Download PDF">pdf</a>, <a href="/format/2310.10076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verbosity Bias in Preference Labeling by Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saito%2C+K">Keita Saito</a>, 
<a href="/search/cs?searchtype=author&query=Wachi%2C+A">Akifumi Wachi</a>, 
<a href="/search/cs?searchtype=author&query=Wataoka%2C+K">Koki Wataoka</a>, 
<a href="/search/cs?searchtype=author&query=Akimoto%2C+Y">Youhei Akimoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, Large Language Models (LLMs) have witnessed a remarkable
surge in prevalence, altering the landscape of natural language processing and
machine learning. One key factor in improving the performance of LLMs is
alignment with humans achieved with Reinforcement Learning from Human Feedback
(RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies
are investigating the replacement of human feedback with feedback from other
LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the
biases that come along with evaluating LLMs with other LLMs and take a closer
look into verbosity bias -- a bias where LLMs sometimes prefer more verbose
answers even if they have similar qualities. We see that in our problem
setting, GPT-4 prefers longer answers more than humans. We also propose a
metric to measure this bias.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10077" title="Abstract">arXiv:2310.10077</a> [<a href="/pdf/2310.10077" title="Download PDF">pdf</a>, <a href="/format/2310.10077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Packer: Deceiving LLMs through Compositional Instruction with  Hidden Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shuyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingshu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Rui Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, Large language models (LLMs) with powerful general capabilities
have been increasingly integrated into various Web applications, while
undergoing alignment training to ensure that the generated content aligns with
user intent and ethics. Unfortunately, they remain the risk of generating
harmful content like hate speech and criminal activities in practical
applications. Current approaches primarily rely on detecting, collecting, and
training against harmful prompts to prevent such risks. However, they typically
focused on the "superficial" harmful prompts with a solitary intent, ignoring
composite attack instructions with multiple intentions that can easily elicit
harmful content in real-world scenarios. In this paper, we introduce an
innovative technique for obfuscating harmful instructions: Compositional
Instruction Attacks (CIA), which refers to attacking by combination and
encapsulation of multiple instructions. CIA hides harmful prompts within
instructions of harmless intentions, making it impossible for the model to
identify underlying malicious intentions. Furthermore, we implement two
transformation methods, known as T-CIA and W-CIA, to automatically disguise
harmful instructions as talking or writing tasks, making them appear harmless
to LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety
assessment datasets and two harmful prompt datasets. It achieves an attack
success rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+
for ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets.
Our approach reveals the vulnerability of LLMs to such compositional
instruction attacks that harbor underlying harmful intentions, contributing
significantly to LLM security development. Warning: this paper may contain
offensive or upsetting content!
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10079" title="Abstract">arXiv:2310.10079</a> [<a href="/pdf/2310.10079" title="Download PDF">pdf</a>, <a href="/format/2310.10079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOCHA: Real-Time Motion Characterization via Context Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+D">Deok-Kyeong Jang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yuting Ye</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+J">Jungdam Won</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sung-Hee Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> presented at Siggraph Asia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transforming neutral, characterless input motions to embody the distinct
style of a notable character in real time is highly compelling for character
animation. This paper introduces MOCHA, a novel online motion characterization
framework that transfers both motion styles and body proportions from a target
character to an input source motion. MOCHA begins by encoding the input motion
into a motion feature that structures the body part topology and captures
motion dependencies for effective characterization. Central to our framework is
the Neural Context Matcher, which generates a motion feature for the target
character with the most similar context to the input motion feature. The
conditioned autoregressive model of the Neural Context Matcher can produce
temporally coherent character features in each time frame. To generate the
final characterized pose, our Characterizer network incorporates the
characteristic aspects of the target motion feature into the input motion
feature while preserving its context. This is achieved through a transformer
model that introduces the adaptive instance normalization and context
mapping-based cross-attention, effectively injecting the character feature into
the source feature. We validate the performance of our framework through
comparisons with prior work and an ablation study. Our framework can easily
accommodate various applications, including characterization with only sparse
input and real-time characterization. Additionally, we contribute a
high-quality motion dataset comprising six different characters performing a
range of motions, which can serve as a valuable resource for future research.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10080" title="Abstract">arXiv:2310.10080</a> [<a href="/pdf/2310.10080" title="Download PDF">pdf</a>, <a href="/format/2310.10080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let&#x27;s reward step by step: Step-Level reward model as the Navigators for  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianli Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haotian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tingkai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianbo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent years have seen considerable advancements in multi-step reasoning with
Large Language Models (LLMs). The previous studies have elucidated the merits
of integrating feedback or search mechanisms during model inference to improve
the reasoning accuracy. The Process-Supervised Reward Model (PRM), typically
furnishes LLMs with step-by-step feedback during the training phase, akin to
Proximal Policy Optimization (PPO) or reject sampling. Our objective is to
examine the efficacy of PRM in the inference phase to help discern the optimal
solution paths for multi-step tasks such as mathematical reasoning and code
generation. To this end, we propose a heuristic greedy search algorithm that
employs the step-level feedback from PRM to optimize the reasoning pathways
explored by LLMs. This tailored PRM demonstrated enhanced results compared to
the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH.
Additionally, to explore the versatility of our approach, we develop a novel
method to automatically generate step-level reward dataset for coding tasks and
observed similar improved performance in the code generation tasks. Thus
highlighting the robust nature of our reward-model-based approach to inference
for reasoning tasks.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10083" title="Abstract">arXiv:2310.10083</a> [<a href="/pdf/2310.10083" title="Download PDF">pdf</a>, <a href="/format/2310.10083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models  using Instruction-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sukeda%2C+I">Issey Sukeda</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+M">Masahiro Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Sakaji%2C+H">Hiroki Sakaji</a>, 
<a href="/search/cs?searchtype=author&query=Kodera%2C+S">Satoshi Kodera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the ongoing wave of impact driven by large language models (LLMs) like
ChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial
research frontier. Since mainstream LLMs tend to be designed for
general-purpose applications, constructing a medical LLM through domain
adaptation is a huge challenge. While instruction-tuning is used to fine-tune
some LLMs, its precise roles in domain adaptation remain unknown. Here we show
the contribution of LoRA-based instruction-tuning to performance in Japanese
medical question-answering tasks. In doing so, we employ a multifaceted
evaluation for multiple-choice questions, including scoring based on "Exact
match" and "Gestalt distance" in addition to the conventional accuracy. Our
findings suggest that LoRA-based instruction-tuning can partially incorporate
domain-specific knowledge into LLMs, with larger models demonstrating more
pronounced effects. Furthermore, our results underscore the potential of
adapting English-centric models for Japanese applications in domain adaptation,
while also highlighting the persisting limitations of Japanese-centric models.
This initiative represents a pioneering effort in enabling medical institutions
to fine-tune and operate models without relying on external services.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10085" title="Abstract">arXiv:2310.10085</a> [<a href="/pdf/2310.10085" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solution to Advanced Manufacturing Process Problems using Cohort  Intelligence Algorithm with Improved Constraint Handling Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nargundkar%2C+A">Aniket Nargundkar</a>, 
<a href="/search/cs?searchtype=author&query=Rawal%2C+M">Madhav Rawal</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Aryaman Patel</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A+J">Anand J Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Shastri%2C+A+S">Apoorva S Shastri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Recently, various Artificial Intelligence (AI) based optimization
metaheuristics are proposed and applied for a variety of problems. Cohort
Intelligence (CI) algorithm is a socio inspired optimization technique which is
successfully applied for solving several unconstrained &amp; constrained real-world
problems from the domains such as design, manufacturing, supply chain,
healthcare, etc. Generally, real-world problems are constrained in nature. Even
though most of the Evolutionary Algorithms (EAs) can efficiently solve
unconstrained problems, their performance degenerates when the constraints are
involved. In this paper, two novel constraint handling approaches based on
modulus and hyperbolic tangent probability distributions are proposed.
Constrained CI algorithm with constraint handling approaches based on
triangular, modulus and hyperbolic tangent is presented and applied for
optimizing advanced manufacturing processes such as Water Jet Machining (WJM),
Abrasive Jet Machining (AJM), Ultrasonic Machining (USM) and Grinding process.
The solutions obtained using proposed CI algorithm are compared with
contemporary algorithms such as Genetic Algorithm, Simulated Annealing,
Teaching Learning Based Optimization, etc. The proposed approaches achieved
2%-127% maximization of material removal rate satisfying hard constraints. As
compared to the GA, CI with Hyperbolic tangent probability distribution
achieved 15%, 2%, 2%, 127%, and 4% improvement in MRR for AJMB, AJMD, WJM, USM,
and Grinding processes, respectively contributing to the productivity
improvement. The contributions in this paper have opened several avenues for
further applicability of the proposed constraint handling approaches for
solving complex constrained problems.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10089" title="Abstract">arXiv:2310.10089</a> [<a href="/pdf/2310.10089" title="Download PDF">pdf</a>, <a href="/format/2310.10089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Over-the-Air Federated Learning and Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jingyang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuanming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chunxiao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Signal Processing (eess.SP)

</div>
<p class="mathjax">Federated learning (FL), as an emerging distributed machine learning
paradigm, allows a mass of edge devices to collaboratively train a global model
while preserving privacy. In this tutorial, we focus on FL via over-the-air
computation (AirComp), which is proposed to reduce the communication overhead
for FL over wireless networks at the cost of compromising in the learning
performance due to model aggregation error arising from channel fading and
noise. We first provide a comprehensive study on the convergence of
AirComp-based FedAvg (AirFedAvg) algorithms under both strongly convex and
non-convex settings with constant and diminishing learning rates in the
presence of data heterogeneity. Through convergence and asymptotic analysis, we
characterize the impact of aggregation error on the convergence bound and
provide insights for system design with convergence guarantees. Then we derive
convergence rates for AirFedAvg algorithms for strongly convex and non-convex
objectives. For different types of local updates that can be transmitted by
edge devices (i.e., local model, gradient, and model difference), we reveal
that transmitting local model in AirFedAvg may cause divergence in the training
procedure. In addition, we consider more practical signal processing schemes to
improve the communication efficiency and further extend the convergence
analysis to different forms of model aggregation error caused by these signal
processing schemes. Extensive simulation results under different settings of
objective functions, transmitted local information, and communication schemes
verify the theoretical conclusions.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10090" title="Abstract">arXiv:2310.10090</a> [<a href="/pdf/2310.10090" title="Download PDF">pdf</a>, <a href="/format/2310.10090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orthogonal Uncertainty Representation of Data Manifold for Robust  Long-Tailed Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yanbiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+L">Licheng Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingling Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10pages,Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In scenarios with long-tailed distributions, the model's ability to identify
tail classes is limited due to the under-representation of tail samples. Class
rebalancing, information augmentation, and other techniques have been proposed
to facilitate models to learn the potential distribution of tail classes. The
disadvantage is that these methods generally pursue models with balanced class
accuracy on the data manifold, while ignoring the ability of the model to
resist interference. By constructing noisy data manifold, we found that the
robustness of models trained on unbalanced data has a long-tail phenomenon.
That is, even if the class accuracy is balanced on the data domain, it still
has bias on the noisy data manifold. However, existing methods cannot
effectively mitigate the above phenomenon, which makes the model vulnerable in
long-tailed scenarios. In this work, we propose an Orthogonal Uncertainty
Representation (OUR) of feature embedding and an end-to-end training strategy
to improve the long-tail phenomenon of model robustness. As a general
enhancement tool, OUR has excellent compatibility with other methods and does
not require additional data generation, ensuring fast and efficient training.
Comprehensive evaluations on long-tailed datasets show that our method
significantly improves the long-tail phenomenon of robustness, bringing
consistent performance gains to other long-tailed learning methods.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10092" title="Abstract">arXiv:2310.10092</a> [<a href="/pdf/2310.10092" title="Download PDF">pdf</a>, <a href="/ps/2310.10092" title="Download PostScript">ps</a>, <a href="/format/2310.10092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Differential Privacy via Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brahmbhatt%2C+A">Anand Brahmbhatt</a>, 
<a href="/search/cs?searchtype=author&query=Saket%2C+R">Rishi Saket</a>, 
<a href="/search/cs?searchtype=author&query=Havaldar%2C+S">Shreyas Havaldar</a>, 
<a href="/search/cs?searchtype=author&query=Nasery%2C+A">Anshul Nasery</a>, 
<a href="/search/cs?searchtype=author&query=Raghuveer%2C+A">Aravindan Raghuveer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In many real-world applications, in particular due to recent developments in
the privacy landscape, training data may be aggregated to preserve the privacy
of sensitive training labels. In the learning from label proportions (LLP)
framework, the dataset is partitioned into bags of feature-vectors which are
available only with the sum of the labels per bag. A further restriction, which
we call learning from bag aggregates (LBA) is where instead of individual
feature-vectors, only the (possibly weighted) sum of the feature-vectors per
bag is available. We study whether such aggregation techniques can provide
privacy guarantees under the notion of label differential privacy (label-DP)
previously studied in for e.g. [Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari
et al.'22].
<br />It is easily seen that naive LBA and LLP do not provide label-DP. Our main
result however, shows that weighted LBA using iid Gaussian weights with $m$
randomly sampled disjoint $k$-sized bags is in fact $(\varepsilon,
\delta)$-label-DP for any $\varepsilon &gt; 0$ with $\delta \approx
\exp(-\Omega(\sqrt{k}))$ assuming a lower bound on the linear-mse regression
loss. Further, this preserves the optimum over linear mse-regressors of bounded
norm to within $(1 \pm o(1))$-factor w.p. $\approx 1 - \exp(-\Omega(m))$. We
emphasize that no additive label noise is required.
<br />The analogous weighted-LLP does not however admit label-DP. Nevertheless, we
show that if additive $N(0, 1)$ noise can be added to any constant fraction of
the instance labels, then the noisy weighted-LLP admits similar label-DP
guarantees without assumptions on the dataset, while preserving the utility of
Lipschitz-bounded neural mse-regression tasks.
<br />Our work is the first to demonstrate that label-DP can be achieved by
randomly weighted aggregation for regression tasks, using no or little additive
noise.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10094" title="Abstract">arXiv:2310.10094</a> [<a href="/pdf/2310.10094" title="Download PDF">pdf</a>, <a href="/format/2310.10094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decomposed Prompt Tuning via Low-Rank Reparameterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoli Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While prompt tuning approaches have achieved competitive performance with
high efficiency, we observe that they invariably employ the same initialization
process, wherein the soft prompt is either randomly initialized or derived from
an existing embedding vocabulary. In contrast to these conventional methods,
this study aims to investigate an alternative way to derive soft prompt. Our
empirical studies show that the soft prompt typically exhibits a low intrinsic
rank characteristic. With such observations, we propose decomposed prompt
tuning, a novel approach that utilizes low-rank matrices to initialize the soft
prompt. Through the low-rank reparameterization, our method significantly
reduces the number of trainable parameters while maintaining effectiveness.
Experimental results on the SuperGLUE benchmark in both high-resource and
low-resource scenarios demonstrate the effectiveness of the proposed method.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10096" title="Abstract">arXiv:2310.10096</a> [<a href="/pdf/2310.10096" title="Download PDF">pdf</a>, <a href="/format/2310.10096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label  Proportions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brahmbhatt%2C+A">Anand Brahmbhatt</a>, 
<a href="/search/cs?searchtype=author&query=Pokala%2C+M">Mohith Pokala</a>, 
<a href="/search/cs?searchtype=author&query=Saket%2C+R">Rishi Saket</a>, 
<a href="/search/cs?searchtype=author&query=Raghuveer%2C+A">Aravindan Raghuveer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In the task of Learning from Label Proportions (LLP), a model is trained on
groups (a.k.a bags) of instances and their corresponding label proportions to
predict labels for individual instances. LLP has been applied pre-dominantly on
two types of datasets - image and tabular. In image LLP, bags of fixed size are
created by randomly sampling instances from an underlying dataset. Bags created
via this methodology are called random bags. Experimentation on Image LLP has
been mostly on random bags on CIFAR-* and MNIST datasets. Despite being a very
crucial task in privacy sensitive applications, tabular LLP does not yet have a
open, large scale LLP benchmark. One of the unique properties of tabular LLP is
the ability to create feature bags where all the instances in a bag have the
same value for a given feature. It has been shown in prior research that
feature bags are very common in practical, real world applications [Chen et. al
'23, Saket et. al. '22].
<br />In this paper, we address the lack of a open, large scale tabular benchmark.
First we propose LLP-Bench, a suite of 56 LLP datasets (52 feature bag and 4
random bag datasets) created from the Criteo CTR prediction dataset consisting
of 45 million instances. The 56 datasets represent diverse ways in which bags
can be constructed from underlying tabular data. To the best of our knowledge,
LLP-Bench is the first large scale tabular LLP benchmark with an extensive
diversity in constituent datasets. Second, we propose four metrics that
characterize and quantify the hardness of a LLP dataset. Using these four
metrics we present deep analysis of the 56 datasets in LLP-Bench. Finally we
present the performance of 9 SOTA and popular tabular LLP techniques on all the
56 datasets. To the best of our knowledge, our study consisting of more than
2500 experiments is the most extensive study of popular tabular LLP techniques
in literature.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10098" title="Abstract">arXiv:2310.10098</a> [<a href="/pdf/2310.10098" title="Download PDF">pdf</a>, <a href="/ps/2310.10098" title="Download PostScript">ps</a>, <a href="/format/2310.10098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC Learning Linear Thresholds from Label Proportions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brahmbhatt%2C+A">Anand Brahmbhatt</a>, 
<a href="/search/cs?searchtype=author&query=Saket%2C+R">Rishi Saket</a>, 
<a href="/search/cs?searchtype=author&query=Raghuveer%2C+A">Aravindan Raghuveer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Spotlight paper at Neural Information Processing Systems (NeurIPS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Learning from label proportions (LLP) is a generalization of supervised
learning in which the training data is available as sets or bags of
feature-vectors (instances) along with the average instance-label of each bag.
The goal is to train a good instance classifier. While most previous works on
LLP have focused on training models on such training data, computational
learnability of LLP was only recently explored by [Saket'21, Saket'22] who
showed worst case intractability of properly learning linear threshold
functions (LTFs) from label proportions. However, their work did not rule out
efficient algorithms for this problem on natural distributions.
<br />In this work we show that it is indeed possible to efficiently learn LTFs
using LTFs when given access to random bags of some label proportion in which
feature-vectors are, conditioned on their labels, independently sampled from a
Gaussian distribution $N(\mathbf{\mu}, \mathbf{\Sigma})$. Our work shows that a
certain matrix -- formed using covariances of the differences of
feature-vectors sampled from the bags with and without replacement --
necessarily has its principal component, after a transformation, in the
direction of the normal vector of the LTF. Our algorithm estimates the means
and covariance matrices using subgaussian concentration bounds which we show
can be applied to efficiently sample bags for approximating the normal
direction. Using this in conjunction with novel generalization error bounds in
the bag setting, we show that a low error hypothesis LTF can be identified. For
some special cases of the $N(\mathbf{0}, \mathbf{I})$ distribution we provide a
simpler mean estimation based algorithm. We include an experimental evaluation
of our learning algorithms along with a comparison with those of [Saket'21,
Saket'22] and random LTFs, demonstrating the effectiveness of our techniques.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10101" title="Abstract">arXiv:2310.10101</a> [<a href="/pdf/2310.10101" title="Download PDF">pdf</a>, <a href="/ps/2310.10101" title="Download PostScript">ps</a>, <a href="/format/2310.10101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random-order Contention Resolution via Continuous Induction: Tightness  for Bipartite Matching under Vertex Arrivals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=MacRury%2C+C">Calum MacRury</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Will Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">We introduce a new approach for designing Random-order Contention Resolution
Schemes (RCRS) via exact solution in continuous time. Given a function
$c(y):[0,1] \rightarrow [0,1]$, we show how to select each element which
arrives at time $y \in [0,1]$ with probability exactly $c(y)$. We provide a
rigorous algorithmic framework for achieving this, which discretizes the time
interval and also needs to sample its past execution to ensure these exact
selection probabilities. We showcase our framework in the context of online
contention resolution schemes for matching with random-order vertex arrivals.
For bipartite graphs with two-sided arrivals, we design a $(1+e^{-2})/2 \approx
0.567$-selectable RCRS, which we also show to be tight. Next, we show that the
presence of short odd-length cycles is the only barrier to attaining a (tight)
$(1+e^{-2})/2$-selectable RCRS on general graphs. By generalizing our bipartite
RCRS, we design an RCRS for graphs with odd-length girth $g$ which is
$(1+e^{-2})/2$-selectable as $g \rightarrow \infty$. This convergence happens
very rapidly: for triangle-free graphs (i.e., $g \ge 5$), we attain a $121/240
+ 7/16 e^2 \approx 0.563$-selectable RCRS. Finally, for general graphs we
improve on the $8/15 \approx 0.533$-selectable RCRS of Fu et al. (ICALP, 2021)
and design an RCRS which is at least $0.535$-selectable. Due to the reduction
of Ezra et al. (EC, 2020), our bounds yield a $0.535$-competitive
(respectively, $(1+e^{-2})/2$-competitive) algorithm for prophet secretary
matching on general (respectively, bipartite) graphs under vertex arrivals.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10102" title="Abstract">arXiv:2310.10102</a> [<a href="/pdf/2310.10102" title="Download PDF">pdf</a>, <a href="/format/2310.10102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KAKURENBO: Adaptively Hiding Samples in Deep Neural Network Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+T">Truong Thao Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Gerofi%2C+B">Balazs Gerofi</a>, 
<a href="/search/cs?searchtype=author&query=Martinez-Noriega%2C+E+J">Edgar Josafat Martinez-Noriega</a>, 
<a href="/search/cs?searchtype=author&query=Trahay%2C+F">Fran&#xe7;ois Trahay</a>, 
<a href="/search/cs?searchtype=author&query=Wahib%2C+M">Mohamed Wahib</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Advances in Neural Information Processing Systems 2023 (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper proposes a method for hiding the least-important samples during
the training of deep neural networks to increase efficiency, i.e., to reduce
the cost of training. Using information about the loss and prediction
confidence during training, we adaptively find samples to exclude in a given
epoch based on their contribution to the overall learning process, without
significantly degrading accuracy. We explore the converge properties when
accounting for the reduction in the number of SGD updates. Empirical results on
various large-scale datasets and models used directly in image classification
and segmentation show that while the with-replacement importance sampling
algorithm performs poorly on large datasets, our method can reduce total
training time by up to 22% impacting accuracy only by 0.4% compared to the
baseline. Code available at https://github.com/TruongThaoNguyen/kakurenbo
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10103" title="Abstract">arXiv:2310.10103</a> [<a href="/pdf/2310.10103" title="Download PDF">pdf</a>, <a href="/format/2310.10103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigation with Large Language Models: Semantic Guesswork as a Heuristic  for Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Dhruv Shah</a>, 
<a href="/search/cs?searchtype=author&query=Equi%2C+M">Michael Equi</a>, 
<a href="/search/cs?searchtype=author&query=Osinski%2C+B">Blazej Osinski</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Ichter%2C+B">Brian Ichter</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Videos, code, and an interactive Colab notebook that runs in your browser <a href="https://sites.google.com/view/lfg-nav/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Navigation in unfamiliar environments presents a major challenge for robots:
while mapping and planning techniques can be used to build up a representation
of the world, quickly discovering a path to a desired goal in unfamiliar
settings with such methods often requires lengthy mapping and exploration.
Humans can rapidly navigate new environments, particularly indoor environments
that are laid out logically, by leveraging semantics -- e.g., a kitchen often
adjoins a living room, an exit sign indicates the way out, and so forth.
Language models can provide robots with such knowledge, but directly using
language models to instruct a robot how to reach some destination can also be
impractical: while language models might produce a narrative about how to reach
some goal, because they are not grounded in real-world observations, this
narrative might be arbitrarily wrong. Therefore, in this paper we study how the
``semantic guesswork'' produced by language models can be utilized as a guiding
heuristic for planning algorithms. Our method, Language Frontier Guide (LFG),
uses the language model to bias exploration of novel real-world environments by
incorporating the semantic knowledge stored in language models as a search
heuristic for planning with either topological or metric maps. We evaluate LFG
in challenging real-world environments and simulated benchmarks, outperforming
uninformed exploration and other ways of using language models.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10106" title="Abstract">arXiv:2310.10106</a> [<a href="/pdf/2310.10106" title="Download PDF">pdf</a>, <a href="/format/2310.10106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder  and Input Feature Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Can Cui</a> (MULTISPEECH), 
<a href="/search/cs?searchtype=author&query=Sheikh%2C+I+A">Imran Ahamad Sheikh</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+M">Mostafa Sadeghi</a> (MULTISPEECH), 
<a href="/search/cs?searchtype=author&query=Vincent%2C+E">Emmanuel Vincent</a> (MULTISPEECH)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2023), Dec 2023, Taipei, Taiwan
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We present an end-to-end multichannel speaker-attributed automatic speech
recognition (MC-SA-ASR) system that combines a Conformer-based encoder with
multi-frame crosschannel attention and a speaker-attributed Transformer-based
decoder. To the best of our knowledge, this is the first model that efficiently
integrates ASR and speaker identification modules in a multichannel setting. On
simulated mixtures of LibriSpeech data, our system reduces the word error rate
(WER) by up to 12% and 16% relative compared to previously proposed
single-channel and multichannel approaches, respectively. Furthermore, we
investigate the impact of different input features, including multichannel
magnitude and phase information, on the ASR performance. Finally, our
experiments on the AMI corpus confirm the effectiveness of our system for
real-world multichannel meeting transcription.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10107" title="Abstract">arXiv:2310.10107</a> [<a href="/pdf/2310.10107" title="Download PDF">pdf</a>, <a href="/ps/2310.10107" title="Download PostScript">ps</a>, <a href="/format/2310.10107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret Analysis of the Posterior Sampling-based Learning Algorithm for  Episodic POMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Dengwang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rahul Jain</a>, 
<a href="/search/cs?searchtype=author&query=Nayyar%2C+A">Ashutosh Nayyar</a>, 
<a href="/search/cs?searchtype=author&query=Nuzzo%2C+P">Pierluigi Nuzzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
<p class="mathjax">Compared to Markov Decision Processes (MDPs), learning in Partially
Observable Markov Decision Processes (POMDPs) can be significantly harder due
to the difficulty of interpreting observations. In this paper, we consider
episodic learning problems in POMDPs with unknown transition and observation
models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL)
algorithm for POMDPs and show that its Bayesian regret scales as the square
root of the number of episodes. In general, the regret scales exponentially
with the horizon length $H$, and we show that this is inevitable by providing a
lower bound. However, under the condition that the POMDP is undercomplete and
weakly revealing, we establish a polynomial Bayesian regret bound that improves
the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result
by <a href="/abs/2204.08967">arXiv:2204.08967</a>.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10108" title="Abstract">arXiv:2310.10108</a> [<a href="/pdf/2310.10108" title="Download PDF">pdf</a>, <a href="/format/2310.10108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Generative Agents in Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">An Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+L">Leheng Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages,14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recommender systems are the cornerstone of today's information dissemination,
yet a disconnect between offline metrics and online performance greatly hinders
their development. Addressing this challenge, we envision a recommendation
simulator, capitalizing on recent breakthroughs in human-level intelligence
exhibited by Large Language Models (LLMs). We propose Agent4Rec, a novel movie
recommendation simulator, leveraging LLM-empowered generative agents equipped
with user profile, memory, and actions modules specifically tailored for the
recommender system. In particular, these agents' profile modules are
initialized using the MovieLens dataset, capturing users' unique tastes and
social traits; memory modules log both factual and emotional memories and are
integrated with an emotion-driven reflection mechanism; action modules support
a wide variety of behaviors, spanning both taste-driven and emotion-driven
actions. Each agent interacts with personalized movie recommendations in a
page-by-page manner, relying on a pre-implemented collaborative filtering-based
recommendation algorithm. We delve into both the capabilities and limitations
of Agent4Rec, aiming to explore an essential research question: to what extent
can LLM-empowered generative agents faithfully simulate the behavior of real,
autonomous humans in recommender systems? Extensive and multi-faceted
evaluations of Agent4Rec highlight both the alignment and deviation between
agents and user-personalized preferences. Beyond mere performance comparison,
we explore insightful experiments, such as emulating the filter bubble effect
and discovering the underlying causal relationships in recommendation tasks.
Our codes are available at https://github.com/LehengTHU/Agent4Rec.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10110" title="Abstract">arXiv:2310.10110</a> [<a href="/pdf/2310.10110" title="Download PDF">pdf</a>, <a href="/ps/2310.10110" title="Download PostScript">ps</a>, <a href="/format/2310.10110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting array manipulation habits to optimize garbage collection and  type flow analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colnet%2C+D">Dominique Colnet</a> (LORIA), 
<a href="/search/cs?searchtype=author&query=Sonntag%2C+B">Beno&#xee;t Sonntag</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Software: Practice and Experience, 2015, 45 (12), pp.1639-1657
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">A widespread practice to implement a flexible array is to consider the
storage area into two parts: the used area, which is already available for
read/write operations, and the supply area, which is used in case of
enlargement of the array. The main purpose of the supply area is to avoid as
much as possible the reallocation of the whole storage area in case of
enlargement. As the supply area is not used by the application, the main idea
of the paper is to convey the information to the garbage collector, making it
possible to avoid completely the marking of the supply area. We also present a
simple method to analyze the types of objects, which are stored in an array as
well as the possible presence of NULL values within the array. This allows us
to better specialize the work of the garbage collector when marking the used
area, and also, by transitivity, to improve overall results for type analysis
of all expressions of the source code. After introducing several abstract data
types, which represent the main arrays concerned by our technique (i.e., zero
or variable indexing, circular arrays and hash maps), we measure its impact
during the bootstrap of two compilers whose libraries are equipped with these
abstract data types. We then measure, on various software products we have not
written, the frequency of certain habits of manipulation of arrays, to assess
the validity of our approach.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10114" title="Abstract">arXiv:2310.10114</a> [<a href="/pdf/2310.10114" title="Download PDF">pdf</a>, <a href="/format/2310.10114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Node classification in networks via simplicial interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koo%2C+E">Eunho Koo</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+T">Tongseok Lim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation (stat.CO)

</div>
<p class="mathjax">In the node classification task, it is intuitively understood that densely
connected nodes tend to exhibit similar attributes. However, it is crucial to
first define what constitutes a dense connection and to develop a reliable
mathematical tool for assessing node cohesiveness. In this paper, we propose a
probability-based objective function for semi-supervised node classification
that takes advantage of higher-order networks' capabilities. The proposed
function embodies the philosophy most aligned with the intuition behind
classifying within higher-order networks, as it is designed to reduce the
likelihood of nodes interconnected through higher-order networks bearing
different labels. We evaluate the function using both balanced and imbalanced
datasets generated by the Planted Partition Model (PPM), as well as a
real-world political book dataset. According to the results, in challenging
classification contexts characterized by low homo-connection probability, high
hetero-connection probability, and limited prior information of nodes,
higher-order networks outperform pairwise interactions in terms of objective
function performance. Notably, the objective function exhibits elevated Recall
and F1-score relative to Precision in the imbalanced dataset, indicating its
potential applicability in many domains where detecting false negatives is
critical, even at the expense of some false positives.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10117" title="Abstract">arXiv:2310.10117</a> [<a href="/pdf/2310.10117" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A proximal augmented Lagrangian based algorithm for federated learning  with global and local convex conic constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chuan He</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Le Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Ju Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper considers federated learning (FL) with constraints, where the
central server and all local clients collectively minimize a sum of convex
local objective functions subject to global and local convex conic constraints.
To train the model without moving local data from clients to the central
server, we propose an FL framework in which each local client performs multiple
updates using the local objective and local constraint, while the central
server handles the global constraint and performs aggregation based on the
updated local models. In particular, we develop a proximal augmented Lagrangian
(AL) based algorithm for FL with global and local convex conic constraints. The
subproblems arising in this algorithm are solved by an inexact alternating
direction method of multipliers (ADMM) in a federated fashion. Under a local
Lipschitz condition and mild assumptions, we establish the worst-case
complexity bounds of the proposed algorithm for finding an approximate KKT
solution. To the best of our knowledge, this work proposes the first algorithm
for FL with global and local constraints. Our numerical experiments demonstrate
the practical advantages of our algorithm in performing Neyman-Pearson
classification and enhancing model fairness in the context of FL.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10118" title="Abstract">arXiv:2310.10118</a> [<a href="/pdf/2310.10118" title="Download PDF">pdf</a>, <a href="/format/2310.10118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Rank Context for Named Entity Recognition Using a Synthetic  Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amalvy%2C+A">Arthur Amalvy</a> (LIA), 
<a href="/search/cs?searchtype=author&query=Labatut%2C+V">Vincent Labatut</a> (LIA), 
<a href="/search/cs?searchtype=author&query=Dufour%2C+R">Richard Dufour</a> (LS2N - &#xe9;quipe TALN )
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing, Dec 2023, Singapore, Singapore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While recent pre-trained transformer-based models can perform named entity
recognition (NER) with great accuracy, their limited range remains an issue
when applied to long documents such as whole novels. To alleviate this issue, a
solution is to retrieve relevant context at the document level. Unfortunately,
the lack of supervision for such a task means one has to settle for
unsupervised approaches. Instead, we propose to generate a synthetic context
retrieval training dataset using Alpaca, an instructiontuned large language
model (LLM). Using this dataset, we train a neural context retriever based on a
BERT model that is able to find relevant context for NER. We show that our
method outperforms several retrieval baselines for the NER task on an English
literary dataset composed of the first chapter of 40 books.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10121" title="Abstract">arXiv:2310.10121</a> [<a href="/pdf/2310.10121" title="Download PDF">pdf</a>, <a href="/format/2310.10121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and  Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+A">Andi Han</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+D">Dai Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lequan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Junbin Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Graph neural networks (GNNs) have demonstrated significant promise in
modelling relational data and have been widely applied in various fields of
interest. The key mechanism behind GNNs is the so-called message passing where
information is being iteratively aggregated to central nodes from their
neighbourhood. Such a scheme has been found to be intrinsically linked to a
physical process known as heat diffusion, where the propagation of GNNs
naturally corresponds to the evolution of heat density. Analogizing the process
of message passing to the heat dynamics allows to fundamentally understand the
power and pitfalls of GNNs and consequently informs better model design.
Recently, there emerges a plethora of works that proposes GNNs inspired from
the continuous dynamics formulation, in an attempt to mitigate the known
limitations of GNNs, such as oversmoothing and oversquashing. In this survey,
we provide the first systematic and comprehensive review of studies that
leverage the continuous perspective of GNNs. To this end, we introduce
foundational ingredients for adapting continuous dynamics to GNNs, along with a
general framework for the design of graph neural dynamics. We then review and
categorize existing works based on their driven mechanisms and underlying
dynamics. We also summarize how the limitations of classic GNNs can be
addressed under the continuous framework. We conclude by identifying multiple
open research directions.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10123" title="Abstract">arXiv:2310.10123</a> [<a href="/pdf/2310.10123" title="Download PDF">pdf</a>, <a href="/format/2310.10123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yitong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+T">Tianfan Xue</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinwei Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we aim to solve complex real-world image restoration
situations, in which, one image may have a variety of unknown degradations. To
this end, we propose an all-in-one image restoration framework with latent
diffusion (AutoDIR), which can automatically detect and address multiple
unknown degradations. Our framework first utilizes a Blind Image Quality
Assessment Module (BIQA) to automatically detect and identify the unknown
dominant image degradation type of the image. Then, an All-in-One Image Editing
(AIR) Module handles multiple kinds of degradation image restoration with the
guidance of BIQA. Finally, a Structure Correction Module (SCM) is proposed to
recover the image details distorted by AIR. Our comprehensive evaluation
demonstrates that AutoDIR outperforms state-of-the-art approaches by achieving
superior restoration results while supporting a wider range of tasks. Notably,
AutoDIR is also the first method to automatically handle real-scenario images
with multiple unknown degradations.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10124" title="Abstract">arXiv:2310.10124</a> [<a href="/pdf/2310.10124" title="Download PDF">pdf</a>, <a href="/format/2310.10124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Study of Privacy Risks in Curriculum Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J+Q">Joann Qiongna Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xinlei He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhou Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Training a machine learning model with data following a meaningful order,
i.e., from easy to hard, has been proven to be effective in accelerating the
training process and achieving better model performance. The key enabling
technique is curriculum learning (CL), which has seen great success and has
been deployed in areas like image and text classification. Yet, how CL affects
the privacy of machine learning is unclear. Given that CL changes the way a
model memorizes the training data, its influence on data privacy needs to be
thoroughly evaluated. To fill this knowledge gap, we perform the first study
and leverage membership inference attack (MIA) and attribute inference attack
(AIA) as two vectors to quantify the privacy leakage caused by CL.
<br />Our evaluation of nine real-world datasets with attack methods (NN-based,
metric-based, label-only MIA, and NN-based AIA) revealed new insights about CL.
First, MIA becomes slightly more effective when CL is applied, but the impact
is much more prominent to a subset of training samples ranked as difficult.
Second, a model trained under CL is less vulnerable under AIA, compared to MIA.
Third, the existing defense techniques like DP-SGD, MemGuard, and MixupMMD are
still effective under CL, though DP-SGD has a significant impact on target
model accuracy. Finally, based on our insights into CL, we propose a new MIA,
termed Diff-Cali, which exploits the difficulty scores for result calibration
and is demonstrated to be effective against all CL methods and the normal
training method. With this study, we hope to draw the community's attention to
the unintended privacy risks of emerging machine-learning techniques and
develop new attack benchmarks and defense solutions.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10125" title="Abstract">arXiv:2310.10125</a> [<a href="/pdf/2310.10125" title="Download PDF">pdf</a>, <a href="/format/2310.10125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-shot Action Recognition with Captioning Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hangjie Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Changxin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Deli Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+N">Nong Sang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Transferring vision-language knowledge from pretrained multimodal foundation
models to various downstream tasks is a promising direction. However, most
current few-shot action recognition methods are still limited to a single
visual modality input due to the high cost of annotating additional textual
descriptions. In this paper, we develop an effective plug-and-play framework
called CapFSAR to exploit the knowledge of multimodal models without manually
annotating text. To be specific, we first utilize a captioning foundation model
(i.e., BLIP) to extract visual features and automatically generate associated
captions for input videos. Then, we apply a text encoder to the synthetic
captions to obtain representative text embeddings. Finally, a visual-text
aggregation module based on Transformer is further designed to incorporate
cross-modal spatio-temporal complementary information for reliable few-shot
matching. In this way, CapFSAR can benefit from powerful multimodal knowledge
of pretrained foundation models, yielding more comprehensive classification in
the low-shot regime. Extensive experiments on multiple standard few-shot
benchmarks demonstrate that the proposed CapFSAR performs favorably against
existing methods and achieves state-of-the-art performance. The code will be
made publicly available.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10126" title="Abstract">arXiv:2310.10126</a> [<a href="/pdf/2310.10126" title="Download PDF">pdf</a>, <a href="/format/2310.10126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Non-monotonic Smooth Activation Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biswas%2C+K">Koushik Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+M">Meghana Karri</a>, 
<a href="/search/cs?searchtype=author&query=Ba%C4%9Fc%C4%B1%2C+U">Ula&#x15f; Ba&#x11f;c&#x131;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Activation functions are crucial in deep learning models since they introduce
non-linearity into the networks, allowing them to learn from errors and make
adjustments, which is essential for learning complex patterns. The essential
purpose of activation functions is to transform unprocessed input signals into
significant output activations, promoting information transmission throughout
the neural network. In this study, we propose a new activation function called
Sqish, which is a non-monotonic and smooth function and an alternative to
existing ones. We showed its superiority in classification, object detection,
segmentation tasks, and adversarial robustness experiments. We got an 8.21%
improvement over ReLU on the CIFAR100 dataset with the ShuffleNet V2 model in
the FGSM adversarial attack. We also got a 5.87% improvement over ReLU on image
classification on the CIFAR100 dataset with the ShuffleNet V2 model.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10131" title="Abstract">arXiv:2310.10131</a> [<a href="/pdf/2310.10131" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3DYoga90: A Hierarchical Video Dataset for Yoga Pose Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seonok Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The increasing popularity of exercises including yoga and Pilates has created
a greater demand for professional exercise video datasets in the realm of
artificial intelligence. In this study, we developed 3DYoga901, which is
organized within a three-level label hierarchy. We have expanded the number of
poses from an existing state-of-the-art dataset, increasing it from 82 to 90
poses. Our dataset includes meticulously curated RGB yoga pose videos and 3D
skeleton sequences. This dataset was created by a dedicated team of six
individuals, including yoga instructors. It stands out as one of the most
comprehensive open datasets, featuring the largest collection of RGB videos and
3D skeleton sequences among publicly available resources. This contribution has
the potential to significantly advance the field of yoga action recognition and
pose assessment. Additionally, we conducted experiments to evaluate the
practicality of our proposed dataset. We employed three different model
variants for benchmarking purposes.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10133" title="Abstract">arXiv:2310.10133</a> [<a href="/pdf/2310.10133" title="Download PDF">pdf</a>, <a href="/ps/2310.10133" title="Download PostScript">ps</a>, <a href="/format/2310.10133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering SMPC: Bridging the Gap Between Scalability, Memory Efficiency  and Privacy in Neural Network Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burra%2C+R">Ramya Burra</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+A">Anshoo Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Srishti Mittal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper aims to develop an efficient open-source Secure Multi-Party
Computation (SMPC) repository, that addresses the issue of practical and
scalable implementation of SMPC protocol on machines with moderate
computational resources, while aiming to reduce the execution time. We
implement the ABY2.0 protocol for SMPC, providing developers with effective
tools for building applications on the ABY 2.0 protocol. This article addresses
the limitations of the C++ based MOTION2NX framework for secure neural network
inference, including memory constraints and operation compatibility issues. Our
enhancements include optimizing the memory usage, reducing execution time using
a third-party Helper node, and enhancing efficiency while still preserving data
privacy. These optimizations enable MNIST dataset inference in just 32 seconds
with only 0.2 GB of RAM for a 5-layer neural network. In contrast, the previous
baseline implementation required 8.03 GB of RAM and 200 seconds of execution
time.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10134" title="Abstract">arXiv:2310.10134</a> [<a href="/pdf/2310.10134" title="Download PDF">pdf</a>, <a href="/format/2310.10134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIN: A Continually Learning Language Agent for Rapid Task Adaptation  and Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majumder%2C+B+P">Bodhisattwa Prasad Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+B+D">Bhavana Dalvi Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Jansen%2C+P">Peter Jansen</a>, 
<a href="/search/cs?searchtype=author&query=Tafjord%2C+O">Oyvind Tafjord</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+N">Niket Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Callison-Burch%2C+C">Chris Callison-Burch</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+P">Peter Clark</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://allenai.github.io/clin/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Language agents have shown some ability to interact with an external
environment, e.g., a virtual world such as ScienceWorld, to perform complex
tasks, e.g., growing a plant, without the startup costs of reinforcement
learning. However, despite their zero-shot capabilities, these agents to date
do not continually improve over time beyond performance refinement on a
specific task. Here we present CLIN, the first language-based agent to achieve
this, so that it continually improves over multiple trials, including when both
the environment and task are varied, and without requiring parameter updates.
Our approach is to use a persistent, dynamic, textual memory centered on causal
abstractions (rather than general "helpful hints") that is regularly updated
after each trial so that the agent gradually learns useful knowledge for new
trials. In the ScienceWorld benchmark, CLIN is able to continually improve on
repeated trials on the same task and environment, outperforming
state-of-the-art reflective language agents like Reflexion by 23 absolute
points. CLIN can also transfer its learning to new environments (or new tasks),
improving its zero-shot performance by 4 points (13 for new tasks) and can
further improve performance there through continual memory updates, enhancing
performance by an additional 17 points (7 for new tasks). This suggests a new
architecture for agents built on frozen models that can still continually and
rapidly improve over time.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10136" title="Abstract">arXiv:2310.10136</a> [<a href="/pdf/2310.10136" title="Download PDF">pdf</a>, <a href="/format/2310.10136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mata, a Fast and Simple Finite Automata Library
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chocholat%C3%BD%2C+D">David Chocholat&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=Fiedor%2C+T">Tom&#xe1;&#x161; Fiedor</a>, 
<a href="/search/cs?searchtype=author&query=Havlena%2C+V">Vojt&#x11b;ch Havlena</a>, 
<a href="/search/cs?searchtype=author&query=Hol%C3%ADk%2C+L">Luk&#xe1;&#x161; Hol&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=Hru%C5%A1ka%2C+M">Martin Hru&#x161;ka</a>, 
<a href="/search/cs?searchtype=author&query=Leng%C3%A1l%2C+O">Ond&#x159;ej Leng&#xe1;l</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%AD%C4%8D%2C+J">Juraj S&#xed;&#x10d;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">Mata is a well-engineered automata library written in C++ that offers a
unique combination of speed and simplicity. It is mean to serve in applications
such as string constraint solving and reasoning about regular expressions, and
a as reference implementation of automata algorithms. Besides basic algorithms
for (non)deterministic automata, it implements a fast simulation reduction and
antichain-based language inclusion checking. The simplicity allows a
straightforward access to the low level structures, making it relatively easy
to extend and modify. Besides the C++ API, the library also implements a Python
binding.
<br />The library comes with a large benchmark of automata problems collected from
relevant applications such as string constraint solving, regular model
checking, and reasoning about regular expressions. We show that Mata is on this
benchmark significantly faster than all libraries from a wide range of automata
libraries we collected. Its usefulness in string constraint solving is
demonstrated by the string solver Z3-Noodler, which is based on Mata and
outperforms the state of the art in string constraint solving on many standard
benchmarks.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10137" title="Abstract">arXiv:2310.10137</a> [<a href="/pdf/2310.10137" title="Download PDF">pdf</a>, <a href="/format/2310.10137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stiffness-Oriented Model Order Reduction Method for Low-Inertia Power  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Muntwiler%2C+S">Simon Muntwiler</a>, 
<a href="/search/eess?searchtype=author&query=Stanojev%2C+O">Ognjen Stanojev</a>, 
<a href="/search/eess?searchtype=author&query=Zanelli%2C+A">Andrea Zanelli</a>, 
<a href="/search/eess?searchtype=author&query=Hug%2C+G">Gabriela Hug</a>, 
<a href="/search/eess?searchtype=author&query=Zeilinger%2C+M+N">Melanie N. Zeilinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, submitted to PSCC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents a novel model order reduction technique tailored for
power systems with a large share of inverter-based energy resources. Such
systems exhibit an increased level of dynamic stiffness compared to traditional
power systems, posing challenges for time-domain simulations and control
design. Our approach involves rotation of the coordinate system of a linearized
system using a transformation matrix derived from the real Jordan canonical
form, leading to mode decoupling. The fast modes are then truncated in the
rotated coordinate system to obtain a lower-order model with reduced stiffness.
Applying the same transformation to the original nonlinear system results in an
approximate separation of slow and fast states, which can be truncated to
reduce the stiffness. The resulting reduced-order model demonstrates an
accurate time-domain performance, the slow eigenvalues of the linearized system
are correctly preserved, and a reduction in the model stiffness is achieved,
allowing for accurate integration with increased step size. Our methodology is
assessed in detail for a 3-bus system with generation units involving
grid-forming/following converters and synchronous machines, where it allows for
a computational speed-up of up to 100x compared to the original system. Several
standard larger test systems are also considered.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10138" title="Abstract">arXiv:2310.10138</a> [<a href="/pdf/2310.10138" title="Download PDF">pdf</a>, <a href="/format/2310.10138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Node-based Knowledge Graph Contrastive Learning for Medical Relationship  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhiguang Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuedong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongming Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages,5 figures,conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Computation and Language (cs.CL); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">The embedding of Biomedical Knowledge Graphs (BKGs) generates robust
representations, valuable for a variety of artificial intelligence
applications, including predicting drug combinations and reasoning disease-drug
relationships. Meanwhile, contrastive learning (CL) is widely employed to
enhance the distinctiveness of these representations. However, constructing
suitable contrastive pairs for CL, especially within Knowledge Graphs (KGs),
has been challenging. In this paper, we proposed a novel node-based contrastive
learning method for knowledge graph embedding, NC-KGE. NC-KGE enhances
knowledge extraction in embeddings and speeds up training convergence by
constructing appropriate contrastive node pairs on KGs. This scheme can be
easily integrated with other knowledge graph embedding (KGE) methods. For
downstream task such as biochemical relationship prediction, we have
incorporated a relation-aware attention mechanism into NC-KGE, focusing on the
semantic relationships and node interactions. Extensive experiments show that
NC-KGE performs competitively with state-of-the-art models on public datasets
like FB15k-237 and WN18RR. Particularly in biomedical relationship prediction
tasks, NC-KGE outperforms all baselines on datasets such as PharmKG8k-28,
DRKG17k-21, and BioKG72k-14, especially in predicting drug combination
relationships. We release our code at https://github.com/zhi520/NC-KGE.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10139" title="Abstract">arXiv:2310.10139</a> [<a href="/pdf/2310.10139" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> White paper on cybersecurity in the healthcare sector. The HEIR solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lampropoulos%2C+K">Konstantinos Lampropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Zarras%2C+A">Apostolis Zarras</a>, 
<a href="/search/cs?searchtype=author&query=Lakka%2C+E">Eftychia Lakka</a>, 
<a href="/search/cs?searchtype=author&query=Barmpaki%2C+P">Polyanthi Barmpaki</a>, 
<a href="/search/cs?searchtype=author&query=Drakonakis%2C+K">Kostas Drakonakis</a>, 
<a href="/search/cs?searchtype=author&query=Athanatos%2C+M">Manos Athanatos</a>, 
<a href="/search/cs?searchtype=author&query=Debar%2C+H">Herve Debar</a>, 
<a href="/search/cs?searchtype=author&query=Alexopoulos%2C+A">Andreas Alexopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Sotiropoulos%2C+A">Aristeidis Sotiropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Tsakirakis%2C+G">George Tsakirakis</a>, 
<a href="/search/cs?searchtype=author&query=Dimakopoulos%2C+N">Nikos Dimakopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Tsolovos%2C+D">Dimitris Tsolovos</a>, 
<a href="/search/cs?searchtype=author&query=Pocs%2C+M">Matthias Pocs</a>, 
<a href="/search/cs?searchtype=author&query=Smyrlis%2C+M">Michalis Smyrlis</a>, 
<a href="/search/cs?searchtype=author&query=Basdekis%2C+I">Ioannis Basdekis</a>, 
<a href="/search/cs?searchtype=author&query=Spanoudakis%2C+G">Georgios Spanoudakis</a>, 
<a href="/search/cs?searchtype=author&query=Mihaila%2C+O">Ovidiu Mihaila</a>, 
<a href="/search/cs?searchtype=author&query=Prelipcean%2C+B">Bogdan Prelipcean</a>, 
<a href="/search/cs?searchtype=author&query=Salant%2C+E">Eliot Salant</a>, 
<a href="/search/cs?searchtype=author&query=Athanassopoulos%2C+S">Sotiris Athanassopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Papachristou%2C+P">Petros Papachristou</a>, 
<a href="/search/cs?searchtype=author&query=Ladakis%2C+I">Ioannis Ladakis</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">John Chang</a>, 
<a href="/search/cs?searchtype=author&query=Floros%2C+E">Evangelos Floros</a>, 
<a href="/search/cs?searchtype=author&query=Smyrlis%2C+K">Konstantinos Smyrlis</a>, 
<a href="/search/cs?searchtype=author&query=Besters%2C+R">Rouven Besters</a>, 
<a href="/search/cs?searchtype=author&query=Randine%2C+P">Pietro Randine</a>, 
<a href="/search/cs?searchtype=author&query=Lovaas%2C+K+F">Karianna Fjeld Lovaas</a>, 
<a href="/search/cs?searchtype=author&query=Cooper%2C+J">John Cooper</a>, 
<a href="/search/cs?searchtype=author&query=Ilie%2C+I">Iulia Ilie</a>, 
<a href="/search/cs?searchtype=author&query=Danciu%2C+G">Gabriel Danciu</a>, 
<a href="/search/cs?searchtype=author&query=Khabbaz%2C+M+D">Marwan Darwish Khabbaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 70 pages, 48 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The healthcare sector is increasingly vulnerable to cyberattacks due to its
growing digitalization. Patient data, including medical records and financial
information, are at risk, potentially leading to identity theft and patient
safety concerns. The European Union and other organizations identify key areas
for healthcare system improvement, yet the industry still grapples with
inadequate security practices. In response, the HEIR project offers a
comprehensive cybersecurity approach, promoting security features from various
regulatory frameworks and introducing tools such as the Secure Healthcare
Framework and Risk Assessment for Medical Applications (RAMA). These measures
aim to enhance digital health security and protect sensitive patient data while
facilitating secure data access and privacy-aware techniques. In a rapidly
evolving threat landscape, HEIR presents a promising framework for healthcare
cybersecurity.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10141" title="Abstract">arXiv:2310.10141</a> [<a href="/pdf/2310.10141" title="Download PDF">pdf</a>, <a href="/format/2310.10141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Search for Prompts: Generating Structured Answers from Contracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roegiest%2C+A">Adam Roegiest</a>, 
<a href="/search/cs?searchtype=author&query=Chitta%2C+R">Radha Chitta</a>, 
<a href="/search/cs?searchtype=author&query=Donnelly%2C+J">Jonathan Donnelly</a>, 
<a href="/search/cs?searchtype=author&query=Lash%2C+M">Maya Lash</a>, 
<a href="/search/cs?searchtype=author&query=Vtyurina%2C+A">Alexandra Vtyurina</a>, 
<a href="/search/cs?searchtype=author&query=Longtin%2C+F">Fran&#xe7;ois Longtin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In many legal processes being able to action on the concrete implication of a
legal question can be valuable to automating human review or signalling certain
conditions (e.g., alerts around automatic renewal). To support such tasks, we
present a form of legal question answering that seeks to return one (or more)
fixed answers for a question about a contract clause. After showing that
unstructured generative question answering can have questionable outcomes for
such a task, we discuss our exploration methodology for legal question
answering prompts using OpenAI's \textit{GPT-3.5-Turbo} and provide a summary
of insights.
<br />Using insights gleaned from our qualitative experiences, we compare our
proposed template prompts against a common semantic matching approach and find
that our prompt templates are far more accurate despite being less reliable in
the exact response return. With some additional tweaks to prompts and the use
of in-context learning, we are able to further improve the performance of our
proposed strategy while maximizing the reliability of responses as best we can.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10146" title="Abstract">arXiv:2310.10146</a> [<a href="/pdf/2310.10146" title="Download PDF">pdf</a>, <a href="/ps/2310.10146" title="Download PostScript">ps</a>, <a href="/format/2310.10146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A CJ-FEAST GSVDsolver for computing a partial GSVD of a large matrix  pair with the generalized singular values in a given interval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jia%2C+Z">Zhongxiao Jia</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+K">Kailiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a CJ-FEAST GSVDsolver to compute a partial generalized singular
value decomposition (GSVD) of a large matrix pair $(A,B)$ with the generalized
singular values in a given interval. The solver is a highly nontrivial
extension of the FEAST eigensolver for the (generalized) eigenvalue problem and
CJ-FEAST SVDsolver for the SVD problem. For a partial GSVD problem, given three
left and right searching subspaces, we propose a general projection method that
works on $(A,B)$ {\em directly}, and computes approximations to the desired
GSVD components. For the concerning GSVD problem, we exploit the
Chebyshev--Jackson (CJ) series to construct an approximate spectral projector
of the generalized eigenvalue problem of the matrix pair $(A^TA,B^TB)$
associated with the generalized singular values of interest, and use subspace
iteration on it to generate a right subspace. Premultiplying it with $A$ and
$B$ constructs two left subspaces. Applying the general projection method to
the subspaces constructed leads to the CJ-FEAST GSVDsolver. We derive accuracy
estimates for the approximate spectral projector and its eigenvalues, and
establish a number of convergence results on the underlying subspaces and the
approximate GSVD components obtained by the CJ-FEAST GSVDsolver. We propose
general-purpose choice strategies for the series degree and subspace dimension.
Numerical experiments illustrate the efficiency of the CJ-FEAST GSVDsolver.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10147" title="Abstract">arXiv:2310.10147</a> [<a href="/pdf/2310.10147" title="Download PDF">pdf</a>, <a href="/ps/2310.10147" title="Download PostScript">ps</a>, <a href="/format/2310.10147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Block-missing data in linear systems: An unbiased stochastic gradient  descent approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huynh%2C+C">Chelsea Huynh</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+A">Anna Ma</a>, 
<a href="/search/math?searchtype=author&query=Strand%2C+M">Michael Strand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Achieving accurate approximations to solutions of large linear systems is
crucial, especially when those systems utilize real-world data. A consequence
of using real-world data is that there will inevitably be missingness. Current
approaches for dealing with missing data, such as deletion and imputation, can
introduce bias. Recent studies proposed an adaptation of stochastic gradient
descent (SGD) in specific missing-data models. In this work, we propose a new
algorithm, $\ell$-tuple mSGD, for the setting in which data is missing in a
block-wise, tuple pattern. We prove that our proposed method uses unbiased
estimates of the gradient of the least squares objective in the presence of
tuple missing data. We also draw connections between $\ell$-tuple mSGD and
previously established SGD-type methods for missing data. Furthermore, we prove
our algorithm converges when using updating step sizes and empirically
demonstrate the convergence of $\ell$-tuple mSGD on synthetic data. Lastly, we
evaluate $\ell$-tuple mSGD applied to real-world continuous glucose monitoring
(CGM) device data.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10149" title="Abstract">arXiv:2310.10149</a> [<a href="/pdf/2310.10149" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach  for Computing Structural Beauty of Images or the Livingness of Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qianxiang%2C+Y">Yao Qianxiang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bin Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study introduces the concept of "structural beauty" as an objective
computational approach for evaluating the aesthetic appeal of images. Through
the utilization of the Segment anything model (SAM), we propose a method that
leverages recursive segmentation to extract finer-grained substructures.
Additionally, by reconstructing the hierarchical structure, we obtain a more
accurate representation of substructure quantity and hierarchy. This approach
reproduces and extends our previous research, allowing for the simultaneous
assessment of Livingness in full-color images without the need for grayscale
conversion or separate computations for foreground and background Livingness.
Furthermore, the application of our method to the Scenic or Not dataset, a
repository of subjective scenic ratings, demonstrates a high degree of
consistency with subjective ratings in the 0-6 score range. This underscores
that structural beauty is not solely a subjective perception, but a
quantifiable attribute accessible through objective computation. Through our
case studies, we have arrived at three significant conclusions. 1) our method
demonstrates the capability to accurately segment meaningful objects, including
trees, buildings, and windows, as well as abstract substructures within
paintings. 2) we observed that the clarity of an image impacts our
computational results; clearer images tend to yield higher Livingness scores.
However, for equally blurry images, Livingness does not exhibit a significant
reduction, aligning with human visual perception. 3) our approach fundamentally
differs from methods employing Convolutional Neural Networks (CNNs) for
predicting image scores. Our method not only provides computational results but
also offers transparency and interpretability, positioning it as a novel avenue
in the realm of Explainable AI (XAI).
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10151" title="Abstract">arXiv:2310.10151</a> [<a href="/pdf/2310.10151" title="Download PDF">pdf</a>, <a href="/format/2310.10151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DNA: Denoised Neighborhood Aggregation for Fine-grained Category  Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+W">Wenbin An</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+F">Feng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenkai Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qinghua Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">QianYing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Ping Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Discovering fine-grained categories from coarsely labeled data is a practical
and challenging task, which can bridge the gap between the demand for
fine-grained analysis and the high annotation cost. Previous works mainly focus
on instance-level discrimination to learn low-level features, but ignore
semantic similarities between data, which may prevent these models learning
compact cluster representations. In this paper, we propose Denoised
Neighborhood Aggregation (DNA), a self-supervised framework that encodes
semantic structures of data into the embedding space. Specifically, we retrieve
k-nearest neighbors of a query as its positive keys to capture semantic
similarities between data and then aggregate information from the neighbors to
learn compact cluster representations, which can make fine-grained categories
more separatable. However, the retrieved neighbors can be noisy and contain
many false-positive keys, which can degrade the quality of learned embeddings.
To cope with this challenge, we propose three principles to filter out these
false neighbors for better representation learning. Furthermore, we
theoretically justify that the learning objective of our framework is
equivalent to a clustering loss, which can capture semantic similarities
between data to form compact fine-grained clusters. Extensive experiments on
three benchmark datasets show that our method can retrieve more accurate
neighbors (21.31% accuracy improvement) and outperform state-of-the-art models
by a large margin (average 9.96% improvement on three metrics). Our code and
data are available at https://github.com/Lackel/DNA.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10155" title="Abstract">arXiv:2310.10155</a> [<a href="/pdf/2310.10155" title="Download PDF">pdf</a>, <a href="/format/2310.10155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis and implementation of nanotargeting on LinkedIn based on  publicly available non-PII
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Merino%2C+%C3%81">&#xc1;ngel Merino</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Caba%C3%B1as%2C+J">Jos&#xe9; Gonz&#xe1;lez-Caba&#xf1;as</a>, 
<a href="/search/cs?searchtype=author&query=Cuevas%2C+%C3%81">&#xc1;ngel Cuevas</a>, 
<a href="/search/cs?searchtype=author&query=Cuevas%2C+R">Rub&#xe9;n Cuevas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">A body of literature has shown multiple times that combining a few
non-Personal Identifiable Information (non-PII) items is enough to make a user
unique in a dataset including millions or even hundreds of millions of users.
This work extends this area of research, demonstrating that a combination of a
few non-PII publicly available attributes can be activated by a third party to
individually target a user with hyper-personalized messages. This paper first
implements a methodology demonstrating that the combination of the location and
6 rare (or 14 random) professional skills reported by a user in their LinkedIn
profile is enough to become unique in a user base formed by $\sim$800M users
with a probability of 75\%. A novel feature in this case, compared to previous
works in the literature, is that the location and skills reported in a LinkedIn
profile are publicly accessible to any other user or company registered in the
platform and, in addition, can be activated through advertising campaigns. We
ran a proof of concept experiment targeting three of the paper's authors. We
demonstrated that all the ad campaigns configured with the location and
$\geq$13 random professional skills retrieved from the authors' LinkedIn
profiles successfully delivered ads exclusively to the targeted user. This
practice is referred to as nanotargeting and may expose LinkedIn users to
potential privacy and security risks such as malvertising or manipulation.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10157" title="Abstract">arXiv:2310.10157</a> [<a href="/pdf/2310.10157" title="Download PDF">pdf</a>, <a href="/format/2310.10157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Workload Distribution for Accuracy-aware DNN Inference on  Collaborative Edge Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taufique%2C+Z">Zain Taufique</a>, 
<a href="/search/cs?searchtype=author&query=Miele%2C+A">Antonio Miele</a>, 
<a href="/search/cs?searchtype=author&query=Liljeberg%2C+P">Pasi Liljeberg</a>, 
<a href="/search/cs?searchtype=author&query=Kanduri%2C+A">Anil Kanduri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 9 figures, 1 table, 1 algorithm, to be published in 29th Asia and South Pacific Design Automation Conference (IEEE ASP-DAC 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Performance (cs.PF); Systems and Control (eess.SY)

</div>
<p class="mathjax">DNN inference can be accelerated by distributing the workload among a cluster
of collaborative edge nodes. Heterogeneity among edge devices and
accuracy-performance trade-offs of DNN models present a complex exploration
space while catering to the inference performance requirements. In this work,
we propose adaptive workload distribution for DNN inference, jointly
considering node-level heterogeneity of edge devices, and application-specific
accuracy and performance requirements. Our proposed approach combinatorially
optimizes heterogeneity-aware workload partitioning and dynamic accuracy
configuration of DNN models to ensure performance and accuracy guarantees. We
tested our approach on an edge cluster of Odroid XU4, Raspberry Pi4, and Jetson
Nano boards and achieved an average gain of 41.52% in performance and 5.2% in
output accuracy as compared to state-of-the-art workload distribution
strategies.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10158" title="Abstract">arXiv:2310.10158</a> [<a href="/pdf/2310.10158" title="Download PDF">pdf</a>, <a href="/format/2310.10158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Character-LLM: A Trainable Agent for Role-Playing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yunfan Shao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Junqi Dai</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023; Repo at <a href="https://github.com/choosewhatulike/trainable-agents">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) can be used to serve as agents to simulate human
behaviors, given the powerful ability to understand human instructions and
provide high-quality generated texts. Such ability stimulates us to wonder
whether LLMs can simulate a person in a higher form than simple human
behaviors. Therefore, we aim to train an agent with the profile, experience,
and emotional states of a specific person instead of using limited prompts to
instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs
to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar,
etc. Our method focuses on editing profiles as experiences of a certain
character and training models to be personal simulacra with these experiences.
To assess the effectiveness of our approach, we build a test playground that
interviews trained agents and evaluates whether the agents \textit{memorize}
their characters and experiences. Experimental results show interesting
observations that help build future simulacra of humankind.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10159" title="Abstract">arXiv:2310.10159</a> [<a href="/pdf/2310.10159" title="Download PDF">pdf</a>, <a href="/format/2310.10159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Music and Language Attention Models for Zero-shot Music Tagging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xingjian Du</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhesong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiaju Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bilei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Q">Qiuqiang Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> \begin{keywords} Music tagging, joint music and language attention models, Music Foundation Model. \end{keywords}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Music tagging is a task to predict the tags of music recordings. However,
previous music tagging research primarily focuses on close-set music tagging
tasks which can not be generalized to new tags. In this work, we propose a
zero-shot music tagging system modeled by a joint music and language attention
(JMLA) model to address the open-set music tagging problem. The JMLA model
consists of an audio encoder modeled by a pretrained masked autoencoder and a
decoder modeled by a Falcon7B. We introduce preceiver resampler to convert
arbitrary length audio into fixed length embeddings. We introduce dense
attention connections between encoder and decoder layers to improve the
information flow between the encoder and decoder layers. We collect a
large-scale music and description dataset from the internet. We propose to use
ChatGPT to convert the raw descriptions into formalized and diverse
descriptions to train the JMLA models. Our proposed JMLA system achieves a
zero-shot audio tagging accuracy of $ 64.82\% $ on the GTZAN dataset,
outperforming previous zero-shot systems and achieves comparable results to
previous systems on the FMA and the MagnaTagATune datasets.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10166" title="Abstract">arXiv:2310.10166</a> [<a href="/pdf/2310.10166" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Road to On-board Change Detection: A Lightweight Patch-Level Change  Detection Network via Exploring the Potential of Pruning and Pooling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Lihui Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing satellite remote sensing change detection (CD) methods often crop
original large-scale bi-temporal image pairs into small patch pairs and then
use pixel-level CD methods to fairly process all the patch pairs. However, due
to the sparsity of change in large-scale satellite remote sensing images,
existing pixel-level CD methods suffer from a waste of computational cost and
memory resources on lots of unchanged areas, which reduces the processing
efficiency of on-board platform with extremely limited computation and memory
resources. To address this issue, we propose a lightweight patch-level CD
network (LPCDNet) to rapidly remove lots of unchanged patch pairs in
large-scale bi-temporal image pairs. This is helpful to accelerate the
subsequent pixel-level CD processing stage and reduce its memory costs. In our
LPCDNet, a sensitivity-guided channel pruning method is proposed to remove
unimportant channels and construct the lightweight backbone network on basis of
ResNet18 network. Then, the multi-layer feature compression (MLFC) module is
designed to compress and fuse the multi-level feature information of
bi-temporal image patch. The output of MLFC module is fed into the
fully-connected decision network to generate the predicted binary label.
Finally, a weighted cross-entropy loss is utilized in the training process of
network to tackle the change/unchange class imbalance problem. Experiments on
two CD datasets demonstrate that our LPCDNet achieves more than 1000 frames per
second on an edge computation platform, i.e., NVIDIA Jetson AGX Orin, which is
more than 3 times that of the existing methods without noticeable CD
performance loss. In addition, our method reduces more than 60% memory costs of
the subsequent pixel-level CD processing stage.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10168" title="Abstract">arXiv:2310.10168</a> [<a href="/pdf/2310.10168" title="Download PDF">pdf</a>, <a href="/format/2310.10168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DaPPA: A Data-Parallel Framework for Processing-in-Memory Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+G+F">Geraldo F. Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Kohli%2C+A">Alain Kohli</a>, 
<a href="/search/cs?searchtype=author&query=Novo%2C+D">David Novo</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Luna%2C+J">Juan G&#xf3;mez-Luna</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">To ease the programmability of PIM architectures, we propose
DaPPA(data-parallel processing-in-memory architecture), a framework that can,
for a given application, automatically distribute input and gather output data,
handle memory management, and parallelize work across the DPUs. The key idea
behind DaPPA is to remove the responsibility of managing hardware resources
from the programmer by providing an intuitive data-parallel pattern-based
programming interface that abstracts the hardware components of the UPMEM
system. Using this key idea, DaPPA transforms a data-parallel pattern-based
application code into the appropriate UPMEM-target code, including the required
APIs for data management and code partition, which can then be compiled into a
UPMEM-based binary transparently from the programmer. While generating
UPMEM-target code, DaPPA implements several code optimizations to improve
end-to-end performance.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10169" title="Abstract">arXiv:2310.10169</a> [<a href="/pdf/2310.10169" title="Download PDF">pdf</a>, <a href="/format/2310.10169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy  Slot Filling Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+T">Tingfeng Hui</a>, 
<a href="/search/cs?searchtype=author&query=GongQue%2C+Z">Zhuoma GongQue</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinxu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Daichi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Gang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 (Short Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, prompt-based generative frameworks have shown impressive
capabilities in sequence labeling tasks. However, in practical dialogue
scenarios, relying solely on simplistic templates and traditional corpora
presents a challenge for these methods in generalizing to unknown input
perturbations. To address this gap, we propose a multi-task demonstration based
generative framework for noisy slot filling, named DemoNSF. Specifically, we
introduce three noisy auxiliary tasks, namely noisy recovery (NR), random mask
(RM), and hybrid discrimination (HD), to implicitly capture semantic structural
information of input perturbations at different granularities. In the
downstream main task, we design a noisy demonstration construction strategy for
the generative framework, which explicitly incorporates task-specific
information and perturbed distribution during training and inference.
Experiments on two benchmarks demonstrate that DemoNSF outperforms all baseline
methods and achieves strong generalization. Further analysis provides empirical
guidance for the practical application of generative frameworks. Our code is
released at https://github.com/dongguanting/Demo-NSF.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10170" title="Abstract">arXiv:2310.10170</a> [<a href="/pdf/2310.10170" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Knowledge Distillation for Efficient Deep Reinforcement  Learning in Resource-Constrained Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+G">Guanlin Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper aims to explore the potential of combining Deep Reinforcement
Learning (DRL) with Knowledge Distillation (KD) by distilling various DRL
algorithms and studying their distillation effects. By doing so, the
computational burden of deep models could be reduced while maintaining the
performance. The primary objective is to provide a benchmark for evaluating the
performance of different DRL algorithms that have been refined using KD
techniques. By distilling these algorithms, the goal is to develop efficient
and fast DRL models. This research is expected to provide valuable insights
that can facilitate further advancements in this promising direction. By
exploring the combination of DRL and KD, this work aims to promote the
development of models that require fewer GPU resources, learn more quickly, and
make faster decisions in complex environments. The results of this research
have the capacity to significantly advance the field of DRL and pave the way
for the future deployment of resource-efficient, decision-making intelligent
systems.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10174" title="Abstract">arXiv:2310.10174</a> [<a href="/pdf/2310.10174" title="Download PDF">pdf</a>, <a href="/format/2310.10174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing An After-Sales Service Process Using Object-Centric Process  Mining: A Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+G">Gyunam Park</a>, 
<a href="/search/cs?searchtype=author&query=Aydin%2C+S">Sevde Aydin</a>, 
<a href="/search/cs?searchtype=author&query=Ugur%2C+C">Cuneyt Ugur</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Aalst%2C+W+M+P">Wil M. P. van der Aalst</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Process mining, a technique turning event data into business process
insights, has traditionally operated on the assumption that each event
corresponds to a singular case or object. However, many real-world processes
are intertwined with multiple objects, making them object-centric. This paper
focuses on the emerging domain of object-centric process mining, highlighting
its potential yet underexplored benefits in actual operational scenarios.
Through an in-depth case study of Borusan Cat's after-sales service process,
this study emphasizes the capability of object-centric process mining to
capture entangled business process details. Utilizing an event log of
approximately 65,000 events, our analysis underscores the importance of
embracing this paradigm for richer business insights and enhanced operational
improvements.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10176" title="Abstract">arXiv:2310.10176</a> [<a href="/pdf/2310.10176" title="Download PDF">pdf</a>, <a href="/format/2310.10176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Meet Open-World Intent Discovery and Recognition:  An Evaluation of ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaoshuai Song</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+Y">Yutao Mou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Yunsen Xian</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xunliang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accpeted to EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The tasks of out-of-domain (OOD) intent discovery and generalized intent
discovery (GID) aim to extend a closed intent classifier to open-world intent
sets, which is crucial to task-oriented dialogue (TOD) systems. Previous
methods address them by fine-tuning discriminative models. Recently, although
some studies have been exploring the application of large language models
(LLMs) represented by ChatGPT to various downstream tasks, it is still unclear
for the ability of ChatGPT to discover and incrementally extent OOD intents. In
this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and
GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT
exhibits consistent advantages under zero-shot settings, but is still at a
disadvantage compared to fine-tuned models. More deeply, through a series of
analytical experiments, we summarize and discuss the challenges faced by LLMs
including clustering, domain-specific understanding, and cross-domain
in-context learning scenarios. Finally, we provide empirical guidance for
future directions to address these challenges.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10177" title="Abstract">arXiv:2310.10177</a> [<a href="/pdf/2310.10177" title="Download PDF">pdf</a>, <a href="/format/2310.10177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypergraph Echo State Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lien%2C+J">Justin Lien</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A hypergraph as a generalization of graphs records higher-order interactions
among nodes, yields a more flexible network model, and allows non-linear
features for a group of nodes. In this article, we propose a hypergraph echo
state network (HypergraphESN) as a generalization of graph echo state network
(GraphESN) designed for efficient processing of hypergraph-structured data,
derive convergence conditions for the algorithm, and discuss its versatility in
comparison to GraphESN. The numerical experiments on the binary classification
tasks demonstrate that HypergraphESN exhibits comparable or superior accuracy
performance to GraphESN for hypergraph-structured data, and accuracy increases
if more higher-order interactions in a network are identified.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10180" title="Abstract">arXiv:2310.10180</a> [<a href="/pdf/2310.10180" title="Download PDF">pdf</a>, <a href="/format/2310.10180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jing Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianhao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yichun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingxing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Ming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automated theorem proving (ATP) has become an appealing domain for exploring
the reasoning ability of the recent successful generative language models.
However, current ATP benchmarks mainly focus on symbolic inference, but rarely
involve the understanding of complex number combination reasoning. In this
work, we propose TRIGO, an ATP benchmark that not only requires a model to
reduce a trigonometric expression with step-by-step proofs but also evaluates a
generative LM's reasoning ability on formulas and its capability to manipulate,
group, and factor number terms. We gather trigonometric expressions and their
reduced forms from the web, annotate the simplification process manually, and
translate it into the ``Lean'' formal language system. We then automatically
generate additional examples from the annotated samples to expand the dataset.
Furthermore, we develop an automatic generator based on Lean-Gym to create
dataset splits of varying difficulties and distributions in order to thoroughly
analyze the model's generalization ability. Our extensive experiments show our
proposed TRIGO poses a new challenge for advanced generative LM's including
GPT-4 which is pre-trained on a considerable amount of open-source formal
theorem-proving language data, and provide a new tool to study the generative
LM's ability on both formal and mathematical reasoning.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10184" title="Abstract">arXiv:2310.10184</a> [<a href="/pdf/2310.10184" title="Download PDF">pdf</a>, <a href="/format/2310.10184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Generalized Intent Discovery: Marching Towards Dynamic and  Open-world Intent Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaoshuai Song</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+Y">Yutao Mou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yueyan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accpeted to EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In a practical dialogue system, users may input out-of-domain (OOD) queries.
The Generalized Intent Discovery (GID) task aims to discover OOD intents from
OOD queries and extend them to the in-domain (IND) classifier. However, GID
only considers one stage of OOD learning, and needs to utilize the data in all
previous stages for joint training, which limits its wide application in
reality. In this paper, we introduce a new task, Continual Generalized Intent
Discovery (CGID), which aims to continuously and automatically discover OOD
intents from dynamic OOD data streams and then incrementally add them to the
classifier with almost no previous data, thus moving towards dynamic intent
recognition in an open world. Next, we propose a method called Prototype-guided
Learning with Replay and Distillation (PLRD) for CGID, which bootstraps new
intent discovery through class prototypes and balances new and old intents
through data replay and feature distillation. Finally, we conduct detailed
experiments and analysis to verify the effectiveness of PLRD and understand the
key challenges of CGID for future research.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10187" title="Abstract">arXiv:2310.10187</a> [<a href="/pdf/2310.10187" title="Download PDF">pdf</a>, <a href="/format/2310.10187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Interpretable Deep-Learning Framework for Predicting Hospital  Readmissions From Electronic Health Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azzalini%2C+F">Fabio Azzalini</a>, 
<a href="/search/cs?searchtype=author&query=Dolci%2C+T">Tommaso Dolci</a>, 
<a href="/search/cs?searchtype=author&query=Vagaggini%2C+M">Marco Vagaggini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">With the increasing availability of patients' data, modern medicine is
shifting towards prospective healthcare. Electronic health records contain a
variety of information useful for clinical patient description and can be
exploited for the construction of predictive models, given that similar medical
histories will likely lead to similar progressions. One example is unplanned
hospital readmission prediction, an essential task for reducing hospital costs
and improving patient health. Despite predictive models showing very good
performances especially with deep-learning models, they are often criticized
for the poor interpretability of their results, a fundamental characteristic in
the medical field, where incorrect predictions might have serious consequences
for the patient health. In this paper we propose a novel, interpretable
deep-learning framework for predicting unplanned hospital readmissions,
supported by NLP findings on word embeddings and by neural-network models
(ConvLSTM) for better handling temporal data. We validate our system on the two
predictive tasks of hospital readmission within 30 and 180 days, using
real-world data. In addition, we introduce and test a model-dependent technique
to make the representation of results easily interpretable by the medical
staff. Our solution achieves better performances compared to traditional models
based on machine learning, while providing at the same time more interpretable
results.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10190" title="Abstract">arXiv:2310.10190</a> [<a href="/pdf/2310.10190" title="Download PDF">pdf</a>, <a href="/format/2310.10190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco  vs Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shuo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jiahuan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yuze Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+D">Donovan Ong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jian Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The success of ChatGPT has ignited an AI race, with researchers striving to
develop new large language models (LLMs) that can match or surpass the language
understanding and generation abilities of commercial ones. In recent times, a
number of models have emerged, claiming performance near that of GPT-3.5 or
GPT-4 through various instruction-tuning methods. As practitioners of
Text-to-SQL parsing, we are grateful for their valuable contributions to
open-source research. However, it is important to approach these claims with a
sense of scrutiny and ascertain the actual effectiveness of these models.
Therefore, we pit six popular large language models against each other,
systematically evaluating their Text-to-SQL parsing capability on nine
benchmark datasets with five different prompting strategies, covering both
zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell
significantly short of the performance achieved by closed-source models like
GPT-3.5, highlighting the need for further work to bridge the performance gap
between these models.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10191" title="Abstract">arXiv:2310.10191</a> [<a href="/pdf/2310.10191" title="Download PDF">pdf</a>, <a href="/format/2310.10191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VIBE: Topic-Driven Temporal Adaptation for Twitter Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language features are evolving in real-world social media, resulting in the
deteriorating performance of text classification in dynamics. To address this
challenge, we study temporal adaptation, where models trained on past data are
tested in the future. Most prior work focused on continued pretraining or
knowledge updating, which may compromise their performance on noisy social
media data. To tackle this issue, we reflect feature change via modeling latent
topic evolution and propose a novel model, VIBE: Variational Information
Bottleneck for Evolutions. Concretely, we first employ two Information
Bottleneck (IB) regularizers to distinguish past and future topics. Then, the
distinguished topics work as adaptive features via multi-task training with
timestamp and class label prediction. In adaptive learning, VIBE utilizes
retrieved unlabeled data from online streams created posterior to training data
time. Substantial Twitter experiments on three classification tasks show that
our model, with only 3% of data, significantly outperforms previous
state-of-the-art continued-pretraining methods.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10195" title="Abstract">arXiv:2310.10195</a> [<a href="/pdf/2310.10195" title="Download PDF">pdf</a>, <a href="/format/2310.10195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaLomo: Low-memory Optimization with Adaptive Learning Rate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+K">Kai Lv</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+H">Haijun Lv</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models have achieved remarkable success, but their extensive
parameter size necessitates substantial memory for training, thereby setting a
high threshold. While the recently proposed low-memory optimization (LOMO)
reduces memory footprint, its optimization technique, akin to stochastic
gradient descent, is sensitive to hyper-parameters and exhibits suboptimal
convergence, failing to match the performance of the prevailing optimizer for
large language models, AdamW. Through empirical analysis of the Adam optimizer,
we found that, compared to momentum, the adaptive learning rate is more
critical for bridging the gap. Building on this insight, we introduce the
low-memory optimization with adaptive learning rate (AdaLomo), which offers an
adaptive learning rate for each parameter. To maintain memory efficiency, we
employ non-negative matrix factorization for the second-order moment estimation
in the optimizer state. Additionally, we suggest the use of a grouped update
normalization to stabilize convergence. Our experiments with instruction-tuning
and further pre-training demonstrate that AdaLomo achieves results on par with
AdamW, while significantly reducing memory requirements, thereby lowering the
hardware barrier to training large language models.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10196" title="Abstract">arXiv:2310.10196</a> [<a href="/pdf/2310.10196" title="Download PDF">pdf</a>, <a href="/format/2310.10196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Models for Time Series and Spatio-Temporal Data: A Survey and  Outlook
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Ming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Siqiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">James Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoli Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+V+S">Vincent S. Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work; 25 pages, 3 figures, 3 tables; Github page: <a href="https://github.com/qingsongedu/Awesome-TimeSeries-SpatioTemporal-LM-LLM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Temporal data, notably time series and spatio-temporal data, are prevalent in
real-world applications. They capture dynamic system measurements and are
produced in vast quantities by both physical and virtual sensors. Analyzing
these data types is vital to harnessing the rich information they encompass and
thus benefits a wide range of downstream tasks. Recent advances in large
language and other foundational models have spurred increased use of these
models in time series and spatio-temporal data mining. Such methodologies not
only enable enhanced pattern recognition and reasoning across diverse domains
but also lay the groundwork for artificial general intelligence capable of
comprehending and processing common temporal data. In this survey, we offer a
comprehensive and up-to-date review of large models tailored (or adapted) for
time series and spatio-temporal data, spanning four key facets: data types,
model categories, model scopes, and application areas/tasks. Our objective is
to equip practitioners with the knowledge to develop applications and further
research in this underexplored domain. We primarily categorize the existing
literature into two major clusters: large models for time series analysis
(LM4TS) and spatio-temporal data mining (LM4STD). On this basis, we further
classify research based on model scopes (i.e., general vs. domain-specific) and
application areas/tasks. We also provide a comprehensive collection of
pertinent resources, including datasets, model assets, and useful tools,
categorized by mainstream applications. This survey coalesces the latest
strides in large model-centric research on time series and spatio-temporal
data, underscoring the solid foundations, current advances, practical
applications, abundant resources, and future research opportunities.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10198" title="Abstract">arXiv:2310.10198</a> [<a href="/pdf/2310.10198" title="Download PDF">pdf</a>, <a href="/format/2310.10198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Heyuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhenhua Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ao%2C+T">Tenglong Ao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baoquan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Libin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">In this work, we present MoConVQ, a novel unified framework for physics-based
motion control leveraging scalable discrete representations. Building upon
vector quantized variational autoencoders (VQ-VAE) and model-based
reinforcement learning, our approach effectively learns motion embeddings from
a large, unstructured dataset spanning tens of hours of motion examples. The
resultant motion representation not only captures diverse motion skills but
also offers a robust and intuitive interface for various applications. We
demonstrate the versatility of MoConVQ through several applications: universal
tracking control from various motion sources, interactive character control
with latent motion representations using supervised learning, physics-based
motion generation from natural language descriptions using the GPT framework,
and, most interestingly, seamless integration with large language models (LLMs)
with in-context learning to tackle complex and abstract tasks.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10203" title="Abstract">arXiv:2310.10203</a> [<a href="/pdf/2310.10203" title="Download PDF">pdf</a>, <a href="/format/2310.10203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Predictive Models to Understand Risk Factors for Maternal  and Fetal Outcomes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bosschieter%2C+T+M">Tomas M. Bosschieter</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zifei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+H">Hui Lan</a>, 
<a href="/search/cs?searchtype=author&query=Lengerich%2C+B+J">Benjamin J. Lengerich</a>, 
<a href="/search/cs?searchtype=author&query=Nori%2C+H">Harsha Nori</a>, 
<a href="/search/cs?searchtype=author&query=Painter%2C+I">Ian Painter</a>, 
<a href="/search/cs?searchtype=author&query=Souter%2C+V">Vivienne Souter</a>, 
<a href="/search/cs?searchtype=author&query=Caruana%2C+R">Rich Caruana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages (including appendix and references), 12 figures, 2 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Bosschieter, T.M., Xu, Z., Lan, H. et al. Interpretable Predictive
  Models to Understand Risk Factors for Maternal and Fetal Outcomes. J Healthc
  Inform Res (2023). https://doi.org/10.1007/s41666-023-00151-4
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Although most pregnancies result in a good outcome, complications are not
uncommon and can be associated with serious implications for mothers and
babies. Predictive modeling has the potential to improve outcomes through
better understanding of risk factors, heightened surveillance for high risk
patients, and more timely and appropriate interventions, thereby helping
obstetricians deliver better care. We identify and study the most important
risk factors for four types of pregnancy complications: (i) severe maternal
morbidity, (ii) shoulder dystocia, (iii) preterm preeclampsia, and (iv)
antepartum stillbirth. We use an Explainable Boosting Machine (EBM), a
high-accuracy glass-box learning method, for prediction and identification of
important risk factors. We undertake external validation and perform an
extensive robustness analysis of the EBM models. EBMs match the accuracy of
other black-box ML methods such as deep neural networks and random forests, and
outperform logistic regression, while being more interpretable. EBMs prove to
be robust. The interpretability of the EBM models reveals surprising insights
into the features contributing to risk (e.g. maternal height is the second most
important feature for shoulder dystocia) and may have potential for clinical
application in the prediction and prevention of serious complications in
pregnancy.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10207" title="Abstract">arXiv:2310.10207</a> [<a href="/pdf/2310.10207" title="Download PDF">pdf</a>, <a href="/format/2310.10207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in  the Real World
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Rujie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://joyjayng.github.io/Bongard-OpenWorld.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world
few-shot reasoning for machine vision. It originates from the classical Bongard
Problems (BPs): Given two sets of images (positive and negative), the model
needs to identify the set that query images belong to by inducing the visual
concepts, which is exclusively depicted by images from the positive set. Our
benchmark inherits the few-shot concept induction of the original BPs while
adding the two novel layers of challenge: 1) open-world free-form concepts, as
the visual concepts in Bongard-OpenWorld are unique compositions of terms from
an open vocabulary, ranging from object categories to abstract visual
attributes and commonsense factual knowledge; 2) real-world images, as opposed
to the synthetic diagrams used by many counterparts. In our exploration,
Bongard-OpenWorld already imposes a significant challenge to current few-shot
reasoning algorithms. We further investigate to which extent the recently
introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can
solve our task, by directly probing VLMs, and combining VLMs and LLMs in an
interactive reasoning scheme. We even designed a neuro-symbolic reasoning
approach that reconciles LLMs &amp; VLMs with logical reasoning to emulate the
human problem-solving process for Bongard Problems. However, none of these
approaches manage to close the human-machine gap, as the best learner achieves
64% accuracy while human participants easily reach 91%. We hope
Bongard-OpenWorld can help us better understand the limitations of current
visual intelligence and facilitate future research on visual agents with
stronger few-shot visual reasoning capabilities.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10211" title="Abstract">arXiv:2310.10211</a> [<a href="/pdf/2310.10211" title="Download PDF">pdf</a>, <a href="/format/2310.10211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GEVO-ML: Optimizing Machine Learning Code with Evolutionary Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liou%2C+J">Jhe-Yu Liou</a>, 
<a href="/search/cs?searchtype=author&query=Forrest%2C+S">Stephanie Forrest</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Carole-Jean Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Parallel accelerators, such as GPUs, are key enablers for large-scale Machine
Learning (ML) applications. However, ML model developers often lack detailed
knowledge of the underlying system architectures, while system programmers
usually do not have a high-level understanding of the ML model that runs on the
specific system. To mitigate this gap between two relevant aspects of domain
knowledge, this paper proposes GEVO-ML, a tool for automatically discovering
optimization opportunities and tuning the performance of ML kernels, where the
model and training/prediction processes are uniformly represented in a single
intermediate language, the Multiple-Layer Intermediate Representation (MLIR).
GEVO-ML uses multi-objective evolutionary search to find edits (mutations) to
MLIR code that ultimately runs on GPUs, improving performance on desired
criteria while retaining required functionality.
<br />We demonstrate GEVO-ML on two different ML workloads for both model training
and prediction. GEVO-ML finds significant Pareto improvements for these models,
achieving 90.43% performance improvement when model accuracy is relaxed by 2%,
from 91.2% to 89.3%. For the training workloads, GEVO-ML finds a 4.88%
improvement in model accuracy, from 91% to 96%, without sacrificing training or
testing speed. Our analysis of key GEVO-ML mutations reveals diverse code
modifications, while might be foreign to human developers, achieving similar
effects with how human developers improve model design, for example, by
changing learning rates or pruning non-essential layer parameters.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10213" title="Abstract">arXiv:2310.10213</a> [<a href="/pdf/2310.10213" title="Download PDF">pdf</a>, <a href="/format/2310.10213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Multi-Connectivity in Beyond 5G Non-Terrestrial Networks:  Challenges and Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majamaa%2C+M">Mikko Majamaa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Non-terrestrial networks (NTNs) will complement the 5G and beyond terrestrial
networks (TNs) thanks to recent deployment and standardization activities.
Maximizing the efficiency of NTN communications is crucial to unlock its full
potential and reap its numerous benefits. One method to make the communications
more efficient is by the usage of multi-connectivity (MC) where a user can be
connected to multiple base stations simultaneously. It has earlier been
standardized and widely used for TNs. However, for MC to be utilized in the NTN
environment, several challenges need to be overcome. In this article,
challenges related to MC in NTNs are discussed, and solutions for the
identified challenges are proposed.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10214" title="Abstract">arXiv:2310.10214</a> [<a href="/pdf/2310.10214" title="Download PDF">pdf</a>, <a href="/format/2310.10214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> K-SMPC: Koopman Operator-Based Stochastic Model Predictive Control for  Enhanced Lateral Control of Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kim%2C+J+S">Jin Sung Kim</a>, 
<a href="/search/eess?searchtype=author&query=Quan%2C+Y+S">Ying Shuai Quan</a>, 
<a href="/search/eess?searchtype=author&query=Chung%2C+C+C">Chung Choo Chung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper proposes Koopman operator-based Stochastic Model Predictive
Control (K-SMPC) for enhanced lateral control of autonomous vehicles. The
Koopman operator is a linear map representing the nonlinear dynamics in an
infinite-dimensional space. Thus, we use the Koopman operator to represent the
nonlinear dynamics of a vehicle in dynamic lane-keeping situations. The
Extended Dynamic Mode Decomposition (EDMD) method is adopted to approximate the
Koopman operator in a finite-dimensional space for practical implementation. We
consider the modeling error of the approximated Koopman operator in the EDMD
method. Then, we design K-SMPC to tackle the Koopman modeling error, where the
error is handled as a probabilistic signal. The recursive feasibility of the
proposed method is investigated with an explicit first-step state constraint by
computing the robust control invariant set. A high-fidelity vehicle simulator,
i.e., CarSim, is used to validate the proposed method with a comparative study.
From the results, it is confirmed that the proposed method outperforms other
methods in tracking performance. Furthermore, it is observed that the proposed
method satisfies the given constraints and is recursively feasible.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10215" title="Abstract">arXiv:2310.10215</a> [<a href="/pdf/2310.10215" title="Download PDF">pdf</a>, <a href="/format/2310.10215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adaptive and Verifiably Proportional Method for Participatory  Budgeting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kraiczy%2C+S">Sonja Kraiczy</a>, 
<a href="/search/cs?searchtype=author&query=Elkind%2C+E">Edith Elkind</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Participatory Budgeting (PB) is a form of participatory democracy in which
citizens select a set of projects to be implemented, subject to a budget
constraint. The Method of Equal Shares (MES), introduced in [18], is a simple
iterative method for this task, which runs in polynomial time and satisfies a
demanding proportionality axiom (Extended Justified Representation) in the
setting of approval utilities. However, a downside of MES is that it is
non-exhaustive: given an MES outcome, it may be possible to expand it by adding
new projects without violating the budget constraint. To complete the outcome,
the approach currently used in practice is as follows: given an instance with
budget $b$, one searches for a budget $b'\ge b$ such that when MES is executed
with budget $b'$, it produces a maximal feasible solution for $b$. The search
is greedy, i.e., one has to execute MES from scratch for each value of $b'$. To
avoid redundant computation, we introduce a variant of MES, which we call
Adaptive Method of Equal Shares (AMES). Our method is budget-adaptive, in the
sense that, given an outcome $W$ for a budget $b$ and a new budget $b'&gt;b$, it
can compute the outcome $W'$ for budget $b'$ by leveraging similarities between
$W$ and $W'$. This eliminates the need to recompute solutions from scratch when
increasing virtual budgets. Furthermore, AMES satisfies EJR in a certifiable
way: given the output of our method, one can check in time $O(n\log n+mn)$ that
it provides EJR (here, $n$ is the number of voters and $m$ is the number of
projects). We evaluate the potential of AMES on real-world PB data, showing
that small increases in budget typically require only minor modifications of
the outcome.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10219" title="Abstract">arXiv:2310.10219</a> [<a href="/pdf/2310.10219" title="Download PDF">pdf</a>, <a href="/format/2310.10219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Global Land Cover Product as Prompt for Cropland Mapping via  Visual Foundation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chao Tao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+A">Aoran Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+R">Rong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuze Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Data-driven deep learning methods have shown great potential in cropland
mapping. However, due to multiple factors such as attributes of cropland
(topography, climate, crop type) and imaging conditions (viewing angle,
illumination, scale), croplands under different scenes demonstrate a great
domain gap. This makes it difficult for models trained in the specific scenes
to directly generalize to other scenes. A common way to handle this problem is
through the "Pretrain+Fine-tuning" paradigm. Unfortunately, considering the
variety of features of cropland that are affected by multiple factors, it is
hardly to handle the complex domain gap between pre-trained data and target
data using only sparse fine-tuned samples as general constraints. Moreover, as
the number of model parameters grows, fine-tuning is no longer an easy and
low-cost task. With the emergence of prompt learning via visual foundation
models, the "Pretrain+Prompting" paradigm redesigns the optimization target by
introducing individual prompts for each single sample. This simplifies the
domain adaption from generic to specific scenes during model reasoning
processes. Therefore, we introduce the "Pretrain+Prompting" paradigm to
interpreting cropland scenes and design the auto-prompting (APT) method based
on freely available global land cover product. It can achieve a fine-grained
adaptation process from generic scenes to specialized cropland scenes without
introducing additional label costs. To our best knowledge, this work pioneers
the exploration of the domain adaption problems for cropland mapping under
prompt learning perspectives. Our experiments using two sub-meter cropland
datasets from southern and northern China demonstrated that the proposed method
via visual foundation models outperforms traditional supervised learning and
fine-tuning approaches in the field of remote sensing.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10221" title="Abstract">arXiv:2310.10221</a> [<a href="/pdf/2310.10221" title="Download PDF">pdf</a>, <a href="/format/2310.10221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zijun Long</a>, 
<a href="/search/cs?searchtype=author&query=Killick%2C+G">George Killick</a>, 
<a href="/search/cs?searchtype=author&query=McCreadie%2C+R">Richard McCreadie</a>, 
<a href="/search/cs?searchtype=author&query=Camarasa%2C+G+A">Gerardo Aragon Camarasa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Robotic vision applications often necessitate a wide range of visual
perception tasks, such as object detection, segmentation, and identification.
While there have been substantial advances in these individual tasks,
integrating specialized models into a unified vision pipeline presents
significant engineering challenges and costs. Recently, Multimodal Large
Language Models (MLLMs) have emerged as novel backbones for various downstream
tasks. We argue that leveraging the pre-training capabilities of MLLMs enables
the creation of a simplified framework, thus mitigating the need for
task-specific encoders. Specifically, the large-scale pretrained knowledge in
MLLMs allows for easier fine-tuning to downstream robotic vision tasks and
yields superior performance. We introduce the RoboLLM framework, equipped with
a BEiT-3 backbone, to address all visual perception tasks in the ARMBench
challenge-a large-scale robotic manipulation dataset about real-world warehouse
scenarios. RoboLLM not only outperforms existing baselines but also
substantially reduces the engineering burden associated with model selection
and tuning. The source code is publicly available at
https://github.com/longkukuhi/armbench.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10226" title="Abstract">arXiv:2310.10226</a> [<a href="/pdf/2310.10226" title="Download PDF">pdf</a>, <a href="/format/2310.10226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Repetition In Repetition Out: Towards Understanding Neural Text  Degeneration from the Data Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huayang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zihao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lemao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+T">Taro Watanabe</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yixuan Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">There are a number of diverging hypotheses about the neural text degeneration
problem, i.e., generating repetitive and dull loops, which makes this problem
both interesting and confusing. In this work, we aim to advance our
understanding by presenting a straightforward and fundamental explanation from
the data perspective. Our preliminary investigation reveals a strong
correlation between the degeneration issue and the presence of repetitions in
training data. Subsequent experiments also demonstrate that by selectively
dropping out the attention to repetitive words in training data, degeneration
can be significantly minimized. Furthermore, our empirical analysis illustrates
that prior works addressing the degeneration issue from various standpoints,
such as the high-inflow words, the likelihood objective, and the
self-reinforcement phenomenon, can be interpreted by one simple explanation.
That is, penalizing the repetitions in training data is a common and
fundamental factor for their effectiveness. Moreover, our experiments reveal
that penalizing the repetitions in training data remains critical even when
considering larger model sizes and instruction tuning.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10237" title="Abstract">arXiv:2310.10237</a> [<a href="/pdf/2310.10237" title="Download PDF">pdf</a>, <a href="/format/2310.10237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zhihao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jieming Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph-level representation learning is important in a wide range of
applications. However, existing graph-level models are generally built on
i.i.d. assumption for both training and testing graphs, which is not realistic
in an open world, where models can encounter out-of-distribution (OOD) testing
graphs that are from different distributions unknown during training. A
trustworthy model should not only produce accurate predictions for
in-distribution (ID) data, but also detect OOD graphs to avoid unreliable
prediction. In this paper, we present SGOOD, a novel graph-level OOD detection
framework. We find that substructure differences commonly exist between ID and
OOD graphs. Hence, SGOOD explicitly utilizes substructures to learn powerful
representations to achieve superior performance. Specifically, we build a super
graph of substructures for every graph, and design a two-level graph encoding
pipeline that works on both original graphs and super graphs to obtain
substructure-enhanced graph representations. To further distinguish ID and OOD
graphs, we develop three graph augmentation techniques that preserve
substructures and increase expressiveness. Extensive experiments against 10
competitors on numerous graph datasets demonstrate the superiority of SGOOD,
often surpassing existing methods by a significant margin. The code is
available at https://anonymous.4open.science/r/SGOOD-0958.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10245" title="Abstract">arXiv:2310.10245</a> [<a href="/pdf/2310.10245" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mask wearing object detection algorithm based on improved YOLOv5
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+P">Peng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junhu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haitao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Wearing a mask is one of the important measures to prevent infectious
diseases. However, it is difficult to detect people's mask-wearing situation in
public places with high traffic flow. To address the above problem, this paper
proposes a mask-wearing face detection model based on YOLOv5l. Firstly,
Multi-Head Attentional Self-Convolution not only improves the convergence speed
of the model but also enhances the accuracy of the model detection. Secondly,
the introduction of Swin Transformer Block is able to extract more useful
feature information, enhance the detection ability of small targets, and
improve the overall accuracy of the model. Our designed I-CBAM module can
improve target detection accuracy. In addition, using enhanced feature fusion
enables the model to better adapt to object detection tasks of different
scales. In the experimentation on the MASK dataset, the results show that the
model proposed in this paper achieved a 1.1% improvement in mAP(0.5) and a 1.3%
improvement in mAP(0.5:0.95) compared to the YOLOv5l model. Our proposed method
significantly enhances the detection capability of mask-wearing.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10250" title="Abstract">arXiv:2310.10250</a> [<a href="/pdf/2310.10250" title="Download PDF">pdf</a>, <a href="/format/2310.10250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Topological Maps in Deep Reinforcement Learning for  Multi-Object Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hakenes%2C+S">Simon Hakenes</a>, 
<a href="/search/cs?searchtype=author&query=Glasmachers%2C+T">Tobias Glasmachers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract, Northern Lights Deep Learning Conference 2024, 3 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This work addresses the challenge of navigating expansive spaces with sparse
rewards through Reinforcement Learning (RL). Using topological maps, we elevate
elementary actions to object-oriented macro actions, enabling a simple Deep
Q-Network (DQN) agent to solve otherwise practically impossible environments.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10259" title="Abstract">arXiv:2310.10259</a> [<a href="/pdf/2310.10259" title="Download PDF">pdf</a>, <a href="/format/2310.10259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging heterogeneous spillover effects in maximizing contextual  bandit rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faruk%2C+A+S">Ahmed Sayeed Faruk</a>, 
<a href="/search/cs?searchtype=author&query=Zheleva%2C+E">Elena Zheleva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Recommender systems relying on contextual multi-armed bandits continuously
improve relevant item recommendations by taking into account the contextual
information. The objective of these bandit algorithms is to learn the best arm
(i.e., best item to recommend) for each user and thus maximize the cumulative
rewards from user engagement with the recommendations. However, current
approaches ignore potential spillover between interacting users, where the
action of one user can impact the actions and rewards of other users. Moreover,
spillover may vary for different people based on their preferences and the
closeness of ties to other users. This leads to heterogeneity in the spillover
effects, i.e., the extent to which the action of one user can impact the action
of another. Here, we propose a framework that allows contextual multi-armed
bandits to account for such heterogeneous spillovers when choosing the best arm
for each user. By experimenting on several real-world datasets using prominent
linear and non-linear contextual bandit algorithms, we observe that our
proposed method leads to significantly higher rewards than existing solutions
that ignore spillover.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10260" title="Abstract">arXiv:2310.10260</a> [<a href="/pdf/2310.10260" title="Download PDF">pdf</a>, <a href="/format/2310.10260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction of Arabic Legal Rulings using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ammar%2C+A">Adel Ammar</a>, 
<a href="/search/cs?searchtype=author&query=Koubaa%2C+A">Anis Koubaa</a>, 
<a href="/search/cs?searchtype=author&query=Benjdira%2C+B">Bilel Benjdira</a>, 
<a href="/search/cs?searchtype=author&query=Najar%2C+O">Omar Najar</a>, 
<a href="/search/cs?searchtype=author&query=Sibaee%2C+S">Serry Sibaee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the intricate field of legal studies, the analysis of court decisions is a
cornerstone for the effective functioning of the judicial system. The ability
to predict court outcomes helps judges during the decision-making process and
equips lawyers with invaluable insights, enhancing their strategic approaches
to cases. Despite its significance, the domain of Arabic court analysis remains
under-explored. This paper pioneers a comprehensive predictive analysis of
Arabic court decisions on a dataset of 10,813 commercial court real cases,
leveraging the advanced capabilities of the current state-of-the-art large
language models. Through a systematic exploration, we evaluate three prevalent
foundational models (LLaMA-7b, JAIS-13b, and GPT3.5-turbo) and three training
paradigms: zero-shot, one-shot, and tailored fine-tuning. Besides, we assess
the benefit of summarizing and/or translating the original Arabic input texts.
This leads to a spectrum of 14 model variants, for which we offer a granular
performance assessment with a series of different metrics (human assessment,
GPT evaluation, ROUGE, and BLEU scores). We show that all variants of LLaMA
models yield limited performance, whereas GPT-3.5-based models outperform all
other models by a wide margin, surpassing the average score of the dedicated
Arabic-centric JAIS model by 50%. Furthermore, we show that all scores except
human evaluation are inconsistent and unreliable for assessing the performance
of large language models on court decision predictions. This study paves the
way for future research, bridging the gap between computational linguistics and
Arabic legal analytics.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10262" title="Abstract">arXiv:2310.10262</a> [<a href="/pdf/2310.10262" title="Download PDF">pdf</a>, <a href="/format/2310.10262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Interpretability using Human Similarity Judgements to Prune  Word Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manrique%2C+N+F">Natalia Flechas Manrique</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+W">Wanqian Bao</a>, 
<a href="/search/cs?searchtype=author&query=Herbelot%2C+A">Aurelie Herbelot</a>, 
<a href="/search/cs?searchtype=author&query=Hasson%2C+U">Uri Hasson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the BlackboxNLP workshop at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Interpretability methods in NLP aim to provide insights into the semantics
underlying specific system architectures. Focusing on word embeddings, we
present a supervised-learning method that, for a given domain (e.g., sports,
professions), identifies a subset of model features that strongly improve
prediction of human similarity judgments. We show this method keeps only 20-40%
of the original embeddings, for 8 independent semantic domains, and that it
retains different feature sets across domains. We then present two approaches
for interpreting the semantics of the retained features. The first obtains the
scores of the domain words (co-hyponyms) on the first principal component of
the retained embeddings, and extracts terms whose co-occurrence with the
co-hyponyms tracks these scores' profile. This analysis reveals that humans
differentiate e.g. sports based on how gender-inclusive and international they
are. The second approach uses the retained sets as variables in a probing task
that predicts values along 65 semantically annotated dimensions for a dataset
of 535 words. The features retained for professions are best at predicting
cognitive, emotional and social dimensions, whereas features retained for
fruits or vegetables best predict the gustation (taste) dimension. We discuss
implications for alignment between AI systems and human knowledge.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10264" title="Abstract">arXiv:2310.10264</a> [<a href="/pdf/2310.10264" title="Download PDF">pdf</a>, <a href="/format/2310.10264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Open-World Co-Salient Object Detection with Generative  Uncertainty-aware Group Selective Exchange-Masking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shenglong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Huihui Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The traditional definition of co-salient object detection (CoSOD) task is to
segment the common salient objects in a group of relevant images. This
definition is based on an assumption of group consensus consistency that is not
always reasonable in the open-world setting, which results in robustness issue
in the model when dealing with irrelevant images in the inputting image group
under the open-word scenarios. To tackle this problem, we introduce a group
selective exchange-masking (GSEM) approach for enhancing the robustness of the
CoSOD model. GSEM takes two groups of images as input, each containing
different types of salient objects. Based on the mixed metric we designed, GSEM
selects a subset of images from each group using a novel learning-based
strategy, then the selected images are exchanged. To simultaneously consider
the uncertainty introduced by irrelevant images and the consensus features of
the remaining relevant images in the group, we designed a latent variable
generator branch and CoSOD transformer branch. The former is composed of a
vector quantised-variational autoencoder to generate stochastic global
variables that model uncertainty. The latter is designed to capture
correlation-based local features that include group consensus. Finally, the
outputs of the two branches are merged and passed to a transformer-based
decoder to generate robust predictions. Taking into account that there are
currently no benchmark datasets specifically designed for open-world scenarios,
we constructed three open-world benchmark datasets, namely OWCoSal, OWCoSOD,
and OWCoCA, based on existing datasets. By breaking the group-consistency
assumption, these datasets provide effective simulations of real-world
scenarios and can better evaluate the robustness and practicality of models.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10266" title="Abstract">arXiv:2310.10266</a> [<a href="/pdf/2310.10266" title="Download PDF">pdf</a>, <a href="/format/2310.10266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Calibration for In-context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhongtao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanzhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As one of the most exciting features of large language models (LLMs),
in-context learning is a mixed blessing. While it allows users to
fast-prototype a task solver with only a few training examples, the performance
is generally sensitive to various configurations of the prompt such as the
choice or order of the training examples. In this paper, we for the first time
theoretically and empirically identify that such a paradox is mainly due to the
label shift of the in-context model to the data distribution, in which LLMs
shift the label marginal $p(y)$ while having a good label conditional $p(x|y)$.
With this understanding, we can simply calibrate the in-context predictive
distribution by adjusting the label marginal, which is estimated via
Monte-Carlo sampling over the in-context model, i.e., generation of LLMs. We
call our approach as generative calibration. We conduct exhaustive experiments
with 12 text classification tasks and 12 LLMs scaling from 774M to 33B,
generally find that the proposed method greatly and consistently outperforms
the ICL as well as state-of-the-art calibration methods, by up to 27% absolute
in macro-F1. Meanwhile, the proposed method is also stable under different
prompt configurations.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10268" title="Abstract">arXiv:2310.10268</a> [<a href="/pdf/2310.10268" title="Download PDF">pdf</a>, <a href="/format/2310.10268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Financial Service Promotion With Hybrid Recommender Systems  at PicPay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mendon%C3%A7a%2C+G">Gabriel Mendon&#xe7;a</a> (Federal University of Rio de Janeiro), 
<a href="/search/cs?searchtype=author&query=Santos%2C+M">Matheus Santos</a> (PicPay), 
<a href="/search/cs?searchtype=author&query=Gon%C3%A7alves%2C+A">Andr&#xe9; Gon&#xe7;alves</a> (PicPay), 
<a href="/search/cs?searchtype=author&query=Almeida%2C+Y">Yan Almeida</a> (PicPay)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 4 figures, submitted to ACM KDD '23 2nd Workshop on End-End Customer Journey Optimization
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The fintech PicPay offers a wide range of financial services to its 30
million monthly active users, with more than 50 thousand items recommended in
the PicPay mobile app. In this scenario, promoting specific items that are
strategic to the company can be very challenging. In this work, we present a
Switching Hybrid Recommender System that combines two algorithms to effectively
promote items without negatively impacting the user's experience. The results
of our A/B tests show an uplift of up to 3.2\% when compared to a default
recommendation strategy.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10272" title="Abstract">arXiv:2310.10272</a> [<a href="/pdf/2310.10272" title="Download PDF">pdf</a>, <a href="/format/2310.10272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A thickness-aware Allen-Cahn equation for the mean curvature flow of  thin structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bretin%2C+E">Elie Bretin</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+C">Chih-Kang Huang</a>, 
<a href="/search/math?searchtype=author&query=Masnou%2C+S">Simon Masnou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work addresses the approximation of the mean curvature flow of thin
structures for which classical phase field methods are not suitable. By thin
structures we mean either structures of higher codimension, typically
filaments, or surfaces (including non orientables surfaces) that are not
boundaries of a set. We propose a novel approach which consists in plugging
into the classical Allen--Cahn equation a penalization term localized around
the skeleton of the evolving set. This ensures that a minimal thickness is
preserved during the evolution process. The numerical efficacy of our approach
is illustrated with accurate approximations of the evolution by mean curvature
flow of filament structures. Furthermore, we show the seamless adaptability of
our approach to compute numerical approximations of solutions to the Steiner
and Plateau problems in three dimensions.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10274" title="Abstract">arXiv:2310.10274</a> [<a href="/pdf/2310.10274" title="Download PDF">pdf</a>, <a href="/format/2310.10274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Compromise in Solution Quality: Speeding Up Belief-dependent  Continuous POMDPs via Adaptive Multilevel Simplification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhitnikov%2C+A">Andrey Zhitnikov</a>, 
<a href="/search/cs?searchtype=author&query=Sztyglic%2C+O">Ori Sztyglic</a>, 
<a href="/search/cs?searchtype=author&query=Indelman%2C+V">Vadim Indelman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Continuous POMDPs with general belief-dependent rewards are notoriously
difficult to solve online. In this paper, we present a complete provable theory
of adaptive multilevel simplification for the setting of a given externally
constructed belief tree and MCTS that constructs the belief tree on the fly
using an exploration technique. Our theory allows to accelerate POMDP planning
with belief-dependent rewards without any sacrifice in the quality of the
obtained solution. We rigorously prove each theoretical claim in the proposed
unified theory. Using the general theoretical results, we present three
algorithms to accelerate continuous POMDP online planning with belief-dependent
rewards. Our two algorithms, SITH-BSP and LAZY-SITH-BSP, can be utilized on top
of any method that constructs a belief tree externally. The third algorithm,
SITH-PFT, is an anytime MCTS method that permits to plug-in any exploration
technique. All our methods are guaranteed to return exactly the same optimal
action as their unsimplified equivalents. We replace the costly computation of
information-theoretic rewards with novel adaptive upper and lower bounds which
we derive in this paper, and are of independent interest. We show that they are
easy to calculate and can be tightened by the demand of our algorithms. Our
approach is general; namely, any bounds that monotonically converge to the
reward can be easily plugged-in to achieve significant speedup without any loss
in performance. Our theory and algorithms support the challenging setting of
continuous states, actions, and observations. The beliefs can be parametric or
general and represented by weighted particles. We demonstrate in simulation a
significant speedup in planning compared to baseline approaches with guaranteed
identical performance.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10275" title="Abstract">arXiv:2310.10275</a> [<a href="/pdf/2310.10275" title="Download PDF">pdf</a>, <a href="/format/2310.10275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A ML-LLM pairing for better code comment classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akl%2C+H+A">Hanna Abi Akl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures, 2 tables, accepted for the Information Retrieval in Software Engineering track at the Forum for Information Retrieval Evaluation 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information Retrieval in Software Engineering track at Forum for
  Information Retrieval Evaluation 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The "Information Retrieval in Software Engineering (IRSE)" at FIRE 2023
shared task introduces code comment classification, a challenging task that
pairs a code snippet with a comment that should be evaluated as either useful
or not useful to the understanding of the relevant code. We answer the code
comment classification shared task challenge by providing a two-fold
evaluation: from an algorithmic perspective, we compare the performance of
classical machine learning systems and complement our evaluations from a
data-driven perspective by generating additional data with the help of large
language model (LLM) prompting to measure the potential increase in
performance. Our best model, which took second place in the shared task, is a
Neural Network with a Macro-F1 score of 88.401% on the provided seed data and a
1.5% overall increase in performance on the data generated by the LLM.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10280" title="Abstract">arXiv:2310.10280</a> [<a href="/pdf/2310.10280" title="Download PDF">pdf</a>, <a href="/format/2310.10280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mimicking the Maestro: Exploring the Efficacy of a Virtual AI Teacher in  Fine Motor Skill Acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mulian%2C+H">Hadar Mulian</a>, 
<a href="/search/cs?searchtype=author&query=Shlomov%2C+S">Segev Shlomov</a>, 
<a href="/search/cs?searchtype=author&query=Limonad%2C+L">Lior Limonad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Motor skills, especially fine motor skills like handwriting, play an
essential role in academic pursuits and everyday life. Traditional methods to
teach these skills, although effective, can be time-consuming and inconsistent.
With the rise of advanced technologies like robotics and artificial
intelligence, there is increasing interest in automating such teaching
processes using these technologies, via human-robot and human-computer
interactions. In this study, we examine the potential of a virtual AI teacher
in emulating the techniques of human educators for motor skill acquisition. We
introduce an AI teacher model that captures the distinct characteristics of
human instructors. Using a Reinforcement Learning environment tailored to mimic
teacher-learner interactions, we tested our AI model against four guiding
hypotheses, emphasizing improved learner performance, enhanced rate of skill
acquisition, and reduced variability in learning outcomes. Our findings,
validated on synthetic learners, revealed significant improvements across all
tested hypotheses. Notably, our model showcased robustness across different
learners and settings and demonstrated adaptability to handwriting. This
research underscores the potential of integrating Reinforcement Learning and
Imitation Learning models with robotics in revolutionizing the teaching of
critical motor skills.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10285" title="Abstract">arXiv:2310.10285</a> [<a href="/pdf/2310.10285" title="Download PDF">pdf</a>, <a href="/format/2310.10285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario  Multi-Domain Dialogue Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Weixiao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gengyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xianfu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xinnian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Junnan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+F">Feifei Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhoujun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dialogue summarization involves a wide range of scenarios and domains.
However, existing methods generally only apply to specific scenarios or
domains. In this study, we propose a new pre-trained model specifically
designed for multi-scenario multi-domain dialogue summarization. It adopts a
multi-stage pre-training strategy to reduce the gap between the pre-training
objective and fine-tuning objective. Specifically, we first conduct
domain-aware pre-training using large-scale multi-scenario multi-domain
dialogue data to enhance the adaptability of our pre-trained model. Then, we
conduct task-oriented pre-training using large-scale multi-scenario
multi-domain "dialogue-summary" parallel data annotated by ChatGPT to enhance
the dialogue summarization ability of our pre-trained model. Experimental
results on three dialogue summarization datasets from different scenarios and
domains indicate that our pre-trained model significantly outperforms previous
state-of-the-art models in full fine-tuning, zero-shot, and few-shot settings.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10289" title="Abstract">arXiv:2310.10289</a> [<a href="/pdf/2310.10289" title="Download PDF">pdf</a>, <a href="/format/2310.10289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moving Object Localization based on the Fusion of Ultra-WideBand and  LiDAR with a Mobile Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shalihan%2C+M">Muhammad Shalihan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiqiang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Pongsirijinda%2C+K">Khattiya Pongsirijinda</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+B+P+L">Billy Pik Lik Lau</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+U">U-Xuan Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by The 2023 IEEE International Conference on Robotics and Biomimetics (IEEE ROBIO 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Localization of objects is vital for robot-object interaction. Light
Detection and Ranging (LiDAR) application in robotics is an emerging and widely
used object localization technique due to its accurate distance measurement,
long-range, wide field of view, and robustness in different conditions.
However, LiDAR is unable to identify the objects when they are obstructed by
obstacles, resulting in inaccuracy and noise in localization. To address this
issue, we present an approach incorporating LiDAR and Ultra-Wideband (UWB)
ranging for object localization. The UWB is popular in sensor fusion
localization algorithms due to its low weight and low power consumption. In
addition, the UWB is able to return ranging measurements even when the object
is not within line-of-sight. Our approach provides an efficient solution to
combine an anonymous optical sensor (LiDAR) with an identity-based radio sensor
(UWB) to improve the localization accuracy of the object. Our approach consists
of three modules. The first module is an object-identification algorithm that
compares successive scans from the LiDAR to detect a moving object in the
environment and returns the position with the closest range to UWB ranging. The
second module estimates the moving object's moving direction using the previous
and current estimated position from our object-identification module. It
removes the suspicious estimations through an outlier rejection criterion.
Lastly, we fuse the LiDAR, UWB ranging, and odometry measurements in pose graph
optimization (PGO) to recover the entire trajectory of the robot and object.
Extensive experiments were performed to evaluate the performance of the
proposed approach.
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10290" title="Abstract">arXiv:2310.10290</a> [<a href="/pdf/2310.10290" title="Download PDF">pdf</a>, <a href="/format/2310.10290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Mapping and Navigation using Fiducial Markers and Pan-Tilt  Camera for Assisting Indoor Mobility of Blind and Visually Impaired People
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adapa%2C+D">Dharmateja Adapa</a>, 
<a href="/search/cs?searchtype=author&query=Shekhawat%2C+V+S">Virendra Singh Shekhawat</a>, 
<a href="/search/cs?searchtype=author&query=Gautam%2C+A">Avinash Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+S">Sudeept Mohan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Large indoor spaces have complex layouts making them difficult to navigate.
Indoor spaces in hospitals, universities, shopping complexes, etc., carry
multi-modal information in the form of text and symbols. Hence, it is difficult
for Blind and Visually Impaired (BVI) people to independently navigate such
spaces. Indoor environments are usually GPS-denied; therefore, Bluetooth-based,
WiFi-based, or Range-based methods are used for localization. These methods
have high setup costs, lesser accuracy, and sometimes need special sensing
equipment. We propose a Visual Assist (VA) system for the indoor navigation of
BVI individuals using visual Fiducial markers for localization.
State-of-the-art (SOTA) approaches for visual localization using Fiducial
markers use fixed cameras having a narrow field of view. These approaches stop
tracking the markers when they are out of sight. We employ a Pan-Tilt
turret-mounted camera which enhances the field of view to 360{\deg} for
enhanced marker tracking. We, therefore, need fewer markers for mapping and
navigation. The efficacy of the proposed VA system is measured on three
metrics, i.e., RMSE (Root Mean Square Error), ADNN (Average Distance to Nearest
Neighbours), and ATE (Absolute Trajectory Error). Our system outperforms
Hector-SLAM, ORB-SLAM3, and UcoSLAM. The proposed system achieves localization
accuracy within $\pm8cm$ compared to $\pm12cm$ and $\pm10cm$ for ORB-SLAM3 and
UcoSLAM, respectively.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10292" title="Abstract">arXiv:2310.10292</a> [<a href="/pdf/2310.10292" title="Download PDF">pdf</a>, <a href="/format/2310.10292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effortless Cross-Platform Video Codec: A Codebook-Based Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+K">Kuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yonghang Guan</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jinxi Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wei Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Under certain circumstances, advanced neural video codecs can surpass the
most complex traditional codecs in their rate-distortion (RD) performance. One
of the main reasons for the high performance of existing neural video codecs is
the use of the entropy model, which can provide more accurate probability
distribution estimations for compressing the latents. This also implies the
rigorous requirement that entropy models running on different platforms should
use consistent distribution estimations. However, in cross-platform scenarios,
entropy models running on different platforms usually yield inconsistent
probability distribution estimations due to floating point computation errors
that are platform-dependent, which can cause the decoding side to fail in
correctly decoding the compressed bitstream sent by the encoding side. In this
paper, we propose a cross-platform video compression framework based on
codebooks, which avoids autoregressive entropy modeling and achieves video
compression by transmitting the index sequence of the codebooks. Moreover,
instead of using optical flow for context alignment, we propose to use the
conditional cross-attention module to obtain the context between frames. Due to
the absence of autoregressive modeling and optical flow alignment, we can
design an extremely minimalist framework that can greatly benefit computational
efficiency. Importantly, our framework no longer contains any distribution
estimation modules for entropy modeling, and thus computations across platforms
are not necessarily consistent. Experimental results show that our method can
outperform the traditional H.265 (medium) even without any entropy constraints,
while achieving the cross-platform property intrinsically.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10294" title="Abstract">arXiv:2310.10294</a> [<a href="/pdf/2310.10294" title="Download PDF">pdf</a>, <a href="/format/2310.10294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Key-phrase boosted unsupervised summary generation for FinTech  organization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+A">Aadit Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+S">Shreya Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Nagwanshi%2C+P">Prateek Nagwanshi</a>, 
<a href="/search/cs?searchtype=author&query=Tripathy%2C+A">Avinash Tripathy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the recent advances in social media, the use of NLP techniques in social
media data analysis has become an emerging research direction. Business
organizations can particularly benefit from such an analysis of social media
discourse, providing an external perspective on consumer behavior. Some of the
NLP applications such as intent detection, sentiment classification, text
summarization can help FinTech organizations to utilize the social media
language data to find useful external insights and can be further utilized for
downstream NLP tasks. Particularly, a summary which highlights the intents and
sentiments of the users can be very useful for these organizations to get an
external perspective. This external perspective can help organizations to
better manage their products, offers, promotional campaigns, etc. However,
certain challenges, such as a lack of labeled domain-specific datasets impede
further exploration of these tasks in the FinTech domain. To overcome these
challenges, we design an unsupervised phrase-based summary generation from
social media data, using 'Action-Object' pairs (intent phrases). We evaluated
the proposed method with other key-phrase based summary generation methods in
the direction of contextual information of various Reddit discussion threads,
available in the different summaries. We introduce certain "Context Metrics"
such as the number of Unique words, Action-Object pairs, and Noun chunks to
evaluate the contextual information retrieved from the source text in these
phrase-based summaries. We demonstrate that our methods significantly
outperform the baseline on these metrics, thus providing a qualitative and
quantitative measure of their efficacy. Proposed framework has been leveraged
as a web utility portal hosted within Amex.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10295" title="Abstract">arXiv:2310.10295</a> [<a href="/pdf/2310.10295" title="Download PDF">pdf</a>, <a href="/format/2310.10295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Software Heritage Open Science Ecosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Cosmo%2C+R">Roberto Di Cosmo</a> (UPCit&#xe9;), 
<a href="/search/cs?searchtype=author&query=Zacchiroli%2C+S">Stefano Zacchiroli</a> (IP Paris, LTCI)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Software Ecosystems, Springer International Publishing, pp.33-61,
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software Heritage is the largest public archive of software source code and
associated development history, as captured by modern version control systems.
As of July 2023, it has archived more than 16 billion unique source code files
coming from more than 250 million collaborative development projects. In this
chapter, we describe the Software Heritage ecosystem, focusing on research and
open science use cases.On the one hand, Software Heritage supports empirical
research on software by materializing in a single Merkle direct acyclic graph
the development history of public code. This giant graph of source code
artifacts (files, directories, and commits) can be used-and has been used-to
study repository forks, open source contributors, vulnerability propagation,
software provenance tracking, source code indexing, and more.On the other hand,
Software Heritage ensures availability and guarantees integrity of the source
code of software artifacts used in any field that relies on software to conduct
experiments, contributing to making research reproducible. The source code used
in scientific experiments can be archived-e.g., via integration with
open-access repositories-referenced using persistent identifiers that allow
downstream integrity checks and linked to/from other scholarly digital
artifacts.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10298" title="Abstract">arXiv:2310.10298</a> [<a href="/pdf/2310.10298" title="Download PDF">pdf</a>, <a href="/format/2310.10298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Summary-based Whole-program Analysis to Identify Unsafe Memory  Accesses in Rust
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingshen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Criswell%2C+J">John Criswell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Rust is one of the most promising systems programming languages to
fundamentally solve the memory safety issues that have plagued low-level
software for over forty years. However, to accommodate the scenarios where
Rust's type rules might be too restrictive for certain systems programming and
where programmers opt for performance over security checks, Rust opens security
escape hatches allowing writing unsafe source code or calling unsafe libraries.
Consequently, unsafe Rust code and directly-linked unsafe foreign libraries may
not only introduce memory safety violations themselves but also compromise the
entire program as they run in the same monolithic address space as the safe
Rust.
<br />This problem can be mitigated by isolating unsafe memory objects (those
accessed by unsafe code) and sandboxing memory accesses to the unsafe memory.
One category of prior work utilizes existing program analysis frameworks on
LLVM IR to identify unsafe memory objects and accesses. However, they suffer
the limitations of prolonged analysis time and low precision. In this paper, we
tackled these two challenges using summary-based whole-program analysis on
Rust's MIR. The summary-based analysis computes information on demand so as to
save analysis time. Performing analysis on Rust's MIR exploits the rich
high-level type information inherent to Rust, which is unavailable in LLVM IR.
This manuscript is a preliminary study of ongoing research. We have prototyped
a whole-program analysis for identifying both unsafe heap allocations and
memory accesses to those unsafe heap objects. We reported the overhead and the
efficacy of the analysis in this paper.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10299" title="Abstract">arXiv:2310.10299</a> [<a href="/pdf/2310.10299" title="Download PDF">pdf</a>, <a href="/format/2310.10299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forking Uncertainties: Reliable Prediction and Model Predictive Control  with Sequence Models via Conformal Risk Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zecchin%2C+M">Matteo Zecchin</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sangwoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Simeone%2C+O">Osvaldo Simeone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In many real-world problems, predictions are leveraged to monitor and control
cyber-physical systems, demanding guarantees on the satisfaction of reliability
and safety requirements. However, predictions are inherently uncertain, and
managing prediction uncertainty presents significant challenges in environments
characterized by complex dynamics and forking trajectories. In this work, we
assume access to a pre-designed probabilistic implicit or explicit sequence
model, which may have been obtained using model-based or model-free methods. We
introduce probabilistic time series-conformal risk prediction (PTS-CRC), a
novel post-hoc calibration procedure that operates on the predictions produced
by any pre-designed probabilistic forecaster to yield reliable error bars. In
contrast to existing art, PTS-CRC produces predictive sets based on an ensemble
of multiple prototype trajectories sampled from the sequence model, supporting
the efficient representation of forking uncertainties. Furthermore, unlike the
state of the art, PTS-CRC can satisfy reliability definitions beyond coverage.
This property is leveraged to devise a novel model predictive control (MPC)
framework that addresses open-loop and closed-loop control problems under
general average constraints on the quality or safety of the control policy. We
experimentally validate the performance of PTS-CRC prediction and control by
studying a number of use cases in the context of wireless networking. Across
all the considered tasks, PTS-CRC predictors are shown to provide more
informative predictive sets, as well as safe control policies with larger
returns.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10300" title="Abstract">arXiv:2310.10300</a> [<a href="/pdf/2310.10300" title="Download PDF">pdf</a>, <a href="/format/2310.10300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework  for Music-Dance Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kaixing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xukun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xulong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+R">Ran Diao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongyan Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jun He</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhaoxin Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Information Retrieval (cs.IR); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Dance and music are closely related forms of expression, with mutual
retrieval between dance videos and music being a fundamental task in various
fields like education, art, and sports. However, existing methods often suffer
from unnatural generation effects or fail to fully explore the correlation
between music and dance. To overcome these challenges, we propose BeatDance, a
novel beat-based model-agnostic contrastive learning framework. BeatDance
incorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat
Blender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval
performance by utilizing the alignment between music beats and dance movements.
We also introduce the Music-Dance (MD) dataset, a large-scale collection of
over 10,000 music-dance video pairs for training and testing. Experimental
results on the MD dataset demonstrate the superiority of our method over
existing baselines, achieving state-of-the-art performance. The code and
dataset will be made public available upon acceptance.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10301" title="Abstract">arXiv:2310.10301</a> [<a href="/pdf/2310.10301" title="Download PDF">pdf</a>, <a href="/format/2310.10301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Body Neural Scene Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidanapathirana%2C+K">Kavisha Vidanapathirana</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+S">Shin-Fang Chng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xueqian Li</a>, 
<a href="/search/cs?searchtype=author&query=Lucey%2C+S">Simon Lucey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for 3DV'2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">The test-time optimization of scene flow - using a coordinate network as a
neural prior - has gained popularity due to its simplicity, lack of dataset
bias, and state-of-the-art performance. We observe, however, that although
coordinate networks capture general motions by implicitly regularizing the
scene flow predictions to be spatially smooth, the neural prior by itself is
unable to identify the underlying multi-body rigid motions present in
real-world data. To address this, we show that multi-body rigidity can be
achieved without the cumbersome and brittle strategy of constraining the
$SE(3)$ parameters of each rigid body as done in previous works. This is
achieved by regularizing the scene flow optimization to encourage isometry in
flow predictions for rigid bodies. This strategy enables multi-body rigidity in
scene flow while maintaining a continuous flow field, hence allowing dense
long-term scene flow integration across a sequence of point clouds. We conduct
extensive experiments on real-world datasets and demonstrate that our approach
outperforms the state-of-the-art in 3D scene flow and long-term point-wise 4D
trajectory prediction. The code is available at:
\href{https://github.com/kavisha725/MBNSF}{https://github.com/kavisha725/MBNSF}.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10307" title="Abstract">arXiv:2310.10307</a> [<a href="/pdf/2310.10307" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning visual-based deformable object rearrangement with local graph  neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuhong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=chen%2C+L">Lipeng chen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Complex &amp; Intelligent Systems, 2023: 1-14
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Goal-conditioned rearrangement of deformable objects (e.g. straightening a
rope and folding a cloth) is one of the most common deformable manipulation
tasks, where the robot needs to rearrange a deformable object into a prescribed
goal configuration with only visual observations. These tasks are typically
confronted with two main challenges: the high dimensionality of deformable
configuration space and the underlying complexity, nonlinearity and uncertainty
inherent in deformable dynamics. To address these challenges, we propose a
novel representation strategy that can efficiently model the deformable object
states with a set of keypoints and their interactions. We further propose
local-graph neural network (GNN), a light local GNN learning to jointly model
the deformable rearrangement dynamics and infer the optimal manipulation
actions (e.g. pick and place) by constructing and updating two dynamic graphs.
Both simulated and real experiments have been conducted to demonstrate that the
proposed dynamic graph representation shows superior expressiveness in modeling
deformable rearrangement dynamics. Our method reaches much higher success rates
on a variety of deformable rearrangement tasks (96.3% on average) than
state-of-the-art method in simulation experiments. Besides, our method is much
more lighter and has a 60% shorter inference time than state-of-the-art
methods. We also demonstrate that our method performs well in the multi-task
learning scenario and can be transferred to real-world applications with an
average success rate of 95% by solely fine tuning a keypoint detector.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10308" title="Abstract">arXiv:2310.10308</a> [<a href="/pdf/2310.10308" title="Download PDF">pdf</a>, <a href="/format/2310.10308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time integration schemes based on neural networks for solving partial  differential equations on coarse grids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yan%2C+X">Xinxin Yan</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Z">Zhideng Zhou</a>, 
<a href="/search/math?searchtype=author&query=Cheng%2C+X">Xiaohan Cheng</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+X">Xiaolei Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The accuracy of solving partial differential equations (PDEs) on coarse grids
is greatly affected by the choice of discretization schemes. In this work, we
propose to learn time integration schemes based on neural networks which
satisfy three distinct sets of mathematical constraints, i.e., unconstrained,
semi-constrained with the root condition, and fully-constrained with both root
and consistency conditions. We focus on the learning of 3-step linear multistep
methods, which we subsequently applied to solve three model PDEs, i.e., the
one-dimensional heat equation, the one-dimensional wave equation, and the
one-dimensional Burgers' equation. The results show that the prediction error
of the learned fully-constrained scheme is close to that of the Runge-Kutta
method and Adams-Bashforth method. Compared to the traditional methods, the
learned unconstrained and semi-constrained schemes significantly reduce the
prediction error on coarse grids. On a grid that is 4 times coarser than the
reference grid, the mean square error shows a reduction of up to an order of
magnitude for some of the heat equation cases, and a substantial improvement in
phase prediction for the wave equation. On a 32 times coarser grid, the mean
square error for the Burgers' equation can be reduced by up to 35% to 40%.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10310" title="Abstract">arXiv:2310.10310</a> [<a href="/pdf/2310.10310" title="Download PDF">pdf</a>, <a href="/format/2310.10310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Bias in Multilingual Language Models: Cross-Lingual  Transfer of Debiasing Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reusens%2C+M">Manon Reusens</a>, 
<a href="/search/cs?searchtype=author&query=Borchert%2C+P">Philipp Borchert</a>, 
<a href="/search/cs?searchtype=author&query=Mieskes%2C+M">Margot Mieskes</a>, 
<a href="/search/cs?searchtype=author&query=De+Weerdt%2C+J">Jochen De Weerdt</a>, 
<a href="/search/cs?searchtype=author&query=Baesens%2C+B">Bart Baesens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper investigates the transferability of debiasing techniques across
different languages within multilingual models. We examine the applicability of
these techniques in English, French, German, and Dutch. Using multilingual BERT
(mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is
not only feasible but also yields promising results. Surprisingly, our findings
reveal no performance disadvantages when applying these techniques to
non-English languages. Using translations of the CrowS-Pairs dataset, our
analysis identifies SentenceDebias as the best technique across different
languages, reducing bias in mBERT by an average of 13%. We also find that
debiasing techniques with additional pretraining exhibit enhanced cross-lingual
effectiveness for the languages included in the analyses, particularly in
lower-resource languages. These novel insights contribute to a deeper
understanding of bias mitigation in multilingual language models and provide
practical guidance for debiasing techniques in different language contexts.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10312" title="Abstract">arXiv:2310.10312</a> [<a href="/pdf/2310.10312" title="Download PDF">pdf</a>, <a href="/format/2310.10312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Offline Reinforcement Learning for Glycemia Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beolet%2C+T">Tristan Beolet</a>, 
<a href="/search/cs?searchtype=author&query=Adenis%2C+A">Alice Adenis</a>, 
<a href="/search/cs?searchtype=author&query=Huneker%2C+E">Erik Huneker</a>, 
<a href="/search/cs?searchtype=author&query=Louis%2C+M">Maxime Louis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">The development of closed-loop systems for glycemia control in type I
diabetes relies heavily on simulated patients. Improving the performances and
adaptability of these close-loops raises the risk of over-fitting the
simulator. This may have dire consequences, especially in unusual cases which
were not faithfully-if at all-captured by the simulator. To address this, we
propose to use offline RL agents, trained on real patient data, to perform the
glycemia control. To further improve the performances, we propose an end-to-end
personalization pipeline, which leverages offline-policy evaluation methods to
remove altogether the need of a simulator, while still enabling an estimation
of clinically relevant metrics for diabetes.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10316" title="Abstract">arXiv:2310.10316</a> [<a href="/pdf/2310.10316" title="Download PDF">pdf</a>, <a href="/ps/2310.10316" title="Download PostScript">ps</a>, <a href="/format/2310.10316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral representation of two-sided signals from $\ell_\infty$ and  applications to signal processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dokuchaev%2C+N">Nikolai Dokuchaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Functional Analysis (math.FA)

</div>
<p class="mathjax">The paper presents a spectral representation for general type two-sided
discrete time signals from $\ell_\infty$, i.e for all bounded discrete time
signals, including signals that do not vanish at $\pm\infty$. This
representation allows to extend on the general type signals from $\ell_\infty$
the notions of transfer functions, spectrum gaps, and filters, and to obtain
some frequency conditions of predictability and data recoverability.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10318" title="Abstract">arXiv:2310.10318</a> [<a href="/pdf/2310.10318" title="Download PDF">pdf</a>, <a href="/format/2310.10318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting and Exploiting Functional Specialization in Multi-Head  Attention under Multi-task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chong Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shaonan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiajun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+C">Chengqing Zong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference. Our code is available at <a href="https://github.com/ZNLP/FunctionalSpecializationInMHA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transformer-based models, even though achieving super-human performance on
several downstream tasks, are often regarded as a black box and used as a
whole. It is still unclear what mechanisms they have learned, especially their
core module: multi-head attention. Inspired by functional specialization in the
human brain, which helps to efficiently handle multiple tasks, this work
attempts to figure out whether the multi-head attention module will evolve
similar function separation under multi-tasking training. If it is, can this
mechanism further improve the model performance? To investigate these
questions, we introduce an interpreting method to quantify the degree of
functional specialization in multi-head attention. We further propose a simple
multi-task training method to increase functional specialization and mitigate
negative information transfer in multi-task learning. Experimental results on
seven pre-trained transformer models have demonstrated that multi-head
attention does evolve functional specialization phenomenon after multi-task
training which is affected by the similarity of tasks. Moreover, the multi-task
training strategy based on functional specialization boosts performance in both
multi-task learning and transfer learning without adding any parameters.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10320" title="Abstract">arXiv:2310.10320</a> [<a href="/pdf/2310.10320" title="Download PDF">pdf</a>, <a href="/format/2310.10320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Particle Swarm Optimization for through-foliage target  detection with drone swarms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=P%C3%B6schl%2C+J">Julia P&#xf6;schl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">This work contributes to efforts on autonomously detecting a
vegetation-occluded target by airborne observers. It investigates and enhances
previous work on a Particle Swarm Optimization (PSO) strategy for Airborne
Optical Sectioning (AOS) drone swarms. First, it identifies two issues with
that method and proposes to resolve them by a leader stabilization for its
scattering and projection-based line positions for its default scanning
pattern. Second, it connects this method to other PSO variants and presents a
new adaptive PSO strategy for AOS drone swarms that draws on the ideas of
Adaptive PSO (APSO).
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10321" title="Abstract">arXiv:2310.10321</a> [<a href="/pdf/2310.10321" title="Download PDF">pdf</a>, <a href="/format/2310.10321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hamming Encoder: Mining Discriminative k-mers for Discrete Sequence  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Junjie Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Mudi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lianyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zengyou He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sequence classification has numerous applications in various fields. Despite
extensive studies in the last decades, many challenges still exist,
particularly in pattern-based methods. Existing pattern-based methods measure
the discriminative power of each feature individually during the mining
process, leading to the result of missing some combinations of features with
discriminative power. Furthermore, it is difficult to ensure the overall
discriminative performance after converting sequences into feature vectors. To
address these challenges, we propose a novel approach called Hamming Encoder,
which utilizes a binarized 1D-convolutional neural network (1DCNN) architecture
to mine discriminative k-mer sets. In particular, we adopt a Hamming
distance-based similarity measure to ensure consistency in the feature mining
and classification procedure. Our method involves training an interpretable CNN
encoder for sequential data and performing a gradient-based search for
discriminative k-mer combinations. Experiments show that the Hamming Encoder
method proposed in this paper outperforms existing state-of-the-art methods in
terms of classification accuracy.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10322" title="Abstract">arXiv:2310.10322</a> [<a href="/pdf/2310.10322" title="Download PDF">pdf</a>, <a href="/format/2310.10322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Untying the Reversal Curse via Bidirectional Language Model Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jun-Yu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jia-Chen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhen-Hua Ling</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Quan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent studies have demonstrated that large language models (LLMs) store
massive factual knowledge within their parameters. But existing LLMs are prone
to hallucinate unintended text due to false or outdated knowledge. Since
retraining LLMs is resource intensive, there has been a growing interest in the
concept of model editing. Despite the emergence of benchmarks and approaches,
these unidirectional editing and evaluation have failed to explore the reversal
curse. Intuitively, if "The capital of France is" is edited to be a counterfact
"London" within a model, then it should be able to naturally reason and recall
the reverse fact, i.e., "London is the capital of" followed by "France" instead
of "England". In this paper, we study bidirectional language model editing,
aiming to provide rigorous model editing evaluation to assess if edited LLMs
can recall the editing knowledge bidirectionally. A new evaluation metric of
reversibility is introduced, and a benchmark dubbed as Bidirectional Assessment
for Knowledge Editing (BAKE) is constructed to evaluate the reversibility of
edited models in recalling knowledge in the reverse direction of editing. We
surprisingly observe that while current editing methods and LLMs can
effectively recall editing facts in the direction of editing, they suffer
serious deficiencies when evaluated in the reverse direction. To mitigate the
reversal curse, a method named Bidirectionally Inversible Relationship moDeling
(BIRD) is proposed. A set of editing objectives that incorporate bidirectional
relationships between subject and object into the updated model weights are
designed. Experiments show that BIRD improves the performance of four
representative LLMs of different sizes via question answering and judgement.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10325" title="Abstract">arXiv:2310.10325</a> [<a href="/pdf/2310.10325" title="Download PDF">pdf</a>, <a href="/format/2310.10325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards image compression with perfect realism at ultra-low bitrates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Careil%2C+M">Marl&#xe8;ne Careil</a>, 
<a href="/search/cs?searchtype=author&query=Muckley%2C+M+J">Matthew J. Muckley</a>, 
<a href="/search/cs?searchtype=author&query=Verbeek%2C+J">Jakob Verbeek</a>, 
<a href="/search/cs?searchtype=author&query=Lathuili%C3%A8re%2C+S">St&#xe9;phane Lathuili&#xe8;re</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Image codecs are typically optimized to trade-off bitrate vs, distortion
metrics. At low bitrates, this leads to compression artefacts which are easily
perceptible, even when training with perceptual or adversarial losses. To
improve image quality, and to make it less dependent on the bitrate, we propose
to decode with iterative diffusion models, instead of feed-forward decoders
trained using MSE or LPIPS distortions used in most neural codecs. In addition
to conditioning the model on a vector-quantized image representation, we also
condition on a global textual image description to provide additional context.
We dub our model PerCo for 'perceptual compression', and compare it to
state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The
latter rate is an order of magnitude smaller than those considered in most
prior work. At this bitrate a 512x768 Kodak image is encoded in less than 153
bytes. Despite this ultra-low bitrate, our approach maintains the ability to
reconstruct realistic images. We find that our model leads to reconstructions
with state-of-the-art visual quality as measured by FID and KID, and that the
visual quality is less dependent on the bitrate than previous methods.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10326" title="Abstract">arXiv:2310.10326</a> [<a href="/pdf/2310.10326" title="Download PDF">pdf</a>, <a href="/ps/2310.10326" title="Download PostScript">ps</a>, <a href="/format/2310.10326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arithmetic as a theory modulo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dowek%2C+G">Gilles Dowek</a>, 
<a href="/search/cs?searchtype=author&query=Werner%2C+B">Benjamin Werner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We present constructive arithmetic in Deduction modulo with rewrite rules
only.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10330" title="Abstract">arXiv:2310.10330</a> [<a href="/pdf/2310.10330" title="Download PDF">pdf</a>, <a href="/format/2310.10330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Metasurface Practicality for B5G Networks: AI-assisted RIS  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Encinas-Lago%2C+G">Guillermo Encinas-Lago</a>, 
<a href="/search/cs?searchtype=author&query=Albanese%2C+A">Antonio Albanese</a>, 
<a href="/search/cs?searchtype=author&query=Sciancalepore%2C+V">Vincenzo Sciancalepore</a>, 
<a href="/search/cs?searchtype=author&query=Di+Renzo%2C+M">Marco Di Renzo</a>, 
<a href="/search/cs?searchtype=author&query=Costa-P%C3%A9rez%2C+X">Xavier Costa-P&#xe9;rez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">The advent of reconfigurable intelligent surfaces(RISs) brings along
significant improvements for wireless technology on the verge of
beyond-fifth-generation networks (B5G).The proven flexibility in influencing
the propagation environment opens up the possibility of programmatically
altering the wireless channel to the advantage of network designers, enabling
the exploitation of higher-frequency bands for superior throughput overcoming
the challenging electromagnetic (EM) propagation properties at these frequency
bands.
<br />However, RISs are not magic bullets. Their employment comes with significant
complexity, requiring ad-hoc deployments and management operations to come to
fruition. In this paper, we tackle the open problem of bringing RISs to the
field, focusing on areas with little or no coverage. In fact, we present a
first-of-its-kind deep reinforcement learning (DRL) solution, dubbed as D-RISA,
which trains a DRL agent and, in turn, obtain san optimal RIS deployment. We
validate our framework in the indoor scenario of the Rennes railway station in
France, assessing the performance of our algorithm against state-of-the-art
(SOA) approaches. Our benchmarks showcase better coverage, i.e., 10-dB increase
in minimum signal-to-noise ratio (SNR), at lower computational time (up to -25
percent) while improving scalability towards denser network deployments.
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10333" title="Abstract">arXiv:2310.10333</a> [<a href="/pdf/2310.10333" title="Download PDF">pdf</a>, <a href="/format/2310.10333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLP for Crypto-Asset Regulation: A Roadmap
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Camassa%2C+C">Carolina Camassa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NLLP23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL); General Finance (q-fin.GN)

</div>
<p class="mathjax">In the rapidly evolving field of crypto-assets, white papers are essential
documents for investor guidance, and are now subject to unprecedented content
requirements under the EU's Markets in Crypto-Assets Regulation (MiCAR).
Natural Language Processing can serve as a powerful tool for both analyzing
these documents and assisting in regulatory compliance. This paper delivers two
contributions to the topic. First, we survey existing applications of textual
analysis to unregulated crypto-asset white papers, uncovering a research gap
that could be bridged with interdisciplinary collaboration. We then conduct an
analysis of the changes introduced by MiCAR, highlighting the opportunities and
challenges of integrating NLP within the new regulatory framework. Our findings
set the stage for further research, with the potential to benefit regulators,
crypto-asset issuers, and investors.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10336" title="Abstract">arXiv:2310.10336</a> [<a href="/pdf/2310.10336" title="Download PDF">pdf</a>, <a href="/format/2310.10336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multilayered Security Infrastructure for Connected Vehicles -- First  Lessons from the Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%A4ckel%2C+T">Timo H&#xe4;ckel</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+P">Philipp Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Stahlbock%2C+L">Lukas Stahlbock</a>, 
<a href="/search/cs?searchtype=author&query=Langer%2C+F">Falk Langer</a>, 
<a href="/search/cs?searchtype=author&query=Eckhardt%2C+S+A">Sebastian A. Eckhardt</a>, 
<a href="/search/cs?searchtype=author&query=Korf%2C+F">Franz Korf</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+T+C">Thomas C. Schmidt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the BROAD workshop at the 2022 IEEE Intelligent Vehicles Symposium (IV) in Aachen, Germany
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Connected vehicles are vulnerable to manipulation and a broad attack surface
can be used to intrude in-vehicle networks from anywhere on earth. In this
work, we present an integrated security infrastructure comprising network
protection, monitoring, incident management, and counteractions, which we built
into a prototype based on a production car. Our vehicle implements a
Software-Defined Networking Ethernet backbone to restrict communication routes,
network anomaly detection to make misbehavior evident, virtual controller
functions to enable agile countermeasures, and an automotive cloud defense
center to analyse and manage incidents on vehicle fleets. We present first
measurements and lessons learned from operating the prototype: many network
attacks can be prevented through software-defined access control in the
backbone; anomaly detection can reliably detect misbehavior but needs to
improve on false positive rate; controller virtualization needs tailored
frameworks to meet in-car requirements; and cloud defence enables fleet
management and advanced countermeasures. Our findings indicate attack
mitigation times in the vehicle from 257 ms to 328 ms and from 2,168 ms to
2,713 ms traversing the cloud.
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10338" title="Abstract">arXiv:2310.10338</a> [<a href="/pdf/2310.10338" title="Download PDF">pdf</a>, <a href="/format/2310.10338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scene Graph Conditioning in Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fundel%2C+F">Frank Fundel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models excel in image generation but lack detailed semantic control
using text prompts. Additional techniques have been developed to address this
limitation. However, conditioning diffusion models solely on text-based
descriptions is challenging due to ambiguity and lack of structure. In
contrast, scene graphs offer a more precise representation of image content,
making them superior for fine-grained control and accurate synthesis in image
generation models. The amount of image and scene-graph data is sparse, which
makes fine-tuning large diffusion models challenging. We propose multiple
approaches to tackle this problem using ControlNet and Gated Self-Attention. We
were able to show that using out proposed methods it is possible to generate
images from scene graphs with much higher quality, outperforming previous
methods. Our source code is publicly available on
https://github.com/FrankFundel/SGCond
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10343" title="Abstract">arXiv:2310.10343</a> [<a href="/pdf/2310.10343" title="Download PDF">pdf</a>, <a href="/format/2310.10343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiayu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Ziang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yunfei Duan</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+P">Pan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongdong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Given a single image of a 3D object, this paper proposes a novel method
(named ConsistNet) that is able to generate multiple images of the same object,
as if seen they are captured from different viewpoints, while the 3D
(multi-view) consistencies among those multiple generated images are
effectively exploited. Central to our method is a multi-view consistency block
which enables information exchange across multiple single-view diffusion
processes based on the underlying multi-view geometry principles. ConsistNet is
an extension to the standard latent diffusion model, and consists of two
sub-modules: (a) a view aggregation module that unprojects multi-view features
into global 3D volumes and infer consistency, and (b) a ray aggregation module
that samples and aggregate 3D consistent features back to each view to enforce
consistency. Our approach departs from previous methods in multi-view image
generation, in that it can be easily dropped-in pre-trained LDMs without
requiring explicit pixel correspondences or depth prediction. Experiments show
that our method effectively learns 3D consistency over a frozen Zero123
backbone and can generate 16 surrounding views of the object within 40 seconds
on a single A100 GPU. Our code will be made available on
https://github.com/JiayuYANG/ConsistNet
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10347" title="Abstract">arXiv:2310.10347</a> [<a href="/pdf/2310.10347" title="Download PDF">pdf</a>, <a href="/format/2310.10347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Editable-DeepSC: Cross-Modal Editable Semantic Communication Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenbo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinshan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shu-Tao Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Different from \emph{data-oriented} communication systems primarily focusing
on how to accurately transmit every bit of data, \emph{task-oriented} semantic
communication systems \iffalse, which are capturing widespread research
attention recently,\fi only transmit the specific semantic information required
by downstream tasks, striving to minimize the communication overhead and
maintain competitive tasks execution performance in the presence of channel
noise. However, it is worth noting that in many scenarios, the transmitted
semantic information needs to be dynamically modified according to the users'
preferences and few existing works take this into consideration. Therefore, in
this paper, we propose a novel cross-modal editable semantic communication
system, named \emph{Editable-DeepSC}, to tackle this challenge. By utilizing
inversion methods based on StyleGAN priors, \emph{Editable-DeepSC} takes
cross-modal text-image pairs as inputs and transmits the edited information of
images based on textual instructions. Extensive numerical results demonstrate
that our proposed \emph{Editable-DeepSC} can achieve remarkable editing effects
under the perturbations of channel noise, outperforming existing
\emph{data-oriented} communication methods.
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10348" title="Abstract">arXiv:2310.10348</a> [<a href="/pdf/2310.10348" title="Download PDF">pdf</a>, <a href="/format/2310.10348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attribution Patching Outperforms Automated Circuit Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Syed%2C+A">Aaquib Syed</a>, 
<a href="/search/cs?searchtype=author&query=Rager%2C+C">Can Rager</a>, 
<a href="/search/cs?searchtype=author&query=Conmy%2C+A">Arthur Conmy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Automated interpretability research has recently attracted attention as a
potential research direction that could scale explanations of neural network
behavior to large models. Existing automated circuit discovery work applies
activation patching to identify subnetworks responsible for solving specific
tasks (circuits). In this work, we show that a simple method based on
attribution patching outperforms all existing methods while requiring just two
forward passes and a backward pass. We apply a linear approximation to
activation patching to estimate the importance of each edge in the
computational subgraph. Using this approximation, we prune the least important
edges of the network. We survey the performance and limitations of this method,
finding that averaged over all tasks our method has greater AUC from circuit
recovery than other methods.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10349" title="Abstract">arXiv:2310.10349</a> [<a href="/pdf/2310.10349" title="Download PDF">pdf</a>, <a href="/format/2310.10349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Layerwise Polynomial Approximation for Efficient Private  Inference on Fully Homomorphic Encryption: A Dynamic Programming Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+E">Eunsang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Sik Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yongwoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joon-Woo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yongjune Kim</a>, 
<a href="/search/cs?searchtype=author&query=No%2C+J">Jong-Seon No</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent research has explored the implementation of privacy-preserving deep
neural networks solely using fully homomorphic encryption. However, its
practicality has been limited because of prolonged inference times. When using
a pre-trained model without retraining, a major factor contributing to these
prolonged inference times is the high-degree polynomial approximation of
activation functions such as the ReLU function. The high-degree approximation
consumes a substantial amount of homomorphic computational resources, resulting
in slower inference. Unlike the previous works approximating activation
functions uniformly and conservatively, this paper presents a \emph{layerwise}
degree optimization of activation functions to aggressively reduce the
inference time while maintaining classification accuracy by taking into account
the characteristics of each layer. Instead of the minimax approximation
commonly used in state-of-the-art private inference models, we employ the
weighted least squares approximation method with the input distributions of
activation functions. Then, we obtain the layerwise optimized degrees for
activation functions through the \emph{dynamic programming} algorithm,
considering how each layer's approximation error affects the classification
accuracy of the deep neural network. Furthermore, we propose modulating the
ciphertext moduli-chain layerwise to reduce the inference time. By these
proposed layerwise optimization methods, we can reduce inference times for the
ResNet-20 model and the ResNet-32 model by 3.44 times and 3.16 times,
respectively, in comparison to the prior implementations employing uniform
degree polynomials and a consistent ciphertext modulus.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10351" title="Abstract">arXiv:2310.10351</a> [<a href="/pdf/2310.10351" title="Download PDF">pdf</a>, <a href="/format/2310.10351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> B-rep Boolean Resulting Model Repair by Correcting Intersection Edges  Based on Inference Procedure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haomian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+E">Enya Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">As the most essential part of CAD modeling operations, boolean operations on
B-rep CAD models often suffer from errors. Errors caused by geometric precision
or numerical uncertainty are hard to eliminate. They will reduce the
reliability of boolean operations and damage the integrity of the resulting
models. And it is difficult to repair false boolean resulting models damaged by
errors. In practice, we find that the illegal boolean resulting models stem
from the false intersection edges caused by errors. Therefore, this paper
proposes an automatic method based on set reasoning to repair flawed structures
of the boolean resulting models by correcting their topological intersection
edges. We provide a local adaptive tolerance estimation method for each
intersection edge based on its geometric features as well as its origin. Then,
we propose a set of inference mechanisms based on set operations to infer
whether a repair is needed based on the tolerance value and how to correct the
inaccurate intersection edge. Our inference strategies are strictly proven,
ensuring the reliability and robustness of the repair process. The inference
process will transform the problem into a geometric equivalent form less
susceptible to errors to get a more accurate intersection edge. Since our
inference procedure focuses on topological features, our method can repair the
flawed boolean resulting models, no matter what source of errors causes the
problem.
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10352" title="Abstract">arXiv:2310.10352</a> [<a href="/pdf/2310.10352" title="Download PDF">pdf</a>, <a href="/format/2310.10352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Crowd Counting with Contextual Modeling: Facilitating  Holistic Understanding of Crowd Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yifei Qian</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+X">Xiaopeng Hong</a>, 
<a href="/search/cs?searchtype=author&query=Arandjelovi%C4%87%2C+O">Ognjen Arandjelovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhongliang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Donovan%2C+C+R">Carl R.Donovan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">To alleviate the heavy annotation burden for training a reliable crowd
counting model and thus make the model more practicable and accurate by being
able to benefit from more data, this paper presents a new semi-supervised
method based on the mean teacher framework. When there is a scarcity of labeled
data available, the model is prone to overfit local patches. Within such
contexts, the conventional approach of solely improving the accuracy of local
patch predictions through unlabeled data proves inadequate. Consequently, we
propose a more nuanced approach: fostering the model's intrinsic 'subitizing'
capability. This ability allows the model to accurately estimate the count in
regions by leveraging its understanding of the crowd scenes, mirroring the
human cognitive process. To achieve this goal, we apply masking on unlabeled
data, guiding the model to make predictions for these masked patches based on
the holistic cues. Furthermore, to help with feature learning, herein we
incorporate a fine-grained density classification task. Our method is general
and applicable to most existing crowd counting methods as it doesn't have
strict structural or loss constraints. In addition, we observe that the model
trained with our framework exhibits a 'subitizing'-like behavior. It accurately
predicts low-density regions with only a 'glance', while incorporating local
details to predict high-density regions. Our method achieves the
state-of-the-art performance, surpassing previous approaches by a large margin
on challenging benchmarks such as ShanghaiTech A and UCF-QNRF. The code is
available at: https://github.com/cha15yq/MRC-Crowd.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10353" title="Abstract">arXiv:2310.10353</a> [<a href="/pdf/2310.10353" title="Download PDF">pdf</a>, <a href="/format/2310.10353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Object Query Initialization for 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Geerenstein%2C+M+R">Mathijs R. van Geerenstein</a>, 
<a href="/search/cs?searchtype=author&query=Ruppel%2C+F">Felicia Ruppel</a>, 
<a href="/search/cs?searchtype=author&query=Dietmayer%2C+K">Klaus Dietmayer</a>, 
<a href="/search/cs?searchtype=author&query=Gavrila%2C+D+M">Dariu M. Gavrila</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">3D object detection models that exploit both LiDAR and camera sensor features
are top performers in large-scale autonomous driving benchmarks. A transformer
is a popular network architecture used for this task, in which so-called object
queries act as candidate objects. Initializing these object queries based on
current sensor inputs is a common practice. For this, existing methods strongly
rely on LiDAR data however, and do not fully exploit image features. Besides,
they introduce significant latency. To overcome these limitations we propose
EfficientQ3M, an efficient, modular, and multimodal solution for object query
initialization for transformer-based 3D object detection models. The proposed
initialization method is combined with a "modality-balanced" transformer
decoder where the queries can access all sensor modalities throughout the
decoder. In experiments, we outperform the state of the art in
transformer-based LiDAR object detection on the competitive nuScenes benchmark
and showcase the benefits of input-dependent multimodal query initialization,
while being more efficient than the available alternatives for LiDAR-camera
initialization. The proposed method can be applied with any combination of
sensor modalities as input, demonstrating its modularity.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10355" title="Abstract">arXiv:2310.10355</a> [<a href="/pdf/2310.10355" title="Download PDF">pdf</a>, <a href="/format/2310.10355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology optimization of fluidic pressure-driven multi-material  compliant mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Prabhat Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Pinskier%2C+J">Josh Pinskier</a>, 
<a href="/search/cs?searchtype=author&query=Howard%2C+D">David Howard</a>, 
<a href="/search/cs?searchtype=author&query=Langelaar%2C+M">Matthijs Langelaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the ASME IDETC conference 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Compliant mechanisms actuated by pneumatic loads are receiving increasing
attention due to their direct applicability as soft robots that perform tasks
using their flexible bodies. Using multiple materials to build them can further
improve their performance and efficiency. Due to developments in additive
manufacturing, the fabrication of multi-material soft robots is becoming a real
possibility. To exploit this opportunity, there is a need for a dedicated
design approach. This paper offers a systematic approach to developing such
mechanisms using topology optimization. The extended SIMP scheme is employed
for multi-material modeling. The design-dependent nature of the pressure load
is modeled using the Darcy law with a volumetric drainage term. Flow
coefficient of each element is interpolated using a smoothed Heaviside
function. The obtained pressure field is converted to consistent nodal loads.
The adjoint-variable approach is employed to determine the sensitivities. A
robust formulation is employed, wherein a min-max optimization problem is
formulated using the output displacements of the eroded and blueprint designs.
Volume constraints are applied to the blueprint design, whereas the strain
energy constraint is formulated with respect to the eroded design. The efficacy
and success of the approach are demonstrated by designing pneumatically
actuated multi-material gripper and contractor mechanisms. A numerical study
confirms that multiple-material mechanisms perform relatively better than their
single-material counterparts.
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10357" title="Abstract">arXiv:2310.10357</a> [<a href="/pdf/2310.10357" title="Download PDF">pdf</a>, <a href="/format/2310.10357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BEVGPT: Generative Pre-trained Large Model for Autonomous Driving  Prediction, Decision-Making, and Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengqin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Meixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hongliang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Hui Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xianda Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shaojie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuesong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinhai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Prediction, decision-making, and motion planning are essential for autonomous
driving. In most contemporary works, they are considered as individual modules
or combined into a multi-task learning paradigm with a shared backbone but
separate task heads. However, we argue that they should be integrated into a
comprehensive framework. Although several recent approaches follow this scheme,
they suffer from complicated input representations and redundant framework
designs. More importantly, they can not make long-term predictions about future
driving scenarios. To address these issues, we rethink the necessity of each
module in an autonomous driving task and incorporate only the required modules
into a minimalist autonomous driving framework. We propose BEVGPT, a generative
pre-trained large model that integrates driving scenario prediction,
decision-making, and motion planning. The model takes the bird's-eye-view (BEV)
images as the only input source and makes driving decisions based on
surrounding traffic scenarios. To ensure driving trajectory feasibility and
smoothness, we develop an optimization-based motion planning method. We
instantiate BEVGPT on Lyft Level 5 Dataset and use Woven Planet L5Kit for
realistic driving simulation. The effectiveness and robustness of the proposed
framework are verified by the fact that it outperforms previous methods in 100%
decision-making metrics and 66% motion planning metrics. Furthermore, the
ability of our framework to accurately generate BEV images over the long term
is demonstrated through the task of driving scenario prediction. To the best of
our knowledge, this is the first generative pre-trained large model for
autonomous driving prediction, decision-making, and motion planning with only
BEV images as input.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10358" title="Abstract">arXiv:2310.10358</a> [<a href="/pdf/2310.10358" title="Download PDF">pdf</a>, <a href="/format/2310.10358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tabular Representation, Noisy Operators, and Impacts on Table Structure  Understanding Tasks in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singha%2C+A">Ananya Singha</a>, 
<a href="/search/cs?searchtype=author&query=Cambronero%2C+J">Jos&#xe9; Cambronero</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Vu Le</a>, 
<a href="/search/cs?searchtype=author&query=Parnin%2C+C">Chris Parnin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are increasingly applied for tabular tasks using
in-context learning. The prompt representation for a table may play a role in
the LLMs ability to process the table. Inspired by prior work, we generate a
collection of self-supervised structural tasks (e.g. navigate to a cell and
row; transpose the table) and evaluate the performance differences when using 8
formats. In contrast to past work, we introduce 8 noise operations inspired by
real-world messy data and adversarial inputs, and show that such operations can
impact LLM performance across formats for different structural understanding
tasks.
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10362" title="Abstract">arXiv:2310.10362</a> [<a href="/pdf/2310.10362" title="Download PDF">pdf</a>, <a href="/format/2310.10362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Tuning for Multi-View Graph Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+C">Chenghua Gong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jianxiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+C">Cheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jiaqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chengcheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, "pre-training and fine-tuning" has emerged as a promising
approach in addressing the issues of label dependency and poor generalization
performance in traditional GNNs. To reduce labeling requirement, the
"pre-train, fine-tune" and "pre-train, prompt" paradigms have become
increasingly common. In particular, prompt tuning is a popular alternative to
"pre-training and fine-tuning" in natural language processing, which is
designed to narrow the gap between pre-training and downstream objectives.
However, existing study of prompting on graphs is still limited, lacking a
framework that can accommodate commonly used graph pre-training methods and
downstream tasks. In this paper, we propose a multi-view graph contrastive
learning method as pretext and design a prompting tuning for it. Specifically,
we first reformulate graph pre-training and downstream tasks into a common
format. Second, we construct multi-view contrasts to capture relevant
information of graphs by GNN. Third, we design a prompting tuning method for
our multi-view graph contrastive learning method to bridge the gap between
pretexts and downsteam tasks. Finally, we conduct extensive experiments on
benchmark datasets to evaluate and analyze our proposed method.
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10368" title="Abstract">arXiv:2310.10368</a> [<a href="/pdf/2310.10368" title="Download PDF">pdf</a>, <a href="/format/2310.10368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine learning in physics: a short guide
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+F+A">Francisco A. Rodrigues</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure. Europhysics Letters (EPL), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistical Mechanics (cond-mat.stat-mech); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">Machine learning is a rapidly growing field with the potential to
revolutionize many areas of science, including physics. This review provides a
brief overview of machine learning in physics, covering the main concepts of
supervised, unsupervised, and reinforcement learning, as well as more
specialized topics such as causal inference, symbolic regression, and deep
learning. We present some of the principal applications of machine learning in
physics and discuss the associated challenges and perspectives.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10371" title="Abstract">arXiv:2310.10371</a> [<a href="/pdf/2310.10371" title="Download PDF">pdf</a>, <a href="/format/2310.10371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Camera-LiDAR Fusion with Latent Contact for Place Recognition in  Challenging Cross-Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jiapeng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bo Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Although significant progress has been made, achieving place recognition in
environments with perspective changes, seasonal variations, and scene
transformations remains challenging. Relying solely on perception information
from a single sensor is insufficient to address these issues. Recognizing the
complementarity between cameras and LiDAR, multi-modal fusion methods have
attracted attention. To address the information waste in existing multi-modal
fusion works, this paper introduces a novel three-channel place descriptor,
which consists of a cascade of image, point cloud, and fusion branches.
Specifically, the fusion-based branch employs a dual-stage pipeline, leveraging
the correlation between the two modalities with latent contacts, thereby
facilitating information interaction and fusion. Extensive experiments on the
KITTI, NCLT, USVInland, and the campus dataset demonstrate that the proposed
place descriptor stands as the state-of-the-art approach, confirming its
robustness and generality in challenging scenarios.
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10372" title="Abstract">arXiv:2310.10372</a> [<a href="/pdf/2310.10372" title="Download PDF">pdf</a>, <a href="/format/2310.10372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Looping LOCI: Developing Object Permanence from Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Traub%2C+M">Manuel Traub</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+F">Frederic Becker</a>, 
<a href="/search/cs?searchtype=author&query=Otte%2C+S">Sebastian Otte</a>, 
<a href="/search/cs?searchtype=author&query=Butz%2C+M+V">Martin V. Butz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent compositional scene representation learning models have become
remarkably good in segmenting and tracking distinct objects within visual
scenes. Yet, many of these models require that objects are continuously, at
least partially, visible. Moreover, they tend to fail on intuitive physics
tests, which infants learn to solve over the first months of their life. Our
goal is to advance compositional scene representation algorithms with an
embedded algorithm that fosters the progressive learning of intuitive physics,
akin to infant development. As a fundamental component for such an algorithm,
we introduce Loci-Looped, which advances a recently published unsupervised
object location, identification, and tracking neural network architecture
(Loci, Traub et al., ICLR 2023) with an internal processing loop. The loop is
designed to adaptively blend pixel-space information with anticipations
yielding information-fused activities as percepts. Moreover, it is designed to
learn compositional representations of both individual object dynamics and
between-objects interaction dynamics. We show that Loci-Looped learns to track
objects through extended periods of object occlusions, indeed simulating their
hidden trajectories and anticipating their reappearance, without the need for
an explicit history buffer. We even find that Loci-Looped surpasses
state-of-the-art models on the ADEPT and the CLEVRER dataset, when confronted
with object occlusions or temporary sensory data interruptions. This indicates
that Loci-Looped is able to learn the physical concepts of object permanence
and inertia in a fully unsupervised emergent manner. We believe that even
further architectural advancements of the internal loop - also in other
compositional scene representation learning models - can be developed in the
near future.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10374" title="Abstract">arXiv:2310.10374</a> [<a href="/pdf/2310.10374" title="Download PDF">pdf</a>, <a href="/format/2310.10374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Factor Spatio-Temporal Prediction based on Graph Decomposition  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiahao Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+Y">Yu Mou</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+C">Cheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spatio-temporal (ST) prediction is an important and widely used technique in
data mining and analytics, especially for ST data in urban systems such as
transportation data. In practice, the ST data generation is usually influenced
by various latent factors tied to natural phenomena or human socioeconomic
activities, impacting specific spatial areas selectively. However, existing ST
prediction methods usually do not refine the impacts of different factors, but
directly model the entangled impacts of multiple factors. This amplifies the
modeling complexity of ST data and compromises model interpretability. To this
end, we propose a multi-factor ST prediction task that predicts partial ST data
evolution under different factors, and combines them for a final prediction. We
make two contributions to this task: an effective theoretical solution and a
portable instantiation framework. Specifically, we first propose a theoretical
solution called decomposed prediction strategy and prove its effectiveness from
the perspective of information entropy theory. On top of that, we instantiate a
novel model-agnostic framework, named spatio-temporal graph decomposition
learning (STGDL), for multi-factor ST prediction. The framework consists of two
main components: an automatic graph decomposition module that decomposes the
original graph structure inherent in ST data into subgraphs corresponding to
different factors, and a decomposed learning network that learns the partial ST
data on each subgraph separately and integrates them for the final prediction.
We conduct extensive experiments on four real-world ST datasets of two types of
graphs, i.e., grid graph and network graph. Results show that our framework
significantly reduces prediction errors of various ST models by 9.41% on
average (35.36% at most). Furthermore, a case study reveals the
interpretability potential of our framework.
</p>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10375" title="Abstract">arXiv:2310.10375</a> [<a href="/pdf/2310.10375" title="Download PDF">pdf</a>, <a href="/format/2310.10375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miyato%2C+T">Takeru Miyato</a>, 
<a href="/search/cs?searchtype=author&query=Jaeger%2C+B">Bernhard Jaeger</a>, 
<a href="/search/cs?searchtype=author&query=Welling%2C+M">Max Welling</a>, 
<a href="/search/cs?searchtype=author&query=Geiger%2C+A">Andreas Geiger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">As transformers are equivariant to the permutation of input tokens, encoding
the positional information of tokens is necessary for many tasks. However,
since existing positional encoding schemes have been initially designed for NLP
tasks, their suitability for vision tasks, which typically exhibit different
structural properties in their data, is questionable. We argue that existing
positional encoding schemes are suboptimal for 3D vision tasks, as they do not
respect their underlying 3D geometric structure. Based on this hypothesis, we
propose a geometry-aware attention mechanism that encodes the geometric
structure of tokens as relative transformation determined by the geometric
relationship between queries and key-value pairs. By evaluating on multiple
novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view
setting, we show that our attention, called Geometric Transform Attention
(GTA), improves learning efficiency and performance of state-of-the-art
transformer-based NVS models without any additional learned parameters and only
minor computational overhead.
</p>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10376" title="Abstract">arXiv:2310.10376</a> [<a href="/pdf/2310.10376" title="Download PDF">pdf</a>, <a href="/format/2310.10376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Research on Train Shunting Impedance Based on Transmission Line Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dong%2C+Y">Yinchao Dong</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+L">Linhai Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">At present, the shunting process of train to track circuit is usually studied
by taking the shunting resistance of the first wheel set of train as the
equivalent model, which ignores the shunting effect of other wheel sets and
cannot study the fault conditions such as "pool shunting". Especially for the
jointless track circuit (JTC), the compensation capacitors connected in
parallel on the rail line will cause more complex train shunting process. To
solve this problem, based on the transmission line theory, starting from the
shunting resistance of each wheel set and considering the leakage of track bet,
this paper established as six-terminal network model of train shunting
impedance, indirectly verified the correctness of the model by using the
working principle of track circuit reader (TCR), and focused on the influence
of wheel set shunting resistance, compensation capacitors and rail line
parameters on train shunting impedance. Finally, based on the calculation of
the structural importance of train wheel set, a simplified model of train
shunting impedance was constructed, providing theoretical support for the
further study of fault conditions such as "poor shunting".
</p>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10378" title="Abstract">arXiv:2310.10378</a> [<a href="/pdf/2310.10378" title="Download PDF">pdf</a>, <a href="/format/2310.10378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Lingual Consistency of Factual Knowledge in Multilingual Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Jirui Qi</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+R">Raquel Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Bisazza%2C+A">Arianna Bisazza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP2023 main conference. All code and data are released at <a href="https://github.com/Betswish/Cross-Lingual-Consistency">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Multilingual large-scale Pretrained Language Models (PLMs) have been shown to
store considerable amounts of factual knowledge, but large variations are
observed across languages. With the ultimate goal of ensuring that users with
different language backgrounds obtain consistent feedback from the same model,
we study the cross-lingual consistency (CLC) of factual knowledge in various
multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)
metric to evaluate knowledge consistency across languages independently from
accuracy. Using this metric, we conduct an in-depth analysis of the determining
factors for CLC, both at model level and at language-pair level. Among other
results, we find that increasing model size leads to higher factual probing
accuracy in most languages, but does not improve cross-lingual consistency.
Finally, we conduct a case study on CLC when new factual associations are
inserted in the PLMs via model editing. Results on a small sample of facts
inserted in English reveal a clear pattern whereby the new piece of knowledge
transfers only to languages with which English has a high RankC score.
</p>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10379" title="Abstract">arXiv:2310.10379</a> [<a href="/pdf/2310.10379" title="Download PDF">pdf</a>, <a href="/format/2310.10379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for  Few-Shot Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ke%2C+T">Tianjun Ke</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Haoqun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zenan Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Feng Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Meta-learning has demonstrated promising results in few-shot classification
(FSC) by learning to solve new problems using prior knowledge. Bayesian methods
are effective at characterizing uncertainty in FSC, which is crucial in
high-risk fields. In this context, the logistic-softmax likelihood is often
employed as an alternative to the softmax likelihood in multi-class Gaussian
process classification due to its conditional conjugacy property. However, the
theoretical property of logistic-softmax is not clear and previous research
indicated that the inherent uncertainty of logistic-softmax leads to suboptimal
performance. To mitigate these issues, we revisit and redesign the
logistic-softmax likelihood, which enables control of the \textit{a priori}
confidence level through a temperature parameter. Furthermore, we theoretically
and empirically show that softmax can be viewed as a special case of
logistic-softmax and logistic-softmax induces a larger family of data
distribution than softmax. Utilizing modified logistic-softmax, we integrate
the data augmentation technique into the deep kernel based Gaussian process
meta-learning framework, and derive an analytical mean-field approximation for
task-specific updates. Our approach yields well-calibrated uncertainty
estimates and achieves comparable or superior results on standard benchmark
datasets. Code is publicly available at
\url{https://github.com/keanson/revisit-logistic-softmax}.
</p>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10380" title="Abstract">arXiv:2310.10380</a> [<a href="/pdf/2310.10380" title="Download PDF">pdf</a>, <a href="/ps/2310.10380" title="Download PostScript">ps</a>, <a href="/format/2310.10380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Data Augmentation for Task-Oriented Dialog Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Axman%2C+D">Dustin Axman</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+A">Avik Ray</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Shubham Garg</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ECML-PKDD 2023 Workshop on Challenges and Opportunities of Large Language Models in Real-World Machine Learning Applications (COLLM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Collection of annotated dialogs for training task-oriented dialog systems
have been one of the key bottlenecks in improving current models. While dialog
response generation has been widely studied on the agent side, it is not
evident if similar generative models can be used to generate a large variety
of, and often unexpected, user inputs that real dialog systems encounter in
practice. Existing data augmentation techniques such as paraphrase generation
do not take the dialog context into consideration. In this paper, we develop a
novel dialog augmentation model that generates a user turn, conditioning on
full dialog context. Additionally, with a new prompt design for language model,
and output re-ranking, the dialogs generated from our model can be directly
used to train downstream dialog systems. On common benchmark datasets MultiWoZ
and SGD, we show that our dialog augmentation model generates high quality
dialogs and improves dialog success rate by as much as $8\%$ over baseline.
</p>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10383" title="Abstract">arXiv:2310.10383</a> [<a href="/pdf/2310.10383" title="Download PDF">pdf</a>, <a href="/format/2310.10383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy in Large Language Models: Attacks, Defenses and Future  Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yulin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jinglong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yan Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaojin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+C">Chunkit Chan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The advancement of large language models (LLMs) has significantly enhanced
the ability to effectively tackle various downstream NLP tasks and unify these
tasks into generative pipelines. On the one hand, powerful language models,
trained on massive textual data, have brought unparalleled accessibility and
usability for both models and users. On the other hand, unrestricted access to
these models can also introduce potential malicious and unintentional privacy
risks. Despite ongoing efforts to address the safety and privacy concerns
associated with LLMs, the problem remains unresolved. In this paper, we provide
a comprehensive analysis of the current privacy attacks targeting LLMs and
categorize them according to the adversary's assumed capabilities to shed light
on the potential vulnerabilities present in LLMs. Then, we present a detailed
overview of prominent defense strategies that have been developed to counter
these privacy attacks. Beyond existing works, we identify upcoming privacy
concerns as LLMs evolve. Lastly, we point out several potential avenues for
future exploration.
</p>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10385" title="Abstract">arXiv:2310.10385</a> [<a href="/pdf/2310.10385" title="Download PDF">pdf</a>, <a href="/format/2310.10385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Better Understanding of Variations in Zero-Shot Neural Machine  Translation Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+S">Shaomu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by the EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing
but often suffers from poor zero-shot (ZS) translation qualities. While prior
work has explored the causes of overall low ZS performance, our work introduces
a fresh perspective: the presence of high variations in ZS performance. This
suggests that MNMT does not uniformly exhibit poor ZS capability; instead,
certain translation directions yield reasonable results. Through systematic
experimentation involving 1,560 language directions spanning 40 languages, we
identify three key factors contributing to high variations in ZS NMT
performance: 1) target side translation capability 2) vocabulary overlap 3)
linguistic properties. Our findings highlight that the target side translation
quality is the most influential factor, with vocabulary overlap consistently
impacting ZS performance. Additionally, linguistic properties, such as language
family and writing system, play a role, particularly with smaller models.
Furthermore, we suggest that the off-target issue is a symptom of inadequate ZS
performance, emphasizing that zero-shot translation challenges extend beyond
addressing the off-target problem. We release the data and models serving as a
benchmark to study zero-shot for future research at
https://github.com/Smu-Tan/ZS-NMT-Variations
</p>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10387" title="Abstract">arXiv:2310.10387</a> [<a href="/pdf/2310.10387" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Edge-Perceptual Guided Image Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Due to the powerful edge-preserving ability and low computational complexity,
Guided image filter (GIF) and its improved versions has been widely applied in
computer vision and image processing. However, all of them are suffered halo
artifacts to some degree, as the regularization parameter increase. In the case
of inconsistent structure of guidance image and input image, edge-preserving
ability degradation will also happen. In this paper, a novel guided image
filter is proposed by integrating an explicit first-order edge-protect
constraint and an explicit residual constraint which will improve the
edge-preserving ability in both cases. To illustrate the efficiency of the
proposed filter, the performances are shown in some typical applications, which
are single image detail enhancement, multi-scale exposure fusion, hyper
spectral images classification. Both theoretical analysis and experimental
results prove that the powerful edge-preserving ability of the proposed filter.
</p>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10391" title="Abstract">arXiv:2310.10391</a> [<a href="/pdf/2310.10391" title="Download PDF">pdf</a>, <a href="/format/2310.10391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Open World Active Learning for 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuoxiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yadan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zijian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zi Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2301.09249">arXiv:2301.09249</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Significant strides have been made in closed world 3D object detection,
testing systems in environments with known classes. However, the challenge
arises in open world scenarios where new object classes appear. Existing
efforts sequentially learn novel classes from streams of labeled data at a
significant annotation cost, impeding efficient deployment to the wild. To seek
effective solutions, we investigate a more practical yet challenging research
task: Open World Active Learning for 3D Object Detection (OWAL-3D), aiming at
selecting a small number of 3D boxes to annotate while maximizing detection
performance on both known and unknown classes. The core difficulty centers on
striking a balance between mining more unknown instances and minimizing the
labeling expenses of point clouds. Empirically, our study finds the harmonious
and inverse relationship between box quantities and their confidences can help
alleviate the dilemma, avoiding the repeated selection of common known
instances and focusing on uncertain objects that are potentially unknown. We
unify both relational constraints into a simple and effective AL strategy
namely OpenCRB, which guides to acquisition of informative point clouds with
the least amount of boxes to label. Furthermore, we develop a comprehensive
codebase for easy reproducing and future research, supporting 15 baseline
methods (i.e., active learning, out-of-distribution detection and open world
detection), 2 types of modern 3D detectors (i.e., one-stage SECOND and
two-stage PV-RCNN) and 3 benchmark 3D datasets (i.e., KITTI, nuScenes and
Waymo). Extensive experiments evidence that the proposed Open-CRB demonstrates
superiority and flexibility in recognizing both novel and shared categories
with very limited labeling costs, compared to state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10392" title="Abstract">arXiv:2310.10392</a> [<a href="/pdf/2310.10392" title="Download PDF">pdf</a>, <a href="/ps/2310.10392" title="Download PostScript">ps</a>, <a href="/format/2310.10392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Differential Graphical Game for Control of Double-Integrator  Multi-Agent Systems with Input Delay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jond%2C+H+B">Hossein B. Jond</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal submission currently under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper studies cooperative control of noncooperative double-integrator
multi-agent systems (MASs) with input delay on connected directed graphs in the
context of a differential graphical game (DGG). In the distributed DGG, each
agent seeks a distributed information control policy by optimizing an
individual local performance index (PI) of distributed information from its
graph neighbors. The local PI, which quadratically penalizes the agent's
deviations from cooperative behavior (e.g., the consensus here), is constructed
through the use of the graph Laplacian matrix. For DGGs for double-integrator
MASs, the existing body of literature lacks the explicit characterization of
Nash equilibrium actions and their associated state trajectories with
distributed information. To address this issue, we first convert the N-player
DGG with m communication links into m coupled optimal control problems (OCPs),
which, in turn, convert to the two-point boundary-value problem (TPBVP). We
derive the explicit solutions for the TPBV that constitute the explicit
distributed information expressions for Nash equilibrium actions and the state
trajectories associated with them for the DGG. An illustrative example verifies
the explicit solutions of local information to achieve fully distributed
consensus.
</p>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10395" title="Abstract">arXiv:2310.10395</a> [<a href="/pdf/2310.10395" title="Download PDF">pdf</a>, <a href="/format/2310.10395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Invitation to the Euler Characteristic Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Munch%2C+E">Elizabeth Munch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">The Euler characteristic transform (ECT) is a simple to define yet powerful
representation of shape. The idea is to encode an embedded shape using
sub-level sets of a a function defined based on a given direction, and then
returning the Euler characteristics of these sublevel sets. Because the ECT has
been shown to be injective on the space of embedded simplicial complexes, it
has been used for applications spanning a range of disciplines, including plant
morphology and protein structural analysis. In this survey article, we present
a comprehensive overview of the Euler characteristic transform, highlighting
the main idea on a simple leaf example, and surveying its its key concepts,
theoretical foundations, and available applications.
</p>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10396" title="Abstract">arXiv:2310.10396</a> [<a href="/pdf/2310.10396" title="Download PDF">pdf</a>, <a href="/format/2310.10396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Current imbalance in dissimilar parallel-connected batteries and the  fate of degradation convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Weng%2C+A">Andrew Weng</a>, 
<a href="/search/eess?searchtype=author&query=Movahedi%2C+H">Hamidreza Movahedi</a>, 
<a href="/search/eess?searchtype=author&query=Wong%2C+C">Clement Wong</a>, 
<a href="/search/eess?searchtype=author&query=Siegel%2C+J+B">Jason B. Siegel</a>, 
<a href="/search/eess?searchtype=author&query=Stefanopoulou%2C+A">Anna Stefanopoulou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 14 figures, submitted to the Journal of Dynamic Systems, Measurement and Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper proposes an analytical framework describing how initial capacity
and resistance variability in parallel-connected battery cells may inflict
additional variability or reduce variability while the cells age. We derive
closed-form equations for current and SOC imbalance dynamics within a charge or
discharge cycle. These dynamics are represented by a first-order equivalent
circuit model and validated against experimental data. To demonstrate how
current and SOC imbalance leads to cell degradation, we developed a successive
update scheme in which the inter-cycle imbalance dynamics update the
intra-cycle degradation dynamics, and vice versa. Using this framework, we
demonstrate that current imbalance can cause convergent degradation
trajectories, consistent with previous reports. However, we also demonstrate
that different degradation assumptions, such as those associated with SOC
imbalance, may cause divergent degradation. We finally highlight the role of
different cell chemistries, including different OCV function nonlinearities, on
system behavior, and derive analytical bounds on the SOC imbalance using
Lyapunov analysis.
</p>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10397" title="Abstract">arXiv:2310.10397</a> [<a href="/pdf/2310.10397" title="Download PDF">pdf</a>, <a href="/format/2310.10397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\textit{Swap and Predict}$ -- Predicting the Semantic Changes in Words  across Corpora by Context Swapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aida%2C+T">Taichi Aida</a>, 
<a href="/search/cs?searchtype=author&query=Bollegala%2C+D">Danushka Bollegala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Meanings of words change over time and across domains. Detecting the semantic
changes of words is an important task for various NLP applications that must
make time-sensitive predictions. We consider the problem of predicting whether
a given target word, $w$, changes its meaning between two different text
corpora, $\mathcal{C}_1$ and $\mathcal{C}_2$. For this purpose, we propose
$\textit{Swapping-based Semantic Change Detection}$ (SSCD), an unsupervised
method that randomly swaps contexts between $\mathcal{C}_1$ and $\mathcal{C}_2$
where $w$ occurs. We then look at the distribution of contextualised word
embeddings of $w$, obtained from a pretrained masked language model (MLM),
representing the meaning of $w$ in its occurrence contexts in $\mathcal{C}_1$
and $\mathcal{C}_2$. Intuitively, if the meaning of $w$ does not change between
$\mathcal{C}_1$ and $\mathcal{C}_2$, we would expect the distributions of
contextualised word embeddings of $w$ to remain the same before and after this
random swapping process. Despite its simplicity, we demonstrate that even by
using pretrained MLMs without any fine-tuning, our proposed context swapping
method accurately predicts the semantic changes of words in four languages
(English, German, Swedish, and Latin) and across different time spans (over 50
years and about five years). Moreover, our method achieves significant
performance improvements compared to strong baselines for the English semantic
change prediction task. Source code is available at
https://github.com/a1da4/svp-swap .
</p>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10399" title="Abstract">arXiv:2310.10399</a> [<a href="/pdf/2310.10399" title="Download PDF">pdf</a>, <a href="/format/2310.10399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Fair and Calibrated Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brahmbhatt%2C+A">Anand Brahmbhatt</a>, 
<a href="/search/cs?searchtype=author&query=Rathore%2C+V">Vipul Rathore</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+P">Parag Singla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent literature has seen a significant focus on building machine learning
models with specific properties such as fairness, i.e., being non-biased with
respect to a given set of attributes, calibration i.e., model confidence being
aligned with its predictive accuracy, and explainability, i.e., ability to be
understandable to humans. While there has been work focusing on each of these
aspects individually, researchers have shied away from simultaneously
addressing more than one of these dimensions. In this work, we address the
problem of building models which are both fair and calibrated. We work with a
specific definition of fairness, which closely matches [Biswas et. al. 2019],
and has the nice property that Bayes optimal classifier has the maximum
possible fairness under our definition. We show that an existing negative
result towards achieving a fair and calibrated model [Kleinberg et. al. 2017]
does not hold for our definition of fairness. Further, we show that ensuring
group-wise calibration with respect to the sensitive attributes automatically
results in a fair model under our definition. Using this result, we provide a
first cut approach for achieving fair and calibrated models, via a simple
post-processing technique based on temperature scaling. We then propose
modifications of existing calibration losses to perform group-wise calibration,
as a way of achieving fair and calibrated models in a variety of settings.
Finally, we perform extensive experimentation of these techniques on a diverse
benchmark of datasets, and present insights on the pareto-optimality of the
resulting solutions.
</p>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10400" title="Abstract">arXiv:2310.10400</a> [<a href="/pdf/2310.10400" title="Download PDF">pdf</a>, <a href="/format/2310.10400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Word Sense Distribution Detect Semantic Changes of Words?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaohang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Aida%2C+T">Taichi Aida</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+P">Procheta Sen</a>, 
<a href="/search/cs?searchtype=author&query=Bollegala%2C+D">Danushka Bollegala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Semantic Change Detection (SCD) of words is an important task for various NLP
applications that must make time-sensitive predictions. Some words are used
over time in novel ways to express new meanings, and these new meanings
establish themselves as novel senses of existing words. On the other hand, Word
Sense Disambiguation (WSD) methods associate ambiguous words with sense ids,
depending on the context in which they occur. Given this relationship between
WSD and SCD, we explore the possibility of predicting whether a target word has
its meaning changed between two corpora collected at different time steps, by
comparing the distributions of senses of that word in each corpora. For this
purpose, we use pretrained static sense embeddings to automatically annotate
each occurrence of the target word in a corpus with a sense id. Next, we
compute the distribution of sense ids of a target word in a given corpus.
Finally, we use different divergence or distance measures to quantify the
semantic change of the target word across the two given corpora. Our
experimental results on SemEval 2020 Task 1 dataset show that word sense
distributions can be accurately used to predict semantic changes of words in
English, German, Swedish and Latin.
</p>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10402" title="Abstract">arXiv:2310.10402</a> [<a href="/pdf/2310.10402" title="Download PDF">pdf</a>, <a href="/format/2310.10402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Fake: Effective Training Data Synthesis Through Distribution  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianhao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shuyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bo Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code released at (<a href="https://github.com/BAAI-DCAI/Training-Data-Synthesis">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Synthetic training data has gained prominence in numerous learning tasks and
scenarios, offering advantages such as dataset augmentation, generalization
evaluation, and privacy preservation. Despite these benefits, the efficiency of
synthetic data generated by current methodologies remains inferior when
training advanced deep models exclusively, limiting its practical utility. To
address this challenge, we analyze the principles underlying training data
synthesis for supervised learning and elucidate a principled theoretical
framework from the distribution-matching perspective that explicates the
mechanisms governing synthesis efficacy. Through extensive experiments, we
demonstrate the effectiveness of our synthetic data across diverse image
classification tasks, both as a replacement for and augmentation to real
datasets, while also benefits challenging tasks such as out-of-distribution
generalization and privacy preservation.
</p>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10404" title="Abstract">arXiv:2310.10404</a> [<a href="/pdf/2310.10404" title="Download PDF">pdf</a>, <a href="/format/2310.10404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Fine-grained Scene Graph Generation via Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kibum Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+K">Kanghoon Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+J">Jaeyeong Jeon</a>, 
<a href="/search/cs?searchtype=author&query=In%2C+Y">Yeonjun In</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+J">Jinyoung Moon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Donghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Weakly-Supervised Scene Graph Generation (WSSGG) research has recently
emerged as an alternative to the fully-supervised approach that heavily relies
on costly annotations. In this regard, studies on WSSGG have utilized image
captions to obtain unlocalized triplets while primarily focusing on grounding
the unlocalized triplets over image regions. However, they have overlooked the
two issues involved in the triplet formation process from the captions: 1)
Semantic over-simplification issue arises when extracting triplets from
captions, where fine-grained predicates in captions are undesirably converted
into coarse-grained predicates, resulting in a long-tailed predicate
distribution, and 2) Low-density scene graph issue arises when aligning the
triplets in the caption with entity/predicate classes of interest, where many
triplets are discarded and not used in training, leading to insufficient
supervision. To tackle the two issues, we propose a new approach, i.e., Large
Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two
issues by leveraging the LLM's in-depth understanding of language and reasoning
ability during the extraction of triplets from captions and alignment of
entity/predicate classes with target data. To further engage the LLM in these
processes, we adopt the idea of Chain-of-Thought and the in-context few-shot
learning strategy. To validate the effectiveness of LLM4SGG, we conduct
extensive experiments on Visual Genome and GQA datasets, showing significant
improvements in both Recall@K and mean Recall@K compared to the
state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is
data-efficient, enabling effective model training with a small amount of
training images.
</p>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10406" title="Abstract">arXiv:2310.10406</a> [<a href="/pdf/2310.10406" title="Download PDF">pdf</a>, <a href="/ps/2310.10406" title="Download PostScript">ps</a>, <a href="/format/2310.10406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quadrature-Free Polytopic Discontinuous Galerkin Methods for Transport  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Radley%2C+T+J">Thomas J. Radley</a>, 
<a href="/search/math?searchtype=author&query=Houston%2C+P">Paul Houston</a>, 
<a href="/search/math?searchtype=author&query=Hubbard%2C+M+E">Matthew E. Hubbard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this article we consider the application of Euler's homogeneous function
theorem together with Stokes' theorem to exactly integrate families of
polynomial spaces over general polygonal and polyhedral (polytopic) domains in
two- and three-dimensions, respectively. This approach allows for the integrals
to be evaluated based on only computing the values of the integrand and its
derivatives at the vertices of the polytopic domain, without the need to
construct a sub-tessellation of the underlying domain of interest. Here, we
present a detailed analysis of the computational complexity of the proposed
algorithm and show that this depends on three key factors: the ambient
dimension of the underlying polytopic domain; the size of the requested
polynomial space to be integrated; and the size of a directed graph related to
the polytopic domain. This general approach is then employed to compute the
volume integrals arising within the discontinuous Galerkin finite element
approximation of the linear transport equation. Numerical experiments are
presented which highlight the efficiency of the proposed algorithm when
compared to standard quadrature approaches defined on a sub-tessellation of the
polytopic elements.
</p>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10410" title="Abstract">arXiv:2310.10410</a> [<a href="/pdf/2310.10410" title="Download PDF">pdf</a>, <a href="/format/2310.10410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loci-Segmented: Improving Scene Segmentation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Traub%2C+M">Manuel Traub</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+F">Frederic Becker</a>, 
<a href="/search/cs?searchtype=author&query=Sauter%2C+A">Adrian Sauter</a>, 
<a href="/search/cs?searchtype=author&query=Otte%2C+S">Sebastian Otte</a>, 
<a href="/search/cs?searchtype=author&query=Butz%2C+M+V">Martin V. Butz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Slot-oriented processing approaches for compositional scene representation
have recently undergone a tremendous development. We present Loci-Segmented
(Loci-s), an advanced scene segmentation neural network that extends the
slot-based location and identity tracking architecture Loci (Traub et al., ICLR
2023). The main advancements are (i) the addition of a pre-trained dynamic
background module; (ii) a hyper-convolution encoder module, which enables
object-focused bottom-up processing; and (iii) a cascaded decoder module, which
successively generates object masks, masked depth maps, and masked,
depth-map-informed RGB reconstructions. The background module features the
learning of both a foreground identifying module and a background re-generator.
We further improve performance via (a) the integration of depth information as
well as improved slot assignments via (b) slot-location-entity regularization
and (b) a prior segmentation network. Even without these latter improvements,
the results reveal superior segmentation performance in the MOVi datasets and
in another established dataset collection. With all improvements, Loci-s
achieves a 32% better intersection over union (IoU) score in MOVi-E than the
previous best. We furthermore show that Loci-s generates well-interpretable
latent representations. We believe that these representations may serve as a
foundation-model-like interpretable basis for solving downstream tasks, such as
grounding language and context- and goal-conditioned event processing.
</p>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10417" title="Abstract">arXiv:2310.10417</a> [<a href="/pdf/2310.10417" title="Download PDF">pdf</a>, <a href="/format/2310.10417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prior-Free Continual Learning with Unlabeled Data in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+T">Tao Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhiyong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Hehe Fan</a>, 
<a href="/search/cs?searchtype=author&query=Kankanhalli%2C+M">Mohan Kankanhalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Continual Learning (CL) aims to incrementally update a trained model on new
tasks without forgetting the acquired knowledge of old ones. Existing CL
methods usually reduce forgetting with task priors, \ie using task identity or
a subset of previously seen samples for model training. However, these methods
would be infeasible when such priors are unknown in real-world applications. To
address this fundamental but seldom-studied problem, we propose a Prior-Free
Continual Learning (PFCL) method, which learns new tasks without knowing the
task identity or any previous data. First, based on a fixed single-head
architecture, we eliminate the need for task identity to select the
task-specific output head. Second, we employ a regularization-based strategy
for consistent predictions between the new and old models, avoiding revisiting
previous samples. However, using this strategy alone often performs poorly in
class-incremental scenarios, particularly for a long sequence of tasks. By
analyzing the effectiveness and limitations of conventional
regularization-based methods, we propose enhancing model consistency with an
auxiliary unlabeled dataset additionally. Moreover, since some auxiliary data
may degrade the performance, we further develop a reliable sample selection
strategy to obtain consistent performance improvement. Extensive experiments on
multiple image classification benchmark datasets show that our PFCL method
significantly mitigates forgetting in all three learning scenarios.
Furthermore, when compared to the most recent rehearsal-based methods that
replay a limited number of previous samples, PFCL achieves competitive
accuracy. Our code is available at: https://github.com/visiontao/pfcl
</p>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10418" title="Abstract">arXiv:2310.10418</a> [<a href="/pdf/2310.10418" title="Download PDF">pdf</a>, <a href="/format/2310.10418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reading Books is Great, But Not if You Are Driving! Visually Grounded  Reasoning about Defeasible Commonsense Norms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Seungju Han</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junhyeok Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+J">Jiwan Chung</a>, 
<a href="/search/cs?searchtype=author&query=Son%2C+Y">Yejin Son</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at EMNLP 2023 (long)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Commonsense norms are defeasible by context: reading books is usually great,
but not when driving a car. While contexts can be explicitly described in
language, in embodied scenarios, contexts are often provided visually. This
type of visually grounded reasoning about defeasible commonsense norms is
generally easy for humans, but (as we show) poses a challenge for machines, as
it necessitates both visual understanding and reasoning about commonsense
norms. We construct a new multimodal benchmark for studying visual-grounded
commonsense norms: NORMLENS. NORMLENS consists of 10K human judgments
accompanied by free-form explanations covering 2K multimodal situations, and
serves as a probe to address two questions: (1) to what extent can models align
with average human judgment? and (2) how well can models explain their
predicted judgments? We find that state-of-the-art model judgments and
explanations are not well-aligned with human annotation. Additionally, we
present a new approach to better align models with humans by distilling social
commonsense knowledge from large language models. The data and code are
released at https://seungjuhan.me/normlens.
</p>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10423" title="Abstract">arXiv:2310.10423</a> [<a href="/pdf/2310.10423" title="Download PDF">pdf</a>, <a href="/format/2310.10423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLOv7 for Mosquito Breeding Grounds Detection and Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laranjeira%2C+C">Camila Laranjeira</a>, 
<a href="/search/cs?searchtype=author&query=Andrade%2C+D">Daniel Andrade</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+J+A+d">Jefersson A. dos Santos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Winning paper of ICIP 2023 Grand Challenge - Automatic Detection of Mosquito Breeding Grounds - <a href="https://www02.smt.ufrj.br/~tvdigital/mosquito/challenge/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">With the looming threat of climate change, neglected tropical diseases such
as dengue, zika, and chikungunya have the potential to become an even greater
global concern. Remote sensing technologies can aid in controlling the spread
of Aedes Aegypti, the transmission vector of such diseases, by automating the
detection and mapping of mosquito breeding sites, such that local entities can
properly intervene. In this work, we leverage YOLOv7, a state-of-the-art and
computationally efficient detection approach, to localize and track mosquito
foci in videos captured by unmanned aerial vehicles. We experiment on a dataset
released to the public as part of the ICIP 2023 grand challenge entitled
Automatic Detection of Mosquito Breeding Grounds. We show that YOLOv7 can be
directly applied to detect larger foci categories such as pools, tires, and
water tanks and that a cheap and straightforward aggregation of frame-by-frame
detection can incorporate time consistency into the tracking process.
</p>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10424" title="Abstract">arXiv:2310.10424</a> [<a href="/pdf/2310.10424" title="Download PDF">pdf</a>, <a href="/format/2310.10424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Benchmarking Paradigm and a Scale- and Motion-Aware Model for  Egocentric Pedestrian Trajectory Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasouli%2C+A">Amir Rasouli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Figures, 16 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Predicting pedestrian behavior is one of the main challenges for intelligent
driving systems. In this paper, we present a new paradigm for evaluating
egocentric pedestrian trajectory prediction algorithms. Based on various
contextual information, we extract driving scenarios for a meaningful and
systematic approach to identifying challenges for prediction models. In this
regard, we also propose a new metric for more effective ranking within the
scenario-based evaluation. We conduct extensive empirical studies of existing
models on these scenarios to expose shortcomings and strengths of different
approaches. The scenario-based analysis highlights the importance of using
multimodal sources of information and challenges caused by inadequate modeling
of ego-motion and scale of pedestrians. To this end, we propose a novel
egocentric trajectory prediction model that benefits from multimodal sources of
data fused in an effective and efficient step-wise hierarchical fashion and two
auxiliary tasks designed to learn more robust representation of scene dynamics.
We show that our approach achieves significant improvement by up to 40% in
challenging scenarios compared to the past arts via empirical evaluation on
common benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10425" title="Abstract">arXiv:2310.10425</a> [<a href="/pdf/2310.10425" title="Download PDF">pdf</a>, <a href="/format/2310.10425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuously Adapting Random Sampling (CARS) for Power Electronics  Parameter Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Happel%2C+D">Dominik Happel</a>, 
<a href="/search/cs?searchtype=author&query=Brendel%2C+P">Philipp Brendel</a>, 
<a href="/search/cs?searchtype=author&query=Rosskopf%2C+A">Andreas Rosskopf</a>, 
<a href="/search/cs?searchtype=author&query=Ditze%2C+S">Stefan Ditze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">To date, power electronics parameter design tasks are usually tackled using
detailed optimization approaches with detailed simulations or using brute force
grid search grid search with very fast simulations. A new method, named
"Continuously Adapting Random Sampling" (CARS) is proposed, which provides a
continuous method in between. This allows for very fast, and / or large amounts
of simulations, but increasingly focuses on the most promising parameter
ranges. Inspirations are drawn from multi-armed bandit research and lead to
prioritized sampling of sub-domains in one high-dimensional parameter tensor.
Performance has been evaluated on three exemplary power electronic use-cases,
where resulting designs appear competitive to genetic algorithms, but
additionally allow for highly parallelizable simulation, as well as continuous
progression between explorative and exploitative settings.
</p>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10427" title="Abstract">arXiv:2310.10427</a> [<a href="/pdf/2310.10427" title="Download PDF">pdf</a>, <a href="/format/2310.10427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DANAA: Towards transferable attacks with double adversarial neuron  attribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhibo Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhiyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaming Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While deep neural networks have excellent results in many fields, they are
susceptible to interference from attacking samples resulting in erroneous
judgments. Feature-level attacks are one of the effective attack types, which
targets the learnt features in the hidden layers to improve its transferability
across different models. Yet it is observed that the transferability has been
largely impacted by the neuron importance estimation results. In this paper, a
double adversarial neuron attribution attack method, termed `DANAA', is
proposed to obtain more accurate feature importance estimation. In our method,
the model outputs are attributed to the middle layer based on an adversarial
non-linear path. The goal is to measure the weight of individual neurons and
retain the features that are more important towards transferability. We have
conducted extensive experiments on the benchmark datasets to demonstrate the
state-of-the-art performance of our method. Our code is available at:
https://github.com/Davidjinzb/DANAA
</p>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10429" title="Abstract">arXiv:2310.10429</a> [<a href="/pdf/2310.10429" title="Download PDF">pdf</a>, <a href="/format/2310.10429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting User Comments for Early Detection of Fake News Prior to  Users&#x27; Commenting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nan%2C+Q">Qiong Nan</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Q">Qiang Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Juan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yongchun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Danding Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jintao Li</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+K">Kai Shu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Both accuracy and timeliness are key factors in detecting fake news on social
media. However, most existing methods encounter an accuracy-timeliness dilemma:
Content-only methods guarantee timeliness but perform moderately because of
limited available information, while social context-based ones generally
perform better but inevitably lead to latency because of social context
accumulation needs. To break such a dilemma, a feasible but not well-studied
solution is to leverage social contexts (e.g., comments) from historical news
for training a detection model and apply it to newly emerging news without
social contexts. This requires the model to (1) sufficiently learn helpful
knowledge from social contexts, and (2) be well compatible with situations that
social contexts are available or not. To achieve this goal, we propose to
absorb and parameterize useful knowledge from comments in historical news and
then inject it into a content-only detection model. Specifically, we design the
Comments Assisted Fake News Detection method (CAS-FEND), which transfers useful
knowledge from a comments-aware teacher model to a content-only student model
during training. The student model is further used to detect newly emerging
fake news. Experiments show that the CAS-FEND student model outperforms all
content-only methods and even those with 1/4 comments as inputs, demonstrating
its superiority for early detection.
</p>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10430" title="Abstract">arXiv:2310.10430</a> [<a href="/pdf/2310.10430" title="Download PDF">pdf</a>, <a href="/format/2310.10430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Parallel-in-time Method Based on Preconditioner for Biot&#x27;s Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhou%2C+Z">Zeyuan Zhou</a>, 
<a href="/search/math?searchtype=author&query=Gu%2C+H">Huipeng Gu</a>, 
<a href="/search/math?searchtype=author&query=Ju%2C+G">Guoliang Ju</a>, 
<a href="/search/math?searchtype=author&query=Xing%2C+W">Wei Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We proposed a parallel-in-time method based on preconditioner for Biot's
consolidation model in poroelasticity. In order to achieve a fast and stable
convergence for the matrix system of the Biot's model, we design two
preconditioners with approximations of the Schur complement. The
parallel-in-time method employs an inverted time-stepping scheme that iterates
to solve the preconditioned linear system in the outer loop and advances the
time step in the inner loop. This allows us to parallelize the iterations with
a theoretical parallel efficiency that approaches 1 as the number of time steps
and spatial steps grows. We demonstrate the stability, accuracy, and linear
speedup of our method on HPC platform through numerical experiments.
</p>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10431" title="Abstract">arXiv:2310.10431</a> [<a href="/pdf/2310.10431" title="Download PDF">pdf</a>, <a href="/format/2310.10431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Longitudinal Self-supervised Learning Using Neural Ordinary Differential  Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeghlache%2C+R">Rachid Zeghlache</a>, 
<a href="/search/cs?searchtype=author&query=Conze%2C+P">Pierre-Henri Conze</a>, 
<a href="/search/cs?searchtype=author&query=Daho%2C+M+E+H">Mostafa El Habib Daho</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Boit%C3%A9%2C+H+L">Hugo Le Boit&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Tadayoni%2C+R">Ramin Tadayoni</a>, 
<a href="/search/cs?searchtype=author&query=Massin%2C+P">Pascal Massin</a>, 
<a href="/search/cs?searchtype=author&query=Cochener%2C+B">B&#xe9;atrice Cochener</a>, 
<a href="/search/cs?searchtype=author&query=Brahim%2C+I">Ikram Brahim</a>, 
<a href="/search/cs?searchtype=author&query=Quellec%2C+G">Gwenol&#xe9; Quellec</a>, 
<a href="/search/cs?searchtype=author&query=Lamard%2C+M">Mathieu Lamard</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Predictive Intelligence in Medicine. PRIME 2023. Part of the
  Lecture Notes in Computer Science book series (LNCS,volume 14277)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Longitudinal analysis in medical imaging is crucial to investigate the
progressive changes in anatomical structures or disease progression over time.
In recent years, a novel class of algorithms has emerged with the goal of
learning disease progression in a self-supervised manner, using either pairs of
consecutive images or time series of images. By capturing temporal patterns
without external labels or supervision, longitudinal self-supervised learning
(LSSL) has become a promising avenue. To better understand this core method, we
explore in this paper the LSSL algorithm under different scenarios. The
original LSSL is embedded in an auto-encoder (AE) structure. However,
conventional self-supervised strategies are usually implemented in a
Siamese-like manner. Therefore, (as a first novelty) in this study, we explore
the use of Siamese-like LSSL. Another new core framework named neural ordinary
differential equation (NODE). NODE is a neural network architecture that learns
the dynamics of ordinary differential equations (ODE) through the use of neural
networks. Many temporal systems can be described by ODE, including modeling
disease progression. We believe that there is an interesting connection to make
between LSSL and NODE. This paper aims at providing a better understanding of
those core algorithms for learning the disease progression with the mentioned
change. In our different experiments, we employ a longitudinal dataset, named
OPHDIAT, targeting diabetic retinopathy (DR) follow-up. Our results demonstrate
the application of LSSL without including a reconstruction term, as well as the
potential of incorporating NODE in conjunction with LSSL.
</p>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10433" title="Abstract">arXiv:2310.10433</a> [<a href="/pdf/2310.10433" title="Download PDF">pdf</a>, <a href="/format/2310.10433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object Detection in Aerial Images in Scarce Data Regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeune%2C+P+L">Pierre Le Jeune</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD Thesis. Work conducted at L2TI (USPN) and COSE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Most contributions on Few-Shot Object Detection (FSOD) evaluate their methods
on natural images only, yet the transferability of the announced performance is
not guaranteed for applications on other kinds of images. We demonstrate this
with an in-depth analysis of existing FSOD methods on aerial images and
observed a large performance gap compared to natural images. Small objects,
more numerous in aerial images, are the cause for the apparent performance gap
between natural and aerial images. As a consequence, we improve FSOD
performance on small objects with a carefully designed attention mechanism. In
addition, we also propose a scale-adaptive box similarity criterion, that
improves the training and evaluation of FSOD methods, particularly for small
objects. We also contribute to generic FSOD with two distinct approaches based
on metric learning and fine-tuning. Impressive results are achieved with the
fine-tuning method, which encourages tackling more complex scenarios such as
Cross-Domain FSOD. We conduct preliminary experiments in this direction and
obtain promising results. Finally, we address the deployment of the detection
models inside COSE's systems. Detection must be done in real-time in extremely
large images (more than 100 megapixels), with limited computation power.
Leveraging existing optimization tools such as TensorRT, we successfully tackle
this engineering challenge.
</p>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10436" title="Abstract">arXiv:2310.10436</a> [<a href="/pdf/2310.10436" title="Download PDF">pdf</a>, <a href="/format/2310.10436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model-Empowered Agents for Simulating Macroeconomic  Activities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nian Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Q">Qingmin Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The advent of the Web has brought about a paradigm shift in traditional
economics, particularly in the digital economy era, enabling the precise
recording and analysis of individual economic behavior. This has led to a
growing emphasis on data-driven modeling in macroeconomics. In macroeconomic
research, Agent-based modeling (ABM) emerged as an alternative, evolving
through rule-based agents, machine learning-enhanced decision-making, and, more
recently, advanced AI agents. However, the existing works are suffering from
three main challenges when endowing agents with human-like decision-making,
including agent heterogeneity, the influence of macroeconomic trends, and
multifaceted economic factors. Large language models (LLMs) have recently
gained prominence in offering autonomous human-like characteristics. Therefore,
leveraging LLMs in macroeconomic simulation presents an opportunity to overcome
traditional limitations. In this work, we take an early step in introducing a
novel approach that leverages LLMs in macroeconomic simulation. We design
prompt-engineering-driven LLM agents to exhibit human-like decision-making and
adaptability in the economic environment, with the abilities of perception,
reflection, and decision-making to address the abovementioned challenges.
Simulation experiments on macroeconomic activities show that LLM-empowered
agents can make realistic work and consumption decisions and emerge more
reasonable macroeconomic phenomena than existing rule-based or AI agents. Our
work demonstrates the promising potential to simulate macroeconomics based on
LLM and its human-like characteristics.
</p>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10441" title="Abstract">arXiv:2310.10441</a> [<a href="/pdf/2310.10441" title="Download PDF">pdf</a>, <a href="/format/2310.10441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficiently matching random inhomogeneous graphs via degree profiles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jian Ding</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+Y">Yumou Fei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanzheng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Probability (math.PR); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we study the problem of recovering the latent vertex
correspondence between two correlated random graphs with vastly inhomogeneous
and unknown edge probabilities between different pairs of vertices. Inspired by
and extending the matching algorithm via degree profiles by Ding, Ma, Wu and Xu
(2021), we obtain an efficient matching algorithm as long as the minimal
average degree is at least $\Omega(\log^{2} n)$ and the minimal correlation is
at least $1 - O(\log^{-2} n)$.
</p>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10443" title="Abstract">arXiv:2310.10443</a> [<a href="/pdf/2310.10443" title="Download PDF">pdf</a>, <a href="/format/2310.10443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grivas%2C+A">Andreas Grivas</a>, 
<a href="/search/cs?searchtype=author&query=Vergari%2C+A">Antonio Vergari</a>, 
<a href="/search/cs?searchtype=author&query=Lopez%2C+A">Adam Lopez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sigmoid output layers are widely used in multi-label classification (MLC)
tasks, in which multiple labels can be assigned to any input. In many practical
MLC tasks, the number of possible labels is in the thousands, often exceeding
the number of input features and resulting in a low-rank output layer. In
multi-class classification, it is known that such a low-rank output layer is a
bottleneck that can result in unargmaxable classes: classes which cannot be
predicted for any input. In this paper, we show that for MLC tasks, the
analogous sigmoid bottleneck results in exponentially many unargmaxable label
combinations. We explain how to detect these unargmaxable outputs and
demonstrate their presence in three widely used MLC datasets. We then show that
they can be prevented in practice by introducing a Discrete Fourier Transform
(DFT) output layer, which guarantees that all sparse label combinations with up
to $k$ active labels are argmaxable. Our DFT layer trains faster and is more
parameter efficient, matching the F1@k score of a sigmoid layer while using up
to 50% fewer trainable parameters. Our code is publicly available at
https://github.com/andreasgrv/sigmoid-bottleneck.
</p>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10445" title="Abstract">arXiv:2310.10445</a> [<a href="/pdf/2310.10445" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MechGPT, a language-based strategy for mechanics and materials modeling  that connects knowledge across scales, disciplines and modalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buehler%2C+M+J">Markus J. Buehler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">For centuries, researchers have sought out ways to connect disparate areas of
knowledge. While early scholars (Galileo, da Vinci, etc.) were experts across
fields, specialization has taken hold later. With the advent of Artificial
Intelligence, we can now explore relationships across areas (e.g.,
mechanics-biology) or disparate domains (e.g., failure mechanics-art). To
achieve this, we use a fine-tuned Large Language Model (LLM), here for a subset
of knowledge in multiscale materials failure. The approach includes the use of
a general-purpose LLM to distill question-answer pairs from raw sources
followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is used
in a series of computational experiments to explore its capacity for knowledge
retrieval, various language tasks, hypothesis generation, and connecting
knowledge across disparate areas. While the model has some ability to recall
knowledge from training, we find that LLMs are particularly useful to extract
structural insights through Ontological Knowledge Graphs. These interpretable
graph structures provide explanatory insights, frameworks for new research
questions, and visual representations of knowledge that also can be used in
retrieval-augmented generation. Three versions of MechGPT are discussed,
featuring different sizes from 13 billion to 70 billion parameters, and
reaching context lengths of more than 10,000 tokens. This provides ample
capacity for sophisticated retrieval augmented strategies, as well as
agent-based modeling where multiple LLMs interact collaboratively and/or
adversarially, the incorporation of new data from the literature or web
searches, as well as multimodality.
</p>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10449" title="Abstract">arXiv:2310.10449</a> [<a href="/pdf/2310.10449" title="Download PDF">pdf</a>, <a href="/ps/2310.10449" title="Download PostScript">ps</a>, <a href="/format/2310.10449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Summarization Using Large Language Models: A Comparative Study of  MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basyal%2C+L">Lochan Basyal</a>, 
<a href="/search/cs?searchtype=author&query=Sanghvi%2C+M">Mihir Sanghvi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text summarization is a critical Natural Language Processing (NLP) task with
applications ranging from information retrieval to content generation.
Leveraging Large Language Models (LLMs) has shown remarkable promise in
enhancing summarization techniques. This paper embarks on an exploration of
text summarization with a diverse set of LLMs, including MPT-7b-instruct,
falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment
was performed with different hyperparameters and evaluated the generated
summaries using widely accepted metrics such as the Bilingual Evaluation
Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation
(ROUGE) Score, and Bidirectional Encoder Representations from Transformers
(BERT) Score. According to the experiment, text-davinci-003 outperformed the
others. This investigation involved two distinct datasets: CNN Daily Mail and
XSum. Its primary objective was to provide a comprehensive understanding of the
performance of Large Language Models (LLMs) when applied to different datasets.
The assessment of these models' effectiveness contributes valuable insights to
researchers and practitioners within the NLP domain. This work serves as a
resource for those interested in harnessing the potential of LLMs for text
summarization and lays the foundation for the development of advanced
Generative AI applications aimed at addressing a wide spectrum of business
challenges.
</p>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10453" title="Abstract">arXiv:2310.10453</a> [<a href="/pdf/2310.10453" title="Download PDF">pdf</a>, <a href="/format/2310.10453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Relevance of Temporal Features for Medical Ultrasound Video  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+D+H">D. Hudson Smith</a>, 
<a href="/search/cs?searchtype=author&query=Lineberger%2C+J+P">John Paul Lineberger</a>, 
<a href="/search/cs?searchtype=author&query=Baker%2C+G+H">George H. Baker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, published in MICCAI 23
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Medical Image Computing and
  Computer-Assisted Intervention, pp. 744-753. Cham: Springer Nature
  Switzerland, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Many medical ultrasound video recognition tasks involve identifying key
anatomical features regardless of when they appear in the video suggesting that
modeling such tasks may not benefit from temporal features. Correspondingly,
model architectures that exclude temporal features may have better sample
efficiency. We propose a novel multi-head attention architecture that
incorporates these hypotheses as inductive priors to achieve better sample
efficiency on common ultrasound tasks. We compare the performance of our
architecture to an efficient 3D CNN video recognition model in two settings:
one where we expect not to require temporal features and one where we do. In
the former setting, our model outperforms the 3D CNN - especially when we
artificially limit the training data. In the latter, the outcome reverses.
These results suggest that expressive time-independent models may be more
effective than state-of-the-art video recognition models for some common
ultrasound tasks in the low-data regime.
</p>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10457" title="Abstract">arXiv:2310.10457</a> [<a href="/pdf/2310.10457" title="Download PDF">pdf</a>, <a href="/format/2310.10457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flag Sequence Set Design for Low-Complexity Delay-Doppler Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Lingsheng Meng</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y+L">Yong Liang Guan</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zilong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper studies Flag sequences for lowcomplexity delay-Doppler estimation
by exploiting their distinctive peak-curtain ambiguity functions (AFs). Unlike
the existing Flag sequence designs that are limited to prime lengths and
periodic auto-AFs, we aim to design Flag sequence sets of arbitrary lengths and
with low (nontrivial) periodic/aperiodic auto- and cross-AFs. Since every Flag
sequence consists of a Curtain sequence and a Peak sequence, we first
investigate the algebraic design of zone-based Curtain sequence sets of
arbitrary lengths. Our proposed design gives rise to novel Curtain sequence
sets with ideal curtain auto-AFs and low/zero cross-AFs within the
delay-Doppler zone of interest. Leveraging these Curtain sequence sets, two
optimization problems are formulated to minimize the summed customized weighted
integrated sidelobe level (SCWISL) of the Flag sequence set. Accelerated
Parallel Partially Majorization-Minimization Algorithms are proposed to jointly
optimize the transmit Flag sequences and matched/mismatched reference sequences
stored in the receiver. Simulations demonstrate that our proposed Flag
sequences lead to improved SCWISL and customized peak-to-max-sidelobe ratio
compared with the existing Flag sequences. Additionally, our Flag sequences
under Flag method exhibit Mean Squared Errors that approach the Cramer-Rao
Lower Bound and the Sampling Bound at high signal-to-noise power ratios.
</p>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10460" title="Abstract">arXiv:2310.10460</a> [<a href="/pdf/2310.10460" title="Download PDF">pdf</a>, <a href="/format/2310.10460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental Validation of Memristor-Aided Logic Using 1T1R TaOx RRAM  Crossbar Array
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bende%2C+A">Ankit Bende</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Simranjeet Singh</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+C+K">Chandan Kumar Jha</a>, 
<a href="/search/cs?searchtype=author&query=Kempen%2C+T">Tim Kempen</a>, 
<a href="/search/cs?searchtype=author&query=C%C3%BCppers%2C+F">Felix C&#xfc;ppers</a>, 
<a href="/search/cs?searchtype=author&query=Bengel%2C+C">Christopher Bengel</a>, 
<a href="/search/cs?searchtype=author&query=Zambanini%2C+A">Andre Zambanini</a>, 
<a href="/search/cs?searchtype=author&query=Nielinger%2C+D">Dennis Nielinger</a>, 
<a href="/search/cs?searchtype=author&query=Patkar%2C+S">Sachin Patkar</a>, 
<a href="/search/cs?searchtype=author&query=Drechsler%2C+R">Rolf Drechsler</a>, 
<a href="/search/cs?searchtype=author&query=Waser%2C+R">Rainer Waser</a>, 
<a href="/search/cs?searchtype=author&query=Merchant%2C+F">Farhad Merchant</a>, 
<a href="/search/cs?searchtype=author&query=Rana%2C+V">Vikas Rana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in VLSID 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
<p class="mathjax">Memristor-aided logic (MAGIC) design style holds a high promise for realizing
digital logic-in-memory functionality. The ability to implement a specific gate
in a MAGIC design style hinges on the SET-to-RESET threshold ratio. The TaOx
memristive devices exhibit distinct SET-to-RESET ratios, enabling the
implementation of OR and NOT operations. As the adoption of the MAGIC design
style gains momentum, it becomes crucial to understand the breakdown of energy
consumption in the various phases of its operation. This paper presents
experimental demonstrations of the OR and NOT gates on a 1T1R crossbar array.
Additionally, it provides insights into the energy distribution for performing
these operations at different stages. Through our experiments across different
gates, we found that the energy consumption is dominated by initialization in
the MAGIC design style. The energy split-up is 14.8%, 85%, and 0.2% for
execution, initialization, and read operations respectively.
</p>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10461" title="Abstract">arXiv:2310.10461</a> [<a href="/pdf/2310.10461" title="Download PDF">pdf</a>, <a href="/format/2310.10461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Selection of Anomaly Detectors in the Absence of Labeled  Validation Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fung%2C+C">Clement Fung</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+C">Chen Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Aodong Li</a>, 
<a href="/search/cs?searchtype=author&query=Rudolph%2C+M">Maja Rudolph</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Anomaly detection requires detecting abnormal samples in large unlabeled
datasets. While progress in deep learning and the advent of foundation models
has produced powerful unsupervised anomaly detection methods, their deployment
in practice is often hindered by the lack of labeled data -- without it, the
detection accuracy of an anomaly detector cannot be evaluated reliably. In this
work, we propose a general-purpose framework for evaluating image-based anomaly
detectors with synthetically generated validation data. Our method assumes
access to a small support set of normal images which are processed with a
pre-trained diffusion model (our proposed method requires no training or
fine-tuning) to produce synthetic anomalies. When mixed with normal samples
from the support set, the synthetic anomalies create detection tasks that
compose a validation framework for anomaly detection evaluation and model
selection. In an extensive empirical study, ranging from natural images to
industrial applications, we find that our synthetic validation framework
selects the same models and hyper-parameters as selection with a ground-truth
validation set. In addition, we find that prompts selected by our method for
CLIP-based anomaly detection outperforms all other prompt selection strategies,
and leads to the overall best detection accuracy, even on the challenging
MVTec-AD dataset.
</p>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10462" title="Abstract">arXiv:2310.10462</a> [<a href="/pdf/2310.10462" title="Download PDF">pdf</a>, <a href="/format/2310.10462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Neural Ranking Framework: Toward Maximized Business Goal for  Cascade Ranking Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunli Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+S">Shiyang Wen</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Dongying Kong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Cascade ranking is widely used for large-scale top-k selection problems in
online advertising and recommendation systems, and learning-to-rank is an
important way to optimize the models in cascade ranking systems. Previous works
on learning-to-rank usually focus on letting the model learn the complete order
or pay more attention to the order of top materials, and adopt the
corresponding rank metrics as optimization targets. However, these optimization
targets can not adapt to various cascade ranking scenarios with varying data
complexities and model capabilities; and the existing metric-driven methods
such as the Lambda framework can only optimize a rough upper bound of the
metric, potentially resulting in performance misalignment. To address these
issues, we first propose a novel perspective on optimizing cascade ranking
systems by highlighting the adaptability of optimization targets to data
complexities and model capabilities. Concretely, we employ multi-task learning
framework to adaptively combine the optimization of relaxed and full targets,
which refers to metrics Recall@m@k and OAP respectively. Then we introduce a
permutation matrix to represent the rank metrics and employ differentiable
sorting techniques to obtain a relaxed permutation matrix with controllable
approximate error bound. This enables us to optimize both the relaxed and full
targets directly and more appropriately using the proposed surrogate losses
within the deep learning framework. We named this method as Adaptive Neural
Ranking Framework. We use the NeuralSort method to obtain the relaxed
permutation matrix and draw on the uncertainty weight method in multi-task
learning to optimize the proposed losses jointly. Experiments on a total of 4
public and industrial benchmarks show the effectiveness and generalization of
our method, and online experiment shows that our method has significant
application value.
</p>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10463" title="Abstract">arXiv:2310.10463</a> [<a href="/pdf/2310.10463" title="Download PDF">pdf</a>, <a href="/format/2310.10463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combating Label Noise With A General Surrogate Model For Sample  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Linchao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Humphrey Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Modern deep learning systems are data-hungry. Learning with web data is one
of the feasible solutions, but will introduce label noise inevitably, which can
hinder the performance of deep neural networks. Sample selection is an
effective way to deal with label noise. The key is to separate clean samples
based on some criterion. Previous methods pay more attention to the small loss
criterion where small-loss samples are regarded as clean ones. Nevertheless,
such a strategy relies on the learning dynamics of each data instance. Some
noisy samples are still memorized due to frequently occurring corrupted
learning patterns. To tackle this problem, a training-free surrogate model is
preferred, freeing from the effect of memorization. In this work, we propose to
leverage the vision-language surrogate model CLIP to filter noisy samples
automatically. CLIP brings external knowledge to facilitate the selection of
clean samples with its ability of text-image alignment. Furthermore, a margin
adaptive loss is designed to regularize the selection bias introduced by CLIP,
providing robustness to label noise. We validate the effectiveness of our
proposed method on both real-world and synthetic noisy datasets. Our method
achieves significant improvement without CLIP involved during the inference
stage.
</p>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10467" title="Abstract">arXiv:2310.10467</a> [<a href="/pdf/2310.10467" title="Download PDF">pdf</a>, <a href="/format/2310.10467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stance Detection with Collaborative Role-Infused LLM-Based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xiaochong Lan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Depeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Stance detection automatically detects the stance in a text towards a target,
vital for content analysis in web and social media research. Despite their
promising capabilities, LLMs encounter challenges when directly applied to
stance detection. First, stance detection demands multi-aspect knowledge, from
deciphering event-related terminologies to understanding the expression styles
in social media platforms. Second, stance detection requires advanced reasoning
to infer authors' implicit viewpoints, as stance are often subtly embedded
rather than overtly stated in the text. To address these challenges, we design
a three-stage framework COLA (short for Collaborative rOle-infused LLM-based
Agents) in which LLMs are designated distinct roles, creating a collaborative
system where each role contributes uniquely. Initially, in the multidimensional
text analysis stage, we configure the LLMs to act as a linguistic expert, a
domain specialist, and a social media veteran to get a multifaceted analysis of
texts, thus overcoming the first challenge. Next, in the reasoning-enhanced
debating stage, for each potential stance, we designate a specific LLM-based
agent to advocate for it, guiding the LLM to detect logical connections between
text features and stance, tackling the second challenge. Finally, in the stance
conclusion stage, a final decision maker agent consolidates prior insights to
determine the stance. Our approach avoids extra annotated data and model
training and is highly usable. We achieve state-of-the-art performance across
multiple datasets. Ablation studies validate the effectiveness of each design
role in handling stance detection. Further experiments have demonstrated the
explainability and the versatility of our approach. Our approach excels in
usability, accuracy, effectiveness, explainability and versatility,
highlighting its value.
</p>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10477" title="Abstract">arXiv:2310.10477</a> [<a href="/pdf/2310.10477" title="Download PDF">pdf</a>, <a href="/format/2310.10477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jianhua Han</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenyong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The rapid advancement of large language models (LLMs) presents both
opportunities and challenges, particularly concerning unintentional generation
of harmful and toxic responses. While the traditional alignment methods strive
to steer LLMs towards desired performance and shield them from malicious
content, this study proposes a novel alignment strategy rooted in mistake
analysis by exposing LLMs to flawed outputs purposefully and then conducting a
thorough assessment to fully comprehend internal reasons via natural language
analysis. Thus, toxic responses can be transformed into instruction tuning
corpus for model alignment, and LLMs can not only be deterred from generating
flawed responses but also trained to self-criticize, leveraging its innate
ability to discriminate toxic content. Experimental results demonstrate that
the proposed method outperforms conventional alignment techniques for safety
instruction following, while maintaining superior efficiency.
</p>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10479" title="Abstract">arXiv:2310.10479</a> [<a href="/pdf/2310.10479" title="Download PDF">pdf</a>, <a href="/ps/2310.10479" title="Download PostScript">ps</a>, <a href="/format/2310.10479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher-order finite element de Rham complexes, partially localized flux  reconstructions, and applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Licht%2C+M+W">Martin Werner Licht</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We construct finite element de~Rham complexes of higher and possibly
non-uniform polynomial order in finite element exterior calculus (FEEC).
Starting from the finite element differential complex of lowest-order, known as
the complex of Whitney forms, we incrementally construct the higher-order
complexes by adjoining exact local complexes associated to simplices. We define
a commuting canonical interpolant. On the one hand, this research provides a
base for studying $hp$-adaptive methods in finite element exterior calculus. On
the other hand, our construction of higher-order spaces enables a new tool in
numerical analysis which we call "partially localized flux reconstruction". One
major application of this concept is in the area of equilibrated a~posteriori
error estimators: we generalize the Braess-Sch\"oberl error estimator to edge
elements of higher and possibly non-uniform order.
</p>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10480" title="Abstract">arXiv:2310.10480</a> [<a href="/pdf/2310.10480" title="Download PDF">pdf</a>, <a href="/format/2310.10480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G-SPEED: General SParse Efficient Editing MoDel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoke Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juntao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiabing Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models~(LLMs) have demonstrated incredible capabilities in
understanding, generating, and manipulating languages. Through human-model
interactions, LLMs can automatically understand human-issued instructions and
output the expected contents, which can significantly increase working
efficiency. In various types of real-world demands, editing-oriented tasks
account for a considerable proportion, which involves an interactive process
that entails the continuous refinement of existing texts to meet specific
criteria. Due to the need for multi-round human-model interaction and the
generation of complicated editing tasks, there is an emergent need for
efficient general editing models. In this paper, we propose
\underline{\textbf{G}}eneral \underline{\textbf{SP}}arse
\underline{\textbf{E}}fficient \underline{\textbf{E}}diting
Mo\underline{\textbf{D}}el~(\textbf{G-SPEED}), which can fulfill diverse
editing requirements through a single model while maintaining low computational
costs. Specifically, we first propose a novel unsupervised text editing data
clustering algorithm to deal with the data scarcity problem. Subsequently, we
introduce a sparse editing model architecture to mitigate the inherently
limited learning capabilities of small language models. The experimental
outcomes indicate that G-SPEED, with its 508M parameters, can surpass LLMs
equipped with 175B parameters. Our code and model checkpoints are available at
\url{https://github.com/Banner-Z/G-SPEED}.
</p>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10481" title="Abstract">arXiv:2310.10481</a> [<a href="/pdf/2310.10481" title="Download PDF">pdf</a>, <a href="/format/2310.10481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource  Event Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Gang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xiaocheng Gong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shudong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Si Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Most current Event Extraction (EE) methods focus on the high-resource
scenario, which requires a large amount of annotated data and can hardly be
applied to low-resource domains. To address EE more effectively with limited
resources, we propose the Demonstration-enhanced Schema-guided Generation
(DemoSG) model, which benefits low-resource EE from two aspects: Firstly, we
propose the demonstration-based learning paradigm for EE to fully use the
annotated data, which transforms them into demonstrations to illustrate the
extraction process and help the model learn effectively. Secondly, we formulate
EE as a natural language generation task guided by schema-based prompts,
thereby leveraging label semantics and promoting knowledge transfer in
low-resource scenarios. We conduct extensive experiments under in-domain and
domain adaptation low-resource settings on three datasets, and study the
robustness of DemoSG. The results show that DemoSG significantly outperforms
current methods in low-resource scenarios.
</p>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10482" title="Abstract">arXiv:2310.10482</a> [<a href="/pdf/2310.10482" title="Download PDF">pdf</a>, <a href="/format/2310.10482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> xCOMET: Transparent Machine Translation Evaluation through Fine-grained  Error Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guerreiro%2C+N+M">Nuno M. Guerreiro</a>, 
<a href="/search/cs?searchtype=author&query=Rei%2C+R">Ricardo Rei</a>, 
<a href="/search/cs?searchtype=author&query=van+Stigt%2C+D">Daan van Stigt</a>, 
<a href="/search/cs?searchtype=author&query=Coheur%2C+L">Luisa Coheur</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+A+F+T">Andr&#xe9; F.T. Martins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Widely used learned metrics for machine translation evaluation, such as COMET
and BLEURT, estimate the quality of a translation hypothesis by providing a
single sentence-level score. As such, they offer little insight into
translation errors (e.g., what are the errors and what is their severity). On
the other hand, generative large language models (LLMs) are amplifying the
adoption of more granular strategies to evaluation, attempting to detail and
categorize translation errors. In this work, we introduce xCOMET, an
open-source learned metric designed to bridge the gap between these approaches.
xCOMET integrates both sentence-level evaluation and error span detection
capabilities, exhibiting state-of-the-art performance across all types of
evaluation (sentence-level, system-level, and error span detection). Moreover,
it does so while highlighting and categorizing error spans, thus enriching the
quality assessment. We also provide a robustness analysis with stress tests,
and show that xCOMET is largely capable of identifying localized critical
errors and hallucinations.
</p>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10483" title="Abstract">arXiv:2310.10483</a> [<a href="/pdf/2310.10483" title="Download PDF">pdf</a>, <a href="/format/2310.10483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Passive Inference Attacks on Split Learning via Adversarial  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaochen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xinjian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuncheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yangfan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xiaokui Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ooi%2C+B+C">Beng Chin Ooi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Split Learning (SL) has emerged as a practical and efficient alternative to
traditional federated learning. While previous attempts to attack SL have often
relied on overly strong assumptions or targeted easily exploitable models, we
seek to develop more practical attacks. We introduce SDAR, a novel attack
framework against SL with an honest-but-curious server. SDAR leverages
auxiliary data and adversarial regularization to learn a decodable simulator of
the client's private model, which can effectively infer the client's private
features under the vanilla SL, and both features and labels under the U-shaped
SL. We perform extensive experiments in both configurations to validate the
effectiveness of our proposed attacks. Notably, in challenging but practical
scenarios where existing passive attacks struggle to reconstruct the client's
private data effectively, SDAR consistently achieves attack performance
comparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR
achieves private feature reconstruction with less than 0.025 mean squared error
in both the vanilla and the U-shaped SL, and attains a label inference accuracy
of over 98% in the U-shaped setting, while existing attacks fail to produce
non-trivial results.
</p>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10486" title="Abstract">arXiv:2310.10486</a> [<a href="/pdf/2310.10486" title="Download PDF">pdf</a>, <a href="/format/2310.10486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse  Quadruped Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shafiee%2C+M">Milad Shafiee</a>, 
<a href="/search/cs?searchtype=author&query=Bellegarda%2C+G">Guillaume Bellegarda</a>, 
<a href="/search/cs?searchtype=author&query=Ijspeert%2C+A">Auke Ijspeert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">Learning a locomotion policy for quadruped robots has traditionally been
constrained to specific robot morphology, mass, and size. The learning process
must usually be repeated for every new robot, where hyperparameters and reward
function weights must be re-tuned to maximize performance for each new system.
Alternatively, attempting to train a single policy to accommodate different
robot sizes, while maintaining the same degrees of freedom (DoF) and
morphology, requires either complex learning frameworks, or mass, inertia, and
dimension randomization, which leads to prolonged training periods. In our
study, we show that drawing inspiration from animal motor control allows us to
effectively train a single locomotion policy capable of controlling a diverse
range of quadruped robots. These differences encompass a variable number of
DoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad mass range
spanning from 2 kg to 200 kg, and nominal standing heights ranging from 16 cm
to 100 cm. Our policy modulates a representation of the Central Pattern
Generator (CPG) in the spinal cord, effectively coordinating both frequencies
and amplitudes of the CPG to produce rhythmic output (Rhythm Generation), which
is then mapped to a Pattern Formation (PF) layer. Across different robots, the
only varying component is the PF layer, which adjusts the scaling parameters
for the stride height and length. Subsequently, we evaluate the sim-to-real
transfer by testing the single policy on both the Unitree Go1 and A1 robots.
Remarkably, we observe robust performance, even when adding a 15 kg load,
equivalent to 125% of the A1 robot's nominal mass.
</p>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10487" title="Abstract">arXiv:2310.10487</a> [<a href="/pdf/2310.10487" title="Download PDF">pdf</a>, <a href="/format/2310.10487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Type-aware Decoding via Explicitly Aggregating Event Information for  Document-level Event Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Gang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yidong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shudong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xiaocheng Gong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Si Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Document-level event extraction (DEE) faces two main challenges:
arguments-scattering and multi-event. Although previous methods attempt to
address these challenges, they overlook the interference of event-unrelated
sentences during event detection and neglect the mutual interference of
different event roles during argument extraction. Therefore, this paper
proposes a novel Schema-based Explicitly Aggregating~(SEA) model to address
these limitations. SEA aggregates event information into event type and role
representations, enabling the decoding of event records based on specific
type-aware representations. By detecting each event based on its event type
representation, SEA mitigates the interference caused by event-unrelated
information. Furthermore, SEA extracts arguments for each role based on its
role-aware representations, reducing mutual interference between different
roles. Experimental results on the ChFinAnn and DuEE-fin datasets show that SEA
outperforms the SOTA methods.
</p>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10490" title="Abstract">arXiv:2310.10490</a> [<a href="/pdf/2310.10490" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Transferability of Learning Models for Semantic Segmentation for  Remote Sensing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+R">Rongjun Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guixiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent deep learning-based methods outperform traditional learning methods on
remote sensing (RS) semantic segmentation/classification tasks. However, they
require large training datasets and are generally known for lack of
transferability due to the highly disparate RS image content across different
geographical regions. Yet, there is no comprehensive analysis of their
transferability, i.e., to which extent a model trained on a source domain can
be readily applicable to a target domain. Therefore, in this paper, we aim to
investigate the raw transferability of traditional and deep learning (DL)
models, as well as the effectiveness of domain adaptation (DA) approaches in
enhancing the transferability of the DL models (adapted transferability). By
utilizing four highly diverse RS datasets, we train six models with and without
three DA approaches to analyze their transferability between these datasets
quantitatively. Furthermore, we developed a straightforward method to quantify
the transferability of a model using the spectral indices as a medium and have
demonstrated its effectiveness in evaluating the model transferability at the
target domain when the labels are unavailable. Our experiments yield several
generally important yet not well-reported observations regarding the raw and
adapted transferability. Moreover, our proposed label-free transferability
assessment method is validated to be better than posterior model confidence.
The findings can guide the future development of generalized RS learning
models. The trained models are released under this link:
https://github.com/GDAOSU/Transferability-Remote-Sensing
</p>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10492" title="Abstract">arXiv:2310.10492</a> [<a href="/pdf/2310.10492" title="Download PDF">pdf</a>, <a href="/format/2310.10492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+M">Min-Yen Kan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Previous zero-shot dialogue state tracking (DST) methods only apply transfer
learning, but ignore unlabelled data in the target domain. We transform
zero-shot DST into few-shot DST by utilising such unlabelled data via joint and
self-training methods. Our method incorporates auxiliary tasks that generate
slot types as inverse prompts for main tasks, creating slot values during joint
training. Cycle consistency between these two tasks enables the generation and
selection of quality samples in unknown target domains for subsequent
fine-tuning. This approach also facilitates automatic label creation, thereby
optimizing the training and fine-tuning of DST models. We demonstrate this
method's effectiveness on large language models in zero-shot scenarios,
improving average joint goal accuracy by $8\%$ across all domains in MultiWOZ.
</p>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10493" title="Abstract">arXiv:2310.10493</a> [<a href="/pdf/2310.10493" title="Download PDF">pdf</a>, <a href="/format/2310.10493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation and improvement of Segment Anything Model for interactive  histopathology image segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">SeungKyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+H">Hyun-Jic Oh</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+S">Seonghui Min</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+W">Won-Ki Jeong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023 workshop accepted (1st International Workshop on Foundation Models for General Medical AI - MedAGI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">With the emergence of the Segment Anything Model (SAM) as a foundational
model for image segmentation, its application has been extensively studied
across various domains, including the medical field. However, its potential in
the context of histopathology data, specifically in region segmentation, has
received relatively limited attention. In this paper, we evaluate SAM's
performance in zero-shot and fine-tuned scenarios on histopathology data, with
a focus on interactive segmentation. Additionally, we compare SAM with other
state-of-the-art interactive models to assess its practical potential and
evaluate its generalization capability with domain adaptability. In the
experimental results, SAM exhibits a weakness in segmentation performance
compared to other models while demonstrating relative strengths in terms of
inference time and generalization capability. To improve SAM's limited local
refinement ability and to enhance prompt stability while preserving its core
strengths, we propose a modification of SAM's decoder. The experimental results
suggest that the proposed modification is effective to make SAM useful for
interactive histology image segmentation. The code is available at
\url{https://github.com/hvcl/SAM_Interactive_Histopathology}
</p>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10495" title="Abstract">arXiv:2310.10495</a> [<a href="/pdf/2310.10495" title="Download PDF">pdf</a>, <a href="/format/2310.10495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metric Ensembles For Hallucination Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Forbes%2C+G+C">Grant C. Forbes</a>, 
<a href="/search/cs?searchtype=author&query=Katlana%2C+P">Parth Katlana</a>, 
<a href="/search/cs?searchtype=author&query=Ortiz%2C+Z">Zeydy Ortiz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Abstractive text summarization has garnered increased interest as of late, in
part due to the proliferation of large language models (LLMs). One of the most
pressing problems related to generation of abstractive summaries is the need to
reduce "hallucinations," information that was not included in the document
being summarized, and which may be wholly incorrect. Due to this need, a wide
array of metrics estimating consistency with the text being summarized have
been proposed. We examine in particular a suite of unsupervised metrics for
summary consistency, and measure their correlations with each other and with
human evaluation scores in the wiki_bio_gpt3_hallucination dataset. We then
compare these evaluations to models made from a simple linear ensemble of these
metrics. We find that LLM-based methods outperform other unsupervised metrics
for hallucination detection. We also find that ensemble methods can improve
these scores even further, provided that the metrics in the ensemble have
sufficiently similar and uncorrelated error rates. Finally, we present an
ensemble method for LLM-based evaluations that we show improves over this
previous SOTA.
</p>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10497" title="Abstract">arXiv:2310.10497</a> [<a href="/pdf/2310.10497" title="Download PDF">pdf</a>, <a href="/format/2310.10497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LocSelect: Target Speaker Localization with an Auditory Selective  Hearing Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xinyuan Qian</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zexu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kainan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The prevailing noise-resistant and reverberation-resistant localization
algorithms primarily emphasize separating and providing directional output for
each speaker in multi-speaker scenarios, without association with the identity
of speakers.In this paper, we present a target speaker localization algorithm
with a selective hearing mechanism. Given a reference speech of the target
speaker, we first produce a speaker-dependent spectrogram mask to eliminate
interfering speakers' speech. Subsequently, a Long short-term memory (LSTM)
network is employed to extract the target speaker's location from the filtered
spectrogram. Experiments validate the superiority of our proposed method over
the existing algorithms for different scale invariant signal-to-noise ratios
(SNR) conditions. Specifically, at SNR = -10 dB, our proposed network LocSelect
achieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.
</p>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10501" title="Abstract">arXiv:2310.10501</a> [<a href="/pdf/2310.10501" title="Download PDF">pdf</a>, <a href="/format/2310.10501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications  with Programmable Rails
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rebedea%2C+T">Traian Rebedea</a>, 
<a href="/search/cs?searchtype=author&query=Dinu%2C+R">Razvan Dinu</a>, 
<a href="/search/cs?searchtype=author&query=Sreedhar%2C+M">Makesh Sreedhar</a>, 
<a href="/search/cs?searchtype=author&query=Parisien%2C+C">Christopher Parisien</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+J">Jonathan Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 - Demo track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">NeMo Guardrails is an open-source toolkit for easily adding programmable
guardrails to LLM-based conversational systems. Guardrails (or rails for short)
are a specific way of controlling the output of an LLM, such as not talking
about topics considered harmful, following a predefined dialogue path, using a
particular language style, and more. There are several mechanisms that allow
LLM providers and developers to add guardrails that are embedded into a
specific model at training, e.g. using model alignment. Differently, using a
runtime inspired from dialogue management, NeMo Guardrails allows developers to
add programmable rails to LLM applications - these are user-defined,
independent of the underlying LLM, and interpretable. Our initial results show
that the proposed approach can be used with several LLM providers to develop
controllable and safe LLM applications using programmable rails.
</p>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10502" title="Abstract">arXiv:2310.10502</a> [<a href="/pdf/2310.10502" title="Download PDF">pdf</a>, <a href="/format/2310.10502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Robot Assistance: Expertise and Influence in Multi-User Task  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dahiya%2C+A">Abhinav Dahiya</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+S+L">Stephen L. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">This paper addresses the challenge of enabling a single robot to effectively
assist multiple humans in decision-making for task planning domains. We
introduce a comprehensive framework designed to enhance overall team
performance by considering both human expertise in making the optimal decisions
and robot influence on human decision-making. Our model integrates these
factors seamlessly within the task-planning domain, formulating the problem as
a partially observable Markov decision process (POMDP) while treating expertise
and influence as unobservable components of the system state. To solve for the
robot's actions in such systems, we propose an efficient Attention-Switching
policy. This policy capitalizes on the inherent structure of such systems,
solving multiple smaller POMDPs to generate heuristics for prioritizing
interactions with different human teammates, thereby reducing the state space
and improving scalability. Our empirical results on a simulated kit fulfillment
task demonstrate improved team performance when the robot's policy accounts for
both expertise and influence. This research represents a significant step
forward in the field of adaptive robot assistance, paving the way for
integration into cost-effective small and mid-scale industries, where
substantial investments in robotic infrastructure may not be economically
viable.
</p>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10505" title="Abstract">arXiv:2310.10505</a> [<a href="/pdf/2310.10505" title="Download PDF">pdf</a>, <a href="/format/2310.10505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReMax: A Simple, Effective, and Efficient Method for Aligning Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziniu Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yushun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhi-Quan Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Alignment is of critical importance for training large language models
(LLMs). The predominant strategy to address this is through Reinforcement
Learning from Human Feedback (RLHF), where PPO serves as the de-facto
algorithm. Yet, PPO is known to suffer from computational inefficiency, a
challenge that this paper aims to address. We identify three important
properties in RLHF tasks: fast simulation, deterministic transitions, and
trajectory-level rewards, which are not leveraged in PPO. Based on such
observations, we develop a new algorithm tailored for RLHF, called ReMax. The
algorithm design of ReMax is built on a celebrated algorithm REINFORCE but is
equipped with a new variance-reduction technique.
<br />Our method has three-fold advantages over PPO: first, it saves about 50%
memory usage in principle. As a result, PPO runs out-of-memory when fine-tuning
a Llama2 (7B) model on 8xA100-40GB GPUs, whereas ReMax can afford training.
This memory improvement is achieved by removing the value model in PPO. Second,
ReMax is simple to implement and removes many hyper-parameters in PPO, which
are scale-sensitive and laborious to tune. Third, on GPT2 (137M), we observe
2.2x speed-up in terms of wall-clock time. Importantly, the above computational
improvements do not sacrifice the performance. We hypothesize these advantages
can be maintained in larger-scaled models. Our implementation of ReMax is
available at https://github.com/liziniu/ReMax
</p>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10506" title="Abstract">arXiv:2310.10506</a> [<a href="/pdf/2310.10506" title="Download PDF">pdf</a>, <a href="/ps/2310.10506" title="Download PostScript">ps</a>, <a href="/format/2310.10506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient numerical method for the anisotropic phase field dendritic  crystal growth model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guo%2C+Y">Yayu Guo</a>, 
<a href="/search/math?searchtype=author&query=Azaiez%2C+M">Mejdi Azaiez</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+C">Chuanju Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we propose and analyze an efficient numerical method for the
anisotropic phase field dendritic crystal growth model, which is challenging
because we are facing the nonlinear coupling and anisotropic coefficient in the
model. The proposed method is a two-step scheme. In the first step, an
intermediate solution is computed by using BDF schemes of order up to three for
both the phase-field and heat equations. In the second step the intermediate
solution is stabilized by multiplying an auxiliary variable. The key of the
second step is to stabilize the overall scheme while maintaining the
convergence order of the stabilized solution. In order to overcome the
difficulty caused by the gradient-dependent anisotropic coefficient and the
nonlinear terms, some stabilization terms are added to the BDF schemes in the
first step. The second step makes use of a generalized auxiliary variable
approach with relaxation. The Fourier spectral method is applied for the
spatial discretization. Our analysis shows that the proposed scheme is
unconditionally stable and has accuracy in time up to third order. We also
provide a sophisticated implementation showing that the computational
complexity of our schemes is equivalent to solving two linear equations and
some algebraic equations. To the best of our knowledge, this is the cheapest
unconditionally stable schemes reported in the literature. Some numerical
examples are given to verify the efficiency of the proposed method.
</p>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10508" title="Abstract">arXiv:2310.10508</a> [<a href="/pdf/2310.10508" title="Download PDF">pdf</a>, <a href="/format/2310.10508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Engineering or Fine Tuning: An Empirical Assessment of Large  Language Models in Automated Software Engineering Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jiho Shin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Clark Tang</a>, 
<a href="/search/cs?searchtype=author&query=Mohati%2C+T">Tahmineh Mohati</a>, 
<a href="/search/cs?searchtype=author&query=Nayebi%2C+M">Maleknaz Nayebi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hemmati%2C+H">Hadi Hemmati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages + reference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In this paper, we investigate the effectiveness of state-of-the-art LLM,
i.e., GPT-4, with three different prompting engineering techniques (i.e., basic
prompting, in-context learning, and task-specific prompting) against 18
fine-tuned LLMs on three typical ASE tasks, i.e., code generation, code
summarization, and code translation. Our quantitative analysis of these
prompting strategies suggests that prompt engineering GPT-4 cannot necessarily
and significantly outperform fine-tuning smaller/older LLMs in all three tasks.
For comment generation, GPT-4 with the best prompting strategy (i.e.,
task-specific prompt) had outperformed the first-ranked fine-tuned model by
8.33% points on average in BLEU. However, for code generation, the first-ranked
fine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3%
points, on average in BLEU. For code translation, GPT-4 and fine-tuned
baselines tie as they outperform each other on different translation tasks. To
explore the impact of different prompting strategies, we conducted a user study
with 27 graduate students and 10 industry practitioners. From our qualitative
analysis, we find that the GPT-4 with conversational prompts (i.e., when a
human provides feedback and instructions back and forth with a model to achieve
best results) showed drastic improvement compared to GPT-4 with automatic
prompting strategies. Moreover, we observe that participants tend to request
improvements, add more context, or give specific instructions as conversational
prompts, which goes beyond typical and generic prompting strategies. Our study
suggests that, at its current state, GPT-4 with conversational prompting has
great potential for ASE tasks, but fully automated prompt engineering with no
human in the loop requires more study and improvement.
</p>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10509" title="Abstract">arXiv:2310.10509</a> [<a href="/pdf/2310.10509" title="Download PDF">pdf</a>, <a href="/format/2310.10509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with  Online Admittance Residual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingfeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Robot Learning (CoRL) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Learning contact-rich manipulation skills is essential. Such skills require
the robots to interact with the environment with feasible manipulation
trajectories and suitable compliance control parameters to enable safe and
stable contact. However, learning these skills is challenging due to data
inefficiency in the real world and the sim-to-real gap in simulation. In this
paper, we introduce a hybrid offline-online framework to learn robust
manipulation skills. We employ model-free reinforcement learning for the
offline phase to obtain the robot motion and compliance control parameters in
simulation \RV{with domain randomization}. Subsequently, in the online phase,
we learn the residual of the compliance control parameters to maximize robot
performance-related criteria with force sensor measurements in real time. To
demonstrate the effectiveness and robustness of our approach, we provide
comparative results against existing methods for assembly, pivoting, and
screwing tasks.
</p>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10513" title="Abstract">arXiv:2310.10513</a> [<a href="/pdf/2310.10513" title="Download PDF">pdf</a>, <a href="/format/2310.10513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying Image Processing as Visual Prompting Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xianzheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiantao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chao Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Image processing is a fundamental task in computer vision, which aims at
enhancing image quality and extracting essential features for subsequent vision
applications. Traditionally, task-specific models are developed for individual
tasks and designing such models requires distinct expertise. Building upon the
success of large language models (LLMs) in natural language processing (NLP),
there is a similar trend in computer vision, which focuses on developing
large-scale models through pretraining and in-context learning. This paradigm
shift reduces the reliance on task-specific models, yielding a powerful unified
model to deal with various tasks. However, these advances have predominantly
concentrated on high-level vision tasks, with less attention paid to low-level
vision tasks. To address this issue, we propose a universal model for general
image processing that covers image restoration, image enhancement, image
feature extraction tasks, \textit{etc}. Our proposed framework, named
PromptGIP, unifies these diverse image processing tasks within a universal
framework. Inspired by NLP question answering (QA) techniques, we employ a
visual prompting question answering paradigm. Specifically, we treat the
input-output image pair as a structured question-answer sentence, thereby
reprogramming the image processing task as a prompting QA problem. PromptGIP
can undertake diverse \textbf{cross-domain} tasks using provided visual
prompts, eliminating the need for task-specific finetuning. Our methodology
offers a universal and adaptive solution to general image processing. While
PromptGIP has demonstrated a certain degree of out-of-domain task
generalization capability, further research is expected to fully explore its
more powerful emergent generalization.
</p>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10517" title="Abstract">arXiv:2310.10517</a> [<a href="/pdf/2310.10517" title="Download PDF">pdf</a>, <a href="/format/2310.10517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution prediction for image compression: An experimental  re-compressor for JPEG images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koroteev%2C+M">Maxim Koroteev</a>, 
<a href="/search/cs?searchtype=author&query=Borisov%2C+Y">Yaroslav Borisov</a>, 
<a href="/search/cs?searchtype=author&query=Frolov%2C+P">Pavel Frolov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a new scheme to re-compress JPEG images in a lossless way. Using a
JPEG image as an input the algorithm partially decodes the signal to obtain
quantized DCT coefficients and then re-compress them in a more effective way.
</p>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10520" title="Abstract">arXiv:2310.10520</a> [<a href="/pdf/2310.10520" title="Download PDF">pdf</a>, <a href="/format/2310.10520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Parsing by Large Language Models for Intricate Updating  Strategies of Zero-Shot Dialogue State Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuxiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023 (Short Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring
and annotating task-oriented dialogues, which can be time consuming and costly.
However, DST extends beyond simple slot-filling and requires effective updating
strategies for tracking dialogue state as conversations progress. In this
paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to
introduce additional intricate updating strategies in zero-shot DST. Our
approach reformulates the DST task by leveraging powerful Large Language Models
(LLMs) and translating the original dialogue text to JSON through semantic
parsing as an intermediate state. We also design a novel framework that
includes more modules to ensure the effectiveness of updating strategies in the
text-to-JSON process. Experimental results demonstrate that our approach
outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant
improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to
existing ICL methods.
</p>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10524" title="Abstract">arXiv:2310.10524</a> [<a href="/pdf/2310.10524" title="Download PDF">pdf</a>, <a href="/format/2310.10524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A second-order SO(3)-preserving and energy-stable scheme for orthonormal  frame gradient flow model of biaxial nematic liquid crystals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+H">Hanbin Wang</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+Z">Zhiguo Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">In this paper, we present a novel second-order generalised rotational
discrete gradient scheme for numerically approximating the orthonormal frame
gradient flow of biaxial nematic liquid crystals. This scheme relies on
reformulating the original gradient flow system into an equivalent generalised
"rotational" form. A second-order discrete gradient approximation of the energy
variation is then devised such that it satisfies an energy difference relation.
The proposed numerical scheme has two remarkable properties: (i) it strictly
obeys the orthonormal property of the tensor field and (ii) it satisfies the
energy dissipation law at the discrete level, regardless of the time step
sizes. We provide ample numerical results to validate the accuracy, efficiency,
unconditional stability and SO(3)-preserving property of this scheme. In
addition, comparisons of the simulation results between the biaxial orthonormal
frame gradient flow model and uniaxial Oseen-Frank gradient flow are made to
demonstrate the ability of the former to characterize non-axisymmetric local
anisotropy.
</p>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10526" title="Abstract">arXiv:2310.10526</a> [<a href="/pdf/2310.10526" title="Download PDF">pdf</a>, <a href="/format/2310.10526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A spectrally accurate step-by-step method for the numerical solution of  fractional differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Brugnano%2C+L">L. Brugnano</a>, 
<a href="/search/math?searchtype=author&query=Burrage%2C+K">K. Burrage</a>, 
<a href="/search/math?searchtype=author&query=Burrage%2C+P">P. Burrage</a>, 
<a href="/search/math?searchtype=author&query=Iavernaro%2C+F">F. Iavernaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 2 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper we consider the numerical solution of fractional differential
equations. In particular, we study a step-by-step graded mesh procedure based
on an expansion of the vector field using orthonormal Jacobi polynomials. Under
mild hypotheses, the proposed procedure is capable of getting spectral
accuracy. A few numerical examples are reported to confirm the theoretical
findings.
</p>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10531" title="Abstract">arXiv:2310.10531</a> [<a href="/pdf/2310.10531" title="Download PDF">pdf</a>, <a href="/format/2310.10531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning optimal integration of spatial and temporal information in  noisy chemotaxis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alonso%2C+A">Albert Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Kirkegaard%2C+J+B">Julius B. Kirkegaard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Biological Physics (physics.bio-ph)

</div>
<p class="mathjax">We investigate the boundary between chemotaxis driven by spatial estimation
of gradients and chemotaxis driven by temporal estimation. While it is well
known that spatial chemotaxis becomes disadvantageous for small organisms at
high noise levels, it is unclear whether there is a discontinuous switch of
optimal strategies or a continuous transition exists. Here, we employ deep
reinforcement learning to study the possible integration of spatial and
temporal information in an a priori unconstrained manner. We parameterize such
a combined chemotactic policy by a recurrent neural network and evaluate it
using a minimal theoretical model of a chemotactic cell. By comparing with
constrained variants of the policy, we show that it converges to purely
temporal and spatial strategies at small and large cell sizes, respectively. We
find that the transition between the regimes is continuous, with the combined
strategy outperforming in the transition region both the constrained variants
as well as models that explicitly integrate spatial and temporal information.
Finally, by utilizing the attribution method of integrated gradients, we show
that the policy relies on a non-trivial combination of spatially and temporally
derived gradient information in a ratio that varies dynamically during the
chemotactic trajectories.
</p>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10532" title="Abstract">arXiv:2310.10532</a> [<a href="/pdf/2310.10532" title="Download PDF">pdf</a>, <a href="/format/2310.10532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One For All &amp; All For One: Bypassing Hyperparameter Tuning with Model  Averaging For Cross-Lingual Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+F+D">Fabian David Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Glava%C5%A1%2C+G">Goran Glava&#x161;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multilingual language models enable zero-shot cross-lingual transfer
(ZS-XLT): fine-tuned on sizable source-language task data, they perform the
task in target languages without labeled instances. The effectiveness of ZS-XLT
hinges on the linguistic proximity between languages and the amount of
pretraining data for a language. Because of this, model selection based on
source-language validation is unreliable: it picks model snapshots with
suboptimal target-language performance. As a remedy, some work optimizes ZS-XLT
by extensively tuning hyperparameters: the follow-up work then routinely
struggles to replicate the original results. Other work searches over narrower
hyperparameter grids, reporting substantially lower performance. In this work,
we therefore propose an unsupervised evaluation protocol for ZS-XLT that
decouples performance maximization from hyperparameter tuning. As a robust and
more transparent alternative to extensive hyperparameter tuning, we propose to
accumulatively average snapshots from different runs into a single model. We
run broad ZS-XLT experiments on both higher-level semantic tasks (NLI,
extractive QA) and a lower-level token classification task (NER) and find that
conventional model selection based on source-language validation quickly
plateaus to suboptimal ZS-XLT performance. On the other hand, our accumulative
run-by-run averaging of models trained with different hyperparameters boosts
ZS-XLT performance and closely correlates with "oracle" ZS-XLT, i.e., model
selection based on target-language validation performance.
</p>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10533" title="Abstract">arXiv:2310.10533</a> [<a href="/pdf/2310.10533" title="Download PDF">pdf</a>, <a href="/format/2310.10533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-efficient Segmentation via Affinity Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wentong Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuqian Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Dongqi Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianke Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS2023 Acceptance. Project Page:<a href="https://LiWentomng.github.io/apro/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Weakly-supervised segmentation with label-efficient sparse annotations has
attracted increasing research attention to reduce the cost of laborious
pixel-wise labeling process, while the pairwise affinity modeling techniques
play an essential role in this task. Most of the existing approaches focus on
using the local appearance kernel to model the neighboring pairwise potentials.
However, such a local operation fails to capture the long-range dependencies
and ignores the topology of objects. In this work, we formulate the affinity
modeling as an affinity propagation process, and propose a local and a global
pairwise affinity terms to generate accurate soft pseudo labels. An efficient
algorithm is also developed to reduce significantly the computational cost. The
proposed approach can be conveniently plugged into existing segmentation
networks. Experiments on three typical label-efficient segmentation tasks, i.e.
box-supervised instance segmentation, point/scribble-supervised semantic
segmentation and CLIP-guided semantic segmentation, demonstrate the superior
performance of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10534" title="Abstract">arXiv:2310.10534</a> [<a href="/pdf/2310.10534" title="Download PDF">pdf</a>, <a href="/format/2310.10534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Comparators in Generalization Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hellstr%C3%B6m%2C+F">Fredrik Hellstr&#xf6;m</a>, 
<a href="/search/cs?searchtype=author&query=Guedj%2C+B">Benjamin Guedj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We derive generic information-theoretic and PAC-Bayesian generalization
bounds involving an arbitrary convex comparator function, which measures the
discrepancy between the training and population loss. The bounds hold under the
assumption that the cumulant-generating function (CGF) of the comparator is
upper-bounded by the corresponding CGF within a family of bounding
distributions. We show that the tightest possible bound is obtained with the
comparator being the convex conjugate of the CGF of the bounding distribution,
also known as the Cram\'er function. This conclusion applies more broadly to
generalization bounds with a similar structure. This confirms the
near-optimality of known bounds for bounded and sub-Gaussian losses and leads
to novel bounds under other bounding distributions.
</p>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10537" title="Abstract">arXiv:2310.10537</a> [<a href="/pdf/2310.10537" title="Download PDF">pdf</a>, <a href="/format/2310.10537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Microscaling Data Formats for Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rouhani%2C+B+D">Bita Darvish Rouhani</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ritchie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=More%2C+A">Ankit More</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+M">Mathew Hall</a>, 
<a href="/search/cs?searchtype=author&query=Khodamoradi%2C+A">Alireza Khodamoradi</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Summer Deng</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+D">Dhruv Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Cornea%2C+M">Marius Cornea</a>, 
<a href="/search/cs?searchtype=author&query=Dellinger%2C+E">Eric Dellinger</a>, 
<a href="/search/cs?searchtype=author&query=Denolf%2C+K">Kristof Denolf</a>, 
<a href="/search/cs?searchtype=author&query=Dusan%2C+S">Stosic Dusan</a>, 
<a href="/search/cs?searchtype=author&query=Elango%2C+V">Venmugil Elango</a>, 
<a href="/search/cs?searchtype=author&query=Golub%2C+M">Maximilian Golub</a>, 
<a href="/search/cs?searchtype=author&query=Heinecke%2C+A">Alexander Heinecke</a>, 
<a href="/search/cs?searchtype=author&query=James-Roxby%2C+P">Phil James-Roxby</a>, 
<a href="/search/cs?searchtype=author&query=Jani%2C+D">Dharmesh Jani</a>, 
<a href="/search/cs?searchtype=author&query=Kolhe%2C+G">Gaurav Kolhe</a>, 
<a href="/search/cs?searchtype=author&query=Langhammer%2C+M">Martin Langhammer</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ada Li</a>, 
<a href="/search/cs?searchtype=author&query=Melnick%2C+L">Levi Melnick</a>, 
<a href="/search/cs?searchtype=author&query=Mesmakhosroshahi%2C+M">Maral Mesmakhosroshahi</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+A">Andres Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Schulte%2C+M">Michael Schulte</a>, 
<a href="/search/cs?searchtype=author&query=Shafipour%2C+R">Rasoul Shafipour</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+L">Lei Shao</a>, 
<a href="/search/cs?searchtype=author&query=Siu%2C+M">Michael Siu</a>, 
<a href="/search/cs?searchtype=author&query=Dubey%2C+P">Pradeep Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Micikevicius%2C+P">Paulius Micikevicius</a>, 
<a href="/search/cs?searchtype=author&query=Naumov%2C+M">Maxim Naumov</a>, 
<a href="/search/cs?searchtype=author&query=Verilli%2C+C">Colin Verilli</a>, 
<a href="/search/cs?searchtype=author&query=Wittig%2C+R">Ralph Wittig</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+E">Eric Chung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Narrow bit-width data formats are key to reducing the computational and
storage costs of modern deep learning applications. This paper evaluates
Microscaling (MX) data formats that combine a per-block scaling factor with
narrow floating-point and integer types for individual elements.MX formats
balance the competing needs of hardware efficiency, model accuracy, and user
friction. Empirical results on over two dozen benchmarks demonstrate
practicality of MX data formats as a drop-in replacement for baseline FP32 for
AI inference and training with low user friction. We also show the first
instance of training generative language models at sub-8-bit weights,
activations, and gradients with minimal accuracy loss and no modifications to
the training recipe.
</p>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10541" title="Abstract">arXiv:2310.10541</a> [<a href="/pdf/2310.10541" title="Download PDF">pdf</a>, <a href="/format/2310.10541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Dataset Distillation through Alignment with Smooth and  High-Quality Expert Trajectories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiyuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenzhuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+K">Kwok-Yan Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Training a large and state-of-the-art machine learning model typically
necessitates the use of large-scale datasets, which, in turn, makes the
training and parameter-tuning process expensive and time-consuming. Some
researchers opt to distil information from real-world datasets into tiny and
compact synthetic datasets while maintaining their ability to train a
well-performing model, hence proposing a data-efficient method known as Dataset
Distillation (DD). Despite recent progress in this field, existing methods
still underperform and cannot effectively replace large datasets. In this
paper, unlike previous methods that focus solely on improving the efficacy of
student distillation, we are the first to recognize the important interplay
between expert and student. We argue the significant impact of expert
smoothness when employing more potent expert trajectories in subsequent dataset
distillation. Based on this, we introduce the integration of clipping loss and
gradient penalty to regulate the rate of parameter changes in expert
trajectories. Furthermore, in response to the sensitivity exhibited towards
randomly initialized variables during distillation, we propose representative
initialization for synthetic dataset and balanced inner-loop loss. Finally, we
present two enhancement strategies, namely intermediate matching loss and
weight perturbation, to mitigate the potential occurrence of cumulative errors.
We conduct extensive experiments on datasets of different scales, sizes, and
resolutions. The results demonstrate that the proposed method significantly
outperforms prior methods.
</p>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10543" title="Abstract">arXiv:2310.10543</a> [<a href="/pdf/2310.10543" title="Download PDF">pdf</a>, <a href="/format/2310.10543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViPE: Visualise Pretty-much Everything
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahmohammadi%2C+H">Hassan Shahmohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+A">Adhiraj Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Lensch%2C+H+P+A">Hendrik P. A. Lensch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented in EMNLP2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Figurative and non-literal expressions are profoundly integrated in human
communication. Visualising such expressions allow us to convey our creative
thoughts, and evoke nuanced emotions. Recent text-to-image models like Stable
Diffusion, on the other hand, struggle to depict non-literal expressions.
Recent works primarily deal with this issue by compiling humanly annotated
datasets on a small scale, which not only demands specialised expertise but
also proves highly inefficient. To address this issue, we introduce ViPE:
Visualise Pretty-much Everything. ViPE offers a series of lightweight and
robust language models that have been trained on a large-scale set of lyrics
with noisy visual descriptions that represent their implicit meaning. The
synthetic visual descriptions are generated by GPT3.5 relying on neither human
annotations nor images. ViPE effectively expresses any arbitrary piece of text
into a visualisable description, enabling meaningful and high-quality image
generation. We provide compelling evidence that ViPE is more robust than GPT3.5
in synthesising visual elaborations. ViPE also exhibits an understanding of
figurative expressions comparable to human experts, providing a powerful and
open-source backbone to many downstream applications such as music video and
caption generation.
</p>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10547" title="Abstract">arXiv:2310.10547</a> [<a href="/pdf/2310.10547" title="Download PDF">pdf</a>, <a href="/format/2310.10547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfoGCN++: Learning Representation by Predicting the Future for Online  Human Skeleton-based Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+S">Seunggeun Chi</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+H">Hyung-gun Chi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qixing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ramani%2C+K">Karthik Ramani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Skeleton-based action recognition has made significant advancements recently,
with models like InfoGCN showcasing remarkable accuracy. However, these models
exhibit a key limitation: they necessitate complete action observation prior to
classification, which constrains their applicability in real-time situations
such as surveillance and robotic systems. To overcome this barrier, we
introduce InfoGCN++, an innovative extension of InfoGCN, explicitly developed
for online skeleton-based action recognition. InfoGCN++ augments the abilities
of the original InfoGCN model by allowing real-time categorization of action
types, independent of the observation sequence's length. It transcends
conventional approaches by learning from current and anticipated future
movements, thereby creating a more thorough representation of the entire
sequence. Our approach to prediction is managed as an extrapolation issue,
grounded on observed actions. To enable this, InfoGCN++ incorporates Neural
Ordinary Differential Equations, a concept that lets it effectively model the
continuous evolution of hidden states. Following rigorous evaluations on three
skeleton-based action recognition benchmarks, InfoGCN++ demonstrates
exceptional performance in online action recognition. It consistently equals or
exceeds existing techniques, highlighting its significant potential to reshape
the landscape of real-time action recognition applications. Consequently, this
work represents a major leap forward from InfoGCN, pushing the limits of what's
possible in online, skeleton-based action recognition. The code for InfoGCN++
is publicly available at https://github.com/stnoah1/infogcn2 for further
exploration and validation.
</p>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10548" title="Abstract">arXiv:2310.10548</a> [<a href="/pdf/2310.10548" title="Download PDF">pdf</a>, <a href="/format/2310.10548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A perching and tilting aerial robot for precise and versatile power tool  work on vertical walls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dautzenberg%2C+R">Roman Dautzenberg</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BCster%2C+T">Timo K&#xfc;ster</a>, 
<a href="/search/cs?searchtype=author&query=Mathis%2C+T">Timon Mathis</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+Y">Yann Roth</a>, 
<a href="/search/cs?searchtype=author&query=Steinauer%2C+C">Curdin Steinauer</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4ppeli%2C+G">Gabriel K&#xe4;ppeli</a>, 
<a href="/search/cs?searchtype=author&query=Santen%2C+J">Julian Santen</a>, 
<a href="/search/cs?searchtype=author&query=Arranhado%2C+A">Alina Arranhado</a>, 
<a href="/search/cs?searchtype=author&query=Biffar%2C+F">Friederike Biffar</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6tter%2C+T">Till K&#xf6;tter</a>, 
<a href="/search/cs?searchtype=author&query=Lanegger%2C+C">Christian Lanegger</a>, 
<a href="/search/cs?searchtype=author&query=Allenspach%2C+M">Mike Allenspach</a>, 
<a href="/search/cs?searchtype=author&query=Siegwart%2C+R">Roland Siegwart</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4hnemann%2C+R">Rik B&#xe4;hnemann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Will appear in IEEE International Conference on Intelligent Robots and Systems (IROS) 2023. Winner of IROS Best Paper Award on Mobile Manipulation sponsored by OMRON Sinic X Corp
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Drilling, grinding, and setting anchors on vertical walls are fundamental
processes in everyday construction work. Manually doing these works is
error-prone, potentially dangerous, and elaborate at height. Today, heavy
mobile ground robots can perform automatic power tool work. However, aerial
vehicles could be deployed in untraversable environments and reach inaccessible
places. Existing drone designs do not provide the large forces, payload, and
high precision required for using power tools. This work presents the first
aerial robot design to perform versatile manipulation tasks on vertical
concrete walls with continuous forces of up to 150 N. The platform combines a
quadrotor with active suction cups for perching on walls and a lightweight,
tiltable linear tool table. This combination minimizes weight using the
propulsion system for flying, surface alignment, and feed during manipulation
and allows precise positioning of the power tool. We evaluate our design in a
concrete drilling application - a challenging construction process that
requires high forces, accuracy, and precision. In 30 trials, our design can
accurately pinpoint a target position despite perching imprecision. Nine
visually guided drilling experiments demonstrate a drilling precision of 6 mm
without further automation. Aside from drilling, we also demonstrate the
versatility of the design by setting an anchor into concrete.
</p>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10549" title="Abstract">arXiv:2310.10549</a> [<a href="/pdf/2310.10549" title="Download PDF">pdf</a>, <a href="/format/2310.10549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applications of Distributed Machine Learning for the Internet-of-Things:  A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+M">Mai Le</a>, 
<a href="/search/cs?searchtype=author&query=Huynh-The%2C+T">Thien Huynh-The</a>, 
<a href="/search/cs?searchtype=author&query=Do-Duy%2C+T">Tan Do-Duy</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Thai-Hoc Vu</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+W">Won-Joo Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+Q">Quoc-Viet Pham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The emergence of new services and applications in emerging wireless networks
(e.g., beyond 5G and 6G) has shown a growing demand for the usage of artificial
intelligence (AI) in the Internet of Things (IoT). However, the proliferation
of massive IoT connections and the availability of computing resources
distributed across future IoT systems have strongly demanded the development of
distributed AI for better IoT services and applications. Therefore, existing
AI-enabled IoT systems can be enhanced by implementing distributed machine
learning (aka distributed learning) approaches. This work aims to provide a
comprehensive survey on distributed learning for IoT services and applications
in emerging networks. In particular, we first provide a background of machine
learning and present a preliminary to typical distributed learning approaches,
such as federated learning, multi-agent reinforcement learning, and distributed
inference. Then, we provide an extensive review of distributed learning for
critical IoT services (e.g., data sharing and computation offloading,
localization, mobile crowdsensing, and security and privacy) and IoT
applications (e.g., smart healthcare, smart grid, autonomous vehicle, aerial
IoT networks, and smart industry). From the reviewed literature, we also
present critical challenges of distributed learning for IoT and propose several
promising solutions and research directions in this emerging area.
</p>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10550" title="Abstract">arXiv:2310.10550</a> [<a href="/pdf/2310.10550" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep learning applied to EEG data with different montages using spatial  attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Truong%2C+D">Dung Truong</a>, 
<a href="/search/cs?searchtype=author&query=Khalid%2C+M+A">Muhammad Abdullah Khalid</a>, 
<a href="/search/cs?searchtype=author&query=Delorme%2C+A">Arnaud Delorme</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The ability of Deep Learning to process and extract relevant information in
complex brain dynamics from raw EEG data has been demonstrated in various
recent works. Deep learning models, however, have also been shown to perform
best on large corpora of data. When processing EEG, a natural approach is to
combine EEG datasets from different experiments to train large deep-learning
models. However, most EEG experiments use custom channel montages, requiring
the data to be transformed into a common space. Previous methods have used the
raw EEG signal to extract features of interest and focused on using a common
feature space across EEG datasets. While this is a sensible approach, it
underexploits the potential richness of EEG raw data. Here, we explore using
spatial attention applied to EEG electrode coordinates to perform channel
harmonization of raw EEG data, allowing us to train deep learning on EEG data
using different montages. We test this model on a gender classification task.
We first show that spatial attention increases model performance. Then, we show
that a deep learning model trained on data using different channel montages
performs significantly better than deep learning models trained on fixed 23-
and 128-channel data montages.
</p>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10552" title="Abstract">arXiv:2310.10552</a> [<a href="/pdf/2310.10552" title="Download PDF">pdf</a>, <a href="/format/2310.10552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal bounds for POD approximations of infinite horizon control  problems based on time derivatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=de+Frutos%2C+J">Javier de Frutos</a>, 
<a href="/search/math?searchtype=author&query=Garcia-Archilla%2C+B">Bosco Garcia-Archilla</a>, 
<a href="/search/math?searchtype=author&query=Novo%2C+J">Julia Novo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper we consider the numerical approximation of infinite horizon
problems via the dynamic programming approach. The value function of the
problem solves a Hamilton-Jacobi-Bellman (HJB) equation that is approximated by
a fully discrete method. It is known that the numerical problem is difficult to
handle by the so called curse of dimensionality. To mitigate this issue we
apply a reduction of the order by means of a new proper orthogonal
decomposition (POD) method based on time derivatives. We carry out the error
analysis of the method using recently proved optimal bounds for the fully
discrete approximations. Moreover, the use of snapshots based on time
derivatives allow us to bound some terms of the error that could not be bounded
in a standard POD approach. Some numerical experiments show the good
performance of the method in practice.
</p>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10553" title="Abstract">arXiv:2310.10553</a> [<a href="/pdf/2310.10553" title="Download PDF">pdf</a>, <a href="/format/2310.10553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TacticAI: an AI assistant for football tactics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Veli%C4%8Dkovi%C4%87%2C+P">Petar Veli&#x10d;kovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Hennes%2C+D">Daniel Hennes</a>, 
<a href="/search/cs?searchtype=author&query=Toma%C5%A1ev%2C+N">Nenad Toma&#x161;ev</a>, 
<a href="/search/cs?searchtype=author&query=Prince%2C+L">Laurel Prince</a>, 
<a href="/search/cs?searchtype=author&query=Kaisers%2C+M">Michael Kaisers</a>, 
<a href="/search/cs?searchtype=author&query=Bachrach%2C+Y">Yoram Bachrach</a>, 
<a href="/search/cs?searchtype=author&query=Elie%2C+R">Romuald Elie</a>, 
<a href="/search/cs?searchtype=author&query=Wenliang%2C+L+K">Li Kevin Wenliang</a>, 
<a href="/search/cs?searchtype=author&query=Piccinini%2C+F">Federico Piccinini</a>, 
<a href="/search/cs?searchtype=author&query=Spearman%2C+W">William Spearman</a>, 
<a href="/search/cs?searchtype=author&query=Graham%2C+I">Ian Graham</a>, 
<a href="/search/cs?searchtype=author&query=Connor%2C+J">Jerome Connor</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Recasens%2C+A">Adri&#xe0; Recasens</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M">Mina Khan</a>, 
<a href="/search/cs?searchtype=author&query=Beauguerlange%2C+N">Nathalie Beauguerlange</a>, 
<a href="/search/cs?searchtype=author&query=Sprechmann%2C+P">Pablo Sprechmann</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+P">Pol Moreno</a>, 
<a href="/search/cs?searchtype=author&query=Heess%2C+N">Nicolas Heess</a>, 
<a href="/search/cs?searchtype=author&query=Bowling%2C+M">Michael Bowling</a>, 
<a href="/search/cs?searchtype=author&query=Hassabis%2C+D">Demis Hassabis</a>, 
<a href="/search/cs?searchtype=author&query=Tuyls%2C+K">Karl Tuyls</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA); Machine Learning (stat.ML)

</div>
<p class="mathjax">Identifying key patterns of tactics implemented by rival teams, and
developing effective responses, lies at the heart of modern football. However,
doing so algorithmically remains an open research challenge. To address this
unmet need, we propose TacticAI, an AI football tactics assistant developed and
evaluated in close collaboration with domain experts from Liverpool FC. We
focus on analysing corner kicks, as they offer coaches the most direct
opportunities for interventions and improvements. TacticAI incorporates both a
predictive and a generative component, allowing the coaches to effectively
sample and explore alternative player setups for each corner kick routine and
to select those with the highest predicted likelihood of success. We validate
TacticAI on a number of relevant benchmark tasks: predicting receivers and shot
attempts and recommending player position adjustments. The utility of TacticAI
is validated by a qualitative study conducted with football domain experts at
Liverpool FC. We show that TacticAI's model suggestions are not only
indistinguishable from real tactics, but also favoured over existing tactics
90% of the time, and that TacticAI offers an effective corner kick retrieval
system. TacticAI achieves these results despite the limited availability of
gold-standard data, achieving data efficiency through geometric deep learning.
</p>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10555" title="Abstract">arXiv:2310.10555</a> [<a href="/pdf/2310.10555" title="Download PDF">pdf</a>, <a href="/format/2310.10555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Population-based wind farm monitoring based on a spatial autoregressive  approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">W. Lin</a>, 
<a href="/search/cs?searchtype=author&query=Worden%2C+K">K. Worden</a>, 
<a href="/search/cs?searchtype=author&query=Cross%2C+E+J">E.J. Cross</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, submitted to the Modern Practice in Stress and Vibration Analysis (MPSVA) 2022 Conference Proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">An important challenge faced by wind farm operators is to reduce operation
and maintenance cost. Structural health monitoring provides a means of cost
reduction through minimising unnecessary maintenance trips as well as
prolonging turbine service life. Population-based structural health monitoring
can further reduce the cost of health monitoring systems by implementing one
system for multiple structures (i.e.~turbines). At the same time, shared data
within a population of structures may improve the predictions of structural
behaviour. To monitor turbine performance at a population/farm level, an
important initial step is to construct a model that describes the behaviour of
all turbines under normal conditions. This paper proposes a population-level
model that explicitly captures the spatial and temporal correlations (between
turbines) induced by the wake effect. The proposed model is a Gaussian
process-based spatial autoregressive model, named here a GP-SPARX model. This
approach is developed since (a) it reflects our physical understanding of the
wake effect, and (b) it benefits from a stochastic data-based learner. A case
study is provided to demonstrate the capability of the GP-SPARX model in
capturing spatial and temporal variations as well as its potential
applicability in a health monitoring system.
</p>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10556" title="Abstract">arXiv:2310.10556</a> [<a href="/pdf/2310.10556" title="Download PDF">pdf</a>, <a href="/format/2310.10556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample Complexity of Preference-Based Nonparametric Off-Policy  Evaluation with Deep Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiang Ji</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minshuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">A recently popular approach to solving reinforcement learning is with data
from human preferences. In fact, human preference data are now used with
classic reinforcement learning algorithms such as actor-critic methods, which
involve evaluating an intermediate policy over a reward learned from human
preference data with distribution shift, known as off-policy evaluation (OPE).
Such algorithm includes (i) learning reward function from human preference
dataset, and (ii) learning expected cumulative reward of a target policy.
Despite the huge empirical success, existing OPE methods with preference data
often lack theoretical understanding and rely heavily on heuristics. In this
paper, we study the sample efficiency of OPE with human preference and
establish a statistical guarantee for it. Specifically, we approach OPE by
learning the value function by fitted-Q-evaluation with a deep neural network.
By appropriately selecting the size of a ReLU network, we show that one can
leverage any low-dimensional manifold structure in the Markov decision process
and obtain a sample-efficient estimator without suffering from the curse of
high data ambient dimensionality. Under the assumption of high reward
smoothness, our results \textit{almost align with the classical OPE results
with observable reward data}. To the best of our knowledge, this is the first
result that establishes a \textit{provably efficient} guarantee for off-policy
evaluation with RLHF.
</p>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10560" title="Abstract">arXiv:2310.10560</a> [<a href="/pdf/2310.10560" title="Download PDF">pdf</a>, <a href="/format/2310.10560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the Imagenets of ML4EDA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+A+B">Animesh Basak Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Thakur%2C+S">Shailja Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Pearce%2C+H">Hammond Pearce</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+R">Ramesh Karri</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Siddharth Garg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Invited paper, ICCAD 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICCAD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Programming Languages (cs.PL)

</div>
<p class="mathjax">Despite the growing interest in ML-guided EDA tools from RTL to GDSII, there
are no standard datasets or prototypical learning tasks defined for the EDA
problem domain. Experience from the computer vision community suggests that
such datasets are crucial to spur further progress in ML for EDA. Here we
describe our experience curating two large-scale, high-quality datasets for
Verilog code generation and logic synthesis. The first, VeriGen, is a dataset
of Verilog code collected from GitHub and Verilog textbooks. The second,
OpenABC-D, is a large-scale, labeled dataset designed to aid ML for logic
synthesis tasks. The dataset consists of 870,000 And-Inverter-Graphs (AIGs)
produced from 1500 synthesis runs on a large number of open-source hardware
projects. In this paper we will discuss challenges in curating, maintaining and
growing the size and scale of these datasets. We will also touch upon questions
of dataset quality and security, and the use of novel data augmentation tools
that are tailored for the hardware domain.
</p>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10563" title="Abstract">arXiv:2310.10563</a> [<a href="/pdf/2310.10563" title="Download PDF">pdf</a>, <a href="/format/2310.10563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RefConv: Re-parameterized Refocusing Convolution for Powerful ConvNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhicheng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xiaohan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qiu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xun Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose Re-parameterized Refocusing Convolution (RefConv) as a replacement
for regular convolutional layers, which is a plug-and-play module to improve
the performance without any inference costs. Specifically, given a pre-trained
model, RefConv applies a trainable Refocusing Transformation to the basis
kernels inherited from the pre-trained model to establish connections among the
parameters. For example, a depth-wise RefConv can relate the parameters of a
specific channel of convolution kernel to the parameters of the other kernel,
i.e., make them refocus on the other parts of the model they have never
attended to, rather than focus on the input features only. From another
perspective, RefConv augments the priors of existing model structures by
utilizing the representations encoded in the pre-trained parameters as the
priors and refocusing on them to learn novel representations, thus further
enhancing the representational capacity of the pre-trained model. Experimental
results validated that RefConv can improve multiple CNN-based models by a clear
margin on image classification (up to 1.47% higher top-1 accuracy on ImageNet),
object detection and semantic segmentation without introducing any extra
inference costs or altering the original model structure. Further studies
demonstrated that RefConv can reduce the redundancy of channels and smooth the
loss landscape, which explains its effectiveness.
</p>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10565" title="Abstract">arXiv:2310.10565</a> [<a href="/pdf/2310.10565" title="Download PDF">pdf</a>, <a href="/format/2310.10565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HelmSim: Learning Helmholtz Dynamics for Interpretable Fluid Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+L">Lanxiang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haixu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuezhou Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Fluid simulation is a long-standing challenge due to the intrinsic
high-dimensional non-linear dynamics. Previous methods usually utilize the
non-linear modeling capability of deep models to directly estimate velocity
fields for future prediction. However, skipping over inherent physical
properties but directly learning superficial velocity fields will overwhelm the
model from generating precise or physics-reliable results. In this paper, we
propose the HelmSim toward an accurate and interpretable simulator for fluid.
Inspired by the Helmholtz theorem, we design a HelmDynamic block to learn the
Helmholtz dynamics, which decomposes fluid dynamics into more solvable
curl-free and divergence-free parts, physically corresponding to potential and
stream functions of fluid. By embedding the HelmDynamic block into a Multiscale
Integration Network, HelmSim can integrate learned Helmholtz dynamics along
temporal dimension in multiple spatial scales to yield future fluid. Comparing
with previous velocity estimating methods, HelmSim is faithfully derived from
Helmholtz theorem and ravels out complex fluid dynamics with physically
interpretable evidence. Experimentally, our proposed HelmSim achieves the
consistent state-of-the-art in both numerical simulated and real-world observed
benchmarks, even for scenarios with complex boundaries.
</p>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10567" title="Abstract">arXiv:2310.10567</a> [<a href="/pdf/2310.10567" title="Download PDF">pdf</a>, <a href="/format/2310.10567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder  for Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jingcheng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Retrieval-augmented language models show promise in addressing issues like
outdated information and hallucinations in language models (LMs). However,
current research faces two main problems: 1) determining what information to
retrieve, and 2) effectively combining retrieved information during generation.
We argue that valuable retrieved information should not only be related to the
current source text but also consider the future target text, given the nature
of LMs that model future tokens. Moreover, we propose that aggregation using
latent variables derived from a compact latent space is more efficient than
utilizing explicit raw text, which is limited by context length and susceptible
to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model
built upon the variational auto-encoder (VAE). It encodes the text corpus into
a latent space, capturing current and future information from both source and
target text. Additionally, we leverage the VAE to initialize the latent space
and adopt the probabilistic form of the retrieval generation paradigm by
expanding the Gaussian prior distribution into a Gaussian mixture distribution.
Theoretical analysis provides an optimizable upper bound for RegaVAE.
Experimental results on various datasets demonstrate significant improvements
in text generation quality and hallucination removal.
</p>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10570" title="Abstract">arXiv:2310.10570</a> [<a href="/pdf/2310.10570" title="Download PDF">pdf</a>, <a href="/format/2310.10570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Position Bias in Summarization with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravaut%2C+M">Mathieu Ravaut</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Aixin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N+F">Nancy F. Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) excel in zero-shot abstractive summarization
tasks, delivering fluent and pertinent summaries. Recent advancements have
extended their capabilities to handle long-input contexts, surpassing token
limits of 32k or more. However, in the realm of multi-document question
answering, language models exhibit uneven utilization of their input context.
They tend to favor the initial and final segments, resulting in a U-shaped
performance pattern concerning where the answer is located within the input.
This bias raises concerns, particularly in summarization tasks where crucial
content may be dispersed throughout the source document(s). This paper presents
a comprehensive investigation encompassing 10 datasets, 4 LLMs, and 5
evaluation metrics to analyze how these models leverage their input for
abstractive summarization. Our findings reveal a pronounced bias towards the
introductory content (and to a lesser extent, the final content), posing
challenges for LLM performance across a range of diverse summarization
benchmarks.
</p>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10571" title="Abstract">arXiv:2310.10571</a> [<a href="/pdf/2310.10571" title="Download PDF">pdf</a>, <a href="/format/2310.10571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emerging Challenges in Personalized Medicine: Assessing Demographic  Effects on Biomedical Question Answering Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaier%2C+S">Sagi Shaier</a>, 
<a href="/search/cs?searchtype=author&query=Bennett%2C+K">Kevin Bennett</a>, 
<a href="/search/cs?searchtype=author&query=Hunter%2C+L">Lawrence Hunter</a>, 
<a href="/search/cs?searchtype=author&query=von+der+Wense%2C+K">Katharina von der Wense</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IJCNLP-AACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">State-of-the-art question answering (QA) models exhibit a variety of social
biases (e.g., with respect to sex or race), generally explained by similar
issues in their training data. However, what has been overlooked so far is that
in the critical domain of biomedicine, any unjustified change in model output
due to patient demographics is problematic: it results in the unfair treatment
of patients. Selecting only questions on biomedical topics whose answers do not
depend on ethnicity, sex, or sexual orientation, we ask the following research
questions: (RQ1) Do the answers of QA models change when being provided with
irrelevant demographic information? (RQ2) Does the answer of RQ1 differ between
knowledge graph (KG)-grounded and text-based QA systems? We find that
irrelevant demographic information change up to 15% of the answers of a
KG-grounded system and up to 23% of the answers of a text-based system,
including changes that affect accuracy. We conclude that unjustified answer
changes caused by patient demographics are a frequent phenomenon, which raises
fairness concerns and should be paid more attention to.
</p>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10573" title="Abstract">arXiv:2310.10573</a> [<a href="/pdf/2310.10573" title="Download PDF">pdf</a>, <a href="/ps/2310.10573" title="Download PostScript">ps</a>, <a href="/format/2310.10573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Content Moderation and the Formation of Online Communities: A  Theoretical Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dwork%2C+C">Cynthia Dwork</a>, 
<a href="/search/cs?searchtype=author&query=Hays%2C+C">Chris Hays</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+J">Jon Kleinberg</a>, 
<a href="/search/cs?searchtype=author&query=Raghavan%2C+M">Manish Raghavan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We study the impact of content moderation policies in online communities. In
our theoretical model, a platform chooses a content moderation policy and
individuals choose whether or not to participate in the community according to
the fraction of user content that aligns with their preferences. The effects of
content moderation, at first blush, might seem obvious: it restricts speech on
a platform. However, when user participation decisions are taken into account,
its effects can be more subtle $\unicode{x2013}$ and counter-intuitive. For
example, our model can straightforwardly demonstrate how moderation policies
may increase participation and diversify content available on the platform. In
our analysis, we explore a rich set of interconnected phenomena related to
content moderation in online communities. We first characterize the
effectiveness of a natural class of moderation policies for creating and
sustaining stable communities. Building on this, we explore how
resource-limited or ideological platforms might set policies, how communities
are affected by differing levels of personalization, and competition between
platforms. Our model provides a vocabulary and mathematically tractable
framework for analyzing platform decisions about content moderation.
</p>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10575" title="Abstract">arXiv:2310.10575</a> [<a href="/pdf/2310.10575" title="Download PDF">pdf</a>, <a href="/format/2310.10575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matching the Neuronal Representations of V1 is Necessary to Improve  Robustness in CNNs with V1-like Front-ends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barbulescu%2C+R">Ruxandra Barbulescu</a>, 
<a href="/search/cs?searchtype=author&query=Marques%2C+T">Tiago Marques</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+A+L">Arlindo L. Oliveira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">While some convolutional neural networks (CNNs) have achieved great success
in object recognition, they struggle to identify objects in images corrupted
with different types of common noise patterns. Recently, it was shown that
simulating computations in early visual areas at the front of CNNs leads to
improvements in robustness to image corruptions. Here, we further explore this
result and show that the neuronal representations that emerge from precisely
matching the distribution of RF properties found in primate V1 is key for this
improvement in robustness. We built two variants of a model with a front-end
modeling the primate primary visual cortex (V1): one sampling RF properties
uniformly and the other sampling from empirical biological distributions. The
model with the biological sampling has a considerably higher robustness to
image corruptions that the uniform variant (relative difference of 8.72%).
While similar neuronal sub-populations across the two variants have similar
response properties and learn similar downstream weights, the impact on
downstream processing is strikingly different. This result sheds light on the
origin of the improvements in robustness observed in some biologically-inspired
models, pointing to the need of precisely mimicking the neuronal
representations found in the primate brain.
</p>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10576" title="Abstract">arXiv:2310.10576</a> [<a href="/pdf/2310.10576" title="Download PDF">pdf</a>, <a href="/ps/2310.10576" title="Download PostScript">ps</a>, <a href="/format/2310.10576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicative models of set theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maschio%2C+S">Samuele Maschio</a>, 
<a href="/search/cs?searchtype=author&query=Miquel%2C+A">Alexandre Miquel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2301.11740">arXiv:2301.11740</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">In this paper we show that using implicative algebras one can produce models
of set theory generalizing Heyting/Boolean-valued models and realizability
models of (I)ZF, both in intuitionistic and classical logic. This has as
consequence that any topos which is obtained from a Set-based tripos as the
result of the tripos-to-topos construction hosts a model of intuitionistic or
classical set theory, provided a large enough strongly inaccessible cardinal
exists.
</p>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10583" title="Abstract">arXiv:2310.10583</a> [<a href="/pdf/2310.10583" title="Download PDF">pdf</a>, <a href="/format/2310.10583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who Are All The Stochastic Parrots Imitating? They Should Tell Us!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaier%2C+S">Sagi Shaier</a>, 
<a href="/search/cs?searchtype=author&query=Hunter%2C+L+E">Lawrence E. Hunter</a>, 
<a href="/search/cs?searchtype=author&query=von+der+Wense%2C+K">Katharina von der Wense</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IJCNLP-AACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Both standalone language models (LMs) as well as LMs within downstream-task
systems have been shown to generate statements which are factually untrue. This
problem is especially severe for low-resource languages, where training data is
scarce and of worse quality than for high-resource languages. In this opinion
piece, we argue that LMs in their current state will never be fully trustworthy
in critical settings and suggest a possible novel strategy to handle this
issue: by building LMs such that can cite their sources - i.e., point a user to
the parts of their training data that back up their outputs. We first discuss
which current NLP tasks would or would not benefit from such models. We then
highlight the expected benefits such models would bring, e.g., quick
verifiability of statements. We end by outlining the individual tasks that
would need to be solved on the way to developing LMs with the ability to cite.
We hope to start a discussion about the field's current approach to building
LMs, especially for low-resource languages, and the role of the training data
in explaining model generations.
</p>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10585" title="Abstract">arXiv:2310.10585</a> [<a href="/pdf/2310.10585" title="Download PDF">pdf</a>, <a href="/format/2310.10585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporally Robust Multi-Agent STL Motion Planning in Continuous Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verhagen%2C+J">Joris Verhagen</a>, 
<a href="/search/cs?searchtype=author&query=Lindemann%2C+L">Lars Lindemann</a>, 
<a href="/search/cs?searchtype=author&query=Tumova%2C+J">Jana Tumova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Signal Temporal Logic (STL) is a formal language over continuous-time signals
(such as trajectories of a multi-agent system) that allows for the
specification of complex spatial and temporal system requirements (such as
staying sufficiently close to each other within certain time intervals). To
promote robustness in multi-agent motion planning with such complex
requirements, we consider motion planning with the goal of maximizing the
temporal robustness of their joint STL specification, i.e. maximizing the
permissible time shifts of each agent's trajectory while still satisfying the
STL specification. Previous methods presented temporally robust motion planning
and control in a discrete-time Mixed Integer Linear Programming (MILP)
optimization scheme. In contrast, we parameterize the trajectory by continuous
B\'ezier curves, where the curvature and the time-traversal of the trajectory
are parameterized individually. We show an algorithm generating continuous-time
temporally robust trajectories and prove soundness of our approach. Moreover,
we empirically show that our parametrization realizes this with a considerable
speed-up compared to state-of-the-art methods based on constant interval time
discretization.
</p>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10586" title="Abstract">arXiv:2310.10586</a> [<a href="/pdf/2310.10586" title="Download PDF">pdf</a>, <a href="/format/2310.10586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools  for Video-based Texts Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Ji Qi</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Kaixuan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Duokang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Building models that generate textual responses to user instructions for
videos is a practical and challenging topic, as it requires both vision
understanding and knowledge reasoning. Compared to language and image
modalities, training efficiency remains a serious problem as existing studies
train models on massive sparse videos aligned with brief descriptions. In this
paper, we introduce BiLL-VTG, a fast adaptive framework that leverages large
language models (LLMs) to reasoning on videos based on essential lightweight
visual tools. Specifically, we reveal the key to response specific instructions
is the concentration on relevant video events, and utilize two visual tools of
structured scene graph generation and descriptive image caption generation to
gather and represent the events information. Thus, a LLM equipped with world
knowledge is adopted as the reasoning agent to achieve the response by
performing multiple reasoning steps on specified video events.To address the
difficulty of specifying events from agent, we further propose an
Instruction-oriented Video Events Recognition (InsOVER) algorithm based on the
efficient Hungarian matching to localize corresponding video events using
linguistic instructions, enabling LLMs to interact with long videos. Extensive
experiments on two typical video-based texts generations tasks show that our
tuning-free framework outperforms the pre-trained models including
Flamingo-80B, to achieve the state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10587" title="Abstract">arXiv:2310.10587</a> [<a href="/pdf/2310.10587" title="Download PDF">pdf</a>, <a href="/ps/2310.10587" title="Download PostScript">ps</a>, <a href="/format/2310.10587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Tri-Level Optimization Model for Interdependent Infrastructure Network  Resilience Against Compound Hazard Events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Oster%2C+M+R">Matthew R. Oster</a>, 
<a href="/search/eess?searchtype=author&query=Amburg%2C+I">Ilya Amburg</a>, 
<a href="/search/eess?searchtype=author&query=Chatterjee%2C+S">Samrat Chatterjee</a>, 
<a href="/search/eess?searchtype=author&query=Eisenberg%2C+D+A">Daniel A. Eisenberg</a>, 
<a href="/search/eess?searchtype=author&query=Thomas%2C+D+G">Dennis G. Thomas</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+F">Feng Pan</a>, 
<a href="/search/eess?searchtype=author&query=Ganguly%2C+A+R">Auroop R. Ganguly</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Resilient operation of interdependent infrastructures against compound hazard
events is essential for maintaining societal well-being. To address consequence
assessment challenges in this problem space, we propose a novel tri-level
optimization model applied to a proof-of-concept case study with fuel
distribution and transportation networks -- encompassing one realistic network;
one fictitious, yet realistic network; as well as networks drawn from three
synthetic distributions. Mathematically, our approach takes the form of a
defender-attacker-defender (DAD) model -- a multi-agent tri-level optimization,
comprised of a defender, attacker, and an operator acting in sequence. Here,
our notional operator may choose proxy actions to operate an interdependent
system comprised of fuel terminals and gas stations (functioning as supplies)
and a transportation network with traffic flow (functioning as demand) to
minimize unmet demand at gas stations. A notional attacker aims to
hypothetically disrupt normal operations by reducing supply at the supply
terminals, and the notional defender aims to identify best proxy defense policy
options which include hardening supply terminals or allowing alternative
distribution methods such as trucking reserve supplies. We solve our DAD
formulation at a metropolitan scale and present practical defense policy
insights against hypothetical compound hazards. We demonstrate the
generalizability of our framework by presenting results for a realistic
network; a fictitious, yet realistic network; as well as for three networks
drawn from synthetic distributions. Additionally, we demonstrate the
scalability of the framework by investigating runtime performance as a function
of the network size. Steps for future research are also discussed.
</p>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10590" title="Abstract">arXiv:2310.10590</a> [<a href="/pdf/2310.10590" title="Download PDF">pdf</a>, <a href="/format/2310.10590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mastering the Task of Open Information Extraction with Large Language  Models and Consistent Reasoning Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Ji Qi</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Kaixuan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaozhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Kaisheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bin Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Open Information Extraction (OIE) aims to extract objective structured
knowledge from natural texts, which has attracted growing attention to build
dedicated models with human experience. As the large language models (LLMs)
have exhibited remarkable in-context learning capabilities, a question arises
as to whether the task of OIE can be effectively tackled with this paradigm? In
this paper, we explore solving the OIE problem by constructing an appropriate
reasoning environment for LLMs. Specifically, we first propose a method to
effectively estimate the discrepancy of syntactic distribution between a LLM
and test samples, which can serve as correlation evidence for preparing
positive demonstrations. Upon the evidence, we introduce a simple yet effective
mechanism to establish the reasoning environment for LLMs on specific tasks.
Without bells and whistles, experimental results on the standard CaRB benchmark
demonstrate that our $6$-shot approach outperforms state-of-the-art supervised
method, achieving an $55.3$ $F_1$ score. Further experiments on TACRED and
ACE05 show that our method can naturally generalize to other information
extraction tasks, resulting in improvements of $5.7$ and $6.8$ $F_1$ scores,
respectively.
</p>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10591" title="Abstract">arXiv:2310.10591</a> [<a href="/pdf/2310.10591" title="Download PDF">pdf</a>, <a href="/format/2310.10591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting and Controlling Vision Foundation Models via Text  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haozhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junfeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Vondrick%2C+C">Carl Vondrick</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+C">Chengzhi Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large-scale pre-trained vision foundation models, such as CLIP, have become
de facto backbones for various vision tasks. However, due to their black-box
nature, understanding the underlying rules behind these models' predictions and
controlling model behaviors have remained open challenges. We present a
framework for interpreting vision transformer's latent tokens with natural
language. Given a latent token, our framework retains its semantic information
to the final layer using transformer's local operations and retrieves the
closest text for explanation. Our approach enables understanding of model
visual reasoning procedure without needing additional model training or data
collection. Based on the obtained interpretations, our framework allows for
model editing that controls model reasoning behaviors and improves model
robustness against biases and spurious correlations.
</p>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10594" title="Abstract">arXiv:2310.10594</a> [<a href="/pdf/2310.10594" title="Download PDF">pdf</a>, <a href="/format/2310.10594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motion2Language, Unsupervised learning of synchronized semantic motion  segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radouane%2C+K">Karim Radouane</a>, 
<a href="/search/cs?searchtype=author&query=Tchechmedjiev%2C+A">Andon Tchechmedjiev</a>, 
<a href="/search/cs?searchtype=author&query=Ranwez%2C+S">Sylvie Ranwez</a>, 
<a href="/search/cs?searchtype=author&query=Lagarde%2C+J">Julien Lagarde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In this paper, we investigate building a sequence to sequence architecture
for motion to language translation and synchronization. The aim is to translate
motion capture inputs into English natural-language descriptions, such that the
descriptions are generated synchronously with the actions performed, enabling
semantic segmentation as a byproduct, but without requiring synchronized
training data. We propose a new recurrent formulation of local attention that
is suited for synchronous/live text generation, as well as an improved motion
encoder architecture better suited to smaller data and for synchronous
generation. We evaluate both contributions in individual experiments, using the
standard BLEU4 metric, as well as a simple semantic equivalence measure, on the
KIT motion language dataset. In a follow-up experiment, we assess the quality
of the synchronization of generated text in our proposed approaches through
multiple evaluation metrics. We find that both contributions to the attention
mechanism and the encoder architecture additively improve the quality of
generated text (BLEU and semantic equivalence), but also of synchronization.
Our code will be made available at
\url{https://github.com/rd20karim/M2T-Segmentation/tree/main}
</p>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10597" title="Abstract">arXiv:2310.10597</a> [<a href="/pdf/2310.10597" title="Download PDF">pdf</a>, <a href="/format/2310.10597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting multi-GNSS Navigation for UAVs -- An Equivariant Filtering  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scheiber%2C+M">Martin Scheiber</a>, 
<a href="/search/cs?searchtype=author&query=Fornasier%2C+A">Alessandro Fornasier</a>, 
<a href="/search/cs?searchtype=author&query=Brommer%2C+C">Christian Brommer</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+S">Stephan Weiss</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print version, accepted to IEEE ICAR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this work, we explore the recent advances in equivariant filtering for
inertial navigation systems to improve state estimation for uncrewed aerial
vehicles (UAVs). Traditional state-of-the-art estimation methods, e.g., the
multiplicative Kalman filter (MEKF), have some limitations concerning their
consistency, errors in the initial state estimate, and convergence performance.
Symmetry-based methods, such as the equivariant filter (EqF), offer significant
advantages for these points by exploiting the mathematical properties of the
system - its symmetry. These filters yield faster convergence rates and
robustness to wrong initial state estimates through their error definition. To
demonstrate the usability of EqFs, we focus on the sensor-fusion problem with
the most common sensors in outdoor robotics: global navigation satellite system
(GNSS) sensors and an inertial measurement unit (IMU). We provide an
implementation of such an EqF leveraging the semi-direct product of the
symmetry group to derive the filter equations. To validate the practical
usability of EqFs in real-world scenarios, we evaluate our method using data
from all outdoor runs of the INSANE Dataset. Our results demonstrate the
performance improvements of the EqF in real-world environments, highlighting
its potential for enhancing state estimation for UAVs.
</p>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10603" title="Abstract">arXiv:2310.10603</a> [<a href="/pdf/2310.10603" title="Download PDF">pdf</a>, <a href="/format/2310.10603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Power of Graph Neural Networks in Solving Linear  Optimization Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chendi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Ch%C3%A9telat%2C+D">Didier Ch&#xe9;telat</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+C">Christopher Morris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recently, machine learning, particularly message-passing graph neural
networks (MPNNs), has gained traction in enhancing exact optimization
algorithms. For example, MPNNs speed up solving mixed-integer optimization
problems by imitating computational intensive heuristics like strong branching,
which entails solving multiple linear optimization problems (LPs). Despite the
empirical success, the reasons behind MPNNs' effectiveness in emulating linear
optimization remain largely unclear. Here, we show that MPNNs can simulate
standard interior-point methods for LPs, explaining their practical success.
Furthermore, we highlight how MPNNs can serve as a lightweight proxy for
solving LPs, adapting to a given problem instance distribution. Empirically, we
show that MPNNs solve LP relaxations of standard combinatorial optimization
problems close to optimality, often surpassing conventional solvers and
competing approaches in solving time.
</p>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10606" title="Abstract">arXiv:2310.10606</a> [<a href="/pdf/2310.10606" title="Download PDF">pdf</a>, <a href="/format/2310.10606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BayRnTune: Adaptive Bayesian Domain Randomization via Strategic  Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tianle Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sontakke%2C+N">Nitish Sontakke</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+K+N">K. Niranjan Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Essa%2C+I">Irfan Essa</a>, 
<a href="/search/cs?searchtype=author&query=Nikolaidis%2C+S">Stefanos Nikolaidis</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+D+W">Dennis W. Hong</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+S">Sehoon Ha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Domain randomization (DR), which entails training a policy with randomized
dynamics, has proven to be a simple yet effective algorithm for reducing the
gap between simulation and the real world. However, DR often requires careful
tuning of randomization parameters. Methods like Bayesian Domain Randomization
(Bayesian DR) and Active Domain Randomization (Adaptive DR) address this issue
by automating parameter range selection using real-world experience. While
effective, these algorithms often require long computation time, as a new
policy is trained from scratch every iteration. In this work, we propose
Adaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune),
which inherits the spirit of BayRn but aims to significantly accelerate the
learning processes by fine-tuning from previously learned policy. This idea
leads to a critical question: which previous policy should we use as a prior
during fine-tuning? We investigated four different fine-tuning strategies and
compared them against baseline algorithms in five simulated environments,
ranging from simple benchmark tasks to more complex legged robot environments.
Our analysis demonstrates that our method yields better rewards in the same
amount of timesteps compared to vanilla domain randomization or Bayesian DR.
</p>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10608" title="Abstract">arXiv:2310.10608</a> [<a href="/pdf/2310.10608" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality control using convolutional neural networks applied to samples  of very small size
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatzimichail%2C+R+A">Rallou A. Chatzimichail</a> (1), 
<a href="/search/cs?searchtype=author&query=Hatjimihail%2C+A+T">Aristides T. Hatjimihail</a> (1) ((1) Hellenic Complex Systems Laboratory, Drama, Greece)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Article: 21 pages, 5 figures, 8 tables. Appendix: 166 pages, 178 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Although there is extensive literature on the application of artificial
neural networks (NNs) in quality control (QC), to monitor the conformity of a
process to quality specifications, at least five QC measurements are required,
increasing the related cost. To explore the application of neural networks to
samples of QC measurements of very small size, four one-dimensional (1-D)
convolutional neural networks (CNNs) were designed, trained, and tested with
datasets of $ n $-tuples of simulated standardized normally distributed QC
measurements, for $ 1 \leq n \leq 4$. The designed neural networks were
compared to statistical QC functions with equal probabilities for false
rejection, applied to samples of the same size. When the $ n $-tuples included
at least two QC measurements distributed as $ \mathcal{N}(\mu, \sigma^2) $,
where $ 0.2 &lt; |\mu| \leq 6.0 $, and $ 1.0 &lt; \sigma \leq 7.0 $, the designed
neural networks outperformed the respective statistical QC functions.
Therefore, 1-D CNNs applied to samples of 2-4 quality control measurements can
be used to increase the probability of detection of the nonconformity of a
process to the quality specifications, with lower cost.
</p>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10610" title="Abstract">arXiv:2310.10610</a> [<a href="/pdf/2310.10610" title="Download PDF">pdf</a>, <a href="/format/2310.10610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Assistive Robustness Via the Natural-Adversarial Frontier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J+Z">Jerry Zhi-Yang He</a>, 
<a href="/search/cs?searchtype=author&query=Erickson%2C+Z">Zackory Erickson</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+D+S">Daniel S. Brown</a>, 
<a href="/search/cs?searchtype=author&query=Dragan%2C+A+D">Anca D. Dragan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Our ultimate goal is to build robust policies for robots that assist people.
What makes this hard is that people can behave unexpectedly at test time,
potentially interacting with the robot outside its training distribution and
leading to failures. Even just measuring robustness is a challenge. Adversarial
perturbations are the default, but they can paint the wrong picture: they can
correspond to human motions that are unlikely to occur during natural
interactions with people. A robot policy might fail under small adversarial
perturbations but work under large natural perturbations. We propose that
capturing robustness in these interactive settings requires constructing and
analyzing the entire natural-adversarial frontier: the Pareto-frontier of human
policies that are the best trade-offs between naturalness and low robot
performance. We introduce RIGID, a method for constructing this frontier by
training adversarial human policies that trade off between minimizing robot
reward and acting human-like (as measured by a discriminator). On an Assistive
Gym task, we use RIGID to analyze the performance of standard collaborative
Reinforcement Learning, as well as the performance of existing methods meant to
increase robustness. We also compare the frontier RIGID identifies with the
failures identified in expert adversarial interaction, and with
naturally-occurring failures during user interaction. Overall, we find evidence
that RIGID can provide a meaningful measure of robustness predictive of
deployment performance, and uncover failure cases in human-robot interaction
that are difficult to find manually. https://ood-human.github.io.
</p>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10611" title="Abstract">arXiv:2310.10611</a> [<a href="/pdf/2310.10611" title="Download PDF">pdf</a>, <a href="/format/2310.10611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IW-GAE: Importance weighted group accuracy estimation for improved  calibration and model selection in unsupervised domain adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joo%2C+T">Taejong Joo</a>, 
<a href="/search/cs?searchtype=author&query=Klabjan%2C+D">Diego Klabjan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Reasoning about a model's accuracy on a test sample from its confidence is a
central problem in machine learning, being connected to important applications
such as uncertainty representation, model selection, and exploration. While
these connections have been well-studied in the i.i.d. settings, distribution
shifts pose significant challenges to the traditional methods. Therefore, model
calibration and model selection remain challenging in the unsupervised domain
adaptation problem--a scenario where the goal is to perform well in a
distribution shifted domain without labels. In this work, we tackle
difficulties coming from distribution shifts by developing a novel importance
weighted group accuracy estimator. Specifically, we formulate an optimization
problem for finding an importance weight that leads to an accurate group
accuracy estimation in the distribution shifted domain with theoretical
analyses. Extensive experiments show the effectiveness of group accuracy
estimation on model calibration and model selection. Our results emphasize the
significance of group accuracy estimation for addressing challenges in
unsupervised domain adaptation, as an orthogonal improvement direction with
improving transferability of accuracy.
</p>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10612" title="Abstract">arXiv:2310.10612</a> [<a href="/pdf/2310.10612" title="Download PDF">pdf</a>, <a href="/format/2310.10612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Filtering for Homography Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Del+Castillo+Bernal%2C+A">Arturo Del Castillo Bernal</a>, 
<a href="/search/cs?searchtype=author&query=Decoste%2C+P">Philippe Decoste</a>, 
<a href="/search/cs?searchtype=author&query=Forbes%2C+J+R">James Richard Forbes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages and 6 figures, to be published in IEEE Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper considers homography estimation in a Bayesian filtering framework
using rate gyro and camera measurements. The use of rate gyro measurements
facilitates a more reliable estimate of homography in the presence of
occlusions, while a Bayesian filtering approach generates both a homography
estimate along with an uncertainty. Uncertainty information opens the door to
adaptive filtering approaches, post-processing procedures, and safety
protocols. In particular, herein an iterative extended Kalman filter and an
interacting multiple model (IMM) filter are tested using both simulated and
experimental datasets. The IMM is shown to have good consistency properties and
better overall performance when compared to the state-of-the-art homography
nonlinear deterministic observer in both simulations and experiments.
</p>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10613" title="Abstract">arXiv:2310.10613</a> [<a href="/pdf/2310.10613" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilization of uncertain linear distributed delay systems with  dissipativity constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Feng%2C+Q">Qian Feng</a>, 
<a href="/search/eess?searchtype=author&query=Nguang%2C+S+K">Sing Kiong Nguang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://doi.org/10.1016/j.sysconle.2016.06.017.">this https URL</a> arXiv admin note: text overlap with <a href="/abs/2204.08353">arXiv:2204.08353</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> https://doi.org/10.1016/j.sysconle.2016.06.017
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper examines the problem of stabilizing linear distributed delay
systems with nonlinear distributed delay kernels and dissipativity constraints.
Specifically, the nonlinear distributed kernel includes functions such as
polynomials, trigonometric and exponential functions. By constructing a
Liapunov-Krasovski\v{i} functional related to the distributed kernels,
sufficient conditions for the existence of a state feedback controller which
stabilizes the uncertain distributed delay systems with dissipativity
constraints are given in terms of linear matrix inequalities (LMIs). In
contrast to existing methods, the proposed scenario is less conservative or
requires a smaller number of decision variables based on the application of a
newly derived integral inequality. Finally, numerical examples are presented to
demonstrate the validity and effectiveness of the proposed methodology.
</p>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10616" title="Abstract">arXiv:2310.10616</a> [<a href="/pdf/2310.10616" title="Download PDF">pdf</a>, <a href="/format/2310.10616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Do Transformers Learn In-Context Beyond Simple Functions? A Case  Study on Learning with Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tianyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+S">Song Mei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Savarese%2C+S">Silvio Savarese</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yu Bai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While large language models based on the transformer architecture have
demonstrated remarkable in-context learning (ICL) capabilities, understandings
of such capabilities are still in an early stage, where existing theory and
mechanistic understanding focus mostly on simple scenarios such as learning
simple function classes. This paper takes initial steps on understanding ICL in
more complex scenarios, by studying learning with representations. Concretely,
we construct synthetic in-context learning problems with a compositional
structure, where the label depends on the input through a possibly complex but
fixed representation function, composed with a linear function that differs in
each instance. By construction, the optimal ICL algorithm first transforms the
inputs by the representation function, and then performs linear ICL on top of
the transformed dataset. We show theoretically the existence of transformers
that approximately implement such algorithms with mild depth and size.
Empirically, we find trained transformers consistently achieve near-optimal ICL
performance in this setting, and exhibit the desired dissection where lower
layers transforms the dataset and upper layers perform linear ICL. Through
extensive probing and a new pasting experiment, we further reveal several
mechanisms within the trained transformers, such as concrete copying behaviors
on both the inputs and the representations, linear ICL capability of the upper
layers alone, and a post-ICL representation selection mechanism in a harder
mixture setting. These observed mechanisms align well with our theory and may
shed light on how transformers perform ICL in more realistic scenarios.
</p>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10618" title="Abstract">arXiv:2310.10618</a> [<a href="/pdf/2310.10618" title="Download PDF">pdf</a>, <a href="/format/2310.10618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\mathcal{H}_2$-optimal Reduced-order Modeling for Structured Linear  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mlinari%C4%87%2C+P">Petar Mlinari&#x107;</a>, 
<a href="/search/math?searchtype=author&query=Benner%2C+P">Peter Benner</a>, 
<a href="/search/math?searchtype=author&query=Gugercin%2C+S">Serkan Gugercin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">Interpolatory necessary optimality conditions for $\mathcal{H}_2$-optimal
reduced-order modeling of unstructured linear time-invariant (LTI) systems are
well-known. Based on previous work on $\mathcal{L}_2$-optimal reduced-order
modeling of stationary parametric problems, in this paper we develop and
investigate optimality conditions for $\mathcal{H}_2$-optimal reduced-order
modeling of structured LTI systems, in particular, for second-order,
port-Hamiltonian, and time-delay systems. We show that across all these
different structured settings, bitangential Hermite interpolation is the common
form for optimality, thus proving a unifying optimality framework for
structured reduced-order modeling.
</p>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10623" title="Abstract">arXiv:2310.10623</a> [<a href="/pdf/2310.10623" title="Download PDF">pdf</a>, <a href="/format/2310.10623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Summaries with Controllable Readability Levels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+L+F+R">Leonardo F. R. Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Dreyer%2C+M">Markus Dreyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as an EMNLP 2023 main paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Readability refers to how easily a reader can understand a written text.
Several factors affect the readability level, such as the complexity of the
text, its subject matter, and the reader's background knowledge. Generating
summaries based on different readability levels is critical for enabling
knowledge consumption by diverse audiences. However, current text generation
approaches lack refined control, resulting in texts that are not customized to
readers' proficiency levels. In this work, we bridge this gap and study
techniques to generate summaries at specified readability levels. Unlike
previous methods that focus on a specific readability level (e.g., lay
summarization), we generate summaries with fine-grained control over their
readability. We develop three text generation techniques for controlling
readability: (1) instruction-based readability control, (2) reinforcement
learning to minimize the gap between requested and observed readability and (3)
a decoding approach that uses lookahead to estimate the readability of upcoming
decoding steps. We show that our generation methods significantly improve
readability control on news summarization (CNN/DM dataset), as measured by
various readability metrics and human judgement, establishing strong baselines
for controllable readability in summarization.
</p>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10624" title="Abstract">arXiv:2310.10624</a> [<a href="/pdf/2310.10624" title="Download PDF">pdf</a>, <a href="/format/2310.10624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and  View-Change Human-Centric Video Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jia-Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yan-Pei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J+Z">Jay Zhangjie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+W">Weijia Mao</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuchao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Keppo%2C+J">Jussi Keppo</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://showlab.github.io/DynVideo-E/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite remarkable research advances in diffusion-based video editing,
existing methods are limited to short-length videos due to the contradiction
between long-range consistency and frame-wise editing. Recent approaches
attempt to tackle this challenge by introducing video-2D representations to
degrade video editing to image editing. However, they encounter significant
difficulties in handling large-scale motion- and view-change videos especially
for human-centric videos. This motivates us to introduce the dynamic Neural
Radiance Fields (NeRF) as the human-centric video representation to ease the
video editing problem to a 3D space editing task. As such, editing can be
performed in the 3D spaces and propagated to the entire video via the
deformation field. To provide finer and direct controllable editing, we propose
the image-based 3D space editing pipeline with a set of effective designs.
These include multi-view multi-pose Score Distillation Sampling (SDS) from both
2D personalized diffusion priors and 3D diffusion priors, reconstruction losses
on the reference image, text-guided local parts super-resolution, and style
transfer for 3D background space. Extensive experiments demonstrate that our
method, dubbed as DynVideo-E, significantly outperforms SOTA approaches on two
challenging datasets by a large margin of 50% ~ 95% in terms of human
preference. Compelling video comparisons are provided in the project page
https://showlab.github.io/DynVideo-E/. Our code and data will be released to
the community.
</p>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10625" title="Abstract">arXiv:2310.10625</a> [<a href="/pdf/2310.10625" title="Download PDF">pdf</a>, <a href="/format/2310.10625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video Language Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengjiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Florence%2C+P">Pete Florence</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wahid%2C+A">Ayzaan Wahid</a>, 
<a href="/search/cs?searchtype=author&query=Ichter%2C+B">Brian Ichter</a>, 
<a href="/search/cs?searchtype=author&query=Sermanet%2C+P">Pierre Sermanet</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianhe Yu</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Kaelbling%2C+L">Leslie Kaelbling</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Andy Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Tompson%2C+J">Jonathan Tompson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://video-language-planning.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">We are interested in enabling visual planning for complex long-horizon tasks
in the space of generated videos and language, leveraging recent advances in
large generative models pretrained on Internet-scale data. To this end, we
present video language planning (VLP), an algorithm that consists of a tree
search procedure, where we train (i) vision-language models to serve as both
policies and value functions, and (ii) text-to-video models as dynamics models.
VLP takes as input a long-horizon task instruction and current image
observation, and outputs a long video plan that provides detailed multimodal
(video and language) specifications that describe how to complete the final
task. VLP scales with increasing computation budget where more computation time
results in improved video plans, and is able to synthesize long-horizon video
plans across different robotics domains: from multi-object rearrangement, to
multi-camera bi-arm dexterous manipulation. Generated video plans can be
translated into real robot actions via goal-conditioned policies, conditioned
on each intermediate frame of the generated video. Experiments show that VLP
substantially improves long-horizon task success rates compared to prior
methods on both simulated and real robots (across 3 hardware platforms).
</p>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10627" title="Abstract">arXiv:2310.10627</a> [<a href="/pdf/2310.10627" title="Download PDF">pdf</a>, <a href="/format/2310.10627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factored Verification: Detecting and Reducing Hallucination in Summaries  of Academic Papers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=George%2C+C">Charlie George</a>, 
<a href="/search/cs?searchtype=author&query=Stuhlm%C3%BCller%2C+A">Andreas Stuhlm&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Second Workshop on Information Extraction from Scientific Publications (WIESP) at IJCNLP-AACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Hallucination plagues even frontier LLMs--but how bad is it really for
summarizing academic papers? We evaluate Factored Verification, a simple
automated method for detecting hallucinations in abstractive summaries. This
method sets a new SotA on hallucination detection in the summarization task of
the HaluEval benchmark, achieving 76.2% accuracy. We then use this method to
estimate how often language models hallucinate when summarizing across multiple
academic papers and find 0.62 hallucinations in the average ChatGPT (16k)
summary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct
using Factored Critiques and find that this lowers the number of hallucinations
to 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations
we find are often subtle, so we advise caution when using models to synthesize
academic papers.
</p>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10628" title="Abstract">arXiv:2310.10628</a> [<a href="/pdf/2310.10628" title="Download PDF">pdf</a>, <a href="/format/2310.10628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Contamination Through the Lens of Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roberts%2C+M">Manley Roberts</a>, 
<a href="/search/cs?searchtype=author&query=Thakur%2C+H">Himanshu Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Herlihy%2C+C">Christine Herlihy</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+C">Colin White</a>, 
<a href="/search/cs?searchtype=author&query=Dooley%2C+S">Samuel Dooley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent claims about the impressive abilities of large language models (LLMs)
are often supported by evaluating publicly available benchmarks. Since LLMs
train on wide swaths of the internet, this practice raises concerns of data
contamination, i.e., evaluating on examples that are explicitly or implicitly
included in the training data. Data contamination remains notoriously
challenging to measure and mitigate, even with partial attempts like controlled
experimentation of training data, canary strings, or embedding similarities. In
this work, we conduct the first thorough longitudinal analysis of data
contamination in LLMs by using the natural experiment of training cutoffs in
GPT models to look at benchmarks released over time. Specifically, we consider
two code/mathematical problem-solving datasets, Codeforces and Project Euler,
and find statistically significant trends among LLM pass rate vs. GitHub
popularity and release date that provide strong evidence of contamination. By
open-sourcing our dataset, raw results, and evaluation framework, our work
paves the way for rigorous analyses of data contamination in modern models. We
conclude with a discussion of best practices and future steps for publicly
releasing benchmarks in the age of LLMs that train on webscale data.
</p>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10629" title="Abstract">arXiv:2310.10629</a> [<a href="/pdf/2310.10629" title="Download PDF">pdf</a>, <a href="/format/2310.10629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certainty In, Certainty Out: REVQCs for Quantum Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Helgesen%2C+H">Hannah Helgesen</a>, 
<a href="/search/cs?searchtype=author&query=Felsberg%2C+M">Michael Felsberg</a>, 
<a href="/search/cs?searchtype=author&query=Larsson%2C+J">Jan-&#xc5;ke Larsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">The field of Quantum Machine Learning (QML) has emerged recently in the hopes
of finding new machine learning protocols or exponential speedups for classical
ones. Apart from problems with vanishing gradients and efficient encoding
methods, these speedups are hard to find because the sampling nature of quantum
computers promotes either simulating computations classically or running them
many times on quantum computers in order to use approximate expectation values
in gradient calculations. In this paper, we make a case for setting high
single-sample accuracy as a primary goal. We discuss the statistical theory
which enables highly accurate and precise sample inference, and propose a
method of reversed training towards this end. We show the effectiveness of this
training method by assessing several effective variational quantum circuits
(VQCs), trained in both the standard and reversed directions, on random binary
subsets of the MNIST and MNIST Fashion datasets, on which our method provides
an increase of $10-15\%$ in single-sample inference accuracy.
</p>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10631" title="Abstract">arXiv:2310.10631</a> [<a href="/pdf/2310.10631" title="Download PDF">pdf</a>, <a href="/format/2310.10631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Llemma: An Open Language Model For Mathematics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azerbayev%2C+Z">Zhangir Azerbayev</a>, 
<a href="/search/cs?searchtype=author&query=Schoelkopf%2C+H">Hailey Schoelkopf</a>, 
<a href="/search/cs?searchtype=author&query=Paster%2C+K">Keiran Paster</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+M+D">Marco Dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=McAleer%2C+S">Stephen McAleer</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+A+Q">Albert Q. Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jia Deng</a>, 
<a href="/search/cs?searchtype=author&query=Biderman%2C+S">Stella Biderman</a>, 
<a href="/search/cs?searchtype=author&query=Welleck%2C+S">Sean Welleck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We present Llemma, a large language model for mathematics. We continue
pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web
data containing mathematics, and mathematical code, yielding Llemma. On the
MATH benchmark Llemma outperforms all known open base models, as well as the
unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is
capable of tool use and formal theorem proving without any further finetuning.
We openly release all artifacts, including 7 billion and 34 billion parameter
models, the Proof-Pile-2, and code to replicate our experiments.
</p>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10632" title="Abstract">arXiv:2310.10632</a> [<a href="/pdf/2310.10632" title="Download PDF">pdf</a>, <a href="/format/2310.10632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=O%27Donoghue%2C+O">Odhran O&#x27;Donoghue</a>, 
<a href="/search/cs?searchtype=author&query=Shtedritski%2C+A">Aleksandar Shtedritski</a>, 
<a href="/search/cs?searchtype=author&query=Ginger%2C+J">John Ginger</a>, 
<a href="/search/cs?searchtype=author&query=Abboud%2C+R">Ralph Abboud</a>, 
<a href="/search/cs?searchtype=author&query=Ghareeb%2C+A+E">Ali Essa Ghareeb</a>, 
<a href="/search/cs?searchtype=author&query=Booth%2C+J">Justin Booth</a>, 
<a href="/search/cs?searchtype=author&query=Rodriques%2C+S+G">Samuel G Rodriques</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. Dataset and code: <a href="https://github.com/bioplanner/bioplanner">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">The ability to automatically generate accurate protocols for scientific
experiments would represent a major step towards the automation of science.
Large Language Models (LLMs) have impressive capabilities on a wide range of
tasks, such as question answering and the generation of coherent text and code.
However, LLMs can struggle with multi-step problems and long-term planning,
which are crucial for designing scientific experiments. Moreover, evaluation of
the accuracy of scientific protocols is challenging, because experiments can be
described correctly in many different ways, require expert knowledge to
evaluate, and cannot usually be executed automatically. Here we present an
automatic evaluation framework for the task of planning experimental protocols,
and we introduce BioProt: a dataset of biology protocols with corresponding
pseudocode representations. To measure performance on generating scientific
protocols, we use an LLM to convert a natural language protocol into
pseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode
from a high-level description and a list of admissible pseudocode functions. We
evaluate GPT-3 and GPT-4 on this task and explore their robustness. We
externally validate the utility of pseudocode representations of text by
generating accurate novel protocols using retrieved pseudocode, and we run a
generated protocol successfully in our biological laboratory. Our framework is
extensible to the evaluation and improvement of language model planning
abilities in other areas of science or other areas that lack automatic
evaluation.
</p>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10634" title="Abstract">arXiv:2310.10634</a> [<a href="/pdf/2310.10634" title="Download PDF">pdf</a>, <a href="/format/2310.10634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenAgents: An Open Platform for Language Agents in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tianbao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhoujun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+P">Peng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+L">Luoxuan Weng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yitao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+T+J">Toh Jing Hua</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junning Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Che Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L+Z">Leo Z. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yiheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hongjin Su</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+D">Dongchan Shin</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language agents show potential in being capable of utilizing natural language
for varied and intricate tasks in diverse environments, particularly when built
upon large language models (LLMs). Current language agent frameworks aim to
facilitate the construction of proof-of-concept language agents while
neglecting the non-expert user access to agents and paying little attention to
application-level designs. We present OpenAgents, an open platform for using
and hosting language agents in the wild of everyday life. OpenAgents includes
three agents: (1) Data Agent for data analysis with Python/SQL and data tools;
(2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web
browsing. OpenAgents enables general users to interact with agent
functionalities through a web user interface optimized for swift responses and
common failures while offering developers and researchers a seamless deployment
experience on local setups, providing a foundation for crafting innovative
language agents and facilitating real-world evaluations. We elucidate the
challenges and opportunities, aspiring to set a foundation for future research
and development of real-world language agents.
</p>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10635" title="Abstract">arXiv:2310.10635</a> [<a href="/pdf/2310.10635" title="Download PDF">pdf</a>, <a href="/format/2310.10635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Scenario-based Safety Validation for Autonomous Trains with Deep  Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Decker%2C+T">Thomas Decker</a>, 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+A+R">Ananta R. Bhattarai</a>, 
<a href="/search/cs?searchtype=author&query=Lebacher%2C+M">Michael Lebacher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Computer Safety, Reliability, and Security 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Modern AI techniques open up ever-increasing possibilities for autonomous
vehicles, but how to appropriately verify the reliability of such systems
remains unclear. A common approach is to conduct safety validation based on a
predefined Operational Design Domain (ODD) describing specific conditions under
which a system under test is required to operate properly. However, collecting
sufficient realistic test cases to ensure comprehensive ODD coverage is
challenging. In this paper, we report our practical experiences regarding the
utility of data simulation with deep generative models for scenario-based ODD
validation. We consider the specific use case of a camera-based rail-scene
segmentation system designed to support autonomous train operation. We
demonstrate the capabilities of semantically editing railway scenes with deep
generative models to make a limited amount of test data more representative. We
also show how our approach helps to analyze the degree to which a system
complies with typical ODD requirements. Specifically, we focus on evaluating
proper operation under different lighting and weather conditions as well as
while transitioning between them.
</p>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10636" title="Abstract">arXiv:2310.10636</a> [<a href="/pdf/2310.10636" title="Download PDF">pdf</a>, <a href="/format/2310.10636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficacy of Dual-Encoders for Extreme Multi-Label Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+N">Nilesh Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Khatri%2C+D">Devvrit Khatri</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+A+S">Ankit S Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Bhojanapalli%2C+S">Srinadh Bhojanapalli</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+P">Prateek Jain</a>, 
<a href="/search/cs?searchtype=author&query=Dhillon%2C+I+S">Inderjit S Dhillon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Dual-encoder models have demonstrated significant success in dense retrieval
tasks for open-domain question answering that mostly involves zero-shot and
few-shot scenarios. However, their performance in many-shot retrieval problems
where training data is abundant, such as extreme multi-label classification
(XMC), remains under-explored. Existing empirical evidence suggests that, for
such problems, the dual-encoder method's accuracies lag behind the performance
of state-of-the-art (SOTA) extreme classification methods that grow the number
of learnable parameters linearly with the number of classes. As a result, some
recent extreme classification techniques use a combination of dual-encoders and
a learnable classification head for each class to excel on these tasks. In this
paper, we investigate the potential of "pure" DE models in XMC tasks. Our
findings reveal that when trained correctly standard dual-encoders can match or
outperform SOTA extreme classification methods by up to 2% at Precision@1 even
on the largest XMC datasets while being 20x smaller in terms of the number of
trainable parameters. We further propose a differentiable topk error-based loss
function, which can be used to specifically optimize for Recall@k metrics. We
include our PyTorch implementation along with other resources for reproducing
the results in the supplementary material.
</p>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10637" title="Abstract">arXiv:2310.10637</a> [<a href="/pdf/2310.10637" title="Download PDF">pdf</a>, <a href="/format/2310.10637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Mistakes Help Us Grow&quot;: Facilitating and Evaluating Growth Mindset  Supportive Language in Classrooms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Handa%2C+K">Kunal Handa</a>, 
<a href="/search/cs?searchtype=author&query=Clapper%2C+M">Margaret Clapper</a>, 
<a href="/search/cs?searchtype=author&query=Boyle%2C+J">Jessica Boyle</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R+E">Rose E Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yeager%2C+D+S">David S Yeager</a>, 
<a href="/search/cs?searchtype=author&query=Demszky%2C+D">Dorottya Demszky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Teachers' growth mindset supportive language (GMSL)--rhetoric emphasizing
that one's skills can be improved over time--has been shown to significantly
reduce disparities in academic achievement and enhance students' learning
outcomes. Although teachers espouse growth mindset principles, most find it
difficult to adopt GMSL in their practice due the lack of effective coaching in
this area. We explore whether large language models (LLMs) can provide
automated, personalized coaching to support teachers' use of GMSL. We establish
an effective coaching tool to reframe unsupportive utterances to GMSL by
developing (i) a parallel dataset containing GMSL-trained teacher reframings of
unsupportive statements with an accompanying annotation guide, (ii) a GMSL
prompt framework to revise teachers' unsupportive language, and (iii) an
evaluation framework grounded in psychological theory for evaluating GMSL with
the help of students and teachers. We conduct a large-scale evaluation
involving 174 teachers and 1,006 students, finding that both teachers and
students perceive GMSL-trained teacher and model reframings as more effective
in fostering a growth mindset and promoting challenge-seeking behavior, among
other benefits. We also find that model-generated reframings outperform those
from the GMSL-trained teachers. These results show promise for harnessing LLMs
to provide automated GMSL feedback for teachers and, more broadly, LLMs'
potentiality for supporting students' learning in the classroom. Our findings
also demonstrate the benefit of large-scale human evaluations when applying
LLMs in educational domains.
</p>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10638" title="Abstract">arXiv:2310.10638</a> [<a href="/pdf/2310.10638" title="Download PDF">pdf</a>, <a href="/format/2310.10638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Pretraining: Language Modeling Beyond Document Boundaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+S">Sewon Min</a>, 
<a href="/search/cs?searchtype=author&query=Lomeli%2C+M">Maria Lomeli</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chunting Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Margaret Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+V">Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Yih%2C+S">Scott Yih</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Mike Lewis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LMs) are currently trained to predict tokens given
document prefixes, enabling them to directly perform long-form generation and
prompting-style tasks which can be reduced to document completion. Existing
pretraining pipelines train LMs by concatenating random sets of short documents
to create input contexts but the prior documents provide no signal for
predicting the next document. We instead present In-Context Pretraining, a new
approach where language models are pretrained on a sequence of related
documents, thereby explicitly encouraging them to read and reason across
document boundaries. We can do In-Context Pretraining by simply changing the
document ordering so that each context contains related documents, and directly
applying existing pretraining pipelines. However, this document sorting problem
is challenging. There are billions of documents and we would like the sort to
maximize contextual similarity for every document without repeating any data.
To do this, we introduce approximate algorithms for finding related documents
with efficient nearest neighbor search and constructing coherent input contexts
with a graph traversal algorithm. Our experiments show In-Context Pretraining
offers a simple and scalable approach to significantly enhance LMs'performance:
we see notable improvements in tasks that require more complex contextual
reasoning, including in-context learning (+8%), reading comprehension (+15%),
faithfulness to previous contexts (+16%), long-context reasoning (+5%), and
retrieval augmentation (+9%).
</p>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10639" title="Abstract">arXiv:2310.10639</a> [<a href="/pdf/2310.10639" title="Download PDF">pdf</a>, <a href="/format/2310.10639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Black%2C+K">Kevin Black</a>, 
<a href="/search/cs?searchtype=author&query=Nakamoto%2C+M">Mitsuhiko Nakamoto</a>, 
<a href="/search/cs?searchtype=author&query=Atreya%2C+P">Pranav Atreya</a>, 
<a href="/search/cs?searchtype=author&query=Walke%2C+H">Homer Walke</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aviral Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">If generalist robots are to operate in truly unstructured environments, they
need to be able to recognize and reason about novel objects and scenarios. Such
objects and scenarios might not be present in the robot's own training data. We
propose SuSIE, a method that leverages an image-editing diffusion model to act
as a high-level planner by proposing intermediate subgoals that a low-level
controller can accomplish. Specifically, we finetune InstructPix2Pix on video
data, consisting of both human videos and robot rollouts, such that it outputs
hypothetical future "subgoal" observations given the robot's current
observation and a language command. We also use the robot data to train a
low-level goal-conditioned policy to act as the aforementioned low-level
controller. We find that the high-level subgoal predictions can utilize
Internet-scale pretraining and visual understanding to guide the low-level
goal-conditioned policy, achieving significantly better generalization and
precision than conventional language-conditioned policies. We achieve
state-of-the-art results on the CALVIN benchmark, and also demonstrate robust
generalization on real-world manipulation tasks, beating strong baselines that
have access to privileged information or that utilize orders of magnitude more
compute and training data. The project website can be found at
<a href="http://rail-berkeley.github.io/susie">this http URL</a> .
</p>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10640" title="Abstract">arXiv:2310.10640</a> [<a href="/pdf/2310.10640" title="Download PDF">pdf</a>, <a href="/format/2310.10640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Blueprint: Enabling Text-to-Image Generation with Complex and  Detailed Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gani%2C+H">Hanan Gani</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+S+F">Shariq Farooq Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Naseer%2C+M">Muzammal Naseer</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Wonka%2C+P">Peter Wonka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/hananshafi/llmblueprint">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion-based generative models have significantly advanced text-to-image
generation but encounter challenges when processing lengthy and intricate text
prompts describing complex scenes with multiple objects. While excelling in
generating images from short, single-object descriptions, these models often
struggle to faithfully capture all the nuanced details within longer and more
elaborate textual inputs. In response, we present a novel approach leveraging
Large Language Models (LLMs) to extract critical components from text prompts,
including bounding box coordinates for foreground objects, detailed textual
descriptions for individual objects, and a succinct background context. These
components form the foundation of our layout-to-image generation model, which
operates in two phases. The initial Global Scene Generation utilizes object
layouts and background context to create an initial scene but often falls short
in faithfully representing object characteristics as specified in the prompts.
To address this limitation, we introduce an Iterative Refinement Scheme that
iteratively evaluates and refines box-level content to align them with their
textual descriptions, recomposing objects as needed to ensure consistency. Our
evaluation on complex prompts featuring multiple objects demonstrates a
substantial improvement in recall compared to baseline diffusion models. This
is further validated by a user study, underscoring the efficacy of our approach
in generating coherent and detailed scenes from intricate textual inputs.
</p>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10642" title="Abstract">arXiv:2310.10642</a> [<a href="/pdf/2310.10642" title="Download PDF">pdf</a>, <a href="/format/2310.10642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Photorealistic Dynamic Scene Representation and Rendering with  4D Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zeyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongye Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zijie Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiatian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Reconstructing dynamic 3D scenes from 2D images and generating diverse views
over time is challenging due to scene complexity and temporal dynamics. Despite
advancements in neural implicit models, limitations persist: (i) Inadequate
Scene Structure: Existing methods struggle to reveal the spatial and temporal
structure of dynamic scenes from directly learning the complex 6D plenoptic
function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element
deformation becomes impractical for complex dynamics. To address these issues,
we consider the spacetime as an entirety and propose to approximate the
underlying spatio-temporal 4D volume of a dynamic scene by optimizing a
collection of 4D primitives, with explicit geometry and appearance modeling.
Learning to optimize the 4D primitives enables us to synthesize novel views at
any desired time with our tailored rendering routine. Our model is conceptually
simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that
can rotate arbitrarily in space and time, as well as view-dependent and
time-evolved appearance represented by the coefficient of 4D spherindrical
harmonics. This approach offers simplicity, flexibility for variable-length
video and end-to-end training, and efficient real-time rendering, making it
suitable for capturing complex dynamic scene motions. Experiments across
various benchmarks, including monocular and multi-view scenarios, demonstrate
our 4DGS model's superior visual quality and efficiency.
</p>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10644" title="Abstract">arXiv:2310.10644</a> [<a href="/pdf/2310.10644" title="Download PDF">pdf</a>, <a href="/format/2310.10644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TOSS:High-quality Text-guided Novel View Synthesis from a Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yukai Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">He Cao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Boshi Tang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xianbiao Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yukun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shilong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shum%2C+H">Heung-Yeung Shum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we present TOSS, which introduces text to the task of novel
view synthesis (NVS) from just a single RGB image. While Zero-1-to-3 has
demonstrated impressive zero-shot open-set NVS capability, it treats NVS as a
pure image-to-image translation problem. This approach suffers from the
challengingly under-constrained nature of single-view NVS: the process lacks
means of explicit user control and often results in implausible NVS
generations. To address this limitation, TOSS uses text as high-level semantic
information to constrain the NVS solution space. TOSS fine-tunes text-to-image
Stable Diffusion pre-trained on large-scale text-image pairs and introduces
modules specifically tailored to image and camera pose conditioning, as well as
dedicated training for pose correctness and preservation of fine details.
Comprehensive experiments are conducted with results showing that our proposed
TOSS outperforms Zero-1-to-3 with more plausible, controllable and
multiview-consistent NVS results. We further support these results with
comprehensive ablations that underscore the effectiveness and potential of the
introduced semantic guidance and architecture design.
</p>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10645" title="Abstract">arXiv:2310.10645</a> [<a href="/pdf/2310.10645" title="Download PDF">pdf</a>, <a href="/format/2310.10645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Task Planning with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Philipp Wu</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+J">Jitendra Malik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">An interactive robot framework accomplishes long-horizon task planning and
can easily generalize to new goals or distinct tasks, even during execution.
However, most traditional methods require predefined module design, which makes
it hard to generalize to different goals. Recent large language model based
approaches can allow for more open-ended planning but often require heavy
prompt engineering or domain-specific pretrained models. To tackle this, we
propose a simple framework that achieves interactive task planning with
language models. Our system incorporates both high-level planning and low-level
function execution via language. We verify the robustness of our system in
generating novel high-level instructions for unseen objectives and its ease of
adaptation to different tasks by merely substituting the task guidelines,
without the need for additional complex prompt engineering. Furthermore, when
the user sends a new request, our system is able to replan accordingly with
precision based on the new request, task guidelines and previously executed
steps. Please check more details on our https://wuphilipp.github.io/itp_site
and https://youtu.be/TrKLuyv26_g.
</p>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10647" title="Abstract">arXiv:2310.10647</a> [<a href="/pdf/2310.10647" title="Download PDF">pdf</a>, <a href="/format/2310.10647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Video Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+Z">Zhen Xing</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Q">Qijun Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haoran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Q">Qi Dai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zuxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu-Gang Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The recent wave of AI-generated content (AIGC) has witnessed substantial
success in computer vision, with the diffusion model playing a crucial role in
this achievement. Due to their impressive generative capabilities, diffusion
models are gradually superseding methods based on GANs and auto-regressive
Transformers, demonstrating exceptional performance not only in image
generation and editing, but also in the realm of video-related research.
However, existing surveys mainly focus on diffusion models in the context of
image generation, with few up-to-date reviews on their application in the video
domain. To address this gap, this paper presents a comprehensive review of
video diffusion models in the AIGC era. Specifically, we begin with a concise
introduction to the fundamentals and evolution of diffusion models.
Subsequently, we present an overview of research on diffusion models in the
video domain, categorizing the work into three key areas: video generation,
video editing, and other video understanding tasks. We conduct a thorough
review of the literature in these three key areas, including further
categorization and practical contributions in the field. Finally, we discuss
the challenges faced by research in this domain and outline potential future
developmental trends. A comprehensive list of video diffusion models studied in
this survey is available at
https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.
</p>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10648" title="Abstract">arXiv:2310.10648</a> [<a href="/pdf/2310.10648" title="Download PDF">pdf</a>, <a href="/format/2310.10648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Step-by-Step Remediation of Students&#x27; Mathematical Mistakes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R+E">Rose E. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+C">Carly Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Loeb%2C+S">Susanna Loeb</a>, 
<a href="/search/cs?searchtype=author&query=Demszky%2C+D">Dorottya Demszky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our work is open-sourced at this link: \url{<a href="https://github.com/rosewang2008/remath">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Scaling high-quality tutoring is a major challenge in education. Because of
the growing demand, many platforms employ novice tutors who, unlike
professional educators, struggle to effectively address student mistakes and
thus fail to seize prime learning opportunities for students. In this paper, we
explore the potential for large language models (LLMs) to assist math tutors in
remediating student mistakes. We present ReMath, a benchmark co-developed with
experienced math teachers that deconstructs their thought process for
remediation. The benchmark consists of three step-by-step tasks: (1) infer the
type of student error, (2) determine the strategy to address the error, and (3)
generate a response that incorporates that information. We evaluate the
performance of state-of-the-art instruct-tuned and dialog models on ReMath. Our
findings suggest that although models consistently improve upon original tutor
responses, we cannot rely on models alone to remediate mistakes. Providing
models with the error type (e.g., the student is guessing) and strategy (e.g.,
simplify the problem) leads to a 75% improvement in the response quality over
models without that information. Nonetheless, despite the improvement, the
quality of the best model's responses still falls short of experienced math
teachers. Our work sheds light on the potential and limitations of using
current LLMs to provide high-quality learning experiences for both tutors and
students at scale. Our work is open-sourced at this link:
\url{https://github.com/rosewang2008/remath}.
</p>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10649" title="Abstract">arXiv:2310.10649</a> [<a href="/pdf/2310.10649" title="Download PDF">pdf</a>, <a href="/format/2310.10649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computational Framework for Solving Wasserstein Lagrangian Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neklyudov%2C+K">Kirill Neklyudov</a>, 
<a href="/search/cs?searchtype=author&query=Brekelmans%2C+R">Rob Brekelmans</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+A">Alexander Tong</a>, 
<a href="/search/cs?searchtype=author&query=Atanackovic%2C+L">Lazar Atanackovic</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Makhzani%2C+A">Alireza Makhzani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The dynamical formulation of the optimal transport can be extended through
various choices of the underlying geometry ($\textit{kinetic energy}$), and the
regularization of density paths ($\textit{potential energy}$). These
combinations yield different variational problems ($\textit{Lagrangians}$),
encompassing many variations of the optimal transport problem such as the
Schr\"odinger bridge, unbalanced optimal transport, and optimal transport with
physical constraints, among others. In general, the optimal density path is
unknown, and solving these variational problems can be computationally
challenging. Leveraging the dual formulation of the Lagrangians, we propose a
novel deep learning based framework approaching all of these problems from a
unified perspective. Our method does not require simulating or backpropagating
through the trajectories of the learned dynamics, and does not need access to
optimal couplings. We showcase the versatility of the proposed framework by
outperforming previous approaches for the single-cell trajectory inference,
where incorporating prior knowledge into the dynamics is crucial for correct
predictions.
</p>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10650" title="Abstract">arXiv:2310.10650</a> [<a href="/pdf/2310.10650" title="Download PDF">pdf</a>, <a href="/format/2310.10650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through  Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Holland%2C+L">Leif Van Holland</a>, 
<a href="/search/cs?searchtype=author&query=Bliersbach%2C+R">Ruben Bliersbach</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+J+U">Jan U. M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Stotko%2C+P">Patrick Stotko</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+R">Reinhard Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Implicit representations like Neural Radiance Fields (NeRF) showed impressive
results for photorealistic rendering of complex scenes with fine details.
However, ideal or near-perfectly specular reflecting objects such as mirrors,
which are often encountered in various indoor scenes, impose ambiguities and
inconsistencies in the representation of the reconstructed scene leading to
severe artifacts in the synthesized renderings. In this paper, we present a
novel reflection tracing method tailored for the involved volume rendering
within NeRF that takes these mirror-like objects into account while avoiding
the cost of straightforward but expensive extensions through standard path
tracing. By explicitly modeling the reflection behavior using physically
plausible materials and estimating the reflected radiance with Monte-Carlo
methods within the volume rendering formulation, we derive efficient strategies
for importance sampling and the transmittance computation along rays from only
few samples. We show that our novel method enables the training of consistent
representations of such challenging scenes and achieves superior results in
comparison to previous state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10651" title="Abstract">arXiv:2310.10651</a> [<a href="/pdf/2310.10651" title="Download PDF">pdf</a>, <a href="/format/2310.10651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianyi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongdong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wenbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jing Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+G">Gang Hua</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023, code is available at <a href="https://github.com/wty-ustc/HairCLIPv2">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Hair editing has made tremendous progress in recent years. Early hair editing
methods use well-drawn sketches or masks to specify the editing conditions.
Even though they can enable very fine-grained local control, such interaction
modes are inefficient for the editing conditions that can be easily specified
by language descriptions or reference images. Thanks to the recent breakthrough
of cross-modal models (e.g., CLIP), HairCLIP is the first work that enables
hair editing based on text descriptions or reference images. However, such
text-driven and reference-driven interaction modes make HairCLIP unable to
support fine-grained controls specified by sketch or mask. In this paper, we
propose HairCLIPv2, aiming to support all the aforementioned interactions with
one unified framework. Simultaneously, it improves upon HairCLIP with better
irrelevant attributes (e.g., identity, background) preservation and unseen text
descriptions support. The key idea is to convert all the hair editing tasks
into hair transfer tasks, with editing conditions converted into different
proxies accordingly. The editing effects are added upon the input image by
blending the corresponding proxy features within the hairstyle or hair color
feature spaces. Besides the unprecedented user interaction mode support,
quantitative and qualitative experiments demonstrate the superiority of
HairCLIPv2 in terms of editing effects, irrelevant attribute preservation and
visual naturalness. Our code is available at
\url{https://github.com/wty-ustc/HairCLIPv2}.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Tue, 17 Oct 23</h3>
<dl>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14714" title="Abstract">arXiv:2308.14714</a> (cross-list from eess.SY) [<a href="/pdf/2308.14714" title="Download PDF">pdf</a>, <a href="/format/2308.14714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stochastic Surveillance Stackelberg Game: Co-Optimizing Defense  Placement and Patrol Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=John%2C+Y">Yohan John</a>, 
<a href="/search/eess?searchtype=author&query=Diaz-Garcia%2C+G">Gilberto Diaz-Garcia</a>, 
<a href="/search/eess?searchtype=author&query=Duan%2C+X">Xiaoming Duan</a>, 
<a href="/search/eess?searchtype=author&query=Marden%2C+J+R">Jason R. Marden</a>, 
<a href="/search/eess?searchtype=author&query=Bullo%2C+F">Francesco Bullo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure, jointly submitted to the IEEE Control Systems Letters and the 2024 American Control Conference. Replaced to fix typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC)

</div>
<p class="mathjax">Stochastic patrol routing is known to be advantageous in adversarial
settings; however, the optimal choice of stochastic routing strategy is
dependent on a model of the adversary. Duan et al. formulated a Stackelberg
game for the worst-case scenario, i.e., a surveillance agent confronted with an
omniscient attacker [IEEE TCNS, 8(2), 769-80, 2021]. In this article, we extend
their formulation to accommodate heterogeneous defenses at the various nodes of
the graph. We derive an upper bound on the value of the game. We identify
methods for computing effective patrol strategies for certain classes of
graphs. Finally, we leverage the heterogeneous defense formulation to develop
novel defense placement algorithms that complement the patrol strategies.
</p>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09294" title="Abstract">arXiv:2310.09294</a> (cross-list from math.OC) [<a href="/pdf/2310.09294" title="Download PDF">pdf</a>, <a href="/format/2310.09294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Potential of Synthetic Fuel Production: Coupled  Optimization of Heat Exchanger Network and Operating Parameters of a 1 MW  Power-to-Liquid Plant
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huber%2C+D">David Huber</a>, 
<a href="/search/math?searchtype=author&query=Birkelbach%2C+F">Felix Birkelbach</a>, 
<a href="/search/math?searchtype=author&query=Hofmann%2C+R">Ren&#xe9; Hofmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The use of synthetic fuels is a promising way to reduce emissions
significantly. To accelerate cost-effective large-scale synthetic fuel
deployment, we optimize a novel 1 MW PtL-plant in terms of PtL-efficiency and
fuel production costs. For numerous plants, the available waste heat and
temperature level depend on the operating point. Thus, to optimize efficiency
and costs, the choice of the operating point is included in the heat exchanger
network synthesis. All nonlinearities are approximated using piecewise linear
models and transferred to MILP. Adapting the epsilon constraint method allows
us to solve the multi-criteria problem with uniformly distributed solutions on
the Pareto front. The results show that compared to the conventional design
process, the production cost can be reduced to 1.83 EUR/kg and the
PtL-efficiency can be increased to 61.30 %. By applying the presented method,
climate-neutral synthetic fuels can be promoted and emissions can be reduced in
the long term.
</p>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09307" title="Abstract">arXiv:2310.09307</a> (cross-list from math.OC) [<a href="/pdf/2310.09307" title="Download PDF">pdf</a>, <a href="/format/2310.09307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chemical process flowsheet optimization with full space, surrogate, and  implicit formulations of a Gibbs reactor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bugosen%2C+S">Sergio Bugosen</a>, 
<a href="/search/math?searchtype=author&query=Laird%2C+C">Carl Laird</a>, 
<a href="/search/math?searchtype=author&query=Parker%2C+R">Robert Parker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Two formulations for the optimization of chemical process flowsheets are
presented that leverage surrogate models and implicit functions to replace and
remove, respectively, the algebraic equations that describe a
difficult-to-converge Gibbs reactor unit operation. Convergence reliability,
solve time, and solution quality of an optimization problem are compared among
full-space, ALAMO surrogate, neural network surrogate, and implicit function
formulations. Both surrogate and implicit formulations lead to better
convergence reliability, with low sensitivity to process parameters. The
surrogate formulations are faster at the cost of minor solution error, while
the implicit formulation provides exact solutions with an increased solve time.
In a parameter sweep on the autothermal reformer flowsheet optimization
problem, the full space formulation solves 30 out of 64 instances, while the
implicit function formulation solves 49 out of 64 instances, the ALAMO
polynomial formulation solves 64 out of 64 instances, and the neural network
formulation solves 37 out of 64 instances. This work demonstrates the trade-off
between accuracy and solve time that exists in current methods for improving
convergence reliability of chemical process flowsheet optimization problems.
</p>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09311" title="Abstract">arXiv:2310.09311</a> (cross-list from physics.ao-ph) [<a href="/pdf/2310.09311" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessment of Climate Change Impacts on the Precipitation and  Temperature: A Case Study on Krishna River Basin
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Syam%2C+N+S+S">Nandikanti Siva Sai Syam</a>, 
<a href="/search/physics?searchtype=author&query=Sunil%2C+A">Akshay Sunil</a>, 
<a href="/search/physics?searchtype=author&query=Pichuka%2C+S">Subbarao Pichuka</a>, 
<a href="/search/physics?searchtype=author&query=Mandal%2C+A">Anirban Mandal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this study, the statistical downscaling model (SDSM) is employed for
downscaling the precipitation (PREC), maximum temperature (T max ) and minimum
temperature (T min ) over Krishna River Basin (KRB). The Canadian Earth System
Model, version 2 (CanESM2) General Circulation Model (GCM) outputs were
considered as predictor variables. First, the SDSM is calibrated using 30-years
(1961-1990) of data and subsequently validated for 15-years (1991-2005). Upon
perceiving the satisfactory performance, the SDSM is further used for
projecting the predictand variables (PRECP, T max and T min ) for the 21 st
century considering three Representative Concentration Pathway (RCP) scenarios
viz. RCP2.6, RCP4.5 and RCP8.5. The future period is divided into three 30-year
time slices named epoch-1 (2011-2040), epoch-2 (2041-2070) and epoch-3
(2071-2100) respectively. Further, 1976-2005 is considered as baseline period
and all the future results are compared with this data. The results were
analysed at various temporal scales, i.e., monthly, seasonal and annual. The
study reveals that the KRB is going to become wetter during all the seasons.
The results are discussed for the worst-case scenario i.e., RCP8.5 epoch-3. The
average annual maximum and minimum temperature is expected to increase. The
extreme event analysis is also carried out considering the 90 th and 95 th
percentile values. It is noticed that the extreme (90 th and 95 th percentiles)
are going to increase. There are events more than extreme values. The outcome
of this study can be used in flood modelling for the KRB and also for the
modelling of future irrigation demands along with the planning of optimal
irrigation in the KRB culturable command area.
</p>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09322" title="Abstract">arXiv:2310.09322</a> (cross-list from math.OC) [<a href="/pdf/2310.09322" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Note on Analyzing the Stability of Oscillator Ising Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bashar%2C+M+K">Mohammad Khairul Bashar</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+Z">Zongli Lin</a>, 
<a href="/search/math?searchtype=author&query=Shukla%2C+N">Nikhil Shukla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Emerging Technologies (cs.ET); Dynamical Systems (math.DS)

</div>
<p class="mathjax">The rich non-linear dynamics of the coupled oscillators (under second
harmonic injection) can be leveraged to solve computationally hard problems in
combinatorial optimization such as finding the ground state of the Ising
Hamiltonian. While prior work on the stability of the so-called Oscillator
Ising Machines (OIMs) has used the linearization method, in this letter, we
present a complementary method to analyze stability using the second order
derivative test of the energy / cost function. We establish the equivalence
between the two methods, thus augmenting the tool kit for the design and
implementation of OIMs.
</p>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09335" title="Abstract">arXiv:2310.09335</a> (cross-list from stat.ML) [<a href="/pdf/2310.09335" title="Download PDF">pdf</a>, <a href="/format/2310.09335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical guarantees for stochastic Metropolis-Hastings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bieringer%2C+S">Sebastian Bieringer</a>, 
<a href="/search/stat?searchtype=author&query=Kasieczka%2C+G">Gregor Kasieczka</a>, 
<a href="/search/stat?searchtype=author&query=Steffen%2C+M+F">Maximilian F. Steffen</a>, 
<a href="/search/stat?searchtype=author&query=Trabs%2C+M">Mathias Trabs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 5 figures. arXiv admin note: text overlap with <a href="/abs/2204.12392">arXiv:2204.12392</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">A Metropolis-Hastings step is widely used for gradient-based Markov chain
Monte Carlo methods in uncertainty quantification. By calculating acceptance
probabilities on batches, a stochastic Metropolis-Hastings step saves
computational costs, but reduces the effective sample size. We show that this
obstacle can be avoided by a simple correction term. We study statistical
properties of the resulting stationary distribution of the chain if the
corrected stochastic Metropolis-Hastings approach is applied to sample from a
Gibbs posterior distribution in a nonparametric regression setting. Focusing on
deep neural network regression, we prove a PAC-Bayes oracle inequality which
yields optimal contraction rates and we analyze the diameter and show high
coverage probability of the resulting credible sets. With a numerical example
in a high-dimensional parameter space, we illustrate that credible sets and
contraction rates of the stochastic Metropolis-Hastings algorithm indeed behave
similar to those obtained from the classical Metropolis-adjusted Langevin
algorithm.
</p>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09388" title="Abstract">arXiv:2310.09388</a> (cross-list from eess.AS) [<a href="/pdf/2310.09388" title="Download PDF">pdf</a>, <a href="/format/2310.09388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CORN: Co-Trained Full-Reference And No-Reference Audio Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Manocha%2C+P">Pranay Manocha</a>, 
<a href="/search/eess?searchtype=author&query=Williamson%2C+D">Donald Williamson</a>, 
<a href="/search/eess?searchtype=author&query=Finkelstein%2C+A">Adam Finkelstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Perceptual evaluation constitutes a crucial aspect of various
audio-processing tasks. Full reference (FR) or similarity-based metrics rely on
high-quality reference recordings, to which lower-quality or corrupted versions
of the recording may be compared for evaluation. In contrast, no-reference (NR)
metrics evaluate a recording without relying on a reference. Both the FR and NR
approaches exhibit advantages and drawbacks relative to each other. In this
paper, we present a novel framework called CORN that amalgamates these dual
approaches, concurrently training both FR and NR models together. After
training, the models can be applied independently. We evaluate CORN by
predicting several common objective metrics and across two different
architectures. The NR model trained using CORN has access to a reference
recording during training, and thus, as one would expect, it consistently
outperforms baseline NR models trained independently. Perhaps even more
remarkable is that the CORN FR model also outperforms its baseline counterpart,
even though it relies on the same training data and the same model
architecture. Thus, a single training regime produces two independently useful
models, each outperforming independently trained models.
</p>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09433" title="Abstract">arXiv:2310.09433</a> (cross-list from physics.optics) [<a href="/pdf/2310.09433" title="Download PDF">pdf</a>, <a href="/format/2310.09433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effects of cavity nonlinearities and linear losses on silicon  microring-based reservoir computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Castro%2C+B+J+G">Bernard J. Giron Castro</a>, 
<a href="/search/physics?searchtype=author&query=Peucheret%2C+C">Christophe Peucheret</a>, 
<a href="/search/physics?searchtype=author&query=Zibar%2C+D">Darko Zibar</a>, 
<a href="/search/physics?searchtype=author&query=Da+Ros%2C+F">Francesco Da Ros</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures, submitted to Optics Express
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Microring resonators (MRRs) are promising devices for time-delay photonic
reservoir computing, but the impact of the different physical effects taking
place in the MRRs on the reservoir computing performance is yet to be fully
understood. We numerically analyze the impact of linear losses as well as
thermo-optic and free-carrier effects relaxation times on the prediction error
of the time-series task NARMA-10. We demonstrate the existence of three
regions, defined by the input power and the frequency detuning between the
optical source and the microring resonance, that reveal the cavity transition
from linear to nonlinear regimes. One of these regions offers very low error in
time-series prediction under relatively low input power and number of nodes
while the other regions either lack nonlinearity or become unstable. This study
provides insight into the design of the MRR and the optimization of its
physical properties for improving the prediction performance of time-delay
reservoir computing.
</p>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09437" title="Abstract">arXiv:2310.09437</a> (cross-list from stat.ML) [<a href="/pdf/2310.09437" title="Download PDF">pdf</a>, <a href="/format/2310.09437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signal reconstruction using determinantal sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Belhadji%2C+A">Ayoub Belhadji</a>, 
<a href="/search/stat?searchtype=author&query=Bardenet%2C+R">R&#xe9;mi Bardenet</a>, 
<a href="/search/stat?searchtype=author&query=Chainais%2C+P">Pierre Chainais</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We study the approximation of a square-integrable function from a finite
number of evaluations on a random set of nodes according to a well-chosen
distribution. This is particularly relevant when the function is assumed to
belong to a reproducing kernel Hilbert space (RKHS). This work proposes to
combine several natural finite-dimensional approximations based two possible
probability distributions of nodes. These distributions are related to
determinantal point processes, and use the kernel of the RKHS to favor
RKHS-adapted regularity in the random design. While previous work on
determinantal sampling relied on the RKHS norm, we prove mean-square guarantees
in $L^2$ norm. We show that determinantal point processes and mixtures thereof
can yield fast convergence rates. Our results also shed light on how the rate
changes as more smoothness is assumed, a phenomenon known as superconvergence.
Besides, determinantal sampling generalizes i.i.d. sampling from the
Christoffel function which is standard in the literature. More importantly,
determinantal sampling guarantees the so-called instance optimality property
for a smaller number of function evaluations than i.i.d. sampling.
</p>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09446" title="Abstract">arXiv:2310.09446</a> (cross-list from eess.IV) [<a href="/pdf/2310.09446" title="Download PDF">pdf</a>, <a href="/format/2310.09446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic segmentation of lung findings in CT and application to Long  COVID
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Carmo%2C+D+S">Diedre S. Carmo</a>, 
<a href="/search/eess?searchtype=author&query=Tudas%2C+R+A">Rosarie A. Tudas</a>, 
<a href="/search/eess?searchtype=author&query=Comellas%2C+A+P">Alejandro P. Comellas</a>, 
<a href="/search/eess?searchtype=author&query=Rittner%2C+L">Leticia Rittner</a>, 
<a href="/search/eess?searchtype=author&query=Lotufo%2C+R+A">Roberto A. Lotufo</a>, 
<a href="/search/eess?searchtype=author&query=Reinhardt%2C+J+M">Joseph M. Reinhardt</a>, 
<a href="/search/eess?searchtype=author&query=Gerard%2C+S+E">Sarah E. Gerard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Versao em portugu\^es brasileiro submetida para o XV EADCA 2023. Brazilian portuguese verson submitted to XV EADCA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Automated segmentation of lung abnormalities in computed tomography is an
important step for diagnosing and characterizing lung disease. In this work, we
improve upon a previous method and propose S-MEDSeg, a deep learning based
approach for accurate segmentation of lung lesions in chest CT images. S-MEDSeg
combines a pre-trained EfficientNet backbone, bidirectional feature pyramid
network, and modern network advancements to achieve improved segmentation
performance. A comprehensive ablation study was performed to evaluate the
contribution of the proposed network modifications. The results demonstrate
modifications introduced in S-MEDSeg significantly improves segmentation
performance compared to the baseline approach. The proposed method is applied
to an independent dataset of long COVID inpatients to study the effect of
post-acute infection vaccination on extent of lung findings. Open-source code,
graphical user interface and pip package are available at
https://github.com/MICLab-Unicamp/medseg.
</p>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09457" title="Abstract">arXiv:2310.09457</a> (cross-list from eess.IV) [<a href="/pdf/2310.09457" title="Download PDF">pdf</a>, <a href="/format/2310.09457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UCM-Net: A Lightweight and Efficient Solution for Skin Lesion  Segmentation using MLP and CNN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yuan%2C+C">Chunyu Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+D">Dongfang Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Agaian%2C+S+S">Sos S. Agaian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Skin cancer is a significant public health problem, and computer-aided
diagnosis can help to prevent and treat it. A crucial step for computer-aided
diagnosis is accurately segmenting skin lesions in images, which allows for
lesion detection, classification, and analysis. However, this task is
challenging due to the diverse characteristics of lesions, such as appearance,
shape, size, color, texture, and location, as well as image quality issues like
noise, artifacts, and occlusions. Deep learning models have recently been
applied to skin lesion segmentation, but they have high parameter counts and
computational demands, making them unsuitable for mobile health applications.
To address this challenge, we propose UCM-Net, a novel, efficient, and
lightweight solution that integrates Multi-Layer Perceptions (MLP) and
Convolutional Neural Networks (CNN). Unlike conventional UNet architectures,
our UCMNet-Block reduces parameter overhead and enhances UCM-Net's learning
capabilities, leading to robust segmentation performance. We validate UCM-Net's
competitiveness through extensive experiments on isic2017 and isic2018
datasets. Remarkably, UCM-Net has less than 50KB parameters and less than 0.05
Giga-Operations Per Second (GLOPs), setting a new possible standard for
efficiency in skin lesion segmentation. The source code will be publicly
available.
</p>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09468" title="Abstract">arXiv:2310.09468</a> (cross-list from quant-ph) [<a href="/pdf/2310.09468" title="Download PDF">pdf</a>, <a href="/format/2310.09468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Benchmarking of Local Zeroth-Order Optimizers for Variational  Quantum Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Tecot%2C+L">Lucas Tecot</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the field of quantum information, classical optimizers play an important
role. From experimentalists optimizing their physical devices to theorists
exploring variational quantum algorithms, many aspects of quantum information
require the use of a classical optimizer. For this reason, there are many
papers that benchmark the effectiveness of different optimizers for specific
quantum optimization tasks and choices of parameterized algorithms. However,
for researchers exploring new algorithms or physical devices, the insights from
these studies don't necessarily translate. To address this concern, we compare
the performance of classical optimizers across a series of partially-randomized
tasks to more broadly sample the space of quantum optimization problems. We
focus on local zeroth-order optimizers due to their generally favorable
performance and query-efficiency on quantum systems. We discuss insights from
these experiments that can help motivate future works to improve these
optimizers for use on quantum systems.
</p>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09488" title="Abstract">arXiv:2310.09488</a> (cross-list from stat.ML) [<a href="/pdf/2310.09488" title="Download PDF">pdf</a>, <a href="/format/2310.09488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lu%2C+J">Jiecheng Lu</a>, 
<a href="/search/stat?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+S">Shihao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Long-term time series forecasting (LTSF) is important for various domains but
is confronted by challenges in handling the complex temporal-contextual
relationships. As multivariate input models underperforming some recent
univariate counterparts, we posit that the issue lies in the inefficiency of
existing multivariate LTSF Transformers to model series-wise relationships: the
characteristic differences between series are often captured incorrectly. To
address this, we introduce ARM: a multivariate temporal-contextual adaptive
learning method, which is an enhanced architecture specifically designed for
multivariate LTSF modelling. ARM employs Adaptive Univariate Effect Learning
(AUEL), Random Dropping (RD) training strategy, and Multi-kernel Local
Smoothing (MKLS), to better handle individual series temporal patterns and
correctly learn inter-series dependencies. ARM demonstrates superior
performance on multiple benchmarks without significantly increasing
computational costs compared to vanilla Transformer, thereby advancing the
state-of-the-art in LTSF. ARM is also generally applicable to other LTSF
architecture beyond vanilla Transformer.
</p>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09553" title="Abstract">arXiv:2310.09553</a> (cross-list from q-bio.PE) [<a href="/pdf/2310.09553" title="Download PDF">pdf</a>, <a href="/format/2310.09553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARTree: A Deep Autoregressive Model for Phylogenetic Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Xie%2C+T">Tianyu Xie</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Designing flexible probabilistic models over tree topologies is important for
developing efficient phylogenetic inference methods. To do that, previous works
often leverage the similarity of tree topologies via hand-engineered heuristic
features which would require pre-sampled tree topologies and may suffer from
limited approximation capability. In this paper, we propose a deep
autoregressive model for phylogenetic inference based on graph neural networks
(GNNs), called ARTree. By decomposing a tree topology into a sequence of leaf
node addition operations and modeling the involved conditional distributions
based on learnable topological features via GNNs, ARTree can provide a rich
family of distributions over the entire tree topology space that have simple
sampling algorithms and density estimation procedures, without using heuristic
features. We demonstrate the effectiveness and efficiency of our method on a
benchmark of challenging real data tree topology density estimation and
variational Bayesian phylogenetic inference problems.
</p>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09594" title="Abstract">arXiv:2310.09594</a> (cross-list from math.OC) [<a href="/pdf/2310.09594" title="Download PDF">pdf</a>, <a href="/ps/2310.09594" title="Download PostScript">ps</a>, <a href="/format/2310.09594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuITO: Numerical software for constrained nonlinear optimal control  problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ganguly%2C+S">Siddhartha Ganguly</a>, 
<a href="/search/math?searchtype=author&query=Randad%2C+N">Nakul Randad</a>, 
<a href="/search/math?searchtype=author&query=D%27Silva%2C+R+A">Rihan Aaron D&#x27;Silva</a>, 
<a href="/search/math?searchtype=author&query=Raj%2C+M+S">Mukesh S Raj</a>, 
<a href="/search/math?searchtype=author&query=Chatterjee%2C+D">Debasish Chatterjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, to appear in SoftwareX
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We introduce the MATLAB-based software QuITO (Quasi-Interpolation based
Trajectory Optimization) to numerically solve a wide class of constrained
nonlinear optimal control problems (OCP). The solver is based on the QuITO (the
same abbreviation) algorithm, which is a direct multiple shooting (DMS)
technique that leverages a particular type of quasi-interpolation scheme for
control trajectory parameterization. The software is equipped with several
options for numerical integration, and optimization solvers along with a
Graphical User Interface (GUI) to make the process of designing and solving the
OCPs smooth and seamless for users with minimum coding experience. We
demonstrate with two benchmark numerical examples the procedure to generate
constrained state and control trajectories using QuITO.
</p>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09597" title="Abstract">arXiv:2310.09597</a> (cross-list from econ.EM) [<a href="/pdf/2310.09597" title="Download PDF">pdf</a>, <a href="/format/2310.09597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive maximization of social welfare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Cesa-Bianchi%2C+N">Nicolo Cesa-Bianchi</a>, 
<a href="/search/econ?searchtype=author&query=Colomboni%2C+R">Roberto Colomboni</a>, 
<a href="/search/econ?searchtype=author&query=Kasy%2C+M">Maximilian Kasy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of repeatedly choosing policies to maximize social
welfare. Welfare is a weighted sum of private utility and public revenue.
Earlier outcomes inform later policies. Utility is not observed, but indirectly
inferred. Response functions are learned through experimentation.
<br />We derive a lower bound on regret, and a matching adversarial upper bound for
a variant of the Exp3 algorithm. Cumulative regret grows at a rate of
$T^{2/3}$. This implies that (i) welfare maximization is harder than the
multi-armed bandit problem (with a rate of $T^{1/2}$ for finite policy sets),
and (ii) our algorithm achieves the optimal rate. For the stochastic setting,
if social welfare is concave, we can achieve a rate of $T^{1/2}$ (for
continuous policy sets), using a dyadic search algorithm.
<br />We analyze an extension to nonlinear income taxation, and sketch an extension
to commodity taxation. We compare our setting to monopoly pricing (which is
easier), and price setting for bilateral trade (which is harder).
</p>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09603" title="Abstract">arXiv:2310.09603</a> (cross-list from eess.IV) [<a href="/pdf/2310.09603" title="Download PDF">pdf</a>, <a href="/format/2310.09603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> B-Spine: Learning B-Spline Curve Representation for Robust and  Interpretable Spinal Curvature Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+Q">Qiang Song</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+R">Ruofeng Yin</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+R">Rui Ma</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+Y">Yizhou Yu</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+Y">Yi Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Spinal curvature estimation is important to the diagnosis and treatment of
the scoliosis. Existing methods face several issues such as the need of
expensive annotations on the vertebral landmarks and being sensitive to the
image quality. It is challenging to achieve robust estimation and obtain
interpretable results, especially for low-quality images which are blurry and
hazy. In this paper, we propose B-Spine, a novel deep learning pipeline to
learn B-spline curve representation of the spine and estimate the Cobb angles
for spinal curvature estimation from low-quality X-ray images. Given a
low-quality input, a novel SegRefine network which employs the unpaired
image-to-image translation is proposed to generate a high quality spine mask
from the initial segmentation result. Next, a novel mask-based B-spline
prediction model is proposed to predict the B-spline curve for the spine
centerline. Finally, the Cobb angles are estimated by a hybrid approach which
combines the curve slope analysis and a curve-based regression model. We
conduct quantitative and qualitative comparisons with the representative and
SOTA learning-based methods on the public AASCE2019 dataset and our new
proposed CJUH-JLU dataset which contains more challenging low-quality images.
The superior performance on both datasets shows our method can achieve both
robustness and interpretability for spinal curvature estimation.
</p>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09625" title="Abstract">arXiv:2310.09625</a> (cross-list from eess.IV) [<a href="/pdf/2310.09625" title="Download PDF">pdf</a>, <a href="/format/2310.09625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JSMoCo: Joint Coil Sensitivity and Motion Correction in Parallel MRI  with a Self-Calibrating Score-Based Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+L">Lixuan Chen</a>, 
<a href="/search/eess?searchtype=author&query=Tian%2C+X">Xuanyu Tian</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Jiangjie Wu</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+R">Ruimin Feng</a>, 
<a href="/search/eess?searchtype=author&query=Lao%2C+G">Guoyan Lao</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yuyao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wei%2C+H">Hongjiang Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages,8 figures, journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Magnetic Resonance Imaging (MRI) stands as a powerful modality in clinical
diagnosis. However, it is known that MRI faces challenges such as long
acquisition time and vulnerability to motion-induced artifacts. Despite the
success of many existing motion correction algorithms, there has been limited
research focused on correcting motion artifacts on the estimated coil
sensitivity maps for fast MRI reconstruction. Existing methods might suffer
from severe performance degradation due to error propagation resulting from the
inaccurate coil sensitivity maps estimation. In this work, we propose to
jointly estimate the motion parameters and coil sensitivity maps for
under-sampled MRI reconstruction, referred to as JSMoCo. However, joint
estimation of motion parameters and coil sensitivities results in a highly
ill-posed inverse problem due to an increased number of unknowns. To address
this, we introduce score-based diffusion models as powerful priors and leverage
the MRI physical principles to efficiently constrain the solution space for
this optimization problem. Specifically, we parameterize the rigid motion as
three trainable variables and model coil sensitivity maps as polynomial
functions. Leveraging the physical knowledge, we then employ Gibbs sampler for
joint estimation, ensuring system consistency between sensitivity maps and
desired images, avoiding error propagation from pre-estimated sensitivity maps
to the reconstructed images. We conduct comprehensive experiments to evaluate
the performance of JSMoCo on the fastMRI dataset. The results show that our
method is capable of reconstructing high-quality MRI images from
sparsely-sampled k-space data, even affected by motion. It achieves this by
accurately estimating both motion parameters and coil sensitivities,
effectively mitigating motion-related challenges during MRI reconstruction.
</p>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09677" title="Abstract">arXiv:2310.09677</a> (cross-list from math.OC) [<a href="/pdf/2310.09677" title="Download PDF">pdf</a>, <a href="/ps/2310.09677" title="Download PostScript">ps</a>, <a href="/format/2310.09677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S-Procedure Relaxation: a Case of Exactness Involving Chebyshev Centers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Foucart%2C+S">Simon Foucart</a>, 
<a href="/search/math?searchtype=author&query=Liao%2C+C">Chunyang Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Optimal recovery is a mathematical framework for learning functions from
observational data by adopting a worst-case perspective tied to model
assumptions on the functions to be learned. Working in a finite-dimensional
Hilbert space, we consider model assumptions based on approximability and
observation inaccuracies modeled as additive errors bounded in $\ell_2$. We
focus on the local recovery problem, which amounts to the determination of
Chebyshev centers. Earlier work by Beck and Eldar presented a semidefinite
recipe for the determination of Chebyshev centers. The result was valid in the
complex setting only, but not necessarily in the real setting, since it relied
on the S-procedure with two quadratic constraints, which offers a tight
relaxation only in the complex setting. Our contribution consists in proving
that this semidefinite recipe is exact in the real setting, too, at least in
the particular instance where the quadratic constraints involve orthogonal
projectors. Our argument exploits a previous work of ours, where exact
Chebyshev centers were obtained in a different way. We conclude by stating some
open questions and by commenting on other recent results in optimal recovery.
</p>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09766" title="Abstract">arXiv:2310.09766</a> (cross-list from stat.ML) [<a href="/pdf/2310.09766" title="Download PDF">pdf</a>, <a href="/format/2310.09766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pseudo-Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+H">Haoxian Chen</a>, 
<a href="/search/stat?searchtype=author&query=Lam%2C+H">Henry Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Bayesian Optimization is a popular approach for optimizing expensive
black-box functions. Its key idea is to use a surrogate model to approximate
the objective and, importantly, quantify the associated uncertainty that allows
a sequential search of query points that balance exploitation-exploration.
Gaussian process (GP) has been a primary candidate for the surrogate model,
thanks to its Bayesian-principled uncertainty quantification power and modeling
flexibility. However, its challenges have also spurred an array of alternatives
whose convergence properties could be more opaque. Motivated by these, we study
in this paper an axiomatic framework that elicits the minimal requirements to
guarantee black-box optimization convergence that could apply beyond GP-related
methods. Moreover, we leverage the design freedom in our framework, which we
call Pseudo-Bayesian Optimization, to construct empirically superior
algorithms. In particular, we show how using simple local regression, and a
suitable "randomized prior" construction to quantify uncertainty, not only
guarantees convergence but also consistently outperforms state-of-the-art
benchmarks in examples ranging from high-dimensional synthetic experiments to
realistic hyperparameter tuning and robotic applications.
</p>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09770" title="Abstract">arXiv:2310.09770</a> (cross-list from q-fin.CP) [<a href="/pdf/2310.09770" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Portfolio Rebalancing Approach for the Indian Stock Market
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Sen%2C+J">Jaydip Sen</a>, 
<a href="/search/q-fin?searchtype=author&query=Dasgupta%2C+A">Arup Dasgupta</a>, 
<a href="/search/q-fin?searchtype=author&query=Dasgupta%2C+S">Subhasis Dasgupta</a>, 
<a href="/search/q-fin?searchtype=author&query=Roychoudhury%2C+S">Sayantani Roychoudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the draft version of the chapter that will appear in the edited volume titled "Data Science: Theory and Applications" edited by Jaydip Sen and Sayantani Royc Choudhury. The volume will be published by Cambridge Scholars Publishing, New Castle upon Tyne, UK, in March 2024. The chapter has 80 pages, and it consists of 50 figures, and 13 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">This chapter presents a calendar rebalancing approach to portfolios of stocks
in the Indian stock market. Ten important sectors of the Indian economy are
first selected. For each of these sectors, the top ten stocks are identified
based on their free-float market capitalization values. Using the ten stocks in
each sector, a sector-specific portfolio is designed. In this study, the
historical stock prices are used from January 4, 2021, to September 20, 2023
(NSE Website). The portfolios are designed based on the training data from
January 4, 2021 to June 30, 2022. The performances of the portfolios are tested
over the period from July 1, 2022, to September 20, 2023. The calendar
rebalancing approach presented in the chapter is based on a yearly rebalancing
method. However, the method presented is perfectly flexible and can be adapted
for weekly or monthly rebalancing. The rebalanced portfolios for the ten
sectors are analyzed in detail for their performances. The performance results
are not only indicative of the relative performances of the sectors over the
training (i.e., in-sample) data and test (out-of-sample) data, but they also
reflect the overall effectiveness of the proposed portfolio rebalancing
approach.
</p>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09804" title="Abstract">arXiv:2310.09804</a> (cross-list from math.OC) [<a href="/pdf/2310.09804" title="Download PDF">pdf</a>, <a href="/format/2310.09804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication Compression for Byzantine Robust Learning: New Efficient  Algorithms and Improved Rates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rammal%2C+A">Ahmad Rammal</a>, 
<a href="/search/math?searchtype=author&query=Gruntkowska%2C+K">Kaja Gruntkowska</a>, 
<a href="/search/math?searchtype=author&query=Fedin%2C+N">Nikita Fedin</a>, 
<a href="/search/math?searchtype=author&query=Gorbunov%2C+E">Eduard Gorbunov</a>, 
<a href="/search/math?searchtype=author&query=Richt%C3%A1rik%2C+P">Peter Richt&#xe1;rik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Byzantine robustness is an essential feature of algorithms for certain
distributed optimization problems, typically encountered in
collaborative/federated learning. These problems are usually huge-scale,
implying that communication compression is also imperative for their
resolution. These factors have spurred recent algorithmic and theoretical
developments in the literature of Byzantine-robust learning with compression.
In this paper, we contribute to this research area in two main directions.
First, we propose a new Byzantine-robust method with compression --
Byz-DASHA-PAGE -- and prove that the new method has better convergence rate
(for non-convex and Polyak-Lojasiewicz smooth optimization problems), smaller
neighborhood size in the heterogeneous case, and tolerates more Byzantine
workers under over-parametrization than the previous method with SOTA
theoretical convergence guarantees (Byz-VR-MARINA). Secondly, we develop the
first Byzantine-robust method with communication compression and error feedback
-- Byz-EF21 -- along with its bidirectional compression version -- Byz-EF21-BC
-- and derive the convergence rates for these methods for non-convex and
Polyak-Lojasiewicz smooth case. We test the proposed methods and illustrate our
theoretical findings in the numerical experiments.
</p>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09830" title="Abstract">arXiv:2310.09830</a> (cross-list from math.PR) [<a href="/pdf/2310.09830" title="Download PDF">pdf</a>, <a href="/ps/2310.09830" title="Download PostScript">ps</a>, <a href="/format/2310.09830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence rates for Chernoff-type approximations of convex monotone  semigroups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Blessing%2C+J">Jonas Blessing</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+L">Lianzi Jiang</a>, 
<a href="/search/math?searchtype=author&query=Kupper%2C+M">Michael Kupper</a>, 
<a href="/search/math?searchtype=author&query=Liang%2C+G">Gechun Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We provide explicit convergence rates for Chernoff-type approximations of
convex monotone semigroups which have the form
$S(t)f=\lim_{n\to\infty}I(\frac{t}{n})^n f$ for bounded continuous functions
$f$. Under suitable conditions on the one-step operators $I(t)$ regarding the
time regularity and consistency of the approximation scheme, we obtain
$\|S(t)f-I(\frac{t}{n})^n f\|_\infty\leq cn^{-\gamma}$ for bounded Lipschitz
continuous functions $f$, where $c\geq 0$ and $\gamma&gt;0$ are determined
explicitly. Moreover, the mapping $t\mapsto S(t)f$ is H\"older continuous.
These results are closely related to monotone approximation schemes for
viscosity solutions but are obtained independently by following a recently
developed semigroup approach to Hamilton-Jacobi-Bellman equations which
uniquely characterizes semigroups via their $\Gamma$-generators. The different
approach allows to consider convex rather than sublinear equations and the
results can be extended to unbounded functions by modifying the norm with a
suitable weight function. Furthermore, up to possibly different consistency
errors for the operators $I(t)$, the upper and lower bound for the error
between the semigroup and the iterated operators are symmetric. The abstract
results are applied to Nisio semigroups and limit theorems for convex
expectations.
</p>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09844" title="Abstract">arXiv:2310.09844</a> (cross-list from math.OC) [<a href="/pdf/2310.09844" title="Download PDF">pdf</a>, <a href="/ps/2310.09844" title="Download PostScript">ps</a>, <a href="/format/2310.09844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk-Adaptive Local Decision Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Royset%2C+J+O">Johannes O. Royset</a>, 
<a href="/search/math?searchtype=author&query=Lejeune%2C+M+A">Miguel A. Lejeune</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">For parameterized mixed-binary optimization problems, we construct local
decision rules that prescribe near-optimal courses of action across a set of
parameter values. The decision rules stem from solving risk-adaptive training
problems over classes of continuous, possibly nonlinear mappings. In asymptotic
and nonasymptotic analysis, we establish that the decision rules prescribe
near-optimal decisions locally for the actual problems, without relying on
linearity, convexity, or smoothness. The development also accounts for
practically important aspects such as inexact function evaluations, solution
tolerances in training problems, regularization, and reformulations to
solver-friendly models. The decision rules also furnish a means to carry out
sensitivity and stability analysis for broad classes of parameterized
optimization problems. We develop a decomposition algorithm for solving the
resulting training problems and demonstrate its ability to generate quality
decision rules on a nonlinear binary optimization model from search theory.
</p>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09885" title="Abstract">arXiv:2310.09885</a> (cross-list from math.OC) [<a href="/pdf/2310.09885" title="Download PDF">pdf</a>, <a href="/ps/2310.09885" title="Download PostScript">ps</a>, <a href="/format/2310.09885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On best-response algorithms for monotone Nash equilibrium problems with  mixed-integer variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fabiani%2C+F">Filippo Fabiani</a>, 
<a href="/search/math?searchtype=author&query=Sagratella%2C+S">Simone Sagratella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computer Science and Game Theory (cs.GT); Systems and Control (eess.SY)

</div>
<p class="mathjax">We characterize the convergence properties of traditional best-response (BR)
algorithms in computing solutions to mixed-integer Nash equilibrium problems
(MI-NEPs) that turn into a class of monotone Nash equilibrium problems (NEPs)
once relaxed the integer restrictions. We show that the sequence produced by a
Jacobi/Gauss-Seidel BR method always approaches a bounded region containing the
entire solution set of the MI-NEP, whose tightness depends on the problem data,
and it is related to the degree of strong monotonicity of the relaxed NEP. When
the underlying algorithm is applied to the relaxed NEP, we establish
data-dependent complexity results characterizing its convergence to the unique
solution of the NEP. In addition, we derive one of the very few sufficient
conditions for the existence of solutions to MI-NEPs. The theoretical results
developed bring important practical advantages that are illustrated on a
numerical instance of a smart building control application.
</p>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09903" title="Abstract">arXiv:2310.09903</a> (cross-list from q-fin.ST) [<a href="/pdf/2310.09903" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of feature selection performance for identification of best  effective technical indicators on stock market price prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Moodi%2C+F">Fatemeh Moodi</a>, 
<a href="/search/q-fin?searchtype=author&query=Jahangard-Rafsanjani%2C+A">Amir Jahangard-Rafsanjani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures,4 tables,45 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Due to the influence of many factors, including technical indicators on stock
market prediction, feature selection is important to choose the best
indicators. One of the feature selection methods that consider the performance
of models during feature selection is the wrapper feature selection method. The
aim of this research is to identify a combination of the best stock market
indicators through feature selection to predict the stock market price with the
least error. In order to evaluate the impact of wrapper feature selection
techniques on stock market prediction, in this paper SFS and SBS with 10
estimators and 123 technical indicators have been examined on the last 10 years
of Apple Company. Also, by the proposed method, the data created by the 3-day
time window were converted to the appropriate input for regression methods.
Based on the results observed: (1) Each wrapper feature selection method has
different results with different machine learning methods, and each method is
more correlated with a specific set of technical indicators of the stock
market. (2) Ridge and LR estimates alone, and with two methods of the wrapper
feature selection, namely SFS and SBS; They had the best results with all
assessment criteria for market forecast. (3)The Ridge and LR method with all
the R2, MSE, RMSE, MAE and MAPE have the best stock market prediction results.
Also, the MLP Regression Method, along with the Sequential Forwards Selection
and the MSE, had the best performance. SVR regression, along with the SFS and
the MSE, has improved greatly compared to the SVR regression with all
indicators. (4) It was also observed that different features are selected by
different ML methods with different evaluation parameters. (5) Most ML methods
have used the Squeeze_pro, Percentage Price Oscillator, Thermo, Decay, Archer
On-Balance Volume, Bollinger Bands, Squeeze and Ichimoku indicator.
</p>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09970" title="Abstract">arXiv:2310.09970</a> (cross-list from eess.SP) [<a href="/pdf/2310.09970" title="Download PDF">pdf</a>, <a href="/format/2310.09970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Estimation with Partially Accessible Information: An IMAT  Approach to LMS Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shamsi%2C+M">Mahdi Shamsi</a>, 
<a href="/search/eess?searchtype=author&query=Marvasti%2C+F">Farokh Marvasti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
<p class="mathjax">Distributed algorithms, particularly Diffusion LMS (DLMS), are widely favored
for their reliability, robustness, and fast convergence in various industries.
However, limited observability of the target can compromise the integrity of
the algorithm. To address this issue, this paper proposes a framework for
analyzing combination strategies by drawing inspiration from signal flow
analysis. A thresholding-based algorithm is also presented to identify and
utilize the support vector in scenarios with missing information about the
target vector's support. The proposed approach is demonstrated in two
combination scenarios, showcasing the effectiveness of the algorithm in
situations characterized by sparse observations in the time and transform
domains. The paper concludes with a discussion of the results obtained and
avenues for further exploration.
</p>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09981" title="Abstract">arXiv:2310.09981</a> (cross-list from eess.IV) [<a href="/pdf/2310.09981" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class-Specific Data Augmentation: Bridging the Imbalance in Multiclass  Breast Cancer Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mahammadli%2C+K">Kanan Mahammadli</a>, 
<a href="/search/eess?searchtype=author&query=Bereketoglu%2C+A+B">Abdullah Burkan Bereketoglu</a>, 
<a href="/search/eess?searchtype=author&query=Kabakci%2C+A+G">Ayse Gul Kabakci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Breast Cancer is the most common cancer among women, which is also visible in
men, and accounts for more than 1 in 10 new cancer diagnoses each year. It is
also the second most common cause of women who die from cancer. Hence, it
necessitates early detection and tailored treatment. Early detection can
provide appropriate and patient-based therapeutic schedules. Moreover, early
detection can also provide the type of cyst. This paper employs class-level
data augmentation, addressing the undersampled classes and raising their
detection rate. This approach suggests two key components: class-level data
augmentation on structure-preserving stain normalization techniques to
hematoxylin and eosin-stained images and transformer-based ViTNet architecture
via transfer learning for multiclass classification of breast cancer images.
This merger enables categorizing breast cancer images with advanced image
processing and deep learning as either benign or as one of four distinct
malignant subtypes by focusing on class-level augmentation and catering to
unique characteristics of each class with increasing precision of
classification on undersampled classes, which leads to lower mortality rates
associated with breast cancer. The paper aims to ease the duties of the medical
specialist by operating multiclass classification and categorizing the image
into benign or one of four different malignant types of breast cancers.
</p>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09992" title="Abstract">arXiv:2310.09992</a> (cross-list from math.NT) [<a href="/pdf/2310.09992" title="Download PDF">pdf</a>, <a href="/ps/2310.09992" title="Download PostScript">ps</a>, <a href="/format/2310.09992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On an uncertainty principle for small index subgroups of finite fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Padilla%2C+D+F+D">Diego Fernando D&#xed;az Padilla</a>, 
<a href="/search/math?searchtype=author&query=Arango%2C+J+A+O">Jes&#xfa;s Alonso Ochoa Arango</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, Comments wellcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Information Theory (cs.IT); Commutative Algebra (math.AC); Group Theory (math.GR)

</div>
<p class="mathjax">In this paper we continue the study of the the nonvanishing minors property
(NVM) initiated in [7] by Garcia, Karaali and Katz, for the compressed Fourier
matrix attached to a subgroup $H$ of the multiplicative group of a finite field
$\mathbb{F}_q$ and a character $\chi$ defined over $H$. Here we provide a
characterization of this aforementioned property for \textit{symmetries}
arising from an index-3 subgroup $H$ and nontrivial character $\chi$.
</p>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09998" title="Abstract">arXiv:2310.09998</a> (cross-list from eess.IV) [<a href="/pdf/2310.09998" title="Download PDF">pdf</a>, <a href="/format/2310.09998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeUNet-Trans: A Simple yet Effective UNet-Transformer Model for Medical  Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pham%2C+T">Tan-Hanh Pham</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xianqi Li</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+K">Kim-Doang Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Automated medical image segmentation is becoming increasingly crucial in
modern clinical practice, driven by the growing demand for precise diagnoses,
the push towards personalized treatment plans, and advancements in machine
learning algorithms, especially the incorporation of deep learning methods.
While convolutional neural networks (CNNs) have been prevalent among these
methods, the remarkable potential of Transformer-based models for computer
vision tasks is gaining more acknowledgment. To harness the advantages of both
CNN-based and Transformer-based models, we propose a simple yet effective
UNet-Transformer (seUNet-Trans) model for medical image segmentation. In our
approach, the UNet model is designed as a feature extractor to generate
multiple feature maps from the input images, and these maps are propagated into
a bridge layer, which sequentially connects the UNet and the Transformer. In
this stage, we employ the pixel-level embedding technique without position
embedding vectors to make the model more efficient. Moreover, we applied
spatial-reduction attention in the Transformer to reduce the
computational/memory overhead. By leveraging the UNet architecture and the
self-attention mechanism, our model not only preserves both local and global
context information but also captures long-range dependencies between input
elements. The proposed model is extensively experimented on five medical image
segmentation datasets, including polyp segmentation, to demonstrate its
efficacy. A comparison with several state-of-the-art segmentation models on
these datasets shows the superior performance of seUNet-Trans.
</p>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09999" title="Abstract">arXiv:2310.09999</a> (cross-list from stat.ML) [<a href="/pdf/2310.09999" title="Download PDF">pdf</a>, <a href="/format/2310.09999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier Detection Using Generative Models with Theoretical Performance  Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yi%2C+J">Jirong Yi</a>, 
<a href="/search/stat?searchtype=author&query=Gao%2C+J">Jingchao Gao</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+T">Tianming Wang</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+X">Xiaodong Wu</a>, 
<a href="/search/stat?searchtype=author&query=Xu%2C+W">Weiyu Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/1810.11335">arXiv:1810.11335</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper considers the problem of recovering signals modeled by generative
models from linear measurements contaminated with sparse outliers. We propose
an outlier detection approach for reconstructing the ground-truth signals
modeled by generative models under sparse outliers. We establish theoretical
recovery guarantees for reconstruction of signals using generative models in
the presence of outliers, giving lower bounds on the number of correctable
outliers. Our results are applicable to both linear generator neural networks
and the nonlinear generator neural networks with an arbitrary number of layers.
We propose an iterative alternating direction method of multipliers (ADMM)
algorithm for solving the outlier detection problem via $\ell_1$ norm
minimization, and a gradient descent algorithm for solving the outlier
detection problem via squared $\ell_1$ norm minimization. We conduct extensive
experiments using variational auto-encoder and deep convolutional generative
adversarial networks, and the experimental results show that the signals can be
successfully reconstructed under outliers using our approach. Our approach
outperforms the traditional Lasso and $\ell_2$ minimization approach.
</p>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10002" title="Abstract">arXiv:2310.10002</a> (cross-list from eess.IV) [<a href="/pdf/2310.10002" title="Download PDF">pdf</a>, <a href="/format/2310.10002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Encoder-Decoder Architectures for Robust Coronary Artery  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+S">Shisheng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Gharleghi%2C+R">Ramtin Gharleghi</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+S">Sonit Singh</a>, 
<a href="/search/eess?searchtype=author&query=Sowmya%2C+A">Arcot Sowmya</a>, 
<a href="/search/eess?searchtype=author&query=Beier%2C+S">Susann Beier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages. This article has been accepted for publication in IEEE 38th International Conference on Image and Vision Computing New Zealand (ICVNZ 2023). This is the author's version which has not been fully edited and content may change prior to final publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Coronary artery diseases are among the leading causes of mortality worldwide.
Timely and accurate diagnosis, facilitated by precise coronary artery
segmentation, is pivotal in changing patient outcomes. In the realm of
biomedical imaging, convolutional neural networks, especially the U-Net
architecture, have revolutionised segmentation processes. However, one of the
primary challenges remains the lack of benchmarking datasets specific to
coronary arteries. However through the use of the recently published public
dataset ASOCA, the potential of deep learning for accurate coronary
segmentation can be improved. This paper delves deep into examining the
performance of 25 distinct encoder-decoder combinations. Through analysis of
the 40 cases provided to ASOCA participants, it is revealed that the
EfficientNet-LinkNet combination, serving as encoder and decoder, stands out.
It achieves a Dice coefficient of 0.882 and a 95th percentile Hausdorff
distance of 4.753. These findings not only underscore the superiority of our
model in comparison to those presented at the MICCAI 2020 challenge but also
set the stage for future advancements in coronary artery segmentation, opening
doors to enhanced diagnostic and treatment strategies.
</p>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10003" title="Abstract">arXiv:2310.10003</a> (cross-list from stat.ME) [<a href="/pdf/2310.10003" title="Download PDF">pdf</a>, <a href="/format/2310.10003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Contextual Robust Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Patel%2C+Y">Yash Patel</a>, 
<a href="/search/stat?searchtype=author&query=Rayan%2C+S">Sahana Rayan</a>, 
<a href="/search/stat?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Data-driven approaches to predict-then-optimize decision-making problems seek
to mitigate the risk of uncertainty region misspecification in safety-critical
settings. Current approaches, however, suffer from considering overly
conservative uncertainty regions, often resulting in suboptimal decisionmaking.
To this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for
leveraging highly informative, nonconvex conformal prediction regions over
high-dimensional spaces based on conditional generative models, which have the
desired distribution-free coverage guarantees. Despite guaranteeing robustness,
such black-box optimization procedures alone inspire little confidence owing to
the lack of explanation of why a particular decision was found to be optimal.
We, therefore, augment CPO to additionally provide semantically meaningful
visual summaries of the uncertainty regions to give qualitative intuition for
the optimal decision. We highlight the CPO framework by demonstrating results
on a suite of simulation-based inference benchmark tasks and a vehicle routing
task based on probabilistic weather prediction.
</p>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10006" title="Abstract">arXiv:2310.10006</a> (cross-list from stat.ML) [<a href="/pdf/2310.10006" title="Download PDF">pdf</a>, <a href="/format/2310.10006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit regularization via soft ascent-descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Holland%2C+M+J">Matthew J. Holland</a>, 
<a href="/search/stat?searchtype=author&query=Nakatani%2C+K">Kosuke Nakatani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As models grow larger and more complex, achieving better off-sample
generalization with minimal trial-and-error is critical to the reliability and
economy of machine learning workflows. As a proxy for the well-studied
heuristic of seeking "flat" local minima, gradient regularization is a natural
avenue, and first-order approximations such as Flooding and sharpness-aware
minimization (SAM) have received significant attention, but their performance
depends critically on hyperparameters (flood threshold and neighborhood radius,
respectively) that are non-trivial to specify in advance. In order to develop a
procedure which is more resilient to misspecified hyperparameters, with the
hard-threshold "ascent-descent" switching device used in Flooding as
motivation, we propose a softened, pointwise mechanism called SoftAD that
downweights points on the borderline, limits the effects of outliers, and
retains the ascent-descent effect. We contrast formal stationarity guarantees
with those for Flooding, and empirically demonstrate how SoftAD can realize
classification accuracy competitive with SAM and Flooding while maintaining a
much smaller loss generalization gap and model norm. Our empirical tests range
from simple binary classification on the plane to image classification using
neural networks with millions of parameters; the key trends are observed across
all datasets and models studied, and suggest a potential new approach to
implicit regularization.
</p>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10013" title="Abstract">arXiv:2310.10013</a> (cross-list from stat.ML) [<a href="/pdf/2310.10013" title="Download PDF">pdf</a>, <a href="/format/2310.10013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Riemannian Residual Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Katsman%2C+I">Isay Katsman</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+E+M">Eric Ming Chen</a>, 
<a href="/search/stat?searchtype=author&query=Holalkere%2C+S">Sidhanth Holalkere</a>, 
<a href="/search/stat?searchtype=author&query=Asch%2C+A">Anna Asch</a>, 
<a href="/search/stat?searchtype=author&query=Lou%2C+A">Aaron Lou</a>, 
<a href="/search/stat?searchtype=author&query=Lim%2C+S">Ser-Nam Lim</a>, 
<a href="/search/stat?searchtype=author&query=De+Sa%2C+C">Christopher De Sa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent methods in geometric deep learning have introduced various neural
networks to operate over data that lie on Riemannian manifolds. Such networks
are often necessary to learn well over graphs with a hierarchical structure or
to learn over manifold-valued data encountered in the natural sciences. These
networks are often inspired by and directly generalize standard Euclidean
neural networks. However, extending Euclidean networks is difficult and has
only been done for a select few manifolds. In this work, we examine the
residual neural network (ResNet) and show how to extend this construction to
general Riemannian manifolds in a geometrically principled manner. Originally
introduced to help solve the vanishing gradient problem, ResNets have become
ubiquitous in machine learning due to their beneficial learning properties,
excellent empirical results, and easy-to-incorporate nature when building
varied neural networks. We find that our Riemannian ResNets mirror these
desirable properties: when compared to existing manifold neural networks
designed to learn over hyperbolic space and the manifold of symmetric positive
definite matrices, we outperform both kinds of networks in terms of relevant
testing metrics and training dynamics.
</p>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10024" title="Abstract">arXiv:2310.10024</a> (cross-list from econ.TH) [<a href="/pdf/2310.10024" title="Download PDF">pdf</a>, <a href="/format/2310.10024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Managing Persuasion Robustly: The Optimality of Quota Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Bergemann%2C+D">Dirk Bergemann</a>, 
<a href="/search/econ?searchtype=author&query=Gan%2C+T">Tan Gan</a>, 
<a href="/search/econ?searchtype=author&query=Li%2C+Y">Yingkai Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We study a sender-receiver model where the receiver can commit to a decision
rule before the sender determines the information policy. The decision rule can
depend on the signal structure and the signal realization that the sender
adopts. This framework captures applications where a decision-maker (the
receiver) solicit advice from an interested party (sender). In these
applications, the receiver faces uncertainty regarding the sender's preferences
and the set of feasible signal structures. Consequently, we adopt a unified
robust analysis framework that includes max-min utility, min-max regret, and
min-max approximation ratio as special cases. We show that it is optimal for
the receiver to sacrifice ex-post optimality to perfectly align the sender's
incentive. The optimal decision rule is a quota rule, i.e., the decision rule
maximizes the receiver's ex-ante payoff subject to the constraint that the
marginal distribution over actions adheres to a consistent quota, regardless of
the sender's chosen signal structure.
</p>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10026" title="Abstract">arXiv:2310.10026</a> (cross-list from eess.AS) [<a href="/pdf/2310.10026" title="Download PDF">pdf</a>, <a href="/format/2310.10026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Speech Enhancement and Separation with a Unified Deep Neural  Network for Single/Dual Talker Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Patel%2C+K">Kashyap Patel</a>, 
<a href="/search/eess?searchtype=author&query=Kovalyov%2C+A">Anton Kovalyov</a>, 
<a href="/search/eess?searchtype=author&query=Panahi%2C+I">Issa Panahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 Pages, Accepted at IEEE Asilomar
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">This paper introduces a practical approach for leveraging a real-time deep
learning model to alternate between speech enhancement and joint speech
enhancement and separation depending on whether the input mixture contains one
or two active speakers. Scale-invariant signal-to-distortion ratio (SI-SDR) has
shown to be a highly effective training measure in time-domain speech
separation. However, the SI-SDR metric is ill-defined for zero-energy target
signals, which is a problem when training a speech separation model using
utterances with varying numbers of talkers. Unlike existing solutions that
focus on modifying the loss function to accommodate zero-energy target signals,
the proposed approach circumvents this problem by training the model to extract
speech on both its output channels regardless if the input is a single or
dual-talker mixture. A lightweight speaker overlap detection (SOD) module is
also introduced to differentiate between single and dual-talker segments in
real-time. The proposed module takes advantage of the new formulation by
operating directly on the separated masks, given by the separation model,
instead of the original mixture, thus effectively simplifying the detection
task. Experimental results show that the proposed training approach outperforms
existing solutions, and the SOD module exhibits high accuracy.
</p>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10044" title="Abstract">arXiv:2310.10044</a> (cross-list from eess.IV) [<a href="/pdf/2310.10044" title="Download PDF">pdf</a>, <a href="/format/2310.10044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperspectral Image Fusion via Logarithmic Low-rank Tensor Ring  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Lipeng Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shutao Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Integrating a low-spatial-resolution hyperspectral image (LR-HSI) with a
high-spatial-resolution multispectral image (HR-MSI) is recognized as a valid
method for acquiring HR-HSI. Among the current fusion approaches, the tensor
ring (TR) decomposition-based method has received growing attention owing to
its superior performance on preserving the spatial-spectral correlation.
Furthermore, the low-rank property in some TR factors has been exploited via
the matrix nuclear norm regularization along mode-2. On the other hand, the
tensor nuclear norm (TNN)-based approaches have recently demonstrated to be
more efficient on keeping high-dimensional low-rank structures in tensor
recovery. Here, we study the low-rankness of TR factors from the TNN
perspective and consider the mode-2 logarithmic TNN (LTNN) on each TR factor. A
novel fusion model is proposed by incorporating this LTNN regularization and
the weighted total variation which is to promote the continuity of HR-HSI in
the spatial-spectral domain. Meanwhile, we have devised a highly efficient
proximal alternating minimization algorithm to solve the proposed model. The
experimental results indicate that our method improves the visual quality and
exceeds the existing state-of-the-art fusion approaches with respect to various
quantitative metrics.
</p>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10082" title="Abstract">arXiv:2310.10082</a> (cross-list from math.OC) [<a href="/pdf/2310.10082" title="Download PDF">pdf</a>, <a href="/format/2310.10082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A simple uniformly optimal method without line search for convex  optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+T">Tianjiao Li</a>, 
<a href="/search/math?searchtype=author&query=Lan%2C+G">Guanghui Lan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Line search (or backtracking) procedures have been widely employed into
first-order methods for solving convex optimization problems, especially those
with unknown problem parameters (e.g., Lipschitz constant). In this paper, we
show that line search is superfluous in attaining the optimal rate of
convergence for solving a convex optimization problem whose parameters are not
given a priori. In particular, we present a novel accelerated gradient descent
type algorithm called auto-conditioned fast gradient method (AC-FGM) that can
achieve an optimal $\mathcal{O}(1/k^2)$ rate of convergence for smooth convex
optimization without requiring the estimate of a global Lipschitz constant or
the employment of line search procedures. We then extend AC-FGM to solve convex
optimization problems with H\"{o}lder continuous gradients and show that it
automatically achieves the optimal rates of convergence uniformly for all
problem classes with the desired accuracy of the solution as the only input.
Finally, we report some encouraging numerical results that demonstrate the
advantages of AC-FGM over the previously developed parameter-free methods for
convex optimization.
</p>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10088" title="Abstract">arXiv:2310.10088</a> (cross-list from eess.IV) [<a href="/pdf/2310.10088" title="Download PDF">pdf</a>, <a href="/format/2310.10088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PUCA: Patch-Unshuffle and Channel Attention for Enhanced Self-Supervised  Image Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jang%2C+H">Hyemi Jang</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+J">Junsung Park</a>, 
<a href="/search/eess?searchtype=author&query=Jung%2C+D">Dahuin Jung</a>, 
<a href="/search/eess?searchtype=author&query=Lew%2C+J">Jaihyun Lew</a>, 
<a href="/search/eess?searchtype=author&query=Bae%2C+H">Ho Bae</a>, 
<a href="/search/eess?searchtype=author&query=Yoon%2C+S">Sungroh Yoon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although supervised image denoising networks have shown remarkable
performance on synthesized noisy images, they often fail in practice due to the
difference between real and synthesized noise. Since clean-noisy image pairs
from the real world are extremely costly to gather, self-supervised learning,
which utilizes noisy input itself as a target, has been studied. To prevent a
self-supervised denoising model from learning identical mapping, each output
pixel should not be influenced by its corresponding input pixel; This
requirement is known as J-invariance. Blind-spot networks (BSNs) have been a
prevalent choice to ensure J-invariance in self-supervised image denoising.
However, constructing variations of BSNs by injecting additional operations
such as downsampling can expose blinded information, thereby violating
J-invariance. Consequently, convolutions designed specifically for BSNs have
been allowed only, limiting architectural flexibility. To overcome this
limitation, we propose PUCA, a novel J-invariant U-Net architecture, for
self-supervised denoising. PUCA leverages patch-unshuffle/shuffle to
dramatically expand receptive fields while maintaining J-invariance and dilated
attention blocks (DABs) for global context incorporation. Experimental results
demonstrate that PUCA achieves state-of-the-art performance, outperforming
existing methods in self-supervised image denoising.
</p>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10093" title="Abstract">arXiv:2310.10093</a> (cross-list from physics.optics) [<a href="/pdf/2310.10093" title="Download PDF">pdf</a>, <a href="/format/2310.10093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-speed full-color computer-generated holography using a digital  micromirror device and fiber-coupled RGB laser diode
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Yoshida%2C+S">Shuhei Yoshida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Computer-generated holography (CGH) can be used to display three-dimensional
(3D) images and has a special feature that no other technology possesses: it
can reconstruct arbitrary object wavefronts. In this study, we investigated a
high-speed full-color reconstruction method for improving the realism of 3D
images produced using CGH. The proposed method uses a digital micromirror
device (DMD) with a high-speed switching capability as the hologram display
device. It produces 3D video by time-division multiplexing using an optical
system incorporating fiber-coupled laser diodes (LDs) operating in red, green,
and blue wavelengths. The wavelength dispersion of the DMD is compensated for
by superimposing plane waves on the hologram. Fourier transform optics are used
to separate the object, conjugate, and zeroth-order light, thus eliminating the
need for an extensive 4f system. The resources used in this research, such as
the programs used for the hologram generation and the schematics of the LD
driver, are available on GitHub.
</p>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10095" title="Abstract">arXiv:2310.10095</a> (cross-list from eess.IV) [<a href="/pdf/2310.10095" title="Download PDF">pdf</a>, <a href="/format/2310.10095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Scale Spatial Transformer U-Net for Simultaneously Automatic  Reorientation and Segmentation of 3D Nuclear Cardiac Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ni%2C+Y">Yangfan Ni</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+D">Duo Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+G">Gege Ma</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+L">Lijun Lu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Z">Zhongke Huang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+W">Wentao Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurate reorientation and segmentation of the left ventricular (LV) is
essential for the quantitative analysis of myocardial perfusion imaging (MPI),
in which one critical step is to reorient the reconstructed transaxial nuclear
cardiac images into standard short-axis slices for subsequent image processing.
Small-scale LV myocardium (LV-MY) region detection and the diverse cardiac
structures of individual patients pose challenges to LV segmentation operation.
To mitigate these issues, we propose an end-to-end model, named as multi-scale
spatial transformer UNet (MS-ST-UNet), that involves the multi-scale spatial
transformer network (MSSTN) and multi-scale UNet (MSUNet) modules to perform
simultaneous reorientation and segmentation of LV region from nuclear cardiac
images. The proposed method is trained and tested using two different nuclear
cardiac image modalities: 13N-ammonia PET and 99mTc-sestamibi SPECT. We use a
multi-scale strategy to generate and extract image features with different
scales. Our experimental results demonstrate that the proposed method
significantly improves the reorientation and segmentation performance. This
joint learning framework promotes mutual enhancement between reorientation and
segmentation tasks, leading to cutting edge performance and an efficient image
processing workflow. The proposed end-to-end deep network has the potential to
reduce the burden of manual delineation for cardiac images, thereby providing
multimodal quantitative analysis assistance for physicists.
</p>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10143" title="Abstract">arXiv:2310.10143</a> (cross-list from stat.ML) [<a href="/pdf/2310.10143" title="Download PDF">pdf</a>, <a href="/format/2310.10143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of Simplicial Representation Learning with  Wasserstein Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yamada%2C+M">Makoto Yamada</a>, 
<a href="/search/stat?searchtype=author&query=Takezawa%2C+Y">Yuki Takezawa</a>, 
<a href="/search/stat?searchtype=author&query=Houry%2C+G">Guillaume Houry</a>, 
<a href="/search/stat?searchtype=author&query=Dusterwald%2C+K+M">Kira Michaela Dusterwald</a>, 
<a href="/search/stat?searchtype=author&query=Sulem%2C+D">Deborah Sulem</a>, 
<a href="/search/stat?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/stat?searchtype=author&query=Tsai%2C+Y+H">Yao-Hung Hubert Tsai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we delve into the problem of simplicial representation
learning utilizing the 1-Wasserstein distance on a tree structure (a.k.a.,
Tree-Wasserstein distance (TWD)), where TWD is defined as the L1 distance
between two tree-embedded vectors. Specifically, we consider a framework for
simplicial representation estimation employing a self-supervised learning
approach based on SimCLR with a negative TWD as a similarity measure. In
SimCLR, the cosine similarity with real-vector embeddings is often utilized;
however, it has not been well studied utilizing L1-based measures with
simplicial embeddings. A key challenge is that training the L1 distance is
numerically challenging and often yields unsatisfactory outcomes, and there are
numerous choices for probability models. Thus, this study empirically
investigates a strategy for optimizing self-supervised learning with TWD and
find a stable training procedure. More specifically, we evaluate the
combination of two types of TWD (total variation and ClusterTree) and several
simplicial models including the softmax function, the ArcFace probability
model, and simplicial embedding. Moreover, we propose a simple yet effective
Jeffrey divergence-based regularization method to stabilize the optimization.
Through empirical experiments on STL10, CIFAR10, CIFAR100, and SVHN, we first
found that the simple combination of softmax function and TWD can obtain
significantly lower results than the standard SimCLR (non-simplicial model and
cosine similarity). We found that the model performance depends on the
combination of TWD and the simplicial model, and the Jeffrey divergence
regularization usually helps model training. Finally, we inferred that the
appropriate choice of combination of TWD and simplicial models outperformed
cosine similarity based representation learning.
</p>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10162" title="Abstract">arXiv:2310.10162</a> (cross-list from math.CO) [<a href="/pdf/2310.10162" title="Download PDF">pdf</a>, <a href="/ps/2310.10162" title="Download PostScript">ps</a>, <a href="/format/2310.10162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bent functions satisfying the dual bent condition and permutations with  the $(\mathcal{A}_m)$ property
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Polujan%2C+A">Alexandr Polujan</a>, 
<a href="/search/math?searchtype=author&query=Pasalic%2C+E">Enes Pasalic</a>, 
<a href="/search/math?searchtype=author&query=Kudin%2C+S">Sadmir Kudin</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+F">Fengrong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The concatenation of four Boolean bent functions $f=f_1||f_2||f_3||f_4$ is
bent if and only if the dual bent condition $f_1^* + f_2^* + f_3^* + f_4^* =1$
is satisfied. However, to specify four bent functions satisfying this duality
condition is in general quite a difficult task. Commonly, to simplify this
problem, certain connections between $f_i$ are assumed, as well as functions
$f_i$ of a special shape are considered, e.g., $f_i(x,y)=x\cdot\pi_i(y)+h_i(y)$
are Maiorana-McFarland bent functions. In the case when permutations $\pi_i$ of
$\mathbb{F}_2^m$ have the $(\mathcal{A}_m)$ property and Maiorana-McFarland
bent functions $f_i$ satisfy the additional condition $f_1+f_2+f_3+f_4=0$, the
dual bent condition is known to have a relatively simple shape allowing to
specify the functions $f_i$ explicitly. In this paper, we generalize this
result for the case when Maiorana-McFarland bent functions $f_i$ satisfy the
condition $f_1(x,y)+f_2(x,y)+f_3(x,y)+f_4(x,y)=s(y)$ and provide a construction
of new permutations with the $(\mathcal{A}_m)$ property from the old ones.
Combining these two results, we obtain a recursive construction method of bent
functions satisfying the dual bent condition. Moreover, we provide a generic
condition on the Maiorana-McFarland bent functions stemming from the
permutations of $\mathbb{F}_2^m$ with the $(\mathcal{A}_m)$ property, such that
their concatenation does not belong, up to equivalence, to the
Maiorana-McFarland class. Using monomial permutations $\pi_i$ of
$\mathbb{F}_{2^m}$ with the $(\mathcal{A}_m)$ property and monomial functions
$h_i$ on $\mathbb{F}_{2^m}$, we provide explicit constructions of such bent
functions. Finally, with our construction method, we explain how one can
construct homogeneous cubic bent functions, noticing that only very few design
methods of these objects are known.
</p>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10171" title="Abstract">arXiv:2310.10171</a> (cross-list from stat.ML) [<a href="/pdf/2310.10171" title="Download PDF">pdf</a>, <a href="/format/2310.10171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On permutation symmetries in Bayesian neural network posteriors: a  variational perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rossi%2C+S">Simone Rossi</a>, 
<a href="/search/stat?searchtype=author&query=Singh%2C+A">Ankit Singh</a>, 
<a href="/search/stat?searchtype=author&query=Hannagan%2C+T">Thomas Hannagan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The elusive nature of gradient-based optimization in neural networks is tied
to their loss landscape geometry, which is poorly understood. However recent
work has brought solid evidence that there is essentially no loss barrier
between the local solutions of gradient descent, once accounting for
weight-permutations that leave the network's computation unchanged. This raises
questions for approximate inference in Bayesian neural networks (BNNs), where
we are interested in marginalizing over multiple points in the loss landscape.
In this work, we first extend the formalism of marginalized loss barrier and
solution interpolation to BNNs, before proposing a matching algorithm to search
for linearly connected solutions. This is achieved by aligning the
distributions of two independent approximate Bayesian solutions with respect to
permutation matrices. We build on the results of Ainsworth et al. (2023),
reframing the problem as a combinatorial optimization one, using an
approximation to the sum of bilinear assignment problem. We then experiment on
a variety of architectures and datasets, finding nearly zero marginalized loss
barriers for linearly connected solutions.
</p>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10179" title="Abstract">arXiv:2310.10179</a> (cross-list from eess.AS) [<a href="/pdf/2310.10179" title="Download PDF">pdf</a>, <a href="/format/2310.10179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Audio Emotion and Intent Recognition with Large Pre-Trained  Models and Bayesian Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Porjazovski%2C+D">Dejan Porjazovski</a>, 
<a href="/search/eess?searchtype=author&query=Getman%2C+Y">Yaroslav Getman</a>, 
<a href="/search/eess?searchtype=author&query=Gr%C3%B3sz%2C+T">Tam&#xe1;s Gr&#xf3;sz</a>, 
<a href="/search/eess?searchtype=author&query=Kurimo%2C+M">Mikko Kurimo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACMM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Large pre-trained models are essential in paralinguistic systems,
demonstrating effectiveness in tasks like emotion recognition and stuttering
detection. In this paper, we employ large pre-trained models for the ACM
Multimedia Computational Paralinguistics Challenge, addressing the Requests and
Emotion Share tasks. We explore audio-only and hybrid solutions leveraging
audio and text modalities. Our empirical results consistently show the
superiority of the hybrid approaches over the audio-only models. Moreover, we
introduce a Bayesian layer as an alternative to the standard linear output
layer. The multimodal fusion approach achieves an 85.4% UAR on HC-Requests and
60.2% on HC-Complaints. The ensemble model for the Emotion Share task yields
the best rho value of .614. The Bayesian wav2vec2 approach, explored in this
study, allows us to easily build ensembles, at the cost of fine-tuning only one
model. Moreover, we can have usable confidence values instead of the usual
overconfident posterior probabilities.
</p>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10200" title="Abstract">arXiv:2310.10200</a> (cross-list from math.CO) [<a href="/pdf/2310.10200" title="Download PDF">pdf</a>, <a href="/ps/2310.10200" title="Download PostScript">ps</a>, <a href="/format/2310.10200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Circular External Difference Families: Construction and Non-Existence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wu%2C+H">Huawei Wu</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+J">Jing Yang</a>, 
<a href="/search/math?searchtype=author&query=Feng%2C+K">Keqin Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The circular external difference family and its strong version, which
themselves are of independent combinatorial interest, were proposed as variants
of the difference family to construct new unconditionally secure non-malleable
threshold schemes. In this paper, we present new results regarding the
construction and non-existence of (strong) circular external difference
families, thereby solving several open problems on this topic.
</p>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10209" title="Abstract">arXiv:2310.10209</a> (cross-list from eess.IV) [<a href="/pdf/2310.10209" title="Download PDF">pdf</a>, <a href="/format/2310.10209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Fetal MRI 3D Reconstruction Based on Radiation Diffusion  Generation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tan%2C+J">Junpeng Tan</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Lv%2C+Y">Yao Lv</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+X">Xiangmin Xu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+G">Gang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although the use of multiple stacks can handle slice-to-volume motion
correction and artifact removal problems, there are still several problems: 1)
The slice-to-volume method usually uses slices as input, which cannot solve the
problem of uniform intensity distribution and complementarity in regions of
different fetal MRI stacks; 2) The integrity of 3D space is not considered,
which adversely affects the discrimination and generation of globally
consistent information in fetal MRI; 3) Fetal MRI with severe motion artifacts
in the real-world cannot achieve high-quality super-resolution reconstruction.
To address these issues, we propose a novel fetal brain MRI high-quality volume
reconstruction method, called the Radiation Diffusion Generation Model (RDGM).
It is a self-supervised generation method, which incorporates the idea of
Neural Radiation Field (NeRF) based on the coordinate generation and diffusion
model based on super-resolution generation. To solve regional intensity
heterogeneity in different directions, we use a pre-trained transformer model
for slice registration, and then, a new regionally Consistent Implicit Neural
Representation (CINR) network sub-module is proposed. CINR can generate the
initial volume by combining a coordinate association map of two different
coordinate mapping spaces. To enhance volume global consistency and
discrimination, we introduce the Volume Diffusion Super-resolution Generation
(VDSG) mechanism. The global intensity discriminant generation from
volume-to-volume is carried out using the idea of diffusion generation, and
CINR becomes the deviation intensity generation network of the volume-to-volume
diffusion model. Finally, the experimental results on real-world fetal brain
MRI stacks demonstrate the state-of-the-art performance of our method.
</p>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10224" title="Abstract">arXiv:2310.10224</a> (cross-list from eess.IV) [<a href="/pdf/2310.10224" title="Download PDF">pdf</a>, <a href="/format/2310.10224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing Medical Image Representations via Quaternion Wavelet  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sigillo%2C+L">Luigi Sigillo</a>, 
<a href="/search/eess?searchtype=author&query=Grassucci%2C+E">Eleonora Grassucci</a>, 
<a href="/search/eess?searchtype=author&query=Uncini%2C+A">Aurelio Uncini</a>, 
<a href="/search/eess?searchtype=author&query=Comminiello%2C+D">Danilo Comminiello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to IEEE Transactions on Medical Imaging
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural network generalizability is becoming a broad research field due to the
increasing availability of datasets from different sources and for various
tasks. This issue is even wider when processing medical data, where a lack of
methodological standards causes large variations being provided by different
imaging centers or acquired with various devices and cofactors. To overcome
these limitations, we introduce a novel, generalizable, data- and task-agnostic
framework able to extract salient features from medical images. The proposed
quaternion wavelet network (QUAVE) can be easily integrated with any
pre-existing medical image analysis or synthesis task, and it can be involved
with real, quaternion, or hypercomplex-valued models, generalizing their
adoption to single-channel data. QUAVE first extracts different sub-bands
through the quaternion wavelet transform, resulting in both
low-frequency/approximation bands and high-frequency/fine-grained features.
Then, it weighs the most representative set of sub-bands to be involved as
input to any other neural model for image processing, replacing standard data
samples. We conduct an extensive experimental evaluation comprising different
datasets, diverse image analysis, and synthesis tasks including reconstruction,
segmentation, and modality translation. We also evaluate QUAVE in combination
with both real and quaternion-valued models. Results demonstrate the
effectiveness and the generalizability of the proposed framework that improves
network performance while being flexible to be adopted in manifold scenarios.
</p>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10239" title="Abstract">arXiv:2310.10239</a> (cross-list from stat.ML) [<a href="/pdf/2310.10239" title="Download PDF">pdf</a>, <a href="/format/2310.10239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural transfer learning of non-Gaussian DAG
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ren%2C+M">Mingyang Ren</a>, 
<a href="/search/stat?searchtype=author&query=He%2C+X">Xin He</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+J">Junhui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Directed acyclic graph (DAG) has been widely employed to represent
directional relationships among a set of collected nodes. Yet, the available
data in one single study is often limited for accurate DAG reconstruction,
whereas heterogeneous data may be collected from multiple relevant studies. It
remains an open question how to pool the heterogeneous data together for better
DAG structure reconstruction in the target study. In this paper, we first
introduce a novel set of structural similarity measures for DAG and then
present a transfer DAG learning framework by effectively leveraging information
from auxiliary DAGs of different levels of similarities. Our theoretical
analysis shows substantial improvement in terms of DAG reconstruction in the
target study, even when no auxiliary DAG is overall similar to the target DAG,
which is in sharp contrast to most existing transfer learning methods. The
advantage of the proposed transfer DAG learning is also supported by extensive
numerical experiments on both synthetic data and multi-site brain functional
connectivity network data.
</p>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10240" title="Abstract">arXiv:2310.10240</a> (cross-list from stat.ML) [<a href="/pdf/2310.10240" title="Download PDF">pdf</a>, <a href="/format/2310.10240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Mixtures and the Neural Critics: On the Pointwise Mutual Information  Profiles of Fine Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Czy%C5%BC%2C+P">Pawe&#x142; Czy&#x17c;</a>, 
<a href="/search/stat?searchtype=author&query=Grabowski%2C+F">Frederic Grabowski</a>, 
<a href="/search/stat?searchtype=author&query=Vogt%2C+J+E">Julia E. Vogt</a>, 
<a href="/search/stat?searchtype=author&query=Beerenwinkel%2C+N">Niko Beerenwinkel</a>, 
<a href="/search/stat?searchtype=author&query=Marx%2C+A">Alexander Marx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The accompanying code is accessible on GitHub: <a href="https://github.com/cbg-ethz/bmi">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">Mutual information quantifies the dependence between two random variables and
remains invariant under diffeomorphisms. In this paper, we explore the
pointwise mutual information profile, an extension of mutual information that
maintains this invariance. We analytically describe the profiles of
multivariate normal distributions and introduce the family of fine
distributions, for which the profile can be accurately approximated using Monte
Carlo methods. We then show how fine distributions can be used to study the
limitations of existing mutual information estimators, investigate the behavior
of neural critics used in variational estimators, and understand the effect of
experimental outliers on mutual information estimation. Finally, we show how
fine distributions can be used to obtain model-based Bayesian estimates of
mutual information, suitable for problems with available domain expertise in
which uncertainty quantification is necessary.
</p>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10248" title="Abstract">arXiv:2310.10248</a> (cross-list from eess.IV) [<a href="/pdf/2310.10248" title="Download PDF">pdf</a>, <a href="/format/2310.10248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-term Dependency for 3D Reconstruction of Freehand Ultrasound  Without External Tracker
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+Z">Ziyi Shen</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/eess?searchtype=author&query=Barratt%2C+D+C">Dean C. Barratt</a>, 
<a href="/search/eess?searchtype=author&query=Dowrick%2C+T">Thomas Dowrick</a>, 
<a href="/search/eess?searchtype=author&query=Clarkson%2C+M+J">Matthew J. Clarkson</a>, 
<a href="/search/eess?searchtype=author&query=Vercauteren%2C+T">Tom Vercauteren</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+Y">Yipeng Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Transactions on Biomedical Engineering (TBME, 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Objective: Reconstructing freehand ultrasound in 3D without any external
tracker has been a long-standing challenge in ultrasound-assisted procedures.
We aim to define new ways of parameterising long-term dependencies, and
evaluate the performance. Methods: First, long-term dependency is encoded by
transformation positions within a frame sequence. This is achieved by combining
a sequence model with a multi-transformation prediction. Second, two dependency
factors are proposed, anatomical image content and scanning protocol, for
contributing towards accurate reconstruction. Each factor is quantified
experimentally by reducing respective training variances. Results: 1) The added
long-term dependency up to 400 frames at 20 frames per second (fps) indeed
improved reconstruction, with an up to 82.4% lowered accumulated error,
compared with the baseline performance. The improvement was found to be
dependent on sequence length, transformation interval and scanning protocol
and, unexpectedly, not on the use of recurrent networks with long-short term
modules; 2) Decreasing either anatomical or protocol variance in training led
to poorer reconstruction accuracy. Interestingly, greater performance was
gained from representative protocol patterns, than from representative
anatomical features. Conclusion: The proposed algorithm uses hyperparameter
tuning to effectively utilise long-term dependency. The proposed dependency
factors are of practical significance in collecting diverse training data,
regulating scanning protocols and developing efficient networks. Significance:
The proposed new methodology with publicly available volunteer data and code
for parametersing the long-term dependency, experimentally shown to be valid
sources of performance improvement, which could potentially lead to better
model development and practical optimisation of the reconstruction application.
</p>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10315" title="Abstract">arXiv:2310.10315</a> (cross-list from quant-ph) [<a href="/pdf/2310.10315" title="Download PDF">pdf</a>, <a href="/format/2310.10315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Quantum Machine Learning: Current Trends, Challenges,  Opportunities, and the Road Ahead
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zaman%2C+K">Kamila Zaman</a>, 
<a href="/search/quant-ph?searchtype=author&query=Marchisio%2C+A">Alberto Marchisio</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hanif%2C+M+A">Muhammad Abdullah Hanif</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shafique%2C+M">Muhammad Shafique</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantum Computing (QC) claims to improve the efficiency of solving complex
problems, compared to classical computing. When QC is applied to Machine
Learning (ML) applications, it forms a Quantum Machine Learning (QML) system.
After discussing the basic concepts of QC and its advantages over classical
computing, this paper reviews the key aspects of QML in a comprehensive manner.
We discuss different QML algorithms and their domain applicability, quantum
datasets, hardware technologies, software tools, simulators, and applications.
In this survey, we provide valuable information and resources for readers to
jumpstart into the current state-of-the-art techniques in the QML field.
</p>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10359" title="Abstract">arXiv:2310.10359</a> (cross-list from stat.ML) [<a href="/pdf/2310.10359" title="Download PDF">pdf</a>, <a href="/format/2310.10359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Anytime Algorithm for Good Arm Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Jourdan%2C+M">Marc Jourdan</a>, 
<a href="/search/stat?searchtype=author&query=R%C3%A9da%2C+C">Cl&#xe9;mence R&#xe9;da</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 56 pages, 23 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In good arm identification (GAI), the goal is to identify one arm whose
average performance exceeds a given threshold, referred to as good arm, if it
exists. Few works have studied GAI in the fixed-budget setting, when the
sampling budget is fixed beforehand, or the anytime setting, when a
recommendation can be asked at any time. We propose APGAI, an anytime and
parameter-free sampling rule for GAI in stochastic bandits. APGAI can be
straightforwardly used in fixed-confidence and fixed-budget settings. First, we
derive upper bounds on its probability of error at any time. They show that
adaptive strategies are more efficient in detecting the absence of good arms
than uniform sampling. Second, when APGAI is combined with a stopping rule, we
prove upper bounds on the expected sampling complexity, holding at any
confidence level. Finally, we show good empirical performance of APGAI on
synthetic and real-world data. Our work offers an extensive overview of the GAI
problem in all settings.
</p>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10408" title="Abstract">arXiv:2310.10408</a> (cross-list from eess.IV) [<a href="/pdf/2310.10408" title="Download PDF">pdf</a>, <a href="/format/2310.10408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A cross Transformer for image denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tian%2C+C">Chunwei Tian</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+M">Menghua Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+S">Shichao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yanning Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Ling%2C+C">Chia-Wen Ling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep convolutional neural networks (CNNs) depend on feedforward and feedback
ways to obtain good performance in image denoising. However, how to obtain
effective structural information via CNNs to efficiently represent given noisy
images is key for complex scenes. In this paper, we propose a cross Transformer
denoising CNN (CTNet) with a serial block (SB), a parallel block (PB), and a
residual block (RB) to obtain clean images for complex scenes. A SB uses an
enhanced residual architecture to deeply search structural information for
image denoising. To avoid loss of key information, PB uses three heterogeneous
networks to implement multiple interactions of multi-level features to broadly
search for extra information for improving the adaptability of an obtained
denoiser for complex scenes. Also, to improve denoising performance,
Transformer mechanisms are embedded into the SB and PB to extract complementary
salient features for effectively removing noise in terms of pixel relations.
Finally, a RB is applied to acquire clean images. Experiments illustrate that
our CTNet is superior to some popular denoising methods in terms of real and
synthetic image denoising. It is suitable to mobile digital devices, i.e.,
phones. Codes can be obtained at https://github.com/hellloxiaotian/CTNet.
</p>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10413" title="Abstract">arXiv:2310.10413</a> (cross-list from eess.IV) [<a href="/pdf/2310.10413" title="Download PDF">pdf</a>, <a href="/format/2310.10413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image super-resolution via dynamic network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tian%2C+C">Chunwei Tian</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xuanyu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+M">Mingming Yang</a>, 
<a href="/search/eess?searchtype=author&query=Ju%2C+Z">Zhaojie Ju</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages,3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Convolutional neural networks (CNNs) depend on deep network architectures to
extract accurate information for image super-resolution. However, obtained
information of these CNNs cannot completely express predicted high-quality
images for complex scenes. In this paper, we present a dynamic network for
image super-resolution (DSRNet), which contains a residual enhancement block,
wide enhancement block, feature refinement block and construction block. The
residual enhancement block is composed of a residual enhanced architecture to
facilitate hierarchical features for image super-resolution. To enhance
robustness of obtained super-resolution model for complex scenes, a wide
enhancement block achieves a dynamic architecture to learn more robust
information to enhance applicability of an obtained super-resolution model for
varying scenes. To prevent interference of components in a wide enhancement
block, a refinement block utilizes a stacked architecture to accurately learn
obtained features. Also, a residual learning operation is embedded in the
refinement block to prevent long-term dependency problem. Finally, a
construction block is responsible for reconstructing high-quality images.
Designed heterogeneous architecture can not only facilitate richer structural
information, but also be lightweight, which is suitable for mobile digital
devices. Experimental results shows that our method is more competitive in
terms of performance and recovering time of image super-resolution and
complexity. The code of DSRNet can be obtained at
https://github.com/hellloxiaotian/DSRNet.
</p>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10414" title="Abstract">arXiv:2310.10414</a> (cross-list from eess.IV) [<a href="/pdf/2310.10414" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Style transfer between Microscopy and Magnetic Resonance Imaging via  Generative Adversarial Network in small sample size settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pytlarz%2C+M">Monika Pytlarz</a>, 
<a href="/search/eess?searchtype=author&query=Onicas%2C+A">Adrian Onicas</a>, 
<a href="/search/eess?searchtype=author&query=Crimi%2C+A">Alessandro Crimi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE International Conference on Image Processing (ICIP)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> M. Pytlarz, A. Onicas and A. Crimi, "Style Transfer Between
  Microscopy and Magnetic Resonance Imaging Via Generative Adversarial Network
  in Small Sample Size Settings," 2023 IEEE ICIP, 2023, pp. 1120-1124
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Cross-modal augmentation of Magnetic Resonance Imaging (MRI) and microscopic
imaging based on the same tissue samples is promising because it can allow
histopathological analysis in the absence of an underlying invasive biopsy
procedure. Here, we tested a method for generating microscopic histological
images from MRI scans of the corpus callosum using conditional generative
adversarial network (cGAN) architecture. To our knowledge, this is the first
multimodal translation of the brain MRI to histological volumetric
representation of the same sample. The technique was assessed by training
paired image translation models taking sets of images from MRI scans and
microscopy. The use of cGAN for this purpose is challenging because microscopy
images are large in size and typically have low sample availability. The
current work demonstrates that the framework reliably synthesizes histology
images from MRI scans of corpus callosum, emphasizing the network's ability to
train on high resolution histologies paired with relatively lower-resolution
MRI scans. With the ultimate goal of avoiding biopsies, the proposed tool can
be used for educational purposes.
</p>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10420" title="Abstract">arXiv:2310.10420</a> (cross-list from eess.IV) [<a href="/pdf/2310.10420" title="Download PDF">pdf</a>, <a href="/format/2310.10420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LMT: Longitudinal Mixing Training, a Framework to Predict Disease  Progression from a Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zeghlache%2C+R">Rachid Zeghlache</a>, 
<a href="/search/eess?searchtype=author&query=Conze%2C+P">Pierre-Henri Conze</a>, 
<a href="/search/eess?searchtype=author&query=Daho%2C+M+E+H">Mostafa El Habib Daho</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yihao Li</a>, 
<a href="/search/eess?searchtype=author&query=boite%2C+H+L">Hugo Le boite</a>, 
<a href="/search/eess?searchtype=author&query=Tadayoni%2C+R">Ramin Tadayoni</a>, 
<a href="/search/eess?searchtype=author&query=Massin%2C+P">Pascal Massin</a>, 
<a href="/search/eess?searchtype=author&query=Cochener%2C+B">B&#xe9;atrice Cochener</a>, 
<a href="/search/eess?searchtype=author&query=Brahim%2C+I">Ikram Brahim</a>, 
<a href="/search/eess?searchtype=author&query=Quellec%2C+G">Gwenol&#xe9; Quellec</a>, 
<a href="/search/eess?searchtype=author&query=Lamard%2C+M">Mathieu Lamard</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Machine Learning in Medical Imaging. MLMI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Longitudinal imaging is able to capture both static anatomical structures and
dynamic changes in disease progression toward earlier and better
patient-specific pathology management. However, conventional approaches rarely
take advantage of longitudinal information for detection and prediction
purposes, especially for Diabetic Retinopathy (DR). In the past years, Mix-up
training and pretext tasks with longitudinal context have effectively enhanced
DR classification results and captured disease progression. In the meantime, a
novel type of neural network named Neural Ordinary Differential Equation (NODE)
has been proposed for solving ordinary differential equations, with a neural
network treated as a black box. By definition, NODE is well suited for solving
time-related problems. In this paper, we propose to combine these three aspects
to detect and predict DR progression. Our framework, Longitudinal Mixing
Training (LMT), can be considered both as a regularizer and as a pretext task
that encodes the disease progression in the latent space. Additionally, we
evaluate the trained model weights on a downstream task with a longitudinal
context using standard and longitudinal pretext tasks. We introduce a new way
to train time-aware models using $t_{mix}$, a weighted average time between two
consecutive examinations. We compare our approach to standard mixing training
on DR classification using OPHDIAT a longitudinal retinal Color Fundus
Photographs (CFP) dataset. We were able to predict whether an eye would develop
a severe DR in the following visit using a single image, with an AUC of 0.798
compared to baseline results of 0.641. Our results indicate that our
longitudinal pretext task can learn the progression of DR disease and that
introducing $t_{mix}$ augmentation is beneficial for time-aware models.
</p>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10434" title="Abstract">arXiv:2310.10434</a> (cross-list from stat.ML) [<a href="/pdf/2310.10434" title="Download PDF">pdf</a>, <a href="/format/2310.10434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Matrix Function Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Batatia%2C+I">Ilyes Batatia</a>, 
<a href="/search/stat?searchtype=author&query=Schaaf%2C+L+L">Lars L. Schaaf</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+H">Huajie Chen</a>, 
<a href="/search/stat?searchtype=author&query=Cs%C3%A1nyi%2C+G">G&#xe1;bor Cs&#xe1;nyi</a>, 
<a href="/search/stat?searchtype=author&query=Ortner%2C+C">Christoph Ortner</a>, 
<a href="/search/stat?searchtype=author&query=Faber%2C+F+A">Felix A. Faber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Graph Neural Networks (GNNs), especially message-passing neural networks
(MPNNs), have emerged as powerful architectures for learning on graphs in
diverse applications. However, MPNNs face challenges when modeling non-local
interactions in systems such as large conjugated molecules, metals, or
amorphous materials. Although Spectral GNNs and traditional neural networks
such as recurrent neural networks and transformers mitigate these challenges,
they often lack extensivity, adaptability, generalizability, computational
efficiency, or fail to capture detailed structural relationships or symmetries
in the data. To address these concerns, we introduce Matrix Function Neural
Networks (MFNs), a novel architecture that parameterizes non-local interactions
through analytic matrix equivariant functions. Employing resolvent expansions
offers a straightforward implementation and the potential for linear scaling
with system size. The MFN architecture achieves state-of-the-art performance in
standard graph benchmarks, such as the ZINC and TU datasets, and is able to
capture intricate non-local interactions in quantum systems, paving the way to
new state-of-the-art force fields.
</p>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10448" title="Abstract">arXiv:2310.10448</a> (cross-list from stat.ML) [<a href="/pdf/2310.10448" title="Download PDF">pdf</a>, <a href="/ps/2310.10448" title="Download PostScript">ps</a>, <a href="/format/2310.10448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Geometric Insight into Equivariant Message Passing Neural Networks on  Riemannian Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Batatia%2C+I">Ilyes Batatia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work proposes a geometric insight into equivariant message passing on
Riemannian manifolds. As previously proposed, numerical features on Riemannian
manifolds are represented as coordinate-independent feature fields on the
manifold. To any coordinate-independent feature field on a manifold comes
attached an equivariant embedding of the principal bundle to the space of
numerical features. We argue that the metric this embedding induces on the
numerical feature space should optimally preserve the principal bundle's
original metric. This optimality criterion leads to the minimization of a
twisted form of the Polyakov action with respect to the graph of this
embedding, yielding an equivariant diffusion process on the associated vector
bundle. We obtain a message passing scheme on the manifold by discretizing the
diffusion equation flow for a fixed time step. We propose a higher-order
equivariant diffusion process equivalent to diffusion on the cartesian product
of the base manifold. The discretization of the higher-order diffusion process
on a graph yields a new general class of equivariant GNN, generalizing the ACE
and MACE formalism to data on Riemannian manifolds.
</p>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10451" title="Abstract">arXiv:2310.10451</a> (cross-list from quant-ph) [<a href="/pdf/2310.10451" title="Download PDF">pdf</a>, <a href="/format/2310.10451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed search on graphs using discrete time quantum walk
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Roget%2C+M">Mathieu Roget</a>, 
<a href="/search/quant-ph?searchtype=author&query=Di+Molfetta%2C+G">Giuseppe Di Molfetta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Searching with coined quantum walk is a problem that has interested the
community since a long time. While most results consider spatial searches on
regular lattices, some work have introduced several models of coined quantum
walks on graphs. This work introduces a distributed searching quantum walk on
graphs. Our contribution is in two parts: (i) we introduce a new mathematical
model of a coined quantum walk on graphs designed to search both nodes or
edges; (ii) we provide an anonymous distributed scheme to implement such a
model.
</p>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10500" title="Abstract">arXiv:2310.10500</a> (cross-list from q-fin.TR) [<a href="/pdf/2310.10500" title="Download PDF">pdf</a>, <a href="/format/2310.10500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Learning Patterns in Financial Time-Series for Trend-Following  Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Wood%2C+K">Kieran Wood</a>, 
<a href="/search/q-fin?searchtype=author&query=Kessler%2C+S">Samuel Kessler</a>, 
<a href="/search/q-fin?searchtype=author&query=Roberts%2C+S+J">Stephen J. Roberts</a>, 
<a href="/search/q-fin?searchtype=author&query=Zohren%2C+S">Stefan Zohren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Trading and Market Microstructure (q-fin.TR)</span>; Machine Learning (cs.LG); Portfolio Management (q-fin.PM)

</div>
<p class="mathjax">Forecasting models for systematic trading strategies do not adapt quickly
when financial market conditions change, as was seen in the advent of the
COVID-19 pandemic in 2020, when market conditions changed dramatically causing
many forecasting models to take loss-making positions. To deal with such
situations, we propose a novel time-series trend-following forecaster that is
able to quickly adapt to new market conditions, referred to as regimes. We
leverage recent developments from the deep learning community and use few-shot
learning. We propose the Cross Attentive Time-Series Trend Network - X-Trend -
which takes positions attending over a context set of financial time-series
regimes. X-Trend transfers trends from similar patterns in the context set to
make predictions and take positions for a new distinct target regime. X-Trend
is able to quickly adapt to new financial regimes with a Sharpe ratio increase
of 18.9% over a neural forecaster and 10-fold over a conventional Time-series
Momentum strategy during the turbulent market period from 2018 to 2023. Our
strategy recovers twice as quickly from the COVID-19 drawdown compared to the
neural-forecaster. X-Trend can also take zero-shot positions on novel unseen
financial assets obtaining a 5-fold Sharpe ratio increase versus a neural
time-series trend forecaster over the same period. X-Trend both forecasts
next-day prices and outputs a trading signal. Furthermore, the cross-attention
mechanism allows us to interpret the relationship between forecasts and
patterns in the context set.
</p>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10521" title="Abstract">arXiv:2310.10521</a> (cross-list from astro-ph.EP) [<a href="/pdf/2310.10521" title="Download PDF">pdf</a>, <a href="/format/2310.10521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reproducing Bayesian Posterior Distributions for Exoplanet Atmospheric  Parameter Retrievals with a Machine Learning Surrogate Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Unlu%2C+E+B">Eyup B. Unlu</a>, 
<a href="/search/astro-ph?searchtype=author&query=Forestano%2C+R+T">Roy T. Forestano</a>, 
<a href="/search/astro-ph?searchtype=author&query=Matchev%2C+K+T">Konstantin T. Matchev</a>, 
<a href="/search/astro-ph?searchtype=author&query=Matcheva%2C+K">Katia Matcheva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures, 2 tables, Proceedings in the European Conference, ECML PKDD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">We describe a machine-learning-based surrogate model for reproducing the
Bayesian posterior distributions for exoplanet atmospheric parameters derived
from transmission spectra of transiting planets with typical retrieval software
such as TauRex. The model is trained on ground truth distributions for seven
parameters: the planet radius, the atmospheric temperature, and the mixing
ratios for five common absorbers: $H_2O$, $CH_4$, $NH_3$, $CO$ and $CO_2$. The
model performance is enhanced by domain-inspired preprocessing of the features
and the use of semi-supervised learning in order to leverage the large amount
of unlabelled training data available. The model was among the winning
solutions in the 2023 Ariel Machine Learning Data Challenge.
</p>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10523" title="Abstract">arXiv:2310.10523</a> (cross-list from math.ST) [<a href="/pdf/2310.10523" title="Download PDF">pdf</a>, <a href="/ps/2310.10523" title="Download PostScript">ps</a>, <a href="/format/2310.10523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Spectral Theorem to Statistical Independence with Application to  System Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Naeem%2C+M+A">Muhammad Abdullah Naeem</a>, 
<a href="/search/math?searchtype=author&query=Khazraei%2C+A">Amir Khazraei</a>, 
<a href="/search/math?searchtype=author&query=Pajic%2C+M">Miroslav Pajic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY); Probability (math.PR)

</div>
<p class="mathjax">High dimensional random dynamical systems are ubiquitous, including -- but
not limited to -- cyber-physical systems, daily return on different stocks of
S&amp;P 1500 and velocity profile of interacting particle systems around
McKeanVlasov limit. Mathematically, underlying phenomenon can be captured via a
stable $n$-dimensional linear transformation `$A$' and additive randomness.
System identification aims at extracting useful information about underlying
dynamical system, given a length $N$ trajectory from it (corresponds to an $n
\times N$ dimensional data matrix). We use spectral theorem for non-Hermitian
operators to show that spatio-temperal correlations are dictated by the
discrepancy between algebraic and geometric multiplicity of distinct
eigenvalues corresponding to state transition matrix. Small discrepancies imply
that original trajectory essentially comprises of multiple lower dimensional
random dynamical systems living on $A$ invariant subspaces and are
statistically independent of each other. In the process, we provide first
quantitative handle on decay rate of finite powers of state transition matrix
$\|A^{k}\|$ . It is shown that when a stable dynamical system has only one
distinct eigenvalue and discrepancy of $n-1$: $\|A\|$ has a dependence on $n$,
resulting dynamics are spatially inseparable and consequently there exist at
least one row with covariates of typical size $\Theta\big(\sqrt{N-n+1}$
$e^{n}\big)$ i.e., even under stability assumption, covariates can suffer from
curse of dimensionality. In the light of these findings we set the stage for
non-asymptotic error analysis in estimation of state transition matrix $A$ via
least squares regression on observed trajectory by showing that element-wise
error is essentially a variant of well-know Littlewood-Offord problem.
</p>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10544" title="Abstract">arXiv:2310.10544</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.10544" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Use of probabilistic phrases in a coordination game: human versus GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Maloney%2C+L+T">Laurence T Maloney</a>, 
<a href="/search/q-bio?searchtype=author&query=Martello%2C+M+F+D">Maria F Dal Martello</a>, 
<a href="/search/q-bio?searchtype=author&query=Fei%2C+V">Vivian Fei</a>, 
<a href="/search/q-bio?searchtype=author&query=Ma%2C+V">Valerie Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">English speakers use probabilistic phrases such as likely to communicate
information about the probability or likelihood of events. Communication is
successful to the extent that the listener grasps what the speaker means to
convey and, if communication is successful, two individuals can potentially
coordinate their actions based on shared knowledge about uncertainty. We first
assessed human ability to estimate the probability and the ambiguity
(imprecision) of 23 probabilistic phrases in two different contexts, investment
advice and medical advice. We then had GPT4 (OpenAI), a recent Large Language
Model, complete the same tasks as the human participants. We found that the
median human participant and GPT4 assigned probability estimates that were in
good agreement (proportions of variance accounted were close to .90). GPT4's
estimates of probability both in the investment and Medical contexts were as
close or closer to that of the human participants as the human participants
were to one another. Estimates of probability for both the human participants
and GPT4 were little affected by context. In contrast, human and GPT4 estimates
of ambiguity were not in as good agreement. We repeated some of the GPT4
estimates to assess their stability: does GPT4, if run twice, produce the same
or similar estimates? There is some indication that it does not.
</p>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10545" title="Abstract">arXiv:2310.10545</a> (cross-list from stat.ML) [<a href="/pdf/2310.10545" title="Download PDF">pdf</a>, <a href="/format/2310.10545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal vintage factor analysis with deflation varimax
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bing%2C+X">Xin Bing</a>, 
<a href="/search/stat?searchtype=author&query=Jin%2C+D">Dian Jin</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+Y">Yuqian Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Vintage factor analysis is one important type of factor analysis that aims to
first find a low-dimensional representation of the original data, and then to
seek a rotation such that the rotated low-dimensional representation is
scientifically meaningful. Perhaps the most widely used vintage factor analysis
is the Principal Component Analysis (PCA) followed by the varimax rotation.
Despite its popularity, little theoretical guarantee can be provided mainly
because varimax rotation requires to solve a non-convex optimization over the
set of orthogonal matrices.
<br />In this paper, we propose a deflation varimax procedure that solves each row
of an orthogonal matrix sequentially. In addition to its net computational gain
and flexibility, we are able to fully establish theoretical guarantees for the
proposed procedure in a broad context.
<br />Adopting this new varimax approach as the second step after PCA, we further
analyze this two step procedure under a general class of factor models. Our
results show that it estimates the factor loading matrix in the optimal rate
when the signal-to-noise-ratio (SNR) is moderate or large. In the low SNR
regime, we offer possible improvement over using PCA and the deflation
procedure when the additive noise under the factor model is structured. The
modified procedure is shown to be optimal in all SNR regimes. Our theory is
valid for finite sample and allows the number of the latent factors to grow
with the sample size as well as the ambient dimension to grow with, or even
exceed, the sample size.
<br />Extensive simulation and real data analysis further corroborate our
theoretical findings.
</p>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10559" title="Abstract">arXiv:2310.10559</a> (cross-list from stat.ML) [<a href="/pdf/2310.10559" title="Download PDF">pdf</a>, <a href="/format/2310.10559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Dynamic Variational Autoencoder for Counterfactual Regression in  Longitudinal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bouchattaoui%2C+M+E">Mouad El Bouchattaoui</a>, 
<a href="/search/stat?searchtype=author&query=Tami%2C+M">Myriam Tami</a>, 
<a href="/search/stat?searchtype=author&query=Lepetit%2C+B">Benoit Lepetit</a>, 
<a href="/search/stat?searchtype=author&query=Courn%C3%A8de%2C+P">Paul-Henry Courn&#xe8;de</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Estimating treatment effects over time is relevant in many real-world
applications, such as precision medicine, epidemiology, economy, and marketing.
Many state-of-the-art methods either assume the observations of all confounders
or seek to infer the unobserved ones. We take a different perspective by
assuming unobserved risk factors, i.e., adjustment variables that affect only
the sequence of outcomes. Under unconfoundedness, we target the Individual
Treatment Effect (ITE) estimation with unobserved heterogeneity in the
treatment response due to missing risk factors. We address the challenges posed
by time-varying effects and unobserved adjustment variables. Led by theoretical
results over the validity of the learned adjustment variables and
generalization bounds over the treatment effect, we devise Causal DVAE (CDVAE).
This model combines a Dynamic Variational Autoencoder (DVAE) framework with a
weighting strategy using propensity scores to estimate counterfactual
responses. The CDVAE model allows for accurate estimation of ITE and captures
the underlying heterogeneity in longitudinal data. Evaluations of our model
show superior performance over state-of-the-art models.
</p>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10598" title="Abstract">arXiv:2310.10598</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.10598" title="Download PDF">pdf</a>, <a href="/format/2310.10598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pareto Optimization to Accelerate Multi-Objective Virtual Screening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Fromer%2C+J+C">Jenna C. Fromer</a>, 
<a href="/search/q-bio?searchtype=author&query=Graff%2C+D+E">David E. Graff</a>, 
<a href="/search/q-bio?searchtype=author&query=Coley%2C+C+W">Connor W. Coley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The discovery of therapeutic molecules is fundamentally a multi-objective
optimization problem. One formulation of the problem is to identify molecules
that simultaneously exhibit strong binding affinity for a target protein,
minimal off-target interactions, and suitable pharmacokinetic properties.
Inspired by prior work that uses active learning to accelerate the
identification of strong binders, we implement multi-objective Bayesian
optimization to reduce the computational cost of multi-property virtual
screening and apply it to the identification of ligands predicted to be
selective based on docking scores to on- and off-targets. We demonstrate the
superiority of Pareto optimization over scalarization across three case
studies. Further, we use the developed optimization tool to search a virtual
library of over 4M molecules for those predicted to be selective dual
inhibitors of EGFR and IGF1R, acquiring 100% of the molecules that form the
library's Pareto front after exploring only 8% of the library. This workflow
and associated open source software can reduce the screening burden of
molecular design projects and is complementary to research aiming to improve
the accuracy of binding predictions and other molecular properties.
</p>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10602" title="Abstract">arXiv:2310.10602</a> (cross-list from physics.geo-ph) [<a href="/pdf/2310.10602" title="Download PDF">pdf</a>, <a href="/format/2310.10602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed neural wavefields with Gabor basis functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Alkhalifah%2C+T">Tariq Alkhalifah</a>, 
<a href="/search/physics?searchtype=author&query=Huang%2C+X">Xinquan Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Artificial Intelligence (cs.AI); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Recently, Physics-Informed Neural Networks (PINNs) have gained significant
attention for their versatile interpolation capabilities in solving partial
differential equations (PDEs). Despite their potential, the training can be
computationally demanding, especially for intricate functions like wavefields.
This is primarily due to the neural-based (learned) basis functions, biased
toward low frequencies, as they are dominated by polynomial calculations, which
are not inherently wavefield-friendly. In response, we propose an approach to
enhance the efficiency and accuracy of neural network wavefield solutions by
modeling them as linear combinations of Gabor basis functions that satisfy the
wave equation. Specifically, for the Helmholtz equation, we augment the fully
connected neural network model with an adaptable Gabor layer constituting the
final hidden layer, employing a weighted summation of these Gabor neurons to
compute the predictions (output). These weights/coefficients of the Gabor
functions are learned from the previous hidden layers that include nonlinear
activation functions. To ensure the Gabor layer's utilization across the model
space, we incorporate a smaller auxiliary network to forecast the center of
each Gabor function based on input coordinates. Realistic assessments showcase
the efficacy of this novel implementation compared to the vanilla PINN,
particularly in scenarios involving high-frequencies and realistic models that
are often challenging for PINNs.
</p>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10604" title="Abstract">arXiv:2310.10604</a> (cross-list from eess.AS) [<a href="/pdf/2310.10604" title="Download PDF">pdf</a>, <a href="/format/2310.10604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generation or Replication: Auscultating Audio Latent Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bralios%2C+D">Dimitrios Bralios</a>, 
<a href="/search/eess?searchtype=author&query=Wichern%2C+G">Gordon Wichern</a>, 
<a href="/search/eess?searchtype=author&query=Germain%2C+F+G">Fran&#xe7;ois G. Germain</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+Z">Zexu Pan</a>, 
<a href="/search/eess?searchtype=author&query=Khurana%2C+S">Sameer Khurana</a>, 
<a href="/search/eess?searchtype=author&query=Hori%2C+C">Chiori Hori</a>, 
<a href="/search/eess?searchtype=author&query=Roux%2C+J+L">Jonathan Le Roux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">The introduction of audio latent diffusion models possessing the ability to
generate realistic sound clips on demand from a text description has the
potential to revolutionize how we work with audio. In this work, we make an
initial attempt at understanding the inner workings of audio latent diffusion
models by investigating how their audio outputs compare with the training data,
similar to how a doctor auscultates a patient by listening to the sounds of
their organs. Using text-to-audio latent diffusion models trained on the
AudioCaps dataset, we systematically analyze memorization behavior as a
function of training set size. We also evaluate different retrieval metrics for
evidence of training data memorization, finding the similarity between mel
spectrograms to be more robust in detecting matches than learned embedding
vectors. In the process of analyzing memorization in audio latent diffusion
models, we also discover a large amount of duplicated audio clips within the
AudioCaps database.
</p>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10605" title="Abstract">arXiv:2310.10605</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2310.10605" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ForceGen: End-to-end de novo protein generation based on nonlinear  mechanical unfolding responses using a protein language diffusion model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Ni%2C+B">Bo Ni</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kaplan%2C+D+L">David L. Kaplan</a>, 
<a href="/search/cond-mat?searchtype=author&query=Buehler%2C+M+J">Markus J. Buehler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Computation and Language (cs.CL); Machine Learning (cs.LG); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Through evolution, nature has presented a set of remarkable protein
materials, including elastins, silks, keratins and collagens with superior
mechanical performances that play crucial roles in mechanobiology. However,
going beyond natural designs to discover proteins that meet specified
mechanical properties remains challenging. Here we report a generative model
that predicts protein designs to meet complex nonlinear mechanical
property-design objectives. Our model leverages deep knowledge on protein
sequences from a pre-trained protein language model and maps mechanical
unfolding responses to create novel proteins. Via full-atom molecular
simulations for direct validation, we demonstrate that the designed proteins
are novel, and fulfill the targeted mechanical properties, including unfolding
energy and mechanical strength, as well as the detailed unfolding
force-separation curves. Our model offers rapid pathways to explore the
enormous mechanobiological protein sequence space unconstrained by biological
synthesis, using mechanical features as target to enable the discovery of
protein materials with superior mechanical properties.
</p>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10619" title="Abstract">arXiv:2310.10619</a> (cross-list from math.OC) [<a href="/pdf/2310.10619" title="Download PDF">pdf</a>, <a href="/format/2310.10619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shortest-path recovery from signature with an optimal control approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rauscher%2C+M">Marco Rauscher</a>, 
<a href="/search/math?searchtype=author&query=Scagliotti%2C+A">Alessandro Scagliotti</a>, 
<a href="/search/math?searchtype=author&query=Pagginelli%2C+F">Felipe Pagginelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper, we consider the signature-to-path reconstruction problem from
the control theoretic perspective. Namely, we design an optimal control problem
whose solution leads to the minimal-length path that generates a given
signature. In order to do that, we minimize a cost functional consisting of two
competing terms, i.e., a weighted final-time cost combined with the $L^2$-norm
squared of the controls. Moreover, we can show that, by taking the limit to
infinity of the parameter that tunes the final-time cost, the problem
$\Gamma$-converges to the problem of finding a sub-Riemannian geodesic
connecting two signatures. Finally, we provide an alternative reformulation of
the latter problem, which is particularly suitable for the numerical
implementation.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Tue, 17 Oct 23</h3>
<dl>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1811.02733" title="Abstract">arXiv:1811.02733</a> (replaced) [<a href="/pdf/1811.02733" title="Download PDF">pdf</a>, <a href="/ps/1811.02733" title="Download PostScript">ps</a>, <a href="/format/1811.02733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized prolate spheroidal functions: algorithms and analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Greengard%2C+P">Philip Greengard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.05606" title="Abstract">arXiv:2002.05606</a> (replaced) [<a href="/pdf/2002.05606" title="Download PDF">pdf</a>, <a href="/ps/2002.05606" title="Download PostScript">ps</a>, <a href="/format/2002.05606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentiment Analysis Using Averaged Weighted Word Vector Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erkan%2C+A">Ali Erkan</a>, 
<a href="/search/cs?searchtype=author&query=Gungor%2C+T">Tunga Gungor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.12530" title="Abstract">arXiv:2002.12530</a> (replaced) [<a href="/pdf/2002.12530" title="Download PDF">pdf</a>, <a href="/format/2002.12530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Convolutional Attention-based Network For Sequence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+H">Hongyan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Siqiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yudi Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+F">Furao Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.02745" title="Abstract">arXiv:2006.02745</a> (replaced) [<a href="/pdf/2006.02745" title="Download PDF">pdf</a>, <a href="/format/2006.02745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotic Analysis of Conditioned Stochastic Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Leluc%2C+R">R&#xe9;mi Leluc</a>, 
<a href="/search/math?searchtype=author&query=Portier%2C+F">Fran&#xe7;ois Portier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Transactions on Machine Learning Research 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.06762" title="Abstract">arXiv:2006.06762</a> (replaced) [<a href="/pdf/2006.06762" title="Download PDF">pdf</a>, <a href="/format/2006.06762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ansor: Generating High-Performance Tensor Programs for Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lianmin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+C">Chengfan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Minmin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C+H">Cody Hao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Haj-Ali%2C+A">Ameer Haj-Ali</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yida Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+D">Danyang Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+K">Koushik Sen</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+E">Joseph E. Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Stoica%2C+I">Ion Stoica</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> OSDI 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Performance (cs.PF); Programming Languages (cs.PL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.05014" title="Abstract">arXiv:2007.05014</a> (replaced) [<a href="/pdf/2007.05014" title="Download PDF">pdf</a>, <a href="/ps/2007.05014" title="Download PostScript">ps</a>, <a href="/format/2007.05014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack  Constraint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amanatidis%2C+G">Georgios Amanatidis</a>, 
<a href="/search/cs?searchtype=author&query=Fusco%2C+F">Federico Fusco</a>, 
<a href="/search/cs?searchtype=author&query=Lazos%2C+P">Philip Lazos</a>, 
<a href="/search/cs?searchtype=author&query=Leonardi%2C+S">Stefano Leonardi</a>, 
<a href="/search/cs?searchtype=author&query=Reiffenh%C3%A4user%2C+R">Rebecca Reiffenh&#xe4;user</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version addresses a gap in the probabilistic analysis of the approximation guarantees in the previous version of this work. We provide a simple fix via a standard sampling routine while maintaining the same approximation guarantees and complexity bounds
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.10274" title="Abstract">arXiv:2010.10274</a> (replaced) [<a href="/pdf/2010.10274" title="Download PDF">pdf</a>, <a href="/format/2010.10274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Fairing Convolutional Networks for Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mesgaran%2C+M">Mahsa Mesgaran</a>, 
<a href="/search/cs?searchtype=author&query=Hamza%2C+A+B">A. Ben Hamza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.12611" title="Abstract">arXiv:2010.12611</a> (replaced) [<a href="/pdf/2010.12611" title="Download PDF">pdf</a>, <a href="/format/2010.12611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information access representations and social capital in networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bashardoust%2C+A">Ashkan Bashardoust</a>, 
<a href="/search/cs?searchtype=author&query=Beilinson%2C+H+C">Hannah C. Beilinson</a>, 
<a href="/search/cs?searchtype=author&query=Friedler%2C+S+A">Sorelle A. Friedler</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiajie Ma</a>, 
<a href="/search/cs?searchtype=author&query=Rousseau%2C+J">Jade Rousseau</a>, 
<a href="/search/cs?searchtype=author&query=Scheidegger%2C+C+E">Carlos E. Scheidegger</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+B+D">Blair D. Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Ulzii-Orshikh%2C+N">Nasanbayar Ulzii-Orshikh</a>, 
<a href="/search/cs?searchtype=author&query=Venkatasubramanian%2C+S">Suresh Venkatasubramanian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.00153" title="Abstract">arXiv:2101.00153</a> (replaced) [<a href="/pdf/2101.00153" title="Download PDF">pdf</a>, <a href="/format/2101.00153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graphmax for Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bin%2C+L">Liu Bin</a>, 
<a href="/search/cs?searchtype=author&query=Guosheng%2C+Y">Yin Guosheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.00674" title="Abstract">arXiv:2103.00674</a> (replaced) [<a href="/pdf/2103.00674" title="Download PDF">pdf</a>, <a href="/format/2103.00674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BEAUTY Powered BEAST
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Zhao%2C+Z">Zhigen Zhao</a>, 
<a href="/search/stat?searchtype=author&query=Zhou%2C+W">Wen Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Applications (stat.AP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.12285" title="Abstract">arXiv:2104.12285</a> (replaced) [<a href="/pdf/2104.12285" title="Download PDF">pdf</a>, <a href="/format/2104.12285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Move Schedules: Fast persistence computations in coarse dynamic settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piekenbrock%2C+M">Matthew Piekenbrock</a>, 
<a href="/search/cs?searchtype=author&query=Perea%2C+J+A">Jose A. Perea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Algebraic Topology (math.AT)

</div>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.11271" title="Abstract">arXiv:2105.11271</a> (replaced) [<a href="/pdf/2105.11271" title="Download PDF">pdf</a>, <a href="/ps/2105.11271" title="Download PostScript">ps</a>, <a href="/format/2105.11271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructions of Binary Optimal Locally Repairable Codes via  Intersection Subspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Deng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+C">Chenhao Ying</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the SCIENCE CHINA Information Sciences
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.00042" title="Abstract">arXiv:2106.00042</a> (replaced) [<a href="/pdf/2106.00042" title="Download PDF">pdf</a>, <a href="/format/2106.00042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A study on the plasticity of neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berariu%2C+T">Tudor Berariu</a>, 
<a href="/search/cs?searchtype=author&query=Czarnecki%2C+W">Wojciech Czarnecki</a>, 
<a href="/search/cs?searchtype=author&query=De%2C+S">Soham De</a>, 
<a href="/search/cs?searchtype=author&query=Bornschein%2C+J">Jorg Bornschein</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+S">Samuel Smith</a>, 
<a href="/search/cs?searchtype=author&query=Pascanu%2C+R">Razvan Pascanu</a>, 
<a href="/search/cs?searchtype=author&query=Clopath%2C+C">Claudia Clopath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02331" title="Abstract">arXiv:2106.02331</a> (replaced) [<a href="/pdf/2106.02331" title="Download PDF">pdf</a>, <a href="/format/2106.02331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Manifold-Aware Deep Clustering: Maximizing Angles between Embedding  Vectors Based on Regular Simplex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tanaka%2C+K">Keitaro Tanaka</a>, 
<a href="/search/eess?searchtype=author&query=Sawata%2C+R">Ryosuke Sawata</a>, 
<a href="/search/eess?searchtype=author&query=Takahashi%2C+S">Shusuke Takahashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.07002" title="Abstract">arXiv:2108.07002</a> (replaced) [<a href="/pdf/2108.07002" title="Download PDF">pdf</a>, <a href="/format/2108.07002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Change is Everywhere: Single-Temporal Supervised Object Change Detection  in Remote Sensing Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhuo Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+A">Ailong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liangpei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yanfei Zhong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.12381" title="Abstract">arXiv:2109.12381</a> (replaced) [<a href="/pdf/2109.12381" title="Download PDF">pdf</a>, <a href="/format/2109.12381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connected Coordinated Motion Planning with Bounded Stretch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fekete%2C+S+P">S&#xe1;ndor P. Fekete</a>, 
<a href="/search/cs?searchtype=author&query=Keldenich%2C+P">Phillip Keldenich</a>, 
<a href="/search/cs?searchtype=author&query=Kosfeld%2C+R">Ramin Kosfeld</a>, 
<a href="/search/cs?searchtype=author&query=Rieck%2C+C">Christian Rieck</a>, 
<a href="/search/cs?searchtype=author&query=Scheffer%2C+C">Christian Scheffer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 18 figures, full version of an extended abstract that appeared in the proceedings of the 32nd International Symposium on Algorithms and Computation (ISAAC 2021); revised version (more details added, and typing errors corrected)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.03135" title="Abstract">arXiv:2110.03135</a> (replaced) [<a href="/pdf/2110.03135" title="Download PDF">pdf</a>, <a href="/format/2110.03135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Noise in Adversarial Training: A Novel Perspective to Study Robust  Overfitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chengyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips 2022 (Oral); A previous version of this paper (v1) used the title `Double Descent in Adversarial Training: An Implicit Label Noise Perspective`
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.05216" title="Abstract">arXiv:2110.05216</a> (replaced) [<a href="/pdf/2110.05216" title="Download PDF">pdf</a>, <a href="/format/2110.05216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-order Tensor Pooling with Attention for Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Koniusz%2C+P">Piotr Koniusz</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Ke Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.04456" title="Abstract">arXiv:2111.04456</a> (replaced) [<a href="/pdf/2111.04456" title="Download PDF">pdf</a>, <a href="/format/2111.04456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IAC: A Framework for Enabling Patient Agency in the Use of AI-Enabled  Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okolo%2C+C+T">Chinasa T. Okolo</a>, 
<a href="/search/cs?searchtype=author&query=Amador%2C+M+G">Michelle Gonz&#xe1;lez Amador</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We have vastly re-worked the paper. This new submission better reflects our work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.07976" title="Abstract">arXiv:2111.07976</a> (replaced) [<a href="/pdf/2111.07976" title="Download PDF">pdf</a>, <a href="/ps/2111.07976" title="Download PostScript">ps</a>, <a href="/format/2111.07976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Convergence of Hessenberg Shifted QR I: Exact Arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Banks%2C+J">Jess Banks</a>, 
<a href="/search/math?searchtype=author&query=Garza-Vargas%2C+J">Jorge Garza-Vargas</a>, 
<a href="/search/math?searchtype=author&query=Srivastava%2C+N">Nikhil Srivastava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31pp. Comments welcome. v3: slight changes in notation, exposition. v4: to motivate the proof of the general result, a section discussing in detail the case of normal inputs has been added. A discussion on the scope of this research has been added to the first section. Many remarks and footnotes with clarifications have been added throughout the paper. The title has been modified
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Data Structures and Algorithms (cs.DS); Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.12938" title="Abstract">arXiv:2112.12938</a> (replaced) [<a href="/pdf/2112.12938" title="Download PDF">pdf</a>, <a href="/format/2112.12938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Memorization in Neural Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chiyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ippolito%2C+D">Daphne Ippolito</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Katherine Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jagielski%2C+M">Matthew Jagielski</a>, 
<a href="/search/cs?searchtype=author&query=Tram%C3%A8r%2C+F">Florian Tram&#xe8;r</a>, 
<a href="/search/cs?searchtype=author&query=Carlini%2C+N">Nicholas Carlini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023; 42 pages, 33 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.06070" title="Abstract">arXiv:2201.06070</a> (replaced) [<a href="/pdf/2201.06070" title="Download PDF">pdf</a>, <a href="/format/2201.06070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALA: Naturalness-aware Adversarial Lightness Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yihao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liangru Sun</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Juefei-Xu%2C+F">Felix Juefei-Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiayi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jincao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+G">Geguang Pu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.03583" title="Abstract">arXiv:2202.03583</a> (replaced) [<a href="/pdf/2202.03583" title="Download PDF">pdf</a>, <a href="/format/2202.03583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Label Classification of Thoracic Diseases using Dense  Convolutional Network on Chest Radiographs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bhusal%2C+D">Dipkamal Bhusal</a>, 
<a href="/search/eess?searchtype=author&query=Panday%2C+S+P">Sanjeeb Prasad Panday</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.06487" title="Abstract">arXiv:2203.06487</a> (replaced) [<a href="/pdf/2203.06487" title="Download PDF">pdf</a>, <a href="/format/2203.06487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can  Existing Algorithms Fulfill Clinical Requirements?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Weina Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Hamarneh%2C+G">Ghassan Hamarneh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.13424" title="Abstract">arXiv:2203.13424</a> (replaced) [<a href="/pdf/2203.13424" title="Download PDF">pdf</a>, <a href="/format/2203.13424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dealing with Sparse Rewards Using Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gerasyov%2C+M">Matvey Gerasyov</a>, 
<a href="/search/cs?searchtype=author&query=Makarov%2C+I">Ilya Makarov</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, vol. 11, pp. 89180-89187, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.15724" title="Abstract">arXiv:2203.15724</a> (replaced) [<a href="/pdf/2203.15724" title="Download PDF">pdf</a>, <a href="/ps/2203.15724" title="Download PostScript">ps</a>, <a href="/format/2203.15724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On $d$-stable locally checkable problems parameterized by mim-width
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+C+L">Carolina Luc&#xed;a Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Mann%2C+F">Felix Mann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.16487" title="Abstract">arXiv:2203.16487</a> (replaced) [<a href="/pdf/2203.16487" title="Download PDF">pdf</a>, <a href="/format/2203.16487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speculative Decoding: Lossless Speedup of Autoregressive Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Heming Xia</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+T">Tao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Si-Qing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR'23 submission version (publicly announced in Sep 2022: <a href="https://openreview.net/forum?id=H-VlwsYvVi">this https URL</a>) where "Generalized Aggressive Decoding" was formally renamed to "Speculative Decoding" (SpecDec). It is the first time that "Speculative Decoding" has been publicly proposed, explicitly using the idea of speculative execution to accelerate Transformer inference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.17168" title="Abstract">arXiv:2203.17168</a> (replaced) [<a href="/pdf/2203.17168" title="Download PDF">pdf</a>, <a href="/ps/2203.17168" title="Download PostScript">ps</a>, <a href="/format/2203.17168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lower bounds for uniform read-once threshold formulae in the randomized  decision tree model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leonardos%2C+N">Nikos Leonardos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Corrected errors (especially in section 3.3). The bounds did not change (except of a typo that was corrected)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.00525" title="Abstract">arXiv:2204.00525</a> (replaced) [<a href="/pdf/2204.00525" title="Download PDF">pdf</a>, <a href="/format/2204.00525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Givens Rotations for QR Decomposition, SVD and PCA over Database Joins
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olteanu%2C+D">Dan Olteanu</a>, 
<a href="/search/cs?searchtype=author&query=Vortmeier%2C+N">Nils Vortmeier</a>, 
<a href="/search/cs?searchtype=author&query=%C5%BDivanovi%C4%87%2C+%C4%90">&#x110;or&#x111;e &#x17d;ivanovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.02235" title="Abstract">arXiv:2204.02235</a> (replaced) [<a href="/pdf/2204.02235" title="Download PDF">pdf</a>, <a href="/format/2204.02235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> At the Locus of Performance: Quantifying the Effects of Copious  3D-Stacked Cache on HPC Workloads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Domke%2C+J">Jens Domke</a>, 
<a href="/search/cs?searchtype=author&query=Vatai%2C+E">Emil Vatai</a>, 
<a href="/search/cs?searchtype=author&query=Gerofi%2C+B">Balazs Gerofi</a>, 
<a href="/search/cs?searchtype=author&query=Kodama%2C+Y">Yuetsu Kodama</a>, 
<a href="/search/cs?searchtype=author&query=Wahib%2C+M">Mohamed Wahib</a>, 
<a href="/search/cs?searchtype=author&query=Podobas%2C+A">Artur Podobas</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Sparsh Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Peric%C3%A0s%2C+M">Miquel Peric&#xe0;s</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Drozd%2C+A">Aleksandr Drozd</a>, 
<a href="/search/cs?searchtype=author&query=Matsuoka%2C+S">Satoshi Matsuoka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.02294" title="Abstract">arXiv:2204.02294</a> (replaced) [<a href="/pdf/2204.02294" title="Download PDF">pdf</a>, <a href="/format/2204.02294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZETAR: Modeling and Computational Design of Strategic and Adaptive  Compliance Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Linan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Quanyan Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.10920" title="Abstract">arXiv:2204.10920</a> (replaced) [<a href="/pdf/2204.10920" title="Download PDF">pdf</a>, <a href="/format/2204.10920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking, Profiling, and Ad Targeting in the Alexa Echo Smart Speaker  Ecosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iqbal%2C+U">Umar Iqbal</a>, 
<a href="/search/cs?searchtype=author&query=Bahrami%2C+P+N">Pouneh Nikkhah Bahrami</a>, 
<a href="/search/cs?searchtype=author&query=Trimananda%2C+R">Rahmadi Trimananda</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hao Cui</a>, 
<a href="/search/cs?searchtype=author&query=Gamero-Garrido%2C+A">Alexander Gamero-Garrido</a>, 
<a href="/search/cs?searchtype=author&query=Dubois%2C+D">Daniel Dubois</a>, 
<a href="/search/cs?searchtype=author&query=Choffnes%2C+D">David Choffnes</a>, 
<a href="/search/cs?searchtype=author&query=Markopoulou%2C+A">Athina Markopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Roesner%2C+F">Franziska Roesner</a>, 
<a href="/search/cs?searchtype=author&query=Shafiq%2C+Z">Zubair Shafiq</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at the ACM Internet Measurement Conference 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.11762" title="Abstract">arXiv:2204.11762</a> (replaced) [<a href="/pdf/2204.11762" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MFA-DVR: Direct Volume Rendering of MFA Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jianxin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Lenz%2C+D">David Lenz</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongfeng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Peterka%2C+T">Tom Peterka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.14229" title="Abstract">arXiv:2204.14229</a> (replaced) [<a href="/pdf/2204.14229" title="Download PDF">pdf</a>, <a href="/format/2204.14229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Pareto-Optimal and Almost Envy-Free Allocations of Indivisible  Goods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+J">Jugal Garg</a>, 
<a href="/search/cs?searchtype=author&query=Murhekar%2C+A">Aniket Murhekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages. A preliminary version appeared at AAAI 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.10898" title="Abstract">arXiv:2205.10898</a> (replaced) [<a href="/pdf/2205.10898" title="Download PDF">pdf</a>, <a href="/format/2205.10898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A meshfree collocation scheme for surface differential operators on  point clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Singh%2C+A">Abhinav Singh</a>, 
<a href="/search/math?searchtype=author&query=Foggia%2C+A">Alejandra Foggia</a>, 
<a href="/search/math?searchtype=author&query=Incardona%2C+P">Pietro Incardona</a>, 
<a href="/search/math?searchtype=author&query=Sbalzarini%2C+I+F">Ivo F. Sbalzarini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.12422" title="Abstract">arXiv:2205.12422</a> (replaced) [<a href="/pdf/2205.12422" title="Download PDF">pdf</a>, <a href="/format/2205.12422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Labeling Programs with Non-Programmers Indirectly via Active Examples: A  Case Study with Text-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+R">Ruiqi Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Snell%2C+C">Charlie Snell</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>, 
<a href="/search/cs?searchtype=author&query=Eisner%2C+J">Jason Eisner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.14307" title="Abstract">arXiv:2205.14307</a> (replaced) [<a href="/pdf/2205.14307" title="Download PDF">pdf</a>, <a href="/format/2205.14307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning  over Temporal Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xueyuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chengjin Xu</a>, 
<a href="/search/cs?searchtype=author&query=E%2C+H">Haihong E</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+F">Fenglong Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Gengxian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Ningyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingzhi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haoran Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.09649" title="Abstract">arXiv:2206.09649</a> (replaced) [<a href="/pdf/2206.09649" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Machine Learning Data Fusion Model for Soil Moisture Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Batchu%2C+V">Vishal Batchu</a>, 
<a href="/search/physics?searchtype=author&query=Nearing%2C+G">Grey Nearing</a>, 
<a href="/search/physics?searchtype=author&query=Gulshan%2C+V">Varun Gulshan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages, 21 tables, 26 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.10982" title="Abstract">arXiv:2206.10982</a> (replaced) [<a href="/pdf/2206.10982" title="Download PDF">pdf</a>, <a href="/format/2206.10982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnostic Tool for Out-of-Sample Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hult%2C+L">Ludvig Hult</a>, 
<a href="/search/stat?searchtype=author&query=Zachariah%2C+D">Dave Zachariah</a>, 
<a href="/search/stat?searchtype=author&query=Stoica%2C+P">Petre Stoica</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updates mainly for readability. some more experimental details in appendix. some connection to VaR added in discussion
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.07173" title="Abstract">arXiv:2207.07173</a> (replaced) [<a href="/pdf/2207.07173" title="Download PDF">pdf</a>, <a href="/format/2207.07173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Image Clustering with Contrastive Learning and Multi-scale Graph  Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuankun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Dong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chang-Dong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+J">Jian-Huang Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the Pattern Recognition journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.07870" title="Abstract">arXiv:2207.07870</a> (replaced) [<a href="/pdf/2207.07870" title="Download PDF">pdf</a>, <a href="/format/2207.07870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scene Graph for Embodied Exploration in Cluttered Scenario
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuhong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sima%2C+Q">Qie Sima</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Di Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huaping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fuchun Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.09324" title="Abstract">arXiv:2207.09324</a> (replaced) [<a href="/pdf/2207.09324" title="Download PDF">pdf</a>, <a href="/format/2207.09324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signed Network Embedding with Application to Simultaneous Detection of  Communities and Anomalies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junhui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 4 figures. The appendix containing technical proof is not included, and will be uploaded in the future
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.14282" title="Abstract">arXiv:2207.14282</a> (replaced) [<a href="/pdf/2207.14282" title="Download PDF">pdf</a>, <a href="/format/2207.14282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometric relative entropies and barycentric R&#xe9;nyi divergences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Mosonyi%2C+M">Mil&#xe1;n Mosonyi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bunth%2C+G">Gergely Bunth</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vrana%2C+P">P&#xe9;ter Vrana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v4: Extended "Conclusion and Outlook". 68 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); Mathematical Physics (math-ph); Functional Analysis (math.FA)

</div>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.00175" title="Abstract">arXiv:2208.00175</a> (replaced) [<a href="/pdf/2208.00175" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global, Unified Representation of Heterogenous Robot Dynamics Using  Composition Operators: A Koopman Direct Encoding Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asada%2C+H">Harry Asada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE/ASME Transactions on Mechatronics, 19 May 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.01430" title="Abstract">arXiv:2208.01430</a> (replaced) [<a href="/pdf/2208.01430" title="Download PDF">pdf</a>, <a href="/format/2208.01430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Model for Multi-Agent Heterogeneous Interaction Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C+D">Christopher D. Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Haile%2C+M+A">Mulugeta A. Haile</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+P">Pratik Chaudhari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.05365" title="Abstract">arXiv:2208.05365</a> (replaced) [<a href="/pdf/2208.05365" title="Download PDF">pdf</a>, <a href="/format/2208.05365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Saturation-based Boolean conjunctive query answering and rewriting for  the guarded quantification fragments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+R+A">Renate A. Schmidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.07730" title="Abstract">arXiv:2208.07730</a> (replaced) [<a href="/pdf/2208.07730" title="Download PDF">pdf</a>, <a href="/ps/2208.07730" title="Download PostScript">ps</a>, <a href="/format/2208.07730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Sum Theorems From Fortification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hao Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> fix some typos; remove a section of agree problem with a gap in the proof pointed by an anonymous reviewer, the author has tried to fix it but could not fix it for now
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09470" title="Abstract">arXiv:2208.09470</a> (replaced) [<a href="/pdf/2208.09470" title="Download PDF">pdf</a>, <a href="/format/2208.09470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Undersampling Raster Scans in Spectromicroscopy for reduced dose and  faster measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Townsend%2C+O">Oliver Townsend</a>, 
<a href="/search/physics?searchtype=author&query=Gazzola%2C+S">Silvia Gazzola</a>, 
<a href="/search/physics?searchtype=author&query=Dolgov%2C+S">Sergey Dolgov</a>, 
<a href="/search/physics?searchtype=author&query=Quinn%2C+P">Paul Quinn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages,11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Numerical Analysis (math.NA); Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.10051" title="Abstract">arXiv:2208.10051</a> (replaced) [<a href="/pdf/2208.10051" title="Download PDF">pdf</a>, <a href="/ps/2208.10051" title="Download PostScript">ps</a>, <a href="/format/2208.10051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Observer-based Leader-following Consensus for Positive Multi-agent  Systems Over Time-varying Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+R">Ruonan Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yichen Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+Y">Yutao Tang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shurong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12227" title="Abstract">arXiv:2208.12227</a> (replaced) [<a href="/pdf/2208.12227" title="Download PDF">pdf</a>, <a href="/ps/2208.12227" title="Download PostScript">ps</a>, <a href="/format/2208.12227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Community Detection in the Hypergraph SBM: Exact Recovery Given the  Similarity Matrix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaudio%2C+J">Julia Gaudio</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+N">Nirmit Joshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at the Conference on Learning Theory (COLT) 2023. Error in footnote page 3
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12909" title="Abstract">arXiv:2208.12909</a> (replaced) [<a href="/pdf/2208.12909" title="Download PDF">pdf</a>, <a href="/format/2208.12909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pipeline-Invariant Representation Learning for Neuroimaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Fedorov%2C+A">Alex Fedorov</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+M">Mrinal Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Abrol%2C+A">Anees Abrol</a>, 
<a href="/search/cs?searchtype=author&query=Kiar%2C+G">Gregory Kiar</a>, 
<a href="/search/cs?searchtype=author&query=Plis%2C+S">Sergey Plis</a>, 
<a href="/search/cs?searchtype=author&query=Calhoun%2C+V">Vince Calhoun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2022, November 28th, 2022, New Orleans, United States &amp; Virtual, <a href="http://www.ml4h.cc">this http URL</a>, 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02167" title="Abstract">arXiv:2209.02167</a> (replaced) [<a href="/pdf/2209.02167" title="Download PDF">pdf</a>, <a href="/format/2209.02167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Red Teaming with Mind Reading: White-Box Adversarial Policies Against RL  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casper%2C+S">Stephen Casper</a>, 
<a href="/search/cs?searchtype=author&query=Killian%2C+T">Taylor Killian</a>, 
<a href="/search/cs?searchtype=author&query=Kreiman%2C+G">Gabriel Kreiman</a>, 
<a href="/search/cs?searchtype=author&query=Hadfield-Menell%2C+D">Dylan Hadfield-Menell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/thestephencasper/lm_white_box_attacks">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.03358" title="Abstract">arXiv:2209.03358</a> (replaced) [<a href="/pdf/2209.03358" title="Download PDF">pdf</a>, <a href="/format/2209.03358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attacking the Spike: On the Transferability and Security of Spiking  Neural Networks to Adversarial Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+K">Kaleel Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Haowen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Rathbun%2C+E">Ethan Rathbun</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Caiwen Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Wujie Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.03851" title="Abstract">arXiv:2209.03851</a> (replaced) [<a href="/pdf/2209.03851" title="Download PDF">pdf</a>, <a href="/format/2209.03851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 5q032e@SMM4H&#x27;22: Transformer-based classification of premise in tweets  related to COVID-19
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Porvatov%2C+V">Vadim Porvatov</a>, 
<a href="/search/cs?searchtype=author&query=Semenova%2C+N">Natalia Semenova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SMM4H Workshop of COLING'22
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Porvatov, V., Semenova, N., 5q032e@ SMM4H'22: Transformer-based
  classification of premise in tweets related to COVID-19, Mining for Health
  Applications, Workshop &amp; Shared Task (SMM4H 2022) (p. 108)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07881" title="Abstract">arXiv:2209.07881</a> (replaced) [<a href="/pdf/2209.07881" title="Download PDF">pdf</a>, <a href="/format/2209.07881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Predictive Robustness of Signal Temporal Logic Predicates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuanfei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Althoff%2C+M">Matthias Althoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> @2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.11046" title="Abstract">arXiv:2209.11046</a> (replaced) [<a href="/pdf/2209.11046" title="Download PDF">pdf</a>, <a href="/ps/2209.11046" title="Download PostScript">ps</a>, <a href="/format/2209.11046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exotic B-series and S-series: algebraic structures and order conditions  for invariant measure sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bronasco%2C+E">Eugen Bronasco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, to appear in Foundations of Computational Mathematics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.13134" title="Abstract">arXiv:2209.13134</a> (replaced) [<a href="/pdf/2209.13134" title="Download PDF">pdf</a>, <a href="/ps/2209.13134" title="Download PostScript">ps</a>, <a href="/format/2209.13134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An $O(3.82^k)$ Time FPT Algorithm for Convex Flip Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haohong Li</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Ge Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.14344" title="Abstract">arXiv:2209.14344</a> (replaced) [<a href="/pdf/2209.14344" title="Download PDF">pdf</a>, <a href="/format/2209.14344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pareto Actor-Critic for Equilibrium Selection in Multi-Agent  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christianos%2C+F">Filippos Christianos</a>, 
<a href="/search/cs?searchtype=author&query=Papoudakis%2C+G">Georgios Papoudakis</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Transactions on Machine Learning Research (TMLR); Reviewed on OpenReview: <a href="https://openreview.net/forum?id=3AzqYa18ah">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.15240" title="Abstract">arXiv:2209.15240</a> (replaced) [<a href="/pdf/2209.15240" title="Download PDF">pdf</a>, <a href="/format/2209.15240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Prompt Tuning for Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Taoran Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunchao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02648" title="Abstract">arXiv:2210.02648</a> (replaced) [<a href="/pdf/2210.02648" title="Download PDF">pdf</a>, <a href="/format/2210.02648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-triggered Consensus of Multi-agent Systems with Quantized Relative  State Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wakaiki%2C+M">Masashi Wakaiki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 3 figures. To appear in IET Control Theory &amp; Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03029" title="Abstract">arXiv:2210.03029</a> (replaced) [<a href="/pdf/2210.03029" title="Download PDF">pdf</a>, <a href="/format/2210.03029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficiently Enhancing Zero-Shot Performance of Instruction Following  Model via Retrieval of Soft Prompt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Seonghyeon Ye</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Joel Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Doyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+Y">Yongrae Jo</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03918" title="Abstract">arXiv:2210.03918</a> (replaced) [<a href="/pdf/2210.03918" title="Download PDF">pdf</a>, <a href="/format/2210.03918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding and Exploring Promising Search Space for the 0-1  Multidimensional Knapsack Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jitao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongbo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+M">Minghao Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.04193" title="Abstract">arXiv:2210.04193</a> (replaced) [<a href="/pdf/2210.04193" title="Download PDF">pdf</a>, <a href="/format/2210.04193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting fluid-structure interaction with graph neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Gao%2C+R">Rui Gao</a>, 
<a href="/search/physics?searchtype=author&query=Jaiman%2C+R+K">Rajeev K. Jaiman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05015" title="Abstract">arXiv:2210.05015</a> (replaced) [<a href="/pdf/2210.05015" title="Download PDF">pdf</a>, <a href="/format/2210.05015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimality Guarantees for Particle Belief Approximation of POMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+M+H">Michael H. Lim</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+T+J">Tyler J. Becker</a>, 
<a href="/search/cs?searchtype=author&query=Kochenderfer%2C+M+J">Mykel J. Kochenderfer</a>, 
<a href="/search/cs?searchtype=author&query=Tomlin%2C+C+J">Claire J. Tomlin</a>, 
<a href="/search/cs?searchtype=author&query=Sunberg%2C+Z+N">Zachary N. Sunberg</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Lim, M. H., Becker, T. J., Kochenderfer, M. J., Tomlin, C. J., &amp;
  Sunberg, Z. N. (2023). Optimality guarantees for particle belief
  approximation of POMDPs. Journal of Artificial Intelligence Research, 77,
  1591-1636
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05521" title="Abstract">arXiv:2210.05521</a> (replaced) [<a href="/pdf/2210.05521" title="Download PDF">pdf</a>, <a href="/format/2210.05521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peitian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shitao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jing Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05903" title="Abstract">arXiv:2210.05903</a> (replaced) [<a href="/pdf/2210.05903" title="Download PDF">pdf</a>, <a href="/ps/2210.05903" title="Download PostScript">ps</a>, <a href="/format/2210.05903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Web3 Applications: Easing the Access and Transition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Guangsheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+T">Tingting Bi</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yifei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R+P">Ren Ping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Georgalas%2C+N">Nektarios Georgalas</a>, 
<a href="/search/cs?searchtype=author&query=Reeves%2C+A">Andrew Reeves</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code snippets, quantative and qualitative evaluations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.06654" title="Abstract">arXiv:2210.06654</a> (replaced) [<a href="/pdf/2210.06654" title="Download PDF">pdf</a>, <a href="/format/2210.06654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Inventory is Dark and Full of Misinformation: Understanding the  Abuse of Ad Inventory Pooling in the Ad-Tech Supply Chain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vekaria%2C+Y">Yash Vekaria</a> (1), 
<a href="/search/cs?searchtype=author&query=Nithyanand%2C+R">Rishab Nithyanand</a> (2), 
<a href="/search/cs?searchtype=author&query=Shafiq%2C+Z">Zubair Shafiq</a> (1) ((1) University of California, Davis, (2) University of Iowa)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at IEEE Symposium on Security &amp; Privacy (Oakland) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY); Networking and Internet Architecture (cs.NI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07373" title="Abstract">arXiv:2210.07373</a> (replaced) [<a href="/pdf/2210.07373" title="Download PDF">pdf</a>, <a href="/format/2210.07373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mind the Labels: Describing Relations in Knowledge Graphs With  Pretrained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kasner%2C+Z">Zden&#x11b;k Kasner</a>, 
<a href="/search/cs?searchtype=author&query=Konstas%2C+I">Ioannis Konstas</a>, 
<a href="/search/cs?searchtype=author&query=Du%C5%A1ek%2C+O">Ond&#x159;ej Du&#x161;ek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long paper at EACL '23. Code and data: <a href="https://github.com/kasnerz/rel2text">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07808" title="Abstract">arXiv:2210.07808</a> (replaced) [<a href="/pdf/2210.07808" title="Download PDF">pdf</a>, <a href="/ps/2210.07808" title="Download PostScript">ps</a>, <a href="/format/2210.07808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal AdaBoost Converges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Snedeker%2C+C">Conor Snedeker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.08577" title="Abstract">arXiv:2210.08577</a> (replaced) [<a href="/pdf/2210.08577" title="Download PDF">pdf</a>, <a href="/format/2210.08577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Occupancy Grid Map Prediction in Dynamic Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhanteng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Dames%2C+P">Philip Dames</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 7th Annual Conference on Robot Learning (CoRL), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.08970" title="Abstract">arXiv:2210.08970</a> (replaced) [<a href="/pdf/2210.08970" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Twins for Industry 4.0 in the 6G Era
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bin Han</a>, 
<a href="/search/cs?searchtype=author&query=Habibi%2C+M+A">Mohammad Asif Habibi</a>, 
<a href="/search/cs?searchtype=author&query=Richerzhagen%2C+B">Bjoern Richerzhagen</a>, 
<a href="/search/cs?searchtype=author&query=Schindhelm%2C+K">Kim Schindhelm</a>, 
<a href="/search/cs?searchtype=author&query=Zeiger%2C+F">Florian Zeiger</a>, 
<a href="/search/cs?searchtype=author&query=Lamberti%2C+F">Fabrizio Lamberti</a>, 
<a href="/search/cs?searchtype=author&query=Prattic%C3%B2%2C+F+G">Filippo Gabriele Prattic&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Upadhya%2C+K">Karthik Upadhya</a>, 
<a href="/search/cs?searchtype=author&query=Korovesis%2C+C">Charalampos Korovesis</a>, 
<a href="/search/cs?searchtype=author&query=Belikaidis%2C+I">Ioannis-Prodromos Belikaidis</a>, 
<a href="/search/cs?searchtype=author&query=Demestichas%2C+P">Panagiotis Demestichas</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Siyu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Schotten%2C+H+D">Hans D. Schotten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Open Journal of Vehicular Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09017" title="Abstract">arXiv:2210.09017</a> (replaced) [<a href="/pdf/2210.09017" title="Download PDF">pdf</a>, <a href="/ps/2210.09017" title="Download PostScript">ps</a>, <a href="/format/2210.09017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Data-Driven Moving Horizon Estimation for Linear Discrete-Time  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wolff%2C+T+M">Tobias M. Wolff</a>, 
<a href="/search/eess?searchtype=author&query=Lopez%2C+V+G">Victor G. Lopez</a>, 
<a href="/search/eess?searchtype=author&query=M%C3%BCller%2C+M+A">Matthias A. M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09964" title="Abstract">arXiv:2210.09964</a> (replaced) [<a href="/pdf/2210.09964" title="Download PDF">pdf</a>, <a href="/format/2210.09964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Evaluation of Arbitrary Relational Calculus Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raszyk%2C+M">Martin Raszyk</a>, 
<a href="/search/cs?searchtype=author&query=Basin%2C+D">David Basin</a>, 
<a href="/search/cs?searchtype=author&query=Krsti%C4%87%2C+S">Sr&#x111;an Krsti&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Traytel%2C+D">Dmitriy Traytel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13179" title="Abstract">arXiv:2210.13179</a> (replaced) [<a href="/pdf/2210.13179" title="Download PDF">pdf</a>, <a href="/format/2210.13179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A simple probabilistic neural network for machine understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Xie%2C+R">Rongrong Xie</a>, 
<a href="/search/cond-mat?searchtype=author&query=Marsili%2C+M">Matteo Marsili</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.00534" title="Abstract">arXiv:2211.00534</a> (replaced) [<a href="/pdf/2211.00534" title="Download PDF">pdf</a>, <a href="/format/2211.00534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Global Wildfire Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prapas%2C+I">Ioannis Prapas</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+A">Akanksha Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Kondylatos%2C+S">Spyros Kondylatos</a>, 
<a href="/search/cs?searchtype=author&query=Karasante%2C+I">Ilektra Karasante</a>, 
<a href="/search/cs?searchtype=author&query=Panagiotou%2C+E">Eleanna Panagiotou</a>, 
<a href="/search/cs?searchtype=author&query=Alonso%2C+L">Lazaro Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Davalas%2C+C">Charalampos Davalas</a>, 
<a href="/search/cs?searchtype=author&query=Michail%2C+D">Dimitrios Michail</a>, 
<a href="/search/cs?searchtype=author&query=Carvalhais%2C+N">Nuno Carvalhais</a>, 
<a href="/search/cs?searchtype=author&query=Papoutsis%2C+I">Ioannis Papoutsis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the NeurIPS 2022 workshop on Tackling Climate Change with Machine Learning. Version 2 has corrected the table of results (Table 1)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.02478" title="Abstract">arXiv:2211.02478</a> (replaced) [<a href="/pdf/2211.02478" title="Download PDF">pdf</a>, <a href="/format/2211.02478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concentration inequalities for leave-one-out cross validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Avelin%2C+B">Benny Avelin</a>, 
<a href="/search/math?searchtype=author&query=Viitasaari%2C+L">Lauri Viitasaari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.03536" title="Abstract">arXiv:2211.03536</a> (replaced) [<a href="/pdf/2211.03536" title="Download PDF">pdf</a>, <a href="/format/2211.03536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Graph Embedding: A Survey from the Perspective of  Representation Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiahang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jinyuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zaiqiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Shangsong Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 6 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04020" title="Abstract">arXiv:2211.04020</a> (replaced) [<a href="/pdf/2211.04020" title="Download PDF">pdf</a>, <a href="/format/2211.04020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating counterfactual explanations of tumor spatial proteomes to  discover effective strategies for enhancing immune infiltration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+Z+J">Zitong Jerry Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Xu%2C+A+M">Alexander M. Xu</a>, 
<a href="/search/q-bio?searchtype=author&query=Bhargava%2C+A">Aman Bhargava</a>, 
<a href="/search/q-bio?searchtype=author&query=Thomson%2C+M+W">Matt W. Thomson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Genomics (q-bio.GN); Tissues and Organs (q-bio.TO)

</div>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.05952" title="Abstract">arXiv:2211.05952</a> (replaced) [<a href="/pdf/2211.05952" title="Download PDF">pdf</a>, <a href="/format/2211.05952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Domain Coverage for Vehicles with Second-Order Dynamics via  Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xinyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fetecau%2C+R+C">Razvan C. Fetecau</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mo Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06843" title="Abstract">arXiv:2211.06843</a> (replaced) [<a href="/pdf/2211.06843" title="Download PDF">pdf</a>, <a href="/format/2211.06843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization Beyond Feature Alignment: Concept Activation-Guided  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yibing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+C+X">Chris Xing Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07866" title="Abstract">arXiv:2211.07866</a> (replaced) [<a href="/pdf/2211.07866" title="Download PDF">pdf</a>, <a href="/format/2211.07866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Estimation for Longitudinal Network via Adaptive Merging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+H">Haoran Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+J">Junhui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages and 2 figures; appendix including technical proof will be uploaded later
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10020" title="Abstract">arXiv:2211.10020</a> (replaced) [<a href="/pdf/2211.10020" title="Download PDF">pdf</a>, <a href="/format/2211.10020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perception-Based Sampled-Data Optimization of Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cothren%2C+L">Liliaokeawawa Cothren</a>, 
<a href="/search/eess?searchtype=author&query=Bianchin%2C+G">Gianluca Bianchin</a>, 
<a href="/search/eess?searchtype=author&query=Dean%2C+S">Sarah Dean</a>, 
<a href="/search/eess?searchtype=author&query=Dall%27Anese%2C+E">Emiliano Dall&#x27;Anese</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extended version of the paper accepted to IFAC World Congress 2023 for publication, containing proofs, and recently updated to address a typo in Assumption 3
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10960" title="Abstract">arXiv:2211.10960</a> (replaced) [<a href="/pdf/2211.10960" title="Download PDF">pdf</a>, <a href="/format/2211.10960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoCoNet: Coupled Contrastive Learning Network with Multi-level Feature  Ensemble for Multi-modality Image Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+R">Runjia Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Guanyao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Risheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhongxuan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xin Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11917" title="Abstract">arXiv:2211.11917</a> (replaced) [<a href="/pdf/2211.11917" title="Download PDF">pdf</a>, <a href="/format/2211.11917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Iterative Refinement for Modular Source Separation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bralios%2C+D">Dimitrios Bralios</a>, 
<a href="/search/cs?searchtype=author&query=Tzinis%2C+E">Efthymios Tzinis</a>, 
<a href="/search/cs?searchtype=author&query=Wichern%2C+G">Gordon Wichern</a>, 
<a href="/search/cs?searchtype=author&query=Smaragdis%2C+P">Paris Smaragdis</a>, 
<a href="/search/cs?searchtype=author&query=Roux%2C+J+L">Jonathan Le Roux</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICASSP 2023 - 2023 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.12421" title="Abstract">arXiv:2211.12421</a> (replaced) [<a href="/pdf/2211.12421" title="Download PDF">pdf</a>, <a href="/format/2211.12421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Network Neuroscience: On Data Collection and Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Xu%2C+J">Jiaxing Xu</a>, 
<a href="/search/q-bio?searchtype=author&query=Yang%2C+Y">Yunhan Yang</a>, 
<a href="/search/q-bio?searchtype=author&query=Huang%2C+D+T+J">David Tse Jung Huang</a>, 
<a href="/search/q-bio?searchtype=author&query=Gururajapathy%2C+S+S">Sophi Shilpa Gururajapathy</a>, 
<a href="/search/q-bio?searchtype=author&query=Ke%2C+Y">Yiping Ke</a>, 
<a href="/search/q-bio?searchtype=author&query=Qiao%2C+M">Miao Qiao</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+A">Alan Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Kumar%2C+H">Haribalan Kumar</a>, 
<a href="/search/q-bio?searchtype=author&query=McGeown%2C+J">Josh McGeown</a>, 
<a href="/search/q-bio?searchtype=author&query=Kwon%2C+E">Eryn Kwon</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13051" title="Abstract">arXiv:2211.13051</a> (replaced) [<a href="/pdf/2211.13051" title="Download PDF">pdf</a>, <a href="/format/2211.13051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Powderworld: A Platform for Understanding Generalization via Rich Task  Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frans%2C+K">Kevin Frans</a>, 
<a href="/search/cs?searchtype=author&query=Isola%2C+P">Phillip Isola</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13282" title="Abstract">arXiv:2211.13282</a> (replaced) [<a href="/pdf/2211.13282" title="Download PDF">pdf</a>, <a href="/format/2211.13282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Voice-preserving Zero-shot Multiple Accent Conversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mumin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Serai%2C+P">Prashant Serai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jilong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tjandra%2C+A">Andros Tjandra</a>, 
<a href="/search/cs?searchtype=author&query=Manohar%2C+V">Vimal Manohar</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Q">Qing He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ICASSP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14320" title="Abstract">arXiv:2211.14320</a> (replaced) [<a href="/pdf/2211.14320" title="Download PDF">pdf</a>, <a href="/format/2211.14320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bidirectional Representations for Low Resource Spoken Language  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meeus%2C+Q">Quentin Meeus</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>, 
<a href="/search/cs?searchtype=author&query=Van+hamme%2C+H">Hugo Van hamme</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Meeus, Q.; Moens, M.-F.; Van hamme, H. Bidirectional
  Representations for Low-Resource Spoken Language Understanding. Appl. Sci.
  2023, 13, 11291
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15154" title="Abstract">arXiv:2211.15154</a> (replaced) [<a href="/pdf/2211.15154" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven multinomial random forest: A new random forest variant with  strong consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">JunHao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02042" title="Abstract">arXiv:2212.02042</a> (replaced) [<a href="/pdf/2212.02042" title="Download PDF">pdf</a>, <a href="/format/2212.02042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refiner: Data Refining against Gradient Leakage Attacks in Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+M">Mingyuan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaodan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wenmeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jun Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02440" title="Abstract">arXiv:2212.02440</a> (replaced) [<a href="/pdf/2212.02440" title="Download PDF">pdf</a>, <a href="/format/2212.02440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Algorithms for the Fair and Efficient Allocation of Indivisible  Chores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+J">Jugal Garg</a>, 
<a href="/search/cs?searchtype=author&query=Murhekar%2C+A">Aniket Murhekar</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">John Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages. Accepted to IJCAI 2023. This version corrects a typo in Theorem 2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02618" title="Abstract">arXiv:2212.02618</a> (replaced) [<a href="/pdf/2212.02618" title="Download PDF">pdf</a>, <a href="/format/2212.02618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collabs: A Flexible and Performant CRDT Collaboration Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weidner%2C+M">Matthew Weidner</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+H">Huairui Qi</a>, 
<a href="/search/cs?searchtype=author&query=Kjaer%2C+M">Maxime Kjaer</a>, 
<a href="/search/cs?searchtype=author&query=Pradeep%2C+R">Ria Pradeep</a>, 
<a href="/search/cs?searchtype=author&query=Geordie%2C+B">Benito Geordie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Schare%2C+G">Gregory Schare</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+S">Sicheng Xing</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+H">Heather Miller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04755" title="Abstract">arXiv:2212.04755</a> (replaced) [<a href="/pdf/2212.04755" title="Download PDF">pdf</a>, <a href="/format/2212.04755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Cloze to Comprehension: Retrofitting Pre-trained Masked Language  Model to Pre-trained Machine Reader
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiwen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Meng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+L">Luo Si</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05154" title="Abstract">arXiv:2212.05154</a> (replaced) [<a href="/pdf/2212.05154" title="Download PDF">pdf</a>, <a href="/format/2212.05154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Control for Quadruped Locomotion using LTV MPC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+A">Andrew Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+S+S+K+S">Sriram S.K.S Narayanan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05525" title="Abstract">arXiv:2212.05525</a> (replaced) [<a href="/pdf/2212.05525" title="Download PDF">pdf</a>, <a href="/format/2212.05525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned  Receipt Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongkuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Whittaker%2C+E">Edward Whittaker</a>, 
<a href="/search/cs?searchtype=author&query=Kitagishi%2C+I">Ikuo Kitagishi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV2023 RCV Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05762" title="Abstract">arXiv:2212.05762</a> (replaced) [<a href="/pdf/2212.05762" title="Download PDF">pdf</a>, <a href="/format/2212.05762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Momentum Contrastive Pre-training for Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Minda Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Muzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+I">Irwin King</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted by EMNLP 2022. Reference to ACL Anthology: <a href="https://aclanthology.org/2022.emnlp-main.291.pdf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07411" title="Abstract">arXiv:2212.07411</a> (replaced) [<a href="/pdf/2212.07411" title="Download PDF">pdf</a>, <a href="/ps/2212.07411" title="Download PostScript">ps</a>, <a href="/format/2212.07411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation schemes for McKean-Vlasov and Boltzmann type equations  (error analyses in total variation distance)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Qin%2C+Y">Yifeng Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09598" title="Abstract">arXiv:2212.09598</a> (replaced) [<a href="/pdf/2212.09598" title="Download PDF">pdf</a>, <a href="/format/2212.09598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query-as-context Pre-training for Dense Passage Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+G">Guangyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Wanhui Qian</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zijia Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Songlin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item801">[801]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09968" title="Abstract">arXiv:2212.09968</a> (replaced) [<a href="/pdf/2212.09968" title="Download PDF">pdf</a>, <a href="/format/2212.09968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Improving Summarization Factual Consistency from Natural Language  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deb%2C+B">Budhaditya Deb</a>, 
<a href="/search/cs?searchtype=author&query=Teruel%2C+M">Milagro Teruel</a>, 
<a href="/search/cs?searchtype=author&query=Halfaker%2C+A">Aaron Halfaker</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+D">Dragomir Radev</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A+H">Ahmed H. Awadallah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2023 Camera Ready, GitHub Repo: <a href="https://github.com/microsoft/DeFacto">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item802">[802]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11337" title="Abstract">arXiv:2212.11337</a> (replaced) [<a href="/pdf/2212.11337" title="Download PDF">pdf</a>, <a href="/format/2212.11337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unscrambling quantum information with Clifford decoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Oliviero%2C+S+F+E">Salvatore F.E. Oliviero</a>, 
<a href="/search/quant-ph?searchtype=author&query=Leone%2C+L">Lorenzo Leone</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lloyd%2C+S">Seth Lloyd</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hamma%2C+A">Alioscia Hamma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); General Relativity and Quantum Cosmology (gr-qc); High Energy Physics - Theory (hep-th)

</div>
</div>
</dd>
<dt><a name="item803">[803]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11580" title="Abstract">arXiv:2212.11580</a> (replaced) [<a href="/pdf/2212.11580" title="Download PDF">pdf</a>, <a href="/ps/2212.11580" title="Download PostScript">ps</a>, <a href="/format/2212.11580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theory of Conversion Relations for Prefixed Units of Measure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Widemann%2C+B+T+y">Baltasar Tranc&#xf3;n y Widemann</a>, 
<a href="/search/cs?searchtype=author&query=Lepper%2C+M">Markus Lepper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item804">[804]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.13381" title="Abstract">arXiv:2212.13381</a> (replaced) [<a href="/pdf/2212.13381" title="Download PDF">pdf</a>, <a href="/format/2212.13381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MixupE: Understanding and Improving Mixup from Directional Derivative  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yingtian Zou</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+V">Vikas Verma</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Sarthak Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W+H">Wai Hoh Tang</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+H">Hieu Pham</a>, 
<a href="/search/cs?searchtype=author&query=Kannala%2C+J">Juho Kannala</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Solin%2C+A">Arno Solin</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, Best Student Paper Award at UAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item805">[805]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.13854" title="Abstract">arXiv:2212.13854</a> (replaced) [<a href="/pdf/2212.13854" title="Download PDF">pdf</a>, <a href="/format/2212.13854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A DRL Approach for RIS-Assisted Full-Duplex UL and DL Transmission:  Beamforming, Phase Shift and Power Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nayak%2C+N">Nancy Nayak</a>, 
<a href="/search/cs?searchtype=author&query=Kalyani%2C+S">Sheetal Kalyani</a>, 
<a href="/search/cs?searchtype=author&query=Suraweera%2C+H+A">Himal A. Suraweera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item806">[806]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02136" title="Abstract">arXiv:2301.02136</a> (replaced) [<a href="/pdf/2301.02136" title="Download PDF">pdf</a>, <a href="/format/2301.02136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale Transforms for Signals on Simplicial Complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saito%2C+N">Naoki Saito</a>, 
<a href="/search/cs?searchtype=author&query=Schonsheck%2C+S+C">Stefan C. Schonsheck</a>, 
<a href="/search/cs?searchtype=author&query=Shvarts%2C+E">Eugene Shvarts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 Pages, Comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Signal Processing (eess.SP); Combinatorics (math.CO); Numerical Analysis (math.NA); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item807">[807]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03749" title="Abstract">arXiv:2301.03749</a> (replaced) [<a href="/pdf/2301.03749" title="Download PDF">pdf</a>, <a href="/format/2301.03749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Markovian Sliced Wasserstein Distances: Beyond Independent Projections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+K">Khai Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Ren%2C+T">Tongzheng Ren</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023, 29 pages, 8 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item808">[808]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04213" title="Abstract">arXiv:2301.04213</a> (replaced) [<a href="/pdf/2301.04213" title="Download PDF">pdf</a>, <a href="/format/2301.04213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Localization Inform Editing? Surprising Differences in  Causality-Based Localization vs. Knowledge Editing in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hase%2C+P">Peter Hase</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Been Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ghandeharioun%2C+A">Asma Ghandeharioun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Spotlight). 26 pages, 22 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item809">[809]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09771" title="Abstract">arXiv:2301.09771</a> (replaced) [<a href="/pdf/2301.09771" title="Download PDF">pdf</a>, <a href="/format/2301.09771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automation and AI Technology in Surface Mining With a Brief Introduction  to Open-Pit Operations in the Pilbara
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leung%2C+R">Raymond Leung</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+A+J">Andrew J Hill</a>, 
<a href="/search/cs?searchtype=author&query=Melkumyan%2C+A">Arman Melkumyan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted manuscript. Paper provides insights on state-of-the-art technologies and future trends. Keywords: Mining automation, robotics, intelligent systems, machine learning, remote sensing, geostatistics, planning, scheduling, optimization, modelling, geology, complex systems. Document: 20 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; General Literature (cs.GL)

</div>
</div>
</dd>
<dt><a name="item810">[810]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10472" title="Abstract">arXiv:2301.10472</a> (replaced) [<a href="/pdf/2301.10472" title="Download PDF">pdf</a>, <a href="/format/2301.10472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Davis Liang</a>, 
<a href="/search/cs?searchtype=author&query=Gonen%2C+H">Hila Gonen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuning Mao</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+R">Rui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+N">Naman Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Ghazvininejad%2C+M">Marjan Ghazvininejad</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Khabsa%2C+M">Madian Khabsa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item811">[811]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11375" title="Abstract">arXiv:2301.11375</a> (replaced) [<a href="/pdf/2301.11375" title="Download PDF">pdf</a>, <a href="/format/2301.11375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural networks learn to magnify areas near decision boundaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zavatone-Veth%2C+J+A">Jacob A. Zavatone-Veth</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Rubinfien%2C+J+A">Julian A. Rubinfien</a>, 
<a href="/search/cs?searchtype=author&query=Pehlevan%2C+C">Cengiz Pehlevan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 93 pages, 48 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item812">[812]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13629" title="Abstract">arXiv:2301.13629</a> (replaced) [<a href="/pdf/2301.13629" title="Download PDF">pdf</a>, <a href="/format/2301.13629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Haomin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Youfang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yutong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+H">Huaiyu Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+R">Roger Zimmermann</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 31st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item813">[813]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13669" title="Abstract">arXiv:2301.13669</a> (replaced) [<a href="/pdf/2301.13669" title="Download PDF">pdf</a>, <a href="/format/2301.13669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards interpretable quantum machine learning via single-photon quantum  walks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Flamini%2C+F">Fulvio Flamini</a>, 
<a href="/search/quant-ph?searchtype=author&query=Krumm%2C+M">Marius Krumm</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fiderer%2C+L+J">Lukas J. Fiderer</a>, 
<a href="/search/quant-ph?searchtype=author&query=M%C3%BCller%2C+T">Thomas M&#xfc;ller</a>, 
<a href="/search/quant-ph?searchtype=author&query=Briegel%2C+H+J">Hans J. Briegel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11+8 pages, 6+9 figures, 2 tables. F. Flamini and M. Krumm contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item814">[814]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13807" title="Abstract">arXiv:2301.13807</a> (replaced) [<a href="/pdf/2301.13807" title="Download PDF">pdf</a>, <a href="/format/2301.13807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying the Hazard Boundary of ML-enabled Autonomous Systems Using  Cooperative Co-Evolutionary Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharifi%2C+S">Sepehr Sharifi</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+D">Donghwan Shin</a>, 
<a href="/search/cs?searchtype=author&query=Briand%2C+L+C">Lionel C. Briand</a>, 
<a href="/search/cs?searchtype=author&query=Aschbacher%2C+N">Nathan Aschbacher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication by IEEE Transactions on Software Engineering (TSE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item815">[815]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01474" title="Abstract">arXiv:2302.01474</a> (replaced) [<a href="/pdf/2302.01474" title="Download PDF">pdf</a>, <a href="/format/2302.01474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defensive ML: Defending Architectural Side-channels with Adversarial  Obfuscation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nam%2C+H">Hyoungwook Nam</a>, 
<a href="/search/cs?searchtype=author&query=Pothukuchi%2C+R+P">Raghavendra Pradyumna Pothukuchi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N+S">Nam Sung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Torrellas%2C+J">Josep Torrellas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item816">[816]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02088" title="Abstract">arXiv:2302.02088</a> (replaced) [<a href="/pdf/2302.02088" title="Download PDF">pdf</a>, <a href="/format/2302.02088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene  Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Susan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yapeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Anurag Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenliang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item817">[817]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02173" title="Abstract">arXiv:2302.02173</a> (replaced) [<a href="/pdf/2302.02173" title="Download PDF">pdf</a>, <a href="/format/2302.02173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Deep Learning based Time Series Analysis with Frequency  Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+K">Kun Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Longbing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shoujin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+G">Guodong Long</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Liang Hu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hui He</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhendong Niu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item818">[818]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02703" title="Abstract">arXiv:2302.02703</a> (replaced) [<a href="/pdf/2302.02703" title="Download PDF">pdf</a>, <a href="/format/2302.02703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging TLA+ Specifications to Improve the Reliability of the  ZooKeeper Coordination Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+L">Lingzhi Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Binyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoxing Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item819">[819]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02913" title="Abstract">arXiv:2302.02913</a> (replaced) [<a href="/pdf/2302.02913" title="Download PDF">pdf</a>, <a href="/format/2302.02913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Statistical Similarity: Rethinking Metrics for Deep Generative  Models in Engineering Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Regenwetter%2C+L">Lyle Regenwetter</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Akash Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Gutfreund%2C+D">Dan Gutfreund</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Faez Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item820">[820]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06397" title="Abstract">arXiv:2302.06397</a> (replaced) [<a href="/pdf/2302.06397" title="Download PDF">pdf</a>, <a href="/format/2302.06397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Type-Aware Decomposed Framework for Few-Shot Named Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+T">Tieyun Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023, camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item821">[821]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08357" title="Abstract">arXiv:2302.08357</a> (replaced) [<a href="/pdf/2302.08357" title="Download PDF">pdf</a>, <a href="/format/2302.08357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boundary Guided Learning-Free Semantic Control with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Ye Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Russakovsky%2C+O">Olga Russakovsky</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yan Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. 27 pages including appendices, code at <a href="https://github.com/L-YeZhu/BoundaryDiffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item822">[822]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08719" title="Abstract">arXiv:2302.08719</a> (replaced) [<a href="/pdf/2302.08719" title="Download PDF">pdf</a>, <a href="/ps/2302.08719" title="Download PostScript">ps</a>, <a href="/format/2302.08719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Morley finite element analysis for fourth-order elliptic equations under  a semi-regular mesh condition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ishizaka%2C+H">Hiroki Ishizaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item823">[823]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09127" title="Abstract">arXiv:2302.09127</a> (replaced) [<a href="/pdf/2302.09127" title="Download PDF">pdf</a>, <a href="/ps/2302.09127" title="Download PostScript">ps</a>, <a href="/format/2302.09127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Pseudo-Markets for Reusable Public Resources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Siddhartha Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Fikioris%2C+G">Giannis Fikioris</a>, 
<a href="/search/cs?searchtype=author&query=Tardos%2C+%C3%89">&#xc9;va Tardos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item824">[824]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09344" title="Abstract">arXiv:2302.09344</a> (replaced) [<a href="/pdf/2302.09344" title="Download PDF">pdf</a>, <a href="/format/2302.09344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Distribution Shift: Spurious Features Through the Lens of  Training Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murali%2C+N">Nihal Murali</a>, 
<a href="/search/cs?searchtype=author&query=Puli%2C+A">Aahlad Puli</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Ke Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ranganath%2C+R">Rajesh Ranganath</a>, 
<a href="/search/cs?searchtype=author&query=Batmanghelich%2C+K">Kayhan Batmanghelich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main paper: 12 pages, 2 tables, and 10 figures. Supplementary: 10 pages and 9 figures. Accepted in TMLR23 (<a href="https://openreview.net/pdf?id=Tkvmt9nDmB">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item825">[825]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09422" title="Abstract">arXiv:2302.09422</a> (replaced) [<a href="/pdf/2302.09422" title="Download PDF">pdf</a>, <a href="/format/2302.09422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Attention Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nam%2C+H">Hyoungwook Nam</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+S+B">Seung Byum Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item826">[826]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11668" title="Abstract">arXiv:2302.11668</a> (replaced) [<a href="/pdf/2302.11668" title="Download PDF">pdf</a>, <a href="/ps/2302.11668" title="Download PostScript">ps</a>, <a href="/format/2302.11668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graphs with minimum fractional domatic number
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gadouleau%2C+M">Maximilien Gadouleau</a>, 
<a href="/search/math?searchtype=author&query=Harms%2C+N">Nathaniel Harms</a>, 
<a href="/search/math?searchtype=author&query=Mertzios%2C+G+B">George B. Mertzios</a>, 
<a href="/search/math?searchtype=author&query=Zamaraev%2C+V">Viktor Zamaraev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item827">[827]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11944" title="Abstract">arXiv:2302.11944</a> (replaced) [<a href="/pdf/2302.11944" title="Download PDF">pdf</a>, <a href="/format/2302.11944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Situation Testing: Uncovering Discrimination under  Fairness given the Difference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Alvarez%2C+J+M">Jose M. Alvarez</a>, 
<a href="/search/stat?searchtype=author&query=Ruggieri%2C+S">Salvatore Ruggieri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item828">[828]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13372" title="Abstract">arXiv:2302.13372</a> (replaced) [<a href="/pdf/2302.13372" title="Download PDF">pdf</a>, <a href="/format/2302.13372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Localizing Moments in Long Video Via Multimodal Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barrios%2C+W">Wayner Barrios</a>, 
<a href="/search/cs?searchtype=author&query=Soldan%2C+M">Mattia Soldan</a>, 
<a href="/search/cs?searchtype=author&query=Ceballos-Arroyo%2C+A+M">Alberto Mario Ceballos-Arroyo</a>, 
<a href="/search/cs?searchtype=author&query=Heilbron%2C+F+C">Fabian Caba Heilbron</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+B">Bernard Ghanem</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item829">[829]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14201" title="Abstract">arXiv:2302.14201</a> (replaced) [<a href="/pdf/2302.14201" title="Download PDF">pdf</a>, <a href="/format/2302.14201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nautilus: A Framework for Cross-Layer Cartography of Submarine Cables  and IP Links
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramanathan%2C+A">Alagappan Ramanathan</a>, 
<a href="/search/cs?searchtype=author&query=Jyothi%2C+S+A">Sangeetha Abdu Jyothi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item830">[830]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01310" title="Abstract">arXiv:2303.01310</a> (replaced) [<a href="/pdf/2303.01310" title="Download PDF">pdf</a>, <a href="/format/2303.01310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Language-Conditioned Deformable Object Manipulation with Graph  Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+K">Kai Mo</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuhong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+C">Chongkun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueqian Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item831">[831]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01483" title="Abstract">arXiv:2303.01483</a> (replaced) [<a href="/pdf/2303.01483" title="Download PDF">pdf</a>, <a href="/format/2303.01483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auxiliary Functions as Koopman Observables: Data-Driven Analysis of  Dynamical Systems via Polynomial Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bramburger%2C+J+J">Jason J. Bramburger</a>, 
<a href="/search/math?searchtype=author&query=Fantuzzi%2C+G">Giovanni Fantuzzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We have significantly expanded the presentation. This has improved the presentation and made the paper more readable. Comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item832">[832]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02563" title="Abstract">arXiv:2303.02563</a> (replaced) [<a href="/pdf/2303.02563" title="Download PDF">pdf</a>, <a href="/format/2303.02563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ong%2C+K">Keane Ong</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Heever%2C+W">Wihan van der Heever</a>, 
<a href="/search/cs?searchtype=author&query=Satapathy%2C+R">Ranjan Satapathy</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>, 
<a href="/search/cs?searchtype=author&query=Mengaldo%2C+G">Gianmarco Mengaldo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item833">[833]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03271" title="Abstract">arXiv:2303.03271</a> (replaced) [<a href="/pdf/2303.03271" title="Download PDF">pdf</a>, <a href="/ps/2303.03271" title="Download PostScript">ps</a>, <a href="/format/2303.03271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fibrational Tale of Operational Logical Relations: Pure, Effectful and  Differential
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dagnino%2C+F">Francesco Dagnino</a>, 
<a href="/search/cs?searchtype=author&query=Gavazzo%2C+F">Francesco Gavazzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item834">[834]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03811" title="Abstract">arXiv:2303.03811</a> (replaced) [<a href="/pdf/2303.03811" title="Download PDF">pdf</a>, <a href="/format/2303.03811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Environment Transformer and Policy Optimization for Model-Based Offline  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengqin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Meixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shaojie Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item835">[835]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04038" title="Abstract">arXiv:2303.04038</a> (replaced) [<a href="/pdf/2303.04038" title="Download PDF">pdf</a>, <a href="/format/2303.04038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Root Cause Identification for Collective Anomalies in Time Series given  an Acyclic Summary Causal Graph with Loops
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Assaad%2C+C+K">Charles K. Assaad</a>, 
<a href="/search/cs?searchtype=author&query=Ez-zejjari%2C+I">Imad Ez-zejjari</a>, 
<a href="/search/cs?searchtype=author&query=Zan%2C+L">Lei Zan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS) 2023, Valencia, Spain
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item836">[836]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06424" title="Abstract">arXiv:2303.06424</a> (replaced) [<a href="/pdf/2303.06424" title="Download PDF">pdf</a>, <a href="/format/2303.06424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularized Vector Quantization for Tokenized Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiahui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+F">Fangneng Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Theobalt%2C+C">Christian Theobalt</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item837">[837]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07186" title="Abstract">arXiv:2303.07186</a> (replaced) [<a href="/pdf/2303.07186" title="Download PDF">pdf</a>, <a href="/format/2303.07186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio-based Roughness Sensing and Tactile Feedback for Haptic Perception  in Telepresence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%C3%A4tzold%2C+B">Bastian P&#xe4;tzold</a>, 
<a href="/search/cs?searchtype=author&query=Rochow%2C+A">Andre Rochow</a>, 
<a href="/search/cs?searchtype=author&query=Schreiber%2C+M">Michael Schreiber</a>, 
<a href="/search/cs?searchtype=author&query=Memmesheimer%2C+R">Raphael Memmesheimer</a>, 
<a href="/search/cs?searchtype=author&query=Lenz%2C+C">Christian Lenz</a>, 
<a href="/search/cs?searchtype=author&query=Schwarz%2C+M">Max Schwarz</a>, 
<a href="/search/cs?searchtype=author&query=Behnke%2C+S">Sven Behnke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE International Conference on Systems, Man, and Cybernetics (SMC), Honolulu, Hawaii, USA, October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item838">[838]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07312" title="Abstract">arXiv:2303.07312</a> (replaced) [<a href="/pdf/2303.07312" title="Download PDF">pdf</a>, <a href="/format/2303.07312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing LiDAR performance: Robust De-skewing Exclusively Relying on  Range Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salem%2C+O">Omar Salem</a>, 
<a href="/search/cs?searchtype=author&query=Giacomini%2C+E">Emanuele Giacomini</a>, 
<a href="/search/cs?searchtype=author&query=Brizi%2C+L">Leonardo Brizi</a>, 
<a href="/search/cs?searchtype=author&query=Di+Giammarino%2C+L">Luca Di Giammarino</a>, 
<a href="/search/cs?searchtype=author&query=Grisetti%2C+G">Giorgio Grisetti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages , 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item839">[839]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07759" title="Abstract">arXiv:2303.07759</a> (replaced) [<a href="/pdf/2303.07759" title="Download PDF">pdf</a>, <a href="/format/2303.07759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Baseline for Supervised Surround-view Depth Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xianda Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Wenjie Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunpeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item840">[840]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08117" title="Abstract">arXiv:2303.08117</a> (replaced) [<a href="/pdf/2303.08117" title="Download PDF">pdf</a>, <a href="/format/2303.08117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Transformers Parse while Predicting the Masked Word?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haoyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Panigrahi%2C+A">Abhishek Panigrahi</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+R">Rong Ge</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Sanjeev Arora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accpeted to EMNLP 2023, 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item841">[841]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09366" title="Abstract">arXiv:2303.09366</a> (replaced) [<a href="/pdf/2303.09366" title="Download PDF">pdf</a>, <a href="/format/2303.09366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Scope of In-Context Learning for the Extraction of Medical Temporal  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seegmiller%2C+P">Parker Seegmiller</a>, 
<a href="/search/cs?searchtype=author&query=Gatto%2C+J">Joseph Gatto</a>, 
<a href="/search/cs?searchtype=author&query=Basak%2C+M">Madhusudan Basak</a>, 
<a href="/search/cs?searchtype=author&query=Cook%2C+D">Diane Cook</a>, 
<a href="/search/cs?searchtype=author&query=Ghasemzadeh%2C+H">Hassan Ghasemzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Stankovic%2C+J">John Stankovic</a>, 
<a href="/search/cs?searchtype=author&query=Preum%2C+S">Sarah Preum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item842">[842]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09639" title="Abstract">arXiv:2303.09639</a> (replaced) [<a href="/pdf/2303.09639" title="Download PDF">pdf</a>, <a href="/format/2303.09639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Architecture Search for Effective Teacher-Student Knowledge  Transfer in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A">Aashka Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Udagawa%2C+T">Takuma Udagawa</a>, 
<a href="/search/cs?searchtype=author&query=Merler%2C+M">Michele Merler</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+R">Rameswar Panda</a>, 
<a href="/search/cs?searchtype=author&query=El-Kurdi%2C+Y">Yousef El-Kurdi</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+B">Bishwaranjan Bhattacharjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item843">[843]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10137" title="Abstract">arXiv:2303.10137</a> (replaced) [<a href="/pdf/2303.10137" title="Download PDF">pdf</a>, <a href="/format/2303.10137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Recipe for Watermarking Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunqing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+N">Ngai-Man Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Min Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item844">[844]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11108" title="Abstract">arXiv:2303.11108</a> (replaced) [<a href="/pdf/2303.11108" title="Download PDF">pdf</a>, <a href="/format/2303.11108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHATEDIT: Towards Multi-turn Interactive Facial Image Editing via  Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+X">Xing Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zekun Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peipei Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yibo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hailin Shi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item845">[845]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13320" title="Abstract">arXiv:2303.13320</a> (replaced) [<a href="/pdf/2303.13320" title="Download PDF">pdf</a>, <a href="/format/2303.13320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QDP: Learning to Sequentially Optimise Quasi-Static and Dynamic  Manipulation Primitives for Robotic Cloth Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blanco-Mulero%2C+D">David Blanco-Mulero</a>, 
<a href="/search/cs?searchtype=author&query=Alcan%2C+G">Gokhan Alcan</a>, 
<a href="/search/cs?searchtype=author&query=Abu-Dakka%2C+F+J">Fares J. Abu-Dakka</a>, 
<a href="/search/cs?searchtype=author&query=Kyrki%2C+V">Ville Kyrki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE/RSJ IROS 2023. 8 pages, 7 figures. Supplementary material available at <a href="https://sites.google.com/view/qdp-srl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item846">[846]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13496" title="Abstract">arXiv:2303.13496</a> (replaced) [<a href="/pdf/2303.13496" title="Download PDF">pdf</a>, <a href="/format/2303.13496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The effectiveness of MAE pre-pretraining for billion-scale pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mannat Singh</a>, 
<a href="/search/cs?searchtype=author&query=Duval%2C+Q">Quentin Duval</a>, 
<a href="/search/cs?searchtype=author&query=Alwala%2C+K+V">Kalyan Vasudev Alwala</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Haoqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaibhav Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Adcock%2C+A">Aaron Adcock</a>, 
<a href="/search/cs?searchtype=author&query=Joulin%2C+A">Armand Joulin</a>, 
<a href="/search/cs?searchtype=author&query=Doll%C3%A1r%2C+P">Piotr Doll&#xe1;r</a>, 
<a href="/search/cs?searchtype=author&query=Feichtenhofer%2C+C">Christoph Feichtenhofer</a>, 
<a href="/search/cs?searchtype=author&query=Girshick%2C+R">Ross Girshick</a>, 
<a href="/search/cs?searchtype=author&query=Girdhar%2C+R">Rohit Girdhar</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+I">Ishan Misra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item847">[847]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13780" title="Abstract">arXiv:2303.13780</a> (replaced) [<a href="/pdf/2303.13780" title="Download PDF">pdf</a>, <a href="/format/2303.13780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Making the Most of ChatGPT for Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Keqin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Q">Qihuang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuebo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+Y">Yuanxin Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item848">[848]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14438" title="Abstract">arXiv:2303.14438</a> (replaced) [<a href="/pdf/2303.14438" title="Download PDF">pdf</a>, <a href="/format/2303.14438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Highly Available Blockchain Nodes With N-Version Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ron%2C+J">Javier Ron</a>, 
<a href="/search/cs?searchtype=author&query=Soto-Valero%2C+C">C&#xe9;sar Soto-Valero</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Long Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Baudry%2C+B">Benoit Baudry</a>, 
<a href="/search/cs?searchtype=author&query=Monperrus%2C+M">Martin Monperrus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item849">[849]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14613" title="Abstract">arXiv:2303.14613</a> (replaced) [<a href="/pdf/2303.14613" title="Download PDF">pdf</a>, <a href="/format/2303.14613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ao%2C+T">Tenglong Ao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Libin Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH 2023 (Journal Track); Project Page: <a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item850">[850]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15652" title="Abstract">arXiv:2303.15652</a> (replaced) [<a href="/pdf/2303.15652" title="Download PDF">pdf</a>, <a href="/format/2303.15652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Dynamic Pricing: Optimal Regret in a Global Shrinkage Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhuyan%2C+R+R">Rashmi Ranjan Bhuyan</a>, 
<a href="/search/cs?searchtype=author&query=Javanmard%2C+A">Adel Javanmard</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungchul Kim</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+G">Gourab Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+R+A">Ryan A. Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Handong Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item851">[851]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16045" title="Abstract">arXiv:2303.16045</a> (replaced) [<a href="/pdf/2303.16045" title="Download PDF">pdf</a>, <a href="/format/2303.16045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Spatial Deconvolution and Message Reconstruction from a Large  Generative Model of Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zenil%2C+H">Hector Zenil</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+A">Alyssa Adams</a>, 
<a href="/search/cs?searchtype=author&query=Abrah%C3%A3o%2C+F+S">Felipe S. Abrah&#xe3;o</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item852">[852]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16317" title="Abstract">arXiv:2303.16317</a> (replaced) [<a href="/pdf/2303.16317" title="Download PDF">pdf</a>, <a href="/ps/2303.16317" title="Download PostScript">ps</a>, <a href="/format/2303.16317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operator learning with PCA-Net: upper and lower complexity bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanthaler%2C+S">Samuel Lanthaler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item853">[853]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.18201" title="Abstract">arXiv:2303.18201</a> (replaced) [<a href="/pdf/2303.18201" title="Download PDF">pdf</a>, <a href="/format/2303.18201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Suraj Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Chattopadhyay%2C+S">Soumi Chattopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Adak%2C+C">Chandranath Adak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 Pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item854">[854]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00485" title="Abstract">arXiv:2304.00485</a> (replaced) [<a href="/pdf/2304.00485" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Mining for Cybersecurity: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bo Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yong Fang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yanfang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Junping Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item855">[855]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01046" title="Abstract">arXiv:2304.01046</a> (replaced) [<a href="/pdf/2304.01046" title="Download PDF">pdf</a>, <a href="/format/2304.01046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Manifold Learning for Reading Comprehension and Logical Reasoning  Tasks with Polytuplet Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jeffrey Lu</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+I">Ivan Rodriguez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to FICC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item856">[856]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01492" title="Abstract">arXiv:2304.01492</a> (replaced) [<a href="/pdf/2304.01492" title="Download PDF">pdf</a>, <a href="/format/2304.01492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Contrastive Transfer Framework with Propagation Structure for  Boosting Low-Resource Rumor Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongzhan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruichao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Mingfei Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An extension of the first contrastive approach for low-resource rumor detection (<a href="/abs/2204.08143">arXiv:2204.08143</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item857">[857]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04403" title="Abstract">arXiv:2304.04403</a> (replaced) [<a href="/pdf/2304.04403" title="Download PDF">pdf</a>, <a href="/format/2304.04403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> H2RBox-v2: Incorporating Symmetry for Boosting Horizontal Box Supervised  Oriented Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xue Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingyun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gefan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Da%2C+F">Feipeng Da</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures, 11 tables, accepted by NeurIPS'23, the source code is available at <a href="https://github.com/open-mmlab/mmrotate">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item858">[858]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04998" title="Abstract">arXiv:2304.04998</a> (replaced) [<a href="/pdf/2304.04998" title="Download PDF">pdf</a>, <a href="/format/2304.04998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EESMR: Energy Efficient BFT-SMR for the masses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhat%2C+A">Adithya Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Bandarupalli%2C+A">Akhil Bandarupalli</a>, 
<a href="/search/cs?searchtype=author&query=Nagaraj%2C+M">Manish Nagaraj</a>, 
<a href="/search/cs?searchtype=author&query=Bagchi%2C+S">Saurabh Bagchi</a>, 
<a href="/search/cs?searchtype=author&query=Kate%2C+A">Aniket Kate</a>, 
<a href="/search/cs?searchtype=author&query=Reiter%2C+M+K">Michael K. Reiter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appearing in Middleware 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item859">[859]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08342" title="Abstract">arXiv:2304.08342</a> (replaced) [<a href="/pdf/2304.08342" title="Download PDF">pdf</a>, <a href="/format/2304.08342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NF-ULA: Langevin Monte Carlo with Normalizing Flow Prior for Imaging  Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cai%2C+Z">Ziruo Cai</a>, 
<a href="/search/math?searchtype=author&query=Tang%2C+J">Junqi Tang</a>, 
<a href="/search/math?searchtype=author&query=Mukherjee%2C+S">Subhadip Mukherjee</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jinglai Li</a>, 
<a href="/search/math?searchtype=author&query=Sch%C3%B6nlieb%2C+C+B">Carola Bibiane Sch&#xf6;nlieb</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xiaoqun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item860">[860]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08612" title="Abstract">arXiv:2304.08612</a> (replaced) [<a href="/pdf/2304.08612" title="Download PDF">pdf</a>, <a href="/format/2304.08612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Discrete and Backpropagation: Straight-Through and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chengyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaodong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item861">[861]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08799" title="Abstract">arXiv:2304.08799</a> (replaced) [<a href="/pdf/2304.08799" title="Download PDF">pdf</a>, <a href="/format/2304.08799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised 3D Action Representation Learning with Skeleton Cloud  Colorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Siyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Hwa%2C+E+M">Er Meng Hwa</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yongjian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Kot%2C+A+C">Alex C. Kot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TPAMI. This work is an extension of our ICCV 2021 paper [<a href="/abs/2108.01959">arXiv:2108.01959</a>] <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Yang_Skeleton_Cloud_Colorization_for_Unsupervised_3D_Action_Representation_Learning_ICCV_2021_paper.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item862">[862]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11165" title="Abstract">arXiv:2304.11165</a> (replaced) [<a href="/pdf/2304.11165" title="Download PDF">pdf</a>, <a href="/format/2304.11165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An open-source pipeline for solving continuous reaction-diffusion models  in image-based geometries of porous media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stark%2C+J">Justina Stark</a>, 
<a href="/search/cs?searchtype=author&query=Sbalzarini%2C+I+F">Ivo F. Sbalzarini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 figures, 1 appendix figure
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Stark and I. F. Sbalzarini. An open-source pipeline for solving
  continuous reaction-diffusion models in image-based geometries of porous
  media. J. Comput. Sci., 72:102118, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item863">[863]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11405" title="Abstract">arXiv:2304.11405</a> (replaced) [<a href="/pdf/2304.11405" title="Download PDF">pdf</a>, <a href="/format/2304.11405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Twins in Wind Energy: Emerging Technologies and  Industry-Informed Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stadtman%2C+F">Florian Stadtman</a>, 
<a href="/search/cs?searchtype=author&query=Rasheed%2C+A">Adil Rasheed</a>, 
<a href="/search/cs?searchtype=author&query=Kvamsdal%2C+T">Trond Kvamsdal</a>, 
<a href="/search/cs?searchtype=author&query=Johannessen%2C+K+A">Kjetil Andr&#xe9; Johannessen</a>, 
<a href="/search/cs?searchtype=author&query=San%2C+O">Omer San</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6lle%2C+K">Konstanze K&#xf6;lle</a>, 
<a href="/search/cs?searchtype=author&query=Tande%2C+J+O+G">John Olav Gi&#xe6;ver Tande</a>, 
<a href="/search/cs?searchtype=author&query=Barstad%2C+I">Idar Barstad</a>, 
<a href="/search/cs?searchtype=author&query=Benhamou%2C+A">Alexis Benhamou</a>, 
<a href="/search/cs?searchtype=author&query=Brathaug%2C+T">Thomas Brathaug</a>, 
<a href="/search/cs?searchtype=author&query=Christiansen%2C+T">Tore Christiansen</a>, 
<a href="/search/cs?searchtype=author&query=Firle%2C+A">Anouk-Letizia Firle</a>, 
<a href="/search/cs?searchtype=author&query=Fjeldly%2C+A">Alexander Fjeldly</a>, 
<a href="/search/cs?searchtype=author&query=Fr%C3%B8yd%2C+L">Lars Fr&#xf8;yd</a>, 
<a href="/search/cs?searchtype=author&query=Gleim%2C+A">Alexander Gleim</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B8iberget%2C+A">Alexander H&#xf8;iberget</a>, 
<a href="/search/cs?searchtype=author&query=Meissner%2C+C">Catherine Meissner</a>, 
<a href="/search/cs?searchtype=author&query=Nyg%C3%A5rd%2C+G">Guttorm Nyg&#xe5;rd</a>, 
<a href="/search/cs?searchtype=author&query=Olsen%2C+J">J&#xf8;rgen Olsen</a>, 
<a href="/search/cs?searchtype=author&query=Paulshus%2C+H">H&#xe5;vard Paulshus</a>, 
<a href="/search/cs?searchtype=author&query=Rasmussen%2C+T">Tore Rasmussen</a>, 
<a href="/search/cs?searchtype=author&query=Rishoff%2C+E">Elling Rishoff</a>, 
<a href="/search/cs?searchtype=author&query=Scibilia%2C+F">Francesco Scibilia</a>, 
<a href="/search/cs?searchtype=author&query=Skog%C3%A5s%2C+J+O">John Olav Skog&#xe5;s</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> F. Stadtmann and A. Rasheed et al., "Digital Twins in Wind Energy:
  Emerging Technologies and Industry-Informed Future Directions," in IEEE
  Access, vol. 11, pp. 110762-110795, 2023, doi: 10.1109/ACCESS.2023.3321320
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item864">[864]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11511" title="Abstract">arXiv:2304.11511</a> (replaced) [<a href="/pdf/2304.11511" title="Download PDF">pdf</a>, <a href="/format/2304.11511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuMoS: A Framework for Preserving Security of Quantum Machine Learning  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+Z">Zhepeng Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+J">Jinyang Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hu%2C+Z">Zhirui Hu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gage%2C+B">Blake Gage</a>, 
<a href="/search/quant-ph?searchtype=author&query=Iwasawa%2C+E">Elizabeth Iwasawa</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jiang%2C+W">Weiwen Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item865">[865]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13586" title="Abstract">arXiv:2304.13586</a> (replaced) [<a href="/pdf/2304.13586" title="Download PDF">pdf</a>, <a href="/format/2304.13586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Based Sliced Wasserstein Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+K">Khai Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023, 31 pages, 8 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item866">[866]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14767" title="Abstract">arXiv:2304.14767</a> (replaced) [<a href="/pdf/2304.14767" title="Download PDF">pdf</a>, <a href="/format/2304.14767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dissecting Recall of Factual Associations in Auto-Regressive Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geva%2C+M">Mor Geva</a>, 
<a href="/search/cs?searchtype=author&query=Bastings%2C+J">Jasmijn Bastings</a>, 
<a href="/search/cs?searchtype=author&query=Filippova%2C+K">Katja Filippova</a>, 
<a href="/search/cs?searchtype=author&query=Globerson%2C+A">Amir Globerson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item867">[867]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01074" title="Abstract">arXiv:2305.01074</a> (replaced) [<a href="/pdf/2305.01074" title="Download PDF">pdf</a>, <a href="/format/2305.01074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical Adversarial Attacks for Surveillance: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Kien Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Fernando%2C+T">Tharindu Fernando</a>, 
<a href="/search/cs?searchtype=author&query=Fookes%2C+C">Clinton Fookes</a>, 
<a href="/search/cs?searchtype=author&query=Sridharan%2C+S">Sridha Sridharan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in T-NNLS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item868">[868]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01177" title="Abstract">arXiv:2305.01177</a> (replaced) [<a href="/pdf/2305.01177" title="Download PDF">pdf</a>, <a href="/format/2305.01177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbounded Differentially Private Quantile and Maximum Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Durfee%2C+D">David Durfee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item869">[869]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01356" title="Abstract">arXiv:2305.01356</a> (replaced) [<a href="/pdf/2305.01356" title="Download PDF">pdf</a>, <a href="/format/2305.01356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quadtree, a Steiner Spanner, and Approximate Nearest Neighbours in  Hyperbolic Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kisfaludi-Bak%2C+S">S&#xe1;ndor Kisfaludi-Bak</a>, 
<a href="/search/cs?searchtype=author&query=van+Wordragen%2C+G">Geert van Wordragen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Significantly expanded original paper with results for (Steiner) $(1+\epsilon)$-spanners and $(1+\epsilon)$-approximate nearest neighbours
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item870">[870]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01523" title="Abstract">arXiv:2305.01523</a> (replaced) [<a href="/pdf/2305.01523" title="Download PDF">pdf</a>, <a href="/format/2305.01523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Unified AI Drug Discovery with Multiple Knowledge Modalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yizhen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X+Y">Xing Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+M">Massimo Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiahuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yushuai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Z">Zaiqing Nie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item871">[871]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01572" title="Abstract">arXiv:2305.01572</a> (replaced) [<a href="/pdf/2305.01572" title="Download PDF">pdf</a>, <a href="/format/2305.01572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> H2CGL: Modeling Dynamics of Citation Network for Impact Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guoxiu He</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhikai Xue</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhuoren Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yangyang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Star Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IP&amp;M
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item872">[872]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01613" title="Abstract">arXiv:2305.01613</a> (replaced) [<a href="/pdf/2305.01613" title="Download PDF">pdf</a>, <a href="/format/2305.01613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity Framework for Forbidden Subgraphs IV: The Steiner Forest  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bodlaender%2C+H+L">Hans L. Bodlaender</a>, 
<a href="/search/math?searchtype=author&query=Johnson%2C+M">Matthew Johnson</a>, 
<a href="/search/math?searchtype=author&query=Martin%2C+B">Barnaby Martin</a>, 
<a href="/search/math?searchtype=author&query=Oostveen%2C+J+J">Jelle J. Oostveen</a>, 
<a href="/search/math?searchtype=author&query=Pandey%2C+S">Sukanya Pandey</a>, 
<a href="/search/math?searchtype=author&query=Paulusma%2C+D">Daniel Paulusma</a>, 
<a href="/search/math?searchtype=author&query=Smith%2C+S">Siani Smith</a>, 
<a href="/search/math?searchtype=author&query=van+Leeuwen%2C+E+J">Erik Jan van Leeuwen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item873">[873]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03868" title="Abstract">arXiv:2305.03868</a> (replaced) [<a href="/pdf/2305.03868" title="Download PDF">pdf</a>, <a href="/format/2305.03868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SE(3) Koopman-MPC: Data-driven Learning and Control of Quadrotor UAVs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+S+S+K+S">Sriram S. K. S. Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Tellez-Castro%2C+D">Duvan Tellez-Castro</a>, 
<a href="/search/cs?searchtype=author&query=Sutavani%2C+S">Sarang Sutavani</a>, 
<a href="/search/cs?searchtype=author&query=Vaidya%2C+U">Umesh Vaidya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item874">[874]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05900" title="Abstract">arXiv:2305.05900</a> (replaced) [<a href="/pdf/2305.05900" title="Download PDF">pdf</a>, <a href="/format/2305.05900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPMLBench: Holistic Evaluation of Differentially Private Machine  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chengkun Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Minghu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+W">Wenlong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yuan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenzhi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the ACM Conference on Computer and Communications Security (CCS), November 2023, Tivoli Congress Center, Copenhagen, Denmark
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item875">[875]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06331" title="Abstract">arXiv:2305.06331</a> (replaced) [<a href="/pdf/2305.06331" title="Download PDF">pdf</a>, <a href="/format/2305.06331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prior Global Search Stability on Finite Graphs with Uncertainty. May  Greedy Search Win?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ananev%2C+A">Andrey Ananev</a>, 
<a href="/search/cs?searchtype=author&query=Khlyupin%2C+A">Aleksey Khlyupin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item876">[876]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07991" title="Abstract">arXiv:2305.07991</a> (replaced) [<a href="/pdf/2305.07991" title="Download PDF">pdf</a>, <a href="/format/2305.07991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Previously Fact-Checked Claim Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pikuliak%2C+M">Mat&#xfa;&#x161; Pikuliak</a>, 
<a href="/search/cs?searchtype=author&query=Srba%2C+I">Ivan Srba</a>, 
<a href="/search/cs?searchtype=author&query=Moro%2C+R">Robert Moro</a>, 
<a href="/search/cs?searchtype=author&query=Hromadka%2C+T">Timo Hromadka</a>, 
<a href="/search/cs?searchtype=author&query=Smolen%2C+T">Timotej Smolen</a>, 
<a href="/search/cs?searchtype=author&query=Melisek%2C+M">Martin Melisek</a>, 
<a href="/search/cs?searchtype=author&query=Vykopal%2C+I">Ivan Vykopal</a>, 
<a href="/search/cs?searchtype=author&query=Simko%2C+J">Jakub Simko</a>, 
<a href="/search/cs?searchtype=author&query=Podrouzek%2C+J">Juraj Podrouzek</a>, 
<a href="/search/cs?searchtype=author&query=Bielikova%2C+M">Maria Bielikova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item877">[877]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08371" title="Abstract">arXiv:2305.08371</a> (replaced) [<a href="/pdf/2305.08371" title="Download PDF">pdf</a>, <a href="/format/2305.08371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SuperDialseg: A Large-scale Dataset for Supervised Dialogue Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junfeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chengzhang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Kurohashi%2C+S">Sadao Kurohashi</a>, 
<a href="/search/cs?searchtype=author&query=Aizawa%2C+A">Akiko Aizawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a Long Paper at EMNLP 2023 (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item878">[878]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09477" title="Abstract">arXiv:2305.09477</a> (replaced) [<a href="/pdf/2305.09477" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unjustified Sample Sizes and Generalizations in Explainable AI Research:  Principles for More Inclusive User Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peters%2C+U">Uwe Peters</a>, 
<a href="/search/cs?searchtype=author&query=Carman%2C+M">Mary Carman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 tables, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item879">[879]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09666" title="Abstract">arXiv:2305.09666</a> (replaced) [<a href="/pdf/2305.09666" title="Download PDF">pdf</a>, <a href="/format/2305.09666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AbdomenAtlas-8K: Annotating 8,000 CT Volumes for Multi-Organ  Segmentation in Three Weeks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qu%2C+C">Chongyu Qu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+T">Tiezheng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Qiao%2C+H">Hualin Qiao</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+Y">Yucheng Tang</a>, 
<a href="/search/eess?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Z">Zongwei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item880">[880]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10445" title="Abstract">arXiv:2305.10445</a> (replaced) [<a href="/pdf/2305.10445" title="Download PDF">pdf</a>, <a href="/format/2305.10445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memorization for Good: Encryption with Autoregressive Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stevens%2C+S">Samuel Stevens</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main text: 9 pages, 4 figures, 1 table. Work-in-progress. Project website at <a href="https://samuelstevens.me/research/encryption/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item881">[881]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10563" title="Abstract">arXiv:2305.10563</a> (replaced) [<a href="/pdf/2305.10563" title="Download PDF">pdf</a>, <a href="/format/2305.10563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HaSa: Hardness and Structure-Aware Contrastive Knowledge Graph Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Honggen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">June Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Molybog%2C+I">Igor Molybog</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item882">[882]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10968" title="Abstract">arXiv:2305.10968</a> (replaced) [<a href="/pdf/2305.10968" title="Download PDF">pdf</a>, <a href="/ps/2305.10968" title="Download PostScript">ps</a>, <a href="/format/2305.10968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On iterative methods based on Sherman-Morrison-Woodbury splitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mitsotakis%2C+D">Dimitrios Mitsotakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item883">[883]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11010" title="Abstract">arXiv:2305.11010</a> (replaced) [<a href="/pdf/2305.11010" title="Download PDF">pdf</a>, <a href="/format/2305.11010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithms for Finding Compatible Constraints in Receding-Horizon  Control of Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parwana%2C+H">Hardik Parwana</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruiyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Panagou%2C+D">Dimitra Panagou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item884">[884]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11283" title="Abstract">arXiv:2305.11283</a> (replaced) [<a href="/pdf/2305.11283" title="Download PDF">pdf</a>, <a href="/ps/2305.11283" title="Download PostScript">ps</a>, <a href="/format/2305.11283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Statistical Efficiency of Mean Field Reinforcement Learning with  General Function Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiawei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yardim%2C+B">Batuhan Yardim</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+N">Niao He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item885">[885]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11420" title="Abstract">arXiv:2305.11420</a> (replaced) [<a href="/pdf/2305.11420" title="Download PDF">pdf</a>, <a href="/format/2305.11420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Exponential Graph: Communication-Efficient Topologies for  Decentralized Learning via Finite-time Convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takezawa%2C+Y">Yuki Takezawa</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+R">Ryoma Sato</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Han Bao</a>, 
<a href="/search/cs?searchtype=author&query=Niwa%2C+K">Kenta Niwa</a>, 
<a href="/search/cs?searchtype=author&query=Yamada%2C+M">Makoto Yamada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item886">[886]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11490" title="Abstract">arXiv:2305.11490</a> (replaced) [<a href="/pdf/2305.11490" title="Download PDF">pdf</a>, <a href="/format/2305.11490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Suhyeon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W+J">Won Jun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item887">[887]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11541" title="Abstract">arXiv:2305.11541</a> (replaced) [<a href="/pdf/2305.11541" title="Download PDF">pdf</a>, <a href="/format/2305.11541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empower Large Language Model to Perform Better on Industrial  Domain-Specific Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fangkai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Pu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zezhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+M">Mohit Garg</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qingwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Rajmohan%2C+S">Saravan Rajmohan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item888">[888]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11651" title="Abstract">arXiv:2305.11651</a> (replaced) [<a href="/pdf/2305.11651" title="Download PDF">pdf</a>, <a href="/format/2305.11651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Channel Cycle Time: A New Measure of Short-term Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+P">Pengfei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yulin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Haoyuan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Eldar%2C+Y+C">Yonina C. Eldar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Multiagent Systems (cs.MA); Performance (cs.PF); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item889">[889]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11788" title="Abstract">arXiv:2305.11788</a> (replaced) [<a href="/pdf/2305.11788" title="Download PDF">pdf</a>, <a href="/format/2305.11788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Bias of Gradient Descent for Logistic Regression at the Edge of  Stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jingfeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Braverman%2C+V">Vladimir Braverman</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item890">[890]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11792" title="Abstract">arXiv:2305.11792</a> (replaced) [<a href="/pdf/2305.11792" title="Download PDF">pdf</a>, <a href="/format/2305.11792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue  Questions with LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zezhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Bin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item891">[891]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12074" title="Abstract">arXiv:2305.12074</a> (replaced) [<a href="/pdf/2305.12074" title="Download PDF">pdf</a>, <a href="/format/2305.12074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DisCo: Distilled Student Models Co-training for Semi-supervised Text  Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weifeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Q">Qianren Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+T">Ting Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Weiyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item892">[892]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12351" title="Abstract">arXiv:2305.12351</a> (replaced) [<a href="/pdf/2305.12351" title="Download PDF">pdf</a>, <a href="/format/2305.12351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Your Explanations Reliable? Investigating the Stability of LIME in  Explaining Text Classifiers by Marrying XAI and Adversarial Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burger%2C+C">Christopher Burger</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lingwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thai Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures. Replacement by the updated version to be published in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item893">[893]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12599" title="Abstract">arXiv:2305.12599</a> (replaced) [<a href="/pdf/2305.12599" title="Download PDF">pdf</a>, <a href="/format/2305.12599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Logical Reasoning of Large Language Models through  Logic-Driven Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+Q">Qiming Bao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+A+Y">Alex Yuxuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhenyun Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Gendron%2C+G">Gael Gendron</a>, 
<a href="/search/cs?searchtype=author&query=Pistotti%2C+T">Timothy Pistotti</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+N">Neset Tan</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+N">Nathan Young</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yonghua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Denny%2C+P">Paul Denny</a>, 
<a href="/search/cs?searchtype=author&query=Witbrock%2C+M">Michael Witbrock</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiamou Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for oral presentation at the LLM@IJCAI 2023 non-archival symposium
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item894">[894]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12767" title="Abstract">arXiv:2305.12767</a> (replaced) [<a href="/pdf/2305.12767" title="Download PDF">pdf</a>, <a href="/format/2305.12767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D$^2$TV: Dual Knowledge Distillation and Target-oriented Vision Modeling  for Many-to-Many Multimodal Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yunlong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jinan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yufeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item895">[895]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12809" title="Abstract">arXiv:2305.12809</a> (replaced) [<a href="/pdf/2305.12809" title="Download PDF">pdf</a>, <a href="/format/2305.12809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relabeling Minimal Training Subset to Flip a Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinghan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Linjie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lequan Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item896">[896]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12870" title="Abstract">arXiv:2305.12870</a> (replaced) [<a href="/pdf/2305.12870" title="Download PDF">pdf</a>, <a href="/format/2305.12870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lion: Adversarial Distillation of Proprietary Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuxin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+C">Chunkit Chan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures, EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item897">[897]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12872" title="Abstract">arXiv:2305.12872</a> (replaced) [<a href="/pdf/2305.12872" title="Download PDF">pdf</a>, <a href="/format/2305.12872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a  Bayesian Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Simin Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xiu%2C+J">Jingqiao Xiu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruixiao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiakai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aishan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item898">[898]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12883" title="Abstract">arXiv:2305.12883</a> (replaced) [<a href="/pdf/2305.12883" title="Download PDF">pdf</a>, <a href="/format/2305.12883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction Risk and Estimation Risk of the Ridgeless Least Squares  Estimator under General Assumptions on Regression Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lee%2C+S">Sungyoon Lee</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+S">Sokbae Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item899">[899]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13499" title="Abstract">arXiv:2305.13499</a> (replaced) [<a href="/pdf/2305.13499" title="Download PDF">pdf</a>, <a href="/format/2305.13499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Easily Updated General Purpose Text Representations with  Adaptable Task-Specific Prefixes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kuan-Hao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+L">Liang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+R">Rui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sinong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Almahairi%2C+A">Amjad Almahairi</a>, 
<a href="/search/cs?searchtype=author&query=Rinott%2C+R">Ruty Rinott</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item900">[900]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13517" title="Abstract">arXiv:2305.13517</a> (replaced) [<a href="/pdf/2305.13517" title="Download PDF">pdf</a>, <a href="/format/2305.13517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Guarantees of Group-Invariant GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+Z">Ziyu Chen</a>, 
<a href="/search/stat?searchtype=author&query=Katsoulakis%2C+M+A">Markos A. Katsoulakis</a>, 
<a href="/search/stat?searchtype=author&query=Rey-Bellet%2C+L">Luc Rey-Bellet</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+W">Wei Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item901">[901]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13626" title="Abstract">arXiv:2305.13626</a> (replaced) [<a href="/pdf/2305.13626" title="Download PDF">pdf</a>, <a href="/format/2305.13626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting and Evaluating Large Language Models for Proactive Dialogues:  Clarification, Target-guided, and Non-collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+L">Lizi Liao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+W">Wenqiang Lei</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item902">[902]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13954" title="Abstract">arXiv:2305.13954</a> (replaced) [<a href="/pdf/2305.13954" title="Download PDF">pdf</a>, <a href="/format/2305.13954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Prompt Optimization for Large Language Models Against  Distribution Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Moxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jizhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item903">[903]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14045" title="Abstract">arXiv:2305.14045</a> (replaced) [<a href="/pdf/2305.14045" title="Download PDF">pdf</a>, <a href="/format/2305.14045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The CoT Collection: Improving Zero-shot and Few-shot Learning of  Language Models via Chain-of-Thought Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungone Kim</a>, 
<a href="/search/cs?searchtype=author&query=Joo%2C+S+J">Se June Joo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Doyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Joel Jang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Seonghyeon Ye</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jamin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item904">[904]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14100" title="Abstract">arXiv:2305.14100</a> (replaced) [<a href="/pdf/2305.14100" title="Download PDF">pdf</a>, <a href="/format/2305.14100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ISP: Multi-Layered Garment Draping with Implicit Sewing Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ren Li</a>, 
<a href="/search/cs?searchtype=author&query=Guillard%2C+B">Beno&#xee;t Guillard</a>, 
<a href="/search/cs?searchtype=author&query=Fua%2C+P">Pascal Fua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item905">[905]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14196" title="Abstract">arXiv:2305.14196</a> (replaced) [<a href="/pdf/2305.14196" title="Download PDF">pdf</a>, <a href="/format/2305.14196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaham%2C+U">Uri Shaham</a>, 
<a href="/search/cs?searchtype=author&query=Ivgi%2C+M">Maor Ivgi</a>, 
<a href="/search/cs?searchtype=author&query=Efrat%2C+A">Avia Efrat</a>, 
<a href="/search/cs?searchtype=author&query=Berant%2C+J">Jonathan Berant</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+O">Omer Levy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item906">[906]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14203" title="Abstract">arXiv:2305.14203</a> (replaced) [<a href="/pdf/2305.14203" title="Download PDF">pdf</a>, <a href="/format/2305.14203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the Gap in Visual Speech Recognition Between Normal and Silent  Speech Based on Metric Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kashiwagi%2C+S">Sara Kashiwagi</a>, 
<a href="/search/eess?searchtype=author&query=Tanaka%2C+K">Keitaro Tanaka</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+Q">Qi Feng</a>, 
<a href="/search/eess?searchtype=author&query=Morishima%2C+S">Shigeo Morishima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by INTERSPEECH 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item907">[907]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14275" title="Abstract">arXiv:2305.14275</a> (replaced) [<a href="/pdf/2305.14275" title="Download PDF">pdf</a>, <a href="/format/2305.14275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amortized Variational Inference with Coverage Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Patel%2C+Y">Yash Patel</a>, 
<a href="/search/stat?searchtype=author&query=McNamara%2C+D">Declan McNamara</a>, 
<a href="/search/stat?searchtype=author&query=Loper%2C+J">Jackson Loper</a>, 
<a href="/search/stat?searchtype=author&query=Regier%2C+J">Jeffrey Regier</a>, 
<a href="/search/stat?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item908">[908]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14302" title="Abstract">arXiv:2305.14302</a> (replaced) [<a href="/pdf/2305.14302" title="Download PDF">pdf</a>, <a href="/format/2305.14302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VIP5: Towards Multimodal Foundation Models for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+S">Shijie Geng</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Juntao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zuohui Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item909">[909]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14380" title="Abstract">arXiv:2305.14380</a> (replaced) [<a href="/pdf/2305.14380" title="Download PDF">pdf</a>, <a href="/format/2305.14380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding the Pillars of Strength for Multi-Head Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jinjie Ni</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+R">Rui Mao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonglin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+H">Han Lei</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item910">[910]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14480" title="Abstract">arXiv:2305.14480</a> (replaced) [<a href="/pdf/2305.14480" title="Download PDF">pdf</a>, <a href="/format/2305.14480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BAND: Biomedical Alert News Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zihao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Meiru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zaiqiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yannan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Buckeridge%2C+D">David Buckeridge</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item911">[911]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14838" title="Abstract">arXiv:2305.14838</a> (replaced) [<a href="/pdf/2305.14838" title="Download PDF">pdf</a>, <a href="/format/2305.14838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+C">Chenyang Le</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yao Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Long Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shujie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yanmin Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+M">Michael Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuedong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, Poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item912">[912]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14874" title="Abstract">arXiv:2305.14874</a> (replaced) [<a href="/pdf/2305.14874" title="Download PDF">pdf</a>, <a href="/format/2305.14874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Words to Wires: Generating Functioning Electronic Devices from  Natural Language Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jansen%2C+P">Peter Jansen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item913">[913]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14950" title="Abstract">arXiv:2305.14950</a> (replaced) [<a href="/pdf/2305.14950" title="Download PDF">pdf</a>, <a href="/format/2305.14950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Demonstration Attacks on Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiongxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zichen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K+H">Keun Hee Park</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhuojun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhaoheng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhuofeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item914">[914]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15015" title="Abstract">arXiv:2305.15015</a> (replaced) [<a href="/pdf/2305.15015" title="Download PDF">pdf</a>, <a href="/format/2305.15015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Faithful and Plausible Visual Grounding in VQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reich%2C+D">Daniel Reich</a>, 
<a href="/search/cs?searchtype=author&query=Putze%2C+F">Felix Putze</a>, 
<a href="/search/cs?searchtype=author&query=Schultz%2C+T">Tanja Schultz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item915">[915]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15062" title="Abstract">arXiv:2305.15062</a> (replaced) [<a href="/pdf/2305.15062" title="Download PDF">pdf</a>, <a href="/format/2305.15062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lawyer LLaMA Technical Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Quzhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Mingxu Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+Z">Zhenwei An</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Cong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhibin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zirui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yansong Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item916">[916]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15171" title="Abstract">arXiv:2305.15171</a> (replaced) [<a href="/pdf/2305.15171" title="Download PDF">pdf</a>, <a href="/format/2305.15171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations  from Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinhang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaben Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+S">Shiu-hong Kao</a>, 
<a href="/search/cs?searchtype=author&query=Tai%2C+Y">Yu-Wing Tai</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chi-Keung Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item917">[917]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16317" title="Abstract">arXiv:2305.16317</a> (replaced) [<a href="/pdf/2305.16317" title="Download PDF">pdf</a>, <a href="/format/2305.16317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel Sampling of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shih%2C+A">Andy Shih</a>, 
<a href="/search/cs?searchtype=author&query=Belkhale%2C+S">Suneel Belkhale</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>, 
<a href="/search/cs?searchtype=author&query=Anari%2C+N">Nima Anari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item918">[918]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16988" title="Abstract">arXiv:2305.16988</a> (replaced) [<a href="/pdf/2305.16988" title="Download PDF">pdf</a>, <a href="/format/2305.16988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharp Bounds for Generalized Causal Sensitivity Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frauen%2C+D">Dennis Frauen</a>, 
<a href="/search/cs?searchtype=author&query=Melnychuk%2C+V">Valentyn Melnychuk</a>, 
<a href="/search/cs?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item919">[919]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17187" title="Abstract">arXiv:2305.17187</a> (replaced) [<a href="/pdf/2305.17187" title="Download PDF">pdf</a>, <a href="/format/2305.17187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clip-OGD: An Experimental Design for Adaptive Neyman Allocation in  Sequential Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dai%2C+J">Jessica Dai</a>, 
<a href="/search/stat?searchtype=author&query=Gradu%2C+P">Paula Gradu</a>, 
<a href="/search/stat?searchtype=author&query=Harshaw%2C+C">Christopher Harshaw</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item920">[920]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17220" title="Abstract">arXiv:2305.17220</a> (replaced) [<a href="/pdf/2305.17220" title="Download PDF">pdf</a>, <a href="/format/2305.17220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VoxDet: Voxel Learning for Novel Instance Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiashun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yaoyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Scherer%2C+S">Sebastian Scherer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item921">[921]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17484" title="Abstract">arXiv:2305.17484</a> (replaced) [<a href="/pdf/2305.17484" title="Download PDF">pdf</a>, <a href="/format/2305.17484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Keep it Upright: Model Predictive Control for Nonprehensile Object  Transportation with Obstacle Avoidance on a Mobile Manipulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heins%2C+A">Adam Heins</a>, 
<a href="/search/cs?searchtype=author&query=Schoellig%2C+A+P">Angela P. Schoellig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 14 figures; longer version of an article published in IEEE Robotics and Automation Letters (RA-L)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item922">[922]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17862" title="Abstract">arXiv:2305.17862</a> (replaced) [<a href="/pdf/2305.17862" title="Download PDF">pdf</a>, <a href="/format/2305.17862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Silent eHMI Enough? A Passenger-Centric Study on Effective eHMI for  Autonomous Personal Mobility Vehicles in the Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hailong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhe Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Chen Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wada%2C+T">Takahiro Wada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item923">[923]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19428" title="Abstract">arXiv:2305.19428</a> (replaced) [<a href="/pdf/2305.19428" title="Download PDF">pdf</a>, <a href="/format/2305.19428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating geospatial context information for travel mode detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Hong%2C+Y">Ye Hong</a>, 
<a href="/search/physics?searchtype=author&query=St%C3%BCdeli%2C+E">Emanuel St&#xfc;deli</a>, 
<a href="/search/physics?searchtype=author&query=Raubal%2C+M">Martin Raubal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated Method and Discussion; accepted by Journal of Transport Geography
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item924">[924]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00038" title="Abstract">arXiv:2306.00038</a> (replaced) [<a href="/pdf/2306.00038" title="Download PDF">pdf</a>, <a href="/format/2306.00038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedCSD: A Federated Learning Based Approach for Code-Smell Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alawadi%2C+S">Sadi Alawadi</a>, 
<a href="/search/cs?searchtype=author&query=Alkharabsheh%2C+K">Khalid Alkharabsheh</a>, 
<a href="/search/cs?searchtype=author&query=Alkhabbas%2C+F">Fahed Alkhabbas</a>, 
<a href="/search/cs?searchtype=author&query=Kebande%2C+V">Victor Kebande</a>, 
<a href="/search/cs?searchtype=author&query=Awaysheh%2C+F+M">Feras M. Awaysheh</a>, 
<a href="/search/cs?searchtype=author&query=Palomba%2C+F">Fabio Palomba</a>, 
<a href="/search/cs?searchtype=author&query=Awad%2C+M">Mohammed Awad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures, Journal paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item925">[925]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00742" title="Abstract">arXiv:2306.00742</a> (replaced) [<a href="/pdf/2306.00742" title="Download PDF">pdf</a>, <a href="/format/2306.00742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Going Deeper with Spectral Decompositions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabannes%2C+V">Vivien Cabannes</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+F">Francis Bach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item926">[926]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00893" title="Abstract">arXiv:2306.00893</a> (replaced) [<a href="/pdf/2306.00893" title="Download PDF">pdf</a>, <a href="/format/2306.00893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Temporal Butterfly Counting and Enumeration on Temporal  Bipartite Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xinwei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+X">Xiangyu Ke</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yunjun Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item927">[927]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00980" title="Abstract">arXiv:2306.00980</a> (replaced) [<a href="/pdf/2306.00980" title="Download PDF">pdf</a>, <a href="/format/2306.00980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two  Seconds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qing Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Ju Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chemerys%2C+P">Pavlo Chemerys</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yun Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tulyakov%2C+S">Sergey Tulyakov</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jian Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our project webpage: <a href="https://snap-research.github.io/SnapFusion/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item928">[928]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01323" title="Abstract">arXiv:2306.01323</a> (replaced) [<a href="/pdf/2306.01323" title="Download PDF">pdf</a>, <a href="/format/2306.01323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Structural Disparity in Graph Neural Networks: Can One Size  Fit All?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhikai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+H">Haoyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages, 24 figures. arXiv admin note: text overlap with <a href="/abs/2106.15535">arXiv:2106.15535</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item929">[929]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02391" title="Abstract">arXiv:2306.02391</a> (replaced) [<a href="/pdf/2306.02391" title="Download PDF">pdf</a>, <a href="/ps/2306.02391" title="Download PostScript">ps</a>, <a href="/format/2306.02391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overlap Splines and Meshless Finite Difference Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Davydov%2C+O">Oleg Davydov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item930">[930]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02558" title="Abstract">arXiv:2306.02558</a> (replaced) [<a href="/pdf/2306.02558" title="Download PDF">pdf</a>, <a href="/format/2306.02558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-View Representation is What You Need for Point-Cloud Pre-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Siming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chen Song</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Y">Youkang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qixing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item931">[931]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03310" title="Abstract">arXiv:2306.03310</a> (replaced) [<a href="/pdf/2306.03310" title="Download PDF">pdf</a>, <a href="/format/2306.03310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chongkai Gao</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yihao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item932">[932]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03812" title="Abstract">arXiv:2306.03812</a> (replaced) [<a href="/pdf/2306.03812" title="Download PDF">pdf</a>, <a href="/format/2306.03812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computation with Sequences in a Model of the Brain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dabagia%2C+M">Max Dabagia</a>, 
<a href="/search/cs?searchtype=author&query=Papadimitriou%2C+C+H">Christos H. Papadimitriou</a>, 
<a href="/search/cs?searchtype=author&query=Vempala%2C+S+S">Santosh S. Vempala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item933">[933]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04324" title="Abstract">arXiv:2306.04324</a> (replaced) [<a href="/pdf/2306.04324" title="Download PDF">pdf</a>, <a href="/format/2306.04324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GCT-TTE: Graph Convolutional Transformer for Travel Time Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mashurov%2C+V">Vladimir Mashurov</a>, 
<a href="/search/cs?searchtype=author&query=Chopurian%2C+V">Vaagn Chopurian</a>, 
<a href="/search/cs?searchtype=author&query=Porvatov%2C+V">Vadim Porvatov</a>, 
<a href="/search/cs?searchtype=author&query=Ivanov%2C+A">Arseny Ivanov</a>, 
<a href="/search/cs?searchtype=author&query=Semenova%2C+N">Natalia Semenova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures, 4 tables; supplementary included; accepted in Journal of Big Data
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item934">[934]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04488" title="Abstract">arXiv:2306.04488</a> (replaced) [<a href="/pdf/2306.04488" title="Download PDF">pdf</a>, <a href="/format/2306.04488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rewarded soups: towards Pareto-optimal alignment by interpolating  weights fine-tuned on diverse rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ram%C3%A9%2C+A">Alexandre Ram&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Couairon%2C+G">Guillaume Couairon</a>, 
<a href="/search/cs?searchtype=author&query=Shukor%2C+M">Mustafa Shukor</a>, 
<a href="/search/cs?searchtype=author&query=Dancette%2C+C">Corentin Dancette</a>, 
<a href="/search/cs?searchtype=author&query=Gaya%2C+J">Jean-Baptiste Gaya</a>, 
<a href="/search/cs?searchtype=author&query=Soulier%2C+L">Laure Soulier</a>, 
<a href="/search/cs?searchtype=author&query=Cord%2C+M">Matthieu Cord</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item935">[935]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05685" title="Abstract">arXiv:2306.05685</a> (replaced) [<a href="/pdf/2306.05685" title="Download PDF">pdf</a>, <a href="/format/2306.05685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lianmin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+W">Wei-Lin Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Y">Ying Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+S">Siyuan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhanghao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yonghao Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuohan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dacheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E+P">Eric. P Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+E">Joseph E. Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Stoica%2C+I">Ion Stoica</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item936">[936]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06335" title="Abstract">arXiv:2306.06335</a> (replaced) [<a href="/pdf/2306.06335" title="Download PDF">pdf</a>, <a href="/format/2306.06335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Learn and Generalize From Three Minutes of Data:  Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Djeumou%2C+F">Franck Djeumou</a>, 
<a href="/search/cs?searchtype=author&query=Neary%2C+C">Cyrus Neary</a>, 
<a href="/search/cs?searchtype=author&query=Topcu%2C+U">Ufuk Topcu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final submission to CoRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item937">[937]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07967" title="Abstract">arXiv:2306.07967</a> (replaced) [<a href="/pdf/2306.07967" title="Download PDF">pdf</a>, <a href="/format/2306.07967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chavan%2C+A">Arnav Chavan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Deepak Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqiang Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report. v2: Add LLaMA-1&amp;2 results. Code and models at <a href="https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item938">[938]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08014" title="Abstract">arXiv:2306.08014</a> (replaced) [<a href="/pdf/2306.08014" title="Download PDF">pdf</a>, <a href="/ps/2306.08014" title="Download PostScript">ps</a>, <a href="/format/2306.08014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Realising Synthetic Active Inference Agents, Part I: Epistemic  Objectives and Graphical Specification Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koudahl%2C+M">Magnus Koudahl</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Laar%2C+T">Thijs van de Laar</a>, 
<a href="/search/cs?searchtype=author&query=de+Vries%2C+B">Bert de Vries</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 pages, 31 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item939">[939]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08052" title="Abstract">arXiv:2306.08052</a> (replaced) [<a href="/pdf/2306.08052" title="Download PDF">pdf</a>, <a href="/ps/2306.08052" title="Download PostScript">ps</a>, <a href="/format/2306.08052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On coloring parameters of triangle-free planar $(n,m)$-graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nandi%2C+S">Soumen Nandi</a>, 
<a href="/search/math?searchtype=author&query=Sen%2C+S">Sagnik Sen</a>, 
<a href="/search/math?searchtype=author&query=Taruni%2C+S">S Taruni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 Pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item940">[940]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09112" title="Abstract">arXiv:2306.09112</a> (replaced) [<a href="/pdf/2306.09112" title="Download PDF">pdf</a>, <a href="/format/2306.09112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Certified Generalization in Structured Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Boll%2C+B">Bastian Boll</a>, 
<a href="/search/stat?searchtype=author&query=Schn%C3%B6rr%2C+C">Christoph Schn&#xf6;rr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item941">[941]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09124" title="Abstract">arXiv:2306.09124</a> (replaced) [<a href="/pdf/2306.09124" title="Download PDF">pdf</a>, <a href="/format/2306.09124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+C">Caixin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yinpeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+S">Shouwei Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yubo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xingxing Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item942">[942]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09597" title="Abstract">arXiv:2306.09597</a> (replaced) [<a href="/pdf/2306.09597" title="Download PDF">pdf</a>, <a href="/format/2306.09597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clickbait Detection via Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yun Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yunhao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Qiang%2C+J">Jipeng Qiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item943">[943]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10792" title="Abstract">arXiv:2306.10792</a> (replaced) [<a href="/pdf/2306.10792" title="Download PDF">pdf</a>, <a href="/format/2306.10792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NAR-Former V2: Rethinking Transformer for Universal Neural Network  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+Y">Yun Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haokui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+R">Rong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nannan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 6 tables. Code is available at <a href="https://github.com/yuny220/NAR-Former-V2">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item944">[944]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11078" title="Abstract">arXiv:2306.11078</a> (replaced) [<a href="/pdf/2306.11078" title="Download PDF">pdf</a>, <a href="/format/2306.11078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Normal: On the Evaluation of Mutual Information Estimators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Czy%C5%BC%2C+P">Pawe&#x142; Czy&#x17c;</a>, 
<a href="/search/stat?searchtype=author&query=Grabowski%2C+F">Frederic Grabowski</a>, 
<a href="/search/stat?searchtype=author&query=Vogt%2C+J+E">Julia E. Vogt</a>, 
<a href="/search/stat?searchtype=author&query=Beerenwinkel%2C+N">Niko Beerenwinkel</a>, 
<a href="/search/stat?searchtype=author&query=Marx%2C+A">Alexander Marx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023. Code available at <a href="https://github.com/cbg-ethz/bmi">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item945">[945]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11626" title="Abstract">arXiv:2306.11626</a> (replaced) [<a href="/pdf/2306.11626" title="Download PDF">pdf</a>, <a href="/format/2306.11626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy  Gradient, and Sample Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+R">Runyu Zhang</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+Y">Yang Hu</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+N">Na Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item946">[946]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11768" title="Abstract">arXiv:2306.11768</a> (replaced) [<a href="/pdf/2306.11768" title="Download PDF">pdf</a>, <a href="/format/2306.11768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Survey in Geometric Deep Learning for Structure-based Drug  Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Z">Zaixi Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Yan%2C+J">Jiaxian Yan</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+E">Enhong Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Zitnik%2C+M">Marinka Zitnik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item947">[947]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13047" title="Abstract">arXiv:2306.13047</a> (replaced) [<a href="/pdf/2306.13047" title="Download PDF">pdf</a>, <a href="/format/2306.13047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of the Cambridge Multiple-Choice Questions Reading Dataset with  a Focus on Candidate Response Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liusie%2C+A">Adian Liusie</a>, 
<a href="/search/cs?searchtype=author&query=Raina%2C+V">Vatsal Raina</a>, 
<a href="/search/cs?searchtype=author&query=Mullooly%2C+A">Andrew Mullooly</a>, 
<a href="/search/cs?searchtype=author&query=Knill%2C+K">Kate Knill</a>, 
<a href="/search/cs?searchtype=author&query=Gales%2C+M+J+F">Mark J. F. Gales</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item948">[948]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13412" title="Abstract">arXiv:2306.13412</a> (replaced) [<a href="/pdf/2306.13412" title="Download PDF">pdf</a>, <a href="/format/2306.13412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zu%2C+L">Lipeng Zu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Li He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Donglin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item949">[949]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14268" title="Abstract">arXiv:2306.14268</a> (replaced) [<a href="/pdf/2306.14268" title="Download PDF">pdf</a>, <a href="/format/2306.14268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Window Pruning for Efficient Local Motion Deblurring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoying Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jixin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shangchen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Huajun Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chongyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Loy%2C+C+C">Chen Change Loy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item950">[950]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14812" title="Abstract">arXiv:2306.14812</a> (replaced) [<a href="/pdf/2306.14812" title="Download PDF">pdf</a>, <a href="/format/2306.14812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOVES: Movable and Moving LiDAR Scene Segmentation in Label-Free  settings using Static Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Prashant Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Makwana%2C+D">Dhruv Makwana</a>, 
<a href="/search/cs?searchtype=author&query=Susladkar%2C+O">Onkar Susladkar</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+A">Anurag Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Kalra%2C+P+K">Prem Kumar Kalra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 8 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item951">[951]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15830" title="Abstract">arXiv:2306.15830</a> (replaced) [<a href="/pdf/2306.15830" title="Download PDF">pdf</a>, <a href="/format/2306.15830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Navigation using Density Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+A">Andrew Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+S+S+K+S">Sriram S.K.S. Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Vaidya%2C+U">Umesh Vaidya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item952">[952]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16913" title="Abstract">arXiv:2306.16913</a> (replaced) [<a href="/pdf/2306.16913" title="Download PDF">pdf</a>, <a href="/format/2306.16913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoML in Heavily Constrained Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neutatz%2C+F">Felix Neutatz</a>, 
<a href="/search/cs?searchtype=author&query=Lindauer%2C+M">Marius Lindauer</a>, 
<a href="/search/cs?searchtype=author&query=Abedjan%2C+Z">Ziawasch Abedjan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item953">[953]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17473" title="Abstract">arXiv:2306.17473</a> (replaced) [<a href="/pdf/2306.17473" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An updated Orbital Solution for WASP-12 b: Updated Ephemeris and  Evidence for Decay leveraging Citizen Science Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Nediyedath%2C+A+S">Avinash S. Nediyedath</a>, 
<a href="/search/astro-ph?searchtype=author&query=Maidur%2C+S+R">Shivaraj R. Maidur</a>, 
<a href="/search/astro-ph?searchtype=author&query=Fowler%2C+M+J">Martin J. Fowler</a>, 
<a href="/search/astro-ph?searchtype=author&query=Davis%2C+K">K. Davis</a>, 
<a href="/search/astro-ph?searchtype=author&query=Das%2C+P">P. Das</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lalla%2C+D">D. Lalla</a>, 
<a href="/search/astro-ph?searchtype=author&query=Martin%2C+B+E">Bryan E. Martin</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dixon%2C+S">S. Dixon</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lewin%2C+P">P. Lewin</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kovacs%2C+A+O">Andre O. Kovacs</a>, 
<a href="/search/astro-ph?searchtype=author&query=Odasso%2C+A">A. Odasso</a>, 
<a href="/search/astro-ph?searchtype=author&query=Primm%2C+M">M. Primm</a>, 
<a href="/search/astro-ph?searchtype=author&query=Norris%2C+A">A. Norris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Computers and Society (cs.CY); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item954">[954]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00654" title="Abstract">arXiv:2307.00654</a> (replaced) [<a href="/pdf/2307.00654" title="Download PDF">pdf</a>, <a href="/format/2307.00654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Looks Can Be Deceiving: Linking User-Item Interactions and User&#x27;s  Propensity Towards Multi-Objective Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dokoupil%2C+P">Patrik Dokoupil</a>, 
<a href="/search/cs?searchtype=author&query=Peska%2C+L">Ladislav Peska</a>, 
<a href="/search/cs?searchtype=author&query=Boratto%2C+L">Ludovico Boratto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a short paper at ACM RecSys 2023 conference. See <a href="https://doi.org/10.1145/3604915.3608848">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item955">[955]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00743" title="Abstract">arXiv:2307.00743</a> (replaced) [<a href="/pdf/2307.00743" title="Download PDF">pdf</a>, <a href="/ps/2307.00743" title="Download PostScript">ps</a>, <a href="/format/2307.00743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Power Allocation and Beamforming for Active IRS-aided Directional  Modulation Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+R">Rongen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+F">Feng Shu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongzhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yongpeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiangzhou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item956">[956]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00799" title="Abstract">arXiv:2307.00799</a> (replaced) [<a href="/pdf/2307.00799" title="Download PDF">pdf</a>, <a href="/ps/2307.00799" title="Download PostScript">ps</a>, <a href="/format/2307.00799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> First Steps Towards a Runtime Analysis of Neuroevolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fischer%2C+P">Paul Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Larsen%2C+E+L">Emil Lundt Larsen</a>, 
<a href="/search/cs?searchtype=author&query=Witt%2C+C">Carsten Witt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages; full version of paper published at FOGA 2023 and available at ACM
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> FOGA 23, 2023, 61-72
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item957">[957]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02131" title="Abstract">arXiv:2307.02131</a> (replaced) [<a href="/pdf/2307.02131" title="Download PDF">pdf</a>, <a href="/format/2307.02131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Known Reality: Exploiting Counterfactual Explanations for Medical  Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tanyel%2C+T">Toygar Tanyel</a>, 
<a href="/search/cs?searchtype=author&query=Ayvaz%2C+S">Serkan Ayvaz</a>, 
<a href="/search/cs?searchtype=author&query=Keserci%2C+B">Bilgin Keserci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item958">[958]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03334" title="Abstract">arXiv:2307.03334</a> (replaced) [<a href="/pdf/2307.03334" title="Download PDF">pdf</a>, <a href="/format/2307.03334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational quantum regression algorithm with encoded data structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+C+-+J">C.-C. Joseph Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bennink%2C+R+S">Ryan S. Bennink</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item959">[959]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03452" title="Abstract">arXiv:2307.03452</a> (replaced) [<a href="/pdf/2307.03452" title="Download PDF">pdf</a>, <a href="/ps/2307.03452" title="Download PostScript">ps</a>, <a href="/format/2307.03452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-performance evaluation of high angular momentum 4-center Gaussian  integrals on modern accelerated processors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Asadchev%2C+A">Andrey Asadchev</a>, 
<a href="/search/physics?searchtype=author&query=Valeev%2C+E+F">Edward F. Valeev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Computational Engineering, Finance, and Science (cs.CE); Chemical Physics (physics.chem-ph)

</div>
</div>
</dd>
<dt><a name="item960">[960]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04013" title="Abstract">arXiv:2307.04013</a> (replaced) [<a href="/pdf/2307.04013" title="Download PDF">pdf</a>, <a href="/format/2307.04013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BPNet: B&#xe9;zier Primitive Segmentation on 3D Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+R">Rao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Cheng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xiao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Alliez%2C+P">Pierre Alliez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item961">[961]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06422" title="Abstract">arXiv:2307.06422</a> (replaced) [<a href="/pdf/2307.06422" title="Download PDF">pdf</a>, <a href="/format/2307.06422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Decoupled Graph Convolutions for Multigranular  Topology Protection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chien%2C+E">Eli Chien</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei-Ning Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chao Pan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96zg%C3%BCr%2C+A">Ayfer &#xd6;zg&#xfc;r</a>, 
<a href="/search/cs?searchtype=author&query=Milenkovic%2C+O">Olgica Milenkovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item962">[962]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06527" title="Abstract">arXiv:2307.06527</a> (replaced) [<a href="/pdf/2307.06527" title="Download PDF">pdf</a>, <a href="/format/2307.06527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Free-Form Composition Networks for Egocentric Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qinghua Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Baosheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Y">Yibing Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dapeng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Haibin Ling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item963">[963]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06840" title="Abstract">arXiv:2307.06840</a> (replaced) [<a href="/pdf/2307.06840" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble learning for blending gridded satellite and gauge-measured  precipitation data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papacharalampous%2C+G">Georgia Papacharalampous</a>, 
<a href="/search/cs?searchtype=author&query=Tyralis%2C+H">Hristos Tyralis</a>, 
<a href="/search/cs?searchtype=author&query=Doulamis%2C+N">Nikolaos Doulamis</a>, 
<a href="/search/cs?searchtype=author&query=Doulamis%2C+A">Anastasios Doulamis</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Remote Sensing 15 (2023) 4912
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph); Applications (stat.AP); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item964">[964]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10837" title="Abstract">arXiv:2307.10837</a> (replaced) [<a href="/pdf/2307.10837" title="Download PDF">pdf</a>, <a href="/format/2307.10837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensing User&#x27;s Activity, Channel, and Location with Near-Field  Extra-Large-Scale MIMO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+L">Li Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+A">Anwen Liao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+P">Pei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+L">Li You</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+D+W+K">Derrick Wing Kwan Ng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE Transactions on Communications. Codes will be open to all on <a href="https://gaozhen16.github.io/">this https URL</a> soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item965">[965]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12510" title="Abstract">arXiv:2307.12510</a> (replaced) [<a href="/pdf/2307.12510" title="Download PDF">pdf</a>, <a href="/ps/2307.12510" title="Download PostScript">ps</a>, <a href="/format/2307.12510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Evaluation of Temporal Graph Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Le Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in progress, more results are added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item966">[966]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12750" title="Abstract">arXiv:2307.12750</a> (replaced) [<a href="/pdf/2307.12750" title="Download PDF">pdf</a>, <a href="/format/2307.12750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DawnIK: Decentralized Collision-Aware Inverse Kinematics Solver for  Heterogeneous Multi-Arm Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marangoz%2C+S">Salih Marangoz</a>, 
<a href="/search/cs?searchtype=author&query=Menon%2C+R">Rohit Menon</a>, 
<a href="/search/cs?searchtype=author&query=Dengler%2C+N">Nils Dengler</a>, 
<a href="/search/cs?searchtype=author&query=Bennewitz%2C+M">Maren Bennewitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Salih Marangoz and Rohit Menon have equal authorship
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item967">[967]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12906" title="Abstract">arXiv:2307.12906</a> (replaced) [<a href="/pdf/2307.12906" title="Download PDF">pdf</a>, <a href="/format/2307.12906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction  Using Interpretable Hybrid Quantum-Classical Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jahin%2C+M+A">Md Abrar Jahin</a>, 
<a href="/search/cs?searchtype=author&query=Shovon%2C+M+S+H">Md Sakib Hossain Shovon</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+S">Md. Saiful Islam</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jungpil Shin</a>, 
<a href="/search/cs?searchtype=author&query=Mridha%2C+M+F">M. F. Mridha</a>, 
<a href="/search/cs?searchtype=author&query=Okuyama%2C+Y">Yuichi Okuyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item968">[968]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13345" title="Abstract">arXiv:2307.13345</a> (replaced) [<a href="/pdf/2307.13345" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do humans and Convolutional Neural Networks attend to similar areas  during scene classification: Effects of task and image type
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+R">Romy M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%BCrschmidt%2C+M">Marcel D&#xfc;rschmidt</a>, 
<a href="/search/cs?searchtype=author&query=Ullrich%2C+J">Julian Ullrich</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+C">Carsten Knoll</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+S">Sascha Weber</a>, 
<a href="/search/cs?searchtype=author&query=Seitz%2C+S">Steffen Seitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item969">[969]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13653" title="Abstract">arXiv:2307.13653</a> (replaced) [<a href="/pdf/2307.13653" title="Download PDF">pdf</a>, <a href="/ps/2307.13653" title="Download PostScript">ps</a>, <a href="/format/2307.13653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A threshold dislocation dynamics method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Qin%2C+X">Xiaoxue Qin</a>, 
<a href="/search/math?searchtype=author&query=Ngan%2C+A+H+W">Alfonso H.W. Ngan</a>, 
<a href="/search/math?searchtype=author&query=Xiang%2C+Y">Yang Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item970">[970]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13842" title="Abstract">arXiv:2307.13842</a> (replaced) [<a href="/pdf/2307.13842" title="Download PDF">pdf</a>, <a href="/format/2307.13842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CosSIF: Cosine similarity-based image filtering to overcome low  inter-class variation in synthetic medical image datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+M">Mominul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Zunair%2C+H">Hasib Zunair</a>, 
<a href="/search/cs?searchtype=author&query=Mohammed%2C+N">Nabeel Mohammed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item971">[971]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14070" title="Abstract">arXiv:2307.14070</a> (replaced) [<a href="/pdf/2307.14070" title="Download PDF">pdf</a>, <a href="/format/2307.14070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PNT-Edge: Towards Robust Edge Detection with Noisy Labels by Learning  Pixel-level Noise Transitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xuan%2C+W">Wenjie Xuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shanshan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Juhua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM-MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item972">[972]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14442" title="Abstract">arXiv:2307.14442</a> (replaced) [<a href="/pdf/2307.14442" title="Download PDF">pdf</a>, <a href="/format/2307.14442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Schr&#xf6;dinger Bridge with Sinkhorn Losses: Application to  Data-driven Minimum Effort Control of Colloidal Self-assembly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nodozi%2C+I">Iman Nodozi</a>, 
<a href="/search/math?searchtype=author&query=Yan%2C+C">Charlie Yan</a>, 
<a href="/search/math?searchtype=author&query=Khare%2C+M">Mira Khare</a>, 
<a href="/search/math?searchtype=author&query=Halder%2C+A">Abhishek Halder</a>, 
<a href="/search/math?searchtype=author&query=Mesbah%2C+A">Ali Mesbah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item973">[973]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15343" title="Abstract">arXiv:2307.15343</a> (replaced) [<a href="/pdf/2307.15343" title="Download PDF">pdf</a>, <a href="/format/2307.15343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Med-HALT: Medical Domain Hallucination Test for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pal%2C+A">Ankit Pal</a>, 
<a href="/search/cs?searchtype=author&query=Umapathi%2C+L+K">Logesh Kumar Umapathi</a>, 
<a href="/search/cs?searchtype=author&query=Sankarasubbu%2C+M">Malaikannan Sankarasubbu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023(The SIGNLL Conference on Computational Natural Language Learning)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item974">[974]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16890" title="Abstract">arXiv:2307.16890</a> (replaced) [<a href="/pdf/2307.16890" title="Download PDF">pdf</a>, <a href="/format/2307.16890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Adaptable Symbolic Algorithms from Scratch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kelly%2C+S">Stephen Kelly</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+D+S">Daniel S. Park</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xingyou Song</a>, 
<a href="/search/cs?searchtype=author&query=McIntire%2C+M">Mitchell McIntire</a>, 
<a href="/search/cs?searchtype=author&query=Nashikkar%2C+P">Pranav Nashikkar</a>, 
<a href="/search/cs?searchtype=author&query=Guha%2C+R">Ritam Guha</a>, 
<a href="/search/cs?searchtype=author&query=Banzhaf%2C+W">Wolfgang Banzhaf</a>, 
<a href="/search/cs?searchtype=author&query=Deb%2C+K">Kalyanmoy Deb</a>, 
<a href="/search/cs?searchtype=author&query=Boddeti%2C+V+N">Vishnu Naresh Boddeti</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jie Tan</a>, 
<a href="/search/cs?searchtype=author&query=Real%2C+E">Esteban Real</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published and Best Overall Paper Finalist at International Conference on Intelligent Robots and Systems (IROS) 2023. See <a href="https://youtu.be/sEFP1Hay4nE">this https URL</a> for associated video file
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item975">[975]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01362" title="Abstract">arXiv:2308.01362</a> (replaced) [<a href="/pdf/2308.01362" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Deep Learning for Tumor Dynamic Modeling and Overall  Survival Prediction using Neural-ODE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Laurie%2C+M">Mark Laurie</a>, 
<a href="/search/q-bio?searchtype=author&query=Lu%2C+J">James Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 4 Figures and 2 Tables. Includes Supplementary Materials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item976">[976]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01937" title="Abstract">arXiv:2308.01937</a> (replaced) [<a href="/pdf/2308.01937" title="Download PDF">pdf</a>, <a href="/format/2308.01937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Data Protection with Compositional Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golatkar%2C+A">Aditya Golatkar</a>, 
<a href="/search/cs?searchtype=author&query=Achille%2C+A">Alessandro Achille</a>, 
<a href="/search/cs?searchtype=author&query=Swaminathan%2C+A">Ashwin Swaminathan</a>, 
<a href="/search/cs?searchtype=author&query=Soatto%2C+S">Stefano Soatto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item977">[977]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02084" title="Abstract">arXiv:2308.02084</a> (replaced) [<a href="/pdf/2308.02084" title="Download PDF">pdf</a>, <a href="/format/2308.02084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Model Adaptation for Continual Learning at the Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daniels%2C+Z+A">Zachary A. Daniels</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lomnitz%2C+M">Michael Lomnitz</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+P">Phil Miller</a>, 
<a href="/search/cs?searchtype=author&query=Raghavan%2C+A">Aswin Raghavan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Joe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Piacentino%2C+M">Michael Piacentino</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">David Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Unpublished White Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item978">[978]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02187" title="Abstract">arXiv:2308.02187</a> (replaced) [<a href="/pdf/2308.02187" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoupling control parameter method to study the coupling  characteristics of subsystems in the feed system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+D">Dongsheng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xuesong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+T">Tingting Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item979">[979]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02909" title="Abstract">arXiv:2308.02909</a> (replaced) [<a href="/pdf/2308.02909" title="Download PDF">pdf</a>, <a href="/format/2308.02909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kalai&#x27;s $3^{d}$-conjecture for unconditional and locally anti-blocking  polytopes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sanyal%2C+R">Raman Sanyal</a>, 
<a href="/search/math?searchtype=author&query=Winter%2C+M">Martin Winter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Metric Geometry (math.MG)

</div>
</div>
</dd>
<dt><a name="item980">[980]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02962" title="Abstract">arXiv:2308.02962</a> (replaced) [<a href="/pdf/2308.02962" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Science and engineering for what? A large-scale analysis of students&#x27;  projects in science fairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eloy%2C+A">Adelmo Eloy</a>, 
<a href="/search/cs?searchtype=author&query=Ferraz%2C+T+P">Thomas Palmeira Ferraz</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+F+S">Fellip Silva Alves</a>, 
<a href="/search/cs?searchtype=author&query=de+Deus+Lopes%2C+R">Roseli de Deus Lopes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at International Conference of the Learning Sciences - ICLS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Physics Education (physics.ed-ph); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item981">[981]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03291" title="Abstract">arXiv:2308.03291</a> (replaced) [<a href="/pdf/2308.03291" title="Download PDF">pdf</a>, <a href="/format/2308.03291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SynJax: Structured Probability Distributions for JAX
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stanojevi%C4%87%2C+M">Milo&#x161; Stanojevi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Sartran%2C+L">Laurent Sartran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item982">[982]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03467" title="Abstract">arXiv:2308.03467</a> (replaced) [<a href="/pdf/2308.03467" title="Download PDF">pdf</a>, <a href="/format/2308.03467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoadScan: A Novel and Robust Transfer Learning Framework for Autonomous  Pothole Detection in Roads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parasnis%2C+G">Guruprasad Parasnis</a>, 
<a href="/search/cs?searchtype=author&query=Chokshi%2C+A">Anmol Chokshi</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vansh Jain</a>, 
<a href="/search/cs?searchtype=author&query=Devadkar%2C+K">Kailas Devadkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, Accepted at the IEEE 7th Conference on Communication and Information Technology 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item983">[983]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04813" title="Abstract">arXiv:2308.04813</a> (replaced) [<a href="/pdf/2308.04813" title="Download PDF">pdf</a>, <a href="/format/2308.04813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLEVA: Chinese Language Models EVAluation Platform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jianqiao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Duo Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zi-Yuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xiaohui Su</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yongfeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shijia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liwei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 System Demonstrations camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item984">[984]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05648" title="Abstract">arXiv:2308.05648</a> (replaced) [<a href="/pdf/2308.05648" title="Download PDF">pdf</a>, <a href="/format/2308.05648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Cross-modality Reasoning for Weakly Supervised Video  Moment Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+Z">Zezhong Lv</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+B">Bing Su</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item985">[985]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06791" title="Abstract">arXiv:2308.06791</a> (replaced) [<a href="/pdf/2308.06791" title="Download PDF">pdf</a>, <a href="/format/2308.06791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PV-SSD: A Multi-Modal Point Cloud Feature Fusion Method for Projection  Features and Variable Receptive Field Voxel Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yongxin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+A">Aihong Tan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhetao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+E">Enhui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tianhong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+P">Peng Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item986">[986]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06953" title="Abstract">arXiv:2308.06953</a> (replaced) [<a href="/pdf/2308.06953" title="Download PDF">pdf</a>, <a href="/format/2308.06953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained  Text Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heineman%2C+D">David Heineman</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Y">Yao Dou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 System Demonstration
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item987">[987]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07327" title="Abstract">arXiv:2308.07327</a> (replaced) [<a href="/pdf/2308.07327" title="Download PDF">pdf</a>, <a href="/format/2308.07327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant  Poker Game Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure, submission to IEEE Transactions on Games
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item988">[988]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07774" title="Abstract">arXiv:2308.07774</a> (replaced) [<a href="/pdf/2308.07774" title="Download PDF">pdf</a>, <a href="/format/2308.07774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mesgaran%2C+M">Mahsa Mesgaran</a>, 
<a href="/search/cs?searchtype=author&query=Hamza%2C+A+B">A. Ben Hamza</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neural Computing and Applications, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item989">[989]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08363" title="Abstract">arXiv:2308.08363</a> (replaced) [<a href="/pdf/2308.08363" title="Download PDF">pdf</a>, <a href="/format/2308.08363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SummHelper: Collaborative Human-Computer Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Slobodkin%2C+A">Aviv Slobodkin</a>, 
<a href="/search/cs?searchtype=author&query=Nachum%2C+N">Niv Nachum</a>, 
<a href="/search/cs?searchtype=author&query=Amar%2C+S">Shmuel Amar</a>, 
<a href="/search/cs?searchtype=author&query=Shapira%2C+O">Ori Shapira</a>, 
<a href="/search/cs?searchtype=author&query=Dagan%2C+I">Ido Dagan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 System Demonstrations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item990">[990]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08696" title="Abstract">arXiv:2308.08696</a> (replaced) [<a href="/pdf/2308.08696" title="Download PDF">pdf</a>, <a href="/format/2308.08696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Anomaly Segmentation with Multi-Granularity Cross-Domain  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Q">Qi He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM Multimedia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item991">[991]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08925" title="Abstract">arXiv:2308.08925</a> (replaced) [<a href="/pdf/2308.08925" title="Download PDF">pdf</a>, <a href="/format/2308.08925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A White-Box False Positive Adversarial Attack Method on Contrastive  Loss-Based Offline Handwritten Signature Verification Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhongliang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weiye Li</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yifei Qian</a>, 
<a href="/search/cs?searchtype=author&query=Arandjelovi%C4%87%2C+O">Ognjen Arandjelovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+L">Lei Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item992">[992]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09487" title="Abstract">arXiv:2308.09487</a> (replaced) [<a href="/pdf/2308.09487" title="Download PDF">pdf</a>, <a href="/format/2308.09487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High  Attack Success Rate in the Absence of Training Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+B">Binhao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dejun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+B">Bo Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item993">[993]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09678" title="Abstract">arXiv:2308.09678</a> (replaced) [<a href="/pdf/2308.09678" title="Download PDF">pdf</a>, <a href="/format/2308.09678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D  Human Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanbing Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jun-Yan He</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+W">Wangmeng Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qize Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+W">Wenhao Chai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gaoang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+X">Xu Bao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yifeng Geng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xuansong Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM Multimedia 2023; 10 pages, 4 figures, 8 tables; the code is at <a href="https://github.com/hbing-l/PoSynDA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item994">[994]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10279" title="Abstract">arXiv:2308.10279</a> (replaced) [<a href="/pdf/2308.10279" title="Download PDF">pdf</a>, <a href="/format/2308.10279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPFL: Simultaneously Learning Global and Personalized Feature  Information for Personalized Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yang Hua</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+T">Tao Song</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhengui Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">Ruhui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jian Cao</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+H">Haibing Guan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item995">[995]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10335" title="Abstract">arXiv:2308.10335</a> (replaced) [<a href="/pdf/2308.10335" title="Download PDF">pdf</a>, <a href="/format/2308.10335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study on Robustness and Reliability of Large Language Model Code  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+L">Li Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zilong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item996">[996]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10633" title="Abstract">arXiv:2308.10633</a> (replaced) [<a href="/pdf/2308.10633" title="Download PDF">pdf</a>, <a href="/format/2308.10633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoshi%2C+Y">Yasuto Hoshi</a>, 
<a href="/search/cs?searchtype=author&query=Miyashita%2C+D">Daisuke Miyashita</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+Y">Youyang Ng</a>, 
<a href="/search/cs?searchtype=author&query=Tatsuno%2C+K">Kento Tatsuno</a>, 
<a href="/search/cs?searchtype=author&query=Morioka%2C+Y">Yasuhiro Morioka</a>, 
<a href="/search/cs?searchtype=author&query=Torii%2C+O">Osamu Torii</a>, 
<a href="/search/cs?searchtype=author&query=Deguchi%2C+J">Jun Deguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures, see <a href="https://youtu.be/JYbm75qnfTg">this https URL</a> for the demonstration screencast, accepted by EMNLP 2023 System Demonstrations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item997">[997]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11073" title="Abstract">arXiv:2308.11073</a> (replaced) [<a href="/pdf/2308.11073" title="Download PDF">pdf</a>, <a href="/format/2308.11073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio-Visual Class-Incremental Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pian%2C+W">Weiguo Pian</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+S">Shentong Mo</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yunhui Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yapeng Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item998">[998]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11829" title="Abstract">arXiv:2308.11829</a> (replaced) [<a href="/pdf/2308.11829" title="Download PDF">pdf</a>, <a href="/format/2308.11829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithm-assisted discovery of an intrinsic order among mathematical  constants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elimelech%2C+R">Rotem Elimelech</a>, 
<a href="/search/cs?searchtype=author&query=David%2C+O">Ofir David</a>, 
<a href="/search/cs?searchtype=author&query=De+la+Cruz+Mengual%2C+C">Carlos De la Cruz Mengual</a>, 
<a href="/search/cs?searchtype=author&query=Kalisch%2C+R">Rotem Kalisch</a>, 
<a href="/search/cs?searchtype=author&query=Berndt%2C+W">Wolfgang Berndt</a>, 
<a href="/search/cs?searchtype=author&query=Shalyt%2C+M">Michael Shalyt</a>, 
<a href="/search/cs?searchtype=author&query=Silberstein%2C+M">Mark Silberstein</a>, 
<a href="/search/cs?searchtype=author&query=Hadad%2C+Y">Yaron Hadad</a>, 
<a href="/search/cs?searchtype=author&query=Kaminer%2C+I">Ido Kaminer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 figures, and 1 table; with 9 appendix sections totaling 12 pages, 1 figure, and 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Number Theory (math.NT)

</div>
</div>
</dd>
<dt><a name="item999">[999]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11873" title="Abstract">arXiv:2308.11873</a> (replaced) [<a href="/pdf/2308.11873" title="Download PDF">pdf</a>, <a href="/format/2308.11873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dcc --help: Generating Context-Aware Compiler Error Explanations with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taylor%2C+A">Andrew Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Vassar%2C+A">Alexandra Vassar</a>, 
<a href="/search/cs?searchtype=author&query=Renzella%2C+J">Jake Renzella</a>, 
<a href="/search/cs?searchtype=author&query=Pearce%2C+H">Hammond Pearce</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures. Accepted in SIGCSE'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item1000">[1000]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11933" title="Abstract">arXiv:2308.11933</a> (replaced) [<a href="/pdf/2308.11933" title="Download PDF">pdf</a>, <a href="/format/2308.11933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> System Identification for Continuous-time Linear Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halmos%2C+P">Peter Halmos</a>, 
<a href="/search/cs?searchtype=author&query=Pillow%2C+J">Jonathan Pillow</a>, 
<a href="/search/cs?searchtype=author&query=Knowles%2C+D+A">David A. Knowles</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 3 figures. Only light changes and restructuring to previous version made
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1001">[1001]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13382" title="Abstract">arXiv:2308.13382</a> (replaced) [<a href="/pdf/2308.13382" title="Download PDF">pdf</a>, <a href="/format/2308.13382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Visual-Language Models for Dynamic Facial Expression  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zengqun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Patras%2C+I">Ioannis Patras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at BMVC 2023 (Camera ready)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1002">[1002]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13904" title="Abstract">arXiv:2308.13904</a> (replaced) [<a href="/pdf/2308.13904" title="Download PDF">pdf</a>, <a href="/format/2308.13904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chengkun Wei</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+W">Wenlong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Minghu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+W">Wenjing Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenzhi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear in the Network and Distributed System Security (NDSS) Symposium 2024, 26 February - 1 March 2024, San Diego, CA, USA; typos corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1003">[1003]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13941" title="Abstract">arXiv:2308.13941</a> (replaced) [<a href="/pdf/2308.13941" title="Download PDF">pdf</a>, <a href="/format/2308.13941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A small vocabulary database of ultrasound image sequences of vocal tract  dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castillo%2C+M">Margareth Castillo</a>, 
<a href="/search/cs?searchtype=author&query=Rubio%2C+F">Felipe Rubio</a>, 
<a href="/search/cs?searchtype=author&query=Porras%2C+D">Dagoberto Porras</a>, 
<a href="/search/cs?searchtype=author&query=Contreras-Ortiz%2C+S+H">Sonia H. Contreras-Ortiz</a>, 
<a href="/search/cs?searchtype=author&query=Sep%C3%BAlveda%2C+A">Alexander Sep&#xfa;lveda</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> STSIVA-2019, Bucaramanga, Colombia, 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1004">[1004]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14577" title="Abstract">arXiv:2308.14577</a> (replaced) [<a href="/pdf/2308.14577" title="Download PDF">pdf</a>, <a href="/format/2308.14577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantitative Data Analysis: CRASAR Small Unmanned Aerial Systems at  Hurricane Ian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manzini%2C+T">Thomas Manzini</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+R">Robin Murphy</a>, 
<a href="/search/cs?searchtype=author&query=Merrick%2C+D">David Merrick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1005">[1005]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14785" title="Abstract">arXiv:2308.14785</a> (replaced) [<a href="/pdf/2308.14785" title="Download PDF">pdf</a>, <a href="/format/2308.14785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A correlation-based fuzzy cluster validity index with secondary options  detector
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wiroonsri%2C+N">Nathakhun Wiroonsri</a>, 
<a href="/search/stat?searchtype=author&query=Preedasawakul%2C+O">Onthada Preedasawakul</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1006">[1006]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14840" title="Abstract">arXiv:2308.14840</a> (replaced) [<a href="/pdf/2308.14840" title="Download PDF">pdf</a>, <a href="/format/2308.14840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying and Mitigating the Security Risks of Generative AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barrett%2C+C">Clark Barrett</a>, 
<a href="/search/cs?searchtype=author&query=Boyd%2C+B">Brad Boyd</a>, 
<a href="/search/cs?searchtype=author&query=Burzstein%2C+E">Ellie Burzstein</a>, 
<a href="/search/cs?searchtype=author&query=Carlini%2C+N">Nicholas Carlini</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Brad Chen</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jihye Choi</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+A+R">Amrita Roy Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Christodorescu%2C+M">Mihai Christodorescu</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+A">Anupam Datta</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>, 
<a href="/search/cs?searchtype=author&query=Fisher%2C+K">Kathleen Fisher</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T">Tatsunori Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Hendrycks%2C+D">Dan Hendrycks</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Daniel Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kerschbaum%2C+F">Florian Kerschbaum</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+E">Eric Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+J">John Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Ramzan%2C+Z">Zulfikar Ramzan</a>, 
<a href="/search/cs?searchtype=author&query=Shams%2C+K">Khawaja Shams</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>, 
<a href="/search/cs?searchtype=author&query=Taly%2C+A">Ankur Taly</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1007">[1007]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16422" title="Abstract">arXiv:2308.16422</a> (replaced) [<a href="/pdf/2308.16422" title="Download PDF">pdf</a>, <a href="/format/2308.16422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DECODE: DilatEd COnvolutional neural network for Detecting  Extreme-mass-ratio inspirals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Zhao%2C+T">Tianyu Zhao</a>, 
<a href="/search/astro-ph?searchtype=author&query=Zhou%2C+Y">Yue Zhou</a>, 
<a href="/search/astro-ph?searchtype=author&query=Shi%2C+R">Ruijun Shi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cao%2C+Z">Zhoujian Cao</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ren%2C+Z">Zhixiang Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, and 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG); General Relativity and Quantum Cosmology (gr-qc)

</div>
</div>
</dd>
<dt><a name="item1008">[1008]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16518" title="Abstract">arXiv:2308.16518</a> (replaced) [<a href="/pdf/2308.16518" title="Download PDF">pdf</a>, <a href="/format/2308.16518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MS23D: A 3D Object Detection Method Using Multi-Scale Semantic Feature  Points to Construct 3D Feature Layer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yongxin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+A">Aihong Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tianhong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhetao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaxin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1009">[1009]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16539" title="Abstract">arXiv:2308.16539</a> (replaced) [<a href="/pdf/2308.16539" title="Download PDF">pdf</a>, <a href="/format/2308.16539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a Connection between Differential Games, Optimal Control, and  Energy-based Models for Multi-Agent Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diehl%2C+C">Christopher Diehl</a>, 
<a href="/search/cs?searchtype=author&query=Klosek%2C+T">Tobias Klosek</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C3%BCger%2C+M">Martin Kr&#xfc;ger</a>, 
<a href="/search/cs?searchtype=author&query=Murzyn%2C+N">Nils Murzyn</a>, 
<a href="/search/cs?searchtype=author&query=Bertram%2C+T">Torsten Bertram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Machine Learning, Workshop on New Frontiers in Learning, Control, and Dynamical Systems (ICML 2023 Frontiers4LCD); added further related work in Energy-based Model sections in V2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item1010">[1010]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00987" title="Abstract">arXiv:2309.00987</a> (replaced) [<a href="/pdf/2309.00987" title="Download PDF">pdf</a>, <a href="/format/2309.00987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuanpei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fei-Fei%2C+L">Li Fei-Fei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C+K">C. Karen Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7th Conference on Robot Learning (CoRL 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1011">[1011]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01289" title="Abstract">arXiv:2309.01289</a> (replaced) [<a href="/pdf/2309.01289" title="Download PDF">pdf</a>, <a href="/format/2309.01289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting  in Continual Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakman%2C+Y+F">Yavuz Faruk Bakman</a>, 
<a href="/search/cs?searchtype=author&query=Yaldiz%2C+D+N">Duygu Nur Yaldiz</a>, 
<a href="/search/cs?searchtype=author&query=Ezzeldin%2C+Y+H">Yahya H. Ezzeldin</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+S">Salman Avestimehr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1012">[1012]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01293" title="Abstract">arXiv:2309.01293</a> (replaced) [<a href="/pdf/2309.01293" title="Download PDF">pdf</a>, <a href="/format/2309.01293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero Trust Real-Time Lightweight Access Control Protocol for Mobile  Cloud-Based IoT Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohseni-Ejiyeh%2C+A">Atefeh Mohseni-Ejiyeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1013">[1013]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01429" title="Abstract">arXiv:2309.01429</a> (replaced) [<a href="/pdf/2309.01429" title="Download PDF">pdf</a>, <a href="/format/2309.01429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting Segment Anything Model for Change Detection in HR Remote  Sensing Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Lei Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Daifeng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kuiwu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bruzzone%2C+L">Lorenzo Bruzzone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1014">[1014]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01839" title="Abstract">arXiv:2309.01839</a> (replaced) [<a href="/pdf/2309.01839" title="Download PDF">pdf</a>, <a href="/format/2309.01839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing a Security System Administration Course for Cybersecurity with  a Companion Project
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zuo%2C+F">Fei Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Rhee%2C+J">Junghwan Rhee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+M">Myungah Park</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+G">Gang Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 37th Annual CCSC: Southeastern Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1015">[1015]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02671" title="Abstract">arXiv:2309.02671</a> (replaced) [<a href="/pdf/2309.02671" title="Download PDF">pdf</a>, <a href="/format/2309.02671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RLSynC: Offline-Online Reinforcement Learning for Synthon Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baker%2C+F+N">Frazier N. Baker</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xia Ning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1016">[1016]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02772" title="Abstract">arXiv:2309.02772</a> (replaced) [<a href="/pdf/2309.02772" title="Download PDF">pdf</a>, <a href="/format/2309.02772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Code Generation by Dynamic Temperature Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuqi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Ge Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">YunFei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Hong Mei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1017">[1017]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02959" title="Abstract">arXiv:2309.02959</a> (replaced) [<a href="/pdf/2309.02959" title="Download PDF">pdf</a>, <a href="/format/2309.02959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Non-Invasive Interpretable NAFLD Diagnostic Method Combining TCM  Tongue Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cao%2C+S">Shan Cao</a>, 
<a href="/search/eess?searchtype=author&query=Ruan%2C+Q">Qunsheng Ruan</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Q">Qingfeng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1018">[1018]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03038" title="Abstract">arXiv:2309.03038</a> (replaced) [<a href="/pdf/2309.03038" title="Download PDF">pdf</a>, <a href="/format/2309.03038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cellular Wireless Networks in the Upper Mid-Band
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Seongjoon Kang</a>, 
<a href="/search/cs?searchtype=author&query=Mezzavilla%2C+M">Marco Mezzavilla</a>, 
<a href="/search/cs?searchtype=author&query=Rangan%2C+S">Sundeep Rangan</a>, 
<a href="/search/cs?searchtype=author&query=Madanayake%2C+A">Arjuna Madanayake</a>, 
<a href="/search/cs?searchtype=author&query=Venkatakrishnan%2C+S+B">Satheesh Bojja Venkatakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Hellbourg%2C+G">Gregory Hellbourg</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+M">Monisha Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+H">Hamed Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Dhananjay%2C+A">Aditya Dhananjay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1019">[1019]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03059" title="Abstract">arXiv:2309.03059</a> (replaced) [<a href="/pdf/2309.03059" title="Download PDF">pdf</a>, <a href="/format/2309.03059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Intelligent Surface Aided Space Shift Keying With  Imperfect CSI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xusheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhendong Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shunqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2307.01994">arXiv:2307.01994</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1020">[1020]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04761" title="Abstract">arXiv:2309.04761</a> (replaced) [<a href="/pdf/2309.04761" title="Download PDF">pdf</a>, <a href="/format/2309.04761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on Deep Learning Techniques in Educational Data  Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuanguo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+W">Wei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Fan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Pengcheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zongyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1021">[1021]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05077" title="Abstract">arXiv:2309.05077</a> (replaced) [<a href="/pdf/2309.05077" title="Download PDF">pdf</a>, <a href="/ps/2309.05077" title="Download PostScript">ps</a>, <a href="/format/2309.05077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization error bounds for iterative learning algorithms with  bounded updates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jingwen Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1022">[1022]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05133" title="Abstract">arXiv:2309.05133</a> (replaced) [<a href="/pdf/2309.05133" title="Download PDF">pdf</a>, <a href="/format/2309.05133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel RAM from Cyclic Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heath%2C+D">David Heath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item1023">[1023]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06397" title="Abstract">arXiv:2309.06397</a> (replaced) [<a href="/pdf/2309.06397" title="Download PDF">pdf</a>, <a href="/ps/2309.06397" title="Download PostScript">ps</a>, <a href="/format/2309.06397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Separation of Control Flow and Data Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arellanes%2C+D">Damian Arellanes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item1024">[1024]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08636" title="Abstract">arXiv:2309.08636</a> (replaced) [<a href="/pdf/2309.08636" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI  chatbots at scientific writing?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lozi%C4%87%2C+E">Edisa Lozi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0tular%2C+B">Benjamin &#x160;tular</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is Preprint with Graphical abstract and Highlights. Please cite the peer-reviewed open access publication at <a href="https://doi.org/10.3390/fi15100336">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Emerging Technologies (cs.ET); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1025">[1025]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09131" title="Abstract">arXiv:2309.09131</a> (replaced) [<a href="/pdf/2309.09131" title="Download PDF">pdf</a>, <a href="/format/2309.09131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynasor: A Dynamic Memory Layout for Accelerating Sparse MTTKRP for  Tensor Decomposition on Multi-core CPU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wijeratne%2C+S">Sasindu Wijeratne</a>, 
<a href="/search/cs?searchtype=author&query=Kannan%2C+R">Rajgopal Kannan</a>, 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+V">Viktor Prasanna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item1026">[1026]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09378" title="Abstract">arXiv:2309.09378</a> (replaced) [<a href="/pdf/2309.09378" title="Download PDF">pdf</a>, <a href="/ps/2309.09378" title="Download PostScript">ps</a>, <a href="/format/2309.09378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamics of Fisheries in the Azores Islands: A Network Analysis Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nogueira%2C+B">Brenda Nogueira</a>, 
<a href="/search/cs?searchtype=author&query=Torres%2C+A">Ana Torres</a>, 
<a href="/search/cs?searchtype=author&query=Moniz%2C+N">Nuno Moniz</a>, 
<a href="/search/cs?searchtype=author&query=Menezes%2C+G+M">Gui M. Menezes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item1027">[1027]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09733" title="Abstract">arXiv:2309.09733</a> (replaced) [<a href="/pdf/2309.09733" title="Download PDF">pdf</a>, <a href="/format/2309.09733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replication: Contrastive Learning and Data Augmentation in Traffic  Classification Using a Flowpic Input Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Finamore%2C+A">Alessandro Finamore</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Krolikowski%2C+J">Jonatan Krolikowski</a>, 
<a href="/search/cs?searchtype=author&query=Navarro%2C+J+M">Jose M. Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fuxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+D">Dario Rossi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear at ACM Internet Traffic Measurement (IMC) 2023, replication track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item1028">[1028]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09919" title="Abstract">arXiv:2309.09919</a> (replaced) [<a href="/pdf/2309.09919" title="Download PDF">pdf</a>, <a href="/format/2309.09919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+S+S">Shreyas S. Raman</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Ankit Shah</a>, 
<a href="/search/cs?searchtype=author&query=Tellex%2C+S">Stefanie Tellex</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item1029">[1029]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10245" title="Abstract">arXiv:2309.10245</a> (replaced) [<a href="/pdf/2309.10245" title="Download PDF">pdf</a>, <a href="/format/2309.10245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language Dataset Generation Framework for Visualizations Powered  by Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+H">Hyung-Kwon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyeon Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+G">Gwanmo Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D+H">Dae Hyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N+W">Nam Wook Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Jinwook Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1030">[1030]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10432" title="Abstract">arXiv:2309.10432</a> (replaced) [<a href="/pdf/2309.10432" title="Download PDF">pdf</a>, <a href="/ps/2309.10432" title="Download PostScript">ps</a>, <a href="/format/2309.10432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The supersingular Endomorphism Ring and One Endomorphism problems are  equivalent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Page%2C+A">Aurel Page</a> (IMB, CANARI, LFANT), 
<a href="/search/cs?searchtype=author&query=Wesolowski%2C+B">Benjamin Wesolowski</a> (CNRS, ENS de Lyon, UMPA-ENSL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Algebraic Geometry (math.AG); Number Theory (math.NT)

</div>
</div>
</dd>
<dt><a name="item1031">[1031]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10462" title="Abstract">arXiv:2309.10462</a> (replaced) [<a href="/pdf/2309.10462" title="Download PDF">pdf</a>, <a href="/ps/2309.10462" title="Download PostScript">ps</a>, <a href="/format/2309.10462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing the Weight Distribution of the Binary Reed-Muller Code  ${\mathcal R} (4,9)$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Markov%2C+M">Miroslav Markov</a>, 
<a href="/search/cs?searchtype=author&query=Borissov%2C+Y">Yuri Borissov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1032">[1032]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10649" title="Abstract">arXiv:2309.10649</a> (replaced) [<a href="/pdf/2309.10649" title="Download PDF">pdf</a>, <a href="/format/2309.10649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-modal and Cross-domain Knowledge Transfer for Label-free 3D  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huitong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dai-Jie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Keung%2C+J">Jacky Keung</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuesong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinge Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuexin Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,4 figures,accepted
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Chinese Conference on Pattern Recognition and Computer Vision
  (PRCV) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1033">[1033]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11044" title="Abstract">arXiv:2309.11044</a> (replaced) [<a href="/pdf/2309.11044" title="Download PDF">pdf</a>, <a href="/format/2309.11044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustered FedStack: Intermediate Global Models with Bayesian Information  Criterion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaik%2C+T">Thanveer Shaik</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaohui Tao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Higgins%2C+N">Niall Higgins</a>, 
<a href="/search/cs?searchtype=author&query=Gururajan%2C+R">Raj Gururajan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xujuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yong%2C+J">Jianming Yong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the ELSEVIER for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1034">[1034]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11331" title="Abstract">arXiv:2309.11331</a> (replaced) [<a href="/pdf/2309.11331" title="Download PDF">pdf</a>, <a href="/format/2309.11331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengcheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wei He</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Ying Nie</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jianyuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chuanjian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1035">[1035]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11751" title="Abstract">arXiv:2309.11751</a> (replaced) [<a href="/pdf/2309.11751" title="Download PDF">pdf</a>, <a href="/format/2309.11751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Robust is Google&#x27;s Bard to Adversarial Image Attacks?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yinpeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huanran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiawei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhengwei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1036">[1036]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12016" title="Abstract">arXiv:2309.12016</a> (replaced) [<a href="/pdf/2309.12016" title="Download PDF">pdf</a>, <a href="/format/2309.12016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven models for predicting the outcome of autonomous wheel loader  operations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aoshima%2C+K">Koji Aoshima</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A4lldin%2C+A">Arvid F&#xe4;lldin</a>, 
<a href="/search/cs?searchtype=author&query=Wadbro%2C+E">Eddie Wadbro</a>, 
<a href="/search/cs?searchtype=author&query=Servin%2C+M">Martin Servin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1037">[1037]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12350" title="Abstract">arXiv:2309.12350</a> (replaced) [<a href="/pdf/2309.12350" title="Download PDF">pdf</a>, <a href="/format/2309.12350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Internet of Things Adoption Challenges in Manufacturing Firms:  A Fuzzy Analytical Hierarchy Process Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahriar%2C+H">Hasan Shahriar</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+S">Md. Saiful Islam</a>, 
<a href="/search/cs?searchtype=author&query=Jahin%2C+M+A">Md Abrar Jahin</a>, 
<a href="/search/cs?searchtype=author&query=Ridoy%2C+I+A">Istiyaque Ahmed Ridoy</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">Taro Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jungpil Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1038">[1038]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12819" title="Abstract">arXiv:2309.12819</a> (replaced) [<a href="/pdf/2309.12819" title="Download PDF">pdf</a>, <a href="/format/2309.12819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Doubly Robust Proximal Causal Learning for Continuous Treatments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wu%2C+Y">Yong Wu</a>, 
<a href="/search/stat?searchtype=author&query=Fu%2C+Y">Yanwei Fu</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+S">Shouyan Wang</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+X">Xinwei Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1039">[1039]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13567" title="Abstract">arXiv:2309.13567</a> (replaced) [<a href="/pdf/2309.13567" title="Download PDF">pdf</a>, <a href="/format/2309.13567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MentaLLaMA: Interpretable Mental Health Analysis on Social Media with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianlin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Z">Ziyan Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1040">[1040]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14735" title="Abstract">arXiv:2309.14735</a> (replaced) [<a href="/pdf/2309.14735" title="Download PDF">pdf</a>, <a href="/format/2309.14735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Legal Question-Answering in the Indian Context: Efficacy, Challenges,  and Potential of Modern AI Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nigam%2C+S+K">Shubham Kumar Nigam</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S+K">Shubham Kumar Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+A+K">Ayush Kumar Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Shallum%2C+N">Noel Shallum</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Arnab Bhattacharya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1041">[1041]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15104" title="Abstract">arXiv:2309.15104</a> (replaced) [<a href="/pdf/2309.15104" title="Download PDF">pdf</a>, <a href="/ps/2309.15104" title="Download PostScript">ps</a>, <a href="/format/2309.15104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient, traceable, and numerical error-free implementation of the MMS  voting rule
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez-Fern%C3%A1ndez%2C+L">Luis S&#xe1;nchez-Fern&#xe1;ndez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item1042">[1042]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15277" title="Abstract">arXiv:2309.15277</a> (replaced) [<a href="/pdf/2309.15277" title="Download PDF">pdf</a>, <a href="/format/2309.15277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting High Resolution Image Classification with Scaling-up  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech report. 4 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1043">[1043]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15402" title="Abstract">arXiv:2309.15402</a> (replaced) [<a href="/pdf/2309.15402" title="Download PDF">pdf</a>, <a href="/format/2309.15402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zheng Chu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingchang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qianglong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Weijiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tao He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haotian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Weihua Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages. Resources are available at <a href="https://github.com/zchuz/CoT-Reasoning-Survey">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1044">[1044]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15642" title="Abstract">arXiv:2309.15642</a> (replaced) [<a href="/pdf/2309.15642" title="Download PDF">pdf</a>, <a href="/format/2309.15642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient tensor network simulation of IBM&#x27;s largest quantum processors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Patra%2C+S">Siddhartha Patra</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jahromi%2C+S+S">Saeed S. Jahromi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Singh%2C+S">Sukhbinder Singh</a>, 
<a href="/search/quant-ph?searchtype=author&query=Orus%2C+R">Roman Orus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, plus Supplementary Material of 1 page with 1 table. Revised version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Strongly Correlated Electrons (cond-mat.str-el); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1045">[1045]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15701" title="Abstract">arXiv:2309.15701</a> (replaced) [<a href="/pdf/2309.15701" title="Download PDF">pdf</a>, <a href="/format/2309.15701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyPoradise: An Open Baseline for Generative Speech Recognition with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuchen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
<a href="/search/cs?searchtype=author&query=Siniscalchi%2C+S+M">Sabato Macro Siniscalchi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+E+S">Eng Siong Chng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023, 24 pages. Datasets and Benchmarks Track. Added the first Mandarin and code-switching (zh-cn and en-us) results from the LLM-based generative ASR error correction to Table 8 on Page 21
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1046">[1046]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15757" title="Abstract">arXiv:2309.15757</a> (replaced) [<a href="/pdf/2309.15757" title="Download PDF">pdf</a>, <a href="/format/2309.15757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Graphs for Semi-Supervised Learning on Biomedical Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koloski%2C+B">Boshko Koloski</a>, 
<a href="/search/cs?searchtype=author&query=Lavra%C4%8D%2C+N">Nada Lavra&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Pollak%2C+S">Senja Pollak</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0krlj%2C+B">Bla&#x17e; &#x160;krlj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IJCLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1047">[1047]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15775" title="Abstract">arXiv:2309.15775</a> (replaced) [<a href="/pdf/2309.15775" title="Download PDF">pdf</a>, <a href="/format/2309.15775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning the Efficient Frontier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatigny%2C+P">Philippe Chatigny</a>, 
<a href="/search/cs?searchtype=author&query=Sergienko%2C+I">Ivan Sergienko</a>, 
<a href="/search/cs?searchtype=author&query=Ferguson%2C+R">Ryan Ferguson</a>, 
<a href="/search/cs?searchtype=author&query=Weir%2C+J">Jordan Weir</a>, 
<a href="/search/cs?searchtype=author&query=Bergeron%2C+M">Maxime Bergeron</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item1048">[1048]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16108" title="Abstract">arXiv:2309.16108</a> (replaced) [<a href="/pdf/2309.16108" title="Download PDF">pdf</a>, <a href="/format/2309.16108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+Y">Yujia Bao</a>, 
<a href="/search/cs?searchtype=author&query=Sivanandan%2C+S">Srinivasan Sivanandan</a>, 
<a href="/search/cs?searchtype=author&query=Karaletsos%2C+T">Theofanis Karaletsos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1049">[1049]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16977" title="Abstract">arXiv:2309.16977</a> (replaced) [<a href="/pdf/2309.16977" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reliability Quantification of Deep Reinforcement Learning-based Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yoshioka%2C+H">Hitoshi Yoshioka</a>, 
<a href="/search/eess?searchtype=author&query=Hashimoto%2C+H">Hirotada Hashimoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages and 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1050">[1050]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17160" title="Abstract">arXiv:2309.17160</a> (replaced) [<a href="/pdf/2309.17160" title="Download PDF">pdf</a>, <a href="/format/2309.17160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redistributing the Precision and Content in 3D-LUT-based Inverse  Tone-mapping for HDR/WCG Display
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Cheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Leidong Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kanglin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiuhua Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in CVMP2023 (the 20th ACM SIGGRAPH European Conference on Visual Media Production)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1051">[1051]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17281" title="Abstract">arXiv:2309.17281</a> (replaced) [<a href="/pdf/2309.17281" title="Download PDF">pdf</a>, <a href="/format/2309.17281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Flow in Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhiquan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingqin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1052">[1052]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17288" title="Abstract">arXiv:2309.17288</a> (replaced) [<a href="/pdf/2309.17288" title="Download PDF">pdf</a>, <a href="/format/2309.17288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoAgents: A Framework for Automatic Agent Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+S">Siwei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Y">Yu Shu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sesay%2C+J">Jaward Sesay</a>, 
<a href="/search/cs?searchtype=author&query=Karlsson%2C+B+F">B&#xf6;rje F. Karlsson</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yemin Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1053">[1053]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17342" title="Abstract">arXiv:2309.17342</a> (replaced) [<a href="/pdf/2309.17342" title="Download PDF">pdf</a>, <a href="/format/2309.17342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Free Data Selection with General-Purpose Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yichen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1054">[1054]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00029" title="Abstract">arXiv:2310.00029</a> (replaced) [<a href="/e-print/2310.00029" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Driving Behavior Generation Incorporating Human Risk  Cognition for Autonomous Vehicle Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+S">Shuo Cai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yunfeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+T">Ting Qu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xun Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We find there is expression error in III.A. A correction edition will be offered
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1055">[1055]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00270" title="Abstract">arXiv:2310.00270</a> (replaced) [<a href="/pdf/2310.00270" title="Download PDF">pdf</a>, <a href="/format/2310.00270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpatialRank: Urban Event Ranking with NDCG Optimization on  Spatiotemporal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yongjian Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianbao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1056">[1056]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00347" title="Abstract">arXiv:2310.00347</a> (replaced) [<a href="/pdf/2310.00347" title="Download PDF">pdf</a>, <a href="/format/2310.00347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Bias Detection: Leveraging Transformer-Based Models for  Content Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raza%2C+S">Shaina Raza</a>, 
<a href="/search/cs?searchtype=author&query=Bamgbose%2C+O">Oluwanifemi Bamgbose</a>, 
<a href="/search/cs?searchtype=author&query=Chatrath%2C+V">Veronica Chatrath</a>, 
<a href="/search/cs?searchtype=author&query=Ghuge%2C+S">Shardul Ghuge</a>, 
<a href="/search/cs?searchtype=author&query=Sidyakin%2C+Y">Yan Sidyakin</a>, 
<a href="/search/cs?searchtype=author&query=Muaad%2C+A+Y">Abdullah Y Muaad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> UNDER REVIEW
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1057">[1057]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00426" title="Abstract">arXiv:2310.00426</a> (replaced) [<a href="/pdf/2310.00426" title="Download PDF">pdf</a>, <a href="/format/2310.00426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PixArt-$&#x3b1;$: Fast Training of Diffusion Transformer for  Photorealistic Text-to-Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junsong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jincheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+C">Chongjian Ge</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lewei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongdao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J">James Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Huchuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://pixart-alpha.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1058">[1058]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00867" title="Abstract">arXiv:2310.00867</a> (replaced) [<a href="/pdf/2310.00867" title="Download PDF">pdf</a>, <a href="/format/2310.00867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (Dynamic) Prompting might be all you need to repair Compressed LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D+N+M">Duc N.M Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+M">Minsik Cho</a>, 
<a href="/search/cs?searchtype=author&query=Merth%2C+T">Thomas Merth</a>, 
<a href="/search/cs?searchtype=author&query=Rastegari%2C+M">Mohammad Rastegari</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1059">[1059]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01164" title="Abstract">arXiv:2310.01164</a> (replaced) [<a href="/pdf/2310.01164" title="Download PDF">pdf</a>, <a href="/format/2310.01164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Any Building For Remote Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1060">[1060]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01228" title="Abstract">arXiv:2310.01228</a> (replaced) [<a href="/pdf/2310.01228" title="Download PDF">pdf</a>, <a href="/format/2310.01228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing 3D Human Pose from RGB-D Data with Occlusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dang%2C+B">Bowen Dang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bowen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1061">[1061]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01376" title="Abstract">arXiv:2310.01376</a> (replaced) [<a href="/pdf/2310.01376" title="Download PDF">pdf</a>, <a href="/format/2310.01376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Distribution-Agnostic Generalized Category Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jianhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hualiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruizhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+L">Lianrui Mu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaomeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haoji Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1062">[1062]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01701" title="Abstract">arXiv:2310.01701</a> (replaced) [<a href="/pdf/2310.01701" title="Download PDF">pdf</a>, <a href="/format/2310.01701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transcending Domains through Text-to-Image Diffusion: A Source-Free  Approach to Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chopra%2C+S">Shivang Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Kothawade%2C+S">Suraj Kothawade</a>, 
<a href="/search/cs?searchtype=author&query=Aynaou%2C+H">Houda Aynaou</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1063">[1063]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01745" title="Abstract">arXiv:2310.01745</a> (replaced) [<a href="/pdf/2310.01745" title="Download PDF">pdf</a>, <a href="/format/2310.01745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Volumetric Approach to Monge&#x27;s Optimal Transport on Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tsai%2C+R">Richard Tsai</a>, 
<a href="/search/math?searchtype=author&query=Turnquist%2C+A+G+R">Axel G. R. Turnquist</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1064">[1064]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01852" title="Abstract">arXiv:2310.01852</a> (replaced) [<a href="/pdf/2310.01852" title="Download PDF">pdf</a>, <a href="/format/2310.01852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LanguageBind: Extending Video-Language Pretraining to N-modality by  Language-based Semantic Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+M">Munan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiaxi Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">HongFa Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yatian Pang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junwu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wancai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1065">[1065]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01957" title="Abstract">arXiv:2310.01957</a> (replaced) [<a href="/pdf/2310.01957" title="Download PDF">pdf</a>, <a href="/format/2310.01957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Driving with LLMs: Fusing Object-Level Vector Modality for Explainable  Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sinavski%2C+O">Oleg Sinavski</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCnermann%2C+J">Jan H&#xfc;nermann</a>, 
<a href="/search/cs?searchtype=author&query=Karnsund%2C+A">Alice Karnsund</a>, 
<a href="/search/cs?searchtype=author&query=Willmott%2C+A+J">Andrew James Willmott</a>, 
<a href="/search/cs?searchtype=author&query=Birch%2C+D">Danny Birch</a>, 
<a href="/search/cs?searchtype=author&query=Maund%2C+D">Daniel Maund</a>, 
<a href="/search/cs?searchtype=author&query=Shotton%2C+J">Jamie Shotton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1066">[1066]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02071" title="Abstract">arXiv:2310.02071</a> (replaced) [<a href="/pdf/2310.02071" title="Download PDF">pdf</a>, <a href="/format/2310.02071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards End-to-End Embodied Decision Making via Multi-modal Large  Language Model: Explorations with GPT4-Vision and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shuhuai Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haozhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zefan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuchi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+B">Baobao Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures, Code and data: <a href="https://github.com/pkunlp-icler/PCA-EVAL/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1067">[1067]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02156" title="Abstract">arXiv:2310.02156</a> (replaced) [<a href="/pdf/2310.02156" title="Download PDF">pdf</a>, <a href="/format/2310.02156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistically Rewired Message-Passing Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chendi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Manolache%2C+A">Andrei Manolache</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+K">Kareem Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhe Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Broeck%2C+G">Guy Van den Broeck</a>, 
<a href="/search/cs?searchtype=author&query=Niepert%2C+M">Mathias Niepert</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+C">Christopher Morris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item1068">[1068]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02234" title="Abstract">arXiv:2310.02234</a> (replaced) [<a href="/pdf/2310.02234" title="Download PDF">pdf</a>, <a href="/format/2310.02234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIS-AVoiDD: Modality Invariant and Specific Representation for  Audio-Visual Deepfake Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katamneni%2C+V+S">Vinaya Sree Katamneni</a>, 
<a href="/search/cs?searchtype=author&query=Rattani%2C+A">Ajita Rattani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1069">[1069]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02250" title="Abstract">arXiv:2310.02250</a> (replaced) [<a href="/pdf/2310.02250" title="Download PDF">pdf</a>, <a href="/format/2310.02250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why do autoencoders work?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kvalheim%2C+M+D">Matthew D. Kvalheim</a>, 
<a href="/search/cs?searchtype=author&query=Sontag%2C+E+D">Eduardo D. Sontag</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1070">[1070]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02299" title="Abstract">arXiv:2310.02299</a> (replaced) [<a href="/pdf/2310.02299" title="Download PDF">pdf</a>, <a href="/format/2310.02299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in  3D Physical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Smidt%2C+T+E">Tess E.Smidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1071">[1071]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02424" title="Abstract">arXiv:2310.02424</a> (replaced) [<a href="/pdf/2310.02424" title="Download PDF">pdf</a>, <a href="/format/2310.02424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AXNav: Replaying Accessibility Tests from Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taeb%2C+M">Maryam Taeb</a>, 
<a href="/search/cs?searchtype=author&query=Swearngin%2C+A">Amanda Swearngin</a>, 
<a href="/search/cs?searchtype=author&query=Schoop%2C+E">Eldon Schoop</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Ruijia Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Nichols%2C+J">Jeffrey Nichols</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1072">[1072]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02772" title="Abstract">arXiv:2310.02772</a> (replaced) [<a href="/pdf/2310.02772" title="Download PDF">pdf</a>, <a href="/format/2310.02772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spike Accumulation Forwarding for Effective Training of Spiking Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saiin%2C+R">Ryuji Saiin</a>, 
<a href="/search/cs?searchtype=author&query=Shirakawa%2C+T">Tomoya Shirakawa</a>, 
<a href="/search/cs?searchtype=author&query=Yoshihara%2C+S">Sota Yoshihara</a>, 
<a href="/search/cs?searchtype=author&query=Sawada%2C+Y">Yoshihide Sawada</a>, 
<a href="/search/cs?searchtype=author&query=Kusumoto%2C+H">Hiroyuki Kusumoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, Appendix:8 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1073">[1073]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02956" title="Abstract">arXiv:2310.02956</a> (replaced) [<a href="/pdf/2310.02956" title="Download PDF">pdf</a>, <a href="/format/2310.02956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Credit card score prediction using machine learning models: A new  dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arram%2C+A">Anas Arram</a>, 
<a href="/search/cs?searchtype=author&query=Ayob%2C+M">Masri Ayob</a>, 
<a href="/search/cs?searchtype=author&query=Albadr%2C+M+A+A">Musatafa Abbas Abbood Albadr</a>, 
<a href="/search/cs?searchtype=author&query=Sulaiman%2C+A">Alaa Sulaiman</a>, 
<a href="/search/cs?searchtype=author&query=Albashish%2C+D">Dheeb Albashish</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1074">[1074]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03166" title="Abstract">arXiv:2310.03166</a> (replaced) [<a href="/pdf/2310.03166" title="Download PDF">pdf</a>, <a href="/format/2310.03166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Raze to the Ground: Query-Efficient Adversarial HTML Attacks on  Machine-Learning Phishing Webpage Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Montaruli%2C+B">Biagio Montaruli</a>, 
<a href="/search/cs?searchtype=author&query=Demetrio%2C+L">Luca Demetrio</a>, 
<a href="/search/cs?searchtype=author&query=Pintor%2C+M">Maura Pintor</a>, 
<a href="/search/cs?searchtype=author&query=Compagna%2C+L">Luca Compagna</a>, 
<a href="/search/cs?searchtype=author&query=Balzarotti%2C+D">Davide Balzarotti</a>, 
<a href="/search/cs?searchtype=author&query=Biggio%2C+B">Battista Biggio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security (AISec '23), November 30, 2023, Copenhagen, Denmark
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1075">[1075]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03186" title="Abstract">arXiv:2310.03186</a> (replaced) [<a href="/pdf/2310.03186" title="Download PDF">pdf</a>, <a href="/format/2310.03186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Raju%2C+R+V">Rajkumar Vasudeva Raju</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Linderman%2C+S">Scott Linderman</a>, 
<a href="/search/q-bio?searchtype=author&query=Pitkow%2C+X">Xaq Pitkow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 4 figures and 1 supplementary figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1076">[1076]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03365" title="Abstract">arXiv:2310.03365</a> (replaced) [<a href="/pdf/2310.03365" title="Download PDF">pdf</a>, <a href="/format/2310.03365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video  Sequences Using Swin Transformer-Enhanced UNet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jafari%2C+H">Hossein Jafari</a>, 
<a href="/search/eess?searchtype=author&query=Faez%2C+K">Karim Faez</a>, 
<a href="/search/eess?searchtype=author&query=Amindavar%2C+H">Hamidreza Amindavar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1077">[1077]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03431" title="Abstract">arXiv:2310.03431</a> (replaced) [<a href="/pdf/2310.03431" title="Download PDF">pdf</a>, <a href="/format/2310.03431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on  Double Covering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+F">Fei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuhui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wencheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Hong Qin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Ying He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to ACM Transactions on Graphics (SIGGRAPH Asia 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1078">[1078]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03559" title="Abstract">arXiv:2310.03559</a> (replaced) [<a href="/pdf/2310.03559" title="Download PDF">pdf</a>, <a href="/format/2310.03559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yanwu Xu</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+L">Li Sun</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+W">Wei Peng</a>, 
<a href="/search/eess?searchtype=author&query=Pohl%2C+K">Kilian Pohl</a>, 
<a href="/search/eess?searchtype=author&query=Visweswaran%2C+S">Shyam Visweswaran</a>, 
<a href="/search/eess?searchtype=author&query=Batmanghelich%2C+K">Kayhan Batmanghelich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1079">[1079]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03870" title="Abstract">arXiv:2310.03870</a> (replaced) [<a href="/pdf/2310.03870" title="Download PDF">pdf</a>, <a href="/format/2310.03870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistency Regularization Improves Placenta Segmentation in Fetal EPI  MRI Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yingcheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Karani%2C+N">Neerav Karani</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+N">Neel Dey</a>, 
<a href="/search/cs?searchtype=author&query=Abulnaga%2C+S+M">S. Mazdak Abulnaga</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Junshen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Grant%2C+P+E">P. Ellen Grant</a>, 
<a href="/search/cs?searchtype=author&query=Turk%2C+E+A">Esra Abaci Turk</a>, 
<a href="/search/cs?searchtype=author&query=Golland%2C+P">Polina Golland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1080">[1080]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03906" title="Abstract">arXiv:2310.03906</a> (replaced) [<a href="/pdf/2310.03906" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PyDCM: Custom Data Center Models with Reinforcement Learning for  Sustainability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naug%2C+A">Avisek Naug</a>, 
<a href="/search/cs?searchtype=author&query=Guillen%2C+A">Antonio Guillen</a>, 
<a href="/search/cs?searchtype=author&query=Guti%C3%A9rrez%2C+R+L">Ricardo Luna Guti&#xe9;rrez</a>, 
<a href="/search/cs?searchtype=author&query=Gundecha%2C+V">Vineet Gundecha</a>, 
<a href="/search/cs?searchtype=author&query=Markovikj%2C+D">Dejan Markovikj</a>, 
<a href="/search/cs?searchtype=author&query=Kashyap%2C+L+D">Lekhapriya Dheeraj Kashyap</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+L">Lorenz Krause</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbanpour%2C+S">Sahand Ghorbanpour</a>, 
<a href="/search/cs?searchtype=author&query=Mousavi%2C+S">Sajad Mousavi</a>, 
<a href="/search/cs?searchtype=author&query=Babu%2C+A+R">Ashwin Ramesh Babu</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Soumyendu Sarkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation (BuildSys '23), November 15--16, 2023, Istanbul, Turkey
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1081">[1081]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04264" title="Abstract">arXiv:2310.04264</a> (replaced) [<a href="/pdf/2310.04264" title="Download PDF">pdf</a>, <a href="/format/2310.04264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep learning modelling of tip clearance variations on multi-stage axial  compressors aerodynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruni%2C+G">Giuseppe Bruni</a>, 
<a href="/search/cs?searchtype=author&query=Maleki%2C+S">Sepehr Maleki</a>, 
<a href="/search/cs?searchtype=author&query=Krishnababu%2C+S+K">Senthil K. Krishnababu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2306.05889">arXiv:2306.05889</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item1082">[1082]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04364" title="Abstract">arXiv:2310.04364</a> (replaced) [<a href="/pdf/2310.04364" title="Download PDF">pdf</a>, <a href="/format/2310.04364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Backpressure Routing Using Wireless Link Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhongyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+G">Gunjan Verma</a>, 
<a href="/search/cs?searchtype=author&query=Swami%2C+A">Ananthram Swami</a>, 
<a href="/search/cs?searchtype=author&query=Segarra%2C+S">Santiago Segarra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, accepted to IEEE CAMSAP 2023. arXiv admin note: text overlap with <a href="/abs/2211.10748">arXiv:2211.10748</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1083">[1083]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04396" title="Abstract">arXiv:2310.04396</a> (replaced) [<a href="/pdf/2310.04396" title="Download PDF">pdf</a>, <a href="/format/2310.04396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpolating Parametrized Quantum Circuits using Blackbox Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Simon%2C+L">Lars Simon</a>, 
<a href="/search/quant-ph?searchtype=author&query=Eble%2C+H">Holger Eble</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kowalski%2C+H">Hagen-Henrik Kowalski</a>, 
<a href="/search/quant-ph?searchtype=author&query=Radons%2C+M">Manuel Radons</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1084">[1084]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04482" title="Abstract">arXiv:2310.04482</a> (replaced) [<a href="/pdf/2310.04482" title="Download PDF">pdf</a>, <a href="/format/2310.04482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMOFM: Ensemble MLP mOdel with Feature-based Mixers for Click-Through  Rate Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y+B">Yujian Betterest Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kai Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1085">[1085]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05082" title="Abstract">arXiv:2310.05082</a> (replaced) [<a href="/pdf/2310.05082" title="Download PDF">pdf</a>, <a href="/format/2310.05082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-head mutual Mean-Teaching for semi-supervised medical image  segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+R">Ruifeng Bian</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenyi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weijin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huihua Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and logs will be available at <a href="https://github.com/Leesoon1984/CMMT-Net">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1086">[1086]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05128" title="Abstract">arXiv:2310.05128</a> (replaced) [<a href="/pdf/2310.05128" title="Download PDF">pdf</a>, <a href="/format/2310.05128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instances and Labels: Hierarchy-aware Joint Supervised Contrastive  Learning for Hierarchical Multi-Label Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=U%2C+S+C+L">Simon Chi Lok U</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jie He</a>, 
<a href="/search/cs?searchtype=author&query=Guti%C3%A9rrez-Basulto%2C+V">V&#xed;ctor Guti&#xe9;rrez-Basulto</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages; 10 figures. Published as a conference paper at EMNLP 2023 Findings (Long Paper). Code and data available at <a href="https://github.com/simonucl/HJCL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1087">[1087]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05269" title="Abstract">arXiv:2310.05269</a> (replaced) [<a href="/pdf/2310.05269" title="Download PDF">pdf</a>, <a href="/format/2310.05269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning: A Cutting-Edge Survey of the Latest Advancements and  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akhtarshenas%2C+A">Azim Akhtarshenas</a>, 
<a href="/search/cs?searchtype=author&query=Vahedifar%2C+M+A">Mohammad Ali Vahedifar</a>, 
<a href="/search/cs?searchtype=author&query=Ayoobi%2C+N">Navid Ayoobi</a>, 
<a href="/search/cs?searchtype=author&query=Maham%2C+B">Behrouz Maham</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+T">Tohid Alizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Ebrahimi%2C+S">Sina Ebrahimi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1088">[1088]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05317" title="Abstract">arXiv:2310.05317</a> (replaced) [<a href="/pdf/2310.05317" title="Download PDF">pdf</a>, <a href="/format/2310.05317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Long-form Text Generation in Mental Health with Task-adaptive  Tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sabour%2C+S">Sahand Sabour</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yilin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the main conference of The 2023 Conference on Empirical Methods in Natural Language Processing; 8 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing(EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1089">[1089]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05318" title="Abstract">arXiv:2310.05318</a> (replaced) [<a href="/pdf/2310.05318" title="Download PDF">pdf</a>, <a href="/format/2310.05318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resolving the Imbalance Issue in Hierarchical Disciplinary Topic  Inference via LLM-based Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xunxin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+M">Meng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+Z">Zhiyuan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuanchun Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, accepted by ICDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1090">[1090]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05351" title="Abstract">arXiv:2310.05351</a> (replaced) [<a href="/pdf/2310.05351" title="Download PDF">pdf</a>, <a href="/format/2310.05351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Neural Collapse for a Large Number of Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiachen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jinxin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>, 
<a href="/search/cs?searchtype=author&query=Mixon%2C+D">Dustin Mixon</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Chong You</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhihui Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1091">[1091]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05374" title="Abstract">arXiv:2310.05374</a> (replaced) [<a href="/pdf/2310.05374" title="Download PDF">pdf</a>, <a href="/format/2310.05374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving End-to-End Speech Processing by Efficient Text Data  Utilization with Latent Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jianqiao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenyong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nianzu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingshan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+Y+T">Yu Ting Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1092">[1092]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05446" title="Abstract">arXiv:2310.05446</a> (replaced) [<a href="/pdf/2310.05446" title="Download PDF">pdf</a>, <a href="/format/2310.05446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RetSeg: Retention-based Colorectal Polyps Segmentation Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=ELKarazle%2C+K">Khaled ELKarazle</a>, 
<a href="/search/eess?searchtype=author&query=Raman%2C+V">Valliappan Raman</a>, 
<a href="/search/eess?searchtype=author&query=Chua%2C+C">Caslon Chua</a>, 
<a href="/search/eess?searchtype=author&query=Then%2C+P">Patrick Then</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version with a PDF
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1093">[1093]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05483" title="Abstract">arXiv:2310.05483</a> (replaced) [<a href="/pdf/2310.05483" title="Download PDF">pdf</a>, <a href="/format/2310.05483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with  Sparse Views
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiawei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1094">[1094]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05647" title="Abstract">arXiv:2310.05647</a> (replaced) [<a href="/pdf/2310.05647" title="Download PDF">pdf</a>, <a href="/format/2310.05647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Manifold Structured Data Priors for Improved MR  Fingerprinting Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/eess?searchtype=author&query=Ji%2C+Y">Yuping Ji</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+Y">Yue Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 10 figures, will submit to IEEE Transactions on Medical Imaging
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1095">[1095]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05719" title="Abstract">arXiv:2310.05719</a> (replaced) [<a href="/pdf/2310.05719" title="Download PDF">pdf</a>, <a href="/format/2310.05719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer Fusion with Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imfeld%2C+M">Moritz Imfeld</a>, 
<a href="/search/cs?searchtype=author&query=Graldi%2C+J">Jacopo Graldi</a>, 
<a href="/search/cs?searchtype=author&query=Giordano%2C+M">Marco Giordano</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostidis%2C+S">Sotiris Anagnostidis</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S+P">Sidak Pal Singh</a> (ETH Zurich)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> M. Imfeld, J. Graldi, and M. Giordano are the first authors and contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1096">[1096]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05720" title="Abstract">arXiv:2310.05720</a> (replaced) [<a href="/pdf/2310.05720" title="Download PDF">pdf</a>, <a href="/format/2310.05720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperLips: Hyper Control Lips with High Resolution Decoder for Talking  Face Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yaosen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Han Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xuming Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1097">[1097]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05793" title="Abstract">arXiv:2310.05793</a> (replaced) [<a href="/pdf/2310.05793" title="Download PDF">pdf</a>, <a href="/format/2310.05793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for  Accelerated Seq2Seq Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shansan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mukai Li</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiangtao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings Camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1098">[1098]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06121" title="Abstract">arXiv:2310.06121</a> (replaced) [<a href="/pdf/2310.06121" title="Download PDF">pdf</a>, <a href="/ps/2310.06121" title="Download PostScript">ps</a>, <a href="/format/2310.06121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Innermost to Full Almost-Sure Termination of Probabilistic Term  Rewriting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kassing%2C+J">Jan-Christoph Kassing</a>, 
<a href="/search/cs?searchtype=author&query=Frohn%2C+F">Florian Frohn</a>, 
<a href="/search/cs?searchtype=author&query=Giesl%2C+J">J&#xfc;rgen Giesl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item1099">[1099]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06155" title="Abstract">arXiv:2310.06155</a> (replaced) [<a href="/pdf/2310.06155" title="Download PDF">pdf</a>, <a href="/format/2310.06155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How AI Processing Delays Foster Creativity: Exploring Research Question  Co-Creation with an LLM-based Agent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiren Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Si Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Haocong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mengxia Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+X">Xiao Ran</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+A">Andrew Mo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yiliu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yun Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item1100">[1100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06313" title="Abstract">arXiv:2310.06313</a> (replaced) [<a href="/pdf/2310.06313" title="Download PDF">pdf</a>, <a href="/format/2310.06313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Pose-Guided Image Synthesis with Progressive Conditional  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+F">Fei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Hu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wei Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1101">[1101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06446" title="Abstract">arXiv:2310.06446</a> (replaced) [<a href="/pdf/2310.06446" title="Download PDF">pdf</a>, <a href="/format/2310.06446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rule Mining for Correcting Classification Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+H">Hirofumi Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Iwashita%2C+H">Hiroaki Iwashita</a>, 
<a href="/search/cs?searchtype=author&query=Takagi%2C+T">Takuya Takagi</a>, 
<a href="/search/cs?searchtype=author&query=Fujishige%2C+Y">Yuta Fujishige</a>, 
<a href="/search/cs?searchtype=author&query=Hara%2C+S">Satoshi Hara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1102">[1102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06511" title="Abstract">arXiv:2310.06511</a> (replaced) [<a href="/pdf/2310.06511" title="Download PDF">pdf</a>, <a href="/format/2310.06511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Dataset Distillation for Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D+B">Dong Bok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seanie Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Joonho Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Juho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1103">[1103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06835" title="Abstract">arXiv:2310.06835</a> (replaced) [<a href="/pdf/2310.06835" title="Download PDF">pdf</a>, <a href="/format/2310.06835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherji%2C+K">Kaustuv Mukherji</a>, 
<a href="/search/cs?searchtype=author&query=Parkar%2C+D">Devendra Parkar</a>, 
<a href="/search/cs?searchtype=author&query=Pokala%2C+L">Lahari Pokala</a>, 
<a href="/search/cs?searchtype=author&query=Aditya%2C+D">Dyuman Aditya</a>, 
<a href="/search/cs?searchtype=author&query=Shakarian%2C+P">Paulo Shakarian</a>, 
<a href="/search/cs?searchtype=author&query=Dorman%2C+C">Clark Dorman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 IEEE International Conference on Semantic Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item1104">[1104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07123" title="Abstract">arXiv:2310.07123</a> (replaced) [<a href="/pdf/2310.07123" title="Download PDF">pdf</a>, <a href="/format/2310.07123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Off-Policy Evaluation for Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Q">Qitong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Ge Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Juncheng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tarokh%2C+V">Vahid Tarokh</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+M">Min Chi</a>, 
<a href="/search/cs?searchtype=author&query=Pajic%2C+M">Miroslav Pajic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1105">[1105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07173" title="Abstract">arXiv:2310.07173</a> (replaced) [<a href="/pdf/2310.07173" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing quantum algorithms with Qinterpreter: bridging the gap  between theory and practice across leading quantum computing platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Sep%C3%BAlveda%2C+W+C">Wilmer Contreras Sep&#xfa;lveda</a>, 
<a href="/search/quant-ph?searchtype=author&query=Torres-Palencia%2C+%C3%81+D">&#xc1;ngel David Torres-Palencia</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mondrag%C3%B3n%2C+J+J+S">Jos&#xe9; Javier S&#xe1;nchez Mondrag&#xf3;n</a>, 
<a href="/search/quant-ph?searchtype=author&query=Villegas-Mart%C3%ADnez%2C+B+M">Braulio Misael Villegas-Mart&#xed;nez</a>, 
<a href="/search/quant-ph?searchtype=author&query=Escobedo-Alatorre%2C+J+J">J. Jes&#xfa;s Escobedo-Alatorre</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gesing%2C+S">Sandra Gesing</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lozano-Cris%C3%B3stomo%2C+N">N&#xe9;stor Lozano-Cris&#xf3;stomo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Garc%C3%ADa-Melgarejo%2C+J+C">Julio C&#xe9;sar Garc&#xed;a-Melgarejo</a>, 
<a href="/search/quant-ph?searchtype=author&query=P%C3%A9rez%2C+J+C+S">Juan Carlos S&#xe1;nchez P&#xe9;rez</a>, 
<a href="/search/quant-ph?searchtype=author&query=P%C3%A9rez%2C+E+N+P">Eddie Nelson Palacios- P&#xe9;rez</a>, 
<a href="/search/quant-ph?searchtype=author&query=PalilleroSandoval%2C+O">Omar PalilleroSandoval</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item1106">[1106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07248" title="Abstract">arXiv:2310.07248</a> (replaced) [<a href="/pdf/2310.07248" title="Download PDF">pdf</a>, <a href="/format/2310.07248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via  Improved Box-dice and Contrastive Latent-anchors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hongkuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Li He</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+M">Man He</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+W">Wenxuan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Ting Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yitong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dun Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1107">[1107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07250" title="Abstract">arXiv:2310.07250</a> (replaced) [<a href="/e-print/2310.07250" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Missing MRI Sequences from Available Modalities using  Generative Adversarial Networks in BraTS Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Hamamci%2C+I+E">Ibrahim Ethem Hamamci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It will be edited and later uploaded
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1108">[1108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07284" title="Abstract">arXiv:2310.07284</a> (replaced) [<a href="/pdf/2310.07284" title="Download PDF">pdf</a>, <a href="/format/2310.07284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Typing to Listen at the Cocktail Party: Text-Guided Target Speaker  Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hao%2C+X">Xiang Hao</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Jibin Wu</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+J">Jianwei Yu</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+C">Chenglin Xu</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+K+C">Kay Chen Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review, <a href="https://github.com/haoxiangsnr/llm-tse">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1109">[1109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07325" title="Abstract">arXiv:2310.07325</a> (replaced) [<a href="/pdf/2310.07325" title="Download PDF">pdf</a>, <a href="/format/2310.07325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adversarial Example for Direct Logit Attribution: Memory Management  in gelu-4l
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dao%2C+J">James Dao</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+Y">Yeu-Tong Lau</a>, 
<a href="/search/cs?searchtype=author&query=Rager%2C+C">Can Rager</a>, 
<a href="/search/cs?searchtype=author&query=Janiak%2C+J">Jett Janiak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1110">[1110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07437" title="Abstract">arXiv:2310.07437</a> (replaced) [<a href="/pdf/2310.07437" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Branched Deep Convolutional Network for Forecasting the Occurrence of  Hazes in Paris using Meteorological Maps with Different Characteristic  Spatial Scales
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+C">Chien Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1111">[1111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07534" title="Abstract">arXiv:2310.07534</a> (replaced) [<a href="/pdf/2310.07534" title="Download PDF">pdf</a>, <a href="/format/2310.07534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Centered Evaluation of XAI Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dawoud%2C+K">Karam Dawoud</a>, 
<a href="/search/cs?searchtype=author&query=Samek%2C+W">Wojciech Samek</a>, 
<a href="/search/cs?searchtype=author&query=Eisert%2C+P">Peter Eisert</a>, 
<a href="/search/cs?searchtype=author&query=Lapuschkin%2C+S">Sebastian Lapuschkin</a>, 
<a href="/search/cs?searchtype=author&query=Bosse%2C+S">Sebastian Bosse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1112">[1112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07652" title="Abstract">arXiv:2310.07652</a> (replaced) [<a href="/pdf/2310.07652" title="Download PDF">pdf</a>, <a href="/format/2310.07652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM4Vis: Explainable Visualization Recommendation using ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+E">Ee-Peng Lim</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Industry Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1113">[1113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07671" title="Abstract">arXiv:2310.07671</a> (replaced) [<a href="/pdf/2310.07671" title="Download PDF">pdf</a>, <a href="/format/2310.07671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovery of Novel Reticular Materials for Carbon Dioxide Capture using  GFlowNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cipcigan%2C+F">Flaviu Cipcigan</a>, 
<a href="/search/cs?searchtype=author&query=Booth%2C+J">Jonathan Booth</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+R+N+B">Rodrigo Neumann Barros Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+C+R+d">Carine Ribeiro dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Steiner%2C+M">Mathias Steiner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
</div>
</dd>
<dt><a name="item1114">[1114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07678" title="Abstract">arXiv:2310.07678</a> (replaced) [<a href="/pdf/2310.07678" title="Download PDF">pdf</a>, <a href="/format/2310.07678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Livieris%2C+I+E">Ioannis E. Livieris</a>, 
<a href="/search/cs?searchtype=author&query=Pintelas%2C+E">Emmanuel Pintelas</a>, 
<a href="/search/cs?searchtype=author&query=Kiriakidou%2C+N">Niki Kiriakidou</a>, 
<a href="/search/cs?searchtype=author&query=Pintelas%2C+P">Panagiotis Pintelas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The manuscript has been accepted for publication in "Journal of Imaging"
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Livieris IE, Pintelas E, Kiriakidou N, Pintelas P. Explainable
  Image Similarity: Integrating Siamese Networks and Grad-CAM. Journal of
  Imaging. 2023; 9(10):224
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1115">[1115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07786" title="Abstract">arXiv:2310.07786</a> (replaced) [<a href="/pdf/2310.07786" title="Download PDF">pdf</a>, <a href="/format/2310.07786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zheqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yueyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+X">Xu Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Van+Roy%2C+B">Benjamin Van Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1116">[1116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07818" title="Abstract">arXiv:2310.07818</a> (replaced) [<a href="/e-print/2310.07818" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Relationship between Analogy Identification and Sentence  Structure Encoding in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wijesiriwardene%2C+T">Thilini Wijesiriwardene</a>, 
<a href="/search/cs?searchtype=author&query=Wickramarachchi%2C+R">Ruwan Wickramarachchi</a>, 
<a href="/search/cs?searchtype=author&query=Reganti%2C+A+N">Aishwarya Naresh Reganti</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vinija Jain</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A">Amit Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been withdrawn due to a mistake in submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1117">[1117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07837" title="Abstract">arXiv:2310.07837</a> (replaced) [<a href="/pdf/2310.07837" title="Download PDF">pdf</a>, <a href="/format/2310.07837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Feature Sparsity in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+M">Mingyang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+L">Lucas Tao</a>, 
<a href="/search/cs?searchtype=author&query=Benton%2C+J">Joe Benton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1118">[1118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07923" title="Abstract">arXiv:2310.07923</a> (replaced) [<a href="/pdf/2310.07923" title="Download PDF">pdf</a>, <a href="/ps/2310.07923" title="Download PostScript">ps</a>, <a href="/format/2310.07923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Expresssive Power of Transformers with Chain of Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Merrill%2C+W">William Merrill</a>, 
<a href="/search/cs?searchtype=author&query=Sabharwal%2C+A">Ashish Sabharwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9-page preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Computation and Language (cs.CL); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item1119">[1119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08097" title="Abstract">arXiv:2310.08097</a> (replaced) [<a href="/pdf/2310.08097" title="Download PDF">pdf</a>, <a href="/format/2310.08097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentinel: An Aggregation Function to Secure Decentralized Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Celdran%2C+A+H">Alberto Huertas Celdran</a>, 
<a href="/search/cs?searchtype=author&query=Baltensperger%2C+J">Janosch Baltensperger</a>, 
<a href="/search/cs?searchtype=author&query=Beltran%2C+E+T+M">Enrique Tomas Martinez Beltran</a>, 
<a href="/search/cs?searchtype=author&query=Bovet%2C+G">Gerome Bovet</a>, 
<a href="/search/cs?searchtype=author&query=Stiller%2C+B">Burkhard Stiller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1120">[1120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08101" title="Abstract">arXiv:2310.08101</a> (replaced) [<a href="/pdf/2310.08101" title="Download PDF">pdf</a>, <a href="/format/2310.08101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Promptor: A Conversational and Autonomous Prompt Generation Agent for  Intelligent Text Entry Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Junxiao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Dudley%2C+J+J">John J. Dudley</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jingyao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Byrne%2C+B">Bill Byrne</a>, 
<a href="/search/cs?searchtype=author&query=Kristensson%2C+P+O">Per Ola Kristensson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1121">[1121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08139" title="Abstract">arXiv:2310.08139</a> (replaced) [<a href="/pdf/2310.08139" title="Download PDF">pdf</a>, <a href="/format/2310.08139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DualAug: Exploiting Additional Heavy Augmentation with OOD Data  Rejection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zehao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qizhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guanglei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1122">[1122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08152" title="Abstract">arXiv:2310.08152</a> (replaced) [<a href="/pdf/2310.08152" title="Download PDF">pdf</a>, <a href="/format/2310.08152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context Compression for Auto-regressive Transformers with Sentinel  Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Siyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Q">Qi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K+Q">Kenny Q. Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1123">[1123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08157" title="Abstract">arXiv:2310.08157</a> (replaced) [<a href="/pdf/2310.08157" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCRepair: Multi-Chunk Program Repair via Patch Optimization with Buggy  Block
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jisung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byeongjung Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> This is the revised manuscript of the conference paper published
  in the 38th ACM/SIGAPP Symposium on Applied Computing (SAC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1124">[1124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08328" title="Abstract">arXiv:2310.08328</a> (replaced) [<a href="/pdf/2310.08328" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for  Traffic Flow Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bailong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhizhen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuefei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures. Spatial self-attention of this work extends AAAI23 - PDFormer(<a href="/abs/2301.07945">arXiv:2301.07945</a>) by other authors, cited as Ref. [17]. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1125">[1125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08372" title="Abstract">arXiv:2310.08372</a> (replaced) [<a href="/pdf/2310.08372" title="Download PDF">pdf</a>, <a href="/format/2310.08372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Factual Consistency for Knowledge-Grounded Dialogue Systems  via Knowledge Enhancement and Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Boyang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1126">[1126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08384" title="Abstract">arXiv:2310.08384</a> (replaced) [<a href="/pdf/2310.08384" title="Download PDF">pdf</a>, <a href="/format/2310.08384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Running Time Analysis of Interactive Multi-objective  Evolutionary Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tianhao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+C">Chao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chao Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item1127">[1127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08419" title="Abstract">arXiv:2310.08419</a> (replaced) [<a href="/pdf/2310.08419" title="Download PDF">pdf</a>, <a href="/format/2310.08419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jailbreaking Black Box Large Language Models in Twenty Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chao%2C+P">Patrick Chao</a>, 
<a href="/search/cs?searchtype=author&query=Robey%2C+A">Alexander Robey</a>, 
<a href="/search/cs?searchtype=author&query=Dobriban%2C+E">Edgar Dobriban</a>, 
<a href="/search/cs?searchtype=author&query=Hassani%2C+H">Hamed Hassani</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+G+J">George J. Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+E">Eric Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1128">[1128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08421" title="Abstract">arXiv:2310.08421</a> (replaced) [<a href="/pdf/2310.08421" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SegLoc: Novel Visual Self-supervised Learning Scheme for Dense  Prediction Tasks of Security Inspection X-ray Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halat%2C+S">Shervin Halat</a>, 
<a href="/search/cs?searchtype=author&query=Rahmati%2C+M">Mohammad Rahmati</a>, 
<a href="/search/cs?searchtype=author&query=Nazerfard%2C+E">Ehsan Nazerfard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1129">[1129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08459" title="Abstract">arXiv:2310.08459</a> (replaced) [<a href="/pdf/2310.08459" title="Download PDF">pdf</a>, <a href="/format/2310.08459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Heterogeneous Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+R">Runxue Bao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yuhe Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhi-Hong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Ye Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1130">[1130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08577" title="Abstract">arXiv:2310.08577</a> (replaced) [<a href="/pdf/2310.08577" title="Download PDF">pdf</a>, <a href="/format/2310.08577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Data-Type Understanding does not emerge from Scaling  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Udandarao%2C+V">Vishaal Udandarao</a>, 
<a href="/search/cs?searchtype=author&query=Burg%2C+M+F">Max F. Burg</a>, 
<a href="/search/cs?searchtype=author&query=Albanie%2C+S">Samuel Albanie</a>, 
<a href="/search/cs?searchtype=author&query=Bethge%2C+M">Matthias Bethge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1131">[1131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08595" title="Abstract">arXiv:2310.08595</a> (replaced) [<a href="/pdf/2310.08595" title="Download PDF">pdf</a>, <a href="/format/2310.08595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning for Autonomous Vehicle Intersection  Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elallid%2C+B+B">Badr Ben Elallid</a>, 
<a href="/search/cs?searchtype=author&query=Alaoui%2C+H+E">Hamza El Alaoui</a>, 
<a href="/search/cs?searchtype=author&query=Benamar%2C+N">Nabil Benamar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the 2023 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1132">[1132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08609" title="Abstract">arXiv:2310.08609</a> (replaced) [<a href="/pdf/2310.08609" title="Download PDF">pdf</a>, <a href="/format/2310.08609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized shock-protecting microstructures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+Z">Zizhou Huang</a>, 
<a href="/search/math?searchtype=author&query=Panozzo%2C+D">Daniele Panozzo</a>, 
<a href="/search/math?searchtype=author&query=Zorin%2C+D">Denis Zorin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1133">[1133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08686" title="Abstract">arXiv:2310.08686</a> (replaced) [<a href="/pdf/2310.08686" title="Download PDF">pdf</a>, <a href="/format/2310.08686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IMU Preintegration for Multi-Robot Systems in the Presence of Bias and  Communication Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shalaby%2C+M+A">Mohammed Ayman Shalaby</a>, 
<a href="/search/cs?searchtype=author&query=Cossette%2C+C+C">Charles Champagne Cossette</a>, 
<a href="/search/cs?searchtype=author&query=Ny%2C+J+L">Jerome Le Ny</a>, 
<a href="/search/cs?searchtype=author&query=Forbes%2C+J+R">James Richard Forbes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1134">[1134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08782" title="Abstract">arXiv:2310.08782</a> (replaced) [<a href="/pdf/2310.08782" title="Download PDF">pdf</a>, <a href="/format/2310.08782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced  Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yimeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Aochuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinghan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiancheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gaowen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+M">Mingyi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shiyu Chang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1135">[1135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08820" title="Abstract">arXiv:2310.08820</a> (replaced) [<a href="/pdf/2310.08820" title="Download PDF">pdf</a>, <a href="/format/2310.08820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAM-guided Unsupervised Domain Adaptation for 3D Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xidong Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Runnan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+F">Feng Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingdong Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Youquan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinge Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuexin Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1136">[1136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08864" title="Abstract">arXiv:2310.08864</a> (replaced) [<a href="/pdf/2310.08864" title="Download PDF">pdf</a>, <a href="/format/2310.08864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open X-Embodiment: Robotic Learning Datasets and RT-X Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Open+X-Embodiment+Collaboration">Open X-Embodiment Collaboration</a>, 
<a href="/search/cs?searchtype=author&query=Padalkar%2C+A">Abhishek Padalkar</a>, 
<a href="/search/cs?searchtype=author&query=Pooley%2C+A">Acorn Pooley</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Ajinkya Jain</a>, 
<a href="/search/cs?searchtype=author&query=Bewley%2C+A">Alex Bewley</a>, 
<a href="/search/cs?searchtype=author&query=Herzog%2C+A">Alex Herzog</a>, 
<a href="/search/cs?searchtype=author&query=Irpan%2C+A">Alex Irpan</a>, 
<a href="/search/cs?searchtype=author&query=Khazatsky%2C+A">Alexander Khazatsky</a>, 
<a href="/search/cs?searchtype=author&query=Rai%2C+A">Anant Rai</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Anikait Singh</a>, 
<a href="/search/cs?searchtype=author&query=Brohan%2C+A">Anthony Brohan</a>, 
<a href="/search/cs?searchtype=author&query=Raffin%2C+A">Antonin Raffin</a>, 
<a href="/search/cs?searchtype=author&query=Wahid%2C+A">Ayzaan Wahid</a>, 
<a href="/search/cs?searchtype=author&query=Burgess-Limerick%2C+B">Ben Burgess-Limerick</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Beomjoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Ichter%2C+B">Brian Ichter</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cewu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Charles Xu</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenfeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+C">Cheng Chi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chenguang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+C">Christine Chan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chuer Pan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chuyuan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Devin%2C+C">Coline Devin</a>, 
<a href="/search/cs?searchtype=author&query=Driess%2C+D">Danny Driess</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+D">Deepak Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Dhruv Shah</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCchler%2C+D">Dieter B&#xfc;chler</a>, 
<a href="/search/cs?searchtype=author&query=Kalashnikov%2C+D">Dmitry Kalashnikov</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>, 
<a href="/search/cs?searchtype=author&query=Johns%2C+E">Edward Johns</a>, 
<a href="/search/cs?searchtype=author&query=Ceola%2C+F">Federico Ceola</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Stulp%2C+F">Freek Stulp</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Gaoyue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sukhatme%2C+G+S">Gaurav S. Sukhatme</a>, 
<a href="/search/cs?searchtype=author&query=Salhotra%2C+G">Gautam Salhotra</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+G">Ge Yan</a>, 
<a href="/search/cs?searchtype=author&query=Schiavi%2C+G">Giulio Schiavi</a>, 
<a href="/search/cs?searchtype=author&query=Kahn%2C+G">Gregory Kahn</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Hao-Shu Fang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haochen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Amor%2C+H+B">Heni Ben Amor</a>, 
<a href="/search/cs?searchtype=author&query=Christensen%2C+H+I">Henrik I Christensen</a>, 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Walke%2C+H">Homer Walke</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Hongjie Fang</a>, 
<a href="/search/cs?searchtype=author&query=Mordatch%2C+I">Igor Mordatch</a>, 
<a href="/search/cs?searchtype=author&query=Radosavovic%2C+I">Ilija Radosavovic</a>,  et al. (124 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1137">[1137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08929" title="Abstract">arXiv:2310.08929</a> (replaced) [<a href="/pdf/2310.08929" title="Download PDF">pdf</a>, <a href="/format/2310.08929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Interpretable Controllability in Object-Centric Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Janghyuk Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jaehyun Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Changyeon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+H">Ho-Jin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S+J">Seon Joo Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1138">[1138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09254" title="Abstract">arXiv:2310.09254</a> (replaced) [<a href="/pdf/2310.09254" title="Download PDF">pdf</a>, <a href="/format/2310.09254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Entropic Neural Optimal Transport To Map Within and Across  Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Klein%2C+D">Dominik Klein</a>, 
<a href="/search/stat?searchtype=author&query=Uscidda%2C+T">Th&#xe9;o Uscidda</a>, 
<a href="/search/stat?searchtype=author&query=Theis%2C+F">Fabian Theis</a>, 
<a href="/search/stat?searchtype=author&query=Cuturi%2C+M">Marco Cuturi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item631">Cross-lists</a></li>
<li><a href="#item703">Replacements</a></li>
</ul>
<small>[ total of 1138 entries:  <b>1-1138</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
