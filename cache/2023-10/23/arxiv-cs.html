<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Thu 19 Oct 23  to  Fri 20 Oct 23, announced Mon, 23 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item329">Cross-lists</a></li>
<li><a href="#item373">Replacements</a></li>
</ul>
<small>[ total of 608 entries:  <b>1-608</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Mon, 23 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12985" title="Abstract">arXiv:2310.12985</a> [<a href="/pdf/2310.12985" title="Download PDF">pdf</a>, <a href="/format/2310.12985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling energy-Efficient object detection with surrogate gradient  descent in spiking neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jilong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shanlin Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yinsheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiyi Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Spiking Neural Networks (SNNs) are a biologically plausible neural network
model with significant advantages in both event-driven processing and
spatio-temporal information processing, rendering SNNs an appealing choice for
energyefficient object detection. However, the non-differentiability of the
biological neuronal dynamics model presents a challenge during the training of
SNNs. Furthermore, a suitable decoding strategy for object detection in SNNs is
currently lacking. In this study, we introduce the Current Mean Decoding (CMD)
method, which solves the regression problem to facilitate the training of deep
SNNs for object detection tasks. Based on the gradient surrogate and CMD, we
propose the SNN-YOLOv3 model for object detection. Our experiments demonstrate
that SNN-YOLOv3 achieves a remarkable performance with an mAP of 61.87% on the
PASCAL VOC dataset, requiring only 6 time steps. Compared to SpikingYOLO, we
have managed to increase mAP by nearly 10% while reducing energy consumption by
two orders of magnitude.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12986" title="Abstract">arXiv:2310.12986</a> [<a href="/pdf/2310.12986" title="Download PDF">pdf</a>, <a href="/format/2310.12986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A survey of manifold learning and its applications for multimedia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fassold%2C+H">Hannes Fassold</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for ICVSP 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Manifold learning is an emerging research domain of machine learning. In this
work, we give an introduction into manifold learning and how it is employed for
important application fields in multimedia.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12989" title="Abstract">arXiv:2310.12989</a> [<a href="/pdf/2310.12989" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Health Data Interoperability with Large Language Models: A  FHIR Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yikuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yerebakan%2C+H">Halid Yerebakan</a>, 
<a href="/search/cs?searchtype=author&query=Shinagawa%2C+Y">Yoshihisa Shinagawa</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 AMIA IS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this study, we investigated the ability of the large language model (LLM)
to enhance healthcare data interoperability. We leveraged the LLM to convert
clinical texts into their corresponding FHIR resources. Our experiments,
conducted on 3,671 snippets of clinical text, demonstrated that the LLM not
only streamlines the multi-step natural language processing and human
calibration processes but also achieves an exceptional accuracy rate of over
90% in exact matches when compared to human annotations.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12990" title="Abstract">arXiv:2310.12990</a> [<a href="/pdf/2310.12990" title="Download PDF">pdf</a>, <a href="/format/2310.12990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wave-informed dictionary learning for high-resolution imaging in complex  media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moscoso%2C+M">Miguel Moscoso</a>, 
<a href="/search/cs?searchtype=author&query=Novikov%2C+A">Alexei Novikov</a>, 
<a href="/search/cs?searchtype=author&query=Papanicolaou%2C+G">George Papanicolaou</a>, 
<a href="/search/cs?searchtype=author&query=Tsogka%2C+C">Chrysoula Tsogka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP); Optimization and Control (math.OC)

</div>
<p class="mathjax">We propose an approach for imaging in scattering media when large and diverse
data sets are available. It has two steps. Using a dictionary learning
algorithm the first step estimates the true Green's function vectors as columns
in an unordered sensing matrix. The array data comes from many sparse sets of
sources whose location and strength are not known to us. In the second step,
the columns of the estimated sensing matrix are ordered for imaging using
Multi-Dimensional Scaling with connectivity information derived from
cross-correlations of its columns, as in time reversal. For these two steps to
work together we need data from large arrays of receivers so the columns of the
sensing matrix are incoherent for the first step, as well as from sub-arrays so
that they are coherent enough to obtain the connectivity needed in the second
step. Through simulation experiments, we show that the proposed approach is
able to provide images in complex media whose resolution is that of a
homogeneous medium.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12995" title="Abstract">arXiv:2310.12995</a> [<a href="/pdf/2310.12995" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comprehensive Multimodal Segmentation in Medical Imaging: Combining  YOLOv8 with SAM and HQ-SAM Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S">Sumit Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kuan-Fu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dam%2C+E+B">Erik B. Dam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces a comprehensive approach for segmenting regions of
interest (ROI) in diverse medical imaging datasets, encompassing ultrasound, CT
scans, and X-ray images. The proposed method harnesses the capabilities of the
YOLOv8 model for approximate boundary box detection across modalities,
alongside the Segment Anything Model (SAM) and High Quality (HQ) SAM for fully
automatic and precise segmentation. To generate boundary boxes, the YOLOv8
model was trained using a limited set of 100 images and masks from each
modality. The results obtained from our approach are extensively computed and
analyzed, demonstrating its effectiveness and potential in medical image
analysis. Various evaluation metrics, including precision, recall, F1 score,
and Dice Score, were employed to quantify the accuracy of the segmentation
results. A comparative analysis was conducted to assess the individual and
combined performance of the YOLOv8, YOLOv8+SAM, and YOLOv8+HQ-SAM models. The
results indicate that the SAM model performs better than the other two models,
exhibiting higher segmentation accuracy and overall performance. While HQ-SAM
offers potential advantages, its incremental gains over the standard SAM model
may not justify the additional computational cost. The YOLOv8+SAM model shows
promise for enhancing medical image segmentation and its clinical implications.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12997" title="Abstract">arXiv:2310.12997</a> [<a href="/pdf/2310.12997" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parking Spot Classification based on surround view camera system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+A">Andy Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Doshi%2C+D">Deep Doshi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gorantla%2C+H">Harsha Gorantla</a>, 
<a href="/search/cs?searchtype=author&query=Heitzmann%2C+T">Thomas Heitzmann</a>, 
<a href="/search/cs?searchtype=author&query=Groth%2C+P">Peter Groth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SPIE Optical Engineering + Applications, 2023, San Diego, California, United States. Proc. SPIE 12675, Applications of Machine Learning 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Surround-view fisheye cameras are commonly used for near-field sensing in
automated driving scenarios, including urban driving and auto valet parking.
Four fisheye cameras, one on each side, are sufficient to cover 360{\deg}
around the vehicle capturing the entire near-field region. Based on surround
view cameras, there has been much research on parking slot detection with main
focus on the occupancy status in recent years, but little work on whether the
free slot is compatible with the mission of the ego vehicle or not. For
instance, some spots are handicap or electric vehicles accessible only. In this
paper, we tackle parking spot classification based on the surround view camera
system. We adapt the object detection neural network YOLOv4 with a novel
polygon bounding box model that is well-suited for various shaped parking
spaces, such as slanted parking slots. To the best of our knowledge, we present
the first detailed study on parking spot detection and classification on
fisheye cameras for auto valet parking scenarios. The results prove that our
proposed classification approach is effective to distinguish between regular,
electric vehicle, and handicap parking spots.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12999" title="Abstract">arXiv:2310.12999</a> [<a href="/pdf/2310.12999" title="Download PDF">pdf</a>, <a href="/format/2310.12999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Dynamic Programming for Energy-Efficient Base Station Cell  Switching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Junliang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y+T">Yi Tian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jenkin%2C+M">Michael Jenkin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dudek%2C+G">Gregory Dudek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Energy saving in wireless networks is growing in importance due to increasing
demand for evolving new-gen cellular networks, environmental and regulatory
concerns, and potential energy crises arising from geopolitical tensions. In
this work, we propose an approximate dynamic programming (ADP)-based method
coupled with online optimization to switch on/off the cells of base stations to
reduce network power consumption while maintaining adequate Quality of Service
(QoS) metrics. We use a multilayer perceptron (MLP) given each state-action
pair to predict the power consumption to approximate the value function in ADP
for selecting the action with optimal expected power saved. To save the largest
possible power consumption without deteriorating QoS, we include another MLP to
predict QoS and a long short-term memory (LSTM) for predicting handovers,
incorporated into an online optimization algorithm producing an adaptive QoS
threshold for filtering cell switching actions based on the overall QoS
history. The performance of the method is evaluated using a practical network
simulator with various real-world scenarios with dynamic traffic patterns.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13000" title="Abstract">arXiv:2310.13000</a> [<a href="/pdf/2310.13000" title="Download PDF">pdf</a>, <a href="/format/2310.13000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Document-Level Relation Extraction with Relation Correlation Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yusheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouhan Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Document-level relation extraction (DocRE) is a task that focuses on
identifying relations between entities within a document. However, existing
DocRE models often overlook the correlation between relations and lack a
quantitative analysis of relation correlations. To address this limitation and
effectively capture relation correlations in DocRE, we propose a relation graph
method, which aims to explicitly exploit the interdependency among relations.
Firstly, we construct a relation graph that models relation correlations using
statistical co-occurrence information derived from prior relation knowledge.
Secondly, we employ a re-weighting scheme to create an effective relation
correlation matrix to guide the propagation of relation information.
Furthermore, we leverage graph attention networks to aggregate relation
embeddings. Importantly, our method can be seamlessly integrated as a
plug-and-play module into existing models. Experimental results demonstrate
that our approach can enhance the performance of multi-relation extraction,
highlighting the effectiveness of considering relation correlations in DocRE.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13001" title="Abstract">arXiv:2310.13001</a> [<a href="/pdf/2310.13001" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversational Financial Information Retrieval Model (ConFIRM)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Stephen Choi</a>, 
<a href="/search/cs?searchtype=author&query=Gazeley%2C+W">William Gazeley</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+S+H">Siu Ho Wong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tingting Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures, 2 tables, 2 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the exponential growth in large language models (LLMs), leveraging their
emergent properties for specialized domains like finance merits exploration.
However, regulated fields such as finance pose unique constraints, requiring
domain-optimized frameworks. We present ConFIRM, an LLM-based conversational
financial information retrieval model tailored for query intent classification
and knowledge base labeling.
<br />ConFIRM comprises two modules:
<br />1) a method to synthesize finance domain-specific question-answer pairs, and
<br />2) evaluation of parameter efficient fine-tuning approaches for the query
classification task. We generate a dataset of over 4000 samples, assessing
accuracy on a separate test set.
<br />ConFIRM achieved over 90% accuracy, essential for regulatory compliance.
ConFIRM provides a data-efficient solution to extract precise query intent for
financial dialog systems.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13002" title="Abstract">arXiv:2310.13002</a> [<a href="/pdf/2310.13002" title="Download PDF">pdf</a>, <a href="/format/2310.13002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models Geospatially Knowledgeable?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhandari%2C+P">Prabin Bhandari</a>, 
<a href="/search/cs?searchtype=author&query=Anastasopoulos%2C+A">Antonios Anastasopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Pfoser%2C+D">Dieter Pfoser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the impressive performance of Large Language Models (LLM) for various
natural language processing tasks, little is known about their comprehension of
geographic data and related ability to facilitate informed geospatial
decision-making. This paper investigates the extent of geospatial knowledge,
awareness, and reasoning abilities encoded within such pretrained LLMs. With a
focus on autoregressive language models, we devise experimental approaches
related to (i) probing LLMs for geo-coordinates to assess geospatial knowledge,
(ii) using geospatial and non-geospatial prepositions to gauge their geospatial
awareness, and (iii) utilizing a multidimensional scaling (MDS) experiment to
assess the models' geospatial reasoning capabilities and to determine locations
of cities based on prompting. Our results confirm that it does not only take
larger, but also more sophisticated LLMs to synthesize geospatial knowledge
from textual information. As such, this research contributes to understanding
the potential and limitations of LLMs in dealing with geospatial information.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13003" title="Abstract">arXiv:2310.13003</a> [<a href="/pdf/2310.13003" title="Download PDF">pdf</a>, <a href="/format/2310.13003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Interactive Web-Based System for Creating Single Panel Cartoons with  Visually Valid Compositions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akleman%2C+E">Ergun Akleman</a>, 
<a href="/search/cs?searchtype=author&query=Vijaykumar%2C+A">Akhilesh Vijaykumar</a>, 
<a href="/search/cs?searchtype=author&query=Furuta%2C+R">Richard Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Akleman%2C+D">Derya Akleman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Digital Libraries (cs.DL); Multimedia (cs.MM)

</div>
<p class="mathjax">The creation of cartoon-based stories (comics) requires a lot of creativity
and hard work for naive users. We observe that single-panel cartoons are the
building blocks of any comic story. To develop a strong comic story, it is
critical to obtain visually valid single panels. In this work, we have
developed a methodology to guarantee the placement of characters to obtain a
valid cartoon frame based on the methods used by professional cartoonists.
Using this methodology, we have developed a web-based system to create
single-panel cartoons from a given set of character images. We have made this
system available in GitHub as open-source so that this basic single-panel
cartoon can be used as infrastructure to develop more complex structures. Our
web-based system for single-panel cartoons can be viewed at
<a href="http://storytelling.viz.tamu.edu.">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13004" title="Abstract">arXiv:2310.13004</a> [<a href="/pdf/2310.13004" title="Download PDF">pdf</a>, <a href="/format/2310.13004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressively Efficient Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruijie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khanh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+K">Karthik Narasimhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Assistant AI agents should be capable of rapidly acquiring novel skills and
adapting to new user preferences. Traditional frameworks like imitation
learning and reinforcement learning do not facilitate this capability because
they support only low-level, inefficient forms of communication. In contrast,
humans communicate with progressive efficiency by defining and sharing abstract
intentions. Reproducing similar capability in AI agents, we develop a novel
learning framework named Communication-Efficient Interactive Learning (CEIL).
By equipping a learning agent with an abstract, dynamic language and an
intrinsic motivation to learn with minimal communication effort, CEIL leads to
emergence of a human-like pattern where the learner and the teacher communicate
progressively efficiently by exchanging increasingly more abstract intentions.
CEIL demonstrates impressive performance and communication efficiency on a 2D
MineCraft domain featuring long-horizon decision-making tasks. Agents trained
with CEIL quickly master new tasks, outperforming non-hierarchical and
hierarchical imitation learning by up to 50% and 20% in absolute success rate,
respectively, given the same number of interactions with the teacher.
Especially, the framework performs robustly with teachers modeled after human
pragmatic communication behavior.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13005" title="Abstract">arXiv:2310.13005</a> [<a href="/pdf/2310.13005" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metacognitive threshold: a computational account
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conway-Smith%2C+B">Brendan Conway-Smith</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R+L">Robert L. West</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 21st International Conference on Cognitive Modeling (2023) <a href="https://mathpsych.org/presentation/1229#/document">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>; Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">This paper will explore ways of computationally accounting for the
metacognitive threshold -- the minimum amount of stimulus needed for a mental
state to be perceived -- and discuss potential cognitive mechanisms by which
this threshold can be influenced through metacognitive training and meditation.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13006" title="Abstract">arXiv:2310.13006</a> [<a href="/pdf/2310.13006" title="Download PDF">pdf</a>, <a href="/format/2310.13006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Software Metadata Classification based on Generative Artificial  Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Killivalavan%2C+S">Seetharam Killivalavan</a>, 
<a href="/search/cs?searchtype=author&query=Thenmozhi%2C+D">Durairaj Thenmozhi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> FIRE Track: Information Retrieval in Software Engineering (IRSE), 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a novel approach to enhance the performance of binary
code comment quality classification models through the application of
Generative Artificial Intelligence (AI). By leveraging the OpenAI API, a
dataset comprising 1239 newly generated code-comment pairs, extracted from
various GitHub repositories and open-source projects, has been labelled as
"Useful" or "Not Useful", and integrated into the existing corpus of 9048 pairs
in the C programming language. Employing a cutting-edge Large Language Model
Architecture, the generated dataset demonstrates notable improvements in model
accuracy. Specifically, when incorporated into the Support Vector Machine (SVM)
model, a 6% increase in precision is observed, rising from 0.79 to 0.85.
Additionally, the Artificial Neural Network (ANN) model exhibits a 1.5%
increase in recall, climbing from 0.731 to 0.746. This paper sheds light on the
potential of Generative AI in augmenting code comment quality classification
models. The results affirm the effectiveness of this methodology, indicating
its applicability in broader contexts within software development and quality
assurance domains. The findings underscore the significance of integrating
generative techniques to advance the accuracy and efficacy of machine learning
models in practical software engineering scenarios.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13007" title="Abstract">arXiv:2310.13007</a> [<a href="/pdf/2310.13007" title="Download PDF">pdf</a>, <a href="/format/2310.13007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Critical Survey on Fairness Benefits of XAI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deck%2C+L">Luca Deck</a>, 
<a href="/search/cs?searchtype=author&query=Schoeffer%2C+J">Jakob Schoeffer</a>, 
<a href="/search/cs?searchtype=author&query=De-Arteaga%2C+M">Maria De-Arteaga</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChl%2C+N">Niklas K&#xfc;hl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In this critical survey, we analyze typical claims on the relationship
between explainable AI (XAI) and fairness to disentangle the multidimensional
relationship between these two concepts. Based on a systematic literature
review and a subsequent qualitative content analysis, we identify seven
archetypal claims from 175 papers on the alleged fairness benefits of XAI. We
present crucial caveats with respect to these claims and provide an entry point
for future discussions around the potentials and limitations of XAI for
specific fairness desiderata. While the literature often suggests XAI to be an
enabler for several fairness desiderata, we notice a misalignment between these
desiderata and the capabilities of XAI. We encourage to conceive XAI as one of
many tools to approach the multidimensional, sociotechnical challenge of
algorithmic fairness and to be more specific about how exactly what kind of XAI
method enables whom to address which fairness desideratum.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13008" title="Abstract">arXiv:2310.13008</a> [<a href="/pdf/2310.13008" title="Download PDF">pdf</a>, <a href="/format/2310.13008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoBaSS: Gauging Learnability in Supervised Fine-tuning Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haotian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tingkai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianli Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianbo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large
Language Models (LLMs) to specific task prerequisites. The selection of
fine-tuning data profoundly influences the model's performance, whose principle
is traditionally grounded in data quality and distribution. In this paper, we
introduce a new dimension in SFT data selection: learnability. This new
dimension is motivated by the intuition that SFT unlocks capabilities acquired
by a LLM during the pretraining phase. Given that different pretrained models
have disparate capabilities, the SFT data appropriate for one may not suit
another. Thus, we introduce the term learnability to define the suitability of
data for effective learning by the model. We present the Loss Based SFT Data
Selection (LoBaSS) method, utilizing data learnability as the principal
criterion for the selection SFT data. This method provides a nuanced approach,
allowing the alignment of data selection with inherent model capabilities,
ensuring optimal compatibility and learning efficiency. In experimental
comparisons involving 7B and 13B models, our LoBaSS method is able to surpass
full-data fine-tuning at merely 6% of the total training data. When employing
16.7% of the data, LoBaSS harmonizes the model's capabilities across
conversational and mathematical domains, proving its efficacy and adaptability.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13011" title="Abstract">arXiv:2310.13011</a> [<a href="/pdf/2310.13011" title="Download PDF">pdf</a>, <a href="/format/2310.13011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional preference models for aligning LMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Go%2C+D">Dongyoung Go</a>, 
<a href="/search/cs?searchtype=author&query=Korbak%2C+T">Tomasz Korbak</a>, 
<a href="/search/cs?searchtype=author&query=Kruszewski%2C+G">Germ&#xe1;n Kruszewski</a>, 
<a href="/search/cs?searchtype=author&query=Rozen%2C+J">Jos Rozen</a>, 
<a href="/search/cs?searchtype=author&query=Dymetman%2C+M">Marc Dymetman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As language models (LMs) become more capable, it is increasingly important to
align them with human preferences. However, the dominant paradigm for training
Preference Models (PMs) for that purpose suffers from fundamental limitations,
such as lack of transparency and scalability, along with susceptibility to
overfitting the preference dataset. We propose Compositional Preference Models
(CPMs), a novel PM framework that decomposes one global preference assessment
into several interpretable features, obtains scalar scores for these features
from a prompted LM, and aggregates these scores using a logistic regression
classifier. CPMs allow to control which properties of the preference data are
used to train the preference model and to build it based on features that are
believed to underlie the human preference judgment. Our experiments show that
CPMs not only improve generalization and are more robust to overoptimization
than standard PMs, but also that best-of-n samples obtained using CPMs tend to
be preferred over samples obtained using conventional PMs. Overall, our
approach demonstrates the benefits of endowing PMs with priors about which
features determine human preferences while relying on LM capabilities to
extract those features in a scalable and robust way.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13012" title="Abstract">arXiv:2310.13012</a> [<a href="/pdf/2310.13012" title="Download PDF">pdf</a>, <a href="/format/2310.13012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> H2O Open Ecosystem for State-of-the-art Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Candel%2C+A">Arno Candel</a>, 
<a href="/search/cs?searchtype=author&query=McKinney%2C+J">Jon McKinney</a>, 
<a href="/search/cs?searchtype=author&query=Singer%2C+P">Philipp Singer</a>, 
<a href="/search/cs?searchtype=author&query=Pfeiffer%2C+P">Pascal Pfeiffer</a>, 
<a href="/search/cs?searchtype=author&query=Jeblick%2C+M">Maximilian Jeblick</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C+M">Chun Ming Lee</a>, 
<a href="/search/cs?searchtype=author&query=Conde%2C+M+V">Marcos V. Conde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Empirical Methods in Natural Language Processing (EMNLP) 2023 Demo
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) represent a revolution in AI. However, they also
pose many significant risks, such as the presence of biased, private,
copyrighted or harmful text. For this reason we need open, transparent and safe
solutions. We introduce a complete open-source ecosystem for developing and
testing LLMs. The goal of this project is to boost open alternatives to
closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7
to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and
no-code GUI designed for efficient fine-tuning, evaluation, and deployment of
LLMs using the most recent state-of-the-art techniques. Our code and models are
licensed under fully permissive Apache 2.0 licenses. We believe open-source
language models help to boost AI development and make it more accessible and
trustworthy. The demo is available at: https://gpt.h2o.ai/
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13013" title="Abstract">arXiv:2310.13013</a> [<a href="/pdf/2310.13013" title="Download PDF">pdf</a>, <a href="/format/2310.13013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative error correction for code-switching speech recognition using  large language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuchen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hexin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Siniscalchi%2C+S+M">Sabato Marco Siniscalchi</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+E+S">Eng Siong Chng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Code-switching (CS) speech refers to the phenomenon of mixing two or more
languages within the same sentence. Despite the recent advances in automatic
speech recognition (ASR), CS-ASR is still a challenging task ought to the
grammatical structure complexity of the phenomenon and the data scarcity of
specific training corpus. In this work, we propose to leverage large language
models (LLMs) and lists of hypotheses generated by an ASR to address the CS
problem. Specifically, we first employ multiple well-trained ASR models for
N-best hypotheses generation, with the aim of increasing the diverse and
informative elements in the set of hypotheses. Next, we utilize the LLMs to
learn the hypotheses-to-transcription (H2T) mapping by adding a trainable
low-rank adapter. Such a generative error correction (GER) method directly
predicts the accurate transcription according to its expert linguistic
knowledge and N-best hypotheses, resulting in a paradigm shift from the
traditional language model rescoring or error correction techniques.
Experimental evidence demonstrates that GER significantly enhances CS-ASR
accuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show
remarkable data efficiency for H2T learning, providing a potential solution to
the data scarcity problem of CS-ASR in low-resource languages.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13014" title="Abstract">arXiv:2310.13014</a> [<a href="/pdf/2310.13014" title="Download PDF">pdf</a>, <a href="/format/2310.13014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Prediction Capabilities: Evidence from a Real-World  Forecasting Tournament
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schoenegger%2C+P">Philipp Schoenegger</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+P+S">Peter S. Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, six visualizations (four figures, two tables)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurately predicting the future would be an important milestone in the
capabilities of artificial intelligence. However, research on the ability of
large language models to provide probabilistic predictions about future events
remains nascent. To empirically test this ability, we enrolled OpenAI's
state-of-the-art large language model, GPT-4, in a three-month forecasting
tournament hosted on the Metaculus platform. The tournament, running from July
to October 2023, attracted 843 participants and covered diverse topics
including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict.
Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are
significantly less accurate than the median human-crowd forecasts. We find that
GPT-4's forecasts did not significantly differ from the no-information
forecasting strategy of assigning a 50% probability to every question. We
explore a potential explanation, that GPT-4 might be predisposed to predict
probabilities close to the midpoint of the scale, but our data do not support
this hypothesis. Overall, we find that GPT-4 significantly underperforms in
real-world predictive tasks compared to median human-crowd forecasts. A
potential explanation for this underperformance is that in real-world
forecasting tournaments, the true answers are genuinely unknown at the time of
prediction; unlike in other benchmark tasks like professional exams or time
series forecasting, where strong performance may at least partly be due to the
answers being memorized from the training data. This makes real-world
forecasting tournaments an ideal environment for testing the generalized
reasoning and prediction capabilities of artificial intelligence going forward.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13015" title="Abstract">arXiv:2310.13015</a> [<a href="/pdf/2310.13015" title="Download PDF">pdf</a>, <a href="/format/2310.13015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio-AdapterFusion: A Task-ID-free Approach for Efficient and  Non-Destructive Multi-task Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ngai%2C+H">Hillary Ngai</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+R">Rohan Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+N">Neeraj Gaur</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ronny Huang</a>, 
<a href="/search/cs?searchtype=author&query=Haghani%2C+P">Parisa Haghani</a>, 
<a href="/search/cs?searchtype=author&query=Mengibar%2C+P+M">Pedro Moreno Mengibar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) Proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Adapters are an efficient, composable alternative to full fine-tuning of
pre-trained models and help scale the deployment of large ASR models to many
tasks. In practice, a task ID is commonly prepended to the input during
inference to route to single-task adapters for the specified task. However, one
major limitation of this approach is that the task ID may not be known during
inference, rendering it unsuitable for most multi-task settings. To address
this, we propose three novel task-ID-free methods to combine single-task
adapters in multi-task ASR and investigate two learning algorithms for
training. We evaluate our methods on 10 test sets from 4 diverse ASR tasks and
show that our methods are non-destructive and parameter-efficient. While only
updating 17% of the model parameters, our methods can achieve an 8% mean WER
improvement relative to full fine-tuning and are on-par with task-ID adapter
routing.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13016" title="Abstract">arXiv:2310.13016</a> [<a href="/pdf/2310.13016" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving the multiplication problem of a large language model system  using a graph-based method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tuncer%2C+T">Turker Tuncer</a>, 
<a href="/search/cs?searchtype=author&query=Dogan%2C+S">Sengul Dogan</a>, 
<a href="/search/cs?searchtype=author&query=Baygin%2C+M">Mehmet Baygin</a>, 
<a href="/search/cs?searchtype=author&query=Barua%2C+P+D">Prabal Datta Barua</a>, 
<a href="/search/cs?searchtype=author&query=Hafeez-Baig%2C+A">Abdul Hafeez-Baig</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+R">Ru-San Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Subrata Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Acharya%2C+U+R">U. Rajendra Acharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The generative pre-trained transformer (GPT)-based chatbot software ChatGPT
possesses excellent natural language processing capabilities but is inadequate
for solving arithmetic problems, especially multiplication. Its GPT structure
uses a computational graph for multiplication, which has limited accuracy
beyond simple multiplication operations. We developed a graph-based
multiplication algorithm that emulated human-like numerical operations by
incorporating a 10k operator, where k represents the maximum power to base 10
of the larger of two input numbers. Our proposed algorithm attained 100%
accuracy for 1,000,000 large number multiplication tasks, effectively solving
the multiplication challenge of GPT-based and other large language models. Our
work highlights the importance of blending simple human insights into the
design of artificial intelligence algorithms. Keywords: Graph-based
multiplication; ChatGPT; Multiplication problem
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13017" title="Abstract">arXiv:2310.13017</a> [<a href="/pdf/2310.13017" title="Download PDF">pdf</a>, <a href="/format/2310.13017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Interpolation Improves ALiBi Extrapolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Khateeb%2C+F">Faisal Al-Khateeb</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+N">Nolan Dey</a>, 
<a href="/search/cs?searchtype=author&query=Soboleva%2C+D">Daria Soboleva</a>, 
<a href="/search/cs?searchtype=author&query=Hestness%2C+J">Joel Hestness</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages content, 1 page references, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Linear position interpolation helps pre-trained models using rotary position
embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using
linear position interpolation to extend the extrapolation range of models using
Attention with Linear Biases (ALiBi). We find position interpolation
significantly improves extrapolation capability on upstream language modelling
and downstream summarization and retrieval tasks.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13019" title="Abstract">arXiv:2310.13019</a> [<a href="/pdf/2310.13019" title="Download PDF">pdf</a>, <a href="/format/2310.13019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class  Manipulation Using DeepFool Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Labib%2C+S+M+F+R">S. M. Fazle Rabby Labib</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+J+J">Joyanta Jyoti Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Manab%2C+M+A">Meem Arafat Manab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, to be submitted at IEEE Computer Vision and Pattern Recognition (CVPR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep neural networks (DNNs) have significantly advanced various domains, but
their vulnerability to adversarial attacks poses serious concerns.
Understanding these vulnerabilities and developing effective defense mechanisms
is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016),
finds minimal perturbations to misclassify input images. However, DeepFool
lacks a targeted approach, making it less effective in specific attack
scenarios. Also, in previous related works, researchers primarily focus on
success, not considering how much an image is getting distorted; the integrity
of the image quality, and the confidence level to misclassifying. So, in this
paper, we propose Targeted DeepFool, an augmented version of DeepFool that
allows targeting specific classes for misclassification. We also introduce a
minimum confidence score requirement hyperparameter to enhance flexibility. Our
experiments demonstrate the effectiveness and efficiency of the proposed method
across different deep neural network architectures while preserving image
integrity as much as possible. Results show that one of the deep convolutional
neural network architectures, AlexNet, and one of the state-of-the-art model
Vision Transformer exhibit high robustness to getting fooled. Our code will be
made public when publishing the paper.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13022" title="Abstract">arXiv:2310.13022</a> [<a href="/pdf/2310.13022" title="Download PDF">pdf</a>, <a href="/format/2310.13022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised  Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qiushi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Ming Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The recent success of large pre-trained language models (PLMs) heavily hinges
on massive labeled data, which typically produces inferior performance in
low-resource scenarios. To remedy this dilemma, we study self-training as one
of the predominant semi-supervised learning (SSL) approaches, which utilizes
large-scale unlabeled data to generate synthetic examples. However, too many
noisy labels will hurt the model performance, and the self-training procedure
requires multiple training iterations making it more expensive if all the model
parameters of the PLM are updated. This paper presents UPET, a novel
Uncertainty-aware Parameter-Efficient self-Training framework to effectively
and efficiently address the labeled data scarcity issue. Specifically, we
incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to
perform uncertainty estimation for the teacher model and then judiciously
select reliable pseudo-labeled examples based on confidence and certainty.
During the student training, we introduce multiple parameter-efficient learning
(PEL) paradigms that allow the optimization of only a small percentage of
parameters. We also propose a novel Easy-Hard Contrastive Tuning to enhance the
robustness and generalization. Extensive experiments over multiple downstream
tasks demonstrate that UPET achieves a substantial improvement in terms of
performance and efficiency. Our codes and data are released at https:
//github.com/wjn1996/UPET.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13023" title="Abstract">arXiv:2310.13023</a> [<a href="/pdf/2310.13023" title="Download PDF">pdf</a>, <a href="/format/2310.13023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphGPT: Graph Instruction Tuning for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiabin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Lei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+L">Lixin Su</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Suqi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have advanced graph structure understanding via
recursive information exchange and aggregation among graph nodes. To improve
model robustness, self-supervised learning (SSL) has emerged as a promising
approach for data augmentation. However, existing methods for generating
pre-trained graph embeddings often rely on fine-tuning with specific downstream
task labels, which limits their usability in scenarios where labeled data is
scarce or unavailable. To address this, our research focuses on advancing the
generalization capabilities of graph models in challenging zero-shot learning
scenarios. Inspired by the success of large language models (LLMs), we aim to
develop a graph-oriented LLM that can achieve high generalization across
diverse downstream datasets and tasks, even without any information available
from the downstream graph data. In this work, we present the GraphGPT framework
that aligns LLMs with graph structural knowledge with a graph instruction
tuning paradigm. Our framework incorporates a text-graph grounding component to
establish a connection between textual information and graph structures.
Additionally, we propose a dual-stage instruction tuning paradigm, accompanied
by a lightweight graph-text alignment projector. This paradigm explores
self-supervised graph structural signals and task-specific graph instructions,
to guide LLMs in understanding complex graph structures and improving their
adaptability across different downstream tasks. Our framework is evaluated on
supervised and zero-shot graph learning tasks, demonstrating superior
generalization and outperforming state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13024" title="Abstract">arXiv:2310.13024</a> [<a href="/pdf/2310.13024" title="Download PDF">pdf</a>, <a href="/format/2310.13024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Anytime Fine-tuning: Continually Pre-trained Language Models  with Hypernetwork Prompt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+G">Gangwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Caigao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Siqiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+Y">James Y. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Ying Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Continual pre-training has been urgent for adapting a pre-trained model to a
multitude of domains and tasks in the fast-evolving world. In practice, a
continually pre-trained model is expected to demonstrate not only greater
capacity when fine-tuned on pre-trained domains but also a non-decreasing
performance on unseen ones. In this work, we first investigate such anytime
fine-tuning effectiveness of existing continual pre-training approaches,
concluding with unanimously decreased performance on unseen domains. To this
end, we propose a prompt-guided continual pre-training method, where we train a
hypernetwork to generate domain-specific prompts by both agreement and
disagreement losses. The agreement loss maximally preserves the generalization
of a pre-trained model to new domains, and the disagreement one guards the
exclusiveness of the generated hidden states for each domain. Remarkably,
prompts by the hypernetwork alleviate the domain identity when fine-tuning and
promote knowledge transfer across domains. Our method achieved improvements of
3.57% and 3.4% on two real-world datasets (including domain shift and temporal
shift), respectively, demonstrating its efficacy.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13025" title="Abstract">arXiv:2310.13025</a> [<a href="/pdf/2310.13025" title="Download PDF">pdf</a>, <a href="/format/2310.13025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Powerset multi-class cross entropy loss for neural speaker diarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Plaquet%2C+A">Alexis Plaquet</a> (IRIT-SAMoVA), 
<a href="/search/cs?searchtype=author&query=Bredin%2C+H">Herv&#xe9; Bredin</a> (IRIT-SAMoVA)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> INTERSPEECH 2023, Aug 2023, Dublin, Ireland. pp.3222-3226
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Since its introduction in 2019, the whole end-to-end neural diarization
(EEND) line of work has been addressing speaker diarization as a frame-wise
multi-label classification problem with permutation-invariant training. Despite
EEND showing great promise, a few recent works took a step back and studied the
possible combination of (local) supervised EEND diarization with (global)
unsupervised clustering. Yet, these hybrid contributions did not question the
original multi-label formulation. We propose to switch from multi-label (where
any two speakers can be active at the same time) to powerset multi-class
classification (where dedicated classes are assigned to pairs of overlapping
speakers). Through extensive experiments on 9 different benchmarks, we show
that this formulation leads to significantly better performance (mostly on
overlapping speech) and robustness to domain mismatch, while eliminating the
detection threshold hyperparameter, critical for the multi-label formulation.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13026" title="Abstract">arXiv:2310.13026</a> [<a href="/pdf/2310.13026" title="Download PDF">pdf</a>, <a href="/format/2310.13026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly-Supervised Semantic Segmentation with Image-Level Labels: from  Traditional Models to Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaozheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qianru Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The rapid development of deep learning has driven significant progress in the
field of image semantic segmentation - a fundamental task in computer vision.
Semantic segmentation algorithms often depend on the availability of
pixel-level labels (i.e., masks of objects), which are expensive,
time-consuming, and labor-intensive. Weakly-supervised semantic segmentation
(WSSS) is an effective solution to avoid such labeling. It utilizes only
partial or incomplete annotations and provides a cost-effective alternative to
fully-supervised semantic segmentation. In this paper, we focus on the WSSS
with image-level labels, which is the most challenging form of WSSS. Our work
has two parts. First, we conduct a comprehensive survey on traditional methods,
primarily focusing on those presented at premier research conferences. We
categorize them into four groups based on where their methods operate:
pixel-wise, image-wise, cross-image, and external data. Second, we investigate
the applicability of visual foundation models, such as the Segment Anything
Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing
scenarios: text prompting and zero-shot learning. We provide insights into the
potential and challenges associated with deploying visual foundational models
for WSSS, facilitating future developments in this exciting research area.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13027" title="Abstract">arXiv:2310.13027</a> [<a href="/pdf/2310.13027" title="Download PDF">pdf</a>, <a href="/format/2310.13027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Be Bayesian by Attachments to Catch More Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shiyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Tianyang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhenwei Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Bayesian Neural Networks (BNNs) have become one of the promising approaches
for uncertainty estimation due to the solid theorical foundations. However, the
performance of BNNs is affected by the ability of catching uncertainty. Instead
of only seeking the distribution of neural network weights by in-distribution
(ID) data, in this paper, we propose a new Bayesian Neural Network with an
Attached structure (ABNN) to catch more uncertainty from out-of-distribution
(OOD) data. We first construct a mathematical description for the uncertainty
of OOD data according to the prior distribution, and then develop an attached
Bayesian structure to integrate the uncertainty of OOD data into the backbone
network. ABNN is composed of an expectation module and several distribution
modules. The expectation module is a backbone deep network which focuses on the
original task, and the distribution modules are mini Bayesian structures which
serve as attachments of the backbone. In particular, the distribution modules
aim at extracting the uncertainty from both ID and OOD data. We further provide
theoretical analysis for the convergence of ABNN, and experimentally validate
its superiority by comparing with some state-of-the-art uncertainty estimation
methods Code will be made available.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13028" title="Abstract">arXiv:2310.13028</a> [<a href="/pdf/2310.13028" title="Download PDF">pdf</a>, <a href="/format/2310.13028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reliable Academic Conference Question Answering: A Study Based on Large  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Long Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+M">Mingchen Tu</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yin Hua</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiqiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+J">Jiawei Meng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wen Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid growth of computer science has led to a proliferation of research
presented at academic conferences, fostering global scholarly communication.
Researchers consistently seek accurate, current information about these events
at all stages. This data surge necessitates an intelligent question-answering
system to efficiently address researchers' queries and ensure awareness of the
latest advancements. The information of conferences is usually published on
their official website, organized in a semi-structured way with a lot of text.
To address this need, we have developed the ConferenceQA dataset for 7 diverse
academic conferences with human annotations. Firstly, we employ a combination
of manual and automated methods to organize academic conference data in a
semi-structured JSON format. Subsequently, we annotate nearly 100
question-answer pairs for each conference. Each pair is classified into four
different dimensions. To ensure the reliability of the data, we manually
annotate the source of each answer. In light of recent advancements, Large
Language Models (LLMs) have demonstrated impressive performance in various NLP
tasks. They have demonstrated impressive capabilities in information-seeking
question answering after instruction fine-tuning, and as such, we present our
conference QA study based on LLM. Due to hallucination and outdated knowledge
of LLMs, we adopt retrieval based methods to enhance LLMs' question-answering
abilities. We have proposed a structure-aware retrieval method, specifically
designed to leverage inherent structural information during the retrieval
process. Empirical validation on the ConferenceQA dataset has demonstrated the
effectiveness of this method. The dataset and code are readily accessible on
https://github.com/zjukg/ConferenceQA.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13029" title="Abstract">arXiv:2310.13029</a> [<a href="/pdf/2310.13029" title="Download PDF">pdf</a>, <a href="/format/2310.13029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blending gradient boosted trees and neural networks for point and  probabilistic forecasting of hierarchical time series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nasios%2C+I">Ioannis Nasios</a>, 
<a href="/search/cs?searchtype=author&query=Vogklis%2C+K">Konstantinos Vogklis</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Volume 38, Issue 4, 2022, Pages 1448-1459
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Analysis, Statistics and Probability (physics.data-an); Statistical Finance (q-fin.ST)

</div>
<p class="mathjax">In this paper we tackle the problem of point and probabilistic forecasting by
describing a blending methodology of machine learning models that belong to
gradient boosted trees and neural networks families. These principles were
successfully applied in the recent M5 Competition on both Accuracy and
Uncertainty tracks. The keypoints of our methodology are: a) transform the task
to regression on sales for a single day b) information rich feature engineering
c) create a diverse set of state-of-the-art machine learning models and d)
carefully construct validation sets for model tuning. We argue that the
diversity of the machine learning models along with the careful selection of
validation examples, where the most important ingredients for the effectiveness
of our approach. Although forecasting data had an inherent hierarchy structure
(12 levels), none of our proposed solutions exploited that hierarchical scheme.
Using the proposed methodology, our team was ranked within the gold medal range
in both Accuracy and the Uncertainty track. Inference code along with already
trained models are available at
https://github.com/IoannisNasios/M5_Uncertainty_3rd_place
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13030" title="Abstract">arXiv:2310.13030</a> [<a href="/pdf/2310.13030" title="Download PDF">pdf</a>, <a href="/format/2310.13030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIRe-IR: Inverse Rendering for BRDF Reconstruction with Shadow and  Illumination Removal in High-Illuminance Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanzhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yazhen Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaowei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaogang Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Implicit neural representation has opened up new possibilities for inverse
rendering. However, existing implicit neural inverse rendering methods struggle
to handle strongly illuminated scenes with significant shadows and indirect
illumination. The existence of shadows and reflections can lead to an
inaccurate understanding of scene geometry, making precise factorization
difficult. To this end, we present SIRe-IR, an implicit neural inverse
rendering approach that uses non-linear mapping and regularized visibility
estimation to decompose the scene into environment map, albedo, and roughness.
By accurately modeling the indirect radiance field, normal, visibility, and
direct light simultaneously, we are able to remove both shadows and indirect
illumination in materials without imposing strict constraints on the scene.
Even in the presence of intense illumination, our method recovers high-quality
albedo and roughness with no shadow interference. SIRe-IR outperforms existing
methods in both quantitative and qualitative evaluations.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13031" title="Abstract">arXiv:2310.13031</a> [<a href="/pdf/2310.13031" title="Download PDF">pdf</a>, <a href="/format/2310.13031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Use Case: Reformulating Query Rewriting as a Statistical Machine  Translation Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Algan%2C+A+C">Abdullah Can Algan</a>, 
<a href="/search/cs?searchtype=author&query=Y%C3%BCrekli%2C+E">Emre Y&#xfc;rekli</a>, 
<a href="/search/cs?searchtype=author&query=%C3%87ay%C4%B1r%2C+A">Aykut &#xc7;ay&#x131;r</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">One of the most important challenges for modern search engines is to retrieve
relevant web content based on user queries. In order to achieve this challenge,
search engines have a module to rewrite user queries. That is why modern web
search engines utilize some statistical and neural models used in the natural
language processing domain. Statistical machine translation is a well-known NLP
method among them. The paper proposes a query rewriting pipeline based on a
monolingual machine translation model that learns to rewrite Arabic user search
queries. This paper also describes preprocessing steps to create a mapping
between user queries and web page titles.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13032" title="Abstract">arXiv:2310.13032</a> [<a href="/pdf/2310.13032" title="Download PDF">pdf</a>, <a href="/format/2310.13032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality-Diversity through AI Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bradley%2C+H">Herbie Bradley</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+A">Andrew Dai</a>, 
<a href="/search/cs?searchtype=author&query=Teufel%2C+H">Hannah Teufel</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jenny Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Oostermeijer%2C+K">Koen Oostermeijer</a>, 
<a href="/search/cs?searchtype=author&query=Bellagente%2C+M">Marco Bellagente</a>, 
<a href="/search/cs?searchtype=author&query=Clune%2C+J">Jeff Clune</a>, 
<a href="/search/cs?searchtype=author&query=Stanley%2C+K">Kenneth Stanley</a>, 
<a href="/search/cs?searchtype=author&query=Schott%2C+G">Gr&#xe9;gory Schott</a>, 
<a href="/search/cs?searchtype=author&query=Lehman%2C+J">Joel Lehman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In many text-generation problems, users may prefer not only a single
response, but a diverse range of high-quality outputs from which to choose.
Quality-diversity (QD) search algorithms aim at such outcomes, by continually
improving and diversifying a population of candidates. However, the
applicability of QD to qualitative domains, like creative writing, has been
limited by the difficulty of algorithmically specifying measures of quality and
diversity. Interestingly, recent developments in language models (LMs) have
enabled guiding search through AI feedback, wherein LMs are prompted in natural
language to evaluate qualitative aspects of text. Leveraging this development,
we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an
evolutionary algorithm applies LMs to both generate variation and evaluate the
quality and diversity of candidate text. When assessed on creative writing
domains, QDAIF covers more of a specified search space with high-quality
samples than do non-QD controls. Further, human evaluation of QDAIF-generated
creative texts validates reasonable agreement between AI and human evaluation.
Our results thus highlight the potential of AI feedback to guide open-ended
search for creative and original solutions, providing a recipe that seemingly
generalizes to many domains and modalities. In this way, QDAIF is a step
towards AI systems that can independently search, diversify, evaluate, and
improve, which are among the core skills underlying human society's capacity
for innovation.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13033" title="Abstract">arXiv:2310.13033</a> [<a href="/pdf/2310.13033" title="Download PDF">pdf</a>, <a href="/format/2310.13033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LASER: Linear Compression in Wireless Distributed Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Makkuva%2C+A+V">Ashok Vardhan Makkuva</a>, 
<a href="/search/cs?searchtype=author&query=Bondaschi%2C+M">Marco Bondaschi</a>, 
<a href="/search/cs?searchtype=author&query=Vogels%2C+T">Thijs Vogels</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeji Kim</a>, 
<a href="/search/cs?searchtype=author&query=Gastpar%2C+M+C">Michael C. Gastpar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">Data-parallel SGD is the de facto algorithm for distributed optimization,
especially for large scale machine learning. Despite its merits, communication
bottleneck is one of its persistent issues. Most compression schemes to
alleviate this either assume noiseless communication links, or fail to achieve
good performance on practical tasks. In this paper, we close this gap and
introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER
capitalizes on the inherent low-rank structure of gradients and transmits them
efficiently over the noisy channels. Whilst enjoying theoretical guarantees
similar to those of the classical SGD, LASER shows consistent gains over
baselines on a variety of practical benchmarks. In particular, it outperforms
the state-of-the-art compression schemes on challenging computer vision and GPT
language modeling tasks. On the latter, we obtain $50$-$64 \%$ improvement in
perplexity over our baselines for noisy channels.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13037" title="Abstract">arXiv:2310.13037</a> [<a href="/pdf/2310.13037" title="Download PDF">pdf</a>, <a href="/format/2310.13037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agri-GNN: A Novel Genotypic-Topological Graph Neural Network Framework  Built on GraphSAGE for Optimized Yield Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aditya Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Asheesh Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages Regeneron STS entry
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Agriculture, as the cornerstone of human civilization, constantly seeks to
integrate technology for enhanced productivity and sustainability. This paper
introduces $\textit{Agri-GNN}$, a novel Genotypic-Topological Graph Neural
Network Framework tailored to capture the intricate spatial and genotypic
interactions of crops, paving the way for optimized predictions of harvest
yields. $\textit{Agri-GNN}$ constructs a Graph $\mathcal{G}$ that considers
farming plots as nodes, and then methodically constructs edges between nodes
based on spatial and genotypic similarity, allowing for the aggregation of node
information through a genotypic-topological filter. Graph Neural Networks
(GNN), by design, consider the relationships between data points, enabling them
to efficiently model the interconnected agricultural ecosystem. By harnessing
the power of GNNs, $\textit{Agri-GNN}$ encapsulates both local and global
information from plants, considering their inherent connections based on
spatial proximity and shared genotypes, allowing stronger predictions to be
made than traditional Machine Learning architectures. $\textit{Agri-GNN}$ is
built from the GraphSAGE architecture, because of its optimal calibration with
large graphs, like those of farming plots and breeding experiments.
$\textit{Agri-GNN}$ experiments, conducted on a comprehensive dataset of
vegetation indices, time, genotype information, and location data, demonstrate
that $\textit{Agri-GNN}$ achieves an $R^2 = .876$ in yield predictions for
farming fields in Iowa. The results show significant improvement over the
baselines and other work in the field. $\textit{Agri-GNN}$ represents a
blueprint for using advanced graph-based neural architectures to predict crop
yield, providing significant improvements over baselines in the field.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13039" title="Abstract">arXiv:2310.13039</a> [<a href="/pdf/2310.13039" title="Download PDF">pdf</a>, <a href="/format/2310.13039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Pose-based Estimation, Tracking and Action Recognition with Deep  Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lijuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+X">Xiang Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhihuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mengqi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhimin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pichao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Human pose analysis has garnered significant attention within both the
research community and practical applications, owing to its expanding array of
uses, including gaming, video surveillance, sports performance analysis, and
human-computer interactions, among others. The advent of deep learning has
significantly improved the accuracy of pose capture, making pose-based
applications increasingly practical. This paper presents a comprehensive survey
of pose-based applications utilizing deep learning, encompassing pose
estimation, pose tracking, and action recognition.Pose estimation involves the
determination of human joint positions from images or image sequences. Pose
tracking is an emerging research direction aimed at generating consistent human
pose trajectories over time. Action recognition, on the other hand, targets the
identification of action types using pose estimation or tracking data. These
three tasks are intricately interconnected, with the latter often reliant on
the former. In this survey, we comprehensively review related works, spanning
from single-person pose estimation to multi-person pose estimation, from 2D
pose estimation to 3D pose estimation, from single image to video, from mining
temporal context gradually to pose tracking, and lastly from tracking to
pose-based action recognition. As a survey centered on the application of deep
learning to pose analysis, we explicitly discuss both the strengths and
limitations of existing techniques. Notably, we emphasize methodologies for
integrating these three tasks into a unified framework within video sequences.
Additionally, we explore the challenges involved and outline potential
directions for future research.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13040" title="Abstract">arXiv:2310.13040</a> [<a href="/pdf/2310.13040" title="Download PDF">pdf</a>, <a href="/format/2310.13040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust multimodal models have outlier features and encode more concepts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crabb%C3%A9%2C+J">Jonathan Crabb&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez%2C+P">Pau Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Shankar%2C+V">Vaishaal Shankar</a>, 
<a href="/search/cs?searchtype=author&query=Zappella%2C+L">Luca Zappella</a>, 
<a href="/search/cs?searchtype=author&query=Blaas%2C+A">Arno Blaas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">What distinguishes robust models from non-robust ones? This question has
gained traction with the appearance of large-scale multimodal models, such as
CLIP. These models have demonstrated unprecedented robustness with respect to
natural distribution shifts. While it has been shown that such differences in
robustness can be traced back to differences in training data, so far it is not
known what that translates to in terms of what the model has learned. In this
work, we bridge this gap by probing the representation spaces of 12 robust
multimodal models with various backbones (ResNets and ViTs) and pretraining
sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two
signatures of robustness in the representation spaces of these models: (1)
Robust models exhibit outlier features characterized by their activations, with
some being several orders of magnitude above average. These outlier features
induce privileged directions in the model's representation space. We
demonstrate that these privileged directions explain most of the predictive
power of the model by pruning up to $80 \%$ of the least important
representation space directions without negative impacts on model accuracy and
robustness; (2) Robust models encode substantially more concepts in their
representation space. While this superposition of concepts allows robust models
to store much information, it also results in highly polysemantic features,
which makes their interpretation challenging. We discuss how these insights
pave the way for future research in various fields, such as model pruning and
mechanistic interpretability.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13061" title="Abstract">arXiv:2310.13061</a> [<a href="/pdf/2310.13061" title="Download PDF">pdf</a>, <a href="/format/2310.13061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To grok or not to grok: Disentangling generalization and memorization on  corrupted algorithmic datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doshi%2C+D">Darshil Doshi</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Aritra Das</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianyu He</a>, 
<a href="/search/cs?searchtype=author&query=Gromov%2C+A">Andrey Gromov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 22 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (stat.ML)

</div>
<p class="mathjax">Robust generalization is a major challenge in deep learning, particularly
when the number of trainable parameters is very large. In general, it is very
difficult to know if the network has memorized a particular set of examples or
understood the underlying rule (or both). Motivated by this challenge, we study
an interpretable model where generalizing representations are understood
analytically, and are easily distinguishable from the memorizing ones. Namely,
we consider two-layer neural networks trained on modular arithmetic tasks where
($\xi \cdot 100\%$) of labels are corrupted (\emph{i.e.} some results of the
modular operations in the training set are incorrect). We show that (i) it is
possible for the network to memorize the corrupted labels \emph{and} achieve
$100\%$ generalization at the same time; (ii) the memorizing neurons can be
identified and pruned, lowering the accuracy on corrupted data and improving
the accuracy on uncorrupted data; (iii) regularization methods such as weight
decay, dropout and BatchNorm force the network to ignore the corrupted data
during optimization, and achieve $100\%$ accuracy on the uncorrupted dataset;
and (iv) the effect of these regularization methods is (``mechanistically'')
interpretable: weight decay and dropout force all the neurons to learn
generalizing representations, while BatchNorm de-amplifies the output of
memorizing neurons and amplifies the output of the generalizing ones. Finally,
we show that in the presence of regularization, the training dynamics involves
two consecutive stages: first, the network undergoes the \emph{grokking}
dynamics reaching high train \emph{and} test accuracy; second, it unlearns the
memorizing representations, where train accuracy suddenly jumps from $100\%$ to
$100 (1-\xi)\%$.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13065" title="Abstract">arXiv:2310.13065</a> [<a href="/pdf/2310.13065" title="Download PDF">pdf</a>, <a href="/format/2310.13065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Creative Robot Tool Use with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengdi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Peide Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xilun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yaru Niu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tingnan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jie Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 14 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Tool use is a hallmark of advanced intelligence, exemplified in both animal
behavior and robotic capabilities. This paper investigates the feasibility of
imbuing robots with the ability to creatively use tools in tasks that involve
implicit physical constraints and long-term planning. Leveraging Large Language
Models (LLMs), we develop RoboTool, a system that accepts natural language
instructions and outputs executable code for controlling robots in both
simulated and real-world environments. RoboTool incorporates four pivotal
components: (i) an "Analyzer" that interprets natural language to discern key
task-related concepts, (ii) a "Planner" that generates comprehensive strategies
based on the language input and key concepts, (iii) a "Calculator" that
computes parameters for each skill, and (iv) a "Coder" that translates these
plans into executable Python code. Our results show that RoboTool can not only
comprehend explicit or implicit physical constraints and environmental factors
but also demonstrate creative tool use. Unlike traditional Task and Motion
Planning (TAMP) methods that rely on explicit optimization, our LLM-based
system offers a more flexible, efficient, and user-friendly solution for
complex robotics tasks. Through extensive experiments, we validate that
RoboTool is proficient in handling tasks that would otherwise be infeasible
without the creative use of tools, thereby expanding the capabilities of
robotic systems. Demos are available on our project page:
https://creative-robotool.github.io/.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13068" title="Abstract">arXiv:2310.13068</a> [<a href="/pdf/2310.13068" title="Download PDF">pdf</a>, <a href="/format/2310.13068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GARI: Graph Attention for Relative Isomorphism of Arabic Word Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+M+A">Muhammad Asif Ali</a>, 
<a href="/search/cs?searchtype=author&query=Alshmrani%2C+M">Maha Alshmrani</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jianbin Qin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Bilingual Lexical Induction (BLI) is a core challenge in NLP, it relies on
the relative isomorphism of individual embedding spaces. Existing attempts
aimed at controlling the relative isomorphism of different embedding spaces
fail to incorporate the impact of semantically related words in the model
training objective. To address this, we propose GARI that combines the
distributional training objectives with multiple isomorphism losses guided by
the graph attention network. GARI considers the impact of semantical variations
of words in order to define the relative isomorphism of the embedding spaces.
Experimental evaluation using the Arabic language data set shows that GARI
outperforms the existing research by improving the average P@1 by a relative
score of up to 40.95% and 76.80% for in-domain and domain mismatch settings
respectively. We release the codes for GARI at
https://github.com/asif6827/GARI.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13073" title="Abstract">arXiv:2310.13073</a> [<a href="/pdf/2310.13073" title="Download PDF">pdf</a>, <a href="/format/2310.13073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Logic Programming and Kernel-Grouping for Improving  Interpretability of Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Padalkar%2C+P">Parth Padalkar</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+G">Gopal Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2301.12667">arXiv:2301.12667</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Within the realm of deep learning, the interpretability of Convolutional
Neural Networks (CNNs), particularly in the context of image classification
tasks, remains a formidable challenge. To this end we present a neurosymbolic
framework, NeSyFOLD-G that generates a symbolic rule-set using the last layer
kernels of the CNN to make its underlying knowledge interpretable. What makes
NeSyFOLD-G different from other similar frameworks is that we first find groups
of similar kernels in the CNN (kernel-grouping) using the cosine-similarity
between the feature maps generated by various kernels. Once such kernel groups
are found, we binarize each kernel group's output in the CNN and use it to
generate a binarization table which serves as input data to FOLD-SE-M which is
a Rule Based Machine Learning (RBML) algorithm. FOLD-SE-M then generates a
rule-set that can be used to make predictions. We present a novel kernel
grouping algorithm and show that grouping similar kernels leads to a
significant reduction in the size of the rule-set generated by FOLD-SE-M,
consequently, improving the interpretability. This rule-set symbolically
encapsulates the connectionist knowledge of the trained CNN. The rule-set can
be viewed as a normal logic program wherein each predicate's truth value
depends on a kernel group in the CNN. Each predicate in the rule-set is mapped
to a concept using a few semantic segmentation masks of the images used for
training, to make it human-understandable. The last layers of the CNN can then
be replaced by this rule-set to obtain the NeSy-G model which can then be used
for the image classification task. The goal directed ASP system s(CASP) can be
used to obtain the justification of any prediction made using the NeSy-G model.
We also propose a novel algorithm for labeling each predicate in the rule-set
with the semantic concept(s) that its corresponding kernel group represents.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13074" title="Abstract">arXiv:2310.13074</a> [<a href="/pdf/2310.13074" title="Download PDF">pdf</a>, <a href="/format/2310.13074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gender Biases in Error Mitigation by Voice Assistants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+A">Amama Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chien-Ming Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Commercial voice assistants are largely feminized and associated with
stereotypically feminine traits such as warmth and submissiveness. As these
assistants continue to be adopted for everyday uses, it is imperative to
understand how the portrayed gender shapes the voice assistant's ability to
mitigate errors, which are still common in voice interactions. We report a
study (N=40) that examined the effects of voice gender (feminine, ambiguous,
masculine), error mitigation strategies (apology, compensation) and
participant's gender on people's interaction behavior and perceptions of the
assistant. Our results show that AI assistants that apologized appeared warmer
than those offered compensation. Moreover, male participants preferred
apologetic feminine assistants over apologetic masculine ones. Furthermore,
male participants interrupted AI assistants regardless of perceived gender more
frequently than female participants when errors occurred. Our results suggest
that the perceived gender of a voice assistant biases user behavior, especially
for male users, and that an ambiguous voice has the potential to reduce biases
associated with gender-specific traits.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13075" title="Abstract">arXiv:2310.13075</a> [<a href="/pdf/2310.13075" title="Download PDF">pdf</a>, <a href="/ps/2310.13075" title="Download PostScript">ps</a>, <a href="/format/2310.13075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Computational Complexities of Complex-valued Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mayer%2C+K+S">Kayol Soares Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+J+A">Jonathan Aguiar Soares</a>, 
<a href="/search/cs?searchtype=author&query=Cruz%2C+A+A">Ariadne Arrais Cruz</a>, 
<a href="/search/cs?searchtype=author&query=Arantes%2C+D+S">Dalton Soares Arantes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Latin-American Conference on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Complex-valued neural networks (CVNNs) are nonlinear filters used in the
digital signal processing of complex-domain data. Compared with real-valued
neural networks~(RVNNs), CVNNs can directly handle complex-valued input and
output signals due to their complex domain parameters and activation functions.
With the trend toward low-power systems, computational complexity analysis has
become essential for measuring an algorithm's power consumption. Therefore,
this paper presents both the quantitative and asymptotic computational
complexities of CVNNs. This is a crucial tool in deciding which algorithm to
implement. The mathematical operations are described in terms of the number of
real-valued multiplications, as these are the most demanding operations. To
determine which CVNN can be implemented in a low-power system, quantitative
computational complexities can be used to accurately estimate the number of
floating-point operations. We have also investigated the computational
complexities of CVNNs discussed in some studies presented in the literature.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13076" title="Abstract">arXiv:2310.13076</a> [<a href="/pdf/2310.13076" title="Download PDF">pdf</a>, <a href="/format/2310.13076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PatchCURE: Improving Certifiable Robustness, Model Utility, and  Computation Efficiency of Adversarial Patch Defenses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+C">Chong Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+S">Sihui Dai</a>, 
<a href="/search/cs?searchtype=author&query=Petit%2C+J">Jonathan Petit</a>, 
<a href="/search/cs?searchtype=author&query=Jana%2C+S">Suman Jana</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+P">Prateek Mittal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">State-of-the-art defenses against adversarial patch attacks can now achieve
strong certifiable robustness with a marginal drop in model utility. However,
this impressive performance typically comes at the cost of 10-100x more
inference-time computation compared to undefended models -- the research
community has witnessed an intense three-way trade-off between certifiable
robustness, model utility, and computation efficiency. In this paper, we
propose a defense framework named PatchCURE to approach this trade-off problem.
PatchCURE provides sufficient "knobs" for tuning defense performance and allows
us to build a family of defenses: the most robust PatchCURE instance can match
the performance of any existing state-of-the-art defense (without efficiency
considerations); the most efficient PatchCURE instance has similar inference
efficiency as undefended models. Notably, PatchCURE achieves state-of-the-art
robustness and utility performance across all different efficiency levels,
e.g., 16-23% absolute clean accuracy and certified robust accuracy advantages
over prior defenses when requiring computation efficiency to be close to
undefended models. The family of PatchCURE defenses enables us to flexibly
choose appropriate defenses to satisfy given computation and/or utility
constraints in practice.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13077" title="Abstract">arXiv:2310.13077</a> [<a href="/pdf/2310.13077" title="Download PDF">pdf</a>, <a href="/format/2310.13077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuroSMPC: A Neural Network guided Sampling Based MPC for On-Road  Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pal%2C+K">Kaustab Pal</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Aditya Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Omama%2C+M">Mohd Omama</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+P+N">Parth N. Shah</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+K+M">K. Madhava Krishna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in 2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper we show an effective means of integrating data driven
frameworks to sampling based optimal control to vastly reduce the compute time
for easy adoption and adaptation to real time applications such as on-road
autonomous driving in the presence of dynamic actors. Presented with training
examples, a spatio-temporal CNN learns to predict the optimal mean control over
a finite horizon that precludes further resampling, an iterative process that
makes sampling based optimal control formulations difficult to adopt in real
time settings. Generating control samples around the network-predicted optimal
mean retains the advantage of sample diversity while enabling real time rollout
of trajectories that avoids multiple dynamic obstacles in an on-road navigation
setting. Further the 3D CNN architecture implicitly learns the future
trajectories of the dynamic agents in the scene resulting in successful
collision free navigation despite no explicit future trajectory prediction. We
show performance gain over multiple baselines in a number of on-road scenes
through closed loop simulations in CARLA. We also showcase the real world
applicability of our system by running it on our custom Autonomous Driving
Platform (AutoDP).
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13079" title="Abstract">arXiv:2310.13079</a> [<a href="/pdf/2310.13079" title="Download PDF">pdf</a>, <a href="/format/2310.13079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Critical Path Prioritization Dashboard for Alert-driven Attack Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz%2C+S+L">S&#xf2;nia Leal D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=Pastrana%2C+S">Sergio Pastrana</a>, 
<a href="/search/cs?searchtype=author&query=Nadeem%2C+A">Azqa Nadeem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Although intrusion alerts can provide threat intelligence regarding attacker
strategies, extracting such intelligence via existing tools is expensive and
time-consuming. Earlier work has proposed SAGE, which generates attack graphs
from intrusion alerts using unsupervised sequential machine learning. This
paper proposes a querying and prioritization-enabled visual analytics dashboard
for SAGE. The dashboard has three main components: (i) a Graph Explorer that
presents a global view of all attacker strategies, (ii) a Timeline Viewer that
correlates attacker actions chronologically, and (iii) a Recommender Matrix
that highlights prevalent critical alerts via a MITRE ATT&amp;CK-inspired attack
stage matrix. We describe the utility of the proposed dashboard using intrusion
alerts collected from a distributed multi-stage team-based attack scenario. We
evaluate the utility of the dashboard through a user study. Based on the
responses of a small set of security practitioners, we find that the dashboard
is useful in depicting attacker strategies and attack progression, but can be
improved in terms of usability.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13080" title="Abstract">arXiv:2310.13080</a> [<a href="/pdf/2310.13080" title="Download PDF">pdf</a>, <a href="/format/2310.13080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Multilingual Complexity to Emotional Clarity: Leveraging  Commonsense to Unveil Emotions in Code-Mixed Dialogues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Shivani Kumar</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+R">Ramaneswaran S</a>, 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+M+S">Md Shad Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted in EMNLP 2023. 15 pages, 6 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Understanding emotions during conversation is a fundamental aspect of human
communication, driving NLP research for Emotion Recognition in Conversation
(ERC). While considerable research has focused on discerning emotions of
individual speakers in monolingual dialogues, understanding the emotional
dynamics in code-mixed conversations has received relatively less attention.
This motivates our undertaking of ERC for code-mixed conversations in this
study. Recognizing that emotional intelligence encompasses a comprehension of
worldly knowledge, we propose an innovative approach that integrates
commonsense information with dialogue context to facilitate a deeper
understanding of emotions. To achieve this, we devise an efficient pipeline
that extracts relevant commonsense from existing knowledge graphs based on the
code-mixed input. Subsequently, we develop an advanced fusion technique that
seamlessly combines the acquired commonsense information with the dialogue
representation obtained from a dedicated dialogue understanding module. Our
comprehensive experimentation showcases the substantial performance improvement
obtained through the systematic incorporation of commonsense in ERC. Both
quantitative assessments and qualitative analyses further corroborate the
validity of our hypothesis, reaffirming the pivotal role of commonsense
integration in enhancing ERC.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13082" title="Abstract">arXiv:2310.13082</a> [<a href="/pdf/2310.13082" title="Download PDF">pdf</a>, <a href="/ps/2310.13082" title="Download PostScript">ps</a>, <a href="/format/2310.13082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge-disjoint paths in expanders: online with removals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dragani%C4%87%2C+N">Nemanja Dragani&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Nenadov%2C+R">Rajko Nenadov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">We consider the problem of finding edge-disjoint paths between given pairs of
vertices in a sufficiently strong $d$-regular expander graph $G$ with $n$
vertices. In particular, we describe a deterministic, polynomial time algorithm
which maintains an initially empty collection of edge-disjoint paths $\mathcal
P$ in $G$ and fulfills any series of two types of requests:
<br />1. Given two vertices $a$ and $b$ such that each appears as an endpoint in
$O(d)$ paths in $\mathcal P$ and, additionally, $|\mathcal P| = O(n d / \log
n)$, the algorithm finds a path of length at most $\log n$ connecting $a$ and
$b$ which is edge-disjoint from all other paths in $\mathcal P$, and adds it to
$\mathcal P$.
<br />2. Remove a given path $P \in \mathcal{P}$ from $\mathcal{P}$.
<br />Importantly, each request is processed before seeing the next one. The upper
bound on the length of found paths and the constraints are the best possible up
to a constant factor. This establishes the first online algorithm for finding
edge-disjoint paths in expanders which also allows removals, significantly
strengthening a long list of previous results on the topic.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13083" title="Abstract">arXiv:2310.13083</a> [<a href="/pdf/2310.13083" title="Download PDF">pdf</a>, <a href="/format/2310.13083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Can Everyday Users Efficiently Teach Robots by Demonstrations?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sakr%2C+M">Maram Sakr</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhikai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Benjamin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haomiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Van+der+Loos%2C+H+F+M">H.F. Machiel Van der Loos</a>, 
<a href="/search/cs?searchtype=author&query=Kulic%2C+D">Dana Kulic</a>, 
<a href="/search/cs?searchtype=author&query=Croft%2C+E">Elizabeth Croft</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning from Demonstration (LfD) is a framework that allows lay users to
easily program robots. However, the efficiency of robot learning and the
robot's ability to generalize to task variations hinges upon the quality and
quantity of the provided demonstrations. Our objective is to guide human
teachers to furnish more effective demonstrations, thus facilitating efficient
robot learning. To achieve this, we propose to use a measure of uncertainty,
namely task-related information entropy, as a criterion for suggesting
informative demonstration examples to human teachers to improve their teaching
skills. In a conducted experiment (N=24), an augmented reality (AR)-based
guidance system was employed to train novice users to produce additional
demonstrations from areas with the highest entropy within the workspace. These
novice users were trained for a few trials to teach the robot a generalizable
task using a limited number of demonstrations. Subsequently, the users'
performance after training was assessed first on the same task (retention) and
then on a novel task (transfer) without guidance. The results indicated a
substantial improvement in robot learning efficiency from the teacher's
demonstrations, with an improvement of up to 198% observed on the novel task.
Furthermore, the proposed approach was compared to a state-of-the-art heuristic
rule and found to improve robot learning efficiency by 210% compared to the
heuristic rule.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13085" title="Abstract">arXiv:2310.13085</a> [<a href="/pdf/2310.13085" title="Download PDF">pdf</a>, <a href="/format/2310.13085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Representation Learning to Aid Semi-Supervised Meta  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faysal%2C+A">Atik Faysal</a>, 
<a href="/search/cs?searchtype=author&query=Rostami%2C+M">Mohammad Rostami</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huaxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+A">Avimanyu Sahoo</a>, 
<a href="/search/cs?searchtype=author&query=Antle%2C+R">Ryan Antle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Few-shot learning or meta-learning leverages the data scarcity problem in
machine learning. Traditionally, training data requires a multitude of samples
and labeling for supervised learning. To address this issue, we propose a
one-shot unsupervised meta-learning to learn the latent representation of the
training samples. We use augmented samples as the query set during the training
phase of the unsupervised meta-learning. A temperature-scaled cross-entropy
loss is used in the inner loop of meta-learning to prevent overfitting during
unsupervised learning. The learned parameters from this step are applied to the
targeted supervised meta-learning in a transfer-learning fashion for
initialization and fast adaptation with improved accuracy. The proposed method
is model agnostic and can aid any meta-learning model to improve accuracy. We
use model agnostic meta-learning (MAML) and relation network (RN) on Omniglot
and mini-Imagenet datasets to demonstrate the performance of the proposed
method. Furthermore, a meta-learning model with the proposed initialization can
achieve satisfactory accuracy with significantly fewer training samples.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13090" title="Abstract">arXiv:2310.13090</a> [<a href="/pdf/2310.13090" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closed-Loop Motion Planning for Differentially Flat Systems: A  Time-Varying Optimization Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zheng%2C+T">Tianqi Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Simpson-Porco%2C+J+W">John W. Simpson-Porco</a>, 
<a href="/search/eess?searchtype=author&query=Mallada%2C+E">Enrique Mallada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Motion planning and control are two core components of the robotic systems
autonomy stack. The standard approach to combine these methodologies comprises
an offline/open-loop stage, planning, that designs a feasible and safe
trajectory to follow, and an online/closed-loop stage, tracking, that corrects
for unmodeled dynamics and disturbances. Such an approach generally introduces
conservativeness into the planning stage, which becomes difficult to overcome
as the model complexity increases and real-time decisions need to be made in a
changing environment. This work addresses these challenges for the class of
differentially flat nonlinear systems by integrating planning and control into
a cohesive closed-loop task. Precisely, we develop an optimization-based
framework that aims to steer a differentially flat system to a trajectory
implicitly defined via a constrained time-varying optimization problem. To that
end, we generalize the notion of feedback linearization, which makes non-linear
systems behave as linear systems, and develop controllers that effectively
transform a differentially flat system into an optimization algorithm that
seeks to find the optimal solution of a (possibly time-varying) optimization
problem. Under sufficient regularity assumptions, we prove global asymptotic
convergence for the optimization dynamics to the minimizer of the time-varying
optimization problem. We illustrate the effectiveness of our method with two
numerical examples: a multi-robot tracking problem and a robot obstacle
avoidance problem.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13091" title="Abstract">arXiv:2310.13091</a> [<a href="/pdf/2310.13091" title="Download PDF">pdf</a>, <a href="/format/2310.13091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Propeller Damage Estimation and Adaptation to Fault Tolerant  Control: Enhancing Quadrotor Resilience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jeffrey Mao</a>, 
<a href="/search/cs?searchtype=author&query=Yeom%2C+J">Jennifer Yeom</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+S">Suraj Nair</a>, 
<a href="/search/cs?searchtype=author&query=Loianno%2C+G">Giuseppe Loianno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages, 8 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Aerial robots are required to remain operational even in the event of system
disturbances, damages, or failures to ensure resilient and robust task
completion and safety. One common failure case is propeller damage, which
presents a significant challenge in both quantification and compensation. We
propose a novel adaptive control scheme capable of detecting and compensating
for multi-rotor propeller damages, ensuring safe and robust flight
performances. Our control scheme includes an L1 adaptive controller for damage
inference and compensation of single or dual propellers, with the capability to
seamlessly transition to a fault-tolerant solution in case the damage becomes
severe. We experimentally identify the conditions under which the L1 adaptive
solution remains preferable over a fault-tolerant alternative. Experimental
results validate the proposed approach, demonstrating its effectiveness in
running the adaptive strategy in real time on a quadrotor even in case of
damage to multiple propellers.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13092" title="Abstract">arXiv:2310.13092</a> [<a href="/pdf/2310.13092" title="Download PDF">pdf</a>, <a href="/format/2310.13092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Language Models Learn about Legal Entity Types during Pretraining?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barale%2C+C">Claire Barale</a>, 
<a href="/search/cs?searchtype=author&query=Rovatsos%2C+M">Michael Rovatsos</a>, 
<a href="/search/cs?searchtype=author&query=Bhuta%2C+N">Nehal Bhuta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 5th Natural Legal Language Processing Workshop (NLLP) hosted at EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language Models (LMs) have proven their ability to acquire diverse linguistic
knowledge during the pretraining phase, potentially serving as a valuable
source of incidental supervision for downstream tasks. However, there has been
limited research conducted on the retrieval of domain-specific knowledge, and
specifically legal knowledge. We propose to explore the task of Entity Typing,
serving as a proxy for evaluating legal knowledge as an essential aspect of
text comprehension, and a foundational task to numerous downstream legal NLP
applications. Through systematic evaluation and analysis and two types of
prompting (cloze sentences and QA-based templates) and to clarify the nature of
these acquired cues, we compare diverse types and lengths of entities both
general and domain-specific entities, semantics or syntax signals, and
different LM pretraining corpus (generic and legal-oriented) and architectures
(encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2
performs well on certain entities and exhibits potential for substantial
improvement with optimized prompt templates, (2) law-oriented LMs show
inconsistent performance, possibly due to variations in their training corpus,
(3) LMs demonstrate the ability to type entities even in the case of
multi-token entities, (4) all models struggle with entities belonging to
sub-domains of the law (5) Llama2 appears to frequently overlook syntactic
cues, a shortcoming less present in BERT-based architectures.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13097" title="Abstract">arXiv:2310.13097</a> [<a href="/pdf/2310.13097" title="Download PDF">pdf</a>, <a href="/format/2310.13097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Stage Temporal Convolutional Network for Volleyball Jumps  Classification Using a Waist-Mounted IMU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shang%2C+M">Meng Shang</a>, 
<a href="/search/cs?searchtype=author&query=De+Bleecker%2C+C">Camilla De Bleecker</a>, 
<a href="/search/cs?searchtype=author&query=Vanrenterghem%2C+J">Jos Vanrenterghem</a>, 
<a href="/search/cs?searchtype=author&query=De+Ridder%2C+R">Roel De Ridder</a>, 
<a href="/search/cs?searchtype=author&query=Verschueren%2C+S">Sabine Verschueren</a>, 
<a href="/search/cs?searchtype=author&query=Varon%2C+C">Carolina Varon</a>, 
<a href="/search/cs?searchtype=author&query=De+Raedt%2C+W">Walter De Raedt</a>, 
<a href="/search/cs?searchtype=author&query=Vanrumste%2C+B">Bart Vanrumste</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Monitoring the number of jumps for volleyball players during training or a
match can be crucial to prevent injuries, yet the measurement requires
considerable workload and cost using traditional methods such as video
analysis. Also, existing methods do not provide accurate differentiation
between different types of jumps. In this study, an unobtrusive system with a
single inertial measurement unit (IMU) on the waist was proposed to recognize
the types of volleyball jumps. A Multi-Layer Temporal Convolutional Network
(MS-TCN) was applied for sample-wise classification. The model was evaluated on
ten volleyball players and twenty-six volleyball players, during a lab session
with a fixed protocol of jumping and landing tasks, and during four volleyball
training sessions, respectively. The MS-TCN model achieved better performance
than a state-of-the-art deep learning model but with lower computational cost.
In the lab sessions, most jump counts showed small differences between the
predicted jumps and video-annotated jumps, with an overall count showing a
Limit of Agreement (LoA) of 0.1+-3.40 (r=0.884). For comparison, the proposed
algorithm showed slightly worse results than VERT (a commercial jumping
assessment device) with a LoA of 0.1+-2.08 (r=0.955) but the differences were
still within a comparable range. In the training sessions, the recognition of
three types of jumps exhibited a mean difference from observation of less than
10 jumps: block, smash, and overhead serve. These results showed the potential
of using a single IMU to recognize the types of volleyball jumps. The
sample-wise architecture provided high resolution of recognition and the MS-TCN
required fewer parameters to train compared with state-of-the-art models.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13098" title="Abstract">arXiv:2310.13098</a> [<a href="/pdf/2310.13098" title="Download PDF">pdf</a>, <a href="/format/2310.13098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SRAI: Towards Standardization of Geospatial AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gramacki%2C+P">Piotr Gramacki</a>, 
<a href="/search/cs?searchtype=author&query=Le%C5%9Bniara%2C+K">Kacper Le&#x15b;niara</a>, 
<a href="/search/cs?searchtype=author&query=Raczycki%2C+K">Kamil Raczycki</a>, 
<a href="/search/cs?searchtype=author&query=Wo%C5%BAniak%2C+S">Szymon Wo&#x17a;niak</a>, 
<a href="/search/cs?searchtype=author&query=Przymus%2C+M">Marcin Przymus</a>, 
<a href="/search/cs?searchtype=author&query=Szyma%C5%84ski%2C+P">Piotr Szyma&#x144;ski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spatial Representations for Artificial Intelligence (\textit{srai}) is a
Python library for working with geospatial data. The library can download
geospatial data, split a given area into micro-regions using multiple
algorithms and train an embedding model using various architectures. It
includes baseline models as well as more complex methods from published works.
Those capabilities make it possible to use \textit{srai} in a complete pipeline
for geospatial task solving. The proposed library is the first step to
standardize the geospatial AI domain toolset. It is fully open-source and
published under Apache 2.0 licence.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13099" title="Abstract">arXiv:2310.13099</a> [<a href="/pdf/2310.13099" title="Download PDF">pdf</a>, <a href="/format/2310.13099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No offence, Bert -- I insult only humans! Multiple addressees  sentence-level attack on toxicity detection neural network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berezin%2C+S">Sergey Berezin</a>, 
<a href="/search/cs?searchtype=author&query=Farahbakhsh%2C+R">Reza Farahbakhsh</a>, 
<a href="/search/cs?searchtype=author&query=Crespi%2C+N">Noel Crespi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a simple yet efficient sentence-level attack on black-box
toxicity detector models. By adding several positive words or sentences to the
end of a hateful message, we are able to change the prediction of a neural
network and pass the toxicity detection system check. This approach is shown to
be working on seven languages from three different language families. We also
describe the defence mechanism against the aforementioned attack and discuss
its limitations.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13102" title="Abstract">arXiv:2310.13102</a> [<a href="/pdf/2310.13102" title="Download PDF">pdf</a>, <a href="/format/2310.13102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corso%2C+G">Gabriele Corso</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yilun Xu</a>, 
<a href="/search/cs?searchtype=author&query=de+Bortoli%2C+V">Valentin de Bortoli</a>, 
<a href="/search/cs?searchtype=author&query=Barzilay%2C+R">Regina Barzilay</a>, 
<a href="/search/cs?searchtype=author&query=Jaakkola%2C+T">Tommi Jaakkola</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In light of the widespread success of generative models, a significant amount
of research has gone into speeding up their sampling time. However, generative
models are often sampled multiple times to obtain a diverse set incurring a
cost that is orthogonal to sampling time. We tackle the question of how to
improve diversity and sample efficiency by moving beyond the common assumption
of independent samples. We propose particle guidance, an extension of
diffusion-based generative sampling where a joint-particle time-evolving
potential enforces diversity. We analyze theoretically the joint distribution
that particle guidance generates, its implications on the choice of potential,
and the connections with methods in other disciplines. Empirically, we test the
framework both in the setting of conditional image generation, where we are
able to increase diversity without affecting quality, and molecular conformer
generation, where we reduce the state-of-the-art median error by 13% on
average.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13103" title="Abstract">arXiv:2310.13103</a> [<a href="/pdf/2310.13103" title="Download PDF">pdf</a>, <a href="/format/2310.13103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting  Multiple Experts for Video Deepfake Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashmi%2C+A">Ammarah Hashmi</a>, 
<a href="/search/cs?searchtype=author&query=Shahzad%2C+S+A">Sahibzada Adil Shahzad</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chia-Wen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hsin-Min Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Forged content shared widely on social media platforms is a major social
problem that requires increased regulation and poses new challenges to the
research community. The recent proliferation of hyper-realistic deepfake videos
has drawn attention to the threat of audio and visual forgeries. Most previous
work on detecting AI-generated fake videos only utilizes visual modality or
audio modality. While there are some methods in the literature that exploit
audio and visual modalities to detect forged videos, they have not been
comprehensively evaluated on multi-modal datasets of deepfake videos involving
acoustic and visual manipulations. Moreover, these existing methods are mostly
based on CNN and suffer from low detection accuracy. Inspired by the recent
success of Transformer in various fields, to address the challenges posed by
deepfake technology, in this paper, we propose an Audio-Visual
Transformer-based Ensemble Network (AVTENet) framework that considers both
acoustic manipulation and visual manipulation to achieve effective video
forgery detection. Specifically, the proposed model integrates several purely
transformer-based variants that capture video, audio, and audio-visual salient
cues to reach a consensus in prediction. For evaluation, we use the recently
released benchmark multi-modal audio-video FakeAVCeleb dataset. For a detailed
analysis, we evaluate AVTENet, its variants, and several existing methods on
multiple test sets of the FakeAVCeleb dataset. Experimental results show that
our best model outperforms all existing methods and achieves state-of-the-art
performance on Testset-I and Testset-II of the FakeAVCeleb dataset.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13104" title="Abstract">arXiv:2310.13104</a> [<a href="/pdf/2310.13104" title="Download PDF">pdf</a>, <a href="/format/2310.13104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Differential Privacy Easier to Use for Data Controllers and Data  Analysts using a Privacy Risk Indicator and an Escrow-Based Platform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhiru Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+R+C">Raul Castro Fernandez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Differential privacy (DP) enables private data analysis but is hard to use in
practice. For data controllers who decide what output to release, choosing the
amount of noise to add to the output is a non-trivial task because of the
difficulty of interpreting the privacy parameter $\epsilon$. For data analysts
who submit queries, it is hard to understand the impact of the noise introduced
by DP on their tasks.
<br />To address these two challenges: 1) we define a privacy risk indicator that
indicates the impact of choosing $\epsilon$ on individuals' privacy and use
that to design an algorithm that chooses $\epsilon$ automatically; 2) we
introduce a utility signaling protocol that helps analysts interpret the impact
of DP on their downstream tasks. We implement the algorithm and the protocol
inside a new platform built on top of a data escrow, which allows the
controller to control the data flow and achieve trustworthiness while
maintaining high performance. We demonstrate our contributions through an
IRB-approved user study, extensive experimental evaluations, and comparison
with other DP platforms. All in all, our work contributes to making DP easier
to use by lowering adoption barriers.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13106" title="Abstract">arXiv:2310.13106</a> [<a href="/pdf/2310.13106" title="Download PDF">pdf</a>, <a href="/format/2310.13106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Candidate Answer Extraction through Differentiable  Masker-Reconstructor Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhuoer Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Ziwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Caverlee%2C+J">James Caverlee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 - Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Question generation is a widely used data augmentation approach with
extensive applications, and extracting qualified candidate answers from context
passages is a critical step for most question generation systems. However,
existing methods for candidate answer extraction are reliant on linguistic
rules or annotated data that face the partial annotation issue and challenges
in generalization. To overcome these limitations, we propose a novel
unsupervised candidate answer extraction approach that leverages the inherent
structure of context passages through a Differentiable Masker-Reconstructor
(DMR) Model with the enforcement of self-consistency for picking up salient
information tokens. We curated two datasets with exhaustively-annotated answers
and benchmark a comprehensive set of supervised and unsupervised candidate
answer extraction methods. We demonstrate the effectiveness of the DMR model by
showing its performance is superior among unsupervised methods and comparable
to supervised methods.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13110" title="Abstract">arXiv:2310.13110</a> [<a href="/pdf/2310.13110" title="Download PDF">pdf</a>, <a href="/format/2310.13110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Learning of Dynamical Systems with Neural Ordinary  Differential Equations: A Teacher-Student Model Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yuxuan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Suryanarayana%2C+K+S+N">Karthik Somayaji Nanjangud Suryanarayana</a>, 
<a href="/search/cs?searchtype=author&query=Drgona%2C+J">Jan Drgona</a>, 
<a href="/search/cs?searchtype=author&query=Schram%2C+M">Malachi Schram</a>, 
<a href="/search/cs?searchtype=author&query=Halappanavar%2C+M">Mahantesh Halappanavar</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Frank Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Modeling dynamical systems is crucial for a wide range of tasks, but it
remains challenging due to complex nonlinear dynamics, limited observations, or
lack of prior knowledge. Recently, data-driven approaches such as Neural
Ordinary Differential Equations (NODE) have shown promising results by
leveraging the expressive power of neural networks to model unknown dynamics.
However, these approaches often suffer from limited labeled training data,
leading to poor generalization and suboptimal predictions. On the other hand,
semi-supervised algorithms can utilize abundant unlabeled data and have
demonstrated good performance in classification and regression tasks. We
propose TS-NODE, the first semi-supervised approach to modeling dynamical
systems with NODE. TS-NODE explores cheaply generated synthetic pseudo rollouts
to broaden exploration in the state space and to tackle the challenges brought
by lack of ground-truth system data under a teacher-student model. TS-NODE
employs an unified optimization framework that corrects the teacher model based
on the student's feedback while mitigating the potential false system dynamics
present in pseudo rollouts. TS-NODE demonstrates significant performance
improvements over a baseline Neural ODE model on multiple dynamical system
modeling tasks.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13119" title="Abstract">arXiv:2310.13119</a> [<a href="/pdf/2310.13119" title="Download PDF">pdf</a>, <a href="/format/2310.13119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DreamSpace: Dreaming Your Room Space with Text-Driven Panoramic Texture  Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bangbang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Wenqi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenbo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Z">Zhaopeng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuewen Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project webpage: <a href="https://ybbbbt.com/publication/dreamspace">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Diffusion-based methods have achieved prominent success in generating 2D
media. However, accomplishing similar proficiencies for scene-level mesh
texturing in 3D spatial applications, e.g., XR/VR, remains constrained,
primarily due to the intricate nature of 3D geometry and the necessity for
immersive free-viewpoint rendering. In this paper, we propose a novel indoor
scene texturing framework, which delivers text-driven texture generation with
enchanting details and authentic spatial coherence. The key insight is to first
imagine a stylized 360{\deg} panoramic texture from the central viewpoint of
the scene, and then propagate it to the rest areas with inpainting and
imitating techniques. To ensure meaningful and aligned textures to the scene,
we develop a novel coarse-to-fine panoramic texture generation approach with
dual texture alignment, which both considers the geometry and texture cues of
the captured scenes. To survive from cluttered geometries during texture
propagation, we design a separated strategy, which conducts texture inpainting
in confidential regions and then learns an implicit imitating network to
synthesize textures in occluded and tiny structural areas. Extensive
experiments and the immersive VR application on real-world indoor scenes
demonstrate the high quality of the generated textures and the engaging
experience on VR headsets. Project webpage:
https://ybbbbt.com/publication/dreamspace
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13120" title="Abstract">arXiv:2310.13120</a> [<a href="/pdf/2310.13120" title="Download PDF">pdf</a>, <a href="/format/2310.13120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuduo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ghamisi%2C+P">Pedram Ghamisi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, with the rapid advancement of transformer models,
transformer-based multimodal architectures have found wide application in
various downstream tasks, including but not limited to Image Captioning, Visual
Question Answering (VQA), and Image-Text Generation. However, contemporary
approaches to Remote Sensing (RS) VQA often involve resource-intensive
techniques, such as full fine-tuning of large models or the extraction of
image-text features from pre-trained multimodal models, followed by modality
fusion using decoders. These approaches demand significant computational
resources and time, and a considerable number of trainable parameters are
introduced. To address these challenges, we introduce a novel method known as
RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter
comprises two key components: the Parallel Adapter and an additional linear
transformation layer inserted after each fully connected (FC) layer within the
Adapter. This approach not only improves adaptation to pre-trained multimodal
models but also allows the parameters of the linear transformation layer to be
integrated into the preceding FC layers during inference, reducing inference
costs. To demonstrate the effectiveness of RSAdapter, we conduct an extensive
series of experiments using three distinct RS-VQA datasets and achieve
state-of-the-art results on all three datasets. The code for RSAdapter will be
available online at https://github.com/Y-D-Wang/RSAdapter.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13121" title="Abstract">arXiv:2310.13121</a> [<a href="/pdf/2310.13121" title="Download PDF">pdf</a>, <a href="/format/2310.13121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Addition in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quirke%2C+P">Philip Quirke</a>, 
<a href="/search/cs?searchtype=author&query=Fazl">Fazl</a> (Kiko)
<a href="/search/cs?searchtype=author&query=Barez">Barez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures, submitted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Understanding the inner workings of machine learning models like Transformers
is vital for their safe and ethical use. This paper presents an in-depth
analysis of a one-layer Transformer model trained for integer addition. We
reveal that the model divides the task into parallel, digit-specific streams
and employs distinct algorithms for different digit positions. Our study also
finds that the model starts calculations late but executes them rapidly. A rare
use case with high loss is identified and explained. Overall, the model's
algorithm is explained in detail. These findings are validated through rigorous
testing and mathematical modeling, contributing to the broader works in
Mechanistic Interpretability, AI safety, and alignment. Our approach opens the
door for analyzing more complex tasks and multi-layer Transformer models.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13123" title="Abstract">arXiv:2310.13123</a> [<a href="/pdf/2310.13123" title="Download PDF">pdf</a>, <a href="/format/2310.13123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuel Consumption Prediction for a Passenger Ferry using Machine Learning  and In-service Data: A Comparative Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agand%2C+P">Pedram Agand</a>, 
<a href="/search/cs?searchtype=author&query=Kennedy%2C+A">Allison Kennedy</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+T">Trevor Harris</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+C">Chanwoo Bae</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+E+J">Edward J Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures, 7 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ocean Engineering 284 (2023): 115271
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">As the importance of eco-friendly transportation increases, providing an
efficient approach for marine vessel operation is essential. Methods for status
monitoring with consideration to the weather condition and forecasting with the
use of in-service data from ships requires accurate and complete models for
predicting the energy efficiency of a ship. The models need to effectively
process all the operational data in real-time. This paper presents models that
can predict fuel consumption using in-service data collected from a passenger
ship. Statistical and domain-knowledge methods were used to select the proper
input variables for the models. These methods prevent over-fitting, missing
data, and multicollinearity while providing practical applicability. Prediction
models that were investigated include multiple linear regression (MLR),
decision tree approach (DT), an artificial neural network (ANN), and ensemble
methods. The best predictive performance was from a model developed using the
XGboost technique which is a boosting ensemble approach. \rvv{Our code is
available on GitHub at
\url{https://github.com/pagand/model_optimze_vessel/tree/OE} for future
research.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13124" title="Abstract">arXiv:2310.13124</a> [<a href="/pdf/2310.13124" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient online cross-covariance monitoring with incremental SVD: An  approach for the detection of emerging dependency patterns in IoT systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Luan%2C+X">Xinmiao Luan</a>, 
<a href="/search/eess?searchtype=author&query=Zou%2C+Q">Qing Zou</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jian Li</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+A">Andi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The development of the manufacturing systems has made it increasingly
necessary to monitor the data generated by multiple interconnected subsystems
with rapid incoming of samples. Based on incremental Singular Value
Decomposition (ISVD), we develop a general online monitoring approach for the
relationship of data generated from two interconnected subsystems, where each
subsystem produces big data streams with several variation patterns in normal
working condition. When special situation happens and new associations occur, a
very small amount of computation is sufficient to update the system status and
compute the control statistics by using this approach. The proposed method
reduces computational overhead and retains only a small number of pairs of
possible dependent patterns at each step. The validation of the method through
simulation studies and a case study on semiconductor manufacturing processes
further supports its effectiveness.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13127" title="Abstract">arXiv:2310.13127</a> [<a href="/pdf/2310.13127" title="Download PDF">pdf</a>, <a href="/format/2310.13127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto-Instruct: Automatic Instruction Generation and Ranking for  Black-Box Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuohang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yichong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Iter%2C+D">Dan Iter</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qingkai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenguang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings. Work was done before July 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) can perform a wide range of tasks by following
natural language instructions, without the necessity of task-specific
fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by
the quality of these instructions, and manually writing effective instructions
for each task is a laborious and subjective process. In this paper, we
introduce Auto-Instruct, a novel method to automatically improve the quality of
instructions provided to LLMs. Our method leverages the inherent generative
ability of LLMs to produce diverse candidate instructions for a given task, and
then ranks them using a scoring model trained on a variety of 575 existing NLP
tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both
human-written instructions and existing baselines of LLM-generated
instructions. Furthermore, our method exhibits notable generalizability even
with other LLMs that are not incorporated into its training process.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13129" title="Abstract">arXiv:2310.13129</a> [<a href="/pdf/2310.13129" title="Download PDF">pdf</a>, <a href="/format/2310.13129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning-based Intelligent Traffic Signal Controls  with Optimized CO2 emissions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Agand%2C+P">Pedram Agand</a>, 
<a href="/search/eess?searchtype=author&query=Iskrov%2C+A">Alexey Iskrov</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+M">Mo Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, 1 table. International Conference on Intelligent Robots and Systems. IEEE/RSJ, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Nowadays, transportation networks face the challenge of sub-optimal control
policies that can have adverse effects on human health, the environment, and
contribute to traffic congestion. Increased levels of air pollution and
extended commute times caused by traffic bottlenecks make intersection traffic
signal controllers a crucial component of modern transportation infrastructure.
Despite several adaptive traffic signal controllers in literature, limited
research has been conducted on their comparative performance. Furthermore,
despite carbon dioxide (CO2) emissions' significance as a global issue, the
literature has paid limited attention to this area. In this report, we propose
EcoLight, a reward shaping scheme for reinforcement learning algorithms that
not only reduces CO2 emissions but also achieves competitive results in metrics
such as travel time. We compare the performance of tabular Q-Learning, DQN,
SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions,
waiting time, and stopped time. Our evaluation considers multiple scenarios
that encompass a range of road users (trucks, buses, cars) with varying
pollution levels.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13132" title="Abstract">arXiv:2310.13132</a> [<a href="/pdf/2310.13132" title="Download PDF">pdf</a>, <a href="/format/2310.13132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask Me in English Instead: Cross-Lingual Evaluation of Large Language  Models for Healthcare Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> Jin, 
<a href="/search/cs?searchtype=author&query=Yiqiao">Yiqiao</a>, 
<a href="/search/cs?searchtype=author&query=Chandra">Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Mohit">Mohit</a>, 
<a href="/search/cs?searchtype=author&query=Verma">Verma</a>, 
<a href="/search/cs?searchtype=author&query=Gaurav">Gaurav</a>, Hu, 
<a href="/search/cs?searchtype=author&query=Yibo">Yibo</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+D">De Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Munmun">Munmun</a>, 
<a href="/search/cs?searchtype=author&query=Kumar">Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Srijan">Srijan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are transforming the ways the general public
accesses and consumes information. Their influence is particularly pronounced
in pivotal sectors like healthcare, where lay individuals are increasingly
appropriating LLMs as conversational agents for everyday queries. While LLMs
demonstrate impressive language understanding and generation proficiencies,
concerns regarding their safety remain paramount in these high-stake domains.
Moreover, the development of LLMs is disproportionately focused on English. It
remains unclear how these LLMs perform in the context of non-English languages,
a gap that is critical for ensuring equity in the real-world use of these
systems.This paper provides a framework to investigate the effectiveness of
LLMs as multi-lingual dialogue systems for healthcare queries. Our
empirically-derived framework XlingEval focuses on three fundamental criteria
for evaluating LLM responses to naturalistic human-authored health-related
questions: correctness, consistency, and verifiability. Through extensive
experiments on four major global languages, including English, Spanish,
Chinese, and Hindi, spanning three expert-annotated large health Q&amp;A datasets,
and through an amalgamation of algorithmic and human-evaluation strategies, we
found a pronounced disparity in LLM responses across these languages,
indicating a need for enhanced cross-lingual capabilities. We further propose
XlingHealth, a cross-lingual benchmark for examining the multilingual
capabilities of LLMs in the healthcare context. Our findings underscore the
pressing need to bolster the cross-lingual capacities of these models, and to
provide an equitable information ecosystem accessible to all.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13134" title="Abstract">arXiv:2310.13134</a> [<a href="/pdf/2310.13134" title="Download PDF">pdf</a>, <a href="/format/2310.13134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient, Dynamic Locomotion through Step Placement with Straight Legs  and Rolling Contacts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fasano%2C+S">Stefan Fasano</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+J">James Foster</a>, 
<a href="/search/cs?searchtype=author&query=Bertrand%2C+S">Sylvain Bertrand</a>, 
<a href="/search/cs?searchtype=author&query=DeBuys%2C+C">Christian DeBuys</a>, 
<a href="/search/cs?searchtype=author&query=Griffin%2C+R">Robert Griffin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">For humans, fast, efficient walking over flat ground represents the vast
majority of locomotion that an individual experiences on a daily basis, and for
an effective, real-world humanoid robot the same will likely be the case. In
this work, we propose a locomotion controller for efficient walking over
near-flat ground using a relatively simple, model-based controller that
utilizes a novel combination of several interesting design features including
an ALIP-based step adjustment strategy, stance leg length control as an
alternative to center of mass height control, and rolling contact for
heel-to-toe motion of the stance foot. We then present the results of this
controller on our robot Nadia, both in simulation and on hardware. These
results include validation of this controller's ability to perform fast,
reliable forward walking at 0.75 m/s along with backwards walking,
side-stepping, turning in place, and push recovery. We also present an
efficiency comparison between the proposed control strategy and our baseline
walking controller over three steady-state walking speeds. Lastly, we
demonstrate some of the benefits of utilizing rolling contact in the stance
foot, specifically the reduction of necessary positive and negative work
throughout the stride.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13135" title="Abstract">arXiv:2310.13135</a> [<a href="/pdf/2310.13135" title="Download PDF">pdf</a>, <a href="/format/2310.13135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LeTFuser: Light-weight End-to-end Transformer-Based Sensor Fusion for  Autonomous Driving with Multi-Task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agand%2C+P">Pedram Agand</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavian%2C+M">Mohammad Mahdavian</a>, 
<a href="/search/cs?searchtype=author&query=Savva%2C+M">Manolis Savva</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mo Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures, 3 tables. CVPR Workshops (VCAD). 2023. arXiv admin note: text overlap with <a href="/abs/2204.05513">arXiv:2204.05513</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In end-to-end autonomous driving, the utilization of existing sensor fusion
techniques for imitation learning proves inadequate in challenging situations
that involve numerous dynamic agents. To address this issue, we introduce
LeTFuser, a transformer-based algorithm for fusing multiple RGB-D camera
representations. To perform perception and control tasks simultaneously, we
utilize multi-task learning. Our model comprises of two modules, the first
being the perception module that is responsible for encoding the observation
data obtained from the RGB-D cameras. It carries out tasks such as semantic
segmentation, semantic depth cloud mapping (SDC), and traffic light state
recognition. Our approach employs the Convolutional vision Transformer (CvT)
\cite{wu2021cvt} to better extract and fuse features from multiple RGB cameras
due to local and global feature extraction capability of convolution and
transformer modules, respectively. Following this, the control module
undertakes the decoding of the encoded characteristics together with
supplementary data, comprising a rough simulator for static and dynamic
environments, as well as various measurements, in order to anticipate the
waypoints associated with a latent feature space. We use two methods to process
these outputs and generate the vehicular controls (e.g. steering, throttle, and
brake) levels. The first method uses a PID algorithm to follow the waypoints on
the fly, whereas the second one directly predicts the control policy using the
measurement features and environmental state. We evaluate the model and conduct
a comparative analysis with recent models on the CARLA simulator using various
scenarios, ranging from normal to adversarial conditions, to simulate
real-world scenarios. Our code is available at
\url{https://github.com/pagand/e2etransfuser/tree/cvpr-w} to facilitate future
studies.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13137" title="Abstract">arXiv:2310.13137</a> [<a href="/pdf/2310.13137" title="Download PDF">pdf</a>, <a href="/format/2310.13137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean Estimation Under Heterogeneous Privacy Demands
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+S">Syomantak Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Miagkov%2C+K">Konstantin Miagkov</a>, 
<a href="/search/cs?searchtype=author&query=Courtade%2C+T+A">Thomas A. Courtade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preliminary conference version was published at ISIT 2023 and uploaded to arxiv (<a href="/abs/2305.09668">arXiv:2305.09668</a>). This version significantly expands on the previous article and is being submitted to a journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Differential Privacy (DP) is a well-established framework to quantify privacy
loss incurred by any algorithm. Traditional formulations impose a uniform
privacy requirement for all users, which is often inconsistent with real-world
scenarios in which users dictate their privacy preferences individually. This
work considers the problem of mean estimation, where each user can impose their
own distinct privacy level. The algorithm we propose is shown to be minimax
optimal and has a near-linear run-time. Our results elicit an interesting
saturation phenomenon that occurs. Namely, the privacy requirements of the most
stringent users dictate the overall error rates. As a consequence, users with
less but differing privacy requirements are all given more privacy than they
require, in equal amounts. In other words, these privacy-indifferent users are
given a nontrivial degree of privacy for free, without any sacrifice in the
performance of the estimator.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13139" title="Abstract">arXiv:2310.13139</a> [<a href="/pdf/2310.13139" title="Download PDF">pdf</a>, <a href="/ps/2310.13139" title="Download PostScript">ps</a>, <a href="/format/2310.13139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks with polynomial activations have limited  expressivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalife%2C+S">Sammy Khalife</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The expressivity of Graph Neural Networks (GNNs) can be entirely
characterized by appropriate fragments of the first order logic. Namely, any
query of the two variable fragment of graded modal logic (GC2) interpreted over
labelled graphs can be expressed using a GNN whose size depends only on the
depth of the query. As pointed out by [Barcelo &amp; Al., 2020, Grohe, 2021 ], this
description holds for a family of activation functions, leaving the
possibibility for a hierarchy of logics expressible by GNNs depending on the
chosen activation function. In this article, we show that such hierarchy indeed
exists by proving that GC2 queries cannot be expressed by GNNs with polynomial
activation functions. This implies a separation between polynomial and popular
non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and
others) and answers an open question formulated by [Grohe, 2021].
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13140" title="Abstract">arXiv:2310.13140</a> [<a href="/pdf/2310.13140" title="Download PDF">pdf</a>, <a href="/format/2310.13140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Preserving Decision Tree Training and Prediction via Fully  Homomorphic Encryption with No Decryption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hunjae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+C">Corey Clark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With data-outsourcing becoming commonplace, there grows a need for secure
outsourcing of data and machine learning models. Namely, data and model owners
(client) often have a need for their information to remain private and secure
against the potentially untrusted computing resource (server) to whom they want
to outsource said data and models to. Various approaches to privacy-preserving
machine learning (PPML) have been devised with different techniques and
solutions introduced in the past. These solutions often involved one of two
compromises: (1) client-server interactions to allow intermediary rounds of
decryption and re-encryption of data or (2) complex architectures for
multi-party computation. This paper devises a paradigm using Fully Homomorphic
Encryption (FHE) that minimizes architectural complexity and removes
client-side involvement during the training and prediction lifecycle of machine
learning models. In addition, the paradigm proposed in this work achieves both
model security as well as data security. To remove client-side involvement, the
devised paradigm proposes a no decryption approach that allows the server to
handle PPML in its entirety without rounds of decryption and re-encryption. To
the best of our knowledge, this paradigm is the first to achieve
privacy-preserving decision tree training with no decryption while maintaining
a simple client-server architecture.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13141" title="Abstract">arXiv:2310.13141</a> [<a href="/pdf/2310.13141" title="Download PDF">pdf</a>, <a href="/ps/2310.13141" title="Download PostScript">ps</a>, <a href="/format/2310.13141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impartial Rank Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cembrano%2C+J">Javier Cembrano</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+F">Felix Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Klimm%2C+M">Max Klimm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study functions that produce a ranking of $n$ individuals from $n$ such
rankings and are impartial in the sense that the position of an individual in
the output ranking does not depend on the input ranking submitted by that
individual. When $n \geq 4$, two properties concerning the quality of the
output in relation to the input can be achieved in addition to impartiality:
individual full rank, which requires that each individual can appear in any
position of the output ranking; and monotonicity, which requires that an
individual cannot move down in the output ranking if it moves up in an input
ranking. When $n \geq 5$, monotonicity can be dropped to strengthen individual
full rank to weak unanimity, requiring that a ranking submitted by every
individual must be chosen as the output ranking. Mechanisms achieving these
results can be implemented in polynomial time. Both results are best possible
in terms of their dependence on $n$. The second result cannot be strengthened
further to a notion of unanimity that requires agreement on pairwise
comparisons to be preserved.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13144" title="Abstract">arXiv:2310.13144</a> [<a href="/pdf/2310.13144" title="Download PDF">pdf</a>, <a href="/format/2310.13144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Symbolic Bound Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cyphert%2C+J">John Cyphert</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+Y">Yotam Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Kincaid%2C+Z">Zachary Kincaid</a>, 
<a href="/search/cs?searchtype=author&query=Reps%2C+T">Thomas Reps</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">The problem of finding a constant bound on a term given a set of assumptions
has wide applications in optimization as well as program analysis. However, in
many contexts the objective term may be unbounded. Still, some sort of symbolic
bound may be useful. In this paper we introduce the optimal symbolic-bound
synthesis problem, and a technique that tackles this problem for non-linear
arithmetic with function symbols. This allows us to automatically produce
symbolic bounds on complex arithmetic expressions from a set of both equality
and inequality assumptions. Our solution employs a novel combination of
powerful mathematical objects -- Gr\"obner bases together with polyhedral cones
-- to represent an infinite set of implied inequalities. We obtain a sound
symbolic bound by reducing the objective term by this infinite set. We
implemented our method in a tool, AutoBound, which we tested on problems
originating from real Solidity programs. We find that AutoBound yields relevant
bounds in each case, matching or nearly-matching upper bounds produced by a
human analyst on the same set of programs.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13146" title="Abstract">arXiv:2310.13146</a> [<a href="/pdf/2310.13146" title="Download PDF">pdf</a>, <a href="/format/2310.13146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIFT: Analysing Natural Distribution Shift on Question Answering Models  in Clinical Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pal%2C+A">Ankit Pal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2022 (Robustness in Sequence Modeling)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 36th Conference on Neural Information Processing Systems,
  Robustness in Sequence Modeling (NeurIPS 2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces a new testbed CLIFT (Clinical Shift) for the clinical
domain Question-answering task. The testbed includes 7.5k high-quality question
answering samples to provide a diverse and reliable benchmark. We performed a
comprehensive experimental study and evaluated several QA deep-learning models
under the proposed testbed. Despite impressive results on the original test
set, the performance degrades when applied to new test sets, which shows the
distribution shift. Our findings emphasize the need for and the potential for
increasing the robustness of clinical domain models under distributional
shifts. The testbed offers one way to track progress in that direction. It also
highlights the necessity of adopting evaluation metrics that consider
robustness to natural distribution shifts. We plan to expand the corpus by
adding more samples and model results. The full paper and the updated benchmark
are available at github.com/openlifescience-ai/clift
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13149" title="Abstract">arXiv:2310.13149</a> [<a href="/pdf/2310.13149" title="Download PDF">pdf</a>, <a href="/format/2310.13149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Generative AI in Art: An Interview Study with Artists on  G-AI from an HCI Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jingyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rahul Jain</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+R">Runlin Duan</a>, 
<a href="/search/cs?searchtype=author&query=Ramani%2C+K">Karthik Ramani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The emergence of Generative Artificial Intelligence (G-AI) has changed the
landscape of creative arts with its power to compose novel artwork and thus
brought ethical concerns. Despite the efforts by prior works to address these
concerns from technical and societal perspectives, there exists little
discussion on this topic from an HCI point of view, considering the artists as
human factors. We sought to investigate the impact of G-AI on artists,
understanding the relationship between artists and G-AI, in order to motivate
the underlying HCI research. We conducted semi-structured interviews with
artists ($N=25$) from diverse artistic disciplines involved with G-AI in their
artistic creation. We found (1) a dilemma among the artists, (2) a disparity in
the understanding of G-AI between the artists and the AI developers(3) a
tendency to oppose G-AI among the artists. We discuss the future opportunities
of HCI research to tackle the problems identified from the interviews.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13157" title="Abstract">arXiv:2310.13157</a> [<a href="/pdf/2310.13157" title="Download PDF">pdf</a>, <a href="/format/2310.13157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Generative Modeling for Images, 3D Animations, and Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Voleti%2C+V">Vikram Voleti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Doctoral thesis, Mila, University of Montreal. 189 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This dissertation attempts to drive innovation in the field of generative
modeling for computer vision, by exploring novel formulations of conditional
generative models, and innovative applications in images, 3D animations, and
video. Our research focuses on architectures that offer reversible
transformations of noise and visual data, and the application of
encoder-decoder architectures for generative tasks and 3D content manipulation.
In all instances, we incorporate conditional information to enhance the
synthesis of visual data, improving the efficiency of the generation process as
well as the generated content.
<br />We introduce the use of Neural ODEs to model video dynamics using an
encoder-decoder architecture, demonstrating their ability to predict future
video frames despite being trained solely to reconstruct current frames. Next,
we propose a conditional variant of continuous normalizing flows that enables
higher-resolution image generation based on lower-resolution input, achieving
comparable image quality while reducing parameters and training time. Our next
contribution presents a pipeline that takes human images as input,
automatically aligns a user-specified 3D character with the pose of the human,
and facilitates pose editing based on partial inputs. Next, we derive the
relevant mathematical details for denoising diffusion models that use
non-isotropic Gaussian processes, and show comparable generation quality.
Finally, we devise a novel denoising diffusion framework capable of solving all
three video tasks of prediction, generation, and interpolation. We perform
ablation studies, and show SOTA results on multiple datasets.
<br />Our contributions are published articles at peer-reviewed venues. Overall,
our research aims to make a meaningful contribution to the pursuit of more
efficient and flexible generative models, with the potential to shape the
future of computer vision.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13161" title="Abstract">arXiv:2310.13161</a> [<a href="/pdf/2310.13161" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Distributed Approach to Meteorological Predictions: Addressing Data  Imbalance in Precipitation Prediction Models through Federated Learning and  GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jafarigol%2C+E">Elaheh Jafarigol</a>, 
<a href="/search/cs?searchtype=author&query=Trafalis%2C+T">Theodore Trafalis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The classification of weather data involves categorizing meteorological
phenomena into classes, thereby facilitating nuanced analyses and precise
predictions for various sectors such as agriculture, aviation, and disaster
management. This involves utilizing machine learning models to analyze large,
multidimensional weather datasets for patterns and trends. These datasets may
include variables such as temperature, humidity, wind speed, and pressure,
contributing to meteorological conditions. Furthermore, it's imperative that
classification algorithms proficiently navigate challenges such as data
imbalances, where certain weather events (e.g., storms or extreme temperatures)
might be underrepresented. This empirical study explores data augmentation
methods to address imbalanced classes in tabular weather data in centralized
and federated settings. Employing data augmentation techniques such as the
Synthetic Minority Over-sampling Technique or Generative Adversarial Networks
can improve the model's accuracy in classifying rare but critical weather
events. Moreover, with advancements in federated learning, machine learning
models can be trained across decentralized databases, ensuring privacy and data
integrity while mitigating the need for centralized data storage and
processing. Thus, the classification of weather data stands as a critical
bridge, linking raw meteorological data to actionable insights, enhancing our
capacity to anticipate and prepare for diverse weather conditions.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13164" title="Abstract">arXiv:2310.13164</a> [<a href="/pdf/2310.13164" title="Download PDF">pdf</a>, <a href="/format/2310.13164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost Equivariance via Lie Algebra Convolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McNeela%2C+D">Daniel McNeela</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, the equivariance of models with respect to a group action has
become an important topic of research in machine learning. However, imbuing an
architecture with a specific group equivariance imposes a strong prior on the
types of data transformations that the model expects to see. While
strictly-equivariant models enforce symmetries, real-world data does not always
conform to such strict equivariances, be it due to noise in the data or
underlying physical laws that encode only approximate or partial symmetries. In
such cases, the prior of strict equivariance can actually prove too strong and
cause models to underperform on real-world data. Therefore, in this work we
study a closely related topic, that of almost equivariance. We provide a
definition of almost equivariance that differs from those extant in the current
literature and give a practical method for encoding almost equivariance in
models by appealing to the Lie algebra of a Lie group. Specifically, we define
Lie algebra convolutions and demonstrate that they offer several benefits over
Lie group convolutions, including being well-defined for non-compact groups.
From there, we pivot to the realm of theory and demonstrate connections between
the notions of equivariance and isometry and those of almost equivariance and
almost isometry, respectively. We prove two existence theorems, one showing the
existence of almost isometries within bounded distance of isometries of a
general manifold, and another showing the converse for Hilbert spaces. We then
extend these theorems to prove the existence of almost equivariant manifold
embeddings within bounded distance of fully equivariant embedding functions,
subject to certain constraints on the group action and the function class.
Finally, we demonstrate the validity of our approach by benchmarking against
datasets in fully equivariant and almost equivariant settings.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13165" title="Abstract">arXiv:2310.13165</a> [<a href="/pdf/2310.13165" title="Download PDF">pdf</a>, <a href="/format/2310.13165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for  Image Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Sihan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziqiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yidong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Honglak Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+J">Joyce Chai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks
but lack an intuitive interface for consistent image-to-image (I2I)
translation. Various methods have been explored to address this issue,
including mask-based methods, attention-based methods, and image-conditioning.
However, it remains a critical challenge to enable unpaired I2I translation
with pre-trained DMs while maintaining satisfying consistency. This paper
introduces Cyclenet, a novel but simple method that incorporates cycle
consistency into DMs to regularize image manipulation. We validate Cyclenet on
unpaired I2I tasks of different granularities. Besides the scene and object
level translation, we additionally contribute a multi-domain I2I translation
dataset to study the physical state changes of objects. Our empirical studies
show that Cyclenet is superior in translation consistency and quality, and can
generate high-quality images for out-of-domain distributions with a simple
change of the textual prompt. Cyclenet is a practical framework, which is
robust even with very limited training data (around 2k) and requires minimal
computational resources (1 GPU) to train. Project homepage:
https://cyclenetweb.github.io/
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13167" title="Abstract">arXiv:2310.13167</a> [<a href="/pdf/2310.13167" title="Download PDF">pdf</a>, <a href="/format/2310.13167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visualizing Causality in Mixed Reality for Manual Task Learning: An  Exploratory Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rahul Jain</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jingyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Benton%2C+A">Andrew Benton</a>, 
<a href="/search/cs?searchtype=author&query=Rasheed%2C+M">Moiz Rasheed</a>, 
<a href="/search/cs?searchtype=author&query=Chidambaram%2C+S">Subramanian Chidambaram</a>, 
<a href="/search/cs?searchtype=author&query=Ramani%2C+K">Karthik Ramani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Mixed Reality (MR) is gaining prominence in manual task skill learning due to
its in-situ, embodied, and immersive experience. To teach manual tasks, current
methodologies break the task into hierarchies (tasks into subtasks) and
visualize the current subtask and future in terms of causality. Existing
psychology literature also shows that humans learn tasks by breaking them into
hierarchies. In order to understand the design space of information visualized
to the learner for better task understanding, we conducted a user study with 48
users. The study was conducted using a complex assembly task, which involves
learning of both actions and tool usage. We aim to explore the effect of
visualization of causality in the hierarchy for manual task learning in MR by
four options: no causality, event level causality, interaction level causality,
and gesture level causality. The results show that the user understands and
performs best when all the level of causality is shown to the user. Based on
the results, we further provide design recommendations and in-depth discussions
for future manual task learning systems.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13168" title="Abstract">arXiv:2310.13168</a> [<a href="/pdf/2310.13168" title="Download PDF">pdf</a>, <a href="/format/2310.13168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximated Modeling and Optimal Design for a Soft Pneumatic Actuator  Considering the Force/Torque and System Controllability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wu-Te Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kurkcu%2C+B">Burak Kurkcu</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Soft pneumatic actuators (SPAs) are widely employed to drive soft robots.
However, their inherent flexibility offers both benefits and challenges. This
property reduces their output force/torque and makes them hard to control. This
paper introduces a new design method that enhances the actuator's performance
and controllability. The complex structure of the soft actuator is simplified
by approximating it as a cantilever beam. This allows us to derive a mechanical
equation between input pressure to output torque. Additionally, a dynamical
model is explored to understand the correlation between the natural frequency
and dimensional parameters of the SPA. The design problem is then transformed
into an optimization problem, using the mechanical equation as the objective
function and the dynamical equation as a constraint. By solving this
optimization problem, the optimal dimensional parameters are determined. Prior
to fabrication, preliminary tests are conducted using the finite element
method. Six prototypes are manufactured to validate the proposed approach. The
optimal actuator successfully generates the desired force/torque, while its
natural frequency remains within the constrained range. This work highlights
the potential of using approximated models and optimization formulation to
boost the efficiency and dynamic performance of soft pneumatic actuators.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13169" title="Abstract">arXiv:2310.13169</a> [<a href="/pdf/2310.13169" title="Download PDF">pdf</a>, <a href="/format/2310.13169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A posteriori analysis for a mixed formulation of the Stokes spectral  problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lepe%2C+F">Felipe Lepe</a>, 
<a href="/search/math?searchtype=author&query=Vellojin%2C+J">Jesus Vellojin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In two and three dimensions, we design and analyze a posteriori error
estimators for the mixed Stokes eigenvalue problem. The unknowns on this mixed
formulation are the pseudotress, velocity and pressure. With a lowest order
mixed finite element scheme, together with a postprocressing technique, we
prove that the proposed estimator is reliable and efficient. We illustrate the
results with several numerical tests in two and three dimensions in order to
assess the performance of the estimator.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13174" title="Abstract">arXiv:2310.13174</a> [<a href="/pdf/2310.13174" title="Download PDF">pdf</a>, <a href="/ps/2310.13174" title="Download PostScript">ps</a>, <a href="/format/2310.13174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster Algorithms for Text-to-Pattern Hamming Distances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+M">Timothy M. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Ce Jin</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+V+V">Virginia Vassilevska Williams</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yinzhan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in FOCS 2023. Abstract shortened to fit arXiv requirements
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study the classic Text-to-Pattern Hamming Distances problem: given a
pattern $P$ of length $m$ and a text $T$ of length $n$, both over a
polynomial-size alphabet, compute the Hamming distance between $P$ and $T[i\,
.\, . \, i+m-1]$ for every shift $i$, under the standard Word-RAM model with
$\Theta(\log n)$-bit words.
<br />- We provide an $O(n\sqrt{m})$ time Las Vegas randomized algorithm for this
problem, beating the decades-old $O(n \sqrt{m \log m})$ running time
[Abrahamson, SICOMP 1987]. We also obtain a deterministic algorithm, with a
slightly higher $O(n\sqrt{m}(\log m\log\log m)^{1/4})$ running time. Our
randomized algorithm extends to the $k$-bounded setting, with running time
$O\big(n+\frac{nk}{\sqrt{m}}\big)$, removing all the extra logarithmic factors
from earlier algorithms [Gawrychowski and Uzna\'{n}ski, ICALP 2018; Chan,
Golan, Kociumaka, Kopelowitz and Porat, STOC 2020].
<br />- For the $(1+\epsilon)$-approximate version of Text-to-Pattern Hamming
Distances, we give an $\tilde{O}(\epsilon^{-0.93}n)$ time Monte Carlo
randomized algorithm, beating the previous $\tilde{O}(\epsilon^{-1}n)$ running
time [Kopelowitz and Porat, FOCS 2015; Kopelowitz and Porat, SOSA 2018].
<br />Our approximation algorithm exploits a connection with $3$SUM, and uses a
combination of Fredman's trick, equality matrix product, and random sampling;
in particular, we obtain new results on approximate counting versions of $3$SUM
and Exact Triangle, which may be of independent interest. Our exact algorithms
use a novel combination of hashing, bit-packed FFT, and recursion; in
particular, we obtain a faster algorithm for computing the sumset of two
integer sets, in the regime when the universe size is close to quadratic in the
number of elements.
<br />We also prove a fine-grained equivalence between the exact Text-to-Pattern
Hamming Distances problem and a range-restricted, counting version of $3$SUM.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13177" title="Abstract">arXiv:2310.13177</a> [<a href="/pdf/2310.13177" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Building Energy Efficiency through Advanced Sizing and  Dispatch Methods for Energy Storage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yu%2C+M+G">Min Gyung Yu</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+X">Xu Ma</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+B">Bowen Huang</a>, 
<a href="/search/eess?searchtype=author&query=Devaprasad%2C+K">Karthik Devaprasad</a>, 
<a href="/search/eess?searchtype=author&query=Brown%2C+F">Fredericka Brown</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+D">Di Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Energy storage and electrification of buildings hold great potential for
future decarbonized energy systems. However, there are several technical and
economic barriers that prevent large-scale adoption and integration of energy
storage in buildings. These barriers include integration with building control
systems, high capital costs, and the necessity to identify and quantify value
streams for different stakeholders. To overcome these obstacles, it is crucial
to develop advanced sizing and dispatch methods to assist planning and
operational decision-making for integrating energy storage in buildings. This
work develops a simple and flexible optimal sizing and dispatch framework for
thermal energy storage (TES) and battery energy storage (BES) systems in
large-scale office buildings. The optimal sizes of TES, BES, as well as other
building assets are determined in a joint manner instead of sequentially to
avoid sub-optimal solutions. The solution is determined considering both
capital costs in optimal sizing and operational benefits in optimal dispatch.
With the optimally sized systems, we implemented real-time operation using the
model-based control (MPC), facilitating the effective and efficient management
of energy resources. Comprehensive assessments are performed using simulation
studies to quantify potential energy and economic benefits by different utility
tariffs and climate locations, to improve our understanding of the
techno-economic performance of different TES and BES systems, and to identify
barriers to adopting energy storage for buildings. Finally, the proposed
framework will provide guidance to a broad range of stakeholders to properly
design energy storage in buildings and maximize potential benefits, thereby
advancing affordable building energy storage deployment and helping accelerate
the transition towards a cleaner and more equitable energy economy.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13182" title="Abstract">arXiv:2310.13182</a> [<a href="/pdf/2310.13182" title="Download PDF">pdf</a>, <a href="/format/2310.13182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Show Me My Users: A Dashboard Visualizing User Interaction Logs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinrui Wang</a>, 
<a href="/search/cs?searchtype=author&query=AlKadi%2C+M">Mashael AlKadi</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+B">Benjamin Bach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the IEEE VIS conference (Melbourne, VIC), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This paper describes the design of a dashboard and analysis pipeline to
monitor users of visualization tools in the wild. Our pipeline describes how to
extract analytical KPIs from extensive log event data involving a mix of user
types. The resulting three-page dashboard displays live KPIs, helping analysts
understand users, detect exploratory behaviors, plan education interventions,
and improve tool features. We propose this case study as a motivation to use
the dashboard approach for a more `casual' monitoring of users and building
carer mindsets for visualization tools.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13183" title="Abstract">arXiv:2310.13183</a> [<a href="/pdf/2310.13183" title="Download PDF">pdf</a>, <a href="/format/2310.13183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking through Deterministic Barriers: Randomized Pruning Mask  Generation and Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Weizhi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Q">Qi Lei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongkuan Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">It is widely acknowledged that large and sparse models have higher accuracy
than small and dense models under the same model size constraints. This
motivates us to train a large model and then remove its redundant neurons or
weights by pruning. Most existing works pruned the networks in a deterministic
way, the performance of which solely depends on a single pruning criterion and
thus lacks variety. Instead, in this paper, we propose a model pruning strategy
that first generates several pruning masks in a designed random way.
Subsequently, along with an effective mask-selection rule, the optimal mask is
chosen from the pool of mask candidates. To further enhance efficiency, we
introduce an early mask evaluation strategy, mitigating the overhead associated
with training multiple masks. Our extensive experiments demonstrate that this
approach achieves state-of-the-art performance across eight datasets from GLUE,
particularly excelling at high levels of sparsity.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13184" title="Abstract">arXiv:2310.13184</a> [<a href="/pdf/2310.13184" title="Download PDF">pdf</a>, <a href="/format/2310.13184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Multi-Drone Coordination for Filming Group Behaviours in  Dynamic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rauniyar%2C+A">Aditya Rauniyar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Scherer%2C+S">Sebastian Scherer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 Pages, 4 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics and AI,
with numerous applications in real-world scenarios. One such scenario is
filming scenes with multiple actors, where the goal is to capture the scene
from multiple angles simultaneously. Here, we present a formation-based filming
directive of task assignment followed by a Conflict-Based MAPF algorithm for
efficient path planning of multiple agents to achieve filming objectives while
avoiding collisions. We propose an extension to the standard MAPF formulation
to accommodate actor-specific requirements and constraints. Our approach
incorporates Conflict-Based Search, a widely used heuristic search technique
for solving MAPF problems. We demonstrate the effectiveness of our approach
through experiments on various MAPF scenarios in a simulated environment. The
proposed algorithm enables the efficient online task assignment of
formation-based filming to capture dynamic scenes, making it suitable for
various filming and coverage applications.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13188" title="Abstract">arXiv:2310.13188</a> [<a href="/pdf/2310.13188" title="Download PDF">pdf</a>, <a href="/format/2310.13188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RMap: Millimeter-Wave Radar Mapping Through Volumetric Upsampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mopidevi%2C+A+N">Ajay Narasimha Mopidevi</a>, 
<a href="/search/cs?searchtype=author&query=Harlow%2C+K">Kyle Harlow</a>, 
<a href="/search/cs?searchtype=author&query=Heckman%2C+C">Christoffer Heckman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Millimeter Wave Radar is being adopted as a viable alternative to lidar and
radar in adverse visually degraded conditions, such as the presence of fog and
dust. However, this sensor modality suffers from severe sparsity and noise
under nominal conditions, which makes it difficult to use in precise
applications such as mapping. This work presents a novel solution to generate
accurate 3D maps from sparse radar point clouds. RMap uses a custom generative
transformer architecture, UpPoinTr, which upsamples, denoises, and fills the
incomplete radar maps to resemble lidar maps. We test this method on the
ColoRadar dataset to demonstrate its efficacy.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13189" title="Abstract">arXiv:2310.13189</a> [<a href="/pdf/2310.13189" title="Download PDF">pdf</a>, <a href="/format/2310.13189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Accurate Factual Inconsistency Detection Over Long Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lattimer%2C+B+M">Barrett Martin Lattimer</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Patrick Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in EMNLP 2023 Main Conference, 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative AI models exhibit remarkable potential; however, hallucinations
across various tasks present a significant challenge, particularly for longer
inputs that current approaches struggle to address effectively. We introduce
SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a
task-agnostic model for detecting factual inconsistencies using a novel
chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI)
based model that uses large text chunks to condition over long texts. This
approach achieves state-of-the-art performance in factual inconsistency
detection for diverse tasks and long inputs. Additionally, we leverage the
chunking mechanism and employ a novel algorithm to explain SCALE's decisions
through relevant source sentence retrieval. Our evaluations reveal that SCALE
outperforms existing methods on both standard benchmarks and a new long-form
dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses
competitive systems in efficiency and model explanation evaluations.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13191" title="Abstract">arXiv:2310.13191</a> [<a href="/pdf/2310.13191" title="Download PDF">pdf</a>, <a href="/format/2310.13191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy  for Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Q">Qi Lei</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongkuan Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The pruning objective has recently extended beyond accuracy and sparsity to
robustness in language models. Despite this, existing methods struggle to
enhance robustness against adversarial attacks when continually increasing
model sparsity and require a retraining process. As humans step into the era of
large language models, these issues become increasingly prominent. This paper
proposes that the robustness of language models is proportional to the extent
of pre-trained knowledge they encompass. Accordingly, we introduce a
post-training pruning strategy designed to faithfully replicate the embedding
space and feature space of dense language models, aiming to conserve more
pre-trained knowledge during the pruning process. In this setup, each layer's
reconstruction error not only originates from itself but also includes
cumulative error from preceding layers, followed by an adaptive rectification.
Compared to other state-of-art baselines, our approach demonstrates a superior
balance between accuracy, sparsity, robustness, and pruning cost with BERT on
datasets SST2, IMDB, and AGNews, marking a significant stride towards robust
pruning in language models.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13192" title="Abstract">arXiv:2310.13192</a> [<a href="/pdf/2310.13192" title="Download PDF">pdf</a>, <a href="/format/2310.13192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The opaque law of artificial intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Calderonio%2C+V">Vincenzo Calderonio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The purpose of this paper is to analyse the opacity of algorithms,
contextualized in the open debate on responsibility for artificial intelligence
causation; with an experimental approach by which, applying the proposed
conversational methodology of the Turing Test, we expect to evaluate the
performance of one of the best existing NLP model of generative AI (Chat-GPT)
to see how far it can go right now and how the shape of a legal regulation of
it could be. The analysis of the problem will be supported by a comment of
Italian classical law categories such as causality, intent and fault to
understand the problem of the usage of AI, focusing in particular on the
human-machine interaction. On the computer science side, for a technical point
of view of the logic used to craft these algorithms, in the second chapter will
be proposed a practical interrogation of Chat-GPT aimed at finding some
critical points of the functioning of AI. The end of the paper will concentrate
on some existing legal solutions which can be applied to the problem, plus a
brief description of the approach proposed by EU Artificial Intelligence act.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13193" title="Abstract">arXiv:2310.13193</a> [<a href="/pdf/2310.13193" title="Download PDF">pdf</a>, <a href="/format/2310.13193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Meidani%2C+H">Hadi Meidani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The traffic assignment problem is one of the significant components of
traffic flow analysis for which various solution approaches have been proposed.
However, deploying these approaches for large-scale networks poses significant
challenges. In this paper, we leverage the power of heterogeneous graph neural
networks to propose a novel data-driven approach for traffic assignment and
traffic flow learning. The proposed model is capable of capturing spatial
traffic patterns across different links, yielding highly accurate results. We
present numerical experiments on urban transportation networks and show that
the proposed heterogeneous graph neural network model outperforms other
conventional neural network models in terms of convergence rate, training loss,
and prediction accuracy. Notably, the proposed heterogeneous graph neural
network model can also be generalized to different network topologies. This
approach offers a promising solution for complex traffic flow analysis and
prediction, enhancing our understanding and management of a wide range of
transportation systems.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13196" title="Abstract">arXiv:2310.13196</a> [<a href="/pdf/2310.13196" title="Download PDF">pdf</a>, <a href="/format/2310.13196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NameGuess: Column Name Expansion for Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiani Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhengyuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+B">Balasubramaniam Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rangwala%2C+H">Huzefa Rangwala</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted to EMNLP'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Databases (cs.DB); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in large language models have revolutionized many sectors,
including the database industry. One common challenge when dealing with large
volumes of tabular data is the pervasive use of abbreviated column names, which
can negatively impact performance on various data search, access, and
understanding tasks. To address this issue, we introduce a new task, called
NameGuess, to expand column names (used in database schema) as a natural
language generation problem. We create a training dataset of 384K
abbreviated-expanded column pairs using a new data fabrication method and a
human-annotated evaluation benchmark that includes 9.2K examples from
real-world tables. To tackle the complexities associated with polysemy and
ambiguity in NameGuess, we enhance auto-regressive language models by
conditioning on table content and column header names -- yielding a fine-tuned
model (with 2.7B parameters) that matches human performance. Furthermore, we
conduct a comprehensive analysis (on multiple LLMs) to validate the
effectiveness of table content in NameGuess and identify promising future
opportunities. Code has been made available at
https://github.com/amazon-science/nameguess.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13198" title="Abstract">arXiv:2310.13198</a> [<a href="/pdf/2310.13198" title="Download PDF">pdf</a>, <a href="/format/2310.13198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Car Model Identification System for Streamlining the Automobile Sales  Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Togru%2C+S">Said Togru</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jenny Huang</a>, 
<a href="/search/cs?searchtype=author&query=Moldovan%2C+M">Marco Moldovan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This project presents an automated solution for the efficient identification
of car models and makes from images, aimed at streamlining the vehicle listing
process on online car-selling platforms. Through a thorough exploration
encompassing various efficient network architectures including Convolutional
Neural Networks (CNNs), Vision Transformers (ViTs), and hybrid models, we
achieved a notable accuracy of 81.97% employing the EfficientNet (V2 b2)
architecture. To refine performance, a combination of strategies, including
data augmentation, fine-tuning pretrained models, and extensive hyperparameter
tuning, were applied. The trained model offers the potential for automating
information extraction, promising enhanced user experiences across car-selling
websites.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13201" title="Abstract">arXiv:2310.13201</a> [<a href="/pdf/2310.13201" title="Download PDF">pdf</a>, <a href="/format/2310.13201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification of Abnormality in Maize Plants From UAV Images Using Deep  Learning Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huq%2C+A">Aminul Huq</a>, 
<a href="/search/cs?searchtype=author&query=Zermas%2C+D">Dimitris Zermas</a>, 
<a href="/search/cs?searchtype=author&query=Bebis%2C+G">George Bebis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted and presented at the 18th International Symposium on Visual Computing (ISVC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Early identification of abnormalities in plants is an important task for
ensuring proper growth and achieving high yields from crops. Precision
agriculture can significantly benefit from modern computer vision tools to make
farming strategies addressing these issues efficient and effective. As farming
lands are typically quite large, farmers have to manually check vast areas to
determine the status of the plants and apply proper treatments. In this work,
we consider the problem of automatically identifying abnormal regions in maize
plants from images captured by a UAV. Using deep learning techniques, we have
developed a methodology which can detect different levels of abnormality (i.e.,
low, medium, high or no abnormality) in maize plants independently of their
growth stage. The primary goal is to identify anomalies at the earliest
possible stage in order to maximize the effectiveness of potential treatments.
At the same time, the proposed system can provide valuable information to human
annotators for ground truth data collection by helping them to focus their
attention on a much smaller set of images only. We have experimented with two
different but complimentary approaches, the first considering abnormality
detection as a classification problem and the second considering it as a
regression problem. Both approaches can be generalized to different types of
abnormalities and do not make any assumption about the abnormality occurring at
an early plant growth stage which might be easier to detect due to the plants
being smaller and easier to separate. As a case study, we have considered a
publicly available data set which exhibits mostly Nitrogen deficiency in maize
plants of various growth stages. We are reporting promising preliminary results
with an 88.89\% detection accuracy of low abnormality and 100\% detection
accuracy of no abnormality.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13203" title="Abstract">arXiv:2310.13203</a> [<a href="/pdf/2310.13203" title="Download PDF">pdf</a>, <a href="/ps/2310.13203" title="Download PostScript">ps</a>, <a href="/format/2310.13203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study of Fitness Gains in Evolving Finite State Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zoltai%2C+G">Gabor Zoltai</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yue Xie</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+F">Frank Neumann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures. Submitted to the Australasian Joint Conference on Artificial Intelligence (AJCAI) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Among the wide variety of evolutionary computing models, Finite State
Machines (FSMs) have several attractions for fundamental research. They are
easy to understand in concept and can be visualised clearly in simple cases.
They have a ready fitness criterion through their relationship with Regular
Languages. They have also been shown to be tractably evolvable, even up to
exhibiting evidence of open-ended evolution in specific scenarios. In addition
to theoretical attraction, they also have industrial applications, as a
paradigm of both automated and user-initiated control. Improving the
understanding of the factors affecting FSM evolution has relevance to both
computer science and practical optimisation of control. We investigate an
evolutionary scenario of FSMs adapting to recognise one of a family of Regular
Languages by categorising positive and negative samples, while also being under
a counteracting selection pressure that favours fewer states. The results
appear to indicate that longer strings provided as samples reduce the speed of
fitness gain, when fitness is measured against a fixed number of sample
strings. We draw the inference that additional information from longer strings
is not sufficient to compensate for sparser coverage of the combinatorial space
of positive and negative sample strings.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13206" title="Abstract">arXiv:2310.13206</a> [<a href="/pdf/2310.13206" title="Download PDF">pdf</a>, <a href="/format/2310.13206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Primacy Effect of ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yujun Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 short paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Instruction-tuned large language models (LLMs), such as ChatGPT, have led to
promising zero-shot performance in discriminative natural language
understanding (NLU) tasks. This involves querying the LLM using a prompt
containing the question, and the candidate labels to choose from. The
question-answering capabilities of ChatGPT arise from its pre-training on large
amounts of human-written text, as well as its subsequent fine-tuning on human
preferences, which motivates us to ask: Does ChatGPT also inherits humans'
cognitive biases? In this paper, we study the primacy effect of ChatGPT: the
tendency of selecting the labels at earlier positions as the answer. We have
two main findings: i) ChatGPT's decision is sensitive to the order of labels in
the prompt; ii) ChatGPT has a clearly higher chance to select the labels at
earlier positions as the answer. We hope that our experiments and analyses
provide additional insights into building more reliable ChatGPT-based
solutions. We release the source code at
https://github.com/wangywUST/PrimacyEffectGPT.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13208" title="Abstract">arXiv:2310.13208</a> [<a href="/pdf/2310.13208" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online energy management system for a fuel cell/battery hybrid system  with multiple fuel cell stacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shi%2C+J">Junzhe Shi</a>, 
<a href="/search/eess?searchtype=author&query=Aarsnes%2C+U+J+F">Ulf Jakob Fl&#xf8; Aarsnes</a>, 
<a href="/search/eess?searchtype=author&query=N%C3%A6rheim%2C+D">Dagfinn N&#xe6;rheim</a>, 
<a href="/search/eess?searchtype=author&query=Moura%2C+S">Scott Moura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In recent years, fuel cell/battery hybrid systems have attracted substantial
attention due to their high energy density and low emissions. The online energy
management system (EMS) is essential for these hybrid systems, tasked with
controlling the energy flow and ensuring optimal system performance,
encompassing fuel efficiency and mitigating fuel cell and battery degradation.
This research proposes a novel approach to energy management for hybrid fuel
cell/battery systems with multiple fuel cell stacks. It introduces an online
EMS that employs Mixed Integer Quadratic Programming (MIQP) to independently
control each fuel cell stack in the hybrid system. The performance of this
method is compared to Dynamic Programming (DP), a standard approach for energy
management in such systems. Results demonstrate that the proposed method
achieves superior computational efficiency compared to DP while delivering
improved performance. Furthermore, the research reveals that independent
control of multiple stacks results in a more optimized system operation
compared to traditional methods, which consider multiple stacks as a single
entity and implement identical control actions across all.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13212" title="Abstract">arXiv:2310.13212</a> [<a href="/pdf/2310.13212" title="Download PDF">pdf</a>, <a href="/format/2310.13212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Facile: Fast, Accurate, and Interpretable Basic-Block Throughput  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abel%2C+A">Andreas Abel</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Shrey Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Reineke%2C+J">Jan Reineke</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Symposium on Workload Characterization, IISWC
  2023, Ghent, Belgium, October 1-3, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">Basic-block throughput models such as uiCA, IACA, GRANITE, Ithemal, llvm-mca,
OSACA, or CQA guide optimizing compilers and help performance engineers
identify and eliminate bottlenecks. For this purpose, basic-block throughput
models should ideally be fast, accurate, and interpretable.
<br />Recent advances have significantly improved accuracy: uiCA, the
state-of-the-art model, achieves an error of about 1% relative to measurements
across a wide range of microarchitectures. The computational efficiency of
throughput models, which is equally important for widespread adoption,
especially in compilers, has so far received little attention.
<br />In this paper, we introduce Facile, an analytical throughput model that is
fast, accurate, and interpretable. Facile analyzes different potential
bottlenecks independently and analytically. Due to its compositional nature,
Facile's predictions directly pinpoint the bottlenecks. We evaluate Facile on a
wide range of microarchitectures and show that it is almost two orders of
magnitude faster than existing models while achieving state-of-the-art
accuracy.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13213" title="Abstract">arXiv:2310.13213</a> [<a href="/pdf/2310.13213" title="Download PDF">pdf</a>, <a href="/format/2310.13213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy  Named Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fetahu%2C+B">Besnik Fetahu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kar%2C+S">Sudipta Kar</a>, 
<a href="/search/cs?searchtype=author&query=Rokhlenko%2C+O">Oleg Rokhlenko</a>, 
<a href="/search/cs?searchtype=author&query=Malmasi%2C+S">Shervin Malmasi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present MULTICONER V2, a dataset for fine-grained Named Entity Recognition
covering 33 entity classes across 12 languages, in both monolingual and
multilingual settings. This dataset aims to tackle the following practical
challenges in NER: (i) effective handling of fine-grained classes that include
complex entities like movie titles, and (ii) performance degradation due to
noise generated from typing mistakes or OCR errors. The dataset is compiled
from open resources like Wikipedia and Wikidata, and is publicly available.
Evaluation based on the XLM-RoBERTa baseline highlights the unique challenges
posed by MULTICONER V2: (i) the fine-grained taxonomy is challenging, where the
scores are low with macro-F1=0.63 (across all languages), and (ii) the
corruption strategy significantly impairs performance, with entity corruption
resulting in 9% lower performance relative to non-entity corruptions across all
languages. This highlights the greater impact of entity noise in contrast to
context noise.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13215" title="Abstract">arXiv:2310.13215</a> [<a href="/pdf/2310.13215" title="Download PDF">pdf</a>, <a href="/format/2310.13215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zone Evaluation: Revealing Spatial Bias in Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhaohui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Q">Qibin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Ping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Ming-Ming Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A fundamental limitation of object detectors is that they suffer from
"spatial bias", and in particular perform less satisfactorily when detecting
objects near image borders. For a long time, there has been a lack of effective
ways to measure and identify spatial bias, and little is known about where it
comes from and what degree it is. To this end, we present a new zone evaluation
protocol, extending from the traditional evaluation to a more generalized one,
which measures the detection performance over zones, yielding a series of Zone
Precisions (ZPs). For the first time, we provide numerical results, showing
that the object detectors perform quite unevenly across the zones.
Surprisingly, the detector's performance in the 96\% border zone of the image
does not reach the AP value (Average Precision, commonly regarded as the
average detection performance in the entire image zone). To better understand
spatial bias, a series of heuristic experiments are conducted. Our
investigation excludes two intuitive conjectures about spatial bias that the
object scale and the absolute positions of objects barely influence the spatial
bias. We find that the key lies in the human-imperceptible divergence in data
patterns between objects in different zones, thus eventually forming a visible
performance gap between the zones. With these findings, we finally discuss a
future direction for object detection, namely, spatial disequilibrium problem,
aiming at pursuing a balanced detection ability over the entire image zone. By
broadly evaluating 10 popular object detectors and 5 detection datasets, we
shed light on the spatial bias of object detectors. We hope this work could
raise a focus on detection robustness. The source codes, evaluation protocols,
and tutorials are publicly available at
\url{https://github.com/Zzh-tju/ZoneEval}.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13218" title="Abstract">arXiv:2310.13218</a> [<a href="/pdf/2310.13218" title="Download PDF">pdf</a>, <a href="/format/2310.13218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning-Enabled Adaptive Forecasting-Aided State  Estimation in Distribution Systems with Multi-Source Multi-Rate Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Ying Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+J">Junbo Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+D">Di Shi</a>, 
<a href="/search/eess?searchtype=author&query=Chung%2C+S">Sungjoo Chung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 2024 IEEE PES Innovative Smart Grid Technologies Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Distribution system state estimation (DSSE) is paramount for effective state
monitoring and control. However, stochastic outputs of renewables and
asynchronous streaming of multi-rate measurements in practical systems largely
degrade the estimation performance. This paper proposes a deep reinforcement
learning (DRL)-enabled adaptive DSSE algorithm in unbalanced distribution
systems, which tackles hybrid measurements with different time scales
efficiently. We construct a three-step forecasting-aided state estimation
framework, including DRL-based parameter identification, prediction, and state
estimation, with multi-rate measurements incorporating limited synchrophasor
data. Furthermore, a DRL-based adaptive parameter identification mechanism is
embedded in the prediction step. As a novel attempt at utilizing DRL to enable
DSSE adaptive to varying operating conditions, this method improves the
prediction performance and further facilitates accurate state estimation. Case
studies in two unbalanced feeders indicate that our method captures state
variation with multi-source multi-rate data efficiently, outperforming the
traditional methods.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13219" title="Abstract">arXiv:2310.13219</a> [<a href="/pdf/2310.13219" title="Download PDF">pdf</a>, <a href="/format/2310.13219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HierCas: Hierarchical Temporal Graph Attention Networks for Popularity  Prediction in Information Cascades
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaohui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yishuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lanshan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yong Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Information cascade popularity prediction is critical for many applications,
including but not limited to identifying fake news and accurate
recommendations. Traditional feature-based methods heavily rely on handcrafted
features, which are domain-specific and lack generalizability to new domains.
To address this problem, researchers have turned to neural network-based
approaches. However, existing methods follow a sampling-based modeling
approach, potentially losing continuous dynamic information and
structural-temporal dependencies that emerge during the information diffusion
process. In this paper, we propose a novel framework called Hierarchical
Temporal Graph Attention Networks for cascade popularity prediction (HierCas).
Unlike existing methods, HierCas operates on the entire cascade graph by a
dynamic graph modeling approach, enabling it to capture the full range of
continuous dynamic information and explicitly model the interplay between
structural and temporal factors. By leveraging time-aware node embedding, graph
attention mechanisms and hierarchical pooling structures, HierCas effectively
captures the popularity trend implicit in the complex cascade. Extensive
experiments conducted on two real-world datasets in different scenarios
demonstrate that our HierCas significantly outperforms the state-of-the-art
approaches.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13220" title="Abstract">arXiv:2310.13220</a> [<a href="/pdf/2310.13220" title="Download PDF">pdf</a>, <a href="/format/2310.13220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-context Learning with Transformer Is Really Equivalent to a  Contrastive Learning Pattern
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+R">Ruifeng Ren</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Pre-trained large language models based on Transformers have demonstrated
amazing in-context learning (ICL) abilities. Given several demonstration
examples, the models can implement new tasks without any parameter updates.
However, it is still an open question to understand the mechanism of ICL. In
this paper, we interpret the inference process of ICL as a gradient descent
process in a contrastive learning pattern. Firstly, leveraging kernel methods,
we establish the relationship between gradient descent and self-attention
mechanism under generally used softmax attention setting instead of linear
attention setting. Then, we analyze the corresponding gradient descent process
of ICL from the perspective of contrastive learning without negative samples
and discuss possible improvements of this contrastive learning pattern, based
on which the self-attention layer can be further modified. Finally, we design
experiments to support our opinions. To the best of our knowledge, our work is
the first to provide the understanding of ICL from the perspective of
contrastive learning and has the potential to facilitate future model design by
referring to related works on contrastive learning.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13224" title="Abstract">arXiv:2310.13224</a> [<a href="/pdf/2310.13224" title="Download PDF">pdf</a>, <a href="/format/2310.13224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Experimental Design for Intrusion Data Collection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Highnam%2C+K">Kate Highnam</a>, 
<a href="/search/cs?searchtype=author&query=Hanif%2C+Z">Zach Hanif</a>, 
<a href="/search/cs?searchtype=author&query=Van+Vogt%2C+E">Ellie Van Vogt</a>, 
<a href="/search/cs?searchtype=author&query=Parbhoo%2C+S">Sonali Parbhoo</a>, 
<a href="/search/cs?searchtype=author&query=Maffeis%2C+S">Sergio Maffeis</a>, 
<a href="/search/cs?searchtype=author&query=Jennings%2C+N+R">Nicholas R. Jennings</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CAMLIS'23 Pre-publication - TO BE UPDATED!!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Intrusion research frequently collects data on attack techniques currently
employed and their potential symptoms. This includes deploying honeypots,
logging events from existing devices, employing a red team for a sample attack
campaign, or simulating system activity. However, these observational studies
do not clearly discern the cause-and-effect relationships between the design of
the environment and the data recorded. Neglecting such relationships increases
the chance of drawing biased conclusions due to unconsidered factors, such as
spurious correlations between features and errors in measurement or
classification. In this paper, we present the theory and empirical data on
methods that aim to discover such causal relationships efficiently. Our
adaptive design (AD) is inspired by the clinical trial community: a variant of
a randomized control trial (RCT) to measure how a particular ``treatment''
affects a population. To contrast our method with observational studies and
RCT, we run the first controlled and adaptive honeypot deployment study,
identifying the causal relationship between an ssh vulnerability and the rate
of server exploitation. We demonstrate that our AD method decreases the total
time needed to run the deployment by at least 33%, while still confidently
stating the impact of our change in the environment. Compared to an analogous
honeypot study with a control group, our AD requests 17% fewer honeypots while
collecting 19% more attack recordings than an analogous honeypot study with a
control group.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13225" title="Abstract">arXiv:2310.13225</a> [<a href="/pdf/2310.13225" title="Download PDF">pdf</a>, <a href="/format/2310.13225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Neural Network Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sehanobish%2C+A">Arijit Sehanobish</a>, 
<a href="/search/cs?searchtype=author&query=Choromanski%2C+K">Krzysztof Choromanski</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunfan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dubey%2C+A">Avinava Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Likhosherstov%2C+V">Valerii Likhosherstov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. 23 pages, 10 figures. Comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce the concept of scalable neural network kernels (SNNKs), the
replacements of regular feedforward layers (FFLs), capable of approximating the
latter, but with favorable computational properties. SNNKs effectively
disentangle the inputs from the parameters of the neural network in the FFL,
only to connect them in the final computation via the dot-product kernel. They
are also strictly more expressive, as allowing to model complicated
relationships beyond the functions of the dot-products of parameter-input
vectors. We also introduce the neural network bundling process that applies
SNNKs to compactify deep neural network architectures, resulting in additional
compression gains. In its extreme version, it leads to the fully bundled
network whose optimal parameters can be expressed via explicit formulae for
several loss functions (e.g. mean squared error), opening a possibility to
bypass backpropagation. As a by-product of our analysis, we introduce the
mechanism of the universal random features (or URFs), applied to instantiate
several SNNK variants, and interesting on its own in the context of scalable
kernel methods. We provide rigorous theoretical analysis of all these concepts
as well as an extensive empirical evaluation, ranging from point-wise kernel
estimation to Transformers' fine-tuning with novel adapter layers inspired by
SNNKs. Our mechanism provides up to 5x reduction in the number of trainable
parameters, while maintaining competitive accuracy.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13226" title="Abstract">arXiv:2310.13226</a> [<a href="/pdf/2310.13226" title="Download PDF">pdf</a>, <a href="/format/2310.13226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and  Prompt Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wahidur%2C+R+S+M">Rahman S M Wahidur</a>, 
<a href="/search/cs?searchtype=author&query=Tashdeed%2C+I">Ishmam Tashdeed</a>, 
<a href="/search/cs?searchtype=author&query=Kaur%2C+M">Manjit Kaur</a>, 
<a href="/search/cs?searchtype=author&query=Heung-No-Lee">Heung-No-Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Blockchain technology has revolutionized the financial landscape, with
cryptocurrencies gaining widespread adoption for their decentralized and
transparent nature. As the sentiment expressed on social media platforms can
significantly influence cryptocurrency discussions and market movements,
sentiment analysis has emerged as a crucial tool for understanding public
opinion and predicting market trends. Motivated by the aim to enhance sentiment
analysis accuracy in the cryptocurrency domain, this paper investigates
fine-tuning techniques on large language models. This paper also investigates
the efficacy of supervised fine-tuning and instruction-based fine-tuning on
large language models for unseen tasks. Experimental results demonstrate a
significant average zero-shot performance gain of 40% after fine-tuning,
highlighting the potential of this technique in optimizing pre-trained language
model efficiency. Additionally, the impact of instruction tuning on models of
varying scales is examined, revealing that larger models benefit from
instruction tuning, achieving the highest average accuracy score of 75.16%. In
contrast, smaller-scale models may experience reduced generalization due to the
complete utilization of model capacity. To gain deeper insight about how
instruction works with these language models, this paper presents an
experimental investigation into the response of an instruction-based model
under different instruction tuning setups. The investigation demonstrates that
the model achieves an average accuracy score of 72.38% for short and simple
instructions. This performance significantly outperforms its accuracy under
long and complex instructions by over 12%, thereby effectively highlighting the
profound significance of instruction characteristics in maximizing model
performance.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13227" title="Abstract">arXiv:2310.13227</a> [<a href="/pdf/2310.13227" title="Download PDF">pdf</a>, <a href="/format/2310.13227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToolChain*: Efficient Action Space Navigation in Large Language Models  with A* Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yuchen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+S">Saayan Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Bursztyn%2C+V">Victor Bursztyn</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+R+A">Ryan A. Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Sarkhel%2C+S">Somdeb Sarkhel</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated powerful decision-making and
planning capabilities in solving complicated real-world problems. LLM-based
autonomous agents can interact with diverse tools (e.g., functional APIs) and
generate solution plans that execute a series of API function calls in a
step-by-step manner. The multitude of candidate API function calls
significantly expands the action space, amplifying the critical need for
efficient action space navigation. However, existing methods either struggle
with unidirectional exploration in expansive action spaces, trapped into a
locally optimal solution, or suffer from exhaustively traversing all potential
actions, causing inefficient navigation. To address these issues, we propose
ToolChain*, an efficient tree search-based planning algorithm for LLM-based
agents. It formulates the entire action space as a decision tree, where each
node represents a possible API function call involved in a solution plan. By
incorporating the A* search algorithm with task-specific cost function design,
it efficiently prunes high-cost branches that may involve incorrect actions,
identifying the most low-cost valid path as the solution. Extensive experiments
on multiple tool-use and reasoning tasks demonstrate that ToolChain*
efficiently balances exploration and exploitation within an expansive action
space. It outperforms state-of-the-art baselines on planning and reasoning
tasks by 3.1% and 3.5% on average while requiring 7.35x and 2.31x less time,
respectively.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13228" title="Abstract">arXiv:2310.13228</a> [<a href="/pdf/2310.13228" title="Download PDF">pdf</a>, <a href="/format/2310.13228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Less the Merrier? Investigating Language Representation in  Multilingual Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nigatu%2C+H+H">Hellina Hailu Nigatu</a>, 
<a href="/search/cs?searchtype=author&query=Tonja%2C+A+L">Atnafu Lambebo Tonja</a>, 
<a href="/search/cs?searchtype=author&query=Kalita%2C+J">Jugal Kalita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023(Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multilingual Language Models offer a way to incorporate multiple languages in
one model and utilize cross-language transfer learning to improve performance
for different Natural Language Processing (NLP) tasks. Despite progress in
multilingual models, not all languages are supported as well, particularly in
low-resource settings. In this work, we investigate the linguistic
representation of different languages in multilingual models. We start by
asking the question which languages are supported in popular multilingual
models and which languages are left behind. Then, for included languages, we
look at models' learned representations based on language family and dialect
and try to understand how models' learned representations for~(1) seen and~(2)
unseen languages vary across different language groups. In addition, we test
and analyze performance on downstream tasks such as text generation and Named
Entity Recognition. We observe from our experiments that community-centered
models -- models that focus on languages of a given family or geographical
location and are built by communities who speak them -- perform better at
distinguishing between languages in the same family for low-resource languages.
Our paper contributes to the literature in understanding multilingual models
and their shortcomings and offers insights on potential ways to improve them.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13229" title="Abstract">arXiv:2310.13229</a> [<a href="/pdf/2310.13229" title="Download PDF">pdf</a>, <a href="/format/2310.13229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The GitHub Recent Bugs Dataset for Evaluating LLM-based Debugging  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+Y">Jae Yong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Sungmin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Juyeon Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Shin Yoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated strong natural language
processing and code synthesis capabilities, which has led to their rapid
adoption in software engineering applications. However, details about LLM
training data are often not made public, which has caused concern as to whether
existing bug benchmarks are included. In lieu of the training data for the
popular GPT models, we examine the training data of the open-source LLM
StarCoder, and find it likely that data from the widely used Defects4J
benchmark was included, raising the possibility of its inclusion in GPT
training data as well. This makes it difficult to tell how well LLM-based
results on Defects4J would generalize, as for any results it would be unclear
whether a technique's performance is due to LLM generalization or memorization.
To remedy this issue and facilitate continued research on LLM-based SE, we
present the GitHub Recent Bugs (GHRB) dataset, which includes 76 real-world
Java bugs that were gathered after the OpenAI data cut-off point.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13230" title="Abstract">arXiv:2310.13230</a> [<a href="/pdf/2310.13230" title="Download PDF">pdf</a>, <a href="/format/2310.13230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Absolute Policy Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weiye Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Feihan Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yifan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianhao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changliu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">In recent years, trust region on-policy reinforcement learning has achieved
impressive results in addressing complex control tasks and gaming scenarios.
However, contemporary state-of-the-art algorithms within this category
primarily emphasize improvement in expected performance, lacking the ability to
control over the worst-case performance outcomes. To address this limitation,
we introduce a novel objective function; by optimizing which, it will lead to
guaranteed monotonic improvement in the lower bound of near-total performance
samples (absolute performance). Considering this groundbreaking theoretical
advancement, we then refine this theoretically grounded algorithm through a
series of approximations, resulting in a practical solution called Absolute
Policy Optimization (APO). Our experiments demonstrate the effectiveness of our
approach across challenging continuous control benchmark tasks and extend its
applicability to mastering Atari games. Our findings reveal that APO
significantly outperforms state-of-the-art policy gradient algorithms,
resulting in substantial improvements in both expected performance and
worst-case performance.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13231" title="Abstract">arXiv:2310.13231</a> [<a href="/pdf/2310.13231" title="Download PDF">pdf</a>, <a href="/format/2310.13231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-level Contrastive Learning for Script-based Character  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanran Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shiping Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference; Camera-ready version will be updated soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we tackle the scenario of understanding characters in scripts,
which aims to learn the characters' personalities and identities from their
utterances. We begin by analyzing several challenges in this scenario, and then
propose a multi-level contrastive learning framework to capture characters'
global information in a fine-grained manner. To validate the proposed
framework, we conduct extensive experiments on three character understanding
sub-tasks by comparing with strong pre-trained language models, including
SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate
that our method improves the performances by a considerable margin. Through
further in-depth analysis, we show the effectiveness of our method in
addressing the challenges and provide more hints on the scenario of character
understanding. We will open-source our work on github at
https://github.com/David-Li0406/Script-based-Character-Understanding.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13235" title="Abstract">arXiv:2310.13235</a> [<a href="/pdf/2310.13235" title="Download PDF">pdf</a>, <a href="/format/2310.13235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auxiliary Features-Guided Super Resolution for Monte Carlo Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+Q">Qiqi Hou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CGF
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Graphics Forum 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper investigates super resolution to reduce the number of pixels to
render and thus speed up Monte Carlo rendering algorithms. While great progress
has been made to super resolution technologies, it is essentially an ill-posed
problem and cannot recover high-frequency details in renderings. To address
this problem, we exploit high-resolution auxiliary features to guide super
resolution of low-resolution renderings. These high-resolution auxiliary
features can be quickly rendered by a rendering engine and at the same time
provide valuable high-frequency details to assist super resolution. To this
end, we develop a cross-modality Transformer network that consists of an
auxiliary feature branch and a low-resolution rendering branch. These two
branches are designed to fuse high-resolution auxiliary features with the
corresponding low-resolution rendering. Furthermore, we design residual
densely-connected Swin Transformer groups to learn to extract representative
features to enable high-quality super-resolution. Our experiments show that our
auxiliary features-guided super-resolution method outperforms both
super-resolution methods and Monte Carlo denoising methods in producing
high-quality renderings.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13236" title="Abstract">arXiv:2310.13236</a> [<a href="/pdf/2310.13236" title="Download PDF">pdf</a>, <a href="/format/2310.13236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training A Semantic Communication System with Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+L+X">Loc X. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H+Q">Huy Q. Le</a>, 
<a href="/search/cs?searchtype=author&query=Tun%2C+Y+L">Ye Lin Tun</a>, 
<a href="/search/cs?searchtype=author&query=Aung%2C+P+S">Pyae Sone Aung</a>, 
<a href="/search/cs?searchtype=author&query=Tun%2C+Y+K">Yan Kyaw Tun</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhu Han</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+C+S">Choong Seon Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Semantic communication has emerged as a pillar for the next generation of
communication systems due to its capabilities in alleviating data redundancy.
Most semantic communication systems are built using advanced deep learning
models whose performance heavily depends on data availability. These studies
assume that an abundance of training data is available, which is unrealistic.
In practice, data is mainly created on the user side. Due to privacy and
security concerns, the transmission of data is restricted, which is necessary
for conventional centralized training schemes. To address this challenge, we
explore semantic communication in federated learning (FL) setting that utilizes
user data without leaking privacy. Additionally, we design our system to tackle
the communication overhead by reducing the quantity of information delivered in
each global round. In this way, we can save significant bandwidth for
resource-limited devices and reduce overall network traffic. Finally, we
propose a mechanism to aggregate the global model from the clients, called
FedLol. Extensive simulation results demonstrate the efficacy of our proposed
technique compared to baseline methods.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13237" title="Abstract">arXiv:2310.13237</a> [<a href="/pdf/2310.13237" title="Download PDF">pdf</a>, <a href="/ps/2310.13237" title="Download PostScript">ps</a>, <a href="/format/2310.13237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Fisher metric as a metric on the cotangent bundle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nagaoka%2C+H">Hiroshi Nagaoka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Differential Geometry (math.DG); Probability (math.PR); Statistics Theory (math.ST)

</div>
<p class="mathjax">The Fisher metric on a manifold of probability distributions is usually
treated as a metric on the tangent bundle. In this paper, we focus on the
metric on the cotangent bundle induced from the Fisher metric with calling it
the Fisher co-metric. We show that the Fisher co-metric can be defined directly
without going through the Fisher metric by establishing a natural
correspondence between cotangent vectors and random variables. This definition
clarifies a close relation between the Fisher co-metric and the
variance/covariance of random variables, whereby the Cram\'{e}r-Rao inequality
is trivialized. We also discuss the monotonicity and the invariance of the
Fisher co-metric with respect to Markov maps, and present a theorem
characterizing the co-metric by the invariance, which can be regarded as a
cotangent version of \v{C}encov's characterization theorem for the Fisher
metric. The obtained theorem can also viewed as giving a characterization of
the variance/covariance.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13240" title="Abstract">arXiv:2310.13240</a> [<a href="/pdf/2310.13240" title="Download PDF">pdf</a>, <a href="/format/2310.13240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transparency challenges in policy evaluation with causal machine  learning -- improving usability and accountability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rehill%2C+P">Patrick Rehill</a>, 
<a href="/search/cs?searchtype=author&query=Biddle%2C+N">Nicholas Biddle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Econometrics (econ.EM)

</div>
<p class="mathjax">Causal machine learning tools are beginning to see use in real-world policy
evaluation tasks to flexibly estimate treatment effects. One issue with these
methods is that the machine learning models used are generally black boxes,
i.e., there is no globally interpretable way to understand how a model makes
estimates. This is a clear problem in policy evaluation applications,
particularly in government, because it is difficult to understand whether such
models are functioning in ways that are fair, based on the correct
interpretation of evidence and transparent enough to allow for accountability
if things go wrong. However, there has been little discussion of transparency
problems in the causal machine learning literature and how these might be
overcome. This paper explores why transparency issues are a problem for causal
machine learning in public policy evaluation applications and considers ways
these problems might be addressed through explainable AI tools and by
simplifying models in line with interpretable AI principles. It then applies
these ideas to a case-study using a causal forest model to estimate conditional
average treatment effects for a hypothetical change in the school leaving age
in Australia. It shows that existing tools for understanding black-box
predictive models are poorly suited to causal machine learning and that
simplifying the model to make it interpretable leads to an unacceptable
increase in error (in this application). It concludes that new tools are needed
to properly understand causal machine learning models and the algorithms that
fit them.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13243" title="Abstract">arXiv:2310.13243</a> [<a href="/pdf/2310.13243" title="Download PDF">pdf</a>, <a href="/format/2310.13243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-source Large Language Models are Strong Zero-shot Query Likelihood  Models for Document Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+S">Shengyao Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Koopman%2C+B">Bevan Koopman</a>, 
<a href="/search/cs?searchtype=author&query=Zuccon%2C+G">Guido Zuccon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In the field of information retrieval, Query Likelihood Models (QLMs) rank
documents based on the probability of generating the query given the content of
a document. Recently, advanced large language models (LLMs) have emerged as
effective QLMs, showcasing promising ranking capabilities. This paper focuses
on investigating the genuine zero-shot ranking effectiveness of recent LLMs,
which are solely pre-trained on unstructured text data without supervised
instruction fine-tuning. Our findings reveal the robust zero-shot ranking
ability of such LLMs, highlighting that additional instruction fine-tuning may
hinder effectiveness unless a question generation task is present in the
fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking
system that integrates LLM-based QLMs with a hybrid zero-shot retriever,
demonstrating exceptional effectiveness in both zero-shot and few-shot
scenarios. We make our codebase publicly available at
https://github.com/ielab/llm-qlm.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13245" title="Abstract">arXiv:2310.13245</a> [<a href="/pdf/2310.13245" title="Download PDF">pdf</a>, <a href="/format/2310.13245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simultaneous Shape Tracking of Multiple Deformable Linear Objects with  Global-Local Topology Preservation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jingyi Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Dinkel%2C+H">Holly Dinkel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This work presents an algorithm for tracking the shape of multiple entangling
Deformable Linear Objects (DLOs) from a sequence of RGB-D images. This
algorithm runs in real-time and improves on previous single-DLO tracking
approaches by enabling tracking of multiple objects. This is achieved using
Global-Local Topology Preservation (GLTP). This work uses the geodesic distance
in GLTP to define the distance between separate objects and the distance
between different parts of the same object. Tracking multiple entangling DLOs
is demonstrated experimentally. The source code is publicly released.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13247" title="Abstract">arXiv:2310.13247</a> [<a href="/pdf/2310.13247" title="Download PDF">pdf</a>, <a href="/format/2310.13247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anomaly Detection of Command Shell Sessions based on DistilBERT:  Unsupervised and Supervised Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zefang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Buford%2C+J">John Buford</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Anomaly detection in command shell sessions is a critical aspect of computer
security. Recent advances in deep learning and natural language processing,
particularly transformer-based models, have shown great promise for addressing
complex security challenges. In this paper, we implement a comprehensive
approach to detect anomalies in Unix shell sessions using a pretrained
DistilBERT model, leveraging both unsupervised and supervised learning
techniques to identify anomalous activity while minimizing data labeling. The
unsupervised method captures the underlying structure and syntax of Unix shell
commands, enabling the detection of session deviations from normal behavior.
Experiments on a large-scale enterprise dataset collected from production
systems demonstrate the effectiveness of our approach in detecting anomalous
behavior in Unix shell sessions. This work highlights the potential of
leveraging recent advances in transformers to address important computer
security challenges.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13248" title="Abstract">arXiv:2310.13248</a> [<a href="/pdf/2310.13248" title="Download PDF">pdf</a>, <a href="/format/2310.13248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural  Network in Analyzing Geospatial Resilience of Multicommodity Food Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Yuxiao Qu</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+J">Jinmeng Rao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Song Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qianheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+W">Wei-Lun Chao</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+M">Michelle Miller</a>, 
<a href="/search/cs?searchtype=author&query=Morales%2C+A">Alfonso Morales</a>, 
<a href="/search/cs?searchtype=author&query=Huber%2C+P">Patrick Huber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM SIGSPATIAL GeoAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Understanding and measuring the resilience of food supply networks is a
global imperative to tackle increasing food insecurity. However, the complexity
of these networks, with their multidimensional interactions and decisions,
presents significant challenges. This paper proposes FLEE-GNN, a novel
Federated Learning System for Edge-Enhanced Graph Neural Network, designed to
overcome these challenges and enhance the analysis of geospatial resilience of
multicommodity food flow network, which is one type of spatial networks.
FLEE-GNN addresses the limitations of current methodologies, such as
entropy-based methods, in terms of generalizability, scalability, and data
privacy. It combines the robustness and adaptability of graph neural networks
with the privacy-conscious and decentralized aspects of federated learning on
food supply network resilience analysis across geographical regions. This paper
also discusses FLEE-GNN's innovative data generation techniques, experimental
designs, and future directions for improvement. The results show the
advancements of this approach to quantifying the resilience of multicommodity
food flow networks, contributing to efforts towards ensuring global food
security using AI methods. The developed FLEE-GNN has the potential to be
applied in other spatial networks with spatially heterogeneous sub-network
distributions.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13249" title="Abstract">arXiv:2310.13249</a> [<a href="/pdf/2310.13249" title="Download PDF">pdf</a>, <a href="/format/2310.13249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TempGNN: Temporal Graph Neural Networks for Dynamic Session-Based  Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+E">Eunkyu Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehun Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Session-based recommendations which predict the next action by understanding
a user's interaction behavior with items within a relatively short ongoing
session have recently gained increasing popularity. Previous research has
focused on capturing the dynamics of sequential dependencies from complicated
item transitions in a session by means of recurrent neural networks,
self-attention models, and recently, mostly graph neural networks. Despite the
plethora of different models relying on the order of items in a session, few
approaches have been proposed for dealing better with the temporal implications
between interactions. We present Temporal Graph Neural Networks (TempGNN), a
generic framework for capturing the structural and temporal dynamics in complex
item transitions utilizing temporal embedding operators on nodes and edges on
dynamic session graphs, represented as sequences of timed events. Extensive
experimental results show the effectiveness and adaptability of the proposed
method by plugging it into existing state-of-the-art models. Finally, TempGNN
achieved state-of-the-art performance on two real-world e-commerce datasets.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13252" title="Abstract">arXiv:2310.13252</a> [<a href="/pdf/2310.13252" title="Download PDF">pdf</a>, <a href="/format/2310.13252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Shared Data Manipulation in Distributed Optimization  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alkhraijah%2C+M">Mohannad Alkhraijah</a>, 
<a href="/search/eess?searchtype=author&query=Harris%2C+R">Rachel Harris</a>, 
<a href="/search/eess?searchtype=author&query=Litchfield%2C+S">Samuel Litchfield</a>, 
<a href="/search/eess?searchtype=author&query=Huggins%2C+D">David Huggins</a>, 
<a href="/search/eess?searchtype=author&query=Molzahn%2C+D+K">Daniel K. Molzahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper investigates the vulnerability of the Alternating Direction Method
of Multipliers (ADMM) algorithm to shared data manipulation, with a focus on
solving optimal power flow (OPF) problems. Deliberate data manipulation may
cause the ADMM algorithm to converge to suboptimal solutions. We derive two
sufficient conditions for detecting data manipulation based on the theoretical
convergence trajectory of the ADMM algorithm. We evaluate the detection
conditions' performance on three data manipulation strategies we previously
proposed: simple, feedback, and bilevel optimization attacks. We then extend
these three data manipulation strategies to avoid detection by considering both
the detection conditions and a neural network (NN) detection model in the
attacks. We also propose an adversarial NN training framework to detect shared
data manipulation. We illustrate the performance of our data manipulation
strategy and detection framework on OPF problems. The results show that the
proposed detection conditions successfully detect most of the data manipulation
attacks. However, a bilevel optimization attack strategy that incorporates the
detection methods may avoid being detected. Countering this, our proposed
adversarial training framework detects all the instances of the bilevel
optimization attack.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13253" title="Abstract">arXiv:2310.13253</a> [<a href="/pdf/2310.13253" title="Download PDF">pdf</a>, <a href="/format/2310.13253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Graph Context-Enhanced Diversified Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaolong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingdai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, accepted by WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The field of Recommender Systems (RecSys) has been extensively studied to
enhance accuracy by leveraging users' historical interactions. Nonetheless,
this persistent pursuit of accuracy frequently engenders diminished diversity,
culminating in the well-recognized "echo chamber" phenomenon. Diversified
RecSys has emerged as a countermeasure, placing diversity on par with accuracy
and garnering noteworthy attention from academic circles and industry
practitioners. This research explores the realm of diversified RecSys within
the intricate context of knowledge graphs (KG). These KGs act as repositories
of interconnected information concerning entities and items, offering a
propitious avenue to amplify recommendation diversity through the incorporation
of insightful contextual information. Our contributions include introducing an
innovative metric, Entity Coverage, and Relation Coverage, which effectively
quantifies diversity within the KG domain. Additionally, we introduce the
Diversified Embedding Learning (DEL) module, meticulously designed to formulate
user representations that possess an innate awareness of diversity. In tandem
with this, we introduce a novel technique named Conditional Alignment and
Uniformity (CAU). It adeptly encodes KG item embeddings while preserving
contextual integrity. Collectively, our contributions signify a substantial
stride towards augmenting the panorama of recommendation diversity within the
realm of KG-informed RecSys paradigms.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13254" title="Abstract">arXiv:2310.13254</a> [<a href="/pdf/2310.13254" title="Download PDF">pdf</a>, <a href="/format/2310.13254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Socially Optimal Energy Usage via Adaptive Pricing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiayi Li</a>, 
<a href="/search/cs?searchtype=author&query=Motoki%2C+M">Matthew Motoki</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Baosen Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Using price signals to coordinate the electricity consumption of a group of
users has been studied extensively. Typically, a system operator broadcasts a
price, and users optimizes their own actions subject to the price and internal
cost functions. A central challenge is the operator's lack of knowledge of the
users, since users may not want to share private information. In addition,
learning algorithms are being increasingly used to load control, and users
maybe unable to provide their costs in analytical form.
<br />In this paper, we develop a two time-scale incentive mechanism that
alternately updates between the users and a system operator. The system
operator selects a price, and the users optimize their consumption. Based on
the consumption, a new price is then computed by the system operator. As long
as the users can optimize their own consumption for a given price, the operator
does not need to know or attempt to learn any private information of the users.
We show that under a wide range of assumptions, this iterative process
converges to the social welfare solution. In particular, the cost of the users
need not be strictly convex and its consumption can be the output of a learning
algorithm.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13255" title="Abstract">arXiv:2310.13255</a> [<a href="/pdf/2310.13255" title="Download PDF">pdf</a>, <a href="/format/2310.13255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in  Open Worlds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sipeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiazheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yicheng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zongqing Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent studies have presented compelling evidence that large language models
(LLMs) can equip embodied agents with the self-driven capability to interact
with the world, which marks an initial step toward versatile robotics. However,
these efforts tend to overlook the visual richness of open worlds, rendering
the entire interactive process akin to "a blindfolded text-based game."
Consequently, LLM-based agents frequently encounter challenges in intuitively
comprehending their surroundings and producing responses that are easy to
understand. In this paper, we propose Steve-Eye, an end-to-end trained large
multimodal model designed to address this limitation. Steve-Eye integrates the
LLM with a visual encoder which enables it to process visual-text inputs and
generate multimodal feedback. In addition, we use a semi-automatic strategy to
collect an extensive dataset comprising 850K open-world instruction pairs,
empowering our model to encompass three essential functions for an agent:
multimodal perception, foundational knowledge base, and skill prediction and
planning. Lastly, we develop three open-world evaluation benchmarks, then carry
out extensive experiments from a wide range of perspectives to validate our
model's capability to strategically act and plan. Codes and datasets will be
released.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13256" title="Abstract">arXiv:2310.13256</a> [<a href="/pdf/2310.13256" title="Download PDF">pdf</a>, <a href="/format/2310.13256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher or Lower: Challenges in Object based SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+H">Hongtao Nie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Simultaneous localization and mapping, as a fundamental task in computer
vision, has gained higher demands for performance in recent years due to the
rapid development of autonomous driving and unmanned aerial vehicles.
Traditional SLAM algorithms highly rely on basic geometry features such as
points and lines, which are susceptible to environment. Conversely,
higher-level object features offer richer information that is crucial for
enhancing the overall performance of the framework. However, the effective
utilization of object features necessitates careful consideration of various
challenges, including complexity and process velocity. Given the advantages and
disadvantages of both high-level object feature and low-level geometry
features, it becomes essential to make informed choices within the SLAM
framework. Taking these factors into account, this paper provides a thorough
comparison between geometry features and object features, analyzes the current
mainstream application methods of object features in SLAM frameworks, and
presents a comprehensive overview of the main challenges involved in
object-based SLAM.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13257" title="Abstract">arXiv:2310.13257</a> [<a href="/pdf/2310.13257" title="Download PDF">pdf</a>, <a href="/format/2310.13257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Grounding Helps Learn Word Meanings in Low-Data Regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+C">Chengxu Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Fedorenko%2C+E">Evelina Fedorenko</a>, 
<a href="/search/cs?searchtype=author&query=Andreas%2C+J">Jacob Andreas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Modern neural language models (LMs) are powerful tools for modeling human
sentence production and comprehension, and their internal representations are
remarkably well-aligned with representations of language in the human brain.
But to achieve these results, LMs must be trained in distinctly un-human-like
ways -- requiring orders of magnitude more language data than children receive
during development, and without any of the accompanying grounding in
perception, action, or social behavior. Do models trained more naturalistically
-- with grounded supervision -- exhibit more human-like language learning? We
investigate this question in the context of word learning, a key sub-task in
language acquisition. We train a diverse set of LM architectures, with and
without auxiliary supervision from image captioning tasks, on datasets of
varying scales. We then evaluate these models on a broad set of benchmarks
characterizing models' learning of syntactic categories, lexical relations,
semantic features, semantic similarity, and alignment with human neural
representations. We find that visual supervision can indeed improve the
efficiency of word learning. However, these improvements are limited: they are
present almost exclusively in the low-data regime, and sometimes canceled out
by the inclusion of rich distributional signals from text. The information
conveyed by text and images is not redundant -- we find that models mainly
driven by visual information yield qualitatively different from those mainly
driven by word co-occurrences. However, our results suggest that current
multi-modal modeling approaches fail to effectively leverage visual information
to build more human-like word representations from human-sized datasets.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13258" title="Abstract">arXiv:2310.13258</a> [<a href="/pdf/2310.13258" title="Download PDF">pdf</a>, <a href="/format/2310.13258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kedia%2C+K">Kushal Kedia</a>, 
<a href="/search/cs?searchtype=author&query=Dan%2C+P">Prithwish Dan</a>, 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+A">Atiksh Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sanjiban Choudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Seamless human-robot manipulation in close proximity relies on accurate
forecasts of human motion. While there has been significant progress in
learning forecast models at scale, when applied to manipulation tasks, these
models accrue high errors at critical transition points leading to degradation
in downstream planning performance. Our key insight is that instead of
predicting the most likely human motion, it is sufficient to produce forecasts
that capture how future human motion would affect the cost of a robot's plan.
We present ManiCast, a novel framework that learns cost-aware human forecasts
and feeds them to a model predictive control planner to execute collaborative
manipulation tasks. Our framework enables fluid, real-time interactions between
a human and a 7-DoF robot arm across a number of real-world tasks such as
reactive stirring, object handovers, and collaborative table setting. We
evaluate both the motion forecasts and the end-to-end forecaster-planner system
against a range of learned and heuristic baselines while additionally
contributing new datasets. We release our code and datasets at
https://portal-cornell.github.io/manicast/.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13260" title="Abstract">arXiv:2310.13260</a> [<a href="/pdf/2310.13260" title="Download PDF">pdf</a>, <a href="/format/2310.13260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-Centric Multi-Objective Learning Framework for Responsible  Recommendation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+J">Jianxun Lian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommendation systems effectively guide users in locating their desired
information within extensive content repositories. Generally, a recommendation
model is optimized to enhance accuracy metrics from a user utility standpoint,
such as click-through rate or matching relevance. However, a responsible
industrial recommendation system must address not only user utility
(responsibility to users) but also other objectives, including increasing
platform revenue (responsibility to platforms), ensuring fairness
(responsibility to content creators), and maintaining unbiasedness
(responsibility to long-term healthy development). Multi-objective learning is
a potent approach for achieving responsible recommendation systems.
Nevertheless, current methods encounter two challenges: difficulty in scaling
to heterogeneous objectives within a unified framework, and inadequate
controllability over objective priority during optimization, leading to
uncontrollable solutions.
<br />In this paper, we present a data-centric optimization framework, MoRec, which
unifies the learning of diverse objectives. MoRec is a tri-level framework: the
outer level manages the balance between different objectives, utilizing a
proportional-integral-derivative (PID)-based controller to ensure a preset
regularization on the primary objective. The middle level transforms
objective-aware optimization into data sampling weights using sign gradients.
The inner level employs a standard optimizer to update model parameters with
the sampled data. Consequently, MoRec can flexibly support various objectives
while maintaining the original model intact. Comprehensive experiments on two
public datasets and one industrial dataset showcase the effectiveness,
controllability, flexibility, and Pareto efficiency of MoRec, making it highly
suitable for real-world implementation.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13261" title="Abstract">arXiv:2310.13261</a> [<a href="/pdf/2310.13261" title="Download PDF">pdf</a>, <a href="/format/2310.13261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming  with Feasibility Guarantee
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jialin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaohan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinshang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wotao Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Mixed-integer linear programming (MILP) stands as a notable NP-hard problem
pivotal to numerous crucial industrial applications. The development of
effective algorithms, the tuning of solvers, and the training of machine
learning models for MILP resolution all hinge on access to extensive, diverse,
and representative data. Yet compared to the abundant naturally occurring data
in image and text realms, MILP is markedly data deficient, underscoring the
vital role of synthetic MILP generation. We present DIG-MILP, a deep generative
framework based on variational auto-encoder (VAE), adept at extracting
deep-level structural features from highly limited MILP data and producing
instances that closely mirror the target data. Notably, by leveraging the MILP
duality, DIG-MILP guarantees a correct and complete generation space as well as
ensures the boundedness and feasibility of the generated instances. Our
empirical study highlights the novelty and quality of the instances generated
by DIG-MILP through two distinct downstream tasks: (S1) Data sharing, where
solver solution times correlate highly positive between original and
DIG-MILP-generated instances, allowing data sharing for solver tuning without
publishing the original data; (S2) Data Augmentation, wherein the
DIG-MILP-generated instances bolster the generalization performance of machine
learning models tasked with resolving MILP problems.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13262" title="Abstract">arXiv:2310.13262</a> [<a href="/pdf/2310.13262" title="Download PDF">pdf</a>, <a href="/format/2310.13262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quality-based Syntactic Template Retriever for  Syntactically-controlled Paraphrase Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yunlong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yufeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wenjuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jinan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing syntactically-controlled paraphrase generation (SPG) models perform
promisingly with human-annotated or well-chosen syntactic templates. However,
the difficulty of obtaining such templates actually hinders the practical
application of SPG models. For one thing, the prohibitive cost makes it
unfeasible to manually design decent templates for every source sentence. For
another, the templates automatically retrieved by current heuristic methods are
usually unreliable for SPG models to generate qualified paraphrases. To escape
this dilemma, we propose a novel Quality-based Syntactic Template Retriever
(QSTR) to retrieve templates based on the quality of the to-be-generated
paraphrases. Furthermore, for situations requiring multiple paraphrases for
each source sentence, we design a Diverse Templates Search (DTS) algorithm,
which can enhance the diversity between paraphrases without sacrificing
quality. Experiments demonstrate that QSTR can significantly surpass existing
retrieval methods in generating high-quality paraphrases and even perform
comparably with human-annotated templates in terms of reference-free metrics.
Additionally, human evaluation and the performance on downstream tasks using
our generated paraphrases for data augmentation showcase the potential of our
QSTR and DTS algorithm in practical scenarios.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13263" title="Abstract">arXiv:2310.13263</a> [<a href="/pdf/2310.13263" title="Download PDF">pdf</a>, <a href="/format/2310.13263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UE4-NeRF:Neural Radiance Field for Real-Time Rendering of Large-Scale  Scene
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiaming Gu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Minchao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xiaoyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Guangming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S+A+A">Syed Afaq Ali Shah</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bennamoun%2C+M">Mohammed Bennamoun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Neural Radiance Fields (NeRF) is a novel implicit 3D reconstruction method
that shows immense potential and has been gaining increasing attention. It
enables the reconstruction of 3D scenes solely from a set of photographs.
However, its real-time rendering capability, especially for interactive
real-time rendering of large-scale scenes, still has significant limitations.
To address these challenges, in this paper, we propose a novel neural rendering
system called UE4-NeRF, specifically designed for real-time rendering of
large-scale scenes. We partitioned each large scene into different sub-NeRFs.
In order to represent the partitioned independent scene, we initialize
polygonal meshes by constructing multiple regular octahedra within the scene
and the vertices of the polygonal faces are continuously optimized during the
training process. Drawing inspiration from Level of Detail (LOD) techniques, we
trained meshes of varying levels of detail for different observation levels.
Our approach combines with the rasterization pipeline in Unreal Engine 4 (UE4),
achieving real-time rendering of large-scale scenes at 4K resolution with a
frame rate of up to 43 FPS. Rendering within UE4 also facilitates scene editing
in subsequent stages. Furthermore, through experiments, we have demonstrated
that our method achieves rendering quality comparable to state-of-the-art
approaches. Project page: https://jamchaos.github.io/UE4-NeRF/.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13265" title="Abstract">arXiv:2310.13265</a> [<a href="/pdf/2310.13265" title="Download PDF">pdf</a>, <a href="/format/2310.13265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with  Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Le Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yihong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+F">Fengran Mo</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+J">Jian-Yun Nie</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Aishwarya Agrawal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted into EMNLP2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multi-modal open-domain question answering typically requires evidence
retrieval from databases across diverse modalities, such as images, tables,
passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this
task. To enable LLMs to tackle the task in a zero-shot manner, we introduce
MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer
strategy that bypasses intricate multi-modality ranking, our framework can
accommodate new modalities and seamlessly transition to new models for the
task. Built upon LLMs, MoqaGPT retrieves and extracts answers from each
modality separately, then fuses this multi-modal information using LLMs to
produce a final answer. Our methodology boosts performance on the MMCoQA
dataset, improving F1 by +37.91 points and EM by +34.07 points over the
supervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the
zero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and
significantly closes the gap with supervised methods. Our codebase is available
at https://github.com/lezhang7/MOQAGPT.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13266" title="Abstract">arXiv:2310.13266</a> [<a href="/pdf/2310.13266" title="Download PDF">pdf</a>, <a href="/format/2310.13266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measurement-Based Small-Scale Channel Model for Sub-6 GHz RIS-Assisted  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sang%2C+J">Jian Sang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+J">Jifeng Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+B">Boning Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wankai Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Matthaiou%2C+M">Michail Matthaiou</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Di+Renzo%2C+M">Marco Di Renzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Reconfigurable intelligent surfaces (RISs) have attracted increasing interest
from both academia and industry, thanks to their unique features on controlling
electromagnetic (EM) waves. Although theoretical models of RIS-empowered
communications have covered a variety of applications, yet, very few papers
have investigated the modeling of the real propagation characteristics. In this
paper, we fill this gap by providing an empirical statistical channel model to
describe the small-scale channel variations for RIS-assisted broadband system
at 2.6 GHz. Based on real channel measurements in outdoor, indoor and
outdoor-to-indoor (O2I) environments, we compare and analyze the global,
inter-cluster and intra-cluster parameters. Measurement results indicate that
the intelligent reflection through an RIS can significantly improve the channel
quality by enhancing the $K$-factor and reducing the time dispersion. The
small-scale fading is well characterized by the proposed statistical model and
the empirical channel parameters. These results are essential for the design of
emerging RIS-assisted wireless systems and communication specifications in
future applications.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13267" title="Abstract">arXiv:2310.13267</a> [<a href="/pdf/2310.13267" title="Download PDF">pdf</a>, <a href="/format/2310.13267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Language Encoder of Contrastive Cross-modal Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Mengjie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ono%2C+J">Junya Ono</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zhi Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+C">Chieh-Hsin Lai</a>, 
<a href="/search/cs?searchtype=author&query=Takida%2C+Y">Yuhta Takida</a>, 
<a href="/search/cs?searchtype=author&query=Murata%2C+N">Naoki Murata</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Wei-Hsiang Liao</a>, 
<a href="/search/cs?searchtype=author&query=Shibuya%2C+T">Takashi Shibuya</a>, 
<a href="/search/cs?searchtype=author&query=Wakaki%2C+H">Hiromi Wakaki</a>, 
<a href="/search/cs?searchtype=author&query=Mitsufuji%2C+Y">Yuki Mitsufuji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Contrastive cross-modal models such as CLIP and CLAP aid various
vision-language (VL) and audio-language (AL) tasks. However, there has been
limited investigation of and improvement in their language encoder, which is
the central component of encoding natural language descriptions of image/audio
into vector representations. We extensively evaluate how unsupervised and
supervised sentence embedding training affect language encoder quality and
cross-modal task performance. In VL pretraining, we found that sentence
embedding training language encoder quality and aids in cross-modal tasks,
improving contrastive VL models such as CyCLIP. In contrast, AL pretraining
benefits less from sentence embedding training, which may result from the
limited amount of pretraining data. We analyze the representation spaces to
understand the strengths of sentence embedding training, and find that it
improves text-space uniformity, at the cost of decreased cross-modal alignment.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13268" title="Abstract">arXiv:2310.13268</a> [<a href="/pdf/2310.13268" title="Download PDF">pdf</a>, <a href="/format/2310.13268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model  Statistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kaiwen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion probabilistic models (DPMs) have exhibited excellent performance
for high-fidelity image generation while suffering from inefficient sampling.
Recent works accelerate the sampling procedure by proposing fast ODE solvers
that leverage the specific ODE form of DPMs. However, they highly rely on
specific parameterization during inference (such as noise/data prediction),
which might not be the optimal choice. In this work, we propose a novel
formulation towards the optimal parameterization during sampling that minimizes
the first-order discretization error of the ODE solution. Based on such
formulation, we propose \textit{DPM-Solver-v3}, a new fast ODE solver for DPMs
by introducing several coefficients efficiently computed on the pretrained
model, which we call \textit{empirical model statistics}. We further
incorporate multistep methods and a predictor-corrector framework, and propose
some techniques for improving sample quality at small numbers of function
evaluations (NFE) or large guidance scales. Experiments show that DPM-Solver-v3
achieves consistently better or comparable performance in both unconditional
and conditional sampling with both pixel-space and latent-space DPMs,
especially in 5$\sim$10 NFEs. We achieve FIDs of 12.21 (5 NFE), 2.51 (10 NFE)
on unconditional CIFAR10, and MSE of 0.55 (5 NFE, 7.5 guidance scale) on Stable
Diffusion, bringing a speed-up of 15\%$\sim$30\% compared to previous
state-of-the-art training-free methods. Code is available at
\url{https://github.com/thu-ml/DPM-Solver-v3}.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13269" title="Abstract">arXiv:2310.13269</a> [<a href="/pdf/2310.13269" title="Download PDF">pdf</a>, <a href="/format/2310.13269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Exploratory Study on Simulated Annealing for Feature Selection in  Learning-to-Rank
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haque%2C+M+S">Mohd. Sayemul Haque</a>, 
<a href="/search/cs?searchtype=author&query=Fahim%2C+M">Md. Fahim</a>, 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+M">Muhammad Ibrahim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Learning-to-rank is an applied domain of supervised machine learning. As
feature selection has been found to be effective for improving the accuracy of
learning models in general, it is intriguing to investigate this process for
learning-to-rank domain. In this study, we investigate the use of a popular
meta-heuristic approach called simulated annealing for this task. Under the
general framework of simulated annealing, we explore various neighborhood
selection strategies and temperature cooling schemes. We further introduce a
new hyper-parameter called the progress parameter that can effectively be used
to traverse the search space. Our algorithms are evaluated on five publicly
benchmark datasets of learning-to-rank. For a better validation, we also
compare the simulated annealing-based feature selection algorithm with another
effective meta-heuristic algorithm, namely local beam search. Extensive
experimental results shows the efficacy of our proposed models.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13273" title="Abstract">arXiv:2310.13273</a> [<a href="/pdf/2310.13273" title="Download PDF">pdf</a>, <a href="/format/2310.13273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Object Detection in Range data using Spatiotemporal Normals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Falque%2C+R">Raphael Falque</a>, 
<a href="/search/cs?searchtype=author&query=Gentil%2C+C+L">Cedric Le Gentil</a>, 
<a href="/search/cs?searchtype=author&query=Sukkar%2C+F">Fouad Sukkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">On the journey to enable robots to interact with the real world where humans,
animals, and unpredictable elements are acting as independent agents; it is
crucial for robots to have the capability to detect dynamic objects. In this
paper, we argue that the detection of dynamic objects can be solved by
computing the spatiotemporal normals of a point cloud. In our experiments, we
demonstrate that this simple method can be used robustly for LiDAR and depth
cameras with performances similar to the state of the art while offering a
significantly simpler method.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13275" title="Abstract">arXiv:2310.13275</a> [<a href="/pdf/2310.13275" title="Download PDF">pdf</a>, <a href="/format/2310.13275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Active Deep Decoding of Linear Codes using Importance Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noghrei%2C+H">Hassan Noghrei</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+M">Mohammad-Reza Sadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Mow%2C+W+H">Wai Ho Mow</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The quality and quantity of data used for training greatly influence the
performance and effectiveness of deep learning models. In the context of error
correction, it is essential to generate high-quality samples that are neither
excessively noisy nor entirely correct but close to the decoding region's
decision boundary. To accomplish this objective, this paper utilizes a
restricted version of a recent result on Importance Sampling (IS) distribution
for fast performance evaluation of linear codes. The IS distribution is used
over the segmented observation space and integrated with active learning. This
combination allows for the iterative generation of samples from the shells
whose acquisition functions, defined as the error probabilities conditioned on
each shell, fall within a specific range. By intelligently sampling based on
the proposed IS distribution, significant improvements are demonstrated in the
performance of BCH(63,36) and BCH(63,45) codes with cycle-reduced parity-check
matrices. The proposed IS-based-active Weight Belief Propagation (WBP) decoder
shows improvements of up to 0.4dB in the waterfall region and up to 1.9dB in
the error-floor region of the BER curve, over the conventional WBP. This
approach can be easily adapted to generate efficient samples to train any other
deep learning-based decoder.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13276" title="Abstract">arXiv:2310.13276</a> [<a href="/pdf/2310.13276" title="Download PDF">pdf</a>, <a href="/format/2310.13276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jian%2C+X">Xiangru Jian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yimu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Over recent decades, significant advancements in cross-modal retrieval are
mainly driven by breakthroughs in visual and linguistic modeling. However, a
recent study shows that multi-modal data representations tend to cluster within
a limited convex cone (as representation degeneration problem), which hinders
retrieval performance due to the inseparability of these representations. In
our study, we first empirically validate the presence of the representation
degeneration problem across multiple cross-modal benchmarks and methods. Next,
to address it, we introduce a novel method, called InvGC, a post-processing
technique inspired by graph convolution and average pooling. Specifically,
InvGC defines the graph topology within the datasets and then applies graph
convolution in a subtractive manner. This method effectively separates
representations by increasing the distances between data points. To improve the
efficiency and effectiveness of InvGC, we propose an advanced graph topology,
LocalAdj, which only aims to increase the distances between each data point and
its nearest neighbors. To understand why InvGC works, we present a detailed
theoretical analysis, proving that the lower bound of recall will be improved
after deploying InvGC. Extensive empirical results show that InvGC and InvGC
w/LocalAdj significantly mitigate the representation degeneration problem,
thereby enhancing retrieval performance.
<br />Our code is available at
https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13283" title="Abstract">arXiv:2310.13283</a> [<a href="/pdf/2310.13283" title="Download PDF">pdf</a>, <a href="/format/2310.13283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA  Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Liping Yi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoguang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning (FL) is an emerging machine learning paradigm in which a
central server coordinates multiple participants (a.k.a. FL clients) to train a
model collaboratively on decentralized data with privacy protection. This
paradigm constrains that all clients have to train models with the same
structures (homogeneous). In practice, FL often faces statistical
heterogeneity, system heterogeneity and model heterogeneity challenges. These
challenging issues inspire the field of Model-Heterogeneous Personalized
Federated Learning (MHPFL) which aims to train a personalized and heterogeneous
local model for each FL client. Existing MHPFL approaches cannot achieve
satisfactory model performance, acceptable computational overhead and efficient
communication simultaneously. To bridge this gap, we propose a novel
computation- and communication-efficient model-heterogeneous personalized
Federated learning framework based on LoRA tuning (FedLoRA). It is designed to
incorporate a homogeneous small adapter for each client's heterogeneous local
model. Both models are trained following the proposed iterative training for
global-local knowledge exchange. The homogeneous small local adapters are sent
to the FL server to be aggregated into a global adapter. In this way, FL
clients can train heterogeneous local models without incurring high computation
and communication costs. We theoretically prove the non-convex convergence rate
of FedLoRA. Extensive experiments on two real-world datasets demonstrate that
FedLoRA outperforms six state-of-the-art baselines, beating the best approach
by 1.35% in terms of test accuracy, 11.81 times computation overhead reduction
and 7.41 times communication cost saving.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13284" title="Abstract">arXiv:2310.13284</a> [<a href="/pdf/2310.13284" title="Download PDF">pdf</a>, <a href="/format/2310.13284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Recurrent Models with Temporally Local Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdulsalam%2C+A">Azwar Abdulsalam</a>, 
<a href="/search/cs?searchtype=author&query=Makin%2C+J+G">Joseph G. Makin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the "Localized Learning" workshop at the International Conference on Machine Learning (ICML), July 2023. 6 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Fitting generative models to sequential data typically involves two recursive
computations through time, one forward and one backward. The latter could be a
computation of the loss gradient (as in backpropagation through time), or an
inference algorithm (as in the RTS/Kalman smoother). The backward pass in
particular is computationally expensive (since it is inherently serial and
cannot exploit GPUs), and difficult to map onto biological processes.
Work-arounds have been proposed; here we explore a very different one:
requiring the generative model to learn the joint distribution over current and
previous states, rather than merely the transition probabilities. We show on
toy datasets that different architectures employing this principle can learn
aspects of the data typically requiring the backward pass.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13286" title="Abstract">arXiv:2310.13286</a> [<a href="/pdf/2310.13286" title="Download PDF">pdf</a>, <a href="/format/2310.13286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Pretraining for Recommendation via Task Hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingdai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaolong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although pretraining has garnered significant attention and popularity in
recent years, its application in graph-based recommender systems is relatively
limited. It is challenging to exploit prior knowledge by pretraining in widely
used ID-dependent datasets. On one hand, user-item interaction history in one
dataset can hardly be transferred to other datasets through pretraining, where
IDs are different. On the other hand, pretraining and finetuning on the same
dataset leads to a high risk of overfitting. In this paper, we propose a novel
multitask pretraining framework named Unified Pretraining for Recommendation
via Task Hypergraphs. For a unified learning pattern to handle diverse
requirements and nuances of various pretext tasks, we design task hypergraphs
to generalize pretext tasks to hyperedge prediction. A novel transitional
attention layer is devised to discriminatively learn the relevance between each
pretext task and recommendation. Experimental results on three benchmark
datasets verify the superiority of UPRTH. Additional detailed investigations
are conducted to demonstrate the effectiveness of the proposed framework.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13289" title="Abstract">arXiv:2310.13289</a> [<a href="/pdf/2310.13289" title="Download PDF">pdf</a>, <a href="/format/2310.13289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALMONN: Towards Generic Hearing Abilities for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Changli Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guangzhi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xianzhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+T">Tian Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zejun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Hearing is arguably an essential ability of artificial intelligence (AI)
agents in the physical world, which refers to the perception and understanding
of general auditory information consisting of at least three types of sounds:
speech, audio events, and music. In this paper, we propose SALMONN, a speech
audio language music open neural network, built by integrating a pre-trained
text-based large language model (LLM) with speech and audio encoders into a
single multimodal model. SALMONN enables the LLM to directly process and
understand general audio inputs and achieve competitive performances on a
number of speech and audio tasks used in training, such as automatic speech
recognition and translation, auditory-information-based question answering,
emotion recognition, speaker verification, and music and audio captioning
\textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in
the training, which includes but is not limited to speech translation to
untrained languages, speech-based slot filling, spoken-query-based question
answering, audio-based storytelling, and speech audio co-reasoning
\textit{etc}. The presence of the cross-modal emergent abilities is studied,
and a novel few-shot activation tuning approach is proposed to activate such
abilities of SALMONN. To our knowledge, SALMONN is the first model of its type
and can be regarded as a step towards AI with generic hearing abilities. An
interactive demo of SALMONN is available at
\texttt{\url{https://github.com/bytedance/SALMONN}}, and the training code and
model checkpoints will be released upon acceptance.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13290" title="Abstract">arXiv:2310.13290</a> [<a href="/pdf/2310.13290" title="Download PDF">pdf</a>, <a href="/format/2310.13290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting Indirect Answers to Yes-No Questions in Multiple Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hossain%2C+M+M">Md Mosharaf Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+S">Shivam Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Melo%2C+T+C">Terry Cruz Melo</a>, 
<a href="/search/cs?searchtype=author&query=Ozler%2C+K+B">Kadir Bulut Ozler</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K+H">Keun Hee Park</a>, 
<a href="/search/cs?searchtype=author&query=Quintero%2C+J">Jacob Quintero</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+M">MohammadHossein Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Shakya%2C+S+N">Shreya Nupur Shakya</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M+N">Md Nayem Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Blanco%2C+E">Eduardo Blanco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Yes-no questions expect a yes or no for an answer, but people often skip
polar keywords. Instead, they answer with long explanations that must be
interpreted. In this paper, we focus on this challenging problem and release
new benchmarks in eight languages. We present a distant supervision approach to
collect training data. We also demonstrate that direct answers (i.e., with
polar keywords) are useful to train models to interpret indirect answers (i.e.,
without polar keywords). Experimental results demonstrate that monolingual
fine-tuning is beneficial if training data can be obtained via distant
supervision for the language of interest (5 languages). Additionally, we show
that cross-lingual fine-tuning is always beneficial (8 languages).
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13291" title="Abstract">arXiv:2310.13291</a> [<a href="/pdf/2310.13291" title="Download PDF">pdf</a>, <a href="/format/2310.13291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Privacy Risks in Language Models: A Case Study on  Summarization Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruixiang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Lueck%2C+G">Gord Lueck</a>, 
<a href="/search/cs?searchtype=author&query=Quispe%2C+R">Rodolfo Quispe</a>, 
<a href="/search/cs?searchtype=author&query=Inan%2C+H+A">Huseyin A Inan</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+J">Janardhan Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models have revolutionized the field of NLP by achieving
state-of-the-art performance on various tasks. However, there is a concern that
these models may disclose information in the training data. In this study, we
focus on the summarization task and investigate the membership inference (MI)
attack: given a sample and black-box access to a model's API, it is possible to
determine if the sample was part of the training data. We exploit text
similarity and the model's resistance to document modifications as potential MI
signals and evaluate their effectiveness on widely used datasets. Our results
demonstrate that summarization models are at risk of exposing data membership,
even in cases where the reference summary is not available. Furthermore, we
discuss several safeguards for training summarization models to protect against
MI attacks and discuss the inherent trade-off between privacy and utility.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13292" title="Abstract">arXiv:2310.13292</a> [<a href="/pdf/2310.13292" title="Download PDF">pdf</a>, <a href="/format/2310.13292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+K">Kihyun You</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jawook Gu</a>, 
<a href="/search/cs?searchtype=author&query=Ham%2C+J">Jiyeon Ham</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Beomhee Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jiho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+E+K">Eun Kyoung Hong</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+W">Woonhyunk Baek</a>, 
<a href="/search/cs?searchtype=author&query=Roh%2C+B">Byungseok Roh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A large-scale image-text pair dataset has greatly contributed to the
development of vision-language pre-training (VLP) models, which enable
zero-shot or few-shot classification without costly annotation. However, in the
medical domain, the scarcity of data remains a significant challenge for
developing a powerful VLP model. In this paper, we tackle the lack of
image-text data in chest X-ray by expanding image-label pair as image-text pair
via general prompt and utilizing multiple images and multiple sections in a
radiologic report. We also design two contrastive losses, named ICL and TCL,
for learning study-level characteristics of medical images and reports,
respectively. Our model outperforms the state-of-the-art models trained under
the same conditions. Also, enlarged dataset improve the discriminative power of
our pre-trained model for classification, while sacrificing marginal retrieval
performance. Code is available at https://github.com/kakaobrain/cxr-clip.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13294" title="Abstract">arXiv:2310.13294</a> [<a href="/pdf/2310.13294" title="Download PDF">pdf</a>, <a href="/format/2310.13294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VR PreM+: An Immersive Pre-learning Branching Visualization System for  Museum Tours
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Ze Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changkun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Anqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+P">Pan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Braud%2C+T">Tristan Braud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at The Eleventh International Symposium of Chinese CHI (Chinese CHI 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">We present VR PreM+, an innovative VR system designed to enhance web
exploration beyond traditional computer screens. Unlike static 2D displays, VR
PreM+ leverages 3D environments to create an immersive pre-learning experience.
Using keyword-based information retrieval allows users to manage and connect
various content sources in a dynamic 3D space, improving communication and data
comparison. We conducted preliminary and user studies that demonstrated
efficient information retrieval, increased user engagement, and a greater sense
of presence. These findings yielded three design guidelines for future VR
information systems: display, interaction, and user-centric design. VR PreM+
bridges the gap between traditional web browsing and immersive VR, offering an
interactive and comprehensive approach to information acquisition. It holds
promise for research, education, and beyond.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13295" title="Abstract">arXiv:2310.13295</a> [<a href="/pdf/2310.13295" title="Download PDF">pdf</a>, <a href="/format/2310.13295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PathRL: An End-to-End Path Generation Method for Collision Avoidance via  Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+J">Jie Peng</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Q">Quecheng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jianmin Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Robot navigation using deep reinforcement learning (DRL) has shown great
potential in improving the performance of mobile robots. Nevertheless, most
existing DRL-based navigation methods primarily focus on training a policy that
directly commands the robot with low-level controls, like linear and angular
velocities, which leads to unstable speeds and unsmooth trajectories of the
robot during the long-term execution. An alternative method is to train a DRL
policy that outputs the navigation path directly. However, two roadblocks arise
for training a DRL policy that outputs paths: (1) The action space for
potential paths often involves higher dimensions comparing to low-level
commands, which increases the difficulties of training; (2) It takes multiple
time steps to track a path instead of a single time step, which requires the
path to predicate the interactions of the robot w.r.t. the dynamic environment
in multiple time steps. This, in turn, amplifies the challenges associated with
training. In response to these challenges, we propose PathRL, a novel DRL
method that trains the policy to generate the navigation path for the robot.
Specifically, we employ specific action space discretization techniques and
tailored state space representation methods to address the associated
challenges. In our experiments, PathRL achieves better success rates and
reduces angular rotation variability compared to other DRL navigation methods,
facilitating stable and smooth robot movement. We demonstrate the competitive
edge of PathRL in both real-world scenarios and multiple challenging simulation
environments.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13297" title="Abstract">arXiv:2310.13297</a> [<a href="/pdf/2310.13297" title="Download PDF">pdf</a>, <a href="/format/2310.13297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding the Silent Majority: Inducing Belief Augmented Social Graph  with Large Language Model for Response Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chenkai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinning Li</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+Y+R">Yi R. Fung</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+H+P">Hou Pong Chan</a>, 
<a href="/search/cs?searchtype=author&query=Abdelzaher%2C+T">Tarek Abdelzaher</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+C">ChengXiang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Automatic response forecasting for news media plays a crucial role in
enabling content producers to efficiently predict the impact of news releases
and prevent unexpected negative outcomes such as social conflict and moral
injury. To effectively forecast responses, it is essential to develop measures
that leverage the social dynamics and contextual information surrounding
individuals, especially in cases where explicit profiles or historical actions
of the users are limited (referred to as lurkers). As shown in a previous
study, 97% of all tweets are produced by only the most active 25% of users.
However, existing approaches have limited exploration of how to best process
and utilize these important features. To address this gap, we propose a novel
framework, named SocialSense, that leverages a large language model to induce a
belief-centered graph on top of an existent social network, along with
graph-based propagation to capture social dynamics. We hypothesize that the
induced graph that bridges the gap between distant users who share similar
beliefs allows the model to effectively capture the response patterns. Our
method surpasses existing state-of-the-art in experimental evaluations for both
zero-shot and supervised settings, demonstrating its effectiveness in response
forecasting. Moreover, the analysis reveals the framework's capability to
effectively handle unseen user and lurker scenarios, further highlighting its
robustness and practical applicability.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13298" title="Abstract">arXiv:2310.13298</a> [<a href="/pdf/2310.13298" title="Download PDF">pdf</a>, <a href="/ps/2310.13298" title="Download PostScript">ps</a>, <a href="/format/2310.13298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cache-Aided Communications in MISO Networks with Dynamic User Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abolpour%2C+M">Milad Abolpour</a>, 
<a href="/search/cs?searchtype=author&query=Salehi%2C+M">MohammadJavad Salehi</a>, 
<a href="/search/cs?searchtype=author&query=T%C3%B6lli%2C+A">Antti T&#xf6;lli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages. arXiv admin note: substantial text overlap with <a href="/abs/2304.11623">arXiv:2304.11623</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Coded caching (CC) can substantially enhance network performance by
leveraging memory as an additional communication resource. However, the use of
CC is challenging in various practical applications due to dynamic user
behavior. The existing solutions, based on shared caching, cannot directly
handle all scenarios where users freely enter and depart the network at any
time as they are constrained by specific conditions on network parameters. This
paper proposes a universally applicable shared-caching scheme for dynamic
setups without any restriction on network parameters. The closed-form
expressions for the achievable degrees of freedom (DoF) are computed for the
resulting generalized scheme, and are shown to achieve the existing optimal
bounds of the shared-cache model. Furthermore, a
successive-interference-cancellation-free extension based on a fast iterative
optimized beamformer design is devised to optimize the use of excess spatial
dimensions freed by cache-aided interference cancellation. Extensive numerical
experiments are carried out to assess the performance of the proposed scheme.
In particular, the results demonstrate that while a dynamic setup may achieve a
DoF substantially lower than the optimal DoF of shared caching, our proposed
scheme significantly improves the performance at the finite signal-to-noise
ratio compared to unicasting, which only benefits from the local caching gain.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13303" title="Abstract">arXiv:2310.13303</a> [<a href="/pdf/2310.13303" title="Download PDF">pdf</a>, <a href="/format/2310.13303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motif-Based Prompt Learning for Universal Cross-Domain Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+B">Bowen Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chaoqun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Junliang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Cross-Domain Recommendation (CDR) stands as a pivotal technology addressing
issues of data sparsity and cold start by transferring general knowledge from
the source to the target domain. However, existing CDR models suffer
limitations in adaptability across various scenarios due to their inherent
complexity. To tackle this challenge, recent advancements introduce universal
CDR models that leverage shared embeddings to capture general knowledge across
domains and transfer it through "Multi-task Learning" or "Pre-train, Fine-tune"
paradigms. However, these models often overlook the broader structural topology
that spans domains and fail to align training objectives, potentially leading
to negative transfer. To address these issues, we propose a motif-based prompt
learning framework, MOP, which introduces motif-based shared embeddings to
encapsulate generalized domain knowledge, catering to both intra-domain and
inter-domain CDR tasks. Specifically, we devise three typical motifs:
butterfly, triangle, and random walk, and encode them through a Motif-based
Encoder to obtain motif-based shared embeddings. Moreover, we train MOP under
the "Pre-training \&amp; Prompt Tuning" paradigm. By unifying pre-training and
recommendation tasks as a common motif-based similarity learning task and
integrating adaptable prompt parameters to guide the model in downstream
recommendation tasks, MOP excels in transferring domain knowledge effectively.
Experimental results on four distinct CDR tasks demonstrate the effectiveness
of MOP than the state-of-the-art models.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13304" title="Abstract">arXiv:2310.13304</a> [<a href="/pdf/2310.13304" title="Download PDF">pdf</a>, <a href="/format/2310.13304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Emotions, Social Roles, Well-being and Mobile Usage  Behaviors During COVID-19 Home Confinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+N">Nan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Nolan%2C+S">Sam Nolan</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Kaixin Ji</a>, 
<a href="/search/cs?searchtype=author&query=Rumi%2C+S+K">Shakila Khan Rumi</a>, 
<a href="/search/cs?searchtype=author&query=Heinisch%2C+J+S">Judith Simone Heinisch</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+C">Christoph Anderson</a>, 
<a href="/search/cs?searchtype=author&query=David%2C+K">Klaus David</a>, 
<a href="/search/cs?searchtype=author&query=Salim%2C+F+D">Flora D. Salim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Pandemics and epidemics have been an essential part of human history. It has
powerfully shaped human civilization, affecting politics, economics, medicine,
and everyday human lives. Home confinement, one of the social isolation
restrictions, has proven to be an effective way to prevent a pandemic and
significantly reduce the spread of the virus. However, these measures have also
led to negative psychological effects, which can be more severe than the
physical discomfort caused by the pandemic itself. To address this issue, it is
crucial to understand how being confined at home affects people's psychological
states and their use of digital devices. To this end, we conducted an in-situ
study with 32 participants living in states affected by COVID-19 lockdowns for
three weeks. We examined human emotions, social roles, well-being, and mobile
usage behaviors and explored the predictability of affective states using
digital traces. Extensive experimental results show that COVID-19 home
confinement is associated with decreased valence and increased work time and
that human well-being could be accurately inferred from mobile usage behaviors.
We also present a set of interesting findings on how different factors of
mobile usage affect human emotions. Overall, the findings of this research have
great potential to inform the development of interventions and remote programs
to support individuals in similar situations, such as those who are ill or
undergoing post-operative home rehabilitation and home work.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13307" title="Abstract">arXiv:2310.13307</a> [<a href="/pdf/2310.13307" title="Download PDF">pdf</a>, <a href="/format/2310.13307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Time Self-Adaptive Small Language Models for Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+S">Soyeong Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+J">Jinheon Baek</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Sukmin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J+C">Jong C. Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent instruction-finetuned large language models (LMs) have achieved
notable performances in various tasks, such as question-answering (QA).
However, despite their ability to memorize a vast amount of general knowledge
across diverse tasks, they might be suboptimal on specific tasks due to their
limited capacity to transfer and adapt knowledge to target tasks. Moreover,
further finetuning LMs with labeled datasets is often infeasible due to their
absence, but it is also questionable if we can transfer smaller LMs having
limited knowledge only with unlabeled test data. In this work, we show and
investigate the capabilities of smaller self-adaptive LMs, only with unlabeled
test data. In particular, we first stochastically generate multiple answers,
and then ensemble them while filtering out low-quality samples to mitigate
noise from inaccurate labels. Our proposed self-adaption strategy demonstrates
significant performance improvements on benchmark QA datasets with higher
robustness across diverse prompts, enabling LMs to stay stable. Code is
available at: https://github.com/starsuzi/T-SAS.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13309" title="Abstract">arXiv:2310.13309</a> [<a href="/pdf/2310.13309" title="Download PDF">pdf</a>, <a href="/format/2310.13309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enumerating regular languages in radix order : Revisiting the  Ackerman-Shallit algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Francis%2C+N">Nadime Francis</a>, 
<a href="/search/cs?searchtype=author&query=Marsault%2C+V">Victor Marsault</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">We consider the problem of enumerating a regular language $L$ in radix order,
or more precisely, the equivalent problem of enumerating all words in $L$ of a
given length in lexicographic order. Ackerman and Shallit gave in 2009 the
principles of an efficient solution to this problem, but they did not use the
enumeration complexity framework for their analysis. We adapt their work into
an explicit algorithm that fits this framework.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13312" title="Abstract">arXiv:2310.13312</a> [<a href="/pdf/2310.13312" title="Download PDF">pdf</a>, <a href="/format/2310.13312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Impact of Corpus Diversity on Financial Pretrained  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choe%2C+J">Jaeyoung Choe</a>, 
<a href="/search/cs?searchtype=author&query=Noh%2C+K">Keonwoong Noh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N">Nayeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Seyun Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+W">Woohwan Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Over the past few years, various domain-specific pretrained language models
(PLMs) have been proposed and have outperformed general-domain PLMs in
specialized areas such as biomedical, scientific, and clinical domains. In
addition, financial PLMs have been studied because of the high economic impact
of financial data analysis. However, we found that financial PLMs were not
pretrained on sufficiently diverse financial data. This lack of diverse
training data leads to a subpar generalization performance, resulting in
general-purpose PLMs, including BERT, often outperforming financial PLMs on
many downstream tasks. To address this issue, we collected a broad range of
financial corpus and trained the Financial Language Model (FiLM) on these
diverse datasets. Our experimental results confirm that FiLM outperforms not
only existing financial PLMs but also general domain PLMs. Furthermore, we
provide empirical evidence that this improvement can be achieved even for
unseen corpus groups.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13313" title="Abstract">arXiv:2310.13313</a> [<a href="/pdf/2310.13313" title="Download PDF">pdf</a>, <a href="/ps/2310.13313" title="Download PostScript">ps</a>, <a href="/format/2310.13313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform convergence of optimal order of a local discontinuous Galerkin  method on a Shishkin mesh under a balanced norm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ma%2C+X">Xiaoqi Ma</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jin Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zheng%2C+W">Wenchao Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This article investigates a local discontinuous Galerkin (LDG) method for
one-dimensional and two-dimensional singularly perturbed reaction-diffusion
problems on a Shishkin mesh. During this process, due to the inability of the
energy norm to fully capture the behavior of the boundary layers appearing in
the solutions, a balanced norm is introduced. By designing novel numerical
fluxes and constructing special interpolations, optimal convergences under the
balanced norm are achieved in both 1D and 2D cases. Numerical experiments
support the main theoretical conclusions.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13314" title="Abstract">arXiv:2310.13314</a> [<a href="/pdf/2310.13314" title="Download PDF">pdf</a>, <a href="/format/2310.13314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Policy Gradient and Safety-Based Control for Autonomous  Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+X">Xi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">With the advancement of data-driven techniques, addressing continuous
con-trol challenges has become more efficient. However, the reliance of these
methods on historical data introduces the potential for unexpected decisions in
novel scenarios. To enhance performance in autonomous driving and collision
avoidance, we propose a symbiotic fusion of policy gradient with safety-based
control. In this study, we em-ploy the Deep Deterministic Policy Gradient
(DDPG) algorithm to enable autono-mous driving in the absence of surrounding
vehicles. By training the vehicle's driving policy within a stable and familiar
environment, a robust and efficient learning pro-cess is achieved.
Subsequently, an artificial potential field approach is utilized to formulate a
collision avoidance algorithm, accounting for the presence of surround-ing
vehicles. Furthermore, meticulous consideration is given to path tracking
meth-ods. The amalgamation of these approaches demonstrates substantial
performance across diverse scenarios, underscoring its potential for advancing
autonomous driving while upholding safety standards.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13315" title="Abstract">arXiv:2310.13315</a> [<a href="/pdf/2310.13315" title="Download PDF">pdf</a>, <a href="/format/2310.13315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Miaoxi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Q">Qihuang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Juhua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP2023 (Main). Miaoxi Zhu and Qihuang Zhong contribute equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Quantization is a promising approach for reducing memory overhead and
accelerating inference, especially in large pre-trained language model (PLM)
scenarios. While having no access to original training data due to security and
privacy concerns has emerged the demand for zero-shot quantization. Most of the
cutting-edge zero-shot quantization methods primarily 1) apply to computer
vision tasks, and 2) neglect of overfitting problem in the generative
adversarial learning process, leading to sub-optimal performance. Motivated by
this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ)
framework for the zero-shot quantization of various PLMs. The key algorithm in
solving ZSAQ is the SAM-SGA optimization, which aims to improve the
quantization accuracy and model generalization via optimizing a minimax
problem. We theoretically prove the convergence rate for the minimax
optimization problem and this result can be applied to other nonconvex-PL
minimax optimization frameworks. Extensive experiments on 11 tasks demonstrate
that our method brings consistent and significant performance gains on both
discriminative and generative PLMs, i.e., up to +6.98 average score.
Furthermore, we empirically validate that our method can effectively improve
the model generalization.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13316" title="Abstract">arXiv:2310.13316</a> [<a href="/pdf/2310.13316" title="Download PDF">pdf</a>, <a href="/format/2310.13316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coarse-to-Fine Dual Encoders are Better Frame Identification Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+K">Kaikai An</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Ce Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+B">Bofei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haozhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+B">Baobao Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Frame identification aims to find semantic frames associated with target
words in a sentence. Recent researches measure the similarity or matching score
between targets and candidate frames by modeling frame definitions. However,
they either lack sufficient representation learning of the definitions or face
challenges in efficiently selecting the most suitable frame from over 1000
candidate frames. Moreover, commonly used lexicon filtering ($lf$) to obtain
candidate frames for the target may ignore out-of-vocabulary targets and cause
inadequate frame modeling. In this paper, we propose CoFFTEA, a
$\underline{Co}$arse-to-$\underline{F}$ine $\underline{F}$rame and
$\underline{T}$arget $\underline{E}$ncoders $\underline{A}$rchitecture. With
contrastive learning and dual encoders, CoFFTEA efficiently and effectively
models the alignment between frames and targets. By employing a coarse-to-fine
curriculum learning procedure, CoFFTEA gradually learns to differentiate frames
with varying degrees of similarity. Experimental results demonstrate that
CoFFTEA outperforms previous models by 0.93 overall scores and 1.53 R@1 without
$lf$. Further analysis suggests that CoFFTEA can better model the relationships
between frame and frame, as well as target and target. The code for our
approach is available at https://github.com/pkunlp-icler/COFFTEA.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13320" title="Abstract">arXiv:2310.13320</a> [<a href="/pdf/2310.13320" title="Download PDF">pdf</a>, <a href="/format/2310.13320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CylinderTag: An Accurate and Flexible Marker for Cylinder-Shape Objects  Pose Estimation Based on Projective Invariants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shaoan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mingzhu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yaoqing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+F">Fusong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Junzhi Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 22 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Robotics (cs.RO)

</div>
<p class="mathjax">High-precision pose estimation based on visual markers has been a thriving
research topic in the field of computer vision. However, the suitability of
traditional flat markers on curved objects is limited due to the diverse shapes
of curved surfaces, which hinders the development of high-precision pose
estimation for curved objects. Therefore, this paper proposes a novel visual
marker called CylinderTag, which is designed for developable curved surfaces
such as cylindrical surfaces. CylinderTag is a cyclic marker that can be firmly
attached to objects with a cylindrical shape. Leveraging the manifold
assumption, the cross-ratio in projective invariance is utilized for encoding
in the direction of zero curvature on the surface. Additionally, to facilitate
the usage of CylinderTag, we propose a heuristic search-based marker generator
and a high-performance recognizer as well. Moreover, an all-encompassing
evaluation of CylinderTag properties is conducted by means of extensive
experimentation, covering detection rate, detection speed, dictionary size,
localization jitter, and pose estimation accuracy. CylinderTag showcases
superior detection performance from varying view angles in comparison to
traditional visual markers, accompanied by higher localization accuracy.
Furthermore, CylinderTag boasts real-time detection capability and an extensive
marker dictionary, offering enhanced versatility and practicality in a wide
range of applications. Experimental results demonstrate that the CylinderTag is
a highly promising visual marker for use on cylindrical-like surfaces, thus
offering important guidance for future research on high-precision visual
localization of cylinder-shaped objects. The code is available at:
https://github.com/wsakobe/CylinderTag.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13321" title="Abstract">arXiv:2310.13321</a> [<a href="/pdf/2310.13321" title="Download PDF">pdf</a>, <a href="/format/2310.13321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Hard Samples: Robust and Effective Grammatical Error Correction  with Cycle Self-Augmenting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zecheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+K">Kaifeng Qi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juntao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent studies have revealed that grammatical error correction methods in the
sequence-to-sequence paradigm are vulnerable to adversarial attack, and simply
utilizing adversarial examples in the pre-training or post-training process can
significantly enhance the robustness of GEC models to certain types of attack
without suffering too much performance loss on clean data. In this paper, we
further conduct a thorough robustness evaluation of cutting-edge GEC methods
for four different types of adversarial attacks and propose a simple yet very
effective Cycle Self-Augmenting (CSA) method accordingly. By leveraging the
augmenting data from the GEC models themselves in the post-training process and
introducing regularization data for cycle training, our proposed method can
effectively improve the model robustness of well-trained GEC models with only a
few more training epochs as an extra cost. More concretely, further training on
the regularization data can prevent the GEC models from over-fitting on
easy-to-learn samples and thus can improve the generalization capability and
robustness towards unseen data (adversarial noise/samples). Meanwhile, the
self-augmented data can provide more high-quality pseudo pairs to improve model
performance on the original testing data. Experiments on four benchmark
datasets and seven strong models indicate that our proposed training method can
significantly enhance the robustness of four types of attacks without using
purposely built adversarial examples in training. Evaluation results on clean
data further confirm that our proposed CSA method significantly improves the
performance of four baselines and yields nearly comparable results with other
state-of-the-art models. Our code is available at
https://github.com/ZetangForward/CSA-GEC.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13324" title="Abstract">arXiv:2310.13324</a> [<a href="/pdf/2310.13324" title="Download PDF">pdf</a>, <a href="/format/2310.13324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ColAG: A Collaborative Air-Ground Framework for Perception-Limited UGVs&#x27;  Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhehan Li</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+R">Rui Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nanhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Fei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yanjun Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures, submitted to ICRA2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Perception is necessary for autonomous navigation in an unknown area crowded
with obstacles. It's challenging for a robot to navigate safely without any
sensors that can sense the environment, resulting in a $\textit{blind}$ robot,
and becomes more difficult when comes to a group of robots. However, it could
be costly to equip all robots with expensive perception or SLAM systems. In
this paper, we propose a novel system named $\textbf{ColAG}$, to solve the
problem of autonomous navigation for a group of $\textit{blind}$ UGVs by
introducing cooperation with one UAV, which is the only robot that has full
perception capabilities in the group. The UAV uses SLAM for its odometry and
mapping while sharing this information with UGVs via limited relative pose
estimation. The UGVs plan their trajectories in the received map and predict
possible failures caused by the uncertainty of its wheel odometry and unknown
risky areas. The UAV dynamically schedules waypoints to prevent UGVs from
collisions, formulated as a Vehicle Routing Problem with Time Windows to
optimize the UAV's trajectories and minimize time when UGVs have to wait to
guarantee safety. We validate our system through extensive simulation with up
to 7 UGVs and real-world experiments with 3 UGVs.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13328" title="Abstract">arXiv:2310.13328</a> [<a href="/pdf/2310.13328" title="Download PDF">pdf</a>, <a href="/format/2310.13328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Phase Batch Update on Sparse Merkle Trees for Rollups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+B">Boqian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+V+N">Vir Nath Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lanping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ruj%2C+S">Sushmita Ruj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">A sparse Merkle tree is a Merkle tree with fixed height and indexed leaves
given by a map from indices to leaf values. It allows for both efficient
membership and non-membership proofs. It has been widely used as an
authenticated data structure in various applications, such as layer-2 rollups
for blockchains. zkSync Lite, a popular Ethereum layer-2 rollup solution, uses
a sparse Merkle tree to represent the state of the layer-2 blockchain. The
account information is recorded in the leaves of the tree. In this paper, we
study the sparse Merkle tree algorithms presented in zkSync Lite, and propose
an efficient batch update algorithm to calculate a new root hash given a list
of account (leaf) operations. Using the construction in zkSync Lite as a
benchmark, our algorithm 1) improves the account update time from
$\mathcal{O}(\log n)$ to $\mathcal{O}(1)$ and 2) reduces the batch update cost
by half using a one-pass traversal. Empirical analysis of real-world block data
shows that our algorithm outperforms the benchmark by at most 14%.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13332" title="Abstract">arXiv:2310.13332</a> [<a href="/pdf/2310.13332" title="Download PDF">pdf</a>, <a href="/format/2310.13332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Democratizing Reasoning Ability: Tailored Learning from Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaohan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Minghui Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haizhen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Feng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) exhibit impressive emergent abilities in natural
language processing, but their democratization is hindered due to huge
computation requirements and closed-source nature. Recent research on advancing
open-source smaller LMs by distilling knowledge from black-box LLMs has
obtained promising results in the instruction-following ability. However, the
reasoning ability which is more challenging to foster, is relatively rarely
explored. In this paper, we propose a tailored learning approach to distill
such reasoning ability to smaller LMs to facilitate the democratization of the
exclusive reasoning ability. In contrast to merely employing LLM as a data
annotator, we exploit the potential of LLM as a reasoning teacher by building
an interactive multi-round learning paradigm. This paradigm enables the student
to expose its deficiencies to the black-box teacher who then can provide
customized training data in return. Further, to exploit the reasoning potential
of the smaller LM, we propose self-reflection learning to motivate the student
to learn from self-made mistakes. The learning from self-reflection and LLM are
all tailored to the student's learning status, thanks to the seamless
integration with the multi-round learning paradigm. Comprehensive experiments
and analysis on mathematical and commonsense reasoning tasks demonstrate the
effectiveness of our method. The code will be available at
https://github.com/Raibows/Learn-to-Reason.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13335" title="Abstract">arXiv:2310.13335</a> [<a href="/pdf/2310.13335" title="Download PDF">pdf</a>, <a href="/format/2310.13335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Intelligent Sensing Surface aided Wireless Powered  Communication Networks: A Sensing-Then-Reflecting Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Cheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+L">Luping Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kun Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper presents a reconfigurable intelligent sensing surface (RISS) that
combines passive and active elements to achieve simultaneous reflection and
direction of arrival (DOA) estimation tasks. By utilizing DOA information from
the RISS instead of conventional channel estimation, the pilot overhead is
reduced and the RISS becomes independent of the hybrid access point (HAP),
enabling efficient operation. Specifically, the RISS autonomously estimates the
DOA of uplink signals from single-antenna users and reflects them using the
HAP's slowly varying DOA information. During downlink transmission, it updates
the HAP's DOA information and designs the reflection phase of energy signals
based on the latest user DOA information. The paper includes a comprehensive
performance analysis, covering system design, protocol details, receiving
performance, and RISS deployment suggestions. We derive a closed-form
expression to analyze system performance under DOA errors, and calculate the
statistical distribution of user received energy using the moment-matching
technique. We provide a recommended transmit power to meet a specified outage
probability and energy threshold. Numerical results demonstrate that the
proposed system outperforms the conventional counterpart by 2.3 dB and 4.7 dB
for Rician factors $\kappa_h=\kappa_G=1$ and $\kappa_h=\kappa_G=10$,
respectively.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13336" title="Abstract">arXiv:2310.13336</a> [<a href="/pdf/2310.13336" title="Download PDF">pdf</a>, <a href="/format/2310.13336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From  Multi-Source Optical Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garioud%2C+A">Anatol Garioud</a>, 
<a href="/search/cs?searchtype=author&query=Gonthier%2C+N">Nicolas Gonthier</a>, 
<a href="/search/cs?searchtype=author&query=Landrieu%2C+L">Loic Landrieu</a>, 
<a href="/search/cs?searchtype=author&query=De+Wit%2C+A">Apolline De Wit</a>, 
<a href="/search/cs?searchtype=author&query=Valette%2C+M">Marion Valette</a>, 
<a href="/search/cs?searchtype=author&query=Poup%C3%A9e%2C+M">Marc Poup&#xe9;e</a>, 
<a href="/search/cs?searchtype=author&query=Giordano%2C+S">S&#xe9;bastien Giordano</a>, 
<a href="/search/cs?searchtype=author&query=Wattrelos%2C+B">Boris Wattrelos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 - Datasets &amp; Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce the French Land cover from Aerospace ImageRy (FLAIR), an
extensive dataset from the French National Institute of Geographical and Forest
Information (IGN) that provides a unique and rich resource for large-scale
geospatial analysis. FLAIR contains high-resolution aerial imagery with a
ground sample distance of 20 cm and over 20 billion individually labeled pixels
for precise land-cover classification. The dataset also integrates temporal and
spectral data from optical satellite time series. FLAIR thus combines data with
varying spatial, spectral, and temporal resolutions across over 817 km2 of
acquisitions representing the full landscape diversity of France. This
diversity makes FLAIR a valuable resource for the development and evaluation of
novel methods for large-scale land-cover semantic segmentation and raises
significant challenges in terms of computer vision, data fusion, and geospatial
analysis. We also provide powerful uni- and multi-sensor baseline models that
can be employed to assess algorithm's performance and for downstream
applications. Through its extent and the quality of its annotation, FLAIR aims
to spur improvements in monitoring and understanding key anthropogenic
development indicators such as urban growth, deforestation, and soil
artificialization. Dataset and codes can be accessed at
https://ignf.github.io/FLAIR/
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13340" title="Abstract">arXiv:2310.13340</a> [<a href="/pdf/2310.13340" title="Download PDF">pdf</a>, <a href="/format/2310.13340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Scale and Multi-Perspective Opinion Summarization with Diverse  Review Subsets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Han Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhihua Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinpeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Opinion summarization is expected to digest larger review sets and provide
summaries from different perspectives. However, most existing solutions are
deficient in epitomizing extensive reviews and offering opinion summaries from
various angles due to the lack of designs for information selection. To this
end, we propose SUBSUMM, a supervised summarization framework for large-scale
multi-perspective opinion summarization. SUBSUMM consists of a review sampling
strategy set and a two-stage training scheme. The sampling strategies take
sentiment orientation and contrastive information value into consideration,
with which the review subsets from different perspectives and quality levels
can be selected. Subsequently, the summarizer is encouraged to learn from the
sub-optimal and optimal subsets successively in order to capitalize on the
massive input. Experimental results on AmaSum and Rotten Tomatoes datasets
demonstrate that SUBSUMM is adept at generating pros, cons, and verdict
summaries from hundreds of input reviews. Furthermore, our in-depth analysis
verifies that the advanced selection of review subsets and the two-stage
training scheme are vital to boosting the summarization performance.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13343" title="Abstract">arXiv:2310.13343</a> [<a href="/pdf/2310.13343" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenges and Contributing Factors in the Utilization of Large Language  Models (LLMs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoliang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+L">Le Chang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yunhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuxuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuxiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dinuo Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the development of large language models (LLMs) like the GPT series,
their widespread use across various application scenarios presents a myriad of
challenges. This review initially explores the issue of domain specificity,
where LLMs may struggle to provide precise answers to specialized questions
within niche fields. The problem of knowledge forgetting arises as these LLMs
might find it hard to balance old and new information. The knowledge repetition
phenomenon reveals that sometimes LLMs might deliver overly mechanized
responses, lacking depth and originality. Furthermore, knowledge illusion
describes situations where LLMs might provide answers that seem insightful but
are actually superficial, while knowledge toxicity focuses on harmful or biased
information outputs. These challenges underscore problems in the training data
and algorithmic design of LLMs. To address these issues, it's suggested to
diversify training data, fine-tune models, enhance transparency and
interpretability, and incorporate ethics and fairness training. Future
technological trends might lean towards iterative methodologies, multimodal
learning, model personalization and customization, and real-time learning and
feedback mechanisms. In conclusion, future LLMs should prioritize fairness,
transparency, and ethics, ensuring they uphold high moral and ethical standards
when serving humanity.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13344" title="Abstract">arXiv:2310.13344</a> [<a href="/pdf/2310.13344" title="Download PDF">pdf</a>, <a href="/format/2310.13344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepFracture: A Generative Approach for Predicting Brittle Fractures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kanai%2C+T">Takashi Kanai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In the realm of brittle fracture animation, generating realistic destruction
animations with physics simulation techniques can be computationally expensive.
Although methods using Voronoi diagrams or pre-fractured patterns work for
real-time applications, they often lack realism in portraying brittle
fractures. This paper introduces a novel learning-based approach for seamlessly
merging realistic brittle fracture animations with rigid-body simulations. Our
method utilizes BEM brittle fracture simulations to create fractured patterns
and collision conditions for a given shape, which serve as training data for
the learning process. To effectively integrate collision conditions and
fractured shapes into a deep learning framework, we introduce the concept of
latent impulse representation and geometrically-segmented signed distance
function (GS-SDF). The latent impulse representation serves as input, capturing
information about impact forces on the shape's surface. Simultaneously, a
GS-SDF is used as the output representation of the fractured shape. To address
the challenge of optimizing multiple fractured pattern targets with a single
latent code, we propose an eight-dimensional latent space based on a normal
distribution code within our latent impulse representation design. This
adaptation effectively transforms our neural network into a generative one. Our
experimental results demonstrate that our approach can generate significantly
more detailed brittle fractures compared to existing techniques, all while
maintaining commendable computational efficiency during run-time.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13345" title="Abstract">arXiv:2310.13345</a> [<a href="/pdf/2310.13345" title="Download PDF">pdf</a>, <a href="/format/2310.13345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An LLM can Fool Itself: A Prompt-Based Adversarial Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xilie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+K">Keyi Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Lizhen Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kankanhalli%2C+M">Mohan Kankanhalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The wide-ranging applications of large language models (LLMs), especially in
safety-critical domains, necessitate the proper evaluation of the LLM's
adversarial robustness. This paper proposes an efficient tool to audit the
LLM's adversarial robustness via a prompt-based adversarial attack
(PromptAttack). PromptAttack converts adversarial textual attacks into an
attack prompt that can cause the victim LLM to output the adversarial sample to
fool itself. The attack prompt is composed of three important components: (1)
original input (OI) including the original sample and its ground-truth label,
(2) attack objective (AO) illustrating a task description of generating a new
sample that can fool itself without changing the semantic meaning, and (3)
attack guidance (AG) containing the perturbation instructions to guide the LLM
on how to complete the task by perturbing the original sample at character,
word, and sentence levels, respectively. Besides, we use a fidelity filter to
ensure that PromptAttack maintains the original semantic meanings of the
adversarial examples. Further, we enhance the attack power of PromptAttack by
ensembling adversarial examples at different perturbation levels. Comprehensive
empirical results using Llama2 and GPT-3.5 validate that PromptAttack
consistently yields a much higher attack success rate compared to AdvGLUE and
AdvGLUE++. Interesting findings include that a simple emoji can easily mislead
GPT-3.5 to make wrong predictions.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13346" title="Abstract">arXiv:2310.13346</a> [<a href="/pdf/2310.13346" title="Download PDF">pdf</a>, <a href="/format/2310.13346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized approaches for autonomous vehicles coordination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gherardini%2C+L">Luca Gherardini</a>, 
<a href="/search/cs?searchtype=author&query=Cabri%2C+G">Giacomo Cabri</a>, 
<a href="/search/cs?searchtype=author&query=Montangero%2C+M">Manuela Montangero</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Internet Technology Letters. 2022;e398
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">The coordination of autonomous vehicles is an open field that is addressed by
different researches comprising many different techniques. In this paper we
focus on decentralized approaches able to provide adaptability to different
infrastructural and traffic conditions. We formalize an Emergent Behavior
Approach that, as per our knowledge, has never been performed for this purpose,
and a Decentralized Auction approach. We compare them against existing
centralized negotiation approaches based on auctions and we determine under
which conditions each approach is preferable to the others.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13347" title="Abstract">arXiv:2310.13347</a> [<a href="/pdf/2310.13347" title="Download PDF">pdf</a>, <a href="/format/2310.13347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NurViD: A Large Expert-Level Video Database for Nursing Procedure  Activity Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Ming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Siyuan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">Don Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Q">Qingli Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+P">Peng Xia</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Wei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+P">Peibo Duan</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+L">Lie Ju</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Z">Zongyuan Ge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The application of deep learning to nursing procedure activity understanding
has the potential to greatly enhance the quality and safety of nurse-patient
interactions. By utilizing the technique, we can facilitate training and
education, improve quality control, and enable operational compliance
monitoring. However, the development of automatic recognition systems in this
field is currently hindered by the scarcity of appropriately labeled datasets.
The existing video datasets pose several limitations: 1) these datasets are
small-scale in size to support comprehensive investigations of nursing
activity; 2) they primarily focus on single procedures, lacking expert-level
annotations for various nursing procedures and action steps; and 3) they lack
temporally localized annotations, which prevents the effective localization of
targeted actions within longer video sequences. To mitigate these limitations,
we propose NurViD, a large video dataset with expert-level annotation for
nursing procedure activity understanding. NurViD consists of over 1.5k videos
totaling 144 hours, making it approximately four times longer than the existing
largest nursing activity datasets. Notably, it encompasses 51 distinct nursing
procedures and 177 action steps, providing a much more comprehensive coverage
compared to existing datasets that primarily focus on limited procedures. To
evaluate the efficacy of current deep learning methods on nursing activity
understanding, we establish three benchmarks on NurViD: procedure recognition
on untrimmed videos, procedure and action recognition on trimmed videos, and
action detection. Our benchmark and code will be available at
\url{https://github.com/minghu0830/NurViD-benchmark}.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13348" title="Abstract">arXiv:2310.13348</a> [<a href="/pdf/2310.13348" title="Download PDF">pdf</a>, <a href="/format/2310.13348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Cognitive Plausibility of Subword Tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beinborn%2C+L">Lisa Beinborn</a>, 
<a href="/search/cs?searchtype=author&query=Pinter%2C+Y">Yuval Pinter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Subword tokenization has become the de-facto standard for tokenization,
although comparative evaluations of subword vocabulary quality across languages
are scarce. Existing evaluation studies focus on the effect of a tokenization
algorithm on the performance in downstream tasks, or on engineering criteria
such as the compression rate. We present a new evaluation paradigm that focuses
on the cognitive plausibility of subword tokenization. We analyze the
correlation of the tokenizer output with the response time and accuracy of
human performance on a lexical decision task. We compare three tokenization
algorithms across several languages and vocabulary sizes. Our results indicate
that the UnigramLM algorithm yields less cognitively plausible tokenization
behavior and a worse coverage of derivational morphemes, in contrast with prior
work.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13350" title="Abstract">arXiv:2310.13350</a> [<a href="/pdf/2310.13350" title="Download PDF">pdf</a>, <a href="/format/2310.13350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EarlyBird: Early-Fusion for Multi-View Tracking in the Bird&#x27;s Eye View
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teepe%2C+T">Torben Teepe</a>, 
<a href="/search/cs?searchtype=author&query=Wolters%2C+P">Philipp Wolters</a>, 
<a href="/search/cs?searchtype=author&query=Gilg%2C+J">Johannes Gilg</a>, 
<a href="/search/cs?searchtype=author&query=Herzog%2C+F">Fabian Herzog</a>, 
<a href="/search/cs?searchtype=author&query=Rigoll%2C+G">Gerhard Rigoll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-view aggregation promises to overcome the occlusion and missed
detection challenge in multi-object detection and tracking. Recent approaches
in multi-view detection and 3D object detection made a huge performance leap by
projecting all views to the ground plane and performing the detection in the
Bird's Eye View (BEV). In this paper, we investigate if tracking in the BEV can
also bring the next performance breakthrough in Multi-Target Multi-Camera
(MTMC) tracking. Most current approaches in multi-view tracking perform the
detection and tracking task in each view and use graph-based approaches to
perform the association of the pedestrian across each view. This spatial
association is already solved by detecting each pedestrian once in the BEV,
leaving only the problem of temporal association. For the temporal association,
we show how to learn strong Re-Identification (re-ID) features for each
detection. The results show that early-fusion in the BEV achieves high accuracy
for both detection and tracking. EarlyBird outperforms the state-of-the-art
methods and improves the current state-of-the-art on Wildtrack by +4.6 MOTA and
+5.6 IDF1.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13355" title="Abstract">arXiv:2310.13355</a> [<a href="/pdf/2310.13355" title="Download PDF">pdf</a>, <a href="/format/2310.13355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SILC: Improving Vision Language Pretraining with Self-Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naeem%2C+M+F">Muhammad Ferjad Naeem</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Yongqin Xian</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+X">Xiaohua Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Hoyer%2C+L">Lukas Hoyer</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>, 
<a href="/search/cs?searchtype=author&query=Tombari%2C+F">Federico Tombari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image-Text pretraining on web-scale image caption dataset has become the
default recipe for open vocabulary classification and retrieval models thanks
to the success of CLIP and its variants. Several works have also used CLIP
features for dense prediction tasks and have shown the emergence of open-set
abilities. However, the contrastive objective only focuses on image-text
alignment and does not incentivise image feature learning for dense prediction
tasks. In this work, we propose the simple addition of local-to-global
correspondence learning by self-distillation as an additional objective for
contrastive pre-training to propose SILC. We show that distilling local image
features from an exponential moving average (EMA) teacher model significantly
improves model performance on several computer vision tasks including
classification, retrieval, and especially segmentation. We further show that
SILC scales better with the same training duration compared to the baselines.
Our model SILC sets a new state of the art for zero-shot classification, few
shot classification, image and text retrieval, zero-shot segmentation, and open
vocabulary segmentation.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13356" title="Abstract">arXiv:2310.13356</a> [<a href="/pdf/2310.13356" title="Download PDF">pdf</a>, <a href="/format/2310.13356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seoha Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+J">Jeongmin Bae</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+Y">Youngsik Yun</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hahyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Bang%2C+G">Gun Bang</a>, 
<a href="/search/cs?searchtype=author&query=Uh%2C+Y">Youngjung Uh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Project page: \href{<a href="https://seoha-kim.github.io/sync-nerf">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advancements in 4D scene reconstruction using neural radiance fields
(NeRF) have demonstrated the ability to represent dynamic scenes from
multi-view videos. However, they fail to reconstruct the dynamic scenes and
struggle to fit even the training views in unsynchronized settings. It happens
because they employ a single latent embedding for a frame while the multi-view
images at the frame were actually captured at different moments. To address
this limitation, we introduce time offsets for individual unsynchronized videos
and jointly optimize the offsets with NeRF. By design, our method is applicable
for various baselines and improves them with large margins. Furthermore,
finding the offsets naturally works as synchronizing the videos without manual
effort. Experiments are conducted on the common Plenoptic Video Dataset and a
newly built Unsynchronized Dynamic Blender Dataset to verify the performance of
our method. Project page: https://seoha-kim.github.io/sync-nerf
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13359" title="Abstract">arXiv:2310.13359</a> [<a href="/pdf/2310.13359" title="Download PDF">pdf</a>, <a href="/format/2310.13359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electrical Fault Localisation Over a Distributed Parameter Transmission  Line
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Selvaratnam%2C+D">Daniel Selvaratnam</a>, 
<a href="/search/eess?searchtype=author&query=Das%2C+A">Amritam Das</a>, 
<a href="/search/eess?searchtype=author&query=Sandberg%2C+H">Henrik Sandberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented at CDC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Motivated by the need to localise faults along electrical power lines, this
paper adopts a frequency-domain approach to parameter estimation for an
infinite-dimensional linear dynamical system with one spatial variable. Since
the time of the fault is unknown, and voltages and currents are measured at
only one end of the line, distance information must be extracted from the
post-fault transients. To properly account for high-frequency transient
behaviour, the line dynamics is modelled directly by the Telegrapher's
equation, rather than the more commonly used lumped-parameter approximations.
First, the governing equations are non-dimensionalised to avoid
ill-conditioning. A closed-form expression for the transfer function is then
derived. Finally, nonlinear least-squares optimisation is employed to search
for the fault location. Requirements on fault bandwidth, sensor bandwidth and
simulation time-step are also presented. The result is a novel end-to-end
algorithm for data generation and fault localisation, the effectiveness of
which is demonstrated via simulation.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13361" title="Abstract">arXiv:2310.13361</a> [<a href="/pdf/2310.13361" title="Download PDF">pdf</a>, <a href="/format/2310.13361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Gap between Synthetic and Authentic Images for Multimodal  Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wenyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Q">Qingkai Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Multimodal machine translation (MMT) simultaneously takes the source sentence
and a relevant image as input for translation. Since there is no paired image
available for the input sentence in most cases, recent studies suggest
utilizing powerful text-to-image generation models to provide image inputs.
Nevertheless, synthetic images generated by these models often follow different
distributions compared to authentic images. Consequently, using authentic
images for training and synthetic images for inference can introduce a
distribution shift, resulting in performance degradation during inference. To
tackle this challenge, in this paper, we feed synthetic and authentic images to
the MMT model, respectively. Then we minimize the gap between the synthetic and
authentic images by drawing close the input image representations of the
Transformer Encoder and the output distributions of the Transformer Decoder.
Therefore, we mitigate the distribution disparity introduced by the synthetic
images during inference, thereby freeing the authentic images from the
inference process.Experimental results show that our approach achieves
state-of-the-art performance on the Multi30K En-De and En-Fr datasets, while
remaining independent of authentic images during inference.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13362" title="Abstract">arXiv:2310.13362</a> [<a href="/pdf/2310.13362" title="Download PDF">pdf</a>, <a href="/format/2310.13362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards General Error Diagnosis via Behavioral Testing in Machine  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junjie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lemao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 2 figures, accepted by Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">Behavioral testing offers a crucial means of diagnosing linguistic errors and
assessing capabilities of NLP models. However, applying behavioral testing to
machine translation (MT) systems is challenging as it generally requires human
efforts to craft references for evaluating the translation quality of such
systems on newly generated test cases. Existing works in behavioral testing of
MT systems circumvent this by evaluating translation quality without
references, but this restricts diagnosis to specific types of errors, such as
incorrect translation of single numeric or currency words. In order to diagnose
general errors, this paper proposes a new Bilingual Translation Pair Generation
based Behavior Testing (BTPGBT) framework for conducting behavioral testing of
MT systems. The core idea of BTPGBT is to employ a novel bilingual translation
pair generation (BTPG) approach that automates the construction of high-quality
test cases and their pseudoreferences. Experimental results on various MT
systems demonstrate that BTPGBT could provide comprehensive and accurate
behavioral testing results for general error diagnosis, which further leads to
several insightful findings. Our code and data are available at https:
//github.com/wujunjie1998/BTPGBT.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13364" title="Abstract">arXiv:2310.13364</a> [<a href="/pdf/2310.13364" title="Download PDF">pdf</a>, <a href="/format/2310.13364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dissecting Causal Biases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Binkyt%C4%97%2C+R">R&#x16b;ta Binkyt&#x117;</a>, 
<a href="/search/cs?searchtype=author&query=Zhioua%2C+S">Sami Zhioua</a>, 
<a href="/search/cs?searchtype=author&query=Turki%2C+Y">Yassine Turki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Accurately measuring discrimination in machine learning-based automated
decision systems is required to address the vital issue of fairness between
subpopulations and/or individuals. Any bias in measuring discrimination can
lead to either amplification or underestimation of the true value of
discrimination. This paper focuses on a class of bias originating in the way
training data is generated and/or collected. We call such class causal biases
and use tools from the field of causality to formally define and analyze such
biases. Four sources of bias are considered, namely, confounding, selection,
measurement, and interaction. The main contribution of this paper is to
provide, for each source of bias, a closed-form expression in terms of the
model parameters. This makes it possible to analyze the behavior of each source
of bias, in particular, in which cases they are absent and in which other cases
they are maximized. We hope that the provided characterizations help the
community better understand the sources of bias in machine learning
applications.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13365" title="Abstract">arXiv:2310.13365</a> [<a href="/pdf/2310.13365" title="Download PDF">pdf</a>, <a href="/format/2310.13365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Multi-Subsession Conversational Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yu Ji</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shixuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Chuan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhihua Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Conversational recommendation systems (CRS) could acquire dynamic user
preferences towards desired items through multi-round interactive dialogue.
Previous CRS mainly focuses on the single conversation (subsession) that user
quits after a successful recommendation, neglecting the common scenario where
user has multiple conversations (multi-subsession) over a short period.
Therefore, we propose a novel conversational recommendation scenario named
Multi-Subsession Multi-round Conversational Recommendation (MSMCR), where user
would still resort to CRS after several subsessions and might preserve vague
interests, and system would proactively ask attributes to activate user
interests in the current subsession. To fill the gap in this new CRS scenario,
we devise a novel framework called Multi-Subsession Conversational Recommender
with Activation Attributes (MSCAA). Specifically, we first develop a
context-aware recommendation module, comprehensively modeling user interests
from historical interactions, previous subsessions, and feedback in the current
subsession. Furthermore, an attribute selection policy module is proposed to
learn a flexible strategy for asking appropriate attributes to elicit user
interests. Finally, we design a conversation policy module to manage the above
two modules to decide actions between asking and recommending. Extensive
experiments on four datasets verify the effectiveness of our MSCAA framework
for the MSMCR setting.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13366" title="Abstract">arXiv:2310.13366</a> [<a href="/pdf/2310.13366" title="Download PDF">pdf</a>, <a href="/format/2310.13366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PSGText: Stroke-Guided Scene Text Editing with PSP Module
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liawi%2C+F">Felix Liawi</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y">Yun-Da Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+G">Guan-Lun Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shou-De Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scene Text Editing (STE) aims to substitute text in an image with new desired
text while preserving the background and styles of the original text. However,
present techniques present a notable challenge in the generation of edited text
images that exhibit a high degree of clarity and legibility. This challenge
primarily stems from the inherent diversity found within various text types and
the intricate textures of complex backgrounds. To address this challenge, this
paper introduces a three-stage framework for transferring texts across text
images. Initially, we introduce a text-swapping network that seamlessly
substitutes the original text with the desired replacement. Subsequently, we
incorporate a background inpainting network into our framework. This
specialized network is designed to skillfully reconstruct background images,
effectively addressing the voids left after the removal of the original text.
This process meticulously preserves visual harmony and coherence in the
background. Ultimately, the synthesis of outcomes from the text-swapping
network and the background inpainting network is achieved through a fusion
network, culminating in the creation of the meticulously edited final image. A
demo video is included in the supplementary material.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13367" title="Abstract">arXiv:2310.13367</a> [<a href="/pdf/2310.13367" title="Download PDF">pdf</a>, <a href="/format/2310.13367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VFedMH: Vertical Federated Learning for Training Multi-party  Heterogeneous Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Keke Gai</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liehuang Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Vertical Federated Learning (VFL) has gained increasing attention as a novel
training paradigm that integrates sample alignment and feature union. However,
existing VFL methods face challenges when dealing with heterogeneous local
models among participants, which affects optimization convergence and
generalization. To address this issue, this paper proposes a novel approach
called Vertical Federated learning for training Multi-parties Heterogeneous
models (VFedMH). VFedMH focuses on aggregating the embeddings of each
participant's knowledge instead of intermediate results during forward
propagation. The active party, who possesses labels and features of the sample,
in VFedMH securely aggregates local embeddings to obtain global knowledge
embeddings, and sends them to passive parties. The passive parties, who own
only features of the sample, then utilize the global embeddings to propagate
forward on their local heterogeneous networks. However, the passive party does
not own the labels, so the local model gradient cannot be calculated locally.
To overcome this limitation, the active party assists the passive party in
computing its local heterogeneous model gradients. Then, each participant
trains their local model using the heterogeneous model gradients. The objective
is to minimize the loss value of their respective local heterogeneous models.
Additionally, the paper provides a theoretical analysis of VFedMH's convergence
performance. Extensive experiments are conducted to demonstrate that VFedMH can
simultaneously train multiple heterogeneous models with heterogeneous
optimization and outperform some recent methods in model performance.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13368" title="Abstract">arXiv:2310.13368</a> [<a href="/pdf/2310.13368" title="Download PDF">pdf</a>, <a href="/format/2310.13368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AP Connection Method for Maximizing Throughput Considering User Moving  and Degree of Interference Based on Potential Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kato%2C+Y">Yu Kato</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jiquan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Murase%2C+T">Tutomu Murase</a>, 
<a href="/search/cs?searchtype=author&query=Miyata%2C+S">Sumiko Miyata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 15 figures, journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">For multi-transmission rate environments, access point (AP) connection
methods have been proposed for maximizing system throughput, which is the
throughput of an entire system, on the basis of the cooperative behavior of
users. These methods derive optimal positions for the cooperative behavior of
users, which means that new users move to improve the system throughput when
connecting to an AP. However, the conventional method only considers the
transmission rate of new users and does not consider existing users, even
though it is necessary to consider the transmission rate of all users to
improve system throughput. In addition, these method do not take into account
the frequency of interference between users. In this paper, we propose an AP
connection method which maximizes system throughput by considering the
interference between users and the initial position of all users. In addition,
our proposed method can improve system throughput by about 6% at most compared
to conventional methods.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13369" title="Abstract">arXiv:2310.13369</a> [<a href="/pdf/2310.13369" title="Download PDF">pdf</a>, <a href="/format/2310.13369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SigFormer: Signature Transformers for Deep Hedging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+A">Anh Tong</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen-Tang%2C+T">Thanh Nguyen-Tang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongeun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T">Toan Tran</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaesik Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICAIF 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep hedging is a promising direction in quantitative finance, incorporating
models and techniques from deep learning research. While giving excellent
hedging strategies, models inherently requires careful treatment in designing
architectures for neural networks. To mitigate such difficulties, we introduce
SigFormer, a novel deep learning model that combines the power of path
signatures and transformers to handle sequential data, particularly in cases
with irregularities. Path signatures effectively capture complex data patterns,
while transformers provide superior sequential attention. Our proposed model is
empirically compared to existing methods on synthetic data, showcasing faster
learning and enhanced robustness, especially in the presence of irregular
underlying price data. Additionally, we validate our model performance through
a real-world backtest on hedging the SP 500 index, demonstrating positive
outcomes.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13373" title="Abstract">arXiv:2310.13373</a> [<a href="/pdf/2310.13373" title="Download PDF">pdf</a>, <a href="/format/2310.13373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-view 3D reconstruction via inverse procedural modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garifullin%2C+A">Albert Garifullin</a>, 
<a href="/search/cs?searchtype=author&query=Maiorov%2C+N">Nikolay Maiorov</a>, 
<a href="/search/cs?searchtype=author&query=Frolov%2C+V">Vladimir Frolov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose an approach to 3D reconstruction via inverse procedural modeling
and investigate two variants of this approach. The first option consists in the
fitting set of input parameters using a genetic algorithm. We demonstrate the
results of our work on tree models, complex objects, with the reconstruction of
which most existing methods cannot handle. The second option allows us to
significantly improve the precision by using gradients within memetic
algorithm, differentiable rendering and also differentiable procedural
generators. In our work we see 2 main contributions. First, we propose a method
to join differentiable rendering and inverse procedural modeling. This gives us
an opportunity to reconstruct 3D model more accurately than existing approaches
when a small number of input images are available (even for single image).
Second, we join both differentiable and non-differentiable procedural
generators in a single framework which allow us to apply inverse procedural
modeling to fairly complex generators: when gradient is available,
reconstructions is precise, when gradient is not available, reconstruction is
approximate, but always high quality without visual artifacts.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13375" title="Abstract">arXiv:2310.13375</a> [<a href="/pdf/2310.13375" title="Download PDF">pdf</a>, <a href="/format/2310.13375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Improved Artificial Fish Swarm Algorithm for Solving the Problem of  Investigation Path Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weiwen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xuan Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Informationization is a prevailing trend in today's world. The increasing
demand for information in decision-making processes poses significant
challenges for investigation activities, particularly in terms of effectively
allocating limited resources to plan investigation programs. This paper
addresses the investigation path planning problem by formulating it as a
multi-traveling salesman problem (MTSP). Our objective is to minimize costs,
and to achieve this, we propose a chaotic artificial fish swarm algorithm based
on multiple population differential evolution (DE-CAFSA). To overcome the
limitations of the artificial fish swarm algorithm, such as low optimization
accuracy and the inability to consider global and local information, we
incorporate adaptive field of view and step size adjustments, replace random
behavior with the 2-opt operation, and introduce chaos theory and sub-optimal
solutions to enhance optimization accuracy and search performance.
Additionally, we integrate the differential evolution algorithm to create a
hybrid algorithm that leverages the complementary advantages of both
approaches. Experimental results demonstrate that DE-CAFSA outperforms other
algorithms on various public datasets of different sizes, as well as showcasing
excellent performance on the examples proposed in this study.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13377" title="Abstract">arXiv:2310.13377</a> [<a href="/pdf/2310.13377" title="Download PDF">pdf</a>, <a href="/format/2310.13377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Human-Robot Mutual Learning System with Affect-Grounded Language  Acquisition and Differential Outcomes Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Markelius%2C+A">Alva Markelius</a>, 
<a href="/search/cs?searchtype=author&query=Sj%C3%B6berg%2C+S">Sofia Sj&#xf6;berg</a>, 
<a href="/search/cs?searchtype=author&query=Lemhauori%2C+Z">Zakaria Lemhauori</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+L">Laura Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Bergstr%C3%B6m%2C+M">Martin Bergstr&#xf6;m</a>, 
<a href="/search/cs?searchtype=author&query=Lowe%2C+R">Robert Lowe</a>, 
<a href="/search/cs?searchtype=author&query=Ca%C3%B1amero%2C+L">Lola Ca&#xf1;amero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint: This is the submitted version of a paper to be presented at The Proceedings of the 15th International Conference on Social Robotics (ICSR 2023). Please cite the official publication once it is available
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This paper presents a novel human-robot interaction setup for robot and human
learning of symbolic language for identifying robot homeostatic needs. The
robot and human learn to use and respond to the same language symbols that
convey homeostatic needs and the stimuli that satisfy the homeostatic needs,
respectively. We adopted a differential outcomes training (DOT) protocol
whereby the robot provides feedback specific (differential) to its internal
needs (e.g. `hunger') when satisfied by the correct stimulus (e.g. cookie). We
found evidence that DOT can enhance the human's learning efficiency, which in
turn enables more efficient robot language acquisition. The robot used in the
study has a vocabulary similar to that of a human infant in the linguistic
``babbling'' phase. The robot software architecture is built upon a model for
affect-grounded language acquisition where the robot associates vocabulary with
internal needs (hunger, thirst, curiosity) through interactions with the human.
The paper presents the results of an initial pilot study conducted with the
interactive setup, which reveal that the robot's language acquisition achieves
higher convergence rate in the DOT condition compared to the non-DOT control
condition. Additionally, participants reported positive affective experiences,
feeling of being in control, and an empathetic connection with the robot. This
mutual learning (teacher-student learning) approach offers a potential
contribution of facilitating cognitive interventions with DOT (e.g. for people
with dementia) through increased therapy adherence as a result of engaging
humans more in training tasks by taking an active teaching-learning role. The
homeostatic motivational grounding of the robot's language acquisition has
potential to contribute to more ecologically valid and social
(collaborative/nurturing) interactions with robots.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13378" title="Abstract">arXiv:2310.13378</a> [<a href="/pdf/2310.13378" title="Download PDF">pdf</a>, <a href="/format/2310.13378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ScalableMap: Scalable Map Learning for Online Long-Range Vectorized HD  Map Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zizhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shengfu Xia</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+J">Jizhang Sang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a novel end-to-end pipeline for online long-range vectorized
high-definition (HD) map construction using on-board camera sensors. The
vectorized representation of HD maps, employing polylines and polygons to
represent map elements, is widely used by downstream tasks. However, previous
schemes designed with reference to dynamic object detection overlook the
structural constraints within linear map elements, resulting in performance
degradation in long-range scenarios. In this paper, we exploit the properties
of map elements to improve the performance of map construction. We extract more
accurate bird's eye view (BEV) features guided by their linear structure, and
then propose a hierarchical sparse map representation to further leverage the
scalability of vectorized map elements and design a progressive decoding
mechanism and a supervision strategy based on this representation. Our
approach, ScalableMap, demonstrates superior performance on the nuScenes
dataset, especially in long-range scenarios, surpassing previous
state-of-the-art model by 6.5 mAP while achieving 18.3 FPS. Code is available
at https://github.com/jingy1yu/ScalableMap.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13379" title="Abstract">arXiv:2310.13379</a> [<a href="/pdf/2310.13379" title="Download PDF">pdf</a>, <a href="/format/2310.13379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher order accurate mass lumping for explicit isogeometric methods  based on approximate dual basis functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hiemstra%2C+R">Rene Hiemstra</a>, 
<a href="/search/math?searchtype=author&query=Nguyen%2C+T">Thi-Hoa Nguyen</a>, 
<a href="/search/math?searchtype=author&query=Eisentrager%2C+S">Sascha Eisentrager</a>, 
<a href="/search/math?searchtype=author&query=Dornisch%2C+W">Wolfgang Dornisch</a>, 
<a href="/search/math?searchtype=author&query=Schillinger%2C+D">Dominik Schillinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper introduces a mathematical framework for explicit structural
dynamics, employing approximate dual functionals and rowsum mass lumping. We
demonstrate that the approach may be interpreted as a Petrov-Galerkin method
that utilizes rowsum mass lumping or as a Galerkin method with a customized
higher-order accurate mass matrix. Unlike prior work, our method correctly
incorporates Dirichlet boundary conditions while preserving higher order
accuracy. The mathematical analysis is substantiated by spectral analysis and a
two-dimensional linear benchmark that involves a non-linear geometric mapping.
Our results reveal that our approach achieves accuracy and robustness
comparable to a traditional Galerkin method employing the consistent mass
formulation, while retaining the explicit nature of the lumped mass
formulation.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13380" title="Abstract">arXiv:2310.13380</a> [<a href="/pdf/2310.13380" title="Download PDF">pdf</a>, <a href="/format/2310.13380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pei Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+Y">Yutao Mou</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaoshuai Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yanan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Yunsen Xian</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xunliang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Detecting out-of-domain (OOD) intents from user queries is essential for a
task-oriented dialogue system. Previous OOD detection studies generally work on
the assumption that plenty of labeled IND intents exist. In this paper, we
focus on a more practical few-shot OOD setting where there are only a few
labeled IND data and massive unlabeled mixed data that may belong to IND or
OOD. The new scenario carries two key challenges: learning discriminative
representations using limited IND data and leveraging unlabeled mixed data.
Therefore, we propose an adaptive prototypical pseudo-labeling (APP) method for
few-shot OOD detection, including a prototypical OOD detection framework
(ProtoOOD) to facilitate low-resource OOD detection using limited IND data, and
an adaptive pseudo-labeling method to produce high-quality pseudo OOD\&amp;IND
labels. Extensive experiments and analysis demonstrate the effectiveness of our
method for few-shot OOD detection.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13381" title="Abstract">arXiv:2310.13381</a> [<a href="/pdf/2310.13381" title="Download PDF">pdf</a>, <a href="/format/2310.13381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerated sparse Kernel Spectral Clustering for large scale data  clustering problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Novak%2C+M">Mihaly Novak</a>, 
<a href="/search/cs?searchtype=author&query=Langone%2C+R">Rocco Langone</a>, 
<a href="/search/cs?searchtype=author&query=Alzate%2C+C">Carlos Alzate</a>, 
<a href="/search/cs?searchtype=author&query=Suykens%2C+J">Johan Suykens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">An improved version of the sparse multiway kernel spectral clustering (KSC)
is presented in this brief. The original algorithm is derived from weighted
kernel principal component (KPCA) analysis formulated within the primal-dual
least-squares support vector machine (LS-SVM) framework. Sparsity is achieved
then by the combination of the incomplete Cholesky decomposition (ICD) based
low rank approximation of the kernel matrix with the so called reduced set
method. The original ICD based sparse KSC algorithm was reported to be
computationally far too demanding, especially when applied on large scale data
clustering problems that actually it was designed for, which has prevented to
gain more than simply theoretical relevance so far. This is altered by the
modifications reported in this brief that drastically improve the computational
characteristics. Solving the alternative, symmetrized version of the
computationally most demanding core eigenvalue problem eliminates the necessity
of forming and SVD of large matrices during the model construction. This
results in solving clustering problems now within seconds that were reported to
require hours without altering the results. Furthermore, sparsity is also
improved significantly, leading to more compact model representation,
increasing further not only the computational efficiency but also the
descriptive power. These transform the original, only theoretically relevant
ICD based sparse KSC algorithm applicable for large scale practical clustering
problems. Theoretical results and improvements are demonstrated by
computational experiments on carefully selected synthetic data as well as on
real life problems such as image segmentation.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13384" title="Abstract">arXiv:2310.13384</a> [<a href="/pdf/2310.13384" title="Download PDF">pdf</a>, <a href="/format/2310.13384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Salted Inference: Enhancing Privacy while Maintaining Efficiency of  Split Inference in Mobile Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malekzadeh%2C+M">Mohammad Malekzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kawsar%2C+F">Fahim Kawsar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 Pages, 2 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Split inference partitions a deep neural network (DNN) to run the early part
at the edge and the later part in the cloud. This meets two key requirements
for on-device machine learning: input privacy and compute efficiency. Still, an
open question in split inference is output privacy, given that the output of a
DNN is visible to the cloud. While encrypted computing can protect output
privacy, it mandates extensive computation and communication resources. In this
paper, we introduce "Salted DNNs": a novel method that lets clients control the
semantic interpretation of DNN output at inference time while maintaining
accuracy and efficiency very close to that of a standard DNN. Experimental
evaluations conducted on both image and sensor data show that Salted DNNs
achieve classification accuracy very close to standard DNNs, particularly when
the salted layer is positioned within the early part to meet the requirements
of split inference. Our method is general and can be applied to various DNNs.
We open-source our code and results, as a benchmark for future studies.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13385" title="Abstract">arXiv:2310.13385</a> [<a href="/pdf/2310.13385" title="Download PDF">pdf</a>, <a href="/format/2310.13385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuna: Instruction Tuning using Feedback from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, code and data are available at <a href="https://github.com/microsoft/LMOps">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Instruction tuning of open-source large language models (LLMs) like LLaMA,
using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4,
has proven to be a cost-effective way to align model behaviors with human
preferences. However, the instruction-tuned model has only seen one response
per instruction, lacking the knowledge of potentially better responses. In this
paper, we propose finetuning an instruction-tuned LLM using our novel
\textit{probabilistic ranking} and \textit{contextual ranking} approaches to
increase the likelihood of generating better responses. Probabilistic ranking
enables the instruction-tuned model to inherit the relative rankings of
high-quality and low-quality responses from the teacher LLM. On the other hand,
learning with contextual ranking allows the model to refine its own response
distribution using the contextual understanding ability of stronger LLMs.
Furthermore, we apply probabilistic ranking and contextual ranking sequentially
to the instruction-tuned LLM. The resulting model, which we call \textbf{Tuna},
consistently improves the performance on Super Natural Instructions (119 test
tasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results
than several strong reinforcement learning baselines. Our code and data are
available at \url{ https://github.com/microsoft/LMOps}.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13388" title="Abstract">arXiv:2310.13388</a> [<a href="/pdf/2310.13388" title="Download PDF">pdf</a>, <a href="/format/2310.13388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Music Augmentation and Denoising For Peak-Based Audio Fingerprinting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akesbi%2C+K">Kamil Akesbi</a>, 
<a href="/search/cs?searchtype=author&query=Desblancs%2C+D">Dorian Desblancs</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+B">Benjamin Martin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Audio fingerprinting is a well-established solution for song identification
from short recording excerpts. Popular methods rely on the extraction of sparse
representations, generally spectral peaks, and have proven to be accurate,
fast, and scalable to large collections. However, real-world applications of
audio identification often happen in noisy environments, which can cause these
systems to fail. In this work, we tackle this problem by introducing and
releasing a new audio augmentation pipeline that adds noise to music snippets
in a realistic way, by stochastically mimicking real-world scenarios. We then
propose and release a deep learning model that removes noisy components from
spectrograms in order to improve peak-based fingerprinting systems' accuracy.
We show that the addition of our model improves the identification performance
of commonly used audio fingerprinting systems, even under noisy conditions.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13389" title="Abstract">arXiv:2310.13389</a> [<a href="/pdf/2310.13389" title="Download PDF">pdf</a>, <a href="/format/2310.13389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Effect of Clock Frequency on Voltage and Electromagnetic Fault  Injection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koffas%2C+S">Stefanos Koffas</a>, 
<a href="/search/cs?searchtype=author&query=Vadnala%2C+P+K">Praveen Kumar Vadnala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in AIHWS workshop held for Applied Cryptography and Network Security Conference (ACNS 2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">We investigate the influence of clock frequency on the success rate of a
fault injection attack. In particular, we examine the success rate of voltage
and electromagnetic fault attacks for varying clock frequencies. Using three
different tests that cover different components of a System-on-Chip, we perform
fault injection while its CPU operates at different clock frequencies. Our
results show that the attack's success rate increases with an increase in clock
frequency for both voltage and EM fault injection attacks. As the technology
advances push the clock frequency further, these results can help assess the
impact of fault injection attacks more accurately and develop appropriate
countermeasures to address them.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13391" title="Abstract">arXiv:2310.13391</a> [<a href="/pdf/2310.13391" title="Download PDF">pdf</a>, <a href="/format/2310.13391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Successor Representations with Distributed Hebbian Temporal  Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dzhivelikian%2C+E">Evgenii Dzhivelikian</a>, 
<a href="/search/cs?searchtype=author&query=Kuderov%2C+P">Petr Kuderov</a>, 
<a href="/search/cs?searchtype=author&query=Panov%2C+A+I">Aleksandr I. Panov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">This paper presents a novel approach to address the challenge of online
hidden representation learning for decision-making under uncertainty in
non-stationary, partially observable environments. The proposed algorithm,
Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism
and a multicomponent neuron model. DHTM aims to capture sequential data
relationships and make cumulative predictions about future observations,
forming Successor Representation (SR). Inspired by neurophysiological models of
the neocortex, the algorithm utilizes distributed representations, sparse
transition matrices, and local Hebbian-like learning rules to overcome the
instability and slow learning process of traditional temporal memory algorithms
like RNN and HMM. Experimental results demonstrate that DHTM outperforms
classical LSTM and performs comparably to more advanced RNN-like algorithms,
speeding up Temporal Difference learning for SR in changing environments.
Additionally, we compare the SRs produced by DHTM to another biologically
inspired HMM-like algorithm, CSCG. Our findings suggest that DHTM is a
promising approach for addressing the challenges of online hidden
representation learning in dynamic environments.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13394" title="Abstract">arXiv:2310.13394</a> [<a href="/pdf/2310.13394" title="Download PDF">pdf</a>, <a href="/format/2310.13394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> POSQA: Probe the World Models of LLMs with Size Comparisons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+C">Chang Shu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiuzhou Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fangyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shareghi%2C+E">Ehsan Shareghi</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Embodied language comprehension emphasizes that language understanding is not
solely a matter of mental processing in the brain but also involves
interactions with the physical and social environment. With the explosive
growth of Large Language Models (LLMs) and their already ubiquitous presence in
our daily lives, it is becoming increasingly necessary to verify their
real-world understanding. Inspired by cognitive theories, we propose POSQA: a
Physical Object Size Question Answering dataset with simple size comparison
questions to examine the extremity and analyze the potential mechanisms of the
embodied comprehension of the latest LLMs.
<br />We show that even the largest LLMs today perform poorly under the zero-shot
setting. We then push their limits with advanced prompting techniques and
external knowledge augmentation. Furthermore, we investigate whether their
real-world comprehension primarily derives from contextual information or
internal weights and analyse the impact of prompt formats and report bias of
different objects. Our results show that real-world understanding that LLMs
shaped from textual data can be vulnerable to deception and confusion by the
surface form of prompts, which makes it less aligned with human behaviours.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13395" title="Abstract">arXiv:2310.13395</a> [<a href="/pdf/2310.13395" title="Download PDF">pdf</a>, <a href="/format/2310.13395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cache me if you Can: an Online Cost-aware Teacher-Student framework to  Reduce the Calls to Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stogiannidis%2C+I">Ilias Stogiannidis</a>, 
<a href="/search/cs?searchtype=author&query=Vassos%2C+S">Stavros Vassos</a>, 
<a href="/search/cs?searchtype=author&query=Malakasiotis%2C+P">Prodromos Malakasiotis</a>, 
<a href="/search/cs?searchtype=author&query=Androutsopoulos%2C+I">Ion Androutsopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Short paper (5 pages), accepted at Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Prompting Large Language Models (LLMs) performs impressively in zero- and
few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot
afford the cost of creating large task-specific training datasets, but also the
cost of pretraining their own LLMs, are increasingly turning to third-party
services that allow them to prompt LLMs. However, such services currently
require a payment per call, which becomes a significant operating expense
(OpEx). Furthermore, customer inputs are often very similar over time, hence
SMEs end-up prompting LLMs with very similar instances. We propose a framework
that allows reducing the calls to LLMs by caching previous LLM responses and
using them to train a local inexpensive model on the SME side. The framework
includes criteria for deciding when to trust the local model or call the LLM,
and a methodology to tune the criteria and measure the tradeoff between
performance and cost. For experimental purposes, we instantiate our framework
with two LLMs, GPT-3.5 or GPT-4, and two inexpensive students, a k-NN
classifier or a Multi-Layer Perceptron, using two common business tasks, intent
recognition and sentiment analysis. Experimental results indicate that
significant OpEx savings can be obtained with only slightly lower performance.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13396" title="Abstract">arXiv:2310.13396</a> [<a href="/pdf/2310.13396" title="Download PDF">pdf</a>, <a href="/format/2310.13396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RL-X: A Deep Reinforcement Learning Library (not only) for RoboCup
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bohlinger%2C+N">Nico Bohlinger</a>, 
<a href="/search/cs?searchtype=author&query=Dorer%2C+K">Klaus Dorer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents the new Deep Reinforcement Learning (DRL) library RL-X
and its application to the RoboCup Soccer Simulation 3D League and classic DRL
benchmarks. RL-X provides a flexible and easy-to-extend codebase with
self-contained single directory algorithms. Through the fast JAX-based
implementations, RL-X can reach up to 4.5x speedups compared to well-known
frameworks like Stable-Baselines3.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13397" title="Abstract">arXiv:2310.13397</a> [<a href="/pdf/2310.13397" title="Download PDF">pdf</a>, <a href="/format/2310.13397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Deep Weight Space Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Navon%2C+A">Aviv Navon</a>, 
<a href="/search/cs?searchtype=author&query=Shamsian%2C+A">Aviv Shamsian</a>, 
<a href="/search/cs?searchtype=author&query=Fetaya%2C+E">Ethan Fetaya</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>, 
<a href="/search/cs?searchtype=author&query=Dym%2C+N">Nadav Dym</a>, 
<a href="/search/cs?searchtype=author&query=Maron%2C+H">Haggai Maron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Permutation symmetries of deep networks make simple operations like model
averaging and similarity estimation challenging. In many cases, aligning the
weights of the networks, i.e., finding optimal permutations between their
weights, is necessary. More generally, weight alignment is essential for a wide
range of applications, from model merging, through exploring the optimization
landscape of deep neural networks, to defining meaningful distance functions
between neural networks. Unfortunately, weight alignment is an NP-hard problem.
Prior research has mainly focused on solving relaxed versions of the alignment
problem, leading to either time-consuming methods or sub-optimal solutions. To
accelerate the alignment process and improve its quality, we propose a novel
framework aimed at learning to solve the weight alignment problem, which we
name Deep-Align. To that end, we first demonstrate that weight alignment
adheres to two fundamental symmetries and then, propose a deep architecture
that respects these symmetries. Notably, our framework does not require any
labeled data. We provide a theoretical analysis of our approach and evaluate
Deep-Align on several types of network architectures and learning setups. Our
experimental results indicate that a feed-forward pass with Deep-Align produces
better or equivalent alignments compared to those produced by current
optimization algorithms. Additionally, our alignments can be used as an
initialization for other methods to gain even better solutions with a
significant speedup in convergence.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13398" title="Abstract">arXiv:2310.13398</a> [<a href="/pdf/2310.13398" title="Download PDF">pdf</a>, <a href="/format/2310.13398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yijie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+L">Likun Cai</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xianhui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Z">Zhongxue Gan</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+X">Xiangyang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenchao Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The source code will be released at <a href="https://github.com/Fudan-ProjectTitan/OpenAnnotate3D">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the era of big data and large models, automatic annotating functions for
multi-modal data are of great significance for real-world AI-driven
applications, such as autonomous driving and embodied AI. Unlike traditional
closed-set annotation, open-vocabulary annotation is essential to achieve
human-level cognition capability. However, there are few open-vocabulary
auto-labeling systems for multi-modal 3D data. In this paper, we introduce
OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can
automatically generate 2D masks, 3D masks, and 3D bounding box annotations for
vision and point cloud data. Our system integrates the chain-of-thought
capabilities of Large Language Models (LLMs) and the cross-modality
capabilities of vision-language models (VLMs). To the best of our knowledge,
OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal
3D auto-labeling. We conduct comprehensive evaluations on both public and
in-house real-world datasets, which demonstrate that the system significantly
improves annotation efficiency compared to manual annotation while providing
accurate open-vocabulary auto-annotating results.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13403" title="Abstract">arXiv:2310.13403</a> [<a href="/pdf/2310.13403" title="Download PDF">pdf</a>, <a href="/format/2310.13403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BRFL: A Blockchain-based Byzantine-Robust Federated Learning Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+C">Chunhe Xia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianbo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">With the increasing importance of machine learning, the privacy and security
of training data have become critical. Federated learning, which stores data in
distributed nodes and shares only model parameters, has gained significant
attention for addressing this concern. However, a challenge arises in federated
learning due to the Byzantine Attack Problem, where malicious local models can
compromise the global model's performance during aggregation. This article
proposes the Blockchain-based Byzantine-Robust Federated Learning (BRLF) model
that combines federated learning with blockchain technology. This integration
enables traceability of malicious models and provides incentives for locally
trained clients. Our approach involves selecting the aggregation node based on
Pearson's correlation coefficient, and we perform spectral clustering and
calculate the average gradient within each cluster, validating its accuracy
using local dataset of the aggregation nodes. Experimental results on public
datasets demonstrate the superior byzantine robustness of our secure
aggregation algorithm compared to other baseline byzantine robust aggregation
methods, and proved our proposed model effectiveness in addressing the resource
consumption problem.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13404" title="Abstract">arXiv:2310.13404</a> [<a href="/pdf/2310.13404" title="Download PDF">pdf</a>, <a href="/format/2310.13404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Definition-independent Formalization of Soundscapes: Towards a Formal  Methodology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jedrusiak%2C+M+D">Mikel D. Jedrusiak</a>, 
<a href="/search/cs?searchtype=author&query=Harweg%2C+T">Thomas Harweg</a>, 
<a href="/search/cs?searchtype=author&query=Haselhoff%2C+T">Timo Haselhoff</a>, 
<a href="/search/cs?searchtype=author&query=Lawrence%2C+B+T">Bryce T. Lawrence</a>, 
<a href="/search/cs?searchtype=author&query=Moebus%2C+S">Susanne Moebus</a>, 
<a href="/search/cs?searchtype=author&query=Weichert%2C+F">Frank Weichert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Soundscapes have been studied by researchers from various disciplines, each
with different perspectives, goals, approaches, and terminologies. Accordingly,
depending on the field, the concept of a soundscape's components changes,
consequently changing the basic definition. This results in complicating
interdisciplinary communication and comparison of results. Especially when
soundscape-unrelated research areas are involved. For this reason, we present a
potential formalization that is independent of the underlying soundscape
definition, with the goal of being able to capture the heterogeneous structure
of the data as well as the different ideologies in one model. In an exemplary
analysis of frequency correlation matrices for land use type detection as an
alternative to features like MFCCs, we show a practical application of our
presented formalization.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13409" title="Abstract">arXiv:2310.13409</a> [<a href="/pdf/2310.13409" title="Download PDF">pdf</a>, <a href="/format/2310.13409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicit Alignment and Many-to-many Entailment Based Reasoning for  Conversational Machine Reading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yangyang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+S">Shiyu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Caixia Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaojie Wang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Conversational Machine Reading (CMR) requires answering a user's initial
question through multi-turn dialogue interactions based on a given document.
Although there exist many effective methods, they largely neglected the
alignment between the document and the user-provided information, which
significantly affects the intermediate decision-making and subsequent follow-up
question generation. To address this issue, we propose a pipeline framework
that (1) aligns the aforementioned two sides in an explicit way, (2)makes
decisions using a lightweight many-to-many entailment reasoning module, and (3)
directly generates follow-up questions based on the document and previously
asked questions. Our proposed method achieves state-of-the-art in
micro-accuracy and ranks the first place on the public leaderboard of the CMR
benchmark dataset ShARC.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13411" title="Abstract">arXiv:2310.13411</a> [<a href="/pdf/2310.13411" title="Download PDF">pdf</a>, <a href="/format/2310.13411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Enhancing Relational Rules for Knowledge Graph Link Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shuhan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+H">Huaiyu Wan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuting Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Junfeng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Youfang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Graph neural networks (GNNs) have shown promising performance for knowledge
graph reasoning. A recent variant of GNN called progressive relational graph
neural network (PRGNN), utilizes relational rules to infer missing knowledge in
relational digraphs and achieves notable results. However, during reasoning
with PRGNN, two important properties are often overlooked: (1) the
sequentiality of relation composition, where the order of combining different
relations affects the semantics of the relational rules, and (2) the lagged
entity information propagation, where the transmission speed of required
information lags behind the appearance speed of new entities. Ignoring these
properties leads to incorrect relational rule learning and decreased reasoning
accuracy. To address these issues, we propose a novel knowledge graph reasoning
approach, the Relational rUle eNhanced Graph Neural Network (RUN-GNN).
Specifically, RUN-GNN employs a query related fusion gate unit to model the
sequentiality of relation composition and utilizes a buffering update mechanism
to alleviate the negative effect of lagged entity information propagation,
resulting in higher-quality relational rule learning. Experimental results on
multiple datasets demonstrate the superiority of RUN-GNN is superior on both
transductive and inductive link prediction tasks.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13413" title="Abstract">arXiv:2310.13413</a> [<a href="/pdf/2310.13413" title="Download PDF">pdf</a>, <a href="/ps/2310.13413" title="Download PostScript">ps</a>, <a href="/format/2310.13413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scoped and Typed Staging by Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allais%2C+G">Guillaume Allais</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> As submitted to PEPM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Using a dependently typed host language, we give a well scoped-and-typed by
construction presentation of a minimal two level simply typed calculus with a
static and a dynamic stage. The staging function partially evaluating the part
of a term that are static is obtained by a model construction inspired by
normalisation by evaluation.
<br />We then go on to demonstrate how this minimal language can be extended to
provide additional metaprogramming capabilities, and to define a higher order
functional language evaluating to digital circuit descriptions.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13415" title="Abstract">arXiv:2310.13415</a> [<a href="/pdf/2310.13415" title="Download PDF">pdf</a>, <a href="/format/2310.13415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure Event-Triggered Control for Vehicle Platooning in the Presence of  Modification Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nikoutadbir%2C+A">Ali Nikoutadbir</a>, 
<a href="/search/eess?searchtype=author&query=Torabi%2C+S">Sajjad Torabi</a>, 
<a href="/search/eess?searchtype=author&query=Bolouki%2C+S">Sadegh Bolouki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper addresses the problem of achieving secure consensus in a vehicular
platoon using event-triggered control. The platoon consists of a leader and
multiple follower vehicles exchanging their position and velocity information
discretely to maintain stability. The paper focuses on the issue of gain
modification attacks, where a malicious actor attempts to alter the controller
gains to destabilize the platoon. To tackle this problem, the paper proposes a
resilient event-triggered control scheme that ensures secure consensus while
considering constraints on the duration and frequency of attacks. The paper
also introduces an attack mitigation strategy through a topology-switching
procedure, which is employed when the attack frequency and duration constraints
are not met. First, sufficient consensus conditions for distributed static and
dynamic event-triggered control schemes are derived under sequential gain
modification attacks. The impact of the system matrices and triggering
parameters on the attack constraints is also discussed. Second, the paper
derives conditions to broaden the range of controller gain stability using the
Schur stability criterion to mitigate attacks and maintain platoon stability.
Finally, the effectiveness of the proposed methodology is demonstrated through
simulation scenarios in various case studies.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13420" title="Abstract">arXiv:2310.13420</a> [<a href="/pdf/2310.13420" title="Download PDF">pdf</a>, <a href="/format/2310.13420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversation Chronicles: Towards Diverse Temporal and Relational  Dynamics in Multi-Session Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Jihyoung Jang</a>, 
<a href="/search/cs?searchtype=author&query=Boo%2C+M">Minseong Boo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyounghun Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (23 pages); Project website: <a href="https://conversation-chronicles.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the field of natural language processing, open-domain chatbots have
emerged as an important research topic. However, a major limitation of existing
open-domain chatbot research is its singular focus on short single-session
dialogue, neglecting the potential need for understanding contextual
information in multiple consecutive sessions that precede an ongoing dialogue.
Among the elements that compose the context in multi-session conversation
settings, the time intervals between sessions and the relationships between
speakers would be particularly important. Despite their importance, current
research efforts have not sufficiently addressed these dialogical components.
In this paper, we introduce a new 1M multi-session dialogue dataset, called
Conversation Chronicles, for implementing a long-term conversation setup in
which time intervals and fine-grained speaker relationships are incorporated.
Following recent works, we exploit a large language model to produce the data.
The extensive human evaluation shows that dialogue episodes in Conversation
Chronicles reflect those properties while maintaining coherent and consistent
interactions across all the sessions. We also propose a dialogue model, called
ReBot, which consists of chronological summarization and dialogue generation
modules using only around 630M parameters. When trained on Conversation
Chronicles, ReBot demonstrates long-term context understanding with a high
human engagement score.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13424" title="Abstract">arXiv:2310.13424</a> [<a href="/pdf/2310.13424" title="Download PDF">pdf</a>, <a href="/format/2310.13424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLTracer: Accurate Poisoning Attack Provenance in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ba%2C+Z">Zhongjie Ba</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yuan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianhang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Feng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Li Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">Kui Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 27 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated Learning (FL) is a promising distributed learning approach that
enables multiple clients to collaboratively train a shared global model.
However, recent studies show that FL is vulnerable to various poisoning
attacks, which can degrade the performance of global models or introduce
backdoors into them. In this paper, we first conduct a comprehensive study on
prior FL attacks and detection methods. The results show that all existing
detection methods are only effective against limited and specific attacks. Most
detection methods suffer from high false positives, which lead to significant
performance degradation, especially in not independent and identically
distributed (non-IID) settings. To address these issues, we propose FLTracer,
the first FL attack provenance framework to accurately detect various attacks
and trace the attack time, objective, type, and poisoned location of updates.
Different from existing methodologies that rely solely on cross-client anomaly
detection, we propose a Kalman filter-based cross-round detection to identify
adversaries by seeking the behavior changes before and after the attack. Thus,
this makes it resilient to data heterogeneity and is effective even in non-IID
settings. To further improve the accuracy of our detection method, we employ
four novel features and capture their anomalies with the joint decisions.
Extensive evaluations show that FLTracer achieves an average true positive rate
of over $96.88\%$ at an average false positive rate of less than $2.67\%$,
significantly outperforming SOTA detection methods. \footnote{Code is available
at \url{https://github.com/Eyr3/FLTracer}.}
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13433" title="Abstract">arXiv:2310.13433</a> [<a href="/pdf/2310.13433" title="Download PDF">pdf</a>, <a href="/format/2310.13433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Y-Diagonal Couplings: Approximating Posteriors with Conditional  Wasserstein Distances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chemseddine%2C+J">Jannis Chemseddine</a>, 
<a href="/search/cs?searchtype=author&query=Hagemann%2C+P">Paul Hagemann</a>, 
<a href="/search/cs?searchtype=author&query=Wald%2C+C">Christian Wald</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">In inverse problems, many conditional generative models approximate the
posterior measure by minimizing a distance between the joint measure and its
learned approximation. While this approach also controls the distance between
the posterior measures in the case of the Kullback Leibler divergence, it does
not hold true for the Wasserstein distance. We will introduce a conditional
Wasserstein distance with a set of restricted couplings that equals the
expected Wasserstein distance of the posteriors. By deriving its dual, we find
a rigorous way to motivate the loss of conditional Wasserstein GANs. We outline
conditions under which the vanilla and the conditional Wasserstein distance
coincide. Furthermore, we will show numerical examples where training with the
conditional Wasserstein distance yields favorable properties for posterior
sampling.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13434" title="Abstract">arXiv:2310.13434</a> [<a href="/pdf/2310.13434" title="Download PDF">pdf</a>, <a href="/format/2310.13434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Matrix Analysis to Balance between Supervised and Unsupervised  Learning under the Low Density Separation Assumption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feofanov%2C+V">Vasilii Feofanov</a>, 
<a href="/search/cs?searchtype=author&query=Tiomoko%2C+M">Malik Tiomoko</a>, 
<a href="/search/cs?searchtype=author&query=Virmaux%2C+A">Aladin Virmaux</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 40th International Conference on Machine
  Learning, PMLR 202:10008-10033, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We propose a theoretical framework to analyze semi-supervised classification
under the low density separation assumption in a high-dimensional regime. In
particular, we introduce QLDS, a linear classification model, where the low
density separation assumption is implemented via quadratic margin maximization.
The algorithm has an explicit solution with rich theoretical properties, and we
show that particular cases of our algorithm are the least-square support vector
machine in the supervised case, the spectral clustering in the fully
unsupervised regime, and a class of semi-supervised graph-based approaches. As
such, QLDS establishes a smooth bridge between these supervised and
unsupervised learning methods. Using recent advances in the random matrix
theory, we formally derive a theoretical evaluation of the classification error
in the asymptotic regime. As an application, we derive a hyperparameter
selection policy that finds the best balance between the supervised and the
unsupervised terms of our learning criterion. Finally, we provide extensive
illustrations of our framework, as well as an experimental study on several
benchmarks to demonstrate that QLDS, while being computationally more
efficient, improves over cross-validation for hyperparameter selection,
indicating a high promise of the usage of random matrix theory for
semi-supervised model selection.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13439" title="Abstract">arXiv:2310.13439</a> [<a href="/pdf/2310.13439" title="Download PDF">pdf</a>, <a href="/format/2310.13439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Consistency of Large Language Models under Ambiguity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartsch%2C+H">Henning Bartsch</a>, 
<a href="/search/cs?searchtype=author&query=Jorgensen%2C+O">Ole Jorgensen</a>, 
<a href="/search/cs?searchtype=author&query=Rosati%2C+D">Domenic Rosati</a>, 
<a href="/search/cs?searchtype=author&query=Hoelscher-Obermaier%2C+J">Jason Hoelscher-Obermaier</a>, 
<a href="/search/cs?searchtype=author&query=Pfau%2C+J">Jacob Pfau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BlackboxNLP @ EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) that do not give consistent answers across
contexts are problematic when used for tasks with expectations of consistency,
e.g., question-answering, explanations, etc. Our work presents an evaluation
benchmark for self-consistency in cases of under-specification where two or
more answers can be correct. We conduct a series of behavioral experiments on
the OpenAI model suite using an ambiguous integer sequence completion task. We
find that average consistency ranges from 67\% to 82\%, far higher than would
be predicted if a model's consistency was random, and increases as model
capability improves. Furthermore, we show that models tend to maintain
self-consistency across a series of robustness checks, including prompting
speaker changes and sequence length changes. These results suggest that
self-consistency arises as an emergent capability without specifically training
for it. Despite this, we find that models are uncalibrated when judging their
own consistency, with models displaying both over- and under-confidence. We
also propose a nonparametric test for determining from token output
distribution whether a model assigns non-trivial probability to alternative
answers. Using this test, we find that despite increases in self-consistency,
models usually place significant weight on alternative, inconsistent answers.
This distribution of probability mass provides evidence that even highly
self-consistent models internally compute multiple possible responses.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13440" title="Abstract">arXiv:2310.13440</a> [<a href="/pdf/2310.13440" title="Download PDF">pdf</a>, <a href="/format/2310.13440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Past, Present, and Future of Typological Databases in NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baylor%2C+E">Emi Baylor</a>, 
<a href="/search/cs?searchtype=author&query=Ploeger%2C+E">Esther Ploeger</a>, 
<a href="/search/cs?searchtype=author&query=Bjerva%2C+J">Johannes Bjerva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Typological information has the potential to be beneficial in the development
of NLP models, particularly for low-resource languages. Unfortunately, current
large-scale typological databases, notably WALS and Grambank, are inconsistent
both with each other and with other sources of typological information, such as
linguistic grammars. Some of these inconsistencies stem from coding errors or
linguistic variation, but many of the disagreements are due to the discrete
categorical nature of these databases. We shed light on this issue by
systematically exploring disagreements across typological databases and
resources, and their uses in NLP, covering the past and present. We next
investigate the future of such work, offering an argument that a continuous
view of typological features is clearly beneficial, echoing recommendations
from linguistics. We propose that such a view of typology has significant
potential in the future, including in language modeling in low-resource
scenarios.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13441" title="Abstract">arXiv:2310.13441</a> [<a href="/pdf/2310.13441" title="Download PDF">pdf</a>, <a href="/ps/2310.13441" title="Download PostScript">ps</a>, <a href="/format/2310.13441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seamless, Correct, and Generic Programming over Serialised Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allais%2C+G">Guillaume Allais</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> As submitted to POPL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">In typed functional languages, one can typically only manipulate data in a
type-safe manner if it first has been deserialised into an in-memory tree
represented as a graph of nodes-as-structs and subterms-as-pointers.
<br />We demonstrate how we can use QTT as implemented in \idris{} to define a
small universe of serialised datatypes, and provide generic programs allowing
users to process values stored contiguously in buffers.
<br />Our approach allows implementors to prove the full functional correctness by
construction of the IO functions processing the data stored in the buffer.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13447" title="Abstract">arXiv:2310.13447</a> [<a href="/pdf/2310.13447" title="Download PDF">pdf</a>, <a href="/format/2310.13447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale Superpixel Structured Difference Graph Convolutional Network  for VL Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Siyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yeming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Sirui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yaoru Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Lizhi Bai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Within the multimodal field, the key to integrating vision and language lies
in establishing a good alignment strategy. Recently, benefiting from the
success of self-supervised learning, significant progress has been made in
multimodal semantic representation based on pre-trained models for vision and
language. However, there is still room for improvement in visual semantic
representation. The lack of spatial semantic coherence and vulnerability to
noise makes it challenging for current pixel or patch-based methods to
accurately extract complex scene boundaries. To this end, this paper develops
superpixel as a comprehensive compact representation of learnable image data,
which effectively reduces the number of visual primitives for subsequent
processing by clustering perceptually similar pixels. To mine more precise
topological relations, we propose a Multiscale Difference Graph Convolutional
Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical
structure of constituent visual patterns, and captures multiscale features by
progressively merging adjacent superpixels as graph nodes. Moreover, we predict
the differences between adjacent nodes through the graph structure,
facilitating key information aggregation of graph nodes to reason actual
semantic relations. Afterward, we design a multi-level fusion rule in a
bottom-up manner to avoid understanding deviation by learning complementary
spatial information at different regional scales. Our proposed method can be
well applied to multiple downstream task learning. Extensive experiments
demonstrate that our method is competitive with other state-of-the-art methods
in visual reasoning. Our code will be released upon publication.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13448" title="Abstract">arXiv:2310.13448</a> [<a href="/pdf/2310.13448" title="Download PDF">pdf</a>, <a href="/format/2310.13448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steering Large Language Models for Machine Translation with Finetuning  and In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alves%2C+D+M">Duarte M. Alves</a>, 
<a href="/search/cs?searchtype=author&query=Guerreiro%2C+N+M">Nuno M. Guerreiro</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+J">Jo&#xe3;o Alves</a>, 
<a href="/search/cs?searchtype=author&query=Pombal%2C+J">Jos&#xe9; Pombal</a>, 
<a href="/search/cs?searchtype=author&query=Rei%2C+R">Ricardo Rei</a>, 
<a href="/search/cs?searchtype=author&query=de+Souza%2C+J+G+C">Jos&#xe9; G. C. de Souza</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+A+F+T">Andr&#xe9; F. T. Martins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 - Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) are a promising avenue for machine translation
(MT). However, current LLM-based MT systems are brittle: their effectiveness
highly depends on the choice of few-shot examples and they often require extra
post-processing due to overgeneration. Alternatives such as finetuning on
translation instructions are computationally expensive and may weaken
in-context learning capabilities, due to overspecialization. In this paper, we
provide a closer look at this problem. We start by showing that adapter-based
finetuning with LoRA matches the performance of traditional finetuning while
reducing the number of training parameters by a factor of 50. This method also
outperforms few-shot prompting and eliminates the need for post-processing or
in-context examples. However, we show that finetuning generally degrades
few-shot performance, hindering adaptation capabilities. Finally, to obtain the
best of both worlds, we propose a simple approach that incorporates few-shot
examples during finetuning. Experiments on 10 language pairs show that our
proposed approach recovers the original few-shot capabilities while keeping the
added benefits of finetuning.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13451" title="Abstract">arXiv:2310.13451</a> [<a href="/pdf/2310.13451" title="Download PDF">pdf</a>, <a href="/format/2310.13451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-Stage Triplet Loss Training with Curriculum Augmentation for  Audio-Visual Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Donghuo Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ikeda%2C+K">Kazushi Ikeda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The cross-modal retrieval model leverages the potential of triple loss
optimization to learn robust embedding spaces. However, existing methods often
train these models in a singular pass, overlooking the distinction between
semi-hard and hard triples in the optimization process. The oversight of not
distinguishing between semi-hard and hard triples leads to suboptimal model
performance. In this paper, we introduce a novel approach rooted in curriculum
learning to address this problem. We propose a two-stage training paradigm that
guides the model's learning process from semi-hard to hard triplets. In the
first stage, the model is trained with a set of semi-hard triplets, starting
from a low-loss base. Subsequently, in the second stage, we augment the
embeddings using an interpolation technique. This process identifies potential
hard negatives, alleviating issues arising from high-loss functions due to a
scarcity of hard triples. Our approach then applies hard triplet mining in the
augmented embedding space to further optimize the model. Extensive experimental
results conducted on two audio-visual datasets show a significant improvement
of approximately 9.8% in terms of average Mean Average Precision (MAP) over the
current state-of-the-art method, MSNSCA, for the Audio-Visual Cross-Modal
Retrieval (AV-CMR) task on the AVE dataset, indicating the effectiveness of our
proposed method.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13452" title="Abstract">arXiv:2310.13452</a> [<a href="/pdf/2310.13452" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quadrotor Dead Recooking with Multiple Inertial Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hurwitz%2C+D">Dror Hurwitz</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+I">Itzik Klein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Quadrotors are widely used for surveillance, mapping, and deliveries. In
several scenarios the quadrotor operates in pure inertial navigation mode
resulting in a navigation solution drift. To handle such situations and bind
the navigation drift, the quadrotor dead reckoning (QDR) approach requires
flying the quadrotor in a periodic trajectory. Then, using model or learning
based approaches the quadrotor position vector can be estimated. We propose to
use multiple inertial measurement units (MIMU) to improve the positioning
accuracy of the QDR approach. Several methods to utilize MIMU data in a deep
learning framework are derived and evaluated. Field experiments were conducted
to validate the proposed approach and show its benefits.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13458" title="Abstract">arXiv:2310.13458</a> [<a href="/pdf/2310.13458" title="Download PDF">pdf</a>, <a href="/format/2310.13458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correspondence learning between morphologically different robots through  task demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aktas%2C+H">Hakan Aktas</a>, 
<a href="/search/cs?searchtype=author&query=Nagai%2C+Y">Yukie Nagai</a>, 
<a href="/search/cs?searchtype=author&query=Asada%2C+M">Minoru Asada</a>, 
<a href="/search/cs?searchtype=author&query=Oztop%2C+E">Erhan Oztop</a>, 
<a href="/search/cs?searchtype=author&query=Ugur%2C+E">Emre Ugur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 11 figures, Submitted to IEEE Robotics Automation Letters (RA-L)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We observe a large variety of robots in terms of their bodies, sensors, and
actuators. Given the commonalities in the skill sets, teaching each skill to
each different robot independently is inefficient and not scalable when the
large variety in the robotic landscape is considered. If we can learn the
correspondences between the sensorimotor spaces of different robots, we can
expect a skill that is learned in one robot can be more directly and easily
transferred to the other robots. In this paper, we propose a method to learn
correspondences between robots that have significant differences in their
morphologies: a fixed-based manipulator robot with joint control and a
differential drive mobile robot. For this, both robots are first given
demonstrations that achieve the same tasks. A common latent representation is
formed while learning the corresponding policies. After this initial learning
stage, the observation of a new task execution by one robot becomes sufficient
to generate a latent space representation pertaining to the other robot to
achieve the same task. We verified our system in a set of experiments where the
correspondence between two simulated robots is learned (1) when the robots need
to follow the same paths to achieve the same task, (2) when the robots need to
follow different trajectories to achieve the same task, and (3) when
complexities of the required sensorimotor trajectories are different for the
robots considered. We also provide a proof-of-the-concept realization of
correspondence learning between a real manipulator robot and a simulated mobile
robot.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13459" title="Abstract">arXiv:2310.13459</a> [<a href="/pdf/2310.13459" title="Download PDF">pdf</a>, <a href="/format/2310.13459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Nonconvex-Nonconcave Training via Linear Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pethick%2C+T">Thomas Pethick</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Wanyun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper presents a theoretical analysis of linear interpolation as a
principled method for stabilizing (large-scale) neural network training. We
argue that instabilities in the optimization process are often caused by the
nonmonotonicity of the loss landscape and show how linear interpolation can
help by leveraging the theory of nonexpansive operators. We construct a new
optimization scheme called relaxed approximate proximal point (RAPP), which is
the first explicit method to achieve last iterate convergence rates for the
full range of cohypomonotone problems. The construction extends to constrained
and regularized settings. By replacing the inner optimizer in RAPP we
rediscover the family of Lookahead algorithms for which we establish
convergence in cohypomonotone problems even when the base optimizer is taken to
be gradient descent ascent. The range of cohypomonotone problems in which
Lookahead converges is further expanded by exploiting that Lookahead inherits
the properties of the base optimizer. We corroborate the results with
experiments on generative adversarial networks which demonstrates the benefits
of the linear interpolation present in both RAPP and Lookahead.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13462" title="Abstract">arXiv:2310.13462</a> [<a href="/pdf/2310.13462" title="Download PDF">pdf</a>, <a href="/format/2310.13462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing the matrix exponential and the Cholesky factor of a related  finite horizon Gramian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Stillfjord%2C+T">Tony Stillfjord</a>, 
<a href="/search/math?searchtype=author&query=Tronarp%2C+F">Filip Tronarp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">In this article, an efficient numerical method for computing finite-horizon
controllability Gramians in Cholesky-factored form is proposed. The method is
applicable to general dense matrices of moderate size and produces a Cholesky
factor of the Gramian without computing the full product. In contrast to other
methods applicable to this task, the proposed method is a generalization of the
scaling-and-squaring approach for approximating the matrix exponential. It
exploits a similar doubling formula for the Gramian, and thereby keeps the
required computational effort modest. Most importantly, a rigorous backward
error analysis is provided, which guarantees that the approximation is accurate
to the round-off error level in double precision. This accuracy is illustrated
in practice on a large number of standard test examples.
<br />The method has been implemented in the Julia package
FiniteHorizonGramians.jl, which is available online under the MIT license. Code
for reproducing the experimental results is included in this package, as well
as code for determining the optimal method parameters. The analysis can thus
easily be adapted to a different finite-precision arithmetic.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13469" title="Abstract">arXiv:2310.13469</a> [<a href="/pdf/2310.13469" title="Download PDF">pdf</a>, <a href="/format/2310.13469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask Language Model to Clean Your Noisy Translation Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bolding%2C+Q">Quinten Bolding</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Baohao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Denis%2C+B+J">Brandon James Denis</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transformer models have demonstrated remarkable performance in neural machine
translation (NMT). However, their vulnerability to noisy input poses a
significant challenge in practical implementation, where generating clean
output from noisy input is crucial. The MTNT dataset \cite{MTNT} is widely used
as a benchmark for evaluating the robustness of NMT models against noisy input.
Nevertheless, its utility is limited due to the presence of noise in both the
source and target sentences. To address this limitation, we focus on cleaning
the noise from the target sentences in MTNT, making it more suitable as a
benchmark for noise evaluation. Leveraging the capabilities of large language
models (LLMs), we observe their impressive abilities in noise removal. For
example, they can remove emojis while considering their semantic meaning.
Additionally, we show that LLM can effectively rephrase slang, jargon, and
profanities. The resulting datasets, called C-MTNT, exhibit significantly less
noise in the target sentences while preserving the semantic integrity of the
original sentences. Our human and GPT-4 evaluations also lead to a consistent
conclusion that LLM performs well on this task. Lastly, experiments on C-MTNT
showcased its effectiveness in evaluating the robustness of NMT models,
highlighting the potential of advanced language models for data cleaning and
emphasizing C-MTNT as a valuable resource.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13473" title="Abstract">arXiv:2310.13473</a> [<a href="/pdf/2310.13473" title="Download PDF">pdf</a>, <a href="/format/2310.13473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Sequential Visual Input Reasoning and Prediction in  Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mingwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+L">Leigang Sha</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Y">Yu Shu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kangjia Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tiancheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jianwei Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multimodal large language models (MLLMs) have shown great potential in
perception and interpretation tasks, but their capabilities in predictive
reasoning remain under-explored. To address this gap, we introduce a novel
benchmark that assesses the predictive reasoning capabilities of MLLMs across
diverse scenarios. Our benchmark targets three important domains: abstract
pattern reasoning, human activity prediction, and physical interaction
prediction. We further develop three evaluation methods powered by large
language model to robustly quantify a model's performance in predicting and
reasoning the future based on multi-visual context. Empirical experiments
confirm the soundness of the proposed benchmark and evaluation methods via
rigorous testing and reveal pros and cons of current popular MLLMs in the task
of predictive reasoning. Lastly, our proposed benchmark provides a standardized
evaluation framework for MLLMs and can facilitate the development of more
advanced models that can reason and predict over complex long sequence of
multimodal input.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13474" title="Abstract">arXiv:2310.13474</a> [<a href="/pdf/2310.13474" title="Download PDF">pdf</a>, <a href="/format/2310.13474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of $D^&#x3b1;$ seeding for $k$-means
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bamas%2C+E">Etienne Bamas</a>, 
<a href="/search/cs?searchtype=author&query=Nagarajan%2C+S+G">Sai Ganesh Nagarajan</a>, 
<a href="/search/cs?searchtype=author&query=Svensson%2C+O">Ola Svensson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Abstract shortened to meet ArXiv requirements
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">One of the most popular clustering algorithms is the celebrated $D^\alpha$
seeding algorithm (also know as $k$-means++ when $\alpha=2$) by Arthur and
Vassilvitskii (2007), who showed that it guarantees in expectation an
$O(2^{2\alpha}\cdot \log k)$-approximate solution to the ($k$,$\alpha$)-means
cost (where euclidean distances are raised to the power $\alpha$) for any
$\alpha\ge 1$. More recently, Balcan, Dick, and White (2018) observed
experimentally that using $D^\alpha$ seeding with $\alpha&gt;2$ can lead to a
better solution with respect to the standard $k$-means objective (i.e. the
$(k,2)$-means cost).
<br />In this paper, we provide a rigorous understanding of this phenomenon. For
any $\alpha&gt;2$, we show that $D^\alpha$ seeding guarantees in expectation an
approximation factor of $$ O_\alpha \left((g_\alpha)^{2/\alpha}\cdot
\left(\frac{\sigma_{\mathrm{max}}}{\sigma_{\mathrm{min}}}\right)^{2-4/\alpha}\cdot
(\min\{\ell,\log k\})^{2/\alpha}\right)$$ with respect to the standard
$k$-means cost of any underlying clustering; where $g_\alpha$ is a parameter
capturing the concentration of the points in each cluster,
$\sigma_{\mathrm{max}}$ and $\sigma_{\mathrm{min}}$ are the maximum and minimum
standard deviation of the clusters around their means, and $\ell$ is the number
of distinct mixing weights in the underlying clustering (after rounding them to
the nearest power of $2$). We complement these results by some lower bounds
showing that the dependency on $g_\alpha$ and
$\sigma_{\mathrm{max}}/\sigma_{\mathrm{min}}$ is tight.
<br />Finally, we provide an experimental confirmation of the effects of the
aforementioned parameters when using $D^\alpha$ seeding. Further, we
corroborate the observation that $\alpha&gt;2$ can indeed improve the $k$-means
cost compared to $D^2$ seeding, and that this advantage remains even if we run
Lloyd's algorithm after the seeding.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13479" title="Abstract">arXiv:2310.13479</a> [<a href="/pdf/2310.13479" title="Download PDF">pdf</a>, <a href="/format/2310.13479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment, Select, Correct: A Framework for Weakly-Supervised Referring  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eiras%2C+F">Francisco Eiras</a>, 
<a href="/search/cs?searchtype=author&query=Oksuz%2C+K">Kemal Oksuz</a>, 
<a href="/search/cs?searchtype=author&query=Bibi%2C+A">Adel Bibi</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Dokania%2C+P+K">Puneet K. Dokania</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Referring Image Segmentation (RIS) - the problem of identifying objects in
images through natural language sentences - is a challenging task currently
mostly solved through supervised learning. However, while collecting referred
annotation masks is a time-consuming process, the few existing
weakly-supervised and zero-shot approaches fall significantly short in
performance compared to fully-supervised learning ones. To bridge the
performance gap without mask annotations, we propose a novel weakly-supervised
framework that tackles RIS by decomposing it into three steps: obtaining
instance masks for the object mentioned in the referencing instruction
(segment), using zero-shot learning to select a potentially correct mask for
the given instruction (select), and bootstrapping a model which allows for
fixing the mistakes of zero-shot selection (correct). In our experiments, using
only the first two steps (zero-shot segment and select) outperforms other
zero-shot baselines by as much as 19%, while our full method improves upon this
much stronger baseline and sets the new state-of-the-art for weakly-supervised
RIS, reducing the gap between the weakly-supervised and fully-supervised
methods in some cases from around 33% to as little as 14%. Code is available at
https://github.com/fgirbal/segment-select-correct.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13481" title="Abstract">arXiv:2310.13481</a> [<a href="/pdf/2310.13481" title="Download PDF">pdf</a>, <a href="/format/2310.13481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A review of individual tree crown detection and delineation from optical  remote sensing images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Juepeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weijia Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haohuan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Le Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Powered by the advances of optical remote sensing sensors, the production of
very high spatial resolution multispectral images provides great potential for
achieving cost-efficient and high-accuracy forest inventory and analysis in an
automated way. Lots of studies that aim at providing an inventory to the level
of each individual tree have generated a variety of methods for Individual Tree
Crown Detection and Delineation (ITCD). This review covers ITCD methods for
detecting and delineating individual tree crowns, and systematically reviews
the past and present of ITCD-related researches applied to the optical remote
sensing images. With the goal to provide a clear knowledge map of existing ITCD
efforts, we conduct a comprehensive review of recent ITCD papers to build a
meta-data analysis, including the algorithm, the study site, the tree species,
the sensor type, the evaluation method, etc. We categorize the reviewed methods
into three classes: (1) traditional image processing methods (such as local
maximum filtering, image segmentation, etc.); (2) traditional machine learning
methods (such as random forest, decision tree, etc.); and (3) deep learning
based methods. With the deep learning-oriented approaches contributing a
majority of the papers, we further discuss the deep learning-based methods as
semantic segmentation and object detection methods. In addition, we discuss
four ITCD-related issues to further comprehend the ITCD domain using optical
remote sensing data, such as comparisons between multi-sensor based data and
optical data in ITCD domain, comparisons among different algorithms and
different ITCD tasks, etc. Finally, this review proposes some ITCD-related
applications and a few exciting prospects and potential hot topics in future
ITCD research.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13482" title="Abstract">arXiv:2310.13482</a> [<a href="/pdf/2310.13482" title="Download PDF">pdf</a>, <a href="/format/2310.13482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HSVRS: A Virtual Reality System of the Hide-and-Seek Game to Enhance  Gaze Fixation Ability for Autistic Children
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chengyan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shihuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=zhang%2C+D">Dong zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cen%2C+C">Chaoqun Cen</a>, 
<a href="/search/cs?searchtype=author&query=you%2C+Z">Zhixiang you</a>, 
<a href="/search/cs?searchtype=author&query=zou%2C+X">Xiaobing zou</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Hongzhu Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Numerous children diagnosed with Autism Spectrum Disorder (ASD) exhibit
abnormal eye gaze pattern in communication and social interaction. Due to the
high cost of ASD interventions and a shortage of professional therapists,
researchers have explored the use of virtual reality (VR) systems as a
supplementary intervention for autistic children. This paper presents the
design of a novel VR-based system called the Hide and Seek Virtual Reality
System (HSVRS). The HSVRS allows children with ASD to enhance their ocular gaze
abilities while engaging in a hide-and-seek game with a virtual avatar. By
employing face and voice manipulation technology, the HSVRS provides the option
to customize the appearance and voice of the avatar, making it resemble someone
familiar to the child, such as their parents. We conducted a pilot study at the
Third Affiliated Hospital of Sun Yat-sen University, China, to evaluate the
feasibility of HSVRS as an auxiliary intervention for children with autism
(N=24). Through the analysis of subjective questionnaires completed by the
participants' parents and objective eye gaze data, we observed that children in
the VR-assisted intervention group demonstrated better performance compared to
those in the control group. Furthermore, our findings indicate that the
utilization of face and voice manipulation techniques to personalize avatars in
hide-and-seek games can enhance the efficiency and effectiveness of the system.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13483" title="Abstract">arXiv:2310.13483</a> [<a href="/pdf/2310.13483" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application of deep learning for livestock behaviour recognition: A  systematic literature review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rohan%2C+A">Ali Rohan</a>, 
<a href="/search/cs?searchtype=author&query=Rafaq%2C+M+S">Muhammad Saad Rafaq</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+J">Md. Junayed Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Asghar%2C+F">Furqan Asghar</a>, 
<a href="/search/cs?searchtype=author&query=Bashir%2C+A+K">Ali Kashif Bashir</a>, 
<a href="/search/cs?searchtype=author&query=Dottorini%2C+T">Tania Dottorini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Livestock health and welfare monitoring has traditionally been a
labor-intensive task performed manually. Recent advances have led to the
adoption of AI and computer vision techniques, particularly deep learning
models, as decision-making tools within the livestock industry. These models
have been employed for tasks like animal identification, tracking, body part
recognition, and species classification. In the past decade, there has been a
growing interest in using these models to explore the connection between
livestock behaviour and health issues. While previous review studies have been
rather generic, there is currently no review study specifically focusing on DL
for livestock behaviour recognition. Hence, this systematic literature review
(SLR) was conducted. The SLR involved an initial search across electronic
databases, resulting in 1101 publications. After applying defined selection
criteria, 126 publications were shortlisted. These publications were further
filtered based on quality criteria, resulting in the selection of 44
high-quality primary studies. These studies were analysed to address the
research questions. The results showed that DL successfully addressed 13
behaviour recognition problems encompassing 44 different behaviour classes. A
variety of DL models and networks were employed, with CNN, Faster R-CNN,
YOLOv5, and YOLOv4 being among the most common models, and VGG16, CSPDarknet53,
GoogLeNet, ResNet101, and ResNet50 being popular networks. Performance
evaluation involved ten different matrices, with precision and accuracy being
the most frequently used. Primary studies identified challenges, including
occlusion, adhesion, data imbalance, and the complexities of the livestock
environment. The SLR study also discussed potential solutions and research
directions to facilitate the development of autonomous livestock behaviour
recognition systems.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13486" title="Abstract">arXiv:2310.13486</a> [<a href="/pdf/2310.13486" title="Download PDF">pdf</a>, <a href="/format/2310.13486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mind the instructions: a holistic evaluation of consistency and  interactions in prompt-based learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weber%2C+L">Lucas Weber</a>, 
<a href="/search/cs?searchtype=author&query=Bruni%2C+E">Elia Bruni</a>, 
<a href="/search/cs?searchtype=author&query=Hupkes%2C+D">Dieuwke Hupkes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Finding the best way of adapting pre-trained language models to a task is a
big challenge in current NLP. Just like the previous generation of task-tuned
models (TT), models that are adapted to tasks via in-context-learning (ICL) are
robust in some setups but not in others. Here, we present a detailed analysis
of which design choices cause instabilities and inconsistencies in LLM
predictions. First, we show how spurious correlations between input
distributions and labels -- a known issue in TT models -- form only a minor
problem for prompted models. Then, we engage in a systematic, holistic
evaluation of different factors that have been found to influence predictions
in a prompting setup. We test all possible combinations of a range of factors
on both vanilla and instruction-tuned (IT) LLMs of different scale and
statistically analyse the results to show which factors are the most
influential, interactive or stable. Our results show which factors can be used
without precautions and which should be avoided or handled with care in most
settings.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13490" title="Abstract">arXiv:2310.13490</a> [<a href="/pdf/2310.13490" title="Download PDF">pdf</a>, <a href="/format/2310.13490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Selection and Hyperparameter Fine-tuning in Artificial Neural  Networks for Wood Quality Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roder%2C+M">Mateus Roder</a>, 
<a href="/search/cs?searchtype=author&query=Passos%2C+L+A">Leandro Aparecido Passos</a>, 
<a href="/search/cs?searchtype=author&query=Papa%2C+J+P">Jo&#xe3;o Paulo Papa</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+A+L+D">Andr&#xe9; Luis Debiaso Rossi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Quality classification of wood boards is an essential task in the sawmill
industry, which is still usually performed by human operators in small to
median companies in developing countries. Machine learning algorithms have been
successfully employed to investigate the problem, offering a more affordable
alternative compared to other solutions. However, such approaches usually
present some drawbacks regarding the proper selection of their hyperparameters.
Moreover, the models are susceptible to the features extracted from wood board
images, which influence the induction of the model and, consequently, its
generalization power. Therefore, in this paper, we investigate the problem of
simultaneously tuning the hyperparameters of an artificial neural network (ANN)
as well as selecting a subset of characteristics that better describes the wood
board quality. Experiments were conducted over a private dataset composed of
images obtained from a sawmill industry and described using different feature
descriptors. The predictive performance of the model was compared against five
baseline methods as well as a random search, performing either ANN
hyperparameter tuning and feature selection. Experimental results suggest that
hyperparameters should be adjusted according to the feature set, or the
features should be selected considering the hyperparameter values. In summary,
the best predictive performance, i.e., a balanced accuracy of $0.80$, was
achieved in two distinct scenarios: (i) performing only feature selection, and
(ii) performing both tasks concomitantly. Thus, we suggest that at least one of
the two approaches should be considered in the context of industrial
applications.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13494" title="Abstract">arXiv:2310.13494</a> [<a href="/pdf/2310.13494" title="Download PDF">pdf</a>, <a href="/format/2310.13494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing the Global/Local Coupling Method: An Asynchronous Parallel  Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kerim%2C+A+E">Ahmed El Kerim</a>, 
<a href="/search/cs?searchtype=author&query=Gosselet%2C+P">Pierre Gosselet</a>, 
<a href="/search/cs?searchtype=author&query=Magoul%C3%A8s%2C+F">Fr&#xe9;d&#xe9;ric Magoul&#xe8;s</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">A novel approach is being developed to introduce a parallel asynchronous
implementation of non-intrusive global-local coupling. This study examines
scenarios involving numerous patches, including those covering the entire
structure. By leveraging asynchronous, the method aims to minimize reliance on
communication, handle failures effectively, and address load imbalances.
Detailed insights into the methodology are presented, accompanied by a
demonstration of its performance through an academic case study.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13498" title="Abstract">arXiv:2310.13498</a> [<a href="/pdf/2310.13498" title="Download PDF">pdf</a>, <a href="/format/2310.13498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Checking History-Determinism is NP-hard for Parity Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prakash%2C+A">Aditya Prakash</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">We show that the problem of checking if a given nondeterministic parity
automaton simulates another given nondeterministic parity automaton is NP-hard.
We then adapt the techniques used for this result to show that the problem of
checking history-determinism for a given parity automaton is NP-hard. This is
an improvement from Kuperberg and Skrzypczak's previous lower bound of parity
games from 2015. We also show that deciding if Eve wins the one-token game or
the two-token game of a given parity automaton is NP-hard. Finally, we show
that the problem of deciding if the language of a nondeterministic parity
automaton is contained in the language of a history-deterministic parity
automaton can be solved in quasi-polynomial time.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13499" title="Abstract">arXiv:2310.13499</a> [<a href="/pdf/2310.13499" title="Download PDF">pdf</a>, <a href="/format/2310.13499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DistillCSE: Distilled Contrastive Learning for Sentence Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiahao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wei Shao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lihui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lemao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper proposes the DistillCSE framework, which performs contrastive
learning under the self-training paradigm with knowledge distillation. The
potential advantage of DistillCSE is its self-enhancing feature: using a base
model to provide additional supervision signals, a stronger model may be
learned through knowledge distillation. However, the vanilla DistillCSE through
the standard implementation of knowledge distillation only achieves marginal
improvements due to severe overfitting. The further quantitative analyses
demonstrate the reason that the standard knowledge distillation exhibits a
relatively large variance of the teacher model's logits due to the essence of
contrastive learning. To mitigate the issue induced by high variance, this
paper accordingly proposed two simple yet effective solutions for knowledge
distillation: a Group-P shuffling strategy as an implicit regularization and
the averaging logits from multiple teacher components. Experiments on standard
benchmarks demonstrate that the proposed DistillCSE outperforms many strong
baseline methods and yields a new state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13500" title="Abstract">arXiv:2310.13500</a> [<a href="/pdf/2310.13500" title="Download PDF">pdf</a>, <a href="/format/2310.13500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analogical Proportions and Creativity: A Preliminary Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Afantenos%2C+S">Stergos Afantenos</a>, 
<a href="/search/cs?searchtype=author&query=Prade%2C+H">Henri Prade</a>, 
<a href="/search/cs?searchtype=author&query=Bernardes%2C+L+C">Leonardo Cortez Bernardes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Analogical proportions are statements of the form "$a$ is to $b$ as $c$ is to
$d$", which expresses that the comparisons of the elements in pair $(a, b)$ and
in pair $(c, d)$ yield similar results. Analogical proportions are creative in
the sense that given 3 distinct items, the representation of a 4th item $d$,
distinct from the previous items, which forms an analogical proportion with
them can be calculated, provided certain conditions are met. After providing an
introduction to analogical proportions and their properties, the paper reports
the results of an experiment made with a database of animal descriptions and
their class, where we try to "create" new animals from existing ones,
retrieving rare animals such as platypus. We perform a series of experiments
using word embeddings as well as Boolean features in order to propose novel
animals based on analogical proportions, showing that word embeddings obtain
better results.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13505" title="Abstract">arXiv:2310.13505</a> [<a href="/pdf/2310.13505" title="Download PDF">pdf</a>, <a href="/format/2310.13505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Training for Conversational Question Answering Models with  Reinforced Reformulation Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaiser%2C+M">Magdalena Kaiser</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+R+S">Rishiraj Saha Roy</a>, 
<a href="/search/cs?searchtype=author&query=Weikum%2C+G">Gerhard Weikum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WSDM 2024 Research Paper, 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Models for conversational question answering (ConvQA) over knowledge graphs
(KGs) are usually trained and tested on benchmarks of gold QA pairs. This
implies that training is limited to surface forms seen in the respective
datasets, and evaluation is on a small set of held-out questions. Through our
proposed framework REIGN, we take several steps to remedy this restricted
learning setup. First, we systematically generate reformulations of training
questions to increase robustness of models to surface form variations. This is
a particularly challenging problem, given the incomplete nature of such
questions. Second, we guide ConvQA models towards higher performance by feeding
it only those reformulations that help improve their answering quality, using
deep reinforcement learning. Third, we demonstrate the viability of training
major model components on one benchmark and applying them zero-shot to another.
Finally, for a rigorous evaluation of robustness for trained models, we use and
release large numbers of diverse reformulations generated by prompting GPT for
benchmark test sets (resulting in 20x increase in sizes). Our findings show
that ConvQA models with robust training via reformulations, significantly
outperform those with standard training from gold QA pairs only.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13506" title="Abstract">arXiv:2310.13506</a> [<a href="/pdf/2310.13506" title="Download PDF">pdf</a>, <a href="/format/2310.13506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining Interactions Between Text Spans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S+R">Sagnik Ray Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Atanasova%2C+P">Pepa Atanasova</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> code: <a href="https://github.com/copenlu/spanex">this https URL</a> , dataset: <a href="https://huggingface.co/datasets/copenlu/spanex.">this https URL</a> Accepted EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reasoning over spans of tokens from different parts of the input is essential
for natural language understanding (NLU) tasks such as fact-checking (FC),
machine reading comprehension (MRC) or natural language inference (NLI).
However, existing highlight-based explanations primarily focus on identifying
individual important tokens or interactions only between adjacent tokens or
tuples of tokens. Most notably, there is a lack of annotations capturing the
human decision-making process w.r.t. the necessary interactions for informed
decision-making in such tasks. To bridge this gap, we introduce SpanEx, a
multi-annotator dataset of human span interaction explanations for two NLU
tasks: NLI and FC. We then investigate the decision-making processes of
multiple fine-tuned large language models in terms of the employed connections
between spans in separate parts of the input and compare them to the human
reasoning processes. Finally, we present a novel community detection based
unsupervised method to extract such interaction explanations from a model's
inner workings.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13508" title="Abstract">arXiv:2310.13508</a> [<a href="/pdf/2310.13508" title="Download PDF">pdf</a>, <a href="/format/2310.13508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Robot Mediator for Multiparty Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adikari%2C+M">Manith Adikari</a>, 
<a href="/search/cs?searchtype=author&query=Cangelosi%2C+A">Angelo Cangelosi</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+R">Randy Gomez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE International Conference on Robotics and Automation (ICRA 2023) Workshop Towards a Balanced Cyberphysical Society: A Focus on Group Social Dynamics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">A social robot acting as a 'mediator' can enhance interactions between
humans, for example, in fields such as education and healthcare. A particularly
promising area of research is the use of a social robot mediator in a
multiparty setting, which tends to be the most applicable in real-world
scenarios. However, research in social robot mediation for multiparty
interactions is still emerging and faces numerous challenges. This paper
provides an overview of social robotics and mediation research by highlighting
relevant literature and some of the ongoing problems. The importance of
incorporating relevant psychological principles for developing social robot
mediators is also presented. Additionally, the potential of implementing a
Theory of Mind in a social robot mediator is explored, given how such a
framework could greatly improve mediation by reading the individual and group
mental states to interact effectively.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13512" title="Abstract">arXiv:2310.13512</a> [<a href="/pdf/2310.13512" title="Download PDF">pdf</a>, <a href="/format/2310.13512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Question Generation with Multi-level Content Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+Z">Zehua Xia</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+Q">Qi Gou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bowen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C">Cam-Tu Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper addresses the problem of generating questions from a given context
and an answer, specifically focusing on questions that require multi-hop
reasoning across an extended context. Previous studies have suggested that key
phrase selection is essential for question generation (QG), yet it is still
challenging to connect such disjointed phrases into meaningful questions,
particularly for long context. To mitigate this issue, we propose MultiFactor,
a novel QG framework based on multi-level content planning. Specifically,
MultiFactor includes two components: FA-model, which simultaneously selects key
phrases and generates full answers, and Q-model which takes the generated full
answer as an additional input to generate questions. Here, full answer
generation is introduced to connect the short answer with the selected key
phrases, thus forming an answer-aware summary to facilitate QG. Both FA-model
and Q-model are formalized as simple-yet-effective Phrase-Enhanced
Transformers, our joint model for phrase selection and text generation.
Experimental results show that our method outperforms strong baselines on two
popular QG datasets. Our code is available at
https://github.com/zeaver/MultiFactor.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13513" title="Abstract">arXiv:2310.13513</a> [<a href="/pdf/2310.13513" title="Download PDF">pdf</a>, <a href="/format/2310.13513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Potential of Flexible 8-bit Format: Design and Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuoyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Gonglei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiuying Wei</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+R">Ruihao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xiaoxu Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lewei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">Neural network quantization is widely used to reduce model inference
complexity in real-world deployments. However, traditional integer quantization
suffers from accuracy degradation when adapting to various dynamic ranges.
Recent research has focused on a new 8-bit format, FP8, with hardware support
for both training and inference of neural networks but lacks guidance for
hardware design. In this paper, we analyze the benefits of using FP8
quantization and provide a comprehensive comparison of FP8 with INT
quantization. Then we propose a flexible mixed-precision quantization framework
that supports various number systems, enabling optimal selection of the most
appropriate quantization format for different neural network architectures.
Experimental results demonstrate that our proposed framework achieves
competitive performance compared to full precision on various tasks, including
image classification, object detection, segmentation, and natural language
understanding. Our work furnishes critical insights into the tangible benefits
and feasibility of employing FP8 quantization, paving the way for heightened
neural network efficiency in tangible scenarios. Our code is available in the
supplementary material.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13515" title="Abstract">arXiv:2310.13515</a> [<a href="/pdf/2310.13515" title="Download PDF">pdf</a>, <a href="/format/2310.13515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RaceLens: A Machine Intelligence-Based Application for Racing Photo  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boiarov%2C+A">Andrei Boiarov</a>, 
<a href="/search/cs?searchtype=author&query=Bleklov%2C+D">Dmitry Bleklov</a>, 
<a href="/search/cs?searchtype=author&query=Bredikhin%2C+P">Pavlo Bredikhin</a>, 
<a href="/search/cs?searchtype=author&query=Koritsky%2C+N">Nikita Koritsky</a>, 
<a href="/search/cs?searchtype=author&query=Ulasen%2C+S">Sergey Ulasen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ISACE 2023 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents RaceLens, a novel application utilizing advanced deep
learning and computer vision models for comprehensive analysis of racing
photos. The developed models have demonstrated their efficiency in a wide array
of tasks, including detecting racing cars, recognizing car numbers, detecting
and quantifying car details, and recognizing car orientations. We discuss the
process of collecting a robust dataset necessary for training our models, and
describe an approach we have designed to augment and improve this dataset
continually. Our method leverages a feedback loop for continuous model
improvement, thus enhancing the performance and accuracy of RaceLens over time.
A significant part of our study is dedicated to illustrating the practical
application of RaceLens, focusing on its successful deployment by NASCAR teams
over four seasons. We provide a comprehensive evaluation of our system's
performance and its direct impact on the team's strategic decisions and
performance metrics. The results underscore the transformative potential of
machine intelligence in the competitive and dynamic world of car racing,
setting a precedent for future applications.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13518" title="Abstract">arXiv:2310.13518</a> [<a href="/pdf/2310.13518" title="Download PDF">pdf</a>, <a href="/format/2310.13518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Based Mobile App GUI Testing: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shengcheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chunrong Fang</a>, 
<a href="/search/cs?searchtype=author&query=Tuo%2C+Z">Ziyuan Tuo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Quanjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhendong Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Graphical User Interface (GUI) has become one of the most significant parts
of mobile applications (apps). It is a direct bridge between mobile apps and
end users, which directly affects the end user's experience. Neglecting GUI
quality can undermine the value and effectiveness of the entire mobile app
solution. Significant research efforts have been devoted to GUI testing, one
effective method to ensure mobile app quality. By conducting rigorous GUI
testing, developers can ensure that the visual and interactive elements of the
mobile apps not only meet functional requirements but also provide a seamless
and user-friendly experience. However, traditional solutions, relying on the
source code or layout files, have met challenges in both effectiveness and
efficiency due to the gap between what is obtained and what app GUI actually
presents. Vision-based mobile app GUI testing approaches emerged with the
development of computer vision technologies and have achieved promising
progress. In this survey paper, we provide a comprehensive investigation of the
state-of-the-art techniques on 226 papers, among which 78 are vision-based
studies. This survey covers different topics of GUI testing, like GUI test
generation, GUI test record &amp; replay, GUI testing framework, etc. Specifically,
the research emphasis of this survey is placed mostly on how vision-based
techniques outperform traditional solutions and have gradually taken a vital
place in the GUI testing field. Based on the investigation of existing studies,
we outline the challenges and opportunities of (vision-based) mobile app GUI
testing and propose promising research directions with the combination of
emerging techniques.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13519" title="Abstract">arXiv:2310.13519</a> [<a href="/pdf/2310.13519" title="Download PDF">pdf</a>, <a href="/format/2310.13519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The performance of the DK (Discrete Kirchhoff) and DKM (Discrete  Kirchhoff-Mindlin) shell element classes in the upper bound  finite-element-based limit analysis and in the sequential  finite-element-based limit analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=%C5%A0tembera%2C+V">V&#xed;t&#x11b;zslav &#x160;tembera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper investigates the applicability of the DK and DKM shell element
classes for the first time within the framework of the standard (and
sequential) finite-element-based limit analysis, which is a direct method used
for determining the plastic collapse (and post-collapse) behaviour of
structures. Despite these elements not being able to guarantee a strict upper
bound, it is shown that they exhibit a significantly lower discretization error
compared to the strict upper-bound formulated shell elements used in
literature. Among these elements, the superiority of quadrangle elements over
triangle elements is observed. The numerical results are also compared with the
MITC shell elements, which have been shown to converge only in the pure
membrane case.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13522" title="Abstract">arXiv:2310.13522</a> [<a href="/pdf/2310.13522" title="Download PDF">pdf</a>, <a href="/format/2310.13522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teaching Language Models to Self-Improve through Interactive  Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Baolin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Galley%2C+M">Michel Galley</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The self-improving ability of large language models (LLMs), enabled by
prompting them to analyze and revise their own outputs, has garnered
significant interest in recent research. However, this ability has been shown
to be absent and difficult to learn for smaller models, thus widening the
performance gap between state-of-the-art LLMs and more cost-effective and
faster ones. To reduce this gap, we introduce TriPosT, a training algorithm
that endows smaller models with such self-improvement ability, and show that
our approach can improve a LLaMA-7b's performance on math and reasoning tasks
by up to 7.13%. In contrast to prior work, we achieve this by using the smaller
model to interact with LLMs to collect feedback and improvements on its own
generations. We then replay this experience to train the small model. Our
experiments on four math and reasoning datasets show that the interactive
experience of learning from and correcting its own mistakes is crucial for
small models to improve their performance.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13526" title="Abstract">arXiv:2310.13526</a> [<a href="/pdf/2310.13526" title="Download PDF">pdf</a>, <a href="/ps/2310.13526" title="Download PostScript">ps</a>, <a href="/format/2310.13526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlled Randomness Improves the Performance of Transformer Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deu%C3%9Fer%2C+T">Tobias Deu&#xdf;er</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Cong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C3%A4mer%2C+W">Wolfgang Kr&#xe4;mer</a>, 
<a href="/search/cs?searchtype=author&query=Leonhard%2C+D">David Leonhard</a>, 
<a href="/search/cs?searchtype=author&query=Bauckhage%2C+C">Christian Bauckhage</a>, 
<a href="/search/cs?searchtype=author&query=Sifa%2C+R">Rafet Sifa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICMLA 2023, 10 pages, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">During the pre-training step of natural language models, the main objective
is to learn a general representation of the pre-training dataset, usually
requiring large amounts of textual data to capture the complexity and diversity
of natural language. Contrasting this, in most cases, the size of the data
available to solve the specific downstream task is often dwarfed by the
aforementioned pre-training dataset, especially in domains where data is
scarce. We introduce controlled randomness, i.e. noise, into the training
process to improve fine-tuning language models and explore the performance of
targeted noise in addition to the parameters of these models. We find that
adding such noise can improve the performance in our two downstream tasks of
joint named entity recognition and relation extraction and text summarization.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13528" title="Abstract">arXiv:2310.13528</a> [<a href="/pdf/2310.13528" title="Download PDF">pdf</a>, <a href="/format/2310.13528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Watch Nearby! Privacy Analysis of the People Nearby Service of Telegram
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caprolu%2C+M">Maurantonio Caprolu</a>, 
<a href="/search/cs?searchtype=author&query=Sciancalepore%2C+S">Savio Sciancalepore</a>, 
<a href="/search/cs?searchtype=author&query=Grigorov%2C+A">Aleksandar Grigorov</a>, 
<a href="/search/cs?searchtype=author&query=Kolev%2C+V">Velyan Kolev</a>, 
<a href="/search/cs?searchtype=author&query=Oligeri%2C+G">Gabriele Oligeri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">People Nearby is a service offered by Telegram that allows a user to discover
other Telegram users, based only on geographical proximity. Nearby users are
reported with a rough estimate of their distance from the position of the
reference user, allowing Telegram to claim location privacy In this paper, we
systematically analyze the location privacy provided by Telegram to users of
the People Nearby service. Through an extensive measurement campaign run by
spoofing the user's location all over the world, we reverse-engineer the
algorithm adopted by People Nearby to compute distances between users. Although
the service protects against precise user localization, we demonstrate that
location privacy is always lower than the one declared by Telegram of 500
meters. Specifically, we discover that location privacy is a function of the
geographical position of the user. Indeed, the radius of the location privacy
area (localization error) spans between 400 meters (close to the equator) and
128 meters (close to the poles), with a difference of up to 75% (worst case)
compared to what Telegram declares. After our responsible disclosure, Telegram
updated the FAQ associated with the service. Finally, we provide some solutions
and countermeasures that Telegram can implement to improve location privacy. In
general, the reported findings highlight the significant privacy risks
associated with using People Nearby service.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13533" title="Abstract">arXiv:2310.13533</a> [<a href="/pdf/2310.13533" title="Download PDF">pdf</a>, <a href="/format/2310.13533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technical Report for ICCV 2023 Visual Continual Learning Challenge:  Continuous Test-time Adaptation for Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%B3jka%2C+D">Damian S&#xf3;jka</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+D">Dipam Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Cygert%2C+S">Sebastian Cygert</a>, 
<a href="/search/cs?searchtype=author&query=Twardowski%2C+B">Bart&#x142;omiej Twardowski</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Weijer%2C+J">Joost van de Weijer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The goal of the challenge is to develop a test-time adaptation (TTA) method,
which could adapt the model to gradually changing domains in video sequences
for semantic segmentation task. It is based on a synthetic driving video
dataset - SHIFT. The source model is trained on images taken during daytime in
clear weather. Domain changes at test-time are mainly caused by varying weather
conditions and times of day. The TTA methods are evaluated in each image
sequence (video) separately, meaning the model is reset to the source model
state before the next sequence. Images come one by one and a prediction has to
be made at the arrival of each frame. Each sequence is composed of 401 images
and starts with the source domain, then gradually drifts to a different one
(changing weather or time of day) until the middle of the sequence. In the
second half of the sequence, the domain gradually shifts back to the source
one. Ground truth data is available only for the validation split of the SHIFT
dataset, in which there are only six sequences that start and end with the
source domain. We conduct an analysis specifically on those sequences. Ground
truth data for test split, on which the developed TTA methods are evaluated for
leader board ranking, are not publicly available.
<br />The proposed solution secured a 3rd place in a challenge and received an
innovation award. Contrary to the solutions that scored better, we did not use
any external pretrained models or specialized data augmentations, to keep the
solutions as general as possible. We have focused on analyzing the
distributional shift and developing a method that could adapt to changing data
dynamics and generalize across different scenarios.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13534" title="Abstract">arXiv:2310.13534</a> [<a href="/pdf/2310.13534" title="Download PDF">pdf</a>, <a href="/ps/2310.13534" title="Download PostScript">ps</a>, <a href="/format/2310.13534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Weighted Generalized Gauss Quadratures for M&#xfc;ntz Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+H">Huaijin Wang</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+C">Chuanju Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A novel recurrence formula for moments with respect to M\"{u}ntz-Legendre
polynomials is proposed and applied to construct a numerical method for solving
generalized Gauss quadratures with power function weight for M\"{u}ntz systems.
These quadrature rules exhibit several properties similar to the classical
Gaussian quadratures for polynomial systems, including positive weights, rapid
convergence, and others. They are applicable to a wide range of functions,
including smooth functions and functions with endpoint singularities, commonly
found in integral equations with singular kernels, complex analysis, potential
theory, and other areas.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13538" title="Abstract">arXiv:2310.13538</a> [<a href="/pdf/2310.13538" title="Download PDF">pdf</a>, <a href="/format/2310.13538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Positive-Unlabeled Node Classification with Structure-aware Graph  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hansi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Q">Quanming Yao</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J">James Kwok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Node classification on graphs is an important research problem with many
applications. Real-world graph data sets may not be balanced and accurate as
assumed by most existing works. A challenging setting is positive-unlabeled
(PU) node classification, where labeled nodes are restricted to positive nodes.
It has diverse applications, e.g., pandemic prediction or network anomaly
detection. Existing works on PU node classification overlook information in the
graph structure, which can be critical. In this paper, we propose to better
utilize graph structure for PU node classification. We first propose a
distance-aware PU loss that uses homophily in graphs to introduce more accurate
supervision. We also propose a regularizer to align the model with graph
structure. Theoretical analysis shows that minimizing the proposed loss also
leads to minimizing the expected loss with both positive and negative labels.
Extensive empirical evaluation on diverse graph data sets demonstrates its
superior performance over existing state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13540" title="Abstract">arXiv:2310.13540</a> [<a href="/pdf/2310.13540" title="Download PDF">pdf</a>, <a href="/format/2310.13540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+Z">Zekai Qu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Ruobing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaojun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+F">Fengzong Lian</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Z">Zhanhui Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">With the thriving of pre-trained language model (PLM) widely verified in
various of NLP tasks, pioneer efforts attempt to explore the possible
cooperation of the general textual information in PLM with the personalized
behavioral information in user historical behavior sequences to enhance
sequential recommendation (SR). However, despite the commonalities of input
format and task goal, there are huge gaps between the behavioral and textual
information, which obstruct thoroughly modeling SR as language modeling via
PLM. To bridge the gap, we propose a novel Unified pre-trained language model
enhanced sequential recommendation (UPSR), aiming to build a unified
pre-trained recommendation model for multi-domain recommendation tasks. We
formally design five key indicators, namely naturalness, domain consistency,
informativeness, noise &amp; ambiguity, and text length, to guide the text-&gt;item
adaptation and behavior sequence-&gt;text sequence adaptation differently for
pre-training and fine-tuning stages, which are essential but under-explored by
previous works. In experiments, we conduct extensive evaluations on seven
datasets with both tuning and zero-shot settings and achieve the overall best
performance. Comprehensive model analyses also provide valuable insights for
behavior modeling via PLM, shedding light on large pre-trained recommendation
models. The source codes will be released in the future.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13541" title="Abstract">arXiv:2310.13541</a> [<a href="/pdf/2310.13541" title="Download PDF">pdf</a>, <a href="/ps/2310.13541" title="Download PostScript">ps</a>, <a href="/format/2310.13541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Adaptive Time-Varying Convex Optimization for Multi-agent  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+L">Liangze Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Z">Zhengguang Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Lei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper focus on the time-varying convex optimization problems with
uncertain parameters. A new class of adaptive algorithms are proposed to solve
time-varying convex optimization problems. Under the mild assumption of Hessian
and partial derivative of the gradient with respect to time, the dependence on
them is reduced through appropriate adaptive law design. By integrating the new
adaptive optimization algorithm and a modified consensus algorithms, the
time-varying optimization problems can be solved in a distributed manner. Then,
they are extended from the agents with single-integrator dynamics to
double-integrator dynamics, which describes more practical systems. As an
example, the source seeking problem is used to verify the proposed design.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13544" title="Abstract">arXiv:2310.13544</a> [<a href="/pdf/2310.13544" title="Download PDF">pdf</a>, <a href="/format/2310.13544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Diachronic Perspective on User Trust in AI under Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhuliawala%2C+S">Shehzaad Dhuliawala</a>, 
<a href="/search/cs?searchtype=author&query=Zouhar%2C+V">Vil&#xe9;m Zouhar</a>, 
<a href="/search/cs?searchtype=author&query=El-Assady%2C+M">Mennatallah El-Assady</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, 14 pages (8+6)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In a human-AI collaboration, users build a mental model of the AI system
based on its reliability and how it presents its decision, e.g. its
presentation of system confidence and an explanation of the output. Modern NLP
systems are often uncalibrated, resulting in confidently incorrect predictions
that undermine user trust. In order to build trustworthy AI, we must understand
how user trust is developed and how it can be regained after potential
trust-eroding events. We study the evolution of user trust in response to these
trust-eroding events using a betting game. We find that even a few incorrect
instances with inaccurate confidence estimates damage user trust and
performance, with very slow recovery. We also show that this degradation in
trust reduces the success of human-AI collaboration and that different types of
miscalibration -- unconfidently correct and confidently incorrect -- have
different negative effects on user trust. Our findings highlight the importance
of calibration in user-facing AI applications and shed light on what aspects
help users decide whether to trust the AI system.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13545" title="Abstract">arXiv:2310.13545</a> [<a href="/pdf/2310.13545" title="Download PDF">pdf</a>, <a href="/format/2310.13545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ScaleLong: Towards More Stable Training of Diffusion Model via Scaling  Network Long Skip Connection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhongzhan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shuicheng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Liang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In diffusion models, UNet is the most popular network backbone, since its
long skip connects (LSCs) to connect distant network blocks can aggregate
long-distant information and alleviate vanishing gradient. Unfortunately, UNet
often suffers from unstable training in diffusion models which can be
alleviated by scaling its LSC coefficients smaller. However, theoretical
understandings of the instability of UNet in diffusion models and also the
performance improvement of LSC scaling remain absent yet. To solve this issue,
we theoretically show that the coefficients of LSCs in UNet have big effects on
the stableness of the forward and backward propagation and robustness of UNet.
Specifically, the hidden feature and gradient of UNet at any layer can
oscillate and their oscillation ranges are actually large which explains the
instability of UNet training. Moreover, UNet is also provably sensitive to
perturbed input, and predicts an output distant from the desired output,
yielding oscillatory loss and thus oscillatory gradient. Besides, we also
observe the theoretical benefits of the LSC coefficient scaling of UNet in the
stableness of hidden features and gradient and also robustness. Finally,
inspired by our theory, we propose an effective coefficient scaling framework
ScaleLong that scales the coefficients of LSC in UNet and better improves the
training stability of UNet. Experimental results on four famous datasets show
that our methods are superior to stabilize training and yield about 1.5x
training acceleration on different diffusion models with UNet or UViT
backbones. Code: https://github.com/sail-sg/ScaleLong
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13548" title="Abstract">arXiv:2310.13548</a> [<a href="/pdf/2310.13548" title="Download PDF">pdf</a>, <a href="/format/2310.13548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding Sycophancy in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+M">Mrinank Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+M">Meg Tong</a>, 
<a href="/search/cs?searchtype=author&query=Korbak%2C+T">Tomasz Korbak</a>, 
<a href="/search/cs?searchtype=author&query=Duvenaud%2C+D">David Duvenaud</a>, 
<a href="/search/cs?searchtype=author&query=Askell%2C+A">Amanda Askell</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+S+R">Samuel R. Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Newton Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Durmus%2C+E">Esin Durmus</a>, 
<a href="/search/cs?searchtype=author&query=Hatfield-Dodds%2C+Z">Zac Hatfield-Dodds</a>, 
<a href="/search/cs?searchtype=author&query=Johnston%2C+S+R">Scott R. Johnston</a>, 
<a href="/search/cs?searchtype=author&query=Kravec%2C+S">Shauna Kravec</a>, 
<a href="/search/cs?searchtype=author&query=Maxwell%2C+T">Timothy Maxwell</a>, 
<a href="/search/cs?searchtype=author&query=McCandlish%2C+S">Sam McCandlish</a>, 
<a href="/search/cs?searchtype=author&query=Ndousse%2C+K">Kamal Ndousse</a>, 
<a href="/search/cs?searchtype=author&query=Rausch%2C+O">Oliver Rausch</a>, 
<a href="/search/cs?searchtype=author&query=Schiefer%2C+N">Nicholas Schiefer</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Da Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miranda Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+E">Ethan Perez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Reinforcement learning from human feedback (RLHF) is a popular technique for
training high-quality AI assistants. However, RLHF may also encourage model
responses that match user beliefs over truthful responses, a behavior known as
sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models
and whether human preference judgements are responsible. We first demonstrate
that five state-of-the-art AI assistants consistently exhibit sycophantic
behavior across four varied free-form text-generation tasks. To understand if
human preferences drive this broadly observed behavior of RLHF models, we
analyze existing human preference data. We find that when a response matches a
user's views, it is more likely to be preferred. Moreover, both humans and
preference models (PMs) prefer convincingly-written sycophantic responses over
correct ones a negligible fraction of the time. Optimizing model outputs
against PMs also sometimes sacrifices truthfulness in favor of sycophancy.
Overall, our results indicate that sycophancy is a general behavior of RLHF
models, likely driven in part by human preference judgements favoring
sycophantic responses.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13549" title="Abstract">arXiv:2310.13549</a> [<a href="/pdf/2310.13549" title="Download PDF">pdf</a>, <a href="/format/2310.13549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Perils &amp; Promises of Fact-checking with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quelle%2C+D">Dorian Quelle</a>, 
<a href="/search/cs?searchtype=author&query=Bovet%2C+A">Alexandre Bovet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Autonomous fact-checking, using machine learning to verify claims, has grown
vital as misinformation spreads beyond human fact-checking capacity. Large
Language Models (LLMs) like GPT-4 are increasingly trusted to verify
information and write academic papers, lawsuits, and news articles, emphasizing
their role in discerning truth from falsehood and the importance of being able
to verify their outputs. Here, we evaluate the use of LLM agents in
fact-checking by having them phrase queries, retrieve contextual data, and make
decisions. Importantly, in our framework, agents explain their reasoning and
cite the relevant sources from the retrieved context. Our results show the
enhanced prowess of LLMs when equipped with contextual information. GPT-4
outperforms GPT-3, but accuracy varies based on query language and claim
veracity. While LLMs show promise in fact-checking, caution is essential due to
inconsistent accuracy. Our investigation calls for further research, fostering
a deeper comprehension of when agents succeed and when they fail.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13550" title="Abstract">arXiv:2310.13550</a> [<a href="/pdf/2310.13550" title="Download PDF">pdf</a>, <a href="/format/2310.13550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Benefits of Multi-task RL under Non-Markovian Decision Making  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruiquan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+V">Vincent Tan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yingbin Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In multi-task reinforcement learning (RL) under Markov decision processes
(MDPs), the presence of shared latent structures among multiple MDPs has been
shown to yield significant benefits to the sample efficiency compared to
single-task RL. In this paper, we investigate whether such a benefit can extend
to more general sequential decision making problems, such as partially
observable MDPs (POMDPs) and more general predictive state representations
(PSRs). The main challenge here is that the large and complex model space makes
it hard to identify what types of common latent structure of multi-task PSRs
can reduce the model complexity and improve sample efficiency. To this end, we
posit a joint model class for tasks and use the notion of $\eta$-bracketing
number to quantify its complexity; this number also serves as a general metric
to capture the similarity of tasks and thus determines the benefit of
multi-task over single-task RL. We first study upstream multi-task learning
over PSRs, in which all tasks share the same observation and action spaces. We
propose a provably efficient algorithm UMT-PSR for finding near-optimal
policies for all PSRs, and demonstrate that the advantage of multi-task
learning manifests if the joint model class of PSRs has a smaller
$\eta$-bracketing number compared to that of individual single-task learning.
We also provide several example multi-task PSRs with small $\eta$-bracketing
numbers, which reap the benefits of multi-task learning. We further investigate
downstream learning, in which the agent needs to learn a new target task that
shares some commonalities with the upstream tasks via a similarity constraint.
By exploiting the learned PSRs from the upstream, we develop a sample-efficient
algorithm that provably finds a near-optimal policy.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13551" title="Abstract">arXiv:2310.13551</a> [<a href="/pdf/2310.13551" title="Download PDF">pdf</a>, <a href="/format/2310.13551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROSS: Radar Off-road Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Peng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Saripalli%2C+S">Srikanth Saripalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, accepted by the 18th International Symposium on Experimental Robotics (ISER 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">As the demand for autonomous navigation in off-road environments increases,
the need for effective solutions to understand these surroundings becomes
essential. In this study, we confront the inherent complexities of semantic
segmentation in RADAR data for off-road scenarios. We present a novel pipeline
that utilizes LIDAR data and an existing annotated off-road LIDAR dataset for
generating RADAR labels, in which the RADAR data are represented as images.
Validated with real-world datasets, our pragmatic approach underscores the
potential of RADAR technology for navigation applications in off-road
environments.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13552" title="Abstract">arXiv:2310.13552</a> [<a href="/pdf/2310.13552" title="Download PDF">pdf</a>, <a href="/format/2310.13552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-prompted Chain-of-Thought on Large Language Models for Open-domain  Multi-hop Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In open-domain question-answering (ODQA), most existing questions require
single-hop reasoning on commonsense. To further extend this task, we officially
introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop
questions with explicit reasoning steps in open-domain setting. Recently, large
language models (LLMs) have found significant utility in facilitating ODQA
without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts
the reasoning capability of LLMs to a greater extent with manual or automated
paradigms. However, existing automated methods lack of quality assurance, while
manual approaches suffer from limited scalability and poor diversity, hindering
the capabilities of LLMs. In this paper, we propose Self-prompted
Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality
CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation
pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT
selection and self-prompted inference via in-context learning. Extensive
experiments on four multi-hop question-answering benchmarks show that our
proposed SP-CoT not only significantly surpasses the previous SOTA methods on
large-scale (175B) LLMs, but also nearly doubles the zero-shot performance of
small-scale (13B) LLMs. Further analysis reveals the remarkable capability of
SP-CoT to elicit direct and concise intermediate reasoning steps by recalling
$\sim$50\% of intermediate answers on MuSiQue-Ans dataset.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13553" title="Abstract">arXiv:2310.13553</a> [<a href="/pdf/2310.13553" title="Download PDF">pdf</a>, <a href="/format/2310.13553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On sample complexity of conditional independence testing with Von Mises  estimator with application to causal discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamshidi%2C+F">Fateme Jamshidi</a>, 
<a href="/search/cs?searchtype=author&query=Ganassali%2C+L">Luca Ganassali</a>, 
<a href="/search/cs?searchtype=author&query=Kiyavash%2C+N">Negar Kiyavash</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Motivated by conditional independence testing, an essential step in
constraint-based causal discovery algorithms, we study the nonparametric Von
Mises estimator for the entropy of multivariate distributions built on a kernel
density estimator. We establish an exponential concentration inequality for
this estimator. We design a test for conditional independence (CI) based on our
estimator, called VM-CI, which achieves optimal parametric rates under
smoothness assumptions. Leveraging the exponential concentration, we prove a
tight upper bound for the overall error of VM-CI. This, in turn, allows us to
characterize the sample complexity of any constraint-based causal discovery
algorithm that uses VM-CI for CI tests. To the best of our knowledge, this is
the first sample complexity guarantee for causal discovery for continuous
variables. Furthermore, we empirically show that VM-CI outperforms other
popular CI tests in terms of either time or sample complexity (or both), which
translates to a better performance in structure learning as well.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13557" title="Abstract">arXiv:2310.13557</a> [<a href="/pdf/2310.13557" title="Download PDF">pdf</a>, <a href="/format/2310.13557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Global Optimal Coverage Control in Multi-agent Systems:  Known and Unknown Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Faghihi%2C+M">Mohammadhasan Faghihi</a>, 
<a href="/search/eess?searchtype=author&query=Yadegar%2C+M">Meysam Yadegar</a>, 
<a href="/search/eess?searchtype=author&query=Bakhtiaridoust%2C+M">Mohammadhosein Bakhtiaridoust</a>, 
<a href="/search/eess?searchtype=author&query=Meskin%2C+N">Nader Meskin</a>, 
<a href="/search/eess?searchtype=author&query=Shari%2C+J">Javad Shari</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+P">Peng Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper introduces a novel approach to solve the coverage optimization
problem in multi-agent systems. The proposed technique offers a solution that
not only achieves the global optimality in the agents configuration but also
effectively handles the issue of agents remaining stationary in regions void of
information. The proposed approach leverages a novel cost function for
optimizing the agents coverage and the cost function eventually aligns with the
conventional Voronoi-based cost function. Theoretical analyses are conducted to
assure the asymptotic convergence of agents towards the optimal configuration.
A distinguishing feature of this approach lies in its departure from the
reliance on geometric methods that are characteristic of Voronoi-based
approaches; hence can be implemented more simply. Remarkably, the technique is
adaptive and applicable to various environments with both known and unknown
information distributions. Lastly, the efficacy of the proposed method is
demonstrated through simulations, and the obtained results are compared with
those of Voronoi-based algorithms.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13558" title="Abstract">arXiv:2310.13558</a> [<a href="/pdf/2310.13558" title="Download PDF">pdf</a>, <a href="/format/2310.13558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Minority Dynamics and the Power of Synchronicity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Becchetti%2C+L">Luca Becchetti</a>, 
<a href="/search/cs?searchtype=author&query=Clementi%2C+A">Andrea Clementi</a>, 
<a href="/search/cs?searchtype=author&query=Pasquale%2C+F">Francesco Pasquale</a>, 
<a href="/search/cs?searchtype=author&query=Trevisan%2C+L">Luca Trevisan</a>, 
<a href="/search/cs?searchtype=author&query=Vacus%2C+R">Robin Vacus</a>, 
<a href="/search/cs?searchtype=author&query=Ziccardi%2C+I">Isabella Ziccardi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 1 figure, to be published in SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">We study the minority-opinion dynamics over a fully-connected network of $n$
nodes with binary opinions. Upon activation, a node receives a sample of
opinions from a limited number of neighbors chosen uniformly at random. Each
activated node then adopts the opinion that is least common within the received
sample. Unlike all other known consensus dynamics, we prove that this
elementary protocol behaves in dramatically different ways, depending on
whether activations occur sequentially or in parallel. Specifically, we show
that its expected consensus time is exponential in $n$ under asynchronous
models, such as asynchronous GOSSIP. On the other hand, despite its chaotic
nature, we show that it converges within $O(\log^2 n)$ rounds with high
probability under synchronous models, such as synchronous GOSSIP. Finally, our
results shed light on the bit-dissemination problem, that was previously
introduced to model the spread of information in biological scenarios.
Specifically, our analysis implies that the minority-opinion dynamics is the
first stateless solution to this problem, in the parallel passive-communication
setting, achieving convergence within a polylogarithmic number of rounds. This,
together with a known lower bound for sequential stateless dynamics, implies a
parallel-vs-sequential gap for this problem that is nearly quadratic in the
number $n$ of nodes. This is in contrast to all known results for problems in
this area, which exhibit a linear gap between the parallel and the sequential
setting.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13561" title="Abstract">arXiv:2310.13561</a> [<a href="/pdf/2310.13561" title="Download PDF">pdf</a>, <a href="/format/2310.13561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cache &amp; Distil: Optimising API Calls to Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ram%C3%ADrez%2C+G">Guillem Ram&#xed;rez</a>, 
<a href="/search/cs?searchtype=author&query=Lindemann%2C+M">Matthias Lindemann</a>, 
<a href="/search/cs?searchtype=author&query=Birch%2C+A">Alexandra Birch</a>, 
<a href="/search/cs?searchtype=author&query=Titov%2C+I">Ivan Titov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large-scale deployment of generative AI tools often depends on costly API
calls to a Large Language Model (LLM) to fulfil user queries. To curtail the
frequency of these calls, one can employ a smaller language model -- a student
-- which is continuously trained on the responses of the LLM. This student
gradually gains proficiency in independently handling an increasing number of
user requests, a process we term neural caching. The crucial element in neural
caching is a policy that decides which requests should be processed by the
student alone and which should be redirected to the LLM, subsequently aiding
the student's learning. In this study, we focus on classification tasks, and we
consider a range of classic active learning-based selection criteria as the
policy. Our experiments suggest that Margin Sampling and Query by Committee
bring consistent benefits across tasks and budgets.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13564" title="Abstract">arXiv:2310.13564</a> [<a href="/pdf/2310.13564" title="Download PDF">pdf</a>, <a href="/format/2310.13564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $hp$-optimal convergence of the original DG method for linear hyperbolic  problems on special simplicial meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dong%2C+Z">Zhaonan Dong</a>, 
<a href="/search/math?searchtype=author&query=Mascotto%2C+L">Lorenzo Mascotto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We prove hp-optimal error estimates for the original DG method when
approximating solutions to first-order hyperbolic problems with constant
convection fields in the L2 and DG norms. The main theoretical tools used in
the analysis are novel hp-optimal approximation properties of the special
projector introduced in [Cockburn, Dong, Guzman, SINUM, 2008]. We assess the
numerical results on some test cases.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13565" title="Abstract">arXiv:2310.13565</a> [<a href="/pdf/2310.13565" title="Download PDF">pdf</a>, <a href="/format/2310.13565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward Shaping for Happier Autonomous Cyber Security Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bates%2C+E">Elizabeth Bates</a>, 
<a href="/search/cs?searchtype=author&query=Mavroudis%2C+V">Vasilios Mavroudis</a>, 
<a href="/search/cs?searchtype=author&query=Hicks%2C+C">Chris Hicks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As machine learning models become more capable, they have exhibited increased
potential in solving complex tasks. One of the most promising directions uses
deep reinforcement learning to train autonomous agents in computer network
defense tasks. This work studies the impact of the reward signal that is
provided to the agents when training for this task. Due to the nature of
cybersecurity tasks, the reward signal is typically 1) in the form of penalties
(e.g., when a compromise occurs), and 2) distributed sparsely across each
defense episode. Such reward characteristics are atypical of classic
reinforcement learning tasks where the agent is regularly rewarded for progress
(cf. to getting occasionally penalized for failures). We investigate reward
shaping techniques that could bridge this gap so as to enable agents to train
more sample-efficiently and potentially converge to a better performance. We
first show that deep reinforcement learning algorithms are sensitive to the
magnitude of the penalties and their relative size. Then, we combine penalties
with positive external rewards and study their effect compared to penalty-only
training. Finally, we evaluate intrinsic curiosity as an internal positive
reward mechanism and discuss why it might not be as advantageous for high-level
network monitoring tasks.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13566" title="Abstract">arXiv:2310.13566</a> [<a href="/pdf/2310.13566" title="Download PDF">pdf</a>, <a href="/format/2310.13566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-Augmented Neural Response Generation Using Logical Reasoning  and Relevance Scoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Walker%2C+N+T">Nicholas Thomas Walker</a>, 
<a href="/search/cs?searchtype=author&query=Ultes%2C+S">Stefan Ultes</a>, 
<a href="/search/cs?searchtype=author&query=Lison%2C+P">Pierre Lison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at SemDial, August 2023 in Maribor, Slovenia
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Constructing responses in task-oriented dialogue systems typically relies on
information sources such the current dialogue state or external databases. This
paper presents a novel approach to knowledge-grounded response generation that
combines retrieval-augmented language models with logical reasoning. The
approach revolves around a knowledge graph representing the current dialogue
state and background information, and proceeds in three steps. The knowledge
graph is first enriched with logically derived facts inferred using
probabilistic logical programming. A neural model is then employed at each turn
to score the conversational relevance of each node and edge of this extended
graph. Finally, the elements with highest relevance scores are converted to a
natural language form, and are integrated into the prompt for the neural
conversational model employed to generate the system response.
<br />We investigate the benefits of the proposed approach on two datasets (KVRET
and GraphWOZ) along with a human evaluation. Experimental results show that the
combination of (probabilistic) logical reasoning with conversational relevance
scoring does increase both the factuality and fluency of the responses.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13570" title="Abstract">arXiv:2310.13570</a> [<a href="/pdf/2310.13570" title="Download PDF">pdf</a>, <a href="/format/2310.13570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Baseline for Knowledge-Based Visual Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xenos%2C+A">Alexandros Xenos</a>, 
<a href="/search/cs?searchtype=author&query=Stafylakis%2C+T">Themos Stafylakis</a>, 
<a href="/search/cs?searchtype=author&query=Patras%2C+I">Ioannis Patras</a>, 
<a href="/search/cs?searchtype=author&query=Tzimiropoulos%2C+G">Georgios Tzimiropoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (camera-ready version)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper is on the problem of Knowledge-Based Visual Question Answering
(KB-VQA). Recent works have emphasized the significance of incorporating both
explicit (through external databases) and implicit (through LLMs) knowledge to
answer questions requiring external knowledge effectively. A common limitation
of such approaches is that they consist of relatively complicated pipelines and
often heavily rely on accessing GPT-3 API. Our main contribution in this paper
is to propose a much simpler and readily reproducible pipeline which, in a
nutshell, is based on efficient in-context learning by prompting LLaMA (1 and
2) using question-informative captions as contextual information. Contrary to
recent approaches, our method is training-free, does not require access to
external databases or APIs, and yet achieves state-of-the-art accuracy on the
OK-VQA and A-OK-VQA datasets. Finally, we perform several ablation studies to
understand important aspects of our method. Our code is publicly available at
https://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13571" title="Abstract">arXiv:2310.13571</a> [<a href="/pdf/2310.13571" title="Download PDF">pdf</a>, <a href="/ps/2310.13571" title="Download PostScript">ps</a>, <a href="/format/2310.13571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why Can Large Language Models Generate Correct Chain-of-Thoughts?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tutunov%2C+R">Rasul Tutunov</a>, 
<a href="/search/cs?searchtype=author&query=Grosnit%2C+A">Antoine Grosnit</a>, 
<a href="/search/cs?searchtype=author&query=Ziomek%2C+J">Juliusz Ziomek</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bou-Ammar%2C+H">Haitham Bou-Ammar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper delves into the capabilities of large language models (LLMs),
specifically focusing on advancing the theoretical comprehension of
chain-of-thought prompting. We investigate how LLMs can be effectively induced
to generate a coherent chain of thoughts. To achieve this, we introduce a
two-level hierarchical graphical model tailored for natural language
generation. Within this framework, we establish a compelling geometrical
convergence rate that gauges the likelihood of an LLM-generated chain of
thoughts compared to those originating from the true language. Our findings
provide a theoretical justification for the ability of LLMs to produce the
correct sequence of thoughts (potentially) explaining performance gains in
tasks demanding reasoning skills.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13572" title="Abstract">arXiv:2310.13572</a> [<a href="/pdf/2310.13572" title="Download PDF">pdf</a>, <a href="/format/2310.13572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unraveling the Enigma of Double Descent: An In-depth Analysis through  the Lens of Learned Feature Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yufei Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Aste%2C+T">Tomaso Aste</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Double descent presents a counter-intuitive aspect within the machine
learning domain, and researchers have observed its manifestation in various
models and tasks. While some theoretical explanations have been proposed for
this phenomenon in specific contexts, an accepted theory to account for its
occurrence in deep learning remains yet to be established. In this study, we
revisit the phenomenon of double descent and demonstrate that its occurrence is
strongly influenced by the presence of noisy data. Through conducting a
comprehensive analysis of the feature space of learned representations, we
unveil that double descent arises in imperfect models trained with noisy data.
We argue that double descent is a consequence of the model first learning the
noisy data until interpolation and then adding implicit regularization via
over-parameterization acquiring therefore capability to separate the
information from the noise. We postulate that double descent should never occur
in well-regularized models.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13573" title="Abstract">arXiv:2310.13573</a> [<a href="/pdf/2310.13573" title="Download PDF">pdf</a>, <a href="/format/2310.13573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Generalization with Adaptive Style Techniques for Fingerprint  Liveness Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kexin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yule%2C+A">Adam Yule</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiajun Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a high-performance fingerprint liveness feature extraction
technique that secured first place in LivDet 2023 Fingerprint Representation
Challenge. Additionally, we developed a practical fingerprint recognition
system with 94.68% accuracy, earning second place in LivDet 2023 Liveness
Detection in Action. By investigating various methods, particularly style
transfer, we demonstrate improvements in accuracy and generalization when faced
with limited training data. As a result, our approach achieved state-of-the-art
performance in LivDet 2023 Challenges.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13575" title="Abstract">arXiv:2310.13575</a> [<a href="/pdf/2310.13575" title="Download PDF">pdf</a>, <a href="/format/2310.13575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Decomposition of Question and SQL for Text-to-SQL Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eyal%2C+B">Ben Eyal</a>, 
<a href="/search/cs?searchtype=author&query=Bachar%2C+A">Amir Bachar</a>, 
<a href="/search/cs?searchtype=author&query=Haroche%2C+O">Ophir Haroche</a>, 
<a href="/search/cs?searchtype=author&query=Mahabi%2C+M">Moran Mahabi</a>, 
<a href="/search/cs?searchtype=author&query=Elhadad%2C+M">Michael Elhadad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text-to-SQL semantic parsing faces challenges in generalizing to cross-domain
and complex queries. Recent research has employed a question decomposition
strategy to enhance the parsing of complex SQL queries. However, this strategy
encounters two major obstacles: (1) existing datasets lack question
decomposition; (2) due to the syntactic complexity of SQL, most complex queries
cannot be disentangled into sub-queries that can be readily recomposed. To
address these challenges, we propose a new modular Query Plan Language (QPL)
that systematically decomposes SQL queries into simple and regular sub-queries.
We develop a translator from SQL to QPL by leveraging analysis of SQL server
query optimization plans, and we augment the Spider dataset with QPL programs.
Experimental results demonstrate that the modular nature of QPL benefits
existing semantic-parsing architectures, and training text-to-QPL parsers is
more effective than text-to-SQL parsing for semantically equivalent queries.
The QPL approach offers two additional advantages: (1) QPL programs can be
paraphrased as simple questions, which allows us to create a dataset of
(complex question, decomposed questions). Training on this dataset, we obtain a
Question Decomposer for data retrieval that is sensitive to database schemas.
(2) QPL is more accessible to non-experts for complex queries, leading to more
interpretable output from the semantic parser.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13576" title="Abstract">arXiv:2310.13576</a> [<a href="/pdf/2310.13576" title="Download PDF">pdf</a>, <a href="/format/2310.13576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree Search in DAG Space with Model-based Reinforcement Learning for  Causal Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darvariu%2C+V">Victor-Alexandru Darvariu</a>, 
<a href="/search/cs?searchtype=author&query=Hailes%2C+S">Stephen Hailes</a>, 
<a href="/search/cs?searchtype=author&query=Musolesi%2C+M">Mirco Musolesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Identifying causal structure is central to many fields ranging from strategic
decision-making to biology and economics. In this work, we propose a
model-based reinforcement learning method for causal discovery based on tree
search, which builds directed acyclic graphs incrementally. We also formalize
and prove the correctness of an efficient algorithm for excluding edges that
would introduce cycles, which enables deeper discrete search and sampling in
DAG space. We evaluate our approach on two real-world tasks, achieving
substantially better performance than the state-of-the-art model-free method
and greedy search, constituting a promising advancement for combinatorial
methods.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13577" title="Abstract">arXiv:2310.13577</a> [<a href="/pdf/2310.13577" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Multi-Agent Deep Reinforcement Learning for Adaptive  Decentralized Emergency Voltage Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Ying Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Yue%2C+M">Meng Yue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 2024 IEEE PES Innovative Smart Grid Technologies Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Under voltage load shedding (UVLS) for power grid emergency control builds
the last defensive perimeter to prevent cascade outages and blackouts in case
of contingencies. This letter proposes a novel cooperative multi-agent deep
reinforcement learning (MADRL)-based UVLS algorithm in an adaptive
decentralized way. With well-designed input signals reflecting the voltage
deviation, newly structured neural networks are developed as intelligent agents
to obtain control actions and their probabilities to accommodate high
uncertainties in volatile power system operations. Moreover, the interaction
among the agents for coordinated control is implemented and refined by a
state-of-the-art attention mechanism, which helps agents concentratively learn
effective interacted information. The proposed method realizes decentralized
coordinated control, adapting to extremely high uncertainties. Case studies on
an IEEE benchmark system indicate the superior performance of the proposed
algorithm.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13579" title="Abstract">arXiv:2310.13579</a> [<a href="/pdf/2310.13579" title="Download PDF">pdf</a>, <a href="/format/2310.13579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical approximation of McKean-Vlasov SDEs via stochastic gradient  descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Agarwal%2C+A">Ankush Agarwal</a>, 
<a href="/search/math?searchtype=author&query=Amato%2C+A">Andrea Amato</a>, 
<a href="/search/math?searchtype=author&query=Reis%2C+G+d">Goncalo dos Reis</a>, 
<a href="/search/math?searchtype=author&query=Pagliarani%2C+S">Stefano Pagliarani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Probability (math.PR)

</div>
<p class="mathjax">We propose a novel approach to numerically approximate McKean-Vlasov
stochastic differential equations (MV-SDE) using stochastic gradient descent
(SGD) while avoiding the use of interacting particle systems. The technique of
SGD is deployed to solve a Euclidean minimization problem, which is obtained by
first representing the MV-SDE as a minimization problem over the set of
continuous functions of time, and then by approximating the domain with a
finite-dimensional subspace. Convergence is established by proving certain
intermediate stability and moment estimates of the relevant stochastic
processes (including the tangent ones). Numerical experiments illustrate the
competitive performance of our SGD based method compared to the IPS benchmarks.
This work offers a theoretical foundation for using the SGD method in the
context of numerical approximation of MV-SDEs, and provides analytical tools to
study its stability and convergence.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13581" title="Abstract">arXiv:2310.13581</a> [<a href="/pdf/2310.13581" title="Download PDF">pdf</a>, <a href="/format/2310.13581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPARE: A Single-Pass Neural Model for Relational Databases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hilprecht%2C+B">Benjamin Hilprecht</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>, 
<a href="/search/cs?searchtype=author&query=Binnig%2C+C">Carsten Binnig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While there has been extensive work on deep neural networks for images and
text, deep learning for relational databases (RDBs) is still a rather
unexplored field.
<br />One direction that recently gained traction is to apply Graph Neural Networks
(GNNs) to RBDs. However, training GNNs on large relational databases (i.e.,
data stored in multiple database tables) is rather inefficient due to multiple
rounds of training and potentially large and inefficient representations.
Hence, in this paper we propose SPARE (Single-Pass Relational models), a new
class of neural models that can be trained efficiently on RDBs while providing
similar accuracies as GNNs. For enabling efficient training, different from
GNNs, SPARE makes use of the fact that data in RDBs has a regular structure,
which allows one to train these models in a single pass while exploiting
symmetries at the same time. Our extensive empirical evaluation demonstrates
that SPARE can significantly speedup both training and inference while offering
competitive predictive performance over numerous baselines.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13583" title="Abstract">arXiv:2310.13583</a> [<a href="/pdf/2310.13583" title="Download PDF">pdf</a>, <a href="/format/2310.13583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arviv%2C+O">Ofir Arviv</a>, 
<a href="/search/cs?searchtype=author&query=Nikolaev%2C+D">Dmitry Nikolaev</a>, 
<a href="/search/cs?searchtype=author&query=Karidi%2C+T">Taelin Karidi</a>, 
<a href="/search/cs?searchtype=author&query=Abend%2C+O">Omri Abend</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite the impressive growth of the abilities of multilingual language
models, such as XLM-R and mT5, it has been shown that they still face
difficulties when tackling typologically-distant languages, particularly in the
low-resource setting. One obstacle for effective cross-lingual transfer is
variability in word-order patterns. It can be potentially mitigated via source-
or target-side word reordering, and numerous approaches to reordering have been
proposed. However, they rely on language-specific rules, work on the level of
POS tags, or only target the main clause, leaving subordinate clauses intact.
To address these limitations, we present a new powerful reordering method,
defined in terms of Universal Dependencies, that is able to learn fine-grained
word-order patterns conditioned on the syntactic context from a small amount of
annotated data and can be applied at all levels of the syntactic tree. We
conduct experiments on a diverse set of tasks and show that our method
consistently outperforms strong baselines over different language pairs and
model architectures. This performance advantage holds true in both zero-shot
and few-shot scenarios.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13585" title="Abstract">arXiv:2310.13585</a> [<a href="/pdf/2310.13585" title="Download PDF">pdf</a>, <a href="/format/2310.13585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal  Action Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vahdani%2C+E">Elahe Vahdani</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yingli Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper tackles the challenge of point-supervised temporal action
detection, wherein only a single frame is annotated for each action instance in
the training set. Most of the current methods, hindered by the sparse nature of
annotated points, struggle to effectively represent the continuous structure of
actions or the inherent temporal and semantic dependencies within action
instances. Consequently, these methods frequently learn merely the most
distinctive segments of actions, leading to the creation of incomplete action
proposals. This paper proposes POTLoc, a Pseudo-label Oriented Transformer for
weakly-supervised Action Localization utilizing only point-level annotation.
POTLoc is designed to identify and track continuous action structures via a
self-training strategy. The base model begins by generating action proposals
solely with point-level supervision. These proposals undergo refinement and
regression to enhance the precision of the estimated action boundaries, which
subsequently results in the production of `pseudo-labels' to serve as
supplementary supervisory signals. The architecture of the model integrates a
transformer with a temporal feature pyramid to capture video snippet
dependencies and model actions of varying duration. The pseudo-labels,
providing information about the coarse locations and boundaries of actions,
assist in guiding the transformer for enhanced learning of action dynamics.
POTLoc outperforms the state-of-the-art point-supervised methods on THUMOS'14
and ActivityNet-v1.2 datasets, showing a significant improvement of 5% average
mAP on the former.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13588" title="Abstract">arXiv:2310.13588</a> [<a href="/pdf/2310.13588" title="Download PDF">pdf</a>, <a href="/format/2310.13588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simultaneous Machine Translation with Tailored Reference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shoutao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaolei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023; 15 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Simultaneous machine translation (SiMT) generates translation while reading
the whole source sentence. However, existing SiMT models are typically trained
using the same reference disregarding the varying amounts of available source
information at different latency. Training the model with ground-truth at low
latency may introduce forced anticipations, whereas utilizing reference
consistent with the source word order at high latency results in performance
degradation. Consequently, it is crucial to train the SiMT model with
appropriate reference that avoids forced anticipations during training while
maintaining high quality. In this paper, we propose a novel method that
provides tailored reference for the SiMT models trained at different latency by
rephrasing the ground-truth. Specifically, we introduce the tailor, induced by
reinforcement learning, to modify ground-truth to the tailored reference. The
SiMT model is trained with the tailored reference and jointly optimized with
the tailor to enhance performance. Importantly, our method is applicable to a
wide range of current SiMT approaches. Experiments on three translation tasks
demonstrate that our method achieves state-of-the-art performance in both fixed
and adaptive policies.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13589" title="Abstract">arXiv:2310.13589</a> [<a href="/pdf/2310.13589" title="Download PDF">pdf</a>, <a href="/format/2310.13589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A R4RS Compliant REPL in 7 KB
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=O%27Leary%2C+L+O">L&#xe9;onard Oest O&#x27;Leary</a>, 
<a href="/search/cs?searchtype=author&query=Laroche%2C+M">Mathis Laroche</a>, 
<a href="/search/cs?searchtype=author&query=Feeley%2C+M">Marc Feeley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at The 2023 Scheme and Functional Programming Workshop (<a href="/abs/cs/0101200">arXiv:cs/0101200</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">The Ribbit system is a compact Scheme implementation running on the Ribbit
Virtual Machine (RVM) that has been ported to a dozen host languages. It
supports a simple Foreign Function Interface (FFI) allowing extensions to the
RVM directly from the program's source code. We have extended the system to
offer conformance to the R4RS standard while staying as compact as possible.
This leads to a R4RS compliant REPL that fits in an 7 KB Linux executable. This
paper explains the various issues encountered and our solutions to make,
arguably, the smallest R4RS conformant Scheme implementation of all time.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13590" title="Abstract">arXiv:2310.13590</a> [<a href="/pdf/2310.13590" title="Download PDF">pdf</a>, <a href="/format/2310.13590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReLM: Leveraging Language Models for Enhanced Chemical Reaction  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yaorui Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">An Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Enzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Predicting chemical reactions, a fundamental challenge in chemistry, involves
forecasting the resulting products from a given reaction process. Conventional
techniques, notably those employing Graph Neural Networks (GNNs), are often
limited by insufficient training data and their inability to utilize textual
information, undermining their applicability in real-world applications. In
this work, we propose ReLM, a novel framework that leverages the chemical
knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing
the accuracy of real-world chemical reaction predictions. To further enhance
the model's robustness and interpretability, we incorporate the confidence
score strategy, enabling the LMs to self-assess the reliability of their
predictions. Our experimental results demonstrate that ReLM improves the
performance of state-of-the-art GNN-based methods across various chemical
reaction datasets, especially in out-of-distribution settings. Codes are
available at https://github.com/syr-cn/ReLM.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13593" title="Abstract">arXiv:2310.13593</a> [<a href="/pdf/2310.13593" title="Download PDF">pdf</a>, <a href="/format/2310.13593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Longer-range Contextualized Masked Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taekyung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chun%2C+S">Sanghyuk Chun</a>, 
<a href="/search/cs?searchtype=author&query=Heo%2C+B">Byeongho Heo</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dongyoon Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Masked image modeling (MIM) has emerged as a promising self-supervised
learning (SSL) strategy. The MIM pre-training facilitates learning powerful
representations using an encoder-decoder framework by randomly masking some
input pixels and reconstructing the masked pixels from the remaining ones.
However, as the encoder is trained with partial pixels, the MIM pre-training
can suffer from a low capability of understanding long-range dependency. This
limitation may hinder its capability to fully understand multiple-range
dependencies, resulting in narrow highlighted regions in the attention map that
may incur accuracy drops. To mitigate the limitation, We propose a
self-supervised learning framework, named Longer-range Contextualized Masked
Autoencoder (LC-MAE). LC-MAE effectively leverages a global context
understanding of visual representations while simultaneously reducing the
spatial redundancy of input at the same time. Our method steers the encoder to
learn from entire pixels in multiple views while also learning local
representation from sparse pixels. As a result, LC-MAE learns more
discriminative representations, leading to a performance improvement of
achieving 84.2% top-1 accuracy with ViT-B on ImageNet-1K with 0.6%p gain. We
attribute the success to the enhanced pre-training method, as evidenced by the
singular value spectrum and attention analyses. Finally, LC-MAE achieves
significant performance gains at the downstream semantic segmentation and
fine-grained visual classification tasks; and on diverse robust evaluation
metrics. Our code will be publicly available.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13595" title="Abstract">arXiv:2310.13595</a> [<a href="/pdf/2310.13595" title="Download PDF">pdf</a>, <a href="/format/2310.13595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entangled Preferences: The History and Risks of Reinforcement Learning  and Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lambert%2C+N">Nathan Lambert</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+T+K">Thomas Krendl Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Zick%2C+T">Tom Zick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Reinforcement learning from human feedback (RLHF) has emerged as a powerful
technique to make large language models (LLMs) easier to use and more
effective. A core piece of the RLHF process is the training and utilization of
a model of human preferences that acts as a reward function for optimization.
This approach, which operates at the intersection of many stakeholders and
academic disciplines, remains poorly understood. RLHF reward models are often
cited as being central to achieving performance, yet very few descriptors of
capabilities, evaluations, training methods, or open-source models exist. Given
this lack of information, further study and transparency is needed for learned
RLHF reward models. In this paper, we illustrate the complex history of
optimizing preferences, and articulate lines of inquiry to understand the
sociotechnical context of reward models. In particular, we highlight the
ontological differences between costs, rewards, and preferences at stake in
RLHF's foundations, related methodological tensions, and possible research
directions to improve general understanding of how reward models function.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13596" title="Abstract">arXiv:2310.13596</a> [<a href="/pdf/2310.13596" title="Download PDF">pdf</a>, <a href="/format/2310.13596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MarineGPT: Unlocking Secrets of Ocean to the Public
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Ziqiang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Tuan-Anh Vu</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Tim%2C+Y+H+W">Yue Him Wong Tim</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+S">Sai-Kit Yeung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress. Code and data will be available at <a href="https://github.com/hkust-vgd/MarineGPT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs), such as ChatGPT/GPT-4, have proven to be
powerful tools in promoting the user experience as an AI assistant. The
continuous works are proposing multi-modal large language models (MLLM),
empowering LLMs with the ability to sense multiple modality inputs through
constructing a joint semantic space (e.g. visual-text space). Though
significant success was achieved in LLMs and MLLMs, exploring LLMs and MLLMs in
domain-specific applications that required domain-specific knowledge and
expertise has been less conducted, especially for \textbf{marine domain}.
Different from general-purpose MLLMs, the marine-specific MLLM is required to
yield much more \textbf{sensitive}, \textbf{informative}, and
\textbf{scientific} responses. In this work, we demonstrate that the existing
MLLMs optimized on huge amounts of readily available general-purpose training
data show a minimal ability to understand domain-specific intents and then
generate informative and satisfactory responses. To address these issues, we
propose \textbf{MarineGPT}, the first vision-language model specially designed
for the marine domain, unlocking the secrets of the ocean to the public. We
present our \textbf{Marine-5M} dataset with more than 5 million marine
image-text pairs to inject domain-specific marine knowledge into our model and
achieve better marine vision and language alignment. Our MarineGPT not only
pushes the boundaries of marine understanding to the general public but also
offers a standard protocol for adapting a general-purpose assistant to
downstream domain-specific experts. We pave the way for a wide range of marine
applications while setting valuable data and pre-trained models for future
research in both academic and industrial communities.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13597" title="Abstract">arXiv:2310.13597</a> [<a href="/pdf/2310.13597" title="Download PDF">pdf</a>, <a href="/ps/2310.13597" title="Download PostScript">ps</a>, <a href="/format/2310.13597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicit orthogonal and unitary designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=O%27Donnell%2C+R">Ryan O&#x27;Donnell</a>, 
<a href="/search/cs?searchtype=author&query=Servedio%2C+R+A">Rocco A. Servedio</a>, 
<a href="/search/cs?searchtype=author&query=Paredes%2C+P">Pedro Paredes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">We give a strongly explicit construction of $\varepsilon$-approximate
$k$-designs for the orthogonal group $\mathrm{O}(N)$ and the unitary group
$\mathrm{U}(N)$, for $N=2^n$. Our designs are of cardinality
$\mathrm{poly}(N^k/\varepsilon)$ (equivalently, they have seed length $O(nk +
\log(1/\varepsilon)))$; up to the polynomial, this matches the number of design
elements used by the construction consisting of completely random matrices.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13605" title="Abstract">arXiv:2310.13605</a> [<a href="/pdf/2310.13605" title="Download PDF">pdf</a>, <a href="/format/2310.13605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhiqiang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+K">Kun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Local Feature Matching, an essential component of several computer vision
tasks (e.g., structure from motion and visual localization), has been
effectively settled by Transformer-based methods. However, these methods only
integrate long-range context information among keypoints with a fixed receptive
field, which constrains the network from reconciling the importance of features
with different receptive fields to realize complete image perception, hence
limiting the matching accuracy. In addition, these methods utilize a
conventional handcrafted encoding approach to integrate the positional
information of keypoints into the visual descriptors, which limits the
capability of the network to extract reliable positional encoding message. In
this study, we propose Feature Matching with Reconciliatory Transformer (FMRT),
a novel Transformer-based detector-free method that reconciles different
features with multiple receptive fields adaptively and utilizes parallel
networks to realize reliable positional encoding. Specifically, FMRT proposes a
dedicated Reconciliatory Transformer (RecFormer) that consists of a Global
Perception Attention Layer (GPAL) to extract visual descriptors with different
receptive fields and integrate global context information under various scales,
Perception Weight Layer (PWL) to measure the importance of various receptive
fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract
deep aggregated multi-scale local feature representation. Extensive experiments
demonstrate that FMRT yields extraordinary performance on multiple benchmarks,
including pose estimation, visual localization, homography estimation, and
image matching.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13606" title="Abstract">arXiv:2310.13606</a> [<a href="/pdf/2310.13606" title="Download PDF">pdf</a>, <a href="/format/2310.13606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection  Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Macko%2C+D">Dominik Macko</a>, 
<a href="/search/cs?searchtype=author&query=Moro%2C+R">Robert Moro</a>, 
<a href="/search/cs?searchtype=author&query=Uchendu%2C+A">Adaku Uchendu</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+J+S">Jason Samuel Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Yamashita%2C+M">Michiharu Yamashita</a>, 
<a href="/search/cs?searchtype=author&query=Pikuliak%2C+M">Mat&#xfa;&#x161; Pikuliak</a>, 
<a href="/search/cs?searchtype=author&query=Srba%2C+I">Ivan Srba</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thai Le</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Simko%2C+J">Jakub Simko</a>, 
<a href="/search/cs?searchtype=author&query=Bielikova%2C+M">Maria Bielikova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">There is a lack of research into capabilities of recent LLMs to generate
convincing text in languages other than English and into performance of
detectors of machine-generated text in multilingual settings. This is also
reflected in the available benchmarks which lack authentic texts in languages
other than English and predominantly cover older generators. To fill this gap,
we introduce MULTITuDE, a novel benchmarking dataset for multilingual
machine-generated text detection comprising of 74,081 authentic and
machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru,
uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare
the performance of zero-shot (statistical and black-box) and fine-tuned
detectors. Considering the multilinguality, we evaluate 1) how these detectors
generalize to unseen languages (linguistically similar as well as dissimilar)
and unseen LLMs and 2) whether the detectors improve their performance when
trained on multiple languages.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13607" title="Abstract">arXiv:2310.13607</a> [<a href="/pdf/2310.13607" title="Download PDF">pdf</a>, <a href="/ps/2310.13607" title="Download PostScript">ps</a>, <a href="/format/2310.13607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing the contribution of different passively collected data to  predict Stress and Depression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonafonte%2C+I">Irene Bonafonte</a>, 
<a href="/search/cs?searchtype=author&query=Bustos%2C+C">Cristina Bustos</a>, 
<a href="/search/cs?searchtype=author&query=Larrazolo%2C+A">Abraham Larrazolo</a>, 
<a href="/search/cs?searchtype=author&query=Luna%2C+G+L+M">Gilberto Lorenzo Martinez Luna</a>, 
<a href="/search/cs?searchtype=author&query=Arenas%2C+A+G">Adolfo Guzman Arenas</a>, 
<a href="/search/cs?searchtype=author&query=Baro%2C+X">Xavier Baro</a>, 
<a href="/search/cs?searchtype=author&query=Tourgeman%2C+I">Isaac Tourgeman</a>, 
<a href="/search/cs?searchtype=author&query=Balcells%2C+M">Mercedes Balcells</a>, 
<a href="/search/cs?searchtype=author&query=Lapedriza%2C+A">Agata Lapedriza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The possibility of recognizing diverse aspects of human behavior and
environmental context from passively captured data motivates its use for mental
health assessment. In this paper, we analyze the contribution of different
passively collected sensor data types (WiFi, GPS, Social interaction, Phone
Log, Physical Activity, Audio, and Academic features) to predict daily
selfreport stress and PHQ-9 depression score. First, we compute 125 mid-level
features from the original raw data. These 125 features include groups of
features from the different sensor data types. Then, we evaluate the
contribution of each feature type by comparing the performance of Neural
Network models trained with all features against Neural Network models trained
with specific feature groups. Our results show that WiFi features (which encode
mobility patterns) and Phone Log features (which encode information correlated
with sleep patterns), provide significative information for stress and
depression prediction.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13610" title="Abstract">arXiv:2310.13610</a> [<a href="/pdf/2310.13610" title="Download PDF">pdf</a>, <a href="/format/2310.13610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Make Your Decision Convincing! A Unified Two-Stage Framework:  Self-Attribution and Decision-Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yanrui Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sendong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haochun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+R">Rui Bai</a>, 
<a href="/search/cs?searchtype=author&query=Qiang%2C+Z">Zewen Qiang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Muzhen Cai</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Explaining black-box model behavior with natural language has achieved
impressive results in various NLP tasks. Recent research has explored the
utilization of subsequences from the input text as a rationale, providing users
with evidence to support the model decision. Although existing frameworks excel
in generating high-quality rationales while achieving high task performance,
they neglect to account for the unreliable link between the generated rationale
and model decision. In simpler terms, a model may make correct decisions while
attributing wrong rationales, or make poor decisions while attributing correct
rationales. To mitigate this issue, we propose a unified two-stage framework
known as Self-Attribution and Decision-Making (SADM). Through extensive
experiments on five reasoning datasets from the ERASER benchmark, we
demonstrate that our framework not only establishes a more reliable link
between the generated rationale and model decision but also achieves
competitive results in task performance and the quality of rationale.
Furthermore, we explore the potential of our framework in semi-supervised
scenarios.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13612" title="Abstract">arXiv:2310.13612</a> [<a href="/pdf/2310.13612" title="Download PDF">pdf</a>, <a href="/format/2310.13612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair $&#x3c9;$-Regular Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hausmann%2C+D">Daniel Hausmann</a>, 
<a href="/search/cs?searchtype=author&query=Piterman%2C+N">Nir Piterman</a>, 
<a href="/search/cs?searchtype=author&query=Sa%C4%9Flam%2C+I">Irmak Sa&#x11f;lam</a>, 
<a href="/search/cs?searchtype=author&query=Schmuck%2C+A">Anne-Kathrin Schmuck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We consider two-player games over finite graphs in which both players are
restricted by fairness constraints on their moves. Given a two player game
graph $G=(V,E)$ and a set of fair moves $E_f\subseteq E$ a player is said to
play "fair" in $G$ if they choose an edge $e \in E_f$ infinitely often whenever
the source vertex of $e$ is visited infinitely often. Otherwise, they play
"unfair". We equip such games with two $\omega$-regular winning conditions
$\alpha$ and $\beta$ deciding the winner of mutually fair and mutually unfair
plays, respectively. Whenever one player plays fair and the other plays unfair,
the fairly playing player wins the game. The resulting games are called "fair
$\alpha/\beta$ games".
<br />We formalize fair $\alpha/\beta$ games and show that they are determined. For
fair parity/parity games, i.e., fair $\alpha/\beta$ games where $\alpha$ and
$\beta$ are given each by a parity condition over $G$, we provide a polynomial
reduction to (normal) parity games via a gadget construction inspired by the
reduction of stochastic parity games to parity games. We further give a direct
symbolic fixpoint algorithm to solve fair parity/parity games. On a conceptual
level, we illustrate the translation between the gadget-based reduction and the
direct symbolic algorithm which uncovers the underlying similarities of
solution algorithms for fair and stochastic parity games, as well as for the
recently considered class of fair games where only one player is restricted by
fair moves.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13613" title="Abstract">arXiv:2310.13613</a> [<a href="/pdf/2310.13613" title="Download PDF">pdf</a>, <a href="/format/2310.13613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hunayn: Elevating Translation Beyond the Literal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almousa%2C+N">Nasser Almousa</a>, 
<a href="/search/cs?searchtype=author&query=Alzamil%2C+N">Nasser Alzamil</a>, 
<a href="/search/cs?searchtype=author&query=Alshehri%2C+A">Abdullah Alshehri</a>, 
<a href="/search/cs?searchtype=author&query=Sait%2C+A">Ahmad Sait</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This project introduces an advanced English-to-Arabic translator surpassing
conventional tools. Leveraging the Helsinki transformer (MarianMT), our
approach involves fine-tuning on a self-scraped, purely literary Arabic
dataset. Evaluations against Google Translate show consistent outperformance in
qualitative assessments. Notably, it excels in cultural sensitivity and context
accuracy. This research underscores the Helsinki transformer's superiority for
English-to-Arabic translation using a Fusha dataset.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13615" title="Abstract">arXiv:2310.13615</a> [<a href="/pdf/2310.13615" title="Download PDF">pdf</a>, <a href="/format/2310.13615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three Questions Concerning the Use of Large Language Models to  Facilitate Mathematics Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yen%2C+A">An-Zi Yen</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+W">Wei-Ling Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Due to the remarkable language understanding and generation abilities of
large language models (LLMs), their use in educational applications has been
explored. However, little work has been done on investigating the pedagogical
ability of LLMs in helping students to learn mathematics. In this position
paper, we discuss the challenges associated with employing LLMs to enhance
students' mathematical problem-solving skills by providing adaptive feedback.
Apart from generating the wrong reasoning processes, LLMs can misinterpret the
meaning of the question, and also exhibit difficulty in understanding the given
questions' rationales when attempting to correct students' answers. Three
research questions are formulated.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13617" title="Abstract">arXiv:2310.13617</a> [<a href="/pdf/2310.13617" title="Download PDF">pdf</a>, <a href="/format/2310.13617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D-Mirrorcle: Bridging the Virtual and Real through Depth Alignment in  Smart Mirror Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yujia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+Q">Qi Xin</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+C">Chenzhuo Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yingqing Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Mirrors are ubiquitous in our daily lives. Recent advancements in technology
have led to the introduction of smart mirrors that have screens integrated into
their reflective surfaces to enable enhanced user interaction. However, this
approach has led to a problem of depth disparity, as it can be difficult to see
both userpics 3D reflection in the mirror and the 2D display on the screen at
the same time, which may significantly impair the user experience of smart
mirrors. To address this depth disparity and achieve seamless mirror-based
augmented reality (AR) with better integration of screen content and mirror
imaging, we present a system named 3D-Mirrorcle. Our system uses a lenticular
grating between the screen and the half-mirror. Incorporated with real-time
image adjustment and position adaptation algorithms, it can align the parallax
of left and right views with the corresponding eyes, ensuring a precise depth
alignment between the AR displays and the mirror reflection. To validate the
effectiveness of 3D-Mirrorcle, we implemented a prototype that displays AR
contours on the face in a makeup scenario, involving 36 participants for
hands-on trials and evaluations. Experimental results demonstrate
3D-Mirrorclepics superiority over various other mirror-based AR technologies in
terms of accuracy, task completion time, immersion, and satisfaction. These
encouraging results highlight the immense potential of our method as a pivotal
technology in bringing AR to everyday life.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13618" title="Abstract">arXiv:2310.13618</a> [<a href="/pdf/2310.13618" title="Download PDF">pdf</a>, <a href="/format/2310.13618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Knowledge Proofs for Questionnaire Result Verification in Smart  Contracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quintero-Narvaez%2C+C+E">Carlos Efrain Quintero-Narvaez</a>, 
<a href="/search/cs?searchtype=author&query=Monroy-Borja%2C+R">Raul Monroy-Borja</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, presented at CISMA 2023 Guanajuato, Mexico
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">We present an implementation of a Web3 platform that leverages the Groth16
Zero-Knowledge Proof schema to verify the validity of questionnaire results
within Smart Contracts. Our approach ensures that the answer key of the
questionnaire remains undisclosed throughout the verification process, while
ensuring that the evaluation is done fairly. To accomplish this, users respond
to a series of questions, and their answers are encoded and securely
transmitted to a hidden backend. The backend then performs an evaluation of the
user's answers, generating the overall result of the questionnaire.
Additionally, it generates a Zero-Knowledge Proof, attesting that the answers
were appropriately evaluated against a valid set of constraints. Next, the user
submits their result along with the proof to a Smart Contract, which verifies
their validity and issues a non-fungible token (NFT) as an attestation of the
user's test result. In this research, we implemented the Zero-Knowledge
functionality using Circom 2 and deployed the Smart Contract using Solidity,
thereby showcasing a practical and secure solution for questionnaire validity
verification in the context of Smart Contracts.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13619" title="Abstract">arXiv:2310.13619</a> [<a href="/pdf/2310.13619" title="Download PDF">pdf</a>, <a href="/format/2310.13619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised multimodal coreference resolution in image narrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goel%2C+A">Arushi Goel</a>, 
<a href="/search/cs?searchtype=author&query=Fernando%2C+B">Basura Fernando</a>, 
<a href="/search/cs?searchtype=author&query=Keller%2C+F">Frank Keller</a>, 
<a href="/search/cs?searchtype=author&query=Bilen%2C+H">Hakan Bilen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long paper at EMNLP'23-Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we study multimodal coreference resolution, specifically where
a longer descriptive text, i.e., a narration is paired with an image. This
poses significant challenges due to fine-grained image-text alignment, inherent
ambiguity present in narrative language, and unavailability of large annotated
training sets. To tackle these challenges, we present a data efficient
semi-supervised approach that utilizes image-narration pairs to resolve
coreferences and narrative grounding in a multimodal context. Our approach
incorporates losses for both labeled and unlabeled data within a cross-modal
framework. Our evaluation shows that the proposed approach outperforms strong
baselines both quantitatively and qualitatively, for the tasks of coreference
resolution and narrative grounding.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13620" title="Abstract">arXiv:2310.13620</a> [<a href="/pdf/2310.13620" title="Download PDF">pdf</a>, <a href="/format/2310.13620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Information-Theoretic and Geometric Compression in Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+E">Emily Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Kervadec%2C+C">Corentin Kervadec</a>, 
<a href="/search/cs?searchtype=author&query=Baroni%2C+M">Marco Baroni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Camera-Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">For a language model (LM) to faithfully model human language, it must
compress vast, potentially infinite information into relatively few dimensions.
We propose analyzing compression in (pre-trained) LMs from two points of view:
geometric and information-theoretic. We demonstrate that the two views are
highly correlated, such that the intrinsic geometric dimension of linguistic
data predicts their coding length under the LM. We then show that, in turn,
high compression of a linguistic dataset predicts rapid adaptation to that
dataset, confirming that being able to compress linguistic information is an
important part of successful LM performance. As a practical byproduct of our
analysis, we evaluate a battery of intrinsic dimension estimators for the first
time on linguistic data, showing that only some encapsulate the relationship
between information-theoretic compression, geometric compression, and
ease-of-adaptation.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13622" title="Abstract">arXiv:2310.13622</a> [<a href="/pdf/2310.13622" title="Download PDF">pdf</a>, <a href="/format/2310.13622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What you see is what you get: Experience ranking with deep neural  dataset-to-dataset similarity for topological localisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gadd%2C+M">Matthew Gadd</a>, 
<a href="/search/cs?searchtype=author&query=Ramtoula%2C+B">Benjamin Ramtoula</a>, 
<a href="/search/cs?searchtype=author&query=De+Martini%2C+D">Daniele De Martini</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+P">Paul Newman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18th International Symposium on Experimental Robotics (ISER 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Recalling the most relevant visual memories for localisation or understanding
a priori the likely outcome of localisation effort against a particular visual
memory is useful for efficient and robust visual navigation. Solutions to this
problem should be divorced from performance appraisal against ground truth - as
this is not available at run-time - and should ideally be based on
generalisable environmental observations. For this, we propose applying the
recently developed Visual DNA as a highly scalable tool for comparing datasets
of images - in this work, sequences of map and live experiences. In the case of
localisation, important dataset differences impacting performance are modes of
appearance change, including weather, lighting, and season. Specifically, for
any deep architecture which is used for place recognition by matching feature
volumes at a particular layer, we use distribution measures to compare
neuron-wise activation statistics between live images and multiple previously
recorded past experiences, with a potentially large seasonal (winter/summer) or
time of day (day/night) shift. We find that differences in these statistics
correlate to performance when localising using a past experience with the same
appearance gap. We validate our approach over the Nordland cross-season dataset
as well as data from Oxford's University Parks with lighting and mild seasonal
change, showing excellent ability of our system to rank actual localisation
performance across candidate experiences.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13625" title="Abstract">arXiv:2310.13625</a> [<a href="/pdf/2310.13625" title="Download PDF">pdf</a>, <a href="/format/2310.13625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oversight for Frontier AI through a Know-Your-Customer Scheme for  Compute Providers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Egan%2C+J">Janet Egan</a>, 
<a href="/search/cs?searchtype=author&query=Heim%2C+L">Lennart Heim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">To address security and safety risks stemming from highly capable artificial
intelligence (AI) models, we propose that the US government should ensure
compute providers implement Know-Your-Customer (KYC) schemes. Compute - the
computational power and infrastructure required to train and run these AI
models - is emerging as a node for oversight. KYC, a standard developed by the
banking sector to identify and verify client identity, could provide a
mechanism for greater public oversight of frontier AI development and close
loopholes in existing export controls. Such a scheme has the potential to
identify and warn stakeholders of potentially problematic and/or sudden
advancements in AI capabilities, build government capacity for AI regulation,
and allow for the development and implementation of more nuanced and targeted
export controls. Unlike the strategy of limiting access to AI chip purchases,
regulating the digital access to compute offers more precise controls, allowing
regulatory control over compute quantities, as well as the flexibility to
suspend access at any time. To enact a KYC scheme, the US government will need
to work closely with industry to (1) establish a dynamic threshold of compute
that effectively captures high-risk frontier model development, while
minimizing imposition on developers not engaged in frontier AI; (2) set
requirements and guidance for compute providers to keep records and report
high-risk entities; (3) establish government capacity that allows for
co-design, implementation, administration and enforcement of the scheme; and
(4) engage internationally to promote international alignment with the scheme
and support its long-term efficacy. While the scheme will not address all AI
risks, it complements proposed solutions by allowing for a more precise and
flexible approach to controlling the development of frontier AI models and
unwanted AI proliferation.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13627" title="Abstract">arXiv:2310.13627</a> [<a href="/pdf/2310.13627" title="Download PDF">pdf</a>, <a href="/format/2310.13627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep-Learning-based Change Detection with Spaceborne Hyperspectral  PRISMA data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amieva%2C+J+F">J.F. Amieva</a>, 
<a href="/search/cs?searchtype=author&query=Austoni%2C+A">A. Austoni</a>, 
<a href="/search/cs?searchtype=author&query=Brovelli%2C+M+A">M.A. Brovelli</a>, 
<a href="/search/cs?searchtype=author&query=Ansalone%2C+L">L. Ansalone</a>, 
<a href="/search/cs?searchtype=author&query=Naylor%2C+P">P. Naylor</a>, 
<a href="/search/cs?searchtype=author&query=Serva%2C+F">F. Serva</a>, 
<a href="/search/cs?searchtype=author&query=Saux%2C+B+L">B. Le Saux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Big Data from Space 2023 (BiDS); 4 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Change detection (CD) methods have been applied to optical data for decades,
while the use of hyperspectral data with a fine spectral resolution has been
rarely explored. CD is applied in several sectors, such as environmental
monitoring and disaster management. Thanks to the PRecursore IperSpettrale
della Missione operativA (PRISMA), hyperspectral-from-space CD is now possible.
In this work, we apply standard and deep-learning (DL) CD methods to different
targets, from natural to urban areas. We propose a pipeline starting from
coregistration, followed by CD with a full-spectrum algorithm and by a DL
network developed for optical data. We find that changes in vegetation and
built environments are well captured. The spectral information is valuable to
identify subtle changes and the DL methods are less affected by noise compared
to the statistical method, but atmospheric effects and the lack of reliable
ground truth represent a major challenge to hyperspectral CD.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13637" title="Abstract">arXiv:2310.13637</a> [<a href="/pdf/2310.13637" title="Download PDF">pdf</a>, <a href="/format/2310.13637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application Performance Benchmarks for Quantum Computers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurowski%2C+K">Krzysztof Kurowski</a>, 
<a href="/search/cs?searchtype=author&query=Rydlichowski%2C+P">Piotr Rydlichowski</a>, 
<a href="/search/cs?searchtype=author&query=Wojciechowski%2C+K">Konrad Wojciechowski</a>, 
<a href="/search/cs?searchtype=author&query=Pecyna%2C+T">Tomasz Pecyna</a>, 
<a href="/search/cs?searchtype=author&query=Slysz%2C+M">Mateusz Slysz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Current technological advancements of quantum computers highlight the need
for application-driven, practical and well-defined methods of benchmarking
their performance. As the existing NISQ device's quality of two-qubit gate
errors rate is even around one percent and the number of qubits is still
limited to a few or several dozen, naturally, we need to propose rather small
algorithms instances taken from key promising application areas, such as
quantum chemistry, combinatorial optimisation or machine learning. While many
techniques for assessing the performance of logical components such as gate
fidelity and qubit coherence exist, it is often challenging to extrapolate
those values onto the performance of different quantum algorithms and
subroutines. This work aims to introduce a series of initial quantum
application benchmarks together with a methodology of execution for measuring
performance and fidelity of the results. The proposed suite refers to several
variational algorithms, widely-used on current NISQ devices, but also includes
examples of quantum circuits designed for a fault-tolerant quantum computer.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13639" title="Abstract">arXiv:2310.13639</a> [<a href="/pdf/2310.13639" title="Download PDF">pdf</a>, <a href="/format/2310.13639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Prefence Learning: Learning from Human Feedback without RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hejna%2C+J">Joey Hejna</a>, 
<a href="/search/cs?searchtype=author&query=Rafailov%2C+R">Rafael Rafailov</a>, 
<a href="/search/cs?searchtype=author&query=Sikchi%2C+H">Harshit Sikchi</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Niekum%2C+S">Scott Niekum</a>, 
<a href="/search/cs?searchtype=author&query=Knox%2C+W+B">W. Bradley Knox</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code released at <a href="https://github.com/jhejna/cpl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular
paradigm for aligning models with human intent. Typically RLHF algorithms
operate in two phases: first, use human preferences to learn a reward function
and second, align the model by optimizing the learned reward via reinforcement
learning (RL). This paradigm assumes that human preferences are distributed
according to reward, but recent work suggests that they instead follow the
regret under the user's optimal policy. Thus, learning a reward function from
feedback is not only based on a flawed assumption of human preference, but also
leads to unwieldy optimization challenges that stem from policy gradients or
bootstrapping in the RL phase. Because of these optimization challenges,
contemporary RLHF methods restrict themselves to contextual bandit settings
(e.g., as in large language models) or limit observation dimensionality (e.g.,
state-based robotics). We overcome these limitations by introducing a new
family of algorithms for optimizing behavior from human feedback using the
regret-based model of human preferences. Using the principle of maximum
entropy, we derive Contrastive Preference Learning (CPL), an algorithm for
learning optimal policies from preferences without learning reward functions,
circumventing the need for RL. CPL is fully off-policy, uses only a simple
contrastive objective, and can be applied to arbitrary MDPs. This enables CPL
to elegantly scale to high-dimensional and sequential RLHF problems while being
simpler than prior methods.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13643" title="Abstract">arXiv:2310.13643</a> [<a href="/pdf/2310.13643" title="Download PDF">pdf</a>, <a href="/format/2310.13643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Prospects and Opportunities in Disassembly with Human-Robot  Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Meng-Lun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Boyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Onel%2C+G">Gulcan Onel</a>, 
<a href="/search/cs?searchtype=author&query=Behdad%2C+S">Sara Behdad</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Minghui Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Product disassembly plays a crucial role in the recycling, remanufacturing,
and reuse of end-of-use (EoU) products. However, the current manual disassembly
process is inefficient due to the complexity and variation of EoU products.
While fully automating disassembly is not economically viable given the
intricate nature of the task, there is potential in using human-robot
collaboration (HRC) to enhance disassembly operations. HRC combines the
flexibility and problem-solving abilities of humans with the precise repetition
and handling of unsafe tasks by robots. Nevertheless, numerous challenges
persist in technology, human workers, and remanufacturing work, that require
comprehensive multidisciplinary research to bridge critical gaps. These
challenges have motivated the authors to provide a detailed discussion on the
opportunities and obstacles associated with introducing HRC to disassembly. In
this regard, the authors have conducted a thorough review of the recent
progress in HRC disassembly and present the insights gained from this analysis
from three distinct perspectives: technology, workers, and work.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13647" title="Abstract">arXiv:2310.13647</a> [<a href="/pdf/2310.13647" title="Download PDF">pdf</a>, <a href="/format/2310.13647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Loop Control Co-Design of Semisubmersible Floating Offshore Wind  Turbines using Linear Parameter-Varying Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sundarrajan%2C+A+K">Athul Krishna Sundarrajan</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+Y+H">Yong Hoon Lee</a>, 
<a href="/search/eess?searchtype=author&query=Allison%2C+J+T">James T Allison</a>, 
<a href="/search/eess?searchtype=author&query=Zalkind%2C+D">Daniel Zalkind</a>, 
<a href="/search/eess?searchtype=author&query=Herber%2C+D">Daniel Herber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages 47 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper discusses a framework to design elements of the plant and control
systems for floating offshore wind turbines in an integrated manner using
linear parameter-varying models. Multiple linearized models derived from
aeroelastic simulation software in different operating regions characterized by
the incoming wind speed are combined to construct an approximate low-fidelity
model of the system. The combined model is then used to generate open-loop,
optimal control trajectories as part of a nested control co-design strategy
that explores the system's power production and stability using the platform
pitch tilt as a proxy in the context of crucial plant and control design
decisions. The radial distance between the central and outer columns and the
diameter of the outer columns of the semisubmersible platform are the plant
design variables. The platform stability and power production are studied for
different plant design decisions. The effect of plant decisions on subsequent
power production and stability response of the floating wind turbine is
quantified in terms of the levelized cost of energy. The results show that the
inner-loop constraints and the plant design decisions affect the turbine's
power and, subsequently, the cost of the system.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13648" title="Abstract">arXiv:2310.13648</a> [<a href="/pdf/2310.13648" title="Download PDF">pdf</a>, <a href="/format/2310.13648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using ChatGPT throughout the Software Development Life Cycle by Novice  Developers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waseem%2C+M">Muhammad Waseem</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+T">Teerath Das</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+A">Aakash Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Fehmideh%2C+M">Mahdi Fehmideh</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Peng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Mikkonen%2C+T">Tommi Mikkonen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">This study investigates the impact of ChatGPT -- a generative AI-based tool
-- on undergraduate students' software development experiences. Through a
three-month project involving seven undergraduate students, ChatGPT was
employed as a supporting tool, and their experiences were systematically
surveyed before and after the projects. The research aims to answer four key
questions related to ChatGPT's effectiveness, advantages, limitations, impact
on learning, and challenges faced. The findings revealed significant skill gaps
among undergraduate students, underscoring the importance of addressing
educational deficiencies in software development. ChatGPT was found to have a
positive influence on various phases of the software development life cycle,
leading to enhanced efficiency, accuracy, and collaboration. ChatGPT also
consistently improved participants' foundational understanding and soft skills
in software development. These findings underscore the significance of
integrating AI tools like ChatGPT into undergraduate students education,
particularly to bridge skill gaps and enhance productivity. However, a nuanced
approach to technology reliance is essential, acknowledging the variability in
opinions and the need for customization. Future research should explore
strategies to optimize ChatGPT's application across development contexts,
ensuring it maximizes learning while addressing specific challenges.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13650" title="Abstract">arXiv:2310.13650</a> [<a href="/pdf/2310.13650" title="Download PDF">pdf</a>, <a href="/format/2310.13650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BotChat: Evaluating LLMs&#x27; Capabilities of Having Multi-Turn Dialogues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Haodong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jueqi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chonghua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yixiao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Interacting with human via high-quality multi-turn dialogues is a key feature
of large language models (LLMs). However, human-based evaluation of such
capability involves intensive manual labor. This report provides a preliminary
evaluation of existing large language models for human-style multi-turn
chatting, through an LLM-based approach. We start from real-world human
dialogues and keep the very first utterances as the ChatSEED. Then we prompt
LLMs to generate a full multi-turn dialogue (tens of utterances) based on the
ChatSEED, utterance by utterance. Finally, we adopt state-of-the-art LLMs
(GPT-4, \etc) as the judge to evaluate the generated dialogues. With different
evaluation protocols, we come to substantially identical conclusions. We find
that GPT-4 can generate human-style multi-turn dialogues with impressive
quality, significantly outperforms its counterparts. It's difficult for a
discriminator to distinguish between GPT-4 generated dialogues and human
dialogues. In contrast, other LLMs struggle to generate multi-turn dialogues of
satisfactory quality due to poor instruction-following capability, tendency to
generate lengthy utterances, or limited general capability. All data and codes
will be provided in https://github.com/open-compass/BotChat/ and we hope they
can serve as a valuable resource for evaluating multi-turn chatting
capabilities of LLMs.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13654" title="Abstract">arXiv:2310.13654</a> [<a href="/pdf/2310.13654" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An experimental study for early diagnosing Parkinson&#x27;s disease using  machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tusar%2C+M+T+H+K">Md. Taufiqul Haque Khan Tusar</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+T">Md. Touhidul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Sakil%2C+A+H">Abul Hasnat Sakil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">One of the most catastrophic neurological disorders worldwide is Parkinson's
Disease. Along with it, the treatment is complicated and abundantly expensive.
The only effective action to control the progression is diagnosing it in the
early stage. However, this is challenging because early detection necessitates
a large and complex clinical study. This experimental work used Machine
Learning techniques to automate the early detection of Parkinson's Disease from
clinical characteristics, voice features and motor examination. In this study,
we develop ML models utilizing a public dataset of 130 individuals, 30 of whom
are untreated Parkinson's Disease patients, 50 of whom are Rapid Eye Movement
Sleep Behaviour Disorder patients who are at a greater risk of contracting
Parkinson's Disease, and 50 of whom are Healthy Controls. We use MinMax Scaler
to rescale the data points, Local Outlier Factor to remove outliers, and SMOTE
to balance existing class frequency. Afterwards, apply a number of Machine
Learning techniques. We implement the approaches in such a way that data
leaking and overfitting are not possible. Finally, obtained 100% accuracy in
classifying PD and RBD patients, as well as 92% accuracy in classifying PD and
HC individuals.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13655" title="Abstract">arXiv:2310.13655</a> [<a href="/pdf/2310.13655" title="Download PDF">pdf</a>, <a href="/format/2310.13655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Robust Control Contraction Metrics: Transient Bounds in  Adaptive Control with Unmatched Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gessow%2C+S+G">Samuel G. Gessow</a>, 
<a href="/search/eess?searchtype=author&query=Lopez%2C+B+T">Brett T. Lopez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This work presents a new sufficient condition for synthesizing nonlinear
controllers that yield bounded closed-loop tracking error transients despite
the presence of unmatched uncertainties that are concurrently being learned
online. The approach utilizes contraction theory and addresses fundamental
limitations of existing approaches by allowing the contraction metric to depend
on the unknown model parameters. This allows the controller to incorporate new
model estimates generated online without sacrificing its strong convergence and
bounded transients guarantees. The approach is specifically designed for
trajectory tracking so the approach is more broadly applicable to adaptive
model predictive control as well. Simulation results on a nonlinear system with
unmatched uncertainties demonstrates the approach.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13659" title="Abstract">arXiv:2310.13659</a> [<a href="/pdf/2310.13659" title="Download PDF">pdf</a>, <a href="/format/2310.13659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking and Improving Text-to-SQL Generation under Ambiguity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhaskar%2C+A">Adithya Bhaskar</a>, 
<a href="/search/cs?searchtype=author&query=Tomar%2C+T">Tushar Tomar</a>, 
<a href="/search/cs?searchtype=author&query=Sathe%2C+A">Ashutosh Sathe</a>, 
<a href="/search/cs?searchtype=author&query=Sarawagi%2C+S">Sunita Sarawagi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023 (Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Research in Text-to-SQL conversion has been largely benchmarked against
datasets where each text query corresponds to one correct SQL. However, natural
language queries over real-life databases frequently involve significant
ambiguity about the intended SQL due to overlapping schema names and multiple
confusing relationship paths. To bridge this gap, we develop a novel benchmark
called AmbiQT with over 3000 examples where each text is interpretable as two
plausible SQLs due to lexical and/or structural ambiguity.
<br />When faced with ambiguity, an ideal top-$k$ decoder should generate all valid
interpretations for possible disambiguation by the user. We evaluate several
Text-to-SQL systems and decoding algorithms, including those employing
state-of-the-art LLMs, and find them to be far from this ideal. The primary
reason is that the prevalent beam search algorithm and its variants, treat SQL
queries as a string and produce unhelpful token-level diversity in the top-$k$.
<br />We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic
space using a blend of plan-based template generation and constrained
infilling. Counterfactually generated plans diversify templates while
in-filling with a beam-search that branches solely on schema names provides
value diversity. LogicalBeam is up to $2.5$ times more effective than
state-of-the-art models at generating all candidate SQLs in the top-$k$ ranked
outputs. It also enhances the top-$5$ Exact and Execution Match Accuracies on
SPIDER and Kaggle DBQA.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13661" title="Abstract">arXiv:2310.13661</a> [<a href="/pdf/2310.13661" title="Download PDF">pdf</a>, <a href="/format/2310.13661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arabic Dialect Identification under Scrutiny: Limitations of  Single-label Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keleg%2C+A">Amr Keleg</a>, 
<a href="/search/cs?searchtype=author&query=Magdy%2C+W">Walid Magdy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the ArabicNLP 2023 conference co-located with EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic Arabic Dialect Identification (ADI) of text has gained great
popularity since it was introduced in the early 2010s. Multiple datasets were
developed, and yearly shared tasks have been running since 2018. However, ADI
systems are reported to fail in distinguishing between the micro-dialects of
Arabic. We argue that the currently adopted framing of the ADI task as a
single-label classification problem is one of the main reasons for that. We
highlight the limitation of the incompleteness of the Dialect labels and
demonstrate how it impacts the evaluation of ADI systems. A manual error
analysis for the predictions of an ADI, performed by 7 native speakers of
different Arabic dialects, revealed that $\approx$ 66% of the validated errors
are not true errors. Consequently, we propose framing ADI as a multi-label
classification task and give recommendations for designing new ADI datasets.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13664" title="Abstract">arXiv:2310.13664</a> [<a href="/pdf/2310.13664" title="Download PDF">pdf</a>, <a href="/format/2310.13664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainability, Interpretability, Depression detection, Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Souto%2C+E+B">Eliseo Bao Souto</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+A">Anxo P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Parapar%2C+J">Javier Parapar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Users of social platforms often perceive these sites as supportive spaces to
post about their mental health issues. Those conversations contain important
traces about individuals' health risks. Recently, researchers have exploited
this online information to construct mental health detection models, which aim
to identify users at risk on platforms like Twitter, Reddit or Facebook. Most
of these models are centred on achieving good classification results, ignoring
the explainability and interpretability of the decisions. Recent research has
pointed out the importance of using clinical markers, such as the use of
symptoms, to improve trust in the computational models by health professionals.
In this paper, we propose using transformer-based architectures to detect and
explain the appearance of depressive symptom markers in the users' writings. We
present two approaches: $i)$ train a model to classify, and another one to
explain the classifier's decision separately and $ii)$ unify the two tasks
simultaneously using a single model. Additionally, for this latter manner, we
also investigated the performance of recent conversational LLMs when using
in-context learning. Our natural language explanations enable clinicians to
interpret the models' decisions based on validated symptoms, enhancing trust in
the automated process. We evaluate our approach using recent symptom-based
datasets, employing both offline and expert-in-the-loop metrics to assess the
quality of the explanations generated by our models. The experimental results
show that it is possible to achieve good classification results while
generating interpretable symptom-based explanations.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13667" title="Abstract">arXiv:2310.13667</a> [<a href="/pdf/2310.13667" title="Download PDF">pdf</a>, <a href="/format/2310.13667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EXPLORA: AI/ML EXPLainability for the Open RAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fiandrino%2C+C">Claudio Fiandrino</a>, 
<a href="/search/cs?searchtype=author&query=Bonati%2C+L">Leonardo Bonati</a>, 
<a href="/search/cs?searchtype=author&query=D%27Oro%2C+S">Salvatore D&#x27;Oro</a>, 
<a href="/search/cs?searchtype=author&query=Polese%2C+M">Michele Polese</a>, 
<a href="/search/cs?searchtype=author&query=Melodia%2C+T">Tommaso Melodia</a>, 
<a href="/search/cs?searchtype=author&query=Widmer%2C+J">Joerg Widmer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The Open Radio Access Network (RAN) paradigm is transforming cellular
networks into a system of disaggregated, virtualized, and software-based
components. These self-optimize the network through programmable, closed-loop
control, leveraging Artificial Intelligence (AI) and Machine Learning (ML)
routines. In this context, Deep Reinforcement Learning (DRL) has shown great
potential in addressing complex resource allocation problems. However, DRL
-based solutions are inherently hard to explain, which hinders their deployment
and use in practice. In this paper, we propose EXPLORA, a framework that
provides explainability of DRL-based control solutions for the Open RAN
ecosystem. EXPLORA synthesizes network-oriented explanations based on an
attributed graph that produces a link between the actions taken by a DRL agent
(i.e., the nodes of the graph) and the input state space (i.e., the attributes
of each node). This novel approach allows EXPLORA to explain models by
providing information on the wireless context in which the DRL agent operates.
EXPLORA is also designed to be lightweight for real-time operation. We
prototype EXPLORA and test it experimentally on an O-RAN-compliant
near-real-time RIC deployed on the Colosseum wireless network emulator. We
evaluate EXPLORA for agents trained for different purposes and showcase how it
generates clear network-oriented explanations. We also show how explanations
can be used to perform informative and targeted intent-based action steering
and achieve median transmission bitrate improvements of 4% and tail
improvements of 10%.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13669" title="Abstract">arXiv:2310.13669</a> [<a href="/pdf/2310.13669" title="Download PDF">pdf</a>, <a href="/format/2310.13669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Unit Test Data Generation and Actor-Critic Reinforcement  Learning for Code Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorinski%2C+P+J">Philip John Gorinski</a>, 
<a href="/search/cs?searchtype=author&query=Zimmer%2C+M">Matthieu Zimmer</a>, 
<a href="/search/cs?searchtype=author&query=Lampouras%2C+G">Gerasimos Lampouras</a>, 
<a href="/search/cs?searchtype=author&query=Deik%2C+D+G+X">Derrick Goh Xin Deik</a>, 
<a href="/search/cs?searchtype=author&query=Iacobacci%2C+I">Ignacio Iacobacci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages + 4 pages appendix; 4 Figures, 4 Tables, 1 Algorithm; Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
<p class="mathjax">The advent of large pre-trained language models in the domain of Code
Synthesis has shown remarkable performance on various benchmarks, treating the
problem of Code Generation in a fashion similar to Natural Language Generation,
trained with a Language Modelling (LM) objective. In addition, the property of
programming language code being precisely evaluable with respect to its
semantics -- through the use of Unit Tests to check its functional correctness
-- lends itself to using Reinforcement Learning (RL) as a further training
paradigm. Previous work has shown that RL can be applied as such to improve
models' coding capabilities; however, such RL-based methods rely on a reward
signal based on defined Unit Tests, which are much harder to obtain compared to
the huge crawled code datasets used in LM objectives. In this work, we present
a novel approach to automatically obtain data consisting of function signatures
and associated Unit Tests, suitable for RL training of Code Synthesis models.
We also introduce a straightforward, simple yet effective Actor-Critic RL
training scheme and show that it, in conjunction with automatically generated
training data, leads to improvement of a pre-trained code language model's
performance by up to 9.9% improvement over the original underlying code
synthesis LM, and up to 4.3% over RL-based models trained with standard PPO or
CodeRL.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13670" title="Abstract">arXiv:2310.13670</a> [<a href="/pdf/2310.13670" title="Download PDF">pdf</a>, <a href="/format/2310.13670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ManifoldNeRF: View-dependent Image Feature Supervision for Few-shot  Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kanaoka%2C+D">Daiju Kanaoka</a>, 
<a href="/search/cs?searchtype=author&query=Sonogashira%2C+M">Motoharu Sonogashira</a>, 
<a href="/search/cs?searchtype=author&query=Tamukoh%2C+H">Hakaru Tamukoh</a>, 
<a href="/search/cs?searchtype=author&query=Kawanishi%2C+Y">Yasutomo Kawanishi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by BMVC2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Novel view synthesis has recently made significant progress with the advent
of Neural Radiance Fields (NeRF). DietNeRF is an extension of NeRF that aims to
achieve this task from only a few images by introducing a new loss function for
unknown viewpoints with no input images. The loss function assumes that a
pre-trained feature extractor should output the same feature even if input
images are captured at different viewpoints since the images contain the same
object. However, while that assumption is ideal, in reality, it is known that
as viewpoints continuously change, also feature vectors continuously change.
Thus, the assumption can harm training. To avoid this harmful training, we
propose ManifoldNeRF, a method for supervising feature vectors at unknown
viewpoints using interpolated features from neighboring known viewpoints. Since
the method provides appropriate supervision for each unknown viewpoint by the
interpolated features, the volume representation is learned better than
DietNeRF. Experimental results show that the proposed method performs better
than others in a complex scene. We also experimented with several subsets of
viewpoints from a set of viewpoints and identified an effective set of
viewpoints for real environments. This provided a basic policy of viewpoint
patterns for real-world application. The code is available at
https://github.com/haganelego/ManifoldNeRF_BMVC2023
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13671" title="Abstract">arXiv:2310.13671</a> [<a href="/pdf/2310.13671" title="Download PDF">pdf</a>, <a href="/format/2310.13671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let&#x27;s Synthesize Step by Step: Iterative Dataset Synthesis with Large  Language Models by Extrapolating Errors from Small Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruida Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023(Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">*Data Synthesis* is a promising way to train a small model with very little
labeled data. One approach for data synthesis is to leverage the rich knowledge
from large language models to synthesize pseudo training examples for small
models, making it possible to achieve both data and compute efficiency at the
same time. However, a key challenge in data synthesis is that the synthesized
dataset often suffers from a large distributional discrepancy from the *real
task* data distribution. Thus, in this paper, we propose *Synthesis Step by
Step* (**S3**), a data synthesis framework that shrinks this distribution gap
by iteratively extrapolating the errors made by a small model trained on the
synthesized dataset on a small real-world validation dataset using a large
language model. Extensive experiments on multiple NLP tasks show that our
approach improves the performance of a small model by reducing the gap between
the synthetic dataset and the real data, resulting in significant improvement
compared to several baselines: 9.48% improvement compared to ZeroGen and 2.73%
compared to GoldGen, and at most 15.17% improvement compared to the small model
trained on human-annotated data.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13673" title="Abstract">arXiv:2310.13673</a> [<a href="/pdf/2310.13673" title="Download PDF">pdf</a>, <a href="/format/2310.13673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeoung%2C+S">Sullam Jeoung</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yubin Ge</a>, 
<a href="/search/cs?searchtype=author&query=Diesner%2C+J">Jana Diesner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have been observed to encode and perpetuate
harmful associations present in the training data. We propose a theoretically
grounded framework called StereoMap to gain insights into their perceptions of
how demographic groups have been viewed by society. The framework is grounded
in the Stereotype Content Model (SCM); a well-established theory from
psychology. According to SCM, stereotypes are not all alike. Instead, the
dimensions of Warmth and Competence serve as the factors that delineate the
nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs'
perceptions of social groups (defined by socio-demographic features) using the
dimensions of Warmth and Competence. Furthermore, the framework enables the
investigation of keywords and verbalizations of reasoning of LLMs' judgments to
uncover underlying factors influencing their perceptions. Our results show that
LLMs exhibit a diverse range of perceptions towards these groups, characterized
by mixed evaluations along the dimensions of Warmth and Competence.
Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs
demonstrate an awareness of social disparities, often stating statistical data
and research findings to support their reasoning. This study contributes to the
understanding of how LLMs perceive and represent social groups, shedding light
on their potential biases and the perpetuation of harmful associations.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13674" title="Abstract">arXiv:2310.13674</a> [<a href="/pdf/2310.13674" title="Download PDF">pdf</a>, <a href="/format/2310.13674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Human-like Mechanism to Weaken Effect of Pre-training Weight Bias  in Face-Recognition Convolutional Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+H">Haojiang Ying</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yi-Fan Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiyang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Convolutional neural network (CNN), as an important model in artificial
intelligence, has been widely used and studied in different disciplines. The
computational mechanisms of CNNs are still not fully revealed due to the their
complex nature. In this study, we focused on 4 extensively studied CNNs
(AlexNet, VGG11, VGG13, and VGG16) which has been analyzed as human-like models
by neuroscientists with ample evidence. We trained these CNNs to emotion
valence classification task by transfer learning. Comparing their performance
with human data, the data unveiled that these CNNs would partly perform as
human does. We then update the object-based AlexNet using self-attention
mechanism based on neuroscience and behavioral data. The updated FE-AlexNet
outperformed all the other tested CNNs and closely resembles human perception.
The results further unveil the computational mechanisms of these CNNs.
Moreover, this study offers a new paradigm to better understand and improve CNN
performance via human data.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13675" title="Abstract">arXiv:2310.13675</a> [<a href="/pdf/2310.13675" title="Download PDF">pdf</a>, <a href="/format/2310.13675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Synthetic Data for Back Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiahao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+Y">Yubin Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Guoping Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lihui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lemao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Back translation (BT) is one of the most significant technologies in NMT
research fields. Existing attempts on BT share a common characteristic: they
employ either beam search or random sampling to generate synthetic data with a
backward model but seldom work studies the role of synthetic data in the
performance of BT. This motivates us to ask a fundamental question: {\em what
kind of synthetic data contributes to BT performance?} Through both theoretical
and empirical studies, we identify two key factors on synthetic data
controlling the back-translation NMT performance, which are quality and
importance. Furthermore, based on our findings, we propose a simple yet
effective method to generate synthetic data to better trade off both factors so
as to yield a better performance for BT. We run extensive experiments on WMT14
DE-EN, EN-DE, and RU-EN benchmark tasks. By employing our proposed method to
generate synthetic data, our BT model significantly outperforms the standard BT
baselines (i.e., beam and sampling based methods for data generation), which
proves the effectiveness of our proposed methods.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13676" title="Abstract">arXiv:2310.13676</a> [<a href="/pdf/2310.13676" title="Download PDF">pdf</a>, <a href="/format/2310.13676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Value: Measuring Utterance Predictability as Distance from  Plausible Alternatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giulianelli%2C+M">Mario Giulianelli</a>, 
<a href="/search/cs?searchtype=author&query=Wallbridge%2C+S">Sarenne Wallbridge</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+R">Raquel Fern&#xe1;ndez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Main, Long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present information value, a measure which quantifies the predictability
of an utterance relative to a set of plausible alternatives. We introduce a
method to obtain interpretable estimates of information value using neural text
generators, and exploit their psychometric predictive power to investigate the
dimensions of predictability that drive human comprehension behaviour.
Information value is a stronger predictor of utterance acceptability in written
and spoken dialogue than aggregates of token-level surprisal and it is
complementary to surprisal for predicting eye-tracked reading times.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13678" title="Abstract">arXiv:2310.13678</a> [<a href="/pdf/2310.13678" title="Download PDF">pdf</a>, <a href="/format/2310.13678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Long-form Speech Translation through Segmentation with Large  Language Models and Finite State Decoding Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCarthy%2C+A+D">Arya D. McCarthy</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Shankar Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Stahlberg%2C+F">Felix Stahlberg</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Ke Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to the Findings of EMNLP 2023. arXiv admin note: text overlap with <a href="/abs/2212.09895">arXiv:2212.09895</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">One challenge in spoken language translation is that plenty of spoken content
is long-form, but short units are necessary for obtaining high-quality
translations. To address this mismatch, we adapt large language models (LLM) to
split long ASR transcripts into segments that can be independently translated
so as to maximize the overall translation quality. To combat the tendency of
hallucination by LLMs, we incorporate finite-state constraints during decoding
to eliminate invalid outputs. We discover that LLMs are adaptable to
transcripts containing ASR errors through prompt-tuning or fine-tuning. In
comparison to a state-of-the-art automatic punctuation baseline, our best LLM
improves the average BLEU for English-German, English-Spanish, and
English-Arabic TED talk translation in 9 test sets by 2.9 points, just by
improving segmentation.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13681" title="Abstract">arXiv:2310.13681</a> [<a href="/pdf/2310.13681" title="Download PDF">pdf</a>, <a href="/format/2310.13681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RealFM: A Realistic Mechanism to Incentivize Data Contribution and  Device Participation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bornstein%2C+M">Marco Bornstein</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>, 
<a href="/search/cs?searchtype=author&query=Sahu%2C+A+K">Anit Kumar Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F">Furqan Khan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computers and Society (cs.CY); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Theoretical Economics (econ.TH)

</div>
<p class="mathjax">Edge device participation in federating learning (FL) has been typically
studied under the lens of device-server communication (e.g., device dropout)
and assumes an undying desire from edge devices to participate in FL. As a
result, current FL frameworks are flawed when implemented in real-world
settings, with many encountering the free-rider problem. In a step to push FL
towards realistic settings, we propose RealFM: the first truly federated
mechanism which (1) realistically models device utility, (2) incentivizes data
contribution and device participation, and (3) provably removes the free-rider
phenomena. RealFM does not require data sharing and allows for a non-linear
relationship between model accuracy and utility, which improves the utility
gained by the server and participating devices compared to non-participating
devices as well as devices participating in other FL mechanisms. On real-world
data, RealFM improves device and server utility, as well as data contribution,
by up to 3 magnitudes and 7x respectively compared to baseline mechanisms.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13682" title="Abstract">arXiv:2310.13682</a> [<a href="/pdf/2310.13682" title="Download PDF">pdf</a>, <a href="/format/2310.13682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Retrieval-augmented Reader Models via Token Elimination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berchansky%2C+M">Moshe Berchansky</a>, 
<a href="/search/cs?searchtype=author&query=Izsak%2C+P">Peter Izsak</a>, 
<a href="/search/cs?searchtype=author&query=Caciularu%2C+A">Avi Caciularu</a>, 
<a href="/search/cs?searchtype=author&query=Dagan%2C+I">Ido Dagan</a>, 
<a href="/search/cs?searchtype=author&query=Wasserblat%2C+M">Moshe Wasserblat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model
applied across a variety of open-domain tasks, such as question answering, fact
checking, etc. In FiD, supporting passages are first retrieved and then
processed using a generative model (Reader), which can cause a significant
bottleneck in decoding time, particularly with long outputs. In this work, we
analyze the contribution and necessity of all the retrieved passages to the
performance of reader models, and propose eliminating some of the retrieved
information, at the token level, that might not contribute essential
information to the answer generation process. We demonstrate that our method
can reduce run-time by up to 62.2%, with only a 2% reduction in performance,
and in some cases, even improve the performance results.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13683" title="Abstract">arXiv:2310.13683</a> [<a href="/pdf/2310.13683" title="Download PDF">pdf</a>, <a href="/format/2310.13683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP  Performance on Low-Resource Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+G+O+d">Gabriel Oliveira dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Moreia%2C+D+A">Diego Alysson Moreia</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+A+I">Alef Iury Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+J">Jhessica Silva</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+L">Luiz Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Bueno%2C+P">Pedro Bueno</a>, 
<a href="/search/cs?searchtype=author&query=Sousa%2C+T">Thiago Sousa</a>, 
<a href="/search/cs?searchtype=author&query=Maia%2C+H">Helena Maia</a>, 
<a href="/search/cs?searchtype=author&query=Da+Silva%2C+N">N&#xe1;dia Da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Colombini%2C+E">Esther Colombini</a>, 
<a href="/search/cs?searchtype=author&query=Pedrini%2C+H">Helio Pedrini</a>, 
<a href="/search/cs?searchtype=author&query=Avila%2C+S">Sandra Avila</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This work introduces CAPIVARA, a cost-efficient framework designed to enhance
the performance of multilingual CLIP models in low-resource languages. While
CLIP has excelled in zero-shot vision-language tasks, the resource-intensive
nature of model training remains challenging. Many datasets lack linguistic
diversity, featuring solely English descriptions for images. CAPIVARA addresses
this by augmenting text data using image captioning and machine translation to
generate multiple synthetic captions in low-resource languages. We optimize the
training pipeline with LiT, LoRA, and gradient checkpointing to alleviate the
computational cost. Through extensive experiments, CAPIVARA emerges as state of
the art in zero-shot tasks involving images and Portuguese texts. We show the
potential for significant improvements in other low-resource languages,
achieved by fine-tuning the pre-trained multilingual CLIP using CAPIVARA on a
single GPU for 2 hours. Our model and code is available at
https://github.com/hiaac-nlp/CAPIVARA.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13686" title="Abstract">arXiv:2310.13686</a> [<a href="/pdf/2310.13686" title="Download PDF">pdf</a>, <a href="/format/2310.13686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Linguistic Probes for Morphological Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kodner%2C+J">Jordan Kodner</a>, 
<a href="/search/cs?searchtype=author&query=Khalifa%2C+S">Salam Khalifa</a>, 
<a href="/search/cs?searchtype=author&query=Payne%2C+S">Sarah Payne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Modern work on the cross-linguistic computational modeling of morphological
inflection has typically employed language-independent data splitting
algorithms. In this paper, we supplement that approach with language-specific
probes designed to test aspects of morphological generalization. Testing these
probes on three morphologically distinct languages, English, Spanish, and
Swahili, we find evidence that three leading morphological inflection systems
employ distinct generalization strategies over conjugational classes and
feature sets on both orthographic and phonologically transcribed inputs.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13691" title="Abstract">arXiv:2310.13691</a> [<a href="/pdf/2310.13691" title="Download PDF">pdf</a>, <a href="/format/2310.13691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural-Base Music Generation for Intelligence Duplication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galajda%2C+J">Jacob Galajda</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+K">Kien Hua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">There are two aspects of machine learning and artificial intelligence: (1)
interpreting information, and (2) inventing new useful information. Much
advance has been made for (1) with a focus on pattern recognition techniques
(e.g., interpreting visual data). This paper focuses on (2) with intelligent
duplication (ID) for invention. We explore the possibility of learning a
specific individual's creative reasoning in order to leverage the learned
expertise and talent to invent new information. More specifically, we employ a
deep learning system to learn from the great composer Beethoven and capture his
composition ability in a hash-based knowledge base. This new form of knowledge
base provides a reasoning facility to drive the music composition through a
novel music generation method.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Mon, 23 Oct 23</h3>
<dl>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09635" title="Abstract">arXiv:2310.09635</a> (cross-list from quant-ph) [<a href="/pdf/2310.09635" title="Download PDF">pdf</a>, <a href="/ps/2310.09635" title="Download PostScript">ps</a>, <a href="/format/2310.09635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On superqubits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Duplij%2C+S">Steven Duplij</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vogl%2C+R">Raimund Vogl</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In book "Innovative Quantum Computing" IOP Publishing (Bristol,
  2023) by S. Duplij, R. Vogl (as Chapter 3), ISBN: 9780750352796,
  https://store.ioppublishing.org/page/detail/Innovative-Quantum-Computing/?K=9780750352796
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); High Energy Physics - Theory (hep-th); Mathematical Physics (math-ph)

</div>
<p class="mathjax">We first reconsider the mathematical background of superqubit theory and
describe important peculiarities of superspaces and supermatrices which are
usually out of attention. Then we study states in super Hilbert spaces using
super-bra/super-ket formalism in details. The qubit (qudit) and superqubit
(superqudit) are defined as linear spans in the corresponding Hilbert
subspaces. A new kind of superqubit carring the odd parity is introduced. The
multi-superqubit states are studied, and the superconcurrence which
distinguishes separable states is proposed.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12570" title="Abstract">arXiv:2310.12570</a> (cross-list from eess.IV) [<a href="/pdf/2310.12570" title="Download PDF">pdf</a>, <a href="/format/2310.12570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DA-TransUNet: Integrating Spatial and Channel Dual Attention with  Transformer U-Net for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+G">Guanqun Sun</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+Y">Yizhi Pan</a>, 
<a href="/search/eess?searchtype=author&query=Kong%2C+W">Weikun Kong</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Z">Zichang Xu</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+J">Jianhua Ma</a>, 
<a href="/search/eess?searchtype=author&query=Racharak%2C+T">Teeradaj Racharak</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+L">Le-Minh Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Xin%2C+J">Junyi Xin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Great progress has been made in automatic medical image segmentation due to
powerful deep representation learning. The influence of transformer has led to
research into its variants, and large-scale replacement of traditional CNN
modules. However, such trend often overlooks the intrinsic feature extraction
capabilities of the transformer and potential refinements to both the model and
the transformer module through minor adjustments. This study proposes a novel
deep medical image segmentation framework, called DA-TransUNet, aiming to
introduce the Transformer and dual attention block into the encoder and decoder
of the traditional U-shaped architecture. Unlike prior transformer-based
solutions, our DA-TransUNet utilizes attention mechanism of transformer and
multifaceted feature extraction of DA-Block, which can efficiently combine
global, local, and multi-scale features to enhance medical image segmentation.
Meanwhile, experimental results show that a dual attention block is added
before the Transformer layer to facilitate feature extraction in the U-net
structure. Furthermore, incorporating dual attention blocks in skip connections
can enhance feature transfer to the decoder, thereby improving image
segmentation performance. Experimental results across various benchmark of
medical image segmentation reveal that DA-TransUNet significantly outperforms
the state-of-the-art methods. The codes and parameters of our model will be
publicly available at https://github.com/SUN-1024/DA-TransUnet.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12987" title="Abstract">arXiv:2310.12987</a> (cross-list from eess.IV) [<a href="/pdf/2310.12987" title="Download PDF">pdf</a>, <a href="/format/2310.12987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spec-NeRF: Multi-spectral Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jiabao Li</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yuqi Li</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+C">Ciliang Sun</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Chong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Xiang%2C+J">Jinhui Xiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)

</div>
<p class="mathjax">We propose Multi-spectral Neural Radiance Fields(Spec-NeRF) for jointly
reconstructing a multispectral radiance field and spectral sensitivity
functions(SSFs) of the camera from a set of color images filtered by different
filters. The proposed method focuses on modeling the physical imaging process,
and applies the estimated SSFs and radiance field to synthesize novel views of
multispectral scenes. In this method, the data acquisition requires only a
low-cost trichromatic camera and several off-the-shelf color filters, making it
more practical than using specialized 3D scanning and spectral imaging
equipment. Our experiments on both synthetic and real scenario datasets
demonstrate that utilizing filtered RGB images with learnable NeRF and SSFs can
achieve high fidelity and promising spectral reconstruction while retaining the
inherent capability of NeRF to comprehend geometric structures. Code is
available at https://github.com/CPREgroup/SpecNeRF-v2.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12994" title="Abstract">arXiv:2310.12994</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.12994" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dimensions of Disagreement: Unpacking Divergence and Misalignment in  Cognitive Science and Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Oktar%2C+K">Kerem Oktar</a>, 
<a href="/search/q-bio?searchtype=author&query=Sucholutsky%2C+I">Ilia Sucholutsky</a>, 
<a href="/search/q-bio?searchtype=author&query=Lombrozo%2C+T">Tania Lombrozo</a>, 
<a href="/search/q-bio?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Currently under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The increasing prevalence of artificial agents creates a correspondingly
increasing need to manage disagreements between humans and artificial agents,
as well as between artificial agents themselves. Considering this larger space
of possible agents exposes an opportunity for furthering our understanding of
the nature of disagreement: past studies in psychology have often cast
disagreement as two agents forming diverging evaluations of the same object,
but disagreement can also arise from differences in how agents represent that
object. AI research on human-machine alignment and recent work in computational
cognitive science have focused on this latter kind of disagreement, and have
developed tools that can be used to quantify the extent of representational
overlap between agents. Understanding how divergence and misalignment interact
to produce disagreement, and how resolution strategies depend on this
interaction, is key to promoting effective collaboration between diverse types
of agents.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12996" title="Abstract">arXiv:2310.12996</a> (cross-list from q-bio.BM) [<a href="/pdf/2310.12996" title="Download PDF">pdf</a>, <a href="/format/2310.12996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot Learning of Drug Response Prediction for Preclinical Drug  Screening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Li%2C+K">Kun Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Luo%2C+Y">Yong Luo</a>, 
<a href="/search/q-bio?searchtype=author&query=Cai%2C+X">Xiantao Cai</a>, 
<a href="/search/q-bio?searchtype=author&query=Hu%2C+W">Wenbin Hu</a>, 
<a href="/search/q-bio?searchtype=author&query=Du%2C+B">Bo Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Cell Behavior (q-bio.CB); Genomics (q-bio.GN)

</div>
<p class="mathjax">Conventional deep learning methods typically employ supervised learning for
drug response prediction (DRP). This entails dependence on labeled response
data from drugs for model training. However, practical applications in the
preclinical drug screening phase demand that DRP models predict responses for
novel compounds, often with unknown drug responses. This presents a challenge,
rendering supervised deep learning methods unsuitable for such scenarios. In
this paper, we propose a zero-shot learning solution for the DRP task in
preclinical drug screening. Specifically, we propose a Multi-branch
Multi-Source Domain Adaptation Test Enhancement Plug-in, called MSDA. MSDA can
be seamlessly integrated with conventional DRP methods, learning invariant
features from the prior response data of similar drugs to enhance real-time
predictions of unlabeled compounds. We conducted experiments using the GDSCv2
and CellMiner datasets. The results demonstrate that MSDA efficiently predicts
drug responses for novel compounds, leading to a general performance
improvement of 5-10\% in the preclinical drug screening phase. The significance
of this solution resides in its potential to accelerate the drug discovery
process, improve drug candidate assessment, and facilitate the success of drug
discovery.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13010" title="Abstract">arXiv:2310.13010</a> (cross-list from eess.AS) [<a href="/pdf/2310.13010" title="Download PDF">pdf</a>, <a href="/format/2310.13010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Speech Abnormalities with a Perceiver-based Sequence  Classifier that Leverages a Universal Speech Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Soltau%2C+H">Hagen Soltau</a>, 
<a href="/search/eess?searchtype=author&query=Shafran%2C+I">Izhak Shafran</a>, 
<a href="/search/eess?searchtype=author&query=Ottenwess%2C+A">Alex Ottenwess</a>, 
<a href="/search/eess?searchtype=author&query=Duffy%2C+J+R+J">Joseph R. JR Duffy</a>, 
<a href="/search/eess?searchtype=author&query=Utianski%2C+R+L">Rene L. Utianski</a>, 
<a href="/search/eess?searchtype=author&query=Barnard%2C+L+R">Leland R. Barnard</a>, 
<a href="/search/eess?searchtype=author&query=Stricker%2C+J+L">John L. Stricker</a>, 
<a href="/search/eess?searchtype=author&query=Wiepert%2C+D">Daniela Wiepert</a>, 
<a href="/search/eess?searchtype=author&query=Jones%2C+D+T">David T. Jones</a>, 
<a href="/search/eess?searchtype=author&query=Botha%2C+H">Hugo Botha</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. ASRU, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose a Perceiver-based sequence classifier to detect abnormalities in
speech reflective of several neurological disorders. We combine this classifier
with a Universal Speech Model (USM) that is trained (unsupervised) on 12
million hours of diverse audio recordings. Our model compresses long sequences
into a small set of class-specific latent representations and a factorized
projection is used to predict different attributes of the disordered input
speech. The benefit of our approach is that it allows us to model different
regions of the input for different classes and is at the same time data
efficient. We evaluated the proposed model extensively on a curated corpus from
the Mayo Clinic. Our model outperforms standard transformer (80.9%) and
perceiver (81.8%) models and achieves an average accuracy of 83.1%. With
limited task-specific data, we find that pretraining is important and
surprisingly pretraining with the unrelated automatic speech recognition (ASR)
task is also beneficial. Encodings from the middle layers provide a mix of both
acoustic and phonetic information and achieve best prediction results compared
to just using the final layer encodings (83.1% vs. 79.6%). The results are
promising and with further refinements may help clinicians detect speech
abnormalities without needing access to highly specialized speech-language
pathologists.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13018" title="Abstract">arXiv:2310.13018</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.13018" title="Download PDF">pdf</a>, <a href="/format/2310.13018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Getting aligned on representational alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Sucholutsky%2C+I">Ilia Sucholutsky</a>, 
<a href="/search/q-bio?searchtype=author&query=Muttenthaler%2C+L">Lukas Muttenthaler</a>, 
<a href="/search/q-bio?searchtype=author&query=Weller%2C+A">Adrian Weller</a>, 
<a href="/search/q-bio?searchtype=author&query=Peng%2C+A">Andi Peng</a>, 
<a href="/search/q-bio?searchtype=author&query=Bobu%2C+A">Andreea Bobu</a>, 
<a href="/search/q-bio?searchtype=author&query=Kim%2C+B">Been Kim</a>, 
<a href="/search/q-bio?searchtype=author&query=Love%2C+B+C">Bradley C. Love</a>, 
<a href="/search/q-bio?searchtype=author&query=Grant%2C+E">Erin Grant</a>, 
<a href="/search/q-bio?searchtype=author&query=Achterberg%2C+J">Jascha Achterberg</a>, 
<a href="/search/q-bio?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
<a href="/search/q-bio?searchtype=author&query=Collins%2C+K+M">Katherine M. Collins</a>, 
<a href="/search/q-bio?searchtype=author&query=Hermann%2C+K+L">Katherine L. Hermann</a>, 
<a href="/search/q-bio?searchtype=author&query=Oktar%2C+K">Kerem Oktar</a>, 
<a href="/search/q-bio?searchtype=author&query=Greff%2C+K">Klaus Greff</a>, 
<a href="/search/q-bio?searchtype=author&query=Hebart%2C+M+N">Martin N. Hebart</a>, 
<a href="/search/q-bio?searchtype=author&query=Jacoby%2C+N">Nori Jacoby</a>, 
<a href="/search/q-bio?searchtype=author&query=Qiuyi">Qiuyi</a> (Richard)
<a href="/search/q-bio?searchtype=author&query=Zhang">Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Marjieh%2C+R">Raja Marjieh</a>, 
<a href="/search/q-bio?searchtype=author&query=Geirhos%2C+R">Robert Geirhos</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+S">Sherol Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Kornblith%2C+S">Simon Kornblith</a>, 
<a href="/search/q-bio?searchtype=author&query=Rane%2C+S">Sunayana Rane</a>, 
<a href="/search/q-bio?searchtype=author&query=Konkle%2C+T">Talia Konkle</a>, 
<a href="/search/q-bio?searchtype=author&query=O%27Connell%2C+T+P">Thomas P. O&#x27;Connell</a>, 
<a href="/search/q-bio?searchtype=author&query=Unterthiner%2C+T">Thomas Unterthiner</a>, 
<a href="/search/q-bio?searchtype=author&query=Lampinen%2C+A+K">Andrew K. Lampinen</a>, 
<a href="/search/q-bio?searchtype=author&query=M%C3%BCller%2C+K">Klaus-Robert M&#xfc;ller</a>, 
<a href="/search/q-bio?searchtype=author&query=Toneva%2C+M">Mariya Toneva</a>, 
<a href="/search/q-bio?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working paper, changes to be made in upcoming revisions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Biological and artificial information processing systems form representations
of the world that they can use to categorize, reason, plan, navigate, and make
decisions. To what extent do the representations formed by these diverse
systems agree? Can diverging representations still lead to the same behaviors?
And how can systems modify their representations to better match those of
another system? These questions pertaining to the study of
\textbf{\emph{representational alignment}} are at the heart of some of the most
active research areas in contemporary cognitive science, neuroscience, and
machine learning. Unfortunately, there is limited knowledge-transfer between
research communities interested in representational alignment, and much of the
progress in one field ends up being rediscovered independently in another, when
greater cross-field communication would be advantageous. To improve
communication between fields, we propose a unifying framework that can serve as
a common language between researchers studying representational alignment. We
survey the literature from the fields of cognitive science, neuroscience, and
machine learning, and demonstrate how prior work fits into this framework.
Finally, we lay out open problems in representational alignment where progress
can benefit all three fields. We hope that our work can catalyze
cross-disciplinary collaboration and accelerate progress for all communities
studying and developing information processing systems. We note that this is a
working paper and encourage readers to reach out with their suggestions for
future revisions.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13021" title="Abstract">arXiv:2310.13021</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.13021" title="Download PDF">pdf</a>, <a href="/format/2310.13021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI for Mathematics: A Cognitive Science Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+C+E">Cedegao E. Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Collins%2C+K+M">Katherine M. Collins</a>, 
<a href="/search/q-bio?searchtype=author&query=Weller%2C+A">Adrian Weller</a>, 
<a href="/search/q-bio?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mathematics is one of the most powerful conceptual systems developed and used
by the human species. Dreams of automated mathematicians have a storied history
in artificial intelligence (AI). Rapid progress in AI, particularly propelled
by advances in large language models (LLMs), has sparked renewed, widespread
interest in building such systems. In this work, we reflect on these goals from
a \textit{cognitive science} perspective. We call attention to several
classical and ongoing research directions from cognitive science, which we
believe are valuable for AI practitioners to consider when seeking to build
truly human (or superhuman)-level mathematical systems. We close with open
discussions and questions that we believe necessitate a multi-disciplinary
perspective -- cognitive scientists working in tandem with AI researchers and
mathematicians -- as we move toward better mathematical AI systems which not
only help us push the frontier of the mathematics, but also offer glimpses into
how we as humans are even capable of such great cognitive feats.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13035" title="Abstract">arXiv:2310.13035</a> (cross-list from math.GM) [<a href="/pdf/2310.13035" title="Download PDF">pdf</a>, <a href="/format/2310.13035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collatz conjecture becomes theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mirkowska%2C+G">Gra&#x17c;yna Mirkowska</a>, 
<a href="/search/math?searchtype=author&query=Salwicki%2C+A">Andrzej Salwicki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Mathematics (math.GM)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">The Collatz hypothesis is a theorem of the algorithmic theory of natural
numbers. We prove the (algorithmic) formula that expresses the halting property
of Collatz algorithm. The observation that Collatz's theorem cannot be proved
in any elementary number theory completes the main result.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13067" title="Abstract">arXiv:2310.13067</a> (cross-list from math.CO) [<a href="/pdf/2310.13067" title="Download PDF">pdf</a>, <a href="/format/2310.13067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Existence and Structure of Universal Partial Cycles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fillmore%2C+D">Dylan Fillmore</a>, 
<a href="/search/math?searchtype=author&query=Goeckner%2C+B">Bennet Goeckner</a>, 
<a href="/search/math?searchtype=author&query=Kirsch%2C+R">Rachel Kirsch</a>, 
<a href="/search/math?searchtype=author&query=Martin%2C+K">Kirin Martin</a>, 
<a href="/search/math?searchtype=author&query=McGinnis%2C+D">Daniel McGinnis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">A universal partial cycle (or upcycle) for $\mathcal{A}^n$ is a cyclic
sequence that covers each word of length $n$ over the alphabet $\mathcal{A}$
exactly once -- like a De Bruijn cycle, except that we also allow a wildcard
symbol $\mathord{\diamond}$ that can represent any letter of $\mathcal{A}$.
Chen et al. in 2017 and Goeckner et al. in 2018 showed that the existence and
structure of upcycles are highly constrained, unlike those of De Bruijn cycles,
which exist for any alphabet size and word length. Moreover, it was not known
whether any upcycles existed for $n \ge 5$. We present several examples of
upcycles over both binary and non-binary alphabets for $n = 8$. We generalize
two graph-theoretic representations of De Bruijn cycles to upcycles. We then
introduce novel approaches to constructing new upcycles from old ones. Notably,
given any upcycle for an alphabet of size $a$, we show how to construct an
upcycle for an alphabet of size $ak$ for any $k \in \mathbb{N}$, so each
example generates an infinite family of upcycles. We also define folds and
lifts of upcycles, which relate upcycles with differing densities of
$\mathord{\diamond}$ characters. In particular, we show that every upcycle
lifts to a De Bruijn cycle. Our constructions rely on a different
generalization of De Bruijn cycles known as perfect necklaces, and we introduce
several new examples of perfect necklaces. We extend the definitions of certain
pseudorandomness properties to partial words and determine which are satisfied
by all upcycles, then draw a conclusion about linear feedback shift registers.
Finally, we prove new nonexistence results based on the word length $n$,
alphabet size, and $\mathord{\diamond}$ density.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13088" title="Abstract">arXiv:2310.13088</a> (cross-list from stat.ML) [<a href="/pdf/2310.13088" title="Download PDF">pdf</a>, <a href="/format/2310.13088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequence Length Independent Norm-Based Generalization Bounds for  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Trauger%2C+J">Jacob Trauger</a>, 
<a href="/search/stat?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper provides norm-based generalization bounds for the Transformer
architecture that do not depend on the input sequence length. We employ a
covering number based approach to prove our bounds. We use three novel covering
number bounds for the function class of bounded linear transformations to upper
bound the Rademacher complexity of the Transformer. Furthermore, we show this
generalization bound applies to the common Transformer training technique of
masking and then predicting the masked word. We also run a simulated study on a
sparse majority data set that empirically validates our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13108" title="Abstract">arXiv:2310.13108</a> (cross-list from eess.IV) [<a href="/pdf/2310.13108" title="Download PDF">pdf</a>, <a href="/format/2310.13108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streamlining Brain Tumor Classification with Custom Transfer Learning in  MRI Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hossain%2C+J">Javed Hossain</a>, 
<a href="/search/eess?searchtype=author&query=Islam%2C+M+T">Md. Touhidul Islam</a>, 
<a href="/search/eess?searchtype=author&query=Tusar%2C+M+T+H+K">Md. Taufiqul Haque Khan Tusar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 9 figures, 4 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Smart Information Systems
  and Technologies (SIST), Astana, Kazakhstan, 2023, pp. 522-526
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Brain tumors are increasingly prevalent, characterized by the uncontrolled
spread of aberrant tissues in the brain, with almost 700,000 new cases
diagnosed globally each year. Magnetic Resonance Imaging (MRI) is commonly used
for the diagnosis of brain tumors and accurate classification is a critical
clinical procedure. In this study, we propose an efficient solution for
classifying brain tumors from MRI images using custom transfer learning
networks. While several researchers have employed various pre-trained
architectures such as RESNET-50, ALEXNET, VGG-16, and VGG-19, these methods
often suffer from high computational complexity. To address this issue, we
present a custom and lightweight model using a Convolutional Neural
Network-based pre-trained architecture with reduced complexity. Specifically,
we employ the VGG-19 architecture with additional hidden layers, which reduces
the complexity of the base architecture but improves computational efficiency.
The objective is to achieve high classification accuracy using a novel
approach. Finally, the result demonstrates a classification accuracy of 96.42%.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13136" title="Abstract">arXiv:2310.13136</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2310.13136" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approaches for Uncertainty Quantification of AI-predicted Material  Properties: A Comparison
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Tavazza%2C+F">Francesca Tavazza</a>, 
<a href="/search/cond-mat?searchtype=author&query=Choudhary%2C+K">Kamal Choudhary</a>, 
<a href="/search/cond-mat?searchtype=author&query=DeCost%2C+B">Brian DeCost</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2107.07997">arXiv:2107.07997</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The development of large databases of material properties, together with the
availability of powerful computers, has allowed machine learning (ML) modeling
to become a widely used tool for predicting material performances. While
confidence intervals are commonly reported for such ML models, prediction
intervals, i.e., the uncertainty on each prediction, are not as frequently
available. Here, we investigate three easy-to-implement approaches to determine
such individual uncertainty, comparing them across ten ML quantities spanning
energetics, mechanical, electronic, optical, and spectral properties.
Specifically, we focused on the Quantile approach, the direct machine learning
of the prediction intervals and Ensemble methods.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13147" title="Abstract">arXiv:2310.13147</a> (cross-list from math.OC) [<a href="/pdf/2310.13147" title="Download PDF">pdf</a>, <a href="/format/2310.13147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Data-Driven Surrogate Modeling for Nonlinear Optimal Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sharma%2C+A">Aayushman Sharma</a>, 
<a href="/search/math?searchtype=author&query=Chakravorty%2C+S">Suman Chakravorty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, we study the use of state-of-the-art nonlinear system
identification techniques for the optimal control of nonlinear systems. We show
that the nonlinear systems identification problem is equivalent to estimating
the generalized moments of an underlying sampling distribution and is bound to
suffer from ill-conditioning and variance when approximating a system to high
order, requiring samples combinatorial-exponential in the order of the
approximation, i.e., the global nature of the approximation. We show that the
iterative identification of "local" linear time varying (LTV) models around the
current estimate of the optimal trajectory, coupled with a suitable optimal
control algorithm such as iterative LQR (ILQR), is necessary as well as
sufficient, to accurately solve the underlying optimal control problem.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13160" title="Abstract">arXiv:2310.13160</a> (cross-list from eess.SP) [<a href="/pdf/2310.13160" title="Download PDF">pdf</a>, <a href="/format/2310.13160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Sensing for Localization with Reconfigurable Intelligent Surface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Z">Zhongze Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+T">Tao Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+W">Wei Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE International Conference on Communications (ICC) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">This paper addresses an uplink localization problem in which the base station
(BS) aims to locate a remote user with the aid of reconfigurable intelligent
surface (RIS). This paper proposes a strategy in which the user transmits
pilots over multiple time frames, and the BS adaptively adjusts the RIS
reflection coefficients based on the observations already received so far in
order to produce an accurate estimate of the user location at the end. This is
a challenging active sensing problem for which finding an optimal solution
involves a search through a complicated functional space whose dimension
increases with the number of measurements. In this paper, we show that the long
short-term memory (LSTM) network can be used to exploit the latent temporal
correlation between measurements to automatically construct scalable
information vectors (called hidden state) based on the measurements.
Subsequently, the state vector can be mapped to the RIS configuration for the
next time frame in a codebook-free fashion via a deep neural network (DNN).
After all the measurements have been received, a final DNN can be used to map
the LSTM cell state to the estimated user equipment (UE) position. Numerical
result shows that the proposed active RIS design results in lower localization
error as compared to existing active and nonactive methods. The proposed
solution produces interpretable results and is generalizable to early stopping
in the sequence of sensing stages.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13200" title="Abstract">arXiv:2310.13200</a> (cross-list from econ.GN) [<a href="/pdf/2310.13200" title="Download PDF">pdf</a>, <a href="/format/2310.13200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Learning Analysis of Climate Change, Innovation, and Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Barnett%2C+M">Michael Barnett</a>, 
<a href="/search/econ?searchtype=author&query=Brock%2C+W">William Brock</a>, 
<a href="/search/econ?searchtype=author&query=Hansen%2C+L+P">Lars Peter Hansen</a>, 
<a href="/search/econ?searchtype=author&query=Hu%2C+R">Ruimeng Hu</a>, 
<a href="/search/econ?searchtype=author&query=Huang%2C+J">Joseph Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the implications of model uncertainty in a climate-economics
framework with three types of capital: "dirty" capital that produces carbon
emissions when used for production, "clean" capital that generates no emissions
but is initially less productive than dirty capital, and knowledge capital that
increases with R\&amp;D investment and leads to technological innovation in green
sector productivity. To solve our high-dimensional, non-linear model framework
we implement a neural-network-based global solution method. We show there are
first-order impacts of model uncertainty on optimal decisions and social
valuations in our integrated climate-economic-innovation framework. Accounting
for interconnected uncertainty over climate dynamics, economic damages from
climate change, and the arrival of a green technological change leads to
substantial adjustments to investment in the different capital types in
anticipation of technological change and the revelation of climate damage
severity.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13209" title="Abstract">arXiv:2310.13209</a> (cross-list from eess.SP) [<a href="/pdf/2310.13209" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundational Techniques for Wireless Communications: Channel Coding,  Modulation, and Equalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=McKiernan%2C+S">Solomon McKiernan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 11 figures, 5 equations, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">This paper analyses foundational techniques for improving wireless
communication systems, including coding methods, modulation schemes, and
channel equalization. Using industry-standard simulation tools, the paper
evaluates the performance of these techniques under different channel
conditions. Convolutional codes, punctured and unpunctured, are assessed for
reliable data transfer. The suitability of various modulation schemes, such as
Phase Shift Keying (PSK) and Quadrature Amplitude Modulation (QAM), are
examined. Linear and decision-feedback equalization techniques are evaluated
for mitigating the effects of channel impairments. The paper provides practical
insights into the implementation of these techniques, emphasizing their
importance in modern wireless communication systems.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13216" title="Abstract">arXiv:2310.13216</a> (cross-list from eess.IV) [<a href="/pdf/2310.13216" title="Download PDF">pdf</a>, <a href="/format/2310.13216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PTSR: Patch Translator for Image Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Baghel%2C+N">Neeraj Baghel</a>, 
<a href="/search/eess?searchtype=author&query=Dubey%2C+S+R">Shiv Ram Dubey</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+S+K">Satish Kumar Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Image super-resolution generation aims to generate a high-resolution image
from its low-resolution image. However, more complex neural networks bring high
computational costs and memory storage. It is still an active area for offering
the promise of overcoming resolution limitations in many applications. In
recent years, transformers have made significant progress in computer vision
tasks as their robust self-attention mechanism. However, recent works on the
transformer for image super-resolution also contain convolution operations. We
propose a patch translator for image super-resolution (PTSR) to address this
problem. The proposed PTSR is a transformer-based GAN network with no
convolution operation. We introduce a novel patch translator module for
regenerating the improved patches utilising multi-head attention, which is
further utilised by the generator to generate the 2x and 4x super-resolution
images. The experiments are performed using benchmark datasets, including
DIV2K, Set5, Set14, and BSD100. The results of the proposed model is improved
on an average for $4\times$ super-resolution by 21.66% in PNSR score and 11.59%
in SSIM score, as compared to the best competitive models. We also analyse the
proposed loss and saliency map to show the effectiveness of the proposed
method.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13222" title="Abstract">arXiv:2310.13222</a> (cross-list from hep-lat) [<a href="/pdf/2310.13222" title="Download PDF">pdf</a>, <a href="/format/2310.13222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Transformer is all you need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-lat?searchtype=author&query=Tomiya%2C+A">Akio Tomiya</a>, 
<a href="/search/hep-lat?searchtype=author&query=Nagai%2C+Y">Yuki Nagai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures, contribution for the 40th International Symposium on Lattice Field Theory (Lattice 2023), July 31st - August 4th, 2023, Fermi National Accelerator Laboratory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Lattice (hep-lat)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning, deep learning, has been accelerating computational physics,
which has been used to simulate systems on a lattice. Equivariance is essential
to simulate a physical system because it imposes a strong induction bias for
the probability distribution described by a machine learning model. This
reduces the risk of erroneous extrapolation that deviates from data symmetries
and physical laws. However, imposing symmetry on the model sometimes occur a
poor acceptance rate in self-learning Monte-Carlo (SLMC). On the other hand,
Attention used in Transformers like GPT realizes a large model capacity. We
introduce symmetry equivariant attention to SLMC. To evaluate our architecture,
we apply it to our proposed new architecture on a spin-fermion model on a
two-dimensional lattice. We find that it overcomes poor acceptance rates for
linear models and observe the scaling law of the acceptance rate as in the
large language models with Transformers.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13250" title="Abstract">arXiv:2310.13250</a> (cross-list from eess.IV) [<a href="/pdf/2310.13250" title="Download PDF">pdf</a>, <a href="/format/2310.13250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnosis-oriented Medical Image Compression with Efficient Transfer  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xie%2C+G">Guangqi Xie</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+X">Xiaohan Pan</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhibo Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE VCIP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Remote medical diagnosis has emerged as a critical and indispensable
technique in practical medical systems, where medical data are required to be
efficiently compressed and transmitted for diagnosis by either professional
doctors or intelligent diagnosis devices. In this process, a large amount of
redundant content irrelevant to the diagnosis is subjected to high-fidelity
coding, leading to unnecessary transmission costs. To mitigate this, we propose
diagnosis-oriented medical image compression, a special semantic compression
task designed for medical scenarios, targeting to reduce the compression cost
without compromising the diagnosis accuracy. However, collecting sufficient
medical data to optimize such a compression system is significantly expensive
and challenging due to privacy issues and the lack of professional annotation.
In this study, we propose DMIC, the first efficient transfer learning-based
codec, for diagnosis-oriented medical image compression, which can be
effectively optimized with only few-shot annotated medical examples, by reusing
the knowledge in the existing reinforcement learning-based task-driven semantic
coding framework, i.e., HRLVSC [1]. Concretely, we focus on tuning only the
partial parameters of the policy network for bit allocation within HRLVSC,
which enables it to adapt to the medical images. In this work, we validate our
DMIC with the typical medical task, Coronary Artery Segmentation. Extensive
experiments have demonstrated that our DMIC can achieve 47.594%BD-Rate savings
compared to the HEVC anchor, by tuning only the A2C module (2.7% parameters) of
the policy network with only 1 medical sample.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13251" title="Abstract">arXiv:2310.13251</a> (cross-list from math.OC) [<a href="/pdf/2310.13251" title="Download PDF">pdf</a>, <a href="/format/2310.13251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Conjugate Frameworks for Nonconvex and Nonsmooth Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+J">Jiangshan Wang</a>, 
<a href="/search/math?searchtype=author&query=Peng%2C+Z">Zheng Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We introduce two new stochastic conjugate frameworks for a class of nonconvex
and possibly also nonsmooth optimization problems. These frameworks are built
upon Stochastic Recursive Gradient Algorithm (SARAH) and we thus refer to them
as Acc-Prox-CG-SARAH and Acc-Prox-CG-SARAH-RS, respectively. They are
efficiently accelerated, easy to implement, tune free and can be smoothly
extended and modified. We devise a deterministic restart scheme for stochastic
optimization and apply it in our second stochastic conjugate framework, which
serves the key difference between the two approaches. In addition, we apply the
ProbAbilistic Gradient Estimator (PAGE) and further develop a practical
variant, denoted as Acc-Prox-CG-SARAH-ST, in order to reduce potential
computational overhead. We provide comprehensive and rigorous convergence
analysis for all three approaches and establish linear convergence rates for
unconstrained minimization problem with nonconvex and nonsmooth objective
functions. Experiments have demonstrated that Acc-Prox-CG-SARAH and
Acc-Prox-CG-SARAH-RS both outperform state-of-art methods consistently and
Acc-Prox-CG-SARAH-ST can as well achieve comparable convergence speed. In terms
of theory and experiments, we verify the strong computational efficiency of the
deterministic restart scheme in stochastic optimization methods.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13259" title="Abstract">arXiv:2310.13259</a> (cross-list from eess.IV) [<a href="/pdf/2310.13259" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-specific optimization and diverse evaluation of self-supervised  models for histopathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lai%2C+J">Jeremy Lai</a>, 
<a href="/search/eess?searchtype=author&query=Ahmed%2C+F">Faruk Ahmed</a>, 
<a href="/search/eess?searchtype=author&query=Vijay%2C+S">Supriya Vijay</a>, 
<a href="/search/eess?searchtype=author&query=Jaroensri%2C+T">Tiam Jaroensri</a>, 
<a href="/search/eess?searchtype=author&query=Loo%2C+J">Jessica Loo</a>, 
<a href="/search/eess?searchtype=author&query=Vyawahare%2C+S">Saurabh Vyawahare</a>, 
<a href="/search/eess?searchtype=author&query=Agarwal%2C+S">Saloni Agarwal</a>, 
<a href="/search/eess?searchtype=author&query=Jamil%2C+F">Fayaz Jamil</a>, 
<a href="/search/eess?searchtype=author&query=Matias%2C+Y">Yossi Matias</a>, 
<a href="/search/eess?searchtype=author&query=Corrado%2C+G+S">Greg S. Corrado</a>, 
<a href="/search/eess?searchtype=author&query=Webster%2C+D+R">Dale R. Webster</a>, 
<a href="/search/eess?searchtype=author&query=Krause%2C+J">Jonathan Krause</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yun Liu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+P+C">Po-Hsuan Cameron Chen</a>, 
<a href="/search/eess?searchtype=author&query=Wulczyn%2C+E">Ellery Wulczyn</a>, 
<a href="/search/eess?searchtype=author&query=Steiner%2C+D+F">David F. Steiner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 main tables, 3 main figures, additional supplemental tables and figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Task-specific deep learning models in histopathology offer promising
opportunities for improving diagnosis, clinical research, and precision
medicine. However, development of such models is often limited by availability
of high-quality data. Foundation models in histopathology that learn general
representations across a wide range of tissue types, diagnoses, and
magnifications offer the potential to reduce the data, compute, and technical
expertise necessary to develop task-specific deep learning models with the
required level of model performance. In this work, we describe the development
and evaluation of foundation models for histopathology via self-supervised
learning (SSL). We first establish a diverse set of benchmark tasks involving
17 unique tissue types and 12 unique cancer types and spanning different
optimal magnifications and task types. Next, we use this benchmark to explore
and evaluate histopathology-specific SSL methods followed by further evaluation
on held out patch-level and weakly supervised tasks. We found that standard SSL
methods thoughtfully applied to histopathology images are performant across our
benchmark tasks and that domain-specific methodological improvements can
further increase performance. Our findings reinforce the value of using
domain-specific SSL methods in pathology, and establish a set of high quality
foundation models to enable further research across diverse applications.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13270" title="Abstract">arXiv:2310.13270</a> (cross-list from stat.ML) [<a href="/pdf/2310.13270" title="Download PDF">pdf</a>, <a href="/format/2310.13270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-learning of Physics-informed Neural Networks for Efficiently  Solving Newly Given PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Iwata%2C+T">Tomoharu Iwata</a>, 
<a href="/search/stat?searchtype=author&query=Tanaka%2C+Y">Yusuke Tanaka</a>, 
<a href="/search/stat?searchtype=author&query=Ueda%2C+N">Naonori Ueda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a neural network-based meta-learning method to efficiently solve
partial differential equation (PDE) problems. The proposed method is designed
to meta-learn how to solve a wide variety of PDE problems, and uses the
knowledge for solving newly given PDE problems. We encode a PDE problem into a
problem representation using neural networks, where governing equations are
represented by coefficients of a polynomial function of partial derivatives,
and boundary conditions are represented by a set of point-condition pairs. We
use the problem representation as an input of a neural network for predicting
solutions, which enables us to efficiently predict problem-specific solutions
by the forwarding process of the neural network without updating model
parameters. To train our model, we minimize the expected error when adapted to
a PDE problem based on the physics-informed neural network framework, by which
we can evaluate the error even when solutions are unknown. We demonstrate that
our proposed method outperforms existing methods in predicting solutions of PDE
problems.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13279" title="Abstract">arXiv:2310.13279</a> (cross-list from eess.IV) [<a href="/pdf/2310.13279" title="Download PDF">pdf</a>, <a href="/format/2310.13279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pathologist-Like Explanations Unveiled: an Explainable Deep Learning  System for White Blood Cell Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pal%2C+A+S">Aditya Shankar Pal</a>, 
<a href="/search/eess?searchtype=author&query=Biswas%2C+D">Debojyoti Biswas</a>, 
<a href="/search/eess?searchtype=author&query=Mahapatra%2C+J">Joy Mahapatra</a>, 
<a href="/search/eess?searchtype=author&query=Banerjee%2C+D">Debasis Banerjee</a>, 
<a href="/search/eess?searchtype=author&query=Chakrabarti%2C+P">Prantar Chakrabarti</a>, 
<a href="/search/eess?searchtype=author&query=Garain%2C+U">Utpal Garain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages including supplementary material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">White blood cells (WBCs) play a crucial role in safeguarding the human body
against pathogens and foreign substances. Leveraging the abundance of WBC
imaging data and the power of deep learning algorithms, automated WBC analysis
has the potential for remarkable accuracy. However, the capability of deep
learning models to explain their WBC classification remains largely unexplored.
In this study, we introduce HemaX, an explainable deep neural network-based
model that produces pathologist-like explanations using five attributes:
granularity, cytoplasm color, nucleus shape, size relative to red blood cells,
and nucleus to cytoplasm ratio (N:C), along with cell classification,
localization, and segmentation. HemaX is trained and evaluated on a novel
dataset, LeukoX, comprising 467 blood smear images encompassing ten (10) WBC
types. The proposed model achieves impressive results, with an average
classification accuracy of 81.08% and a Jaccard index of 89.16% for cell
localization. Additionally, HemaX performs well in generating the five
explanations with a normalized mean square error of 0.0317 for N:C ratio and
over 80% accuracy for the other four attributes. Comprehensive experiments
comparing against multiple state-of-the-art models demonstrate that HemaX's
classification accuracy remains unaffected by its ability to provide
explanations. Moreover, empirical analyses and validation by expert
hematologists confirm the faithfulness of explanations predicted by our
proposed model.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13311" title="Abstract">arXiv:2310.13311</a> (cross-list from stat.ML) [<a href="/pdf/2310.13311" title="Download PDF">pdf</a>, <a href="/format/2310.13311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Negative Spherical Relaxations for Universe-Free Multi-Matching and  Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Thunberg%2C+J">Johan Thunberg</a>, 
<a href="/search/stat?searchtype=author&query=Bernard%2C+F">Florian Bernard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at Scandinavian Conference on Image Analysis (SCIA) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">We propose a novel non-negative spherical relaxation for optimization
problems over binary matrices with injectivity constraints, which in particular
has applications in multi-matching and clustering. We relax respective binary
matrix constraints to the (high-dimensional) non-negative sphere. To optimize
our relaxed problem, we use a conditional power iteration method to iteratively
improve the objective function, while at same time sweeping over a continuous
scalar parameter that is (indirectly) related to the universe size (or number
of clusters). Opposed to existing procedures that require to fix the integer
universe size before optimization, our method automatically adjusts the
analogous continuous parameter. Furthermore, while our approach shares
similarities with spectral multi-matching and spectral clustering, our
formulation has the strong advantage that we do not rely on additional
post-processing procedures to obtain binary results. Our method shows
compelling results in various multi-matching and clustering settings, even when
compared to methods that use the ground truth universe size (or number of
clusters).
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13341" title="Abstract">arXiv:2310.13341</a> (cross-list from math.CO) [<a href="/pdf/2310.13341" title="Download PDF">pdf</a>, <a href="/ps/2310.13341" title="Download PostScript">ps</a>, <a href="/format/2310.13341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Packing forests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hoppenot%2C+P">Pierre Hoppenot</a>, 
<a href="/search/math?searchtype=author&query=Martin%2C+M">Mathis Martin</a>, 
<a href="/search/math?searchtype=author&query=Szigeti%2C+Z">Zolt&#xe1;n Szigeti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The seminal papers of Edmonds \cite{Egy}, Nash-Williams \cite{NW} and Tutte
\cite{Tu} have laid the foundations of the theories of packing arborescences
and packing trees. The directed version has been extensively investigated,
resulting in a great number of generalizations. In contrast, the undirected
version has been marginally considered. The aim of this paper is to further
develop the theory of packing trees and forests. We present a broad extension
of an already generic theorem on packing spanning branchings of B\'erczi, Frank
\cite{BF3}, as well as its universal undirected counterpart which is our main
result.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13349" title="Abstract">arXiv:2310.13349</a> (cross-list from stat.ML) [<a href="/pdf/2310.13349" title="Download PDF">pdf</a>, <a href="/format/2310.13349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepFDR: A Deep Learning-based False Discovery Rate Control Method for  Neuroimaging Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kim%2C+T">Taehyo Kim</a>, 
<a href="/search/stat?searchtype=author&query=Shu%2C+H">Hai Shu</a>, 
<a href="/search/stat?searchtype=author&query=Jia%2C+Q">Qiran Jia</a>, 
<a href="/search/stat?searchtype=author&query=de+Leon%2C+M">Mony de Leon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Voxel-based multiple testing is widely used in neuroimaging data analysis.
Traditional false discovery rate (FDR) control methods often ignore the spatial
dependence among the voxel-based tests and thus suffer from substantial loss of
testing power. While recent spatial FDR control methods have emerged, their
validity and optimality remain questionable when handling the complex spatial
dependencies of the brain. Concurrently, deep learning methods have
revolutionized image segmentation, a task closely related to voxel-based
multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR
control method that leverages unsupervised deep learning-based image
segmentation to address the voxel-based multiple testing problem. Numerical
studies, including comprehensive simulations and Alzheimer's disease FDG-PET
image analysis, demonstrate DeepFDR's superiority over existing methods.
DeepFDR not only excels in FDR control and effectively diminishes the false
nondiscovery rate, but also boasts exceptional computational efficiency highly
suited for tackling large-scale neuroimaging data.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13387" title="Abstract">arXiv:2310.13387</a> (cross-list from stat.ME) [<a href="/pdf/2310.13387" title="Download PDF">pdf</a>, <a href="/format/2310.13387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assumption violations in causal discovery and the robustness of score  matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Montagna%2C+F">Francesco Montagna</a>, 
<a href="/search/stat?searchtype=author&query=Mastakouri%2C+A+A">Atalanti A. Mastakouri</a>, 
<a href="/search/stat?searchtype=author&query=Eulig%2C+E">Elias Eulig</a>, 
<a href="/search/stat?searchtype=author&query=Noceti%2C+N">Nicoletta Noceti</a>, 
<a href="/search/stat?searchtype=author&query=Rosasco%2C+L">Lorenzo Rosasco</a>, 
<a href="/search/stat?searchtype=author&query=Janzing%2C+D">Dominik Janzing</a>, 
<a href="/search/stat?searchtype=author&query=Aragam%2C+B">Bryon Aragam</a>, 
<a href="/search/stat?searchtype=author&query=Locatello%2C+F">Francesco Locatello</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">When domain knowledge is limited and experimentation is restricted by
ethical, financial, or time constraints, practitioners turn to observational
causal discovery methods to recover the causal structure, exploiting the
statistical properties of their data. Because causal discovery without further
assumptions is an ill-posed problem, each algorithm comes with its own set of
usually untestable assumptions, some of which are hard to meet in real
datasets. Motivated by these considerations, this paper extensively benchmarks
the empirical performance of recent causal discovery methods on observational
i.i.d. data generated under different background conditions, allowing for
violations of the critical assumptions required by each selected approach. Our
experimental findings show that score matching-based methods demonstrate
surprising performance in the false positive and false negative rate of the
inferred graph in these challenging scenarios, and we provide theoretical
insights into their performance. This work is also the first effort to
benchmark the stability of causal discovery algorithms with respect to the
values of their hyperparameters. Finally, we hope this paper will set a new
standard for the evaluation of causal discovery methods and can serve as an
accessible entry point for practitioners interested in the field, highlighting
the empirical implications of different algorithm choices.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13393" title="Abstract">arXiv:2310.13393</a> (cross-list from stat.ML) [<a href="/pdf/2310.13393" title="Download PDF">pdf</a>, <a href="/ps/2310.13393" title="Download PostScript">ps</a>, <a href="/format/2310.13393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Best Arm Identification with Fixed Confidence in Restless  Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Karthik%2C+P+N">P. N. Karthik</a>, 
<a href="/search/stat?searchtype=author&query=Tan%2C+V+Y+F">Vincent Y. F. Tan</a>, 
<a href="/search/stat?searchtype=author&query=Mukherjee%2C+A">Arpan Mukherjee</a>, 
<a href="/search/stat?searchtype=author&query=Tajer%2C+A">Ali Tajer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">We study best arm identification in a restless multi-armed bandit setting
with finitely many arms. The discrete-time data generated by each arm forms a
homogeneous Markov chain taking values in a common, finite state space. The
state transitions in each arm are captured by an ergodic transition probability
matrix (TPM) that is a member of a single-parameter exponential family of TPMs.
The real-valued parameters of the arm TPMs are unknown and belong to a given
space. Given a function $f$ defined on the common state space of the arms, the
goal is to identify the best arm -- the arm with the largest average value of
$f$ evaluated under the arm's stationary distribution -- with the fewest number
of samples, subject to an upper bound on the decision's error probability
(i.e., the fixed-confidence regime). A lower bound on the growth rate of the
expected stopping time is established in the asymptote of a vanishing error
probability. Furthermore, a policy for best arm identification is proposed, and
its expected stopping time is proved to have an asymptotic growth rate that
matches the lower bound. It is demonstrated that tracking the long-term
behavior of a certain Markov decision process and its state-action visitation
proportions are the key ingredients in analyzing the converse and achievability
bounds. It is shown that under every policy, the state-action visitation
proportions satisfy a specific approximate flow conservation constraint and
that these proportions match the optimal proportions dictated by the lower
bound under any asymptotically optimal policy. The prior studies on best arm
identification in restless bandits focus on independent observations from the
arms, rested Markov arms, and restless Markov arms with known arm TPMs. In
contrast, this work is the first to study best arm identification in restless
bandits with unknown arm TPMs.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13402" title="Abstract">arXiv:2310.13402</a> (cross-list from stat.ML) [<a href="/pdf/2310.13402" title="Download PDF">pdf</a>, <a href="/format/2310.13402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibrating Neural Simulation-Based Inference with Differentiable  Coverage Probability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Falkiewicz%2C+M">Maciej Falkiewicz</a>, 
<a href="/search/stat?searchtype=author&query=Takeishi%2C+N">Naoya Takeishi</a>, 
<a href="/search/stat?searchtype=author&query=Shekhzadeh%2C+I">Imahn Shekhzadeh</a>, 
<a href="/search/stat?searchtype=author&query=Wehenkel%2C+A">Antoine Wehenkel</a>, 
<a href="/search/stat?searchtype=author&query=Delaunoy%2C+A">Arnaud Delaunoy</a>, 
<a href="/search/stat?searchtype=author&query=Louppe%2C+G">Gilles Louppe</a>, 
<a href="/search/stat?searchtype=author&query=Kalousis%2C+A">Alexandros Kalousis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/DMML-Geneva/calibrated-posterior">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Bayesian inference allows expressing the uncertainty of posterior belief
under a probabilistic model given prior information and the likelihood of the
evidence. Predominantly, the likelihood function is only implicitly established
by a simulator posing the need for simulation-based inference (SBI). However,
the existing algorithms can yield overconfident posteriors (Hermans *et al.*,
2022) defeating the whole purpose of credibility if the uncertainty
quantification is inaccurate. We propose to include a calibration term directly
into the training objective of the neural model in selected amortized SBI
techniques. By introducing a relaxation of the classical formulation of
calibration error we enable end-to-end backpropagation. The proposed method is
not tied to any particular neural model and brings moderate computational
overhead compared to the profits it introduces. It is directly applicable to
existing computational pipelines allowing reliable black-box posterior
inference. We empirically show on six benchmark problems that the proposed
method achieves competitive or better results in terms of coverage and expected
posterior density than the previously existing approaches.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13430" title="Abstract">arXiv:2310.13430</a> (cross-list from eess.AS) [<a href="/pdf/2310.13430" title="Download PDF">pdf</a>, <a href="/format/2310.13430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HRTF Interpolation using a Spherical Neural Process Meta-Learner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Thuillier%2C+E">Etienne Thuillier</a>, 
<a href="/search/eess?searchtype=author&query=Jin%2C+C">Craig Jin</a>, 
<a href="/search/eess?searchtype=author&query=V%C3%A4lim%C3%A4ki%2C+V">Vesa V&#xe4;lim&#xe4;ki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages. 11 figures. Submitted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing (T-ASL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Several individualization methods have recently been proposed to estimate a
subject's Head-Related Transfer Function (HRTF) using convenient input
modalities such as anthropometric measurements or pinnae photographs. There
exists a need for adaptively correcting the estimation error committed by such
methods using a few data point samples from the subject's HRTF, acquired using
acoustic measurements or perceptual feedback. To this end, we introduce a
Convolutional Conditional Neural Process meta-learner specialized in HRTF error
interpolation. In particular, the model includes a Spherical Convolutional
Neural Network component to accommodate the spherical geometry of HRTF data. It
also exploits potential symmetries between the HRTF's left and right channels
about the median axis. In this work, we evaluate the proposed model's
performance purely on time-aligned spectrum interpolation grounds under a
simplified setup where a generic population-mean HRTF forms the initial
estimates prior to corrections instead of individualized ones. The trained
model achieves up to 3 dB relative error reduction compared to state-of-the-art
interpolation methods despite being trained using only 85 subjects. This
improvement translates up to nearly a halving of the data point count required
to achieve comparable accuracy, in particular from 50 to 28 points to reach an
average of -20 dB relative error per interpolated feature. Moreover, we show
that the trained model provides well-calibrated uncertainty estimates.
Accordingly, such estimates can inform the sequential decision problem of
acquiring as few correcting HRTF data points as needed to meet a desired level
of HRTF individualization accuracy.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13471" title="Abstract">arXiv:2310.13471</a> (cross-list from eess.AS) [<a href="/pdf/2310.13471" title="Download PDF">pdf</a>, <a href="/ps/2310.13471" title="Download PostScript">ps</a>, <a href="/format/2310.13471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural domain alignment for spoken language recognition based on optimal  transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lu%2C+X">Xugang Lu</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+P">Peng Shen</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>, 
<a href="/search/eess?searchtype=author&query=Kawai%2C+H">Hisashi Kawai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Domain shift poses a significant challenge in cross-domain spoken language
recognition (SLR) by reducing its effectiveness. Unsupervised domain adaptation
(UDA) algorithms have been explored to address domain shifts in SLR without
relying on class labels in the target domain. One successful UDA approach
focuses on learning domain-invariant representations to align feature
distributions between domains. However, disregarding the class structure during
the learning process of domain-invariant representations can result in
over-alignment, negatively impacting the classification task. To overcome this
limitation, we propose an optimal transport (OT)-based UDA algorithm for a
cross-domain SLR, leveraging the distribution geometry structure-aware property
of OT. An OT-based discrepancy measure on a joint distribution over feature and
label information is considered during domain alignment in OT-based UDA. Our
previous study discovered that completely aligning the distributions between
the source and target domains can introduce a negative transfer, where classes
or irrelevant classes from the source domain map to a different class in the
target domain during distribution alignment. This negative transfer degrades
the performance of the adaptive model. To mitigate this issue, we introduce
coupling-weighted partial optimal transport (POT) within our UDA framework for
SLR, where soft weighting on the OT coupling based on transport cost is
adaptively set during domain alignment. A cross-domain SLR task was used in the
experiments to evaluate the proposed UDA. The results demonstrated that our
proposed UDA algorithm significantly improved the performance over existing UDA
algorithms in a cross-channel SLR task.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13475" title="Abstract">arXiv:2310.13475</a> (cross-list from cond-mat.supr-con) [<a href="/pdf/2310.13475" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Magnetization of Flat Superconducting Films on Ferromagnetic Substrates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Prigozhin%2C+L">Leonid Prigozhin</a>, 
<a href="/search/cond-mat?searchtype=author&query=Sokolovsky%2C+V">Vladimir Sokolovsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Superconductivity (cond-mat.supr-con)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Ferromagnetic substrate influences the electromagnetic response of a type-II
superconducting film to the applied magnetic field. We present a
two-dimensional integrodifferential model for the magnetization of a flat
superconductor/ferromagnet bilayer of an arbitrary shape using a thin shell
quasistatic model for the ferromagnetic substrate and an infinitely thin
approximation for the superconducting layer. An efficient numerical method is
developed and used to investigate the effect of a ferromagnetic substrate. In
particular, we simulate the thin bilayer magnetization in a parallel field and,
for a high field, the critical-state distributions of the superconducting
current density. These critical-state distributions are different from those
known for a normal external field.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13480" title="Abstract">arXiv:2310.13480</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.13480" title="Download PDF">pdf</a>, <a href="/format/2310.13480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized identification, prediction, and stimulation of neural  oscillations via data-driven models of epileptic network dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Dubcek%2C+T">Tena Dubcek</a>, 
<a href="/search/q-bio?searchtype=author&query=Ledergerber%2C+D">Debora Ledergerber</a>, 
<a href="/search/q-bio?searchtype=author&query=Thomann%2C+J">Jana Thomann</a>, 
<a href="/search/q-bio?searchtype=author&query=Aiello%2C+G">Giovanna Aiello</a>, 
<a href="/search/q-bio?searchtype=author&query=Serra-Garcia%2C+M">Marc Serra-Garcia</a>, 
<a href="/search/q-bio?searchtype=author&query=Imbach%2C+L">Lukas Imbach</a>, 
<a href="/search/q-bio?searchtype=author&query=Polania%2C+R">Rafael Polania</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4+2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Adaptation and Self-Organizing Systems (nlin.AO); Medical Physics (physics.med-ph); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Neural oscillations are considered to be brain-specific signatures of
information processing and communication in the brain. They also reflect
pathological brain activity in neurological disorders, thus offering a basis
for diagnoses and forecasting. Epilepsy is one of the most common neurological
disorders, characterized by abnormal synchronization and desynchronization of
the oscillations in the brain. About one third of epilepsy cases are
pharmacoresistant, and as such emphasize the need for novel therapy approaches,
where brain stimulation appears to be a promising therapeutic option. The
development of brain stimulation paradigms, however, is often based on
generalized assumptions about brain dynamics, although it is known that
significant differences occur between patients and brain states. We developed a
framework to extract individualized predictive models of epileptic network
dynamics directly from EEG data. The models are based on the dominant coherent
oscillations and their dynamical coupling, thus combining an established
interpretation of dynamics through neural oscillations, with accurate
patient-specific features. We show that it is possible to build a direct
correspondence between the models of brain-network dynamics under periodic
driving, and the mechanism of neural entrainment via periodic stimulation. When
our framework is applied to EEG recordings of patients in status epilepticus (a
brain state of perpetual seizure activity), it yields a model-driven predictive
analysis of the therapeutic performance of periodic brain stimulation. This
suggests that periodic brain stimulation can drive pathological states of
epileptic network dynamics towards a healthy functional brain state.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13524" title="Abstract">arXiv:2310.13524</a> (cross-list from quant-ph) [<a href="/pdf/2310.13524" title="Download PDF">pdf</a>, <a href="/format/2310.13524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational measurement-based quantum computation for generative  modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Majumder%2C+A">Arunava Majumder</a>, 
<a href="/search/quant-ph?searchtype=author&query=Krumm%2C+M">Marius Krumm</a>, 
<a href="/search/quant-ph?searchtype=author&query=Radkohl%2C+T">Tina Radkohl</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nautrup%2C+H+P">Hendrik Poulsen Nautrup</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jerbi%2C+S">Sofiene Jerbi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Briegel%2C+H+J">Hans J. Briegel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Measurement-based quantum computation (MBQC) offers a fundamentally unique
paradigm to design quantum algorithms. Indeed, due to the inherent randomness
of quantum measurements, the natural operations in MBQC are not deterministic
and unitary, but are rather augmented with probabilistic byproducts. Yet, the
main algorithmic use of MBQC so far has been to completely counteract this
probabilistic nature in order to simulate unitary computations expressed in the
circuit model. In this work, we propose designing MBQC algorithms that embrace
this inherent randomness and treat the random byproducts in MBQC as a resource
for computation. As a natural application where randomness can be beneficial,
we consider generative modeling, a task in machine learning centered around
generating complex probability distributions. To address this task, we propose
a variational MBQC algorithm equipped with control parameters that allow to
directly adjust the degree of randomness to be admitted in the computation. Our
numerical findings indicate that this additional randomness can lead to
significant gains in learning performance in certain generative modeling tasks.
These results highlight the potential advantages in exploiting the inherent
randomness of MBQC and motivate further research into MBQC-based algorithms.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13563" title="Abstract">arXiv:2310.13563</a> (cross-list from math.CO) [<a href="/pdf/2310.13563" title="Download PDF">pdf</a>, <a href="/ps/2310.13563" title="Download PostScript">ps</a>, <a href="/format/2310.13563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trifferent codes with small lengths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kurz%2C+S">Sascha Kurz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">A code $C \subseteq \{0, 1, 2\}^n$ of length $n$ is called trifferent if for
any three distinct elements of $C$ there exists a coordinate in which they all
differ. By $T(n)$ we denote the maximum cardinality of trifferent codes with
length. $T(5)=10$ and $T(6)=13$ were recently determined. Here we determine
$T(7)=16$, $T(8)=20$, and $T(9)=27$. For the latter case $n=9$ there also exist
linear codes attaining the maximum possible cardinality $27$.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13574" title="Abstract">arXiv:2310.13574</a> (cross-list from eess.IV) [<a href="/pdf/2310.13574" title="Download PDF">pdf</a>, <a href="/format/2310.13574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive Dual Priori Network for Generalized Breast Tumor  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Lihui Wang</a>, 
<a href="/search/eess?searchtype=author&query=Kuai%2C+Z">Zixiang Kuai</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+L">Lei Tang</a>, 
<a href="/search/eess?searchtype=author&query=Ou%2C+Y">Yingfeng Ou</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+C">Chen Ye</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+Y">Yuemin Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">To promote the generalization ability of breast tumor segmentation models, as
well as to improve the segmentation performance for breast tumors with smaller
size, low-contrast amd irregular shape, we propose a progressive dual priori
network (PDPNet) to segment breast tumors from dynamic enhanced magnetic
resonance images (DCE-MRI) acquired at different sites. The PDPNet first
cropped tumor regions with a coarse-segmentation based localization module,
then the breast tumor mask was progressively refined by using the weak semantic
priori and cross-scale correlation prior knowledge. To validate the
effectiveness of PDPNet, we compared it with several state-of-the-art methods
on multi-center datasets. The results showed that, comparing against the
suboptimal method, the DSC, SEN, KAPPA and HD95 of PDPNet were improved 3.63\%,
8.19\%, 5.52\%, and 3.66\% respectively. In addition, through ablations, we
demonstrated that the proposed localization module can decrease the influence
of normal tissues and therefore improve the generalization ability of the
model. The weak semantic priors allow focusing on tumor regions to avoid
missing small tumors and low-contrast tumors. The cross-scale correlation
priors are beneficial for promoting the shape-aware ability for irregual
tumors. Thus integrating them in a unified framework improved the multi-center
breast tumor segmentation performance.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13604" title="Abstract">arXiv:2310.13604</a> (cross-list from eess.IV) [<a href="/pdf/2310.13604" title="Download PDF">pdf</a>, <a href="/format/2310.13604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skin Lesion Segmentation Improved by Transformer-based Networks with  Inter-scale Dependency Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Eskandari%2C+S">Sania Eskandari</a>, 
<a href="/search/eess?searchtype=author&query=Lumpp%2C+J">Janet Lumpp</a>, 
<a href="/search/eess?searchtype=author&query=Giraldo%2C+L+S">Luis Sanchez Giraldo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Melanoma, a dangerous type of skin cancer resulting from abnormal skin cell
growth, can be treated if detected early. Various approaches using Fully
Convolutional Networks (FCNs) have been proposed, with the U-Net architecture
being prominent To aid in its diagnosis through automatic skin lesion
segmentation. However, the symmetrical U-Net model's reliance on convolutional
operations hinders its ability to capture long-range dependencies crucial for
accurate medical image segmentation. Several Transformer-based U-Net topologies
have recently been created to overcome this limitation by replacing CNN blocks
with different Transformer modules to capture local and global representations.
Furthermore, the U-shaped structure is hampered by semantic gaps between the
encoder and decoder. This study intends to increase the network's feature
re-usability by carefully building the skip connection path. Integrating an
already calculated attention affinity within the skip connection path improves
the typical concatenation process utilized in the conventional skip connection
path. As a result, we propose a U-shaped hierarchical Transformer-based
structure for skin lesion segmentation and an Inter-scale Context Fusion (ISCF)
method that uses attention correlations in each stage of the encoder to
adaptively combine the contexts from each stage to mitigate semantic gaps. The
findings from two skin lesion segmentation benchmarks support the ISCF module's
applicability and effectiveness. The code is publicly available at
\url{https://github.com/saniaesk/skin-lesion-segmentation}
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13616" title="Abstract">arXiv:2310.13616</a> (cross-list from math.CO) [<a href="/pdf/2310.13616" title="Download PDF">pdf</a>, <a href="/format/2310.13616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Copnumbers of periodic graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=De+Carufel%2C+J">Jean-Lou De Carufel</a>, 
<a href="/search/math?searchtype=author&query=Flocchini%2C+P">Paola Flocchini</a>, 
<a href="/search/math?searchtype=author&query=Santoro%2C+N">Nicola Santoro</a>, 
<a href="/search/math?searchtype=author&query=Simard%2C+F">Fr&#xe9;d&#xe9;ric Simard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the proceedings of the 54th Southeastern International Conference on Combinatorics, Graph Theory &amp; Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">A periodic temporal graph $\mathcal{G}=(G_0, G_1, \dots, G_{p-1})^*$ is an
infinite periodic sequence of graphs $G_i=(V,E_i)$ where $G=(V,\cup_i E_i)$ is
called the footprint. Recently, the arena where the Cops and Robber game is
played has been extended from a graph to a periodic graph; in this case, the
copnumber is also the minimum number of cops sufficient for capturing the
robber. We study the connections and distinctions between the copnumber
$c(\mathcal{G})$ of a periodic graph $\mathcal{G}$ and the copnumber $c(G)$ of
its footprint $G$ and establish several facts. For instance, we show that the
smallest periodic graph with $c(\mathcal{G}) = 3$ has at most $8$ nodes; in
contrast, the smallest graph $G$ with $c(G) = 3$ has $10$ nodes. We push this
investigation by generating multiple examples showing how the copnumbers of a
periodic graph $\mathcal{G}$, the subgraphs $G_i$ and its footprint $G$ can be
loosely tied. Based on these results, we derive upper bounds on the copnumber
of a periodic graph from properties of its footprint such as its treewidth.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13628" title="Abstract">arXiv:2310.13628</a> (cross-list from quant-ph) [<a href="/pdf/2310.13628" title="Download PDF">pdf</a>, <a href="/format/2310.13628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SurfaceNet: Fault-Tolerant Quantum Networks with Surface Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Hu%2C+T">Tianjie Hu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+J">Jindi Wu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+Q">Qun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version of paper submitted to IEEE Network
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Quantum networks serve as the means to transmit information, encoded in
quantum bits or qubits, between quantum processors that are physically
separated. Given the instability of qubits, the design of such networks is
challenging, necessitating a careful balance between reliability and
efficiency. Typically, quantum networks fall into two categories: those utilize
quantum entanglements for quantum teleportation, and those directly transfer
quantum message. In this paper, we present SurfaceNet, a quantum network in the
second category that employs surface codes as logical qubits for preserving and
transferring message. Our approach of using surface codes can fault-tolerantly
correct both operational and photon loss errors within the network. We propose
a novel one-way quantum communication procedure, designed to better integrate
surface codes into our network architecture. We also propose an efficient
routing protocol that optimizes resource utilization for our communication
procedure. Simulation results demonstrate that SurfaceNet significantly
enhances the overall communication fidelity.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13633" title="Abstract">arXiv:2310.13633</a> (cross-list from eess.SP) [<a href="/pdf/2310.13633" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Free space optics communication system design using iterative  optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lema%2C+G+G">Gebrehiwet Gebrekrstos Lema</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures, 4 tables, original research paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Optical Communications, 2020, pp. 000010151520200007
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Free Space Optics (FSO) communication provides attractive bandwidth
enhancement with unlicensed bands worldwide spectrum. However, the link
capacity and availability are the major concern in the different atmospheric
conditions. The reliability of the link is highly dependent on weather
conditions that attenuate the signal strength. Hence, this study focuses to
mitigate the weather and geographic effects using iterative optimization on FSO
communication. The optimization maximizes the visibility distance while
guaranteeing the reliability by minimizing the Bit Error Rate (BER). The
wireless optical communication system is designed for the data rate of 10 Gbps.
The performance of the proposed wireless optical communication is compared
against the literature in terms of visibility distance, quality factor, BER,
and Eye diagram at different atmospheric conditions. The simulation results
have shown that the proposed work has achieved better performance.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13635" title="Abstract">arXiv:2310.13635</a> (cross-list from math.OC) [<a href="/pdf/2310.13635" title="Download PDF">pdf</a>, <a href="/ps/2310.13635" title="Download PostScript">ps</a>, <a href="/format/2310.13635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A mathematical study of joint image reconstruction and motion estimation  using optimal control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wei%2C+Z">Zhentong Wei</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chong Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Spatiotemporal dynamic medical imaging is critical in clinical applications,
such as tomographic imaging of the heart or lung. To address such kind of
spatiotemporal imaging problems, essentially, a time-dependent dynamic inverse
problem, the variational model with intensity, edge feature and topology
preservations was proposed for joint image reconstruction and motion estimation
in the previous paper [C. Chen, B. Gris, and O. \"Oktem, SIAM J. Imaging Sci.,
12 (2019), pp. 1686--1719], which is suitable to invert the time-dependent
sparse sampling data for the motion target with large diffeomorphic
deformations. However, the existence of solution to the model has not been
given yet. In order to preserve its topological structure and edge feature of
the motion target, the unknown velocity field in the model is restricted into
the admissible Hilbert space, and the unknown template image is modeled in the
space of bounded variation functions. Under this framework, this paper analyzes
and proves the solution existence of its time-discretized version from the
point view of optimal control. Specifically, there exists a constraint of
transport equation in the equivalent optimal control model. We rigorously
demonstrate the closure of the equation, including the solution existence and
uniqueness, the stability of the associated nonlinear solution operator, and
the convergence. Finally, the solution existence of that model can be
concluded.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13653" title="Abstract">arXiv:2310.13653</a> (cross-list from stat.ML) [<a href="/pdf/2310.13653" title="Download PDF">pdf</a>, <a href="/format/2310.13653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport for Measures with Noisy Tree Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Le%2C+T">Tam Le</a>, 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+T">Truyen Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Fukumizu%2C+K">Kenji Fukumizu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study optimal transport (OT) problem for probability measures supported on
a tree metric space. It is known that such OT problem (i.e., tree-Wasserstein
(TW)) admits a closed-form expression, but depends fundamentally on the
underlying tree structure over supports of input measures. In practice, the
given tree structure may be, however, perturbed due to noisy or adversarial
measurements. In order to mitigate this issue, we follow the max-min robust OT
approach which considers the maximal possible distances between two input
measures over an uncertainty set of tree metrics. In general, this approach is
hard to compute, even for measures supported in $1$-dimensional space, due to
its non-convexity and non-smoothness which hinders its practical applications,
especially for large-scale settings. In this work, we propose \emph{novel
uncertainty sets of tree metrics} from the lens of edge deletion/addition which
covers a diversity of tree structures in an elegant framework. Consequently, by
building upon the proposed uncertainty sets, and leveraging the tree structure
over supports, we show that the max-min robust OT also admits a closed-form
expression for a fast computation as its counterpart standard OT (i.e., TW).
Furthermore, we demonstrate that the max-min robust OT satisfies the metric
property and is negative definite. We then exploit its negative definiteness to
propose \emph{positive definite kernels} and test them in several simulations
on various real-world datasets on document classification and topological data
analysis for measures with noisy tree metric.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13694" title="Abstract">arXiv:2310.13694</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.13694" title="Download PDF">pdf</a>, <a href="/format/2310.13694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Studying speed-accuracy trade-offs in best-of-n collective  decision-making through heterogeneous mean-field modelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Reina%2C+A">Andreagiovanni Reina</a>, 
<a href="/search/physics?searchtype=author&query=Njougouo%2C+T">Thierry Njougouo</a>, 
<a href="/search/physics?searchtype=author&query=Tuci%2C+E">Elio Tuci</a>, 
<a href="/search/physics?searchtype=author&query=Carletti%2C+T">Timoteo Carletti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Multiagent Systems (cs.MA); Robotics (cs.RO)

</div>
<p class="mathjax">To succeed in their objectives, groups of individuals must be able to make
quick and accurate collective decisions on the best among alternatives with
different qualities. Group-living animals aim to do that all the time. Plants
and fungi are thought to do so too. Swarms of autonomous robots can also be
programmed to make best-of-n decisions for solving tasks collaboratively.
Ultimately, humans critically need it and so many times they should be better
at it! Despite their simplicity, mathematical tractability made models like the
voter model (VM) and the local majority rule model (MR) useful to describe in
simple terms such collective decision-making processes. To reach a consensus,
individuals change their opinion by interacting with neighbours in their social
network. At least among animals and robots, options with a better quality are
exchanged more often and therefore spread faster than lower-quality options,
leading to the collective selection of the best option. With our work, we study
the impact of individuals making errors in pooling others' opinions caused, for
example, to reduce the cognitive load. Our analysis in grounded on the
introduction of a model that generalises the two existing VM and MR models,
showing a speed-accuracy trade-off regulated by the cognitive effort of
individuals. We also investigate the impact of the interaction network topology
on the collective dynamics. To do so, we extend our model and, by using the
heterogeneous mean-field approach, we show that another speed-accuracy
trade-off is regulated by network connectivity. An interesting result is that
reduced network connectivity corresponds to an increase in collective decision
accuracy
</p>
</div>
</dd>
</dl>
<h3>Replacements for Mon, 23 Oct 23</h3>
<dl>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.02516" title="Abstract">arXiv:2002.02516</a> (replaced) [<a href="/pdf/2002.02516" title="Download PDF">pdf</a>, <a href="/ps/2002.02516" title="Download PostScript">ps</a>, <a href="/format/2002.02516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the $O(\sqrt n)$-Bit Barrier: Byzantine Agreement with Polylog  Bits Per Party
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boyle%2C+E">Elette Boyle</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+R">Ran Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+A">Aarushi Goel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preliminary version appeared in PODC'21; full version appeared in Journal of Cryptology 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.01777" title="Abstract">arXiv:2007.01777</a> (replaced) [<a href="/pdf/2007.01777" title="Download PDF">pdf</a>, <a href="/format/2007.01777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Sequence Classification Via Prototype Trajectory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+D">Dat Hong</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+S+S">Stephen S. Baek</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.11152" title="Abstract">arXiv:2011.11152</a> (replaced) [<a href="/pdf/2011.11152" title="Download PDF">pdf</a>, <a href="/format/2011.11152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A  Gradient-Norm Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zeke Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiqiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+I">Issei Sato</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, 21 pages, 20 figures. Keywords: Weight Decay, Regularization, Optimization, Deep Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.06969" title="Abstract">arXiv:2101.06969</a> (replaced) [<a href="/pdf/2101.06969" title="Download PDF">pdf</a>, <a href="/format/2101.06969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Red Alarm for Pre-trained Models: Universal Vulnerability to  Neuron-Level Backdoor Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+G">Guangxuan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+T">Tian Lv</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+F">Fanchao Qi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Machine Intelligence Research (<a href="https://link.springer.com/article/10.1007/s11633-022-1377-5">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.01741" title="Abstract">arXiv:2106.01741</a> (replaced) [<a href="/pdf/2106.01741" title="Download PDF">pdf</a>, <a href="/format/2106.01741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lifetime policy reuse and the importance of task capacity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bossens%2C+D+M">David M. Bossens</a>, 
<a href="/search/cs?searchtype=author&query=Sobey%2C+A+J">Adam J. Sobey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.12915" title="Abstract">arXiv:2106.12915</a> (replaced) [<a href="/pdf/2106.12915" title="Download PDF">pdf</a>, <a href="/format/2106.12915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical influence of ReLU&#x27;(0) on backpropagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertoin%2C+D">David Bertoin</a> (ISAE-SUPAERO), 
<a href="/search/cs?searchtype=author&query=Bolte%2C+J">J&#xe9;r&#xf4;me Bolte</a> (TSE), 
<a href="/search/cs?searchtype=author&query=Gerchinovitz%2C+S">S&#xe9;bastien Gerchinovitz</a> (IMT), 
<a href="/search/cs?searchtype=author&query=Pauwels%2C+E">Edouard Pauwels</a> (IRIT-ADRIA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.06333" title="Abstract">arXiv:2109.06333</a> (replaced) [<a href="/pdf/2109.06333" title="Download PDF">pdf</a>, <a href="/format/2109.06333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connecting degree and polarity: An artificial language learning study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bylinina%2C+L">Lisa Bylinina</a>, 
<a href="/search/cs?searchtype=author&query=Tikhonov%2C+A">Alexey Tikhonov</a>, 
<a href="/search/cs?searchtype=author&query=Garmash%2C+E">Ekaterina Garmash</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.06275" title="Abstract">arXiv:2112.06275</a> (replaced) [<a href="/pdf/2112.06275" title="Download PDF">pdf</a>, <a href="/format/2112.06275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Restless Bandit Model for Energy-Efficient Job Assignments in Server  Farms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fu%2C+J">Jing Fu</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xinyu Wang</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Z">Zengfu Wang</a>, 
<a href="/search/math?searchtype=author&query=Zukerman%2C+M">Moshe Zukerman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.07292" title="Abstract">arXiv:2112.07292</a> (replaced) [<a href="/pdf/2112.07292" title="Download PDF">pdf</a>, <a href="/format/2112.07292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verifying an Effect-Handler-Based Define-By-Run Reverse-Mode AD Library
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Vilhena%2C+P+E">Paulo Em&#xed;lio de Vilhena</a>, 
<a href="/search/cs?searchtype=author&query=Pottier%2C+F">Fran&#xe7;ois Pottier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.06776" title="Abstract">arXiv:2201.06776</a> (replaced) [<a href="/pdf/2201.06776" title="Download PDF">pdf</a>, <a href="/format/2201.06776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pruning-aware Sparse Regularization for Network Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nanfei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chaoyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+Y">Yongqi An</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+M">Ming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinqiao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MIR 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Machine Intelligence Research, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.05185" title="Abstract">arXiv:2204.05185</a> (replaced) [<a href="/pdf/2204.05185" title="Download PDF">pdf</a>, <a href="/format/2204.05185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform Complexity for Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imperial%2C+J+M">Joseph Marvin Imperial</a>, 
<a href="/search/cs?searchtype=author&query=Madabushi%2C+H+T">Harish Tayyar Madabushi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final camera-ready for EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00934" title="Abstract">arXiv:2206.00934</a> (replaced) [<a href="/pdf/2206.00934" title="Download PDF">pdf</a>, <a href="/format/2206.00934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep neural networks can stably solve high-dimensional, noisy,  non-linear inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pineda%2C+A+F+L">Andr&#xe9;s Felipe Lerma Pineda</a>, 
<a href="/search/math?searchtype=author&query=Petersen%2C+P+C">Philipp Christian Petersen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.01178" title="Abstract">arXiv:2206.01178</a> (replaced) [<a href="/pdf/2206.01178" title="Download PDF">pdf</a>, <a href="/format/2206.01178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discretization Invariant Networks for Learning Maps between Neural  Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C+J">Clinton J. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Golland%2C+P">Polina Golland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Transactions on Machine Learning Research 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.09920" title="Abstract">arXiv:2207.09920</a> (replaced) [<a href="/pdf/2207.09920" title="Download PDF">pdf</a>, <a href="/ps/2207.09920" title="Download PostScript">ps</a>, <a href="/format/2207.09920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+K">Kailiang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+F">Fengtong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yaorong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wenqing Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaofeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cen%2C+L">Ling Cen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by SIGKDD 2022 Applied Data Science Track
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.10853" title="Abstract">arXiv:2207.10853</a> (replaced) [<a href="/pdf/2207.10853" title="Download PDF">pdf</a>, <a href="/ps/2207.10853" title="Download PostScript">ps</a>, <a href="/format/2207.10853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Estimate of Multiscale Finite Element Method for Periodic Media  Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ming%2C+P">Pingbing Ming</a>, 
<a href="/search/math?searchtype=author&query=Song%2C+S">Siqi Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.07502" title="Abstract">arXiv:2208.07502</a> (replaced) [<a href="/pdf/2208.07502" title="Download PDF">pdf</a>, <a href="/format/2208.07502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combinatorial optimization solving by coherent Ising machines based on  spiking neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Lu%2C+B">Bo Lu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gao%2C+Y">Yong-Pan Gao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wen%2C+K">Kai Wen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+C">Chuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, accepted by Quantum
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.10790" title="Abstract">arXiv:2208.10790</a> (replaced) [<a href="/pdf/2208.10790" title="Download PDF">pdf</a>, <a href="/format/2208.10790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-Triggered Time-Varying Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brunzema%2C+P">Paul Brunzema</a>, 
<a href="/search/cs?searchtype=author&query=von+Rohr%2C+A">Alexander von Rohr</a>, 
<a href="/search/cs?searchtype=author&query=Solowjow%2C+F">Friedrich Solowjow</a>, 
<a href="/search/cs?searchtype=author&query=Trimpe%2C+S">Sebastian Trimpe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05112" title="Abstract">arXiv:2209.05112</a> (replaced) [<a href="/pdf/2209.05112" title="Download PDF">pdf</a>, <a href="/ps/2209.05112" title="Download PostScript">ps</a>, <a href="/format/2209.05112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A challenge-based survey of e-recruitment recommendation systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mashayekhi%2C+Y">Yoosof Mashayekhi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nan Li</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+B">Bo Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lijffijt%2C+J">Jefrey Lijffijt</a>, 
<a href="/search/cs?searchtype=author&query=De+Bie%2C+T">Tijl De Bie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07825" title="Abstract">arXiv:2209.07825</a> (replaced) [<a href="/pdf/2209.07825" title="Download PDF">pdf</a>, <a href="/format/2209.07825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A System of Interaction and Structure III: The Complexity of BV and  Pomset Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguy%C3%AAn%2C+L+T+D">L&#xea; Th&#xe0;nh D&#x169;ng Nguy&#xea;n</a>, 
<a href="/search/cs?searchtype=author&query=Stra%C3%9Fburger%2C+L">Lutz Stra&#xdf;burger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.10507" title="Abstract">arXiv:2209.10507</a> (replaced) [<a href="/pdf/2209.10507" title="Download PDF">pdf</a>, <a href="/format/2209.10507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gemino: Practical and Robust Neural Compression for Video Conferencing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaraman%2C+V">Vibhaalakshmi Sivaraman</a>, 
<a href="/search/cs?searchtype=author&query=Karimi%2C+P">Pantea Karimi</a>, 
<a href="/search/cs?searchtype=author&query=Venkatapathy%2C+V">Vedantha Venkatapathy</a>, 
<a href="/search/cs?searchtype=author&query=Khani%2C+M">Mehrdad Khani</a>, 
<a href="/search/cs?searchtype=author&query=Fouladi%2C+S">Sadjad Fouladi</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+M">Mohammad Alizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Durand%2C+F">Fr&#xe9;do Durand</a>, 
<a href="/search/cs?searchtype=author&query=Sze%2C+V">Vivienne Sze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 appendix
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> USENIX NSDI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05242" title="Abstract">arXiv:2210.05242</a> (replaced) [<a href="/pdf/2210.05242" title="Download PDF">pdf</a>, <a href="/format/2210.05242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging the Video-level Semantic Consistency of Event for  Audio-visual Event Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuanyuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jianqin Yin</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+Y">Yonghao Dang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures, Accepted by IEEE Transactions on Multimedia
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11388" title="Abstract">arXiv:2210.11388</a> (replaced) [<a href="/pdf/2210.11388" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed Deep Diffusion MRI Reconstruction with Clinical Data  Evaluation: Break Training Data Bottleneck in Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+Y">Yuncheng Gao</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+M">Mingyang Han</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ruan%2C+D">Dan Ruan</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+Y">Yu Shen</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Yiping Wu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yirong Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Chengyan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+B">Boyu Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Tao%2C+R">Ran Tao</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Z">Zhigang Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiazheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Liuhong Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+Y">Yi Guo</a>, 
<a href="/search/eess?searchtype=author&query=Kang%2C+T">Taishan Kang</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+J">Jianzhong Lin</a>, 
<a href="/search/eess?searchtype=author&query=Gong%2C+T">Tao Gong</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+C">Chen Yang</a>, 
<a href="/search/eess?searchtype=author&query=Fei%2C+G">Guoqiang Fei</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+M">Meijin Lin</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+D">Di Guo</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jianjun Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+M">Meiyun Wang</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+X">Xiaobo Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.12407" title="Abstract">arXiv:2210.12407</a> (replaced) [<a href="/pdf/2210.12407" title="Download PDF">pdf</a>, <a href="/ps/2210.12407" title="Download PostScript">ps</a>, <a href="/format/2210.12407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two new families of fourth-order explicit exponential Runge-Kutta  methods with four stages for stiff or highly oscillatory systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+X">Xianfa Hu</a>, 
<a href="/search/math?searchtype=author&query=Fang%2C+Y">Yonglei Fang</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+B">Bin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.12595" title="Abstract">arXiv:2210.12595</a> (replaced) [<a href="/pdf/2210.12595" title="Download PDF">pdf</a>, <a href="/format/2210.12595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Probabilistic Model Identification using Adaptive Recursive MCMC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agand%2C+P">Pedram Agand</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Taghirad%2C+H+D">Hamid D. Taghirad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures, 3 tables. 2023 International Joint Conference on Neural Networks (IJCNN). IEEE, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.17426" title="Abstract">arXiv:2210.17426</a> (replaced) [<a href="/pdf/2210.17426" title="Download PDF">pdf</a>, <a href="/format/2210.17426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trade-off Between Efficiency and Consistency for Removal-based  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Haowei He</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhiquan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yang Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Code: <a href="https://github.com/trusty-ai/efficient-consistent-explanation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.02809" title="Abstract">arXiv:2211.02809</a> (replaced) [<a href="/pdf/2211.02809" title="Download PDF">pdf</a>, <a href="/format/2211.02809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAMASSU: Streaming Language-Agnostic Multilingual Speech Recognition and  Translation Using Neural Transducers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+E">Eric Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jian Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Long Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+Y">Yashesh Gaur</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shujie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> INTERSPEECH 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04807" title="Abstract">arXiv:2211.04807</a> (replaced) [<a href="/pdf/2211.04807" title="Download PDF">pdf</a>, <a href="/format/2211.04807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A nonsmooth primal-dual method with interwoven PDE constraint solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jensen%2C+B">Bj&#xf8;rn Jensen</a>, 
<a href="/search/math?searchtype=author&query=Valkonen%2C+T">Tuomo Valkonen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07208" title="Abstract">arXiv:2211.07208</a> (replaced) [<a href="/pdf/2211.07208" title="Download PDF">pdf</a>, <a href="/ps/2211.07208" title="Download PostScript">ps</a>, <a href="/format/2211.07208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lego-Brick Approach to Coding for Network Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghaddar%2C+N">Nadim Ghaddar</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+S">Shouvik Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lele Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Han Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08203" title="Abstract">arXiv:2211.08203</a> (replaced) [<a href="/pdf/2211.08203" title="Download PDF">pdf</a>, <a href="/format/2211.08203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Frequency Distortion of Word Embeddings and Its Impact  on Bias Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valentini%2C+F">Francisco Valentini</a>, 
<a href="/search/cs?searchtype=author&query=Sosa%2C+J+C">Juan Cruz Sosa</a>, 
<a href="/search/cs?searchtype=author&query=Slezak%2C+D+F">Diego Fernandez Slezak</a>, 
<a href="/search/cs?searchtype=author&query=Altszyler%2C+E">Edgar Altszyler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready for EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11363" title="Abstract">arXiv:2211.11363</a> (replaced) [<a href="/pdf/2211.11363" title="Download PDF">pdf</a>, <a href="/format/2211.11363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AF Adapter: Continual Pretraining for Building Chinese Biomedical  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yongyu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+K">Kui Xue</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaoming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+T">Tong Ruan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14312" title="Abstract">arXiv:2211.14312</a> (replaced) [<a href="/pdf/2211.14312" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Karyotype AI for Precision Oncology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Shamsi%2C+Z">Zahra Shamsi</a>, 
<a href="/search/q-bio?searchtype=author&query=Bryant%2C+D">Drew Bryant</a>, 
<a href="/search/q-bio?searchtype=author&query=Wilson%2C+J">Jacob Wilson</a>, 
<a href="/search/q-bio?searchtype=author&query=Qu%2C+X">Xiaoyu Qu</a>, 
<a href="/search/q-bio?searchtype=author&query=Dubey%2C+A">Avinava Dubey</a>, 
<a href="/search/q-bio?searchtype=author&query=Kothari%2C+K">Konik Kothari</a>, 
<a href="/search/q-bio?searchtype=author&query=Dehghani%2C+M">Mostafa Dehghani</a>, 
<a href="/search/q-bio?searchtype=author&query=Chavarha%2C+M">Mariya Chavarha</a>, 
<a href="/search/q-bio?searchtype=author&query=Likhosherstov%2C+V">Valerii Likhosherstov</a>, 
<a href="/search/q-bio?searchtype=author&query=Williams%2C+B">Brian Williams</a>, 
<a href="/search/q-bio?searchtype=author&query=Frumkin%2C+M">Michael Frumkin</a>, 
<a href="/search/q-bio?searchtype=author&query=Appelbaum%2C+F">Fred Appelbaum</a>, 
<a href="/search/q-bio?searchtype=author&query=Choromanski%2C+K">Krzysztof Choromanski</a>, 
<a href="/search/q-bio?searchtype=author&query=Bashir%2C+A">Ali Bashir</a>, 
<a href="/search/q-bio?searchtype=author&query=Fang%2C+M">Min Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.16773" title="Abstract">arXiv:2211.16773</a> (replaced) [<a href="/pdf/2211.16773" title="Download PDF">pdf</a>, <a href="/format/2211.16773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KRLS: Improving End-to-End Response Generation in Task Oriented Dialog  with Reinforced Keywords Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kun Qian</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00060" title="Abstract">arXiv:2212.00060</a> (replaced) [<a href="/pdf/2212.00060" title="Download PDF">pdf</a>, <a href="/format/2212.00060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capacity of an infinite family of networks related to the diamond  network for fixed alphabet sizes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurz%2C+S">Sascha Kurz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 tables, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02083" title="Abstract">arXiv:2212.02083</a> (replaced) [<a href="/pdf/2212.02083" title="Download PDF">pdf</a>, <a href="/format/2212.02083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Overlooked Structure of Stochastic Gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zeke Xie</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Q">Qian-Yuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Ping Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. 20 pages, 16 figures, 17 Tables; Key Words: Deep Learning, Stochastic Gradient, Optimization. arXiv admin note: text overlap with <a href="/abs/2201.13011">arXiv:2201.13011</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03416" title="Abstract">arXiv:2212.03416</a> (replaced) [<a href="/pdf/2212.03416" title="Download PDF">pdf</a>, <a href="/format/2212.03416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Spectral Bias Reduction of Multi-scale Neural Networks for Regression  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/math?searchtype=author&query=Yuan%2C+H">Heng Yuan</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+L">Lizuo Liu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+W">Wenzhong Zhang</a>, 
<a href="/search/math?searchtype=author&query=Cai%2C+W">Wei Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10341" title="Abstract">arXiv:2212.10341</a> (replaced) [<a href="/pdf/2212.10341" title="Download PDF">pdf</a>, <a href="/format/2212.10341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data  Limitation With Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaohan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+H">Hang Pu</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yu Lan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main cofference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11237" title="Abstract">arXiv:2212.11237</a> (replaced) [<a href="/pdf/2212.11237" title="Download PDF">pdf</a>, <a href="/format/2212.11237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not Just Pretty Pictures: Toward Interventional Data Augmentation Using  Text-to-Image Generators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianhao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Pinto%2C+F">Francesco Pinto</a>, 
<a href="/search/cs?searchtype=author&query=Davies%2C+A">Adam Davies</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11854" title="Abstract">arXiv:2212.11854</a> (replaced) [<a href="/pdf/2212.11854" title="Download PDF">pdf</a>, <a href="/format/2212.11854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Centric Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jakubik%2C+J">Johannes Jakubik</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%B6ssing%2C+M">Michael V&#xf6;ssing</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChl%2C+N">Niklas K&#xfc;hl</a>, 
<a href="/search/cs?searchtype=author&query=Walk%2C+J">Jannis Walk</a>, 
<a href="/search/cs?searchtype=author&query=Satzger%2C+G">Gerhard Satzger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00200" title="Abstract">arXiv:2301.00200</a> (replaced) [<a href="/pdf/2301.00200" title="Download PDF">pdf</a>, <a href="/format/2301.00200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logic Mill -- A Knowledge Navigation System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erhardt%2C+S">Sebastian Erhardt</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+M">Mainak Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Buunk%2C+E">Erik Buunk</a>, 
<a href="/search/cs?searchtype=author&query=Rose%2C+M+E">Michael E. Rose</a>, 
<a href="/search/cs?searchtype=author&query=Harhoff%2C+D">Dietmar Harhoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03462" title="Abstract">arXiv:2301.03462</a> (replaced) [<a href="/pdf/2301.03462" title="Download PDF">pdf</a>, <a href="/format/2301.03462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Power of Shared Label Structures for Human Activity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+R+R">Ranak Roy Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+D">Dezhi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R+K">Rajesh K. Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05217" title="Abstract">arXiv:2301.05217</a> (replaced) [<a href="/pdf/2301.05217" title="Download PDF">pdf</a>, <a href="/format/2301.05217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progress measures for grokking via mechanistic interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nanda%2C+N">Neel Nanda</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+L">Lawrence Chan</a>, 
<a href="/search/cs?searchtype=author&query=Lieberum%2C+T">Tom Lieberum</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+J">Jess Smith</a>, 
<a href="/search/cs?searchtype=author&query=Steinhardt%2C+J">Jacob Steinhardt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 page main body, 2 page references, 24 page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05253" title="Abstract">arXiv:2301.05253</a> (replaced) [<a href="/pdf/2301.05253" title="Download PDF">pdf</a>, <a href="/format/2301.05253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scalable Technique for Weak-Supervised Learning with Domain  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Sudhir Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Sreepathy%2C+A">Anu Sreepathy</a>, 
<a href="/search/cs?searchtype=author&query=Mouatadid%2C+L">Lalla Mouatadid</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures. Accepted at NeurIPS 2022 Workshop "Has it trained yet"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05294" title="Abstract">arXiv:2301.05294</a> (replaced) [<a href="/pdf/2301.05294" title="Download PDF">pdf</a>, <a href="/format/2301.05294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles  at Complex and Unsignalized Intersections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dawei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weizi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jia Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper introduces the first method to control and coordinate mixed traffic (i.e., human-driven vehicles and robot vehicles) at unsignalized intersections with both complicated topology and real-world traffic demands
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11604" title="Abstract">arXiv:2301.11604</a> (replaced) [<a href="/e-print/2301.11604" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A critical look at deep neural network for dynamic system modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jinming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yucai Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The failure of NARX model modeling a noiseless LTI system is mainly due to some initilization issues with the current Matlab SYSID Toolbox. If this procedure is done purely in the Neural Network Toolbox, the situation can be improved for great extent
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00225" title="Abstract">arXiv:2302.00225</a> (replaced) [<a href="/pdf/2302.00225" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Past, Current, and Future of Neonatal Intensive Care Units with  Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keles%2C+E">Elif Keles</a>, 
<a href="/search/cs?searchtype=author&query=Bagci%2C+U">Ulas Bagci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 90 pages, review article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00407" title="Abstract">arXiv:2302.00407</a> (replaced) [<a href="/pdf/2302.00407" title="Download PDF">pdf</a>, <a href="/format/2302.00407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Role of Morphological Information for Contextual Lemmatization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toporkov%2C+O">Olia Toporkov</a>, 
<a href="/search/cs?searchtype=author&query=Agerri%2C+R">Rodrigo Agerri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 5 figures, 11 tables; Accepted for publication in Computational Linguistics journal (to appear)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06578" title="Abstract">arXiv:2302.06578</a> (replaced) [<a href="/pdf/2302.06578" title="Download PDF">pdf</a>, <a href="/format/2302.06578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel Ridge Regression Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Singh%2C+R">Rahul Singh</a>, 
<a href="/search/math?searchtype=author&query=Vijaykumar%2C+S">Suhas Vijaykumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08397" title="Abstract">arXiv:2302.08397</a> (replaced) [<a href="/pdf/2302.08397" title="Download PDF">pdf</a>, <a href="/ps/2302.08397" title="Download PostScript">ps</a>, <a href="/format/2302.08397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Selective Sampling for Online Prediction with Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Castro%2C+R+M">Rui M. Castro</a>, 
<a href="/search/stat?searchtype=author&query=Hellstr%C3%B6m%2C+F">Fredrik Hellstr&#xf6;m</a>, 
<a href="/search/stat?searchtype=author&query=van+Erven%2C+T">Tim van Erven</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08681" title="Abstract">arXiv:2302.08681</a> (replaced) [<a href="/pdf/2302.08681" title="Download PDF">pdf</a>, <a href="/format/2302.08681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CarbonScaler: Leveraging Cloud Workload Elasticity for Optimizing  Carbon-Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanafy%2C+W+A">Walid A. Hanafy</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Q">Qianlin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Bashir%2C+N">Noman Bashir</a>, 
<a href="/search/cs?searchtype=author&query=Irwin%2C+D">David Irwin</a>, 
<a href="/search/cs?searchtype=author&query=Shenoy%2C+P">Prashant Shenoy</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. ACM Meas. Anal. Comput. Syst. 7, 3, Article 57 (December
  2023), 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12343" title="Abstract">arXiv:2302.12343</a> (replaced) [<a href="/pdf/2302.12343" title="Download PDF">pdf</a>, <a href="/format/2302.12343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical  Notes with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McInerney%2C+D+J">Denis Jered McInerney</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+G">Geoffrey Young</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Meent%2C+J">Jan-Willem van de Meent</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B+C">Byron C. Wallace</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03898" title="Abstract">arXiv:2303.03898</a> (replaced) [<a href="/pdf/2303.03898" title="Download PDF">pdf</a>, <a href="/format/2303.03898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Virtual Omnidirectional Perception for Downwash Prediction within a Team  of Nano Multirotors Flying in Close Proximity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moldagalieva%2C+A">Akmaral Moldagalieva</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6nig%2C+W">Wolfgang H&#xf6;nig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the 2023 IEEE International Symposium on Multi-Robot &amp; Multi-Agent Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04749" title="Abstract">arXiv:2303.04749</a> (replaced) [<a href="/pdf/2303.04749" title="Download PDF">pdf</a>, <a href="/ps/2303.04749" title="Download PostScript">ps</a>, <a href="/format/2303.04749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Robust Backward Reachable Sets for Set-Theoretic Model  Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Attar%2C+M">Mehran Attar</a>, 
<a href="/search/eess?searchtype=author&query=Lucia%2C+W">Walter Lucia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint jointly submitted to IEEE Control Systems Letters (L-CSS) and IEEE Conference on Decision and Control (CDC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04947" title="Abstract">arXiv:2303.04947</a> (replaced) [<a href="/pdf/2303.04947" title="Download PDF">pdf</a>, <a href="/format/2303.04947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Ziheng Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zangwei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jianyang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xiangyu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhaopan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Daquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lei Shang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Baigui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xuansong Xie</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The second version of InfoBatch, we extend it into SSL and LLM tasks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05089" title="Abstract">arXiv:2303.05089</a> (replaced) [<a href="/pdf/2303.05089" title="Download PDF">pdf</a>, <a href="/format/2303.05089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adiabatic elimination for composite open quantum systems: reduced model  formulation and numerical simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=R%C3%A9gent%2C+F+L">Fran&#xe7;ois-Marie Le R&#xe9;gent</a>, 
<a href="/search/quant-ph?searchtype=author&query=Rouchon%2C+P">Pierre Rouchon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted, major release from the previous one
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Dynamical Systems (math.DS); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06573" title="Abstract">arXiv:2303.06573</a> (replaced) [<a href="/pdf/2303.06573" title="Download PDF">pdf</a>, <a href="/format/2303.06573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Know Your Contextual Search Intent: A Prompting  Framework for Conversational Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+K">Kelong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+F">Fengran Mo</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jiewen Hou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haonan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+H">Hongjin Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023. Code: <a href="https://github.com/kyriemao/LLM4CS/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12570" title="Abstract">arXiv:2303.12570</a> (replaced) [<a href="/pdf/2303.12570" title="Download PDF">pdf</a>, <a href="/format/2303.12570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RepoCoder: Repository-Level Code Completion Through Iterative Retrieval  and Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fengji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Keung%2C+J">Jacky Keung</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zan%2C+D">Daoguang Zan</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian-Guang Lou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13780" title="Abstract">arXiv:2303.13780</a> (replaced) [<a href="/pdf/2303.13780" title="Download PDF">pdf</a>, <a href="/format/2303.13780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Making the Most of ChatGPT for Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Keqin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Q">Qihuang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuebo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+Y">Yuanxin Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14697" title="Abstract">arXiv:2303.14697</a> (replaced) [<a href="/pdf/2303.14697" title="Download PDF">pdf</a>, <a href="/ps/2303.14697" title="Download PostScript">ps</a>, <a href="/format/2303.14697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The central tree property and algorithmic problems on subgroups of free  groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Roy%2C+M">Mallika Roy</a>, 
<a href="/search/math?searchtype=author&query=Ventura%2C+E">Enric Ventura</a>, 
<a href="/search/math?searchtype=author&query=Weil%2C+P">Pascal Weil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages. Inaccuracies corrected. To appear in Journal of Group Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Group Theory (math.GR)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15433" title="Abstract">arXiv:2303.15433</a> (replaced) [<a href="/pdf/2303.15433" title="Download PDF">pdf</a>, <a href="/format/2303.15433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anti-DreamBooth: Protecting users from personalized text-to-image  synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Le%2C+T">Thanh Van Le</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+H">Hao Phung</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+H">Thuan Hoang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Dao%2C+Q">Quan Dao</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+N">Ngoc Tran</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+A">Anh Tran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023. Project page: <a href="https://anti-dreambooth.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15457" title="Abstract">arXiv:2303.15457</a> (replaced) [<a href="/pdf/2303.15457" title="Download PDF">pdf</a>, <a href="/format/2303.15457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical Study and Efficient Evaluation of the Josephus Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bello-Cruz%2C+Y">Yunier Bello-Cruz</a>, 
<a href="/search/math?searchtype=author&query=Quintero-Contreras%2C+R">Roy Quintero-Contreras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16857" title="Abstract">arXiv:2303.16857</a> (replaced) [<a href="/pdf/2303.16857" title="Download PDF">pdf</a>, <a href="/format/2303.16857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Did You Mean...? Confidence-based Trade-offs in Semantic Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stengel-Eskin%2C+E">Elias Stengel-Eskin</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, Camera ready. arXiv admin note: substantial text overlap with <a href="/abs/2211.07443">arXiv:2211.07443</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01510" title="Abstract">arXiv:2304.01510</a> (replaced) [<a href="/pdf/2304.01510" title="Download PDF">pdf</a>, <a href="/format/2304.01510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Communication-efficient Local Differentially Private Algorithm in  Federated Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+S+E">Syed Eqbal Alam</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+D">Dhirendra Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+S">Shrisha Rao</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, vol. 11, pp. 58254-58268, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02008" title="Abstract">arXiv:2304.02008</a> (replaced) [<a href="/pdf/2304.02008" title="Download PDF">pdf</a>, <a href="/format/2304.02008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GlueStick: Robust Image Matching by Sticking Points and Lines Together
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pautrat%2C+R">R&#xe9;mi Pautrat</a>, 
<a href="/search/cs?searchtype=author&query=Su%C3%A1rez%2C+I">Iago Su&#xe1;rez</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Pollefeys%2C+M">Marc Pollefeys</a>, 
<a href="/search/cs?searchtype=author&query=Larsson%2C+V">Viktor Larsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03215" title="Abstract">arXiv:2304.03215</a> (replaced) [<a href="/pdf/2304.03215" title="Download PDF">pdf</a>, <a href="/format/2304.03215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Graph Neural Network with Cross-Attention for Cross-Device  User Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taghibakhshi%2C+A">Ali Taghibakhshi</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Mingyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Aithal%2C+A">Ashwath Aithal</a>, 
<a href="/search/cs?searchtype=author&query=Yilmaz%2C+O">Onur Yilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Maron%2C+H">Haggai Maron</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+M">Matthew West</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06040" title="Abstract">arXiv:2304.06040</a> (replaced) [<a href="/pdf/2304.06040" title="Download PDF">pdf</a>, <a href="/format/2304.06040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring Population Dynamics in Macaque Cortex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Meghanath%2C+G">Ganga Meghanath</a>, 
<a href="/search/q-bio?searchtype=author&query=Jimenez%2C+B">Bryan Jimenez</a>, 
<a href="/search/q-bio?searchtype=author&query=Makin%2C+J+G">Joseph G. Makin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main text: 22 pages, 6 figures, 1 table Supplementary Material: 6 pages, 8 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08931" title="Abstract">arXiv:2304.08931</a> (replaced) [<a href="/pdf/2304.08931" title="Download PDF">pdf</a>, <a href="/format/2304.08931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Textbooks with Visuals from the Web for Improved Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+J">Janvijay Singh</a>, 
<a href="/search/cs?searchtype=author&query=Zouhar%2C+V">Vil&#xe9;m Zouhar</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023; 14 pages (8+6)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09145" title="Abstract">arXiv:2304.09145</a> (replaced) [<a href="/pdf/2304.09145" title="Download PDF">pdf</a>, <a href="/format/2304.09145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier Suppression+: Accurate quantization of large language models by  equivalent and optimal shifting and scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiuying Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangguo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+R">Ruihao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jinyang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09257" title="Abstract">arXiv:2304.09257</a> (replaced) [<a href="/pdf/2304.09257" title="Download PDF">pdf</a>, <a href="/format/2304.09257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A structure-preserving upwind DG scheme for a degenerate phase-field  tumor model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Acosta-Soba%2C+D">Daniel Acosta-Soba</a>, 
<a href="/search/math?searchtype=author&query=Guill%C3%A9n-Gonz%C3%A1lez%2C+F">Francisco Guill&#xe9;n-Gonz&#xe1;lez</a>, 
<a href="/search/math?searchtype=author&query=Galv%C3%A1n%2C+J+R+R">J. Rafael Rodr&#xed;guez Galv&#xe1;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09319" title="Abstract">arXiv:2304.09319</a> (replaced) [<a href="/pdf/2304.09319" title="Download PDF">pdf</a>, <a href="/format/2304.09319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The conditional DPP approach to random matrix distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math-ph?searchtype=author&query=Edelman%2C+A">Alan Edelman</a>, 
<a href="/search/math-ph?searchtype=author&query=Jeong%2C+S">Sungwoo Jeong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Physics (math-ph)</span>; Numerical Analysis (math.NA); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12647" title="Abstract">arXiv:2304.12647</a> (replaced) [<a href="/pdf/2304.12647" title="Download PDF">pdf</a>, <a href="/format/2304.12647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Q-learning with biased policy rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Compte%2C+O">Olivier Compte</a> (Paris School of Economics)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 19 figures, 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14108" title="Abstract">arXiv:2304.14108</a> (replaced) [<a href="/pdf/2304.14108" title="Download PDF">pdf</a>, <a href="/format/2304.14108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DataComp: In search of the next generation of multimodal datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gadre%2C+S+Y">Samir Yitzhak Gadre</a>, 
<a href="/search/cs?searchtype=author&query=Ilharco%2C+G">Gabriel Ilharco</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+A">Alex Fang</a>, 
<a href="/search/cs?searchtype=author&query=Hayase%2C+J">Jonathan Hayase</a>, 
<a href="/search/cs?searchtype=author&query=Smyrnis%2C+G">Georgios Smyrnis</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thao Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Marten%2C+R">Ryan Marten</a>, 
<a href="/search/cs?searchtype=author&query=Wortsman%2C+M">Mitchell Wortsman</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+D">Dhruba Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Orgad%2C+E">Eyal Orgad</a>, 
<a href="/search/cs?searchtype=author&query=Entezari%2C+R">Rahim Entezari</a>, 
<a href="/search/cs?searchtype=author&query=Daras%2C+G">Giannis Daras</a>, 
<a href="/search/cs?searchtype=author&query=Pratt%2C+S">Sarah Pratt</a>, 
<a href="/search/cs?searchtype=author&query=Ramanujan%2C+V">Vivek Ramanujan</a>, 
<a href="/search/cs?searchtype=author&query=Bitton%2C+Y">Yonatan Bitton</a>, 
<a href="/search/cs?searchtype=author&query=Marathe%2C+K">Kalyani Marathe</a>, 
<a href="/search/cs?searchtype=author&query=Mussmann%2C+S">Stephen Mussmann</a>, 
<a href="/search/cs?searchtype=author&query=Vencu%2C+R">Richard Vencu</a>, 
<a href="/search/cs?searchtype=author&query=Cherti%2C+M">Mehdi Cherti</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+P+W">Pang Wei Koh</a>, 
<a href="/search/cs?searchtype=author&query=Saukh%2C+O">Olga Saukh</a>, 
<a href="/search/cs?searchtype=author&query=Ratner%2C+A">Alexander Ratner</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Farhadi%2C+A">Ali Farhadi</a>, 
<a href="/search/cs?searchtype=author&query=Beaumont%2C+R">Romain Beaumont</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sewoong Oh</a>, 
<a href="/search/cs?searchtype=author&query=Dimakis%2C+A">Alex Dimakis</a>, 
<a href="/search/cs?searchtype=author&query=Jitsev%2C+J">Jenia Jitsev</a>, 
<a href="/search/cs?searchtype=author&query=Carmon%2C+Y">Yair Carmon</a>, 
<a href="/search/cs?searchtype=author&query=Shankar%2C+V">Vaishaal Shankar</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+L">Ludwig Schmidt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14399" title="Abstract">arXiv:2304.14399</a> (replaced) [<a href="/pdf/2304.14399" title="Download PDF">pdf</a>, <a href="/format/2304.14399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> We&#x27;re Afraid Language Models Aren&#x27;t Modeling Ambiguity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Alisa Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhaofeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Michael%2C+J">Julian Michael</a>, 
<a href="/search/cs?searchtype=author&query=Suhr%2C+A">Alane Suhr</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+P">Peter West</a>, 
<a href="/search/cs?searchtype=author&query=Koller%2C+A">Alexander Koller</a>, 
<a href="/search/cs?searchtype=author&query=Swayamdipta%2C+S">Swabha Swayamdipta</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14975" title="Abstract">arXiv:2304.14975</a> (replaced) [<a href="/pdf/2304.14975" title="Download PDF">pdf</a>, <a href="/format/2304.14975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concept-centric Software Development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilczynski%2C+P">Peter Wilczynski</a>, 
<a href="/search/cs?searchtype=author&query=Gregoire-Wright%2C+T">Taylor Gregoire-Wright</a>, 
<a href="/search/cs?searchtype=author&query=Jackson%2C+D">Daniel Jackson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00467" title="Abstract">arXiv:2305.00467</a> (replaced) [<a href="/pdf/2305.00467" title="Download PDF">pdf</a>, <a href="/ps/2305.00467" title="Download PostScript">ps</a>, <a href="/format/2305.00467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The iteration time and the general position number in graph convexities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Araujo%2C+J">Julio Araujo</a>, 
<a href="/search/cs?searchtype=author&query=Dourado%2C+M+C">Mitre C. Dourado</a>, 
<a href="/search/cs?searchtype=author&query=Protti%2C+F">F&#xe1;bio Protti</a>, 
<a href="/search/cs?searchtype=author&query=Sampaio%2C+R">Rudini Sampaio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02321" title="Abstract">arXiv:2305.02321</a> (replaced) [<a href="/pdf/2305.02321" title="Download PDF">pdf</a>, <a href="/format/2305.02321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entity-Based Evaluation of Political Bias in Automatic Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Karen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chenhao Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures, Findings of EMNLP 2023, code at <a href="https://github.com/ChicagoHAI/entity-based-political-bias">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02967" title="Abstract">arXiv:2305.02967</a> (replaced) [<a href="/pdf/2305.02967" title="Download PDF">pdf</a>, <a href="/ps/2305.02967" title="Download PostScript">ps</a>, <a href="/format/2305.02967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Urgency Annotations for Alternating Choices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keskin%2C+E">Eren Keskin</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+R">Roland Meyer</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Wall%2C+S">S&#xf6;ren van der Wall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03626" title="Abstract">arXiv:2305.03626</a> (replaced) [<a href="/pdf/2305.03626" title="Download PDF">pdf</a>, <a href="/format/2305.03626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verifiable Learning for Robust Tree Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Calzavara%2C+S">Stefano Calzavara</a>, 
<a href="/search/cs?searchtype=author&query=Cazzaro%2C+L">Lorenzo Cazzaro</a>, 
<a href="/search/cs?searchtype=author&query=Pibiri%2C+G+E">Giulio Ermanno Pibiri</a>, 
<a href="/search/cs?searchtype=author&query=Prezza%2C+N">Nicola Prezza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures; full version of the revised paper accepted at ACM CCS 2023 with corrected proofs of Lemma A.6 and Lemma A.7
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Logic in Computer Science (cs.LO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03668" title="Abstract">arXiv:2305.03668</a> (replaced) [<a href="/pdf/2305.03668" title="Download PDF">pdf</a>, <a href="/format/2305.03668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Suite of Generative Tasks for Multi-Level Multimodal Webpage  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burns%2C+A">Andrea Burns</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+K">Krishna Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Ainslie%2C+J">Joshua Ainslie</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+G">Geoff Brown</a>, 
<a href="/search/cs?searchtype=author&query=Plummer%2C+B+A">Bryan A. Plummer</a>, 
<a href="/search/cs?searchtype=author&query=Saenko%2C+K">Kate Saenko</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jianmo Ni</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Mandy Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in EMNLP 2023, revision contains camera ready edits. Data can be downloaded at <a href="https://github.com/google-research-datasets/wit/blob/main/wikiweb2m.md">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03686" title="Abstract">arXiv:2305.03686</a> (replaced) [<a href="/pdf/2305.03686" title="Download PDF">pdf</a>, <a href="/format/2305.03686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Preimage Under-Approximation for Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiyue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kwiatkowska%2C+M">Marta Kwiatkowska</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04422" title="Abstract">arXiv:2305.04422</a> (replaced) [<a href="/pdf/2305.04422" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivariate Analysis on Performance Gaps of Artificial Intelligence  Models in Screening Mammography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+L">Linglin Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Brown-Mulry%2C+B">Beatrice Brown-Mulry</a>, 
<a href="/search/eess?searchtype=author&query=Nalla%2C+V">Vineela Nalla</a>, 
<a href="/search/eess?searchtype=author&query=Hwang%2C+I">InChan Hwang</a>, 
<a href="/search/eess?searchtype=author&query=Gichoya%2C+J+W">Judy Wawira Gichoya</a>, 
<a href="/search/eess?searchtype=author&query=Gastounioti%2C+A">Aimilia Gastounioti</a>, 
<a href="/search/eess?searchtype=author&query=Banerjee%2C+I">Imon Banerjee</a>, 
<a href="/search/eess?searchtype=author&query=Seyyed-Kalantari%2C+L">Laleh Seyyed-Kalantari</a>, 
<a href="/search/eess?searchtype=author&query=Woo%2C+M">MinJae Woo</a>, 
<a href="/search/eess?searchtype=author&query=Trivedi%2C+H">Hari Trivedi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 6 tables, 7 figures, 2 supplemental tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04629" title="Abstract">arXiv:2305.04629</a> (replaced) [<a href="/pdf/2305.04629" title="Download PDF">pdf</a>, <a href="/ps/2305.04629" title="Download PostScript">ps</a>, <a href="/format/2305.04629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uplink Multiplexing of eMBB/URLLC Services Assisted by Reconfigurable  Intelligent Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Souza%2C+J+H+I">Jo&#xe3;o Henrique Inacio de Souza</a>, 
<a href="/search/cs?searchtype=author&query=Croisfelt%2C+V">Victor Croisfelt</a>, 
<a href="/search/cs?searchtype=author&query=Kotaba%2C+R">Rados&#x142;aw Kotaba</a>, 
<a href="/search/cs?searchtype=author&query=Abr%C3%A3o%2C+T">Taufik Abr&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Popovski%2C+P">Petar Popovski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04812" title="Abstract">arXiv:2305.04812</a> (replaced) [<a href="/pdf/2305.04812" title="Download PDF">pdf</a>, <a href="/format/2305.04812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Influence of External Information on Large Language Models Mirrors  Social Cognitive Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bian%2C+N">Ning Bian</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peilin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yaojie Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chunkang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Ben He</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xianpei Han</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Le Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06677" title="Abstract">arXiv:2305.06677</a> (replaced) [<a href="/pdf/2305.06677" title="Download PDF">pdf</a>, <a href="/format/2305.06677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Renduchintala%2C+H+S+V+N+S+K">H S V N S Kowndinya Renduchintala</a>, 
<a href="/search/cs?searchtype=author&query=Killamsetty%2C+K">Krishnateja Killamsetty</a>, 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+S">Sumit Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+M">Milan Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+R">Rishabh Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+B">Balaji Krishnamurthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06888" title="Abstract">arXiv:2305.06888</a> (replaced) [<a href="/pdf/2305.06888" title="Download PDF">pdf</a>, <a href="/ps/2305.06888" title="Download PostScript">ps</a>, <a href="/format/2305.06888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Socioeconomic disparities in mobility behavior during the COVID-19  pandemic in developing countries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lucchini%2C+L">Lorenzo Lucchini</a>, 
<a href="/search/physics?searchtype=author&query=Langle-Chimal%2C+O">Ollin Langle-Chimal</a>, 
<a href="/search/physics?searchtype=author&query=Candeago%2C+L">Lorenzo Candeago</a>, 
<a href="/search/physics?searchtype=author&query=Melito%2C+L">Lucio Melito</a>, 
<a href="/search/physics?searchtype=author&query=Chunet%2C+A">Alex Chunet</a>, 
<a href="/search/physics?searchtype=author&query=Montfort%2C+A">Aleister Montfort</a>, 
<a href="/search/physics?searchtype=author&query=Lepri%2C+B">Bruno Lepri</a>, 
<a href="/search/physics?searchtype=author&query=Lozano-Gracia%2C+N">Nancy Lozano-Gracia</a>, 
<a href="/search/physics?searchtype=author&query=Fraiberger%2C+S+P">Samuel P. Fraiberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Computers and Society (cs.CY); General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07875" title="Abstract">arXiv:2305.07875</a> (replaced) [<a href="/pdf/2305.07875" title="Download PDF">pdf</a>, <a href="/ps/2305.07875" title="Download PostScript">ps</a>, <a href="/format/2305.07875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On $\ell_2$-performance of weakly-hard real-time control systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Seidel%2C+M">Marc Seidel</a>, 
<a href="/search/eess?searchtype=author&query=Lang%2C+S">Simon Lang</a>, 
<a href="/search/eess?searchtype=author&query=Allg%C3%B6wer%2C+F">Frank Allg&#xf6;wer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ECC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08503" title="Abstract">arXiv:2305.08503</a> (replaced) [<a href="/pdf/2305.08503" title="Download PDF">pdf</a>, <a href="/format/2305.08503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chenhui Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Liying Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+X">Xuan-Phi Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08828" title="Abstract">arXiv:2305.08828</a> (replaced) [<a href="/pdf/2305.08828" title="Download PDF">pdf</a>, <a href="/format/2305.08828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for  Languages in India
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Urlana%2C+A">Ashok Urlana</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pinzhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S+B">Shay B. Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+M">Manish Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Haddow%2C+B">Barry Haddow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09601" title="Abstract">arXiv:2305.09601</a> (replaced) [<a href="/pdf/2305.09601" title="Download PDF">pdf</a>, <a href="/format/2305.09601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operationalizing content moderation &quot;accuracy&#x27;&#x27; in the Digital Services  Act
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J+T">Johnny Tian-Zheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zufall%2C+F">Frederike Zufall</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Robin Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09928" title="Abstract">arXiv:2305.09928</a> (replaced) [<a href="/pdf/2305.09928" title="Download PDF">pdf</a>, <a href="/format/2305.09928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud  Segmentation in the Geosciences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Afifi%2C+A+J">Ahmed J. Afifi</a>, 
<a href="/search/cs?searchtype=author&query=Thiele%2C+S+T">Samuel T. Thiele</a>, 
<a href="/search/cs?searchtype=author&query=Rizaldy%2C+A">Aldino Rizaldy</a>, 
<a href="/search/cs?searchtype=author&query=Lorenz%2C+S">Sandra Lorenz</a>, 
<a href="/search/cs?searchtype=author&query=Ghamisi%2C+P">Pedram Ghamisi</a>, 
<a href="/search/cs?searchtype=author&query=Tolosana-Delgado%2C+R">Raimon Tolosana-Delgado</a>, 
<a href="/search/cs?searchtype=author&query=Kirsch%2C+M">Moritz Kirsch</a>, 
<a href="/search/cs?searchtype=author&query=Gloaguen%2C+R">Richard Gloaguen</a>, 
<a href="/search/cs?searchtype=author&query=Heizmann%2C+M">Michael Heizmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10387" title="Abstract">arXiv:2305.10387</a> (replaced) [<a href="/pdf/2305.10387" title="Download PDF">pdf</a>, <a href="/format/2305.10387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elaborative Simplification as Implicit Questions Under Discussion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yating Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sheffield%2C+W">William Sheffield</a>, 
<a href="/search/cs?searchtype=author&query=Mahowald%2C+K">Kyle Mahowald</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+J">Junyi Jessy Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Equal contribution by Yating Wu and William Sheffield
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10613" title="Abstract">arXiv:2305.10613</a> (replaced) [<a href="/pdf/2305.10613" title="Download PDF">pdf</a>, <a href="/format/2305.10613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dong-Ho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ahrabian%2C+K">Kian Ahrabian</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Woojeong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Morstatter%2C+F">Fred Morstatter</a>, 
<a href="/search/cs?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference. 14 pages, 4 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11596" title="Abstract">arXiv:2305.11596</a> (replaced) [<a href="/pdf/2305.11596" title="Download PDF">pdf</a>, <a href="/format/2305.11596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Backdoor Poisoning Attacks through the Lens of Spurious  Correlation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuanli He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiongkai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+B">Benjamin Rubinstein</a>, 
<a href="/search/cs?searchtype=author&query=Cohn%2C+T">Trevor Cohn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to EMNLP2023 (main conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11707" title="Abstract">arXiv:2305.11707</a> (replaced) [<a href="/pdf/2305.11707" title="Download PDF">pdf</a>, <a href="/format/2305.11707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Comes Next? Evaluating Uncertainty in Neural Text Generators  Against Human Production Variability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giulianelli%2C+M">Mario Giulianelli</a>, 
<a href="/search/cs?searchtype=author&query=Baan%2C+J">Joris Baan</a>, 
<a href="/search/cs?searchtype=author&query=Aziz%2C+W">Wilker Aziz</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+R">Raquel Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready version for EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12027" title="Abstract">arXiv:2305.12027</a> (replaced) [<a href="/pdf/2305.12027" title="Download PDF">pdf</a>, <a href="/format/2305.12027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck  Typing and Polar Box Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atzeni%2C+M">Mattia Atzeni</a>, 
<a href="/search/cs?searchtype=author&query=Plekhanov%2C+M">Mikhail Plekhanov</a>, 
<a href="/search/cs?searchtype=author&query=Dreyer%2C+F+A">Fr&#xe9;d&#xe9;ric A. Dreyer</a>, 
<a href="/search/cs?searchtype=author&query=Kassner%2C+N">Nora Kassner</a>, 
<a href="/search/cs?searchtype=author&query=Merello%2C+S">Simone Merello</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+L">Louis Martin</a>, 
<a href="/search/cs?searchtype=author&query=Cancedda%2C+N">Nicola Cancedda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12074" title="Abstract">arXiv:2305.12074</a> (replaced) [<a href="/pdf/2305.12074" title="Download PDF">pdf</a>, <a href="/format/2305.12074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DisCo: Distilled Student Models Co-training for Semi-supervised Text  Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weifeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Q">Qianren Mao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+T">Ting Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Weiyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12138" title="Abstract">arXiv:2305.12138</a> (replaced) [<a href="/pdf/2305.12138" title="Download PDF">pdf</a>, <a href="/format/2305.12138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT: Understanding Code Syntax and Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shangqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ye Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liming Nie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12947" title="Abstract">arXiv:2305.12947</a> (replaced) [<a href="/pdf/2305.12947" title="Download PDF">pdf</a>, <a href="/format/2305.12947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT to Replace Crowdsourcing of Paraphrases for Intent  Classification: Higher Diversity and Comparable Model Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cegin%2C+J">Jan Cegin</a>, 
<a href="/search/cs?searchtype=author&query=Simko%2C+J">Jakub Simko</a>, 
<a href="/search/cs?searchtype=author&query=Brusilovsky%2C+P">Peter Brusilovsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long paper accepted to EMNLP 2023 conference main track, 17 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13062" title="Abstract">arXiv:2305.13062</a> (replaced) [<a href="/pdf/2305.13062" title="Download PDF">pdf</a>, <a href="/format/2305.13062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT4Table: Can Large Language Models Understand Structured Table Data? A  Benchmark and Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yuan Sui</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mengyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shi Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted as a full paper at WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13091" title="Abstract">arXiv:2305.13091</a> (replaced) [<a href="/pdf/2305.13091" title="Download PDF">pdf</a>, <a href="/format/2305.13091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are Not Yet Human-Level Evaluators for Abstractive  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chenhui Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Liying Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+X">Xuan-Phi Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13186" title="Abstract">arXiv:2305.13186</a> (replaced) [<a href="/pdf/2305.13186" title="Download PDF">pdf</a>, <a href="/format/2305.13186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim  Verification on Scientific Tables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xinyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+M">Min-Yen Kan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (main conference, long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13192" title="Abstract">arXiv:2305.13192</a> (replaced) [<a href="/pdf/2305.13192" title="Download PDF">pdf</a>, <a href="/format/2305.13192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimCSE++: Improving Contrastive Learning for Sentence Embeddings from  Two Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiahao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wei Shao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lihui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lemao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13303" title="Abstract">arXiv:2305.13303</a> (replaced) [<a href="/pdf/2305.13303" title="Download PDF">pdf</a>, <a href="/format/2305.13303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Unsupervised Recognition of Token-level Semantic Differences in  Related Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vamvas%2C+J">Jannis Vamvas</a>, 
<a href="/search/cs?searchtype=author&query=Sennrich%2C+R">Rico Sennrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13386" title="Abstract">arXiv:2305.13386</a> (replaced) [<a href="/pdf/2305.13386" title="Download PDF">pdf</a>, <a href="/format/2305.13386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs facilitate interpretation of pre-trained language models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mousi%2C+B">Basel Mousi</a>, 
<a href="/search/cs?searchtype=author&query=Durrani%2C+N">Nadir Durrani</a>, 
<a href="/search/cs?searchtype=author&query=Dalvi%2C+F">Fahim Dalvi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13395" title="Abstract">arXiv:2305.13395</a> (replaced) [<a href="/pdf/2305.13395" title="Download PDF">pdf</a>, <a href="/format/2305.13395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioDEX: Large-Scale Biomedical Adverse Drug Event Extraction for  Real-World Pharmacovigilance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Oosterlinck%2C+K">Karel D&#x27;Oosterlinck</a>, 
<a href="/search/cs?searchtype=author&query=Remy%2C+F">Fran&#xe7;ois Remy</a>, 
<a href="/search/cs?searchtype=author&query=Deleu%2C+J">Johannes Deleu</a>, 
<a href="/search/cs?searchtype=author&query=Demeester%2C+T">Thomas Demeester</a>, 
<a href="/search/cs?searchtype=author&query=Develder%2C+C">Chris Develder</a>, 
<a href="/search/cs?searchtype=author&query=Zaporojets%2C+K">Klim Zaporojets</a>, 
<a href="/search/cs?searchtype=author&query=Ghodsi%2C+A">Aneiss Ghodsi</a>, 
<a href="/search/cs?searchtype=author&query=Ellershaw%2C+S">Simon Ellershaw</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+J">Jack Collins</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages. EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13602" title="Abstract">arXiv:2305.13602</a> (replaced) [<a href="/pdf/2305.13602" title="Download PDF">pdf</a>, <a href="/format/2305.13602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReSee: Responding through Seeing Fine-grained Visual Knowledge in  Open-domain Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+H">Haoqin Tu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yitong Li</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhongliang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, accepted to EMNLP 2023 (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13660" title="Abstract">arXiv:2305.13660</a> (replaced) [<a href="/pdf/2305.13660" title="Download PDF">pdf</a>, <a href="/format/2305.13660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Maximillian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13723" title="Abstract">arXiv:2305.13723</a> (replaced) [<a href="/pdf/2305.13723" title="Download PDF">pdf</a>, <a href="/format/2305.13723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PIEClass: Weakly-Supervised Text Classification with Prompting and  Noise-Robust Iterative Ensemble Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Minhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13785" title="Abstract">arXiv:2305.13785</a> (replaced) [<a href="/pdf/2305.13785" title="Download PDF">pdf</a>, <a href="/format/2305.13785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Danqing Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiahui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14221" title="Abstract">arXiv:2305.14221</a> (replaced) [<a href="/pdf/2305.14221" title="Download PDF">pdf</a>, <a href="/format/2305.14221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Question Answering as Programming for Solving Time-Sensitive Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian-Guang Lou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14283" title="Abstract">arXiv:2305.14283</a> (replaced) [<a href="/pdf/2305.14283" title="Download PDF">pdf</a>, <a href="/format/2305.14283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query Rewriting for Retrieval-Augmented Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinbei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengcheng He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14312" title="Abstract">arXiv:2305.14312</a> (replaced) [<a href="/pdf/2305.14312" title="Download PDF">pdf</a>, <a href="/format/2305.14312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-guided 3D Human Generation from 2D Collections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tsu-Jui Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wenhan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Yixin Nie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=O%C4%9Fuz%2C+B">Barlas O&#x11f;uz</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP'23 (Findings) ; Project website: <a href="https://text-3dh.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14600" title="Abstract">arXiv:2305.14600</a> (replaced) [<a href="/pdf/2305.14600" title="Download PDF">pdf</a>, <a href="/format/2305.14600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Semantic Role Labeling from Compatible Label Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Kazeminejad%2C+G">Ghazaleh Kazeminejad</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+S+W">Susan W. Brown</a>, 
<a href="/search/cs?searchtype=author&query=Palmer%2C+M">Martha Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Srikumar%2C+V">Vivek Srikumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14628" title="Abstract">arXiv:2305.14628</a> (replaced) [<a href="/pdf/2305.14628" title="Download PDF">pdf</a>, <a href="/format/2305.14628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Getting MoRE out of Mixture of Language Model Reasoning Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+C">Chenglei Si</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Boyd-Graber%2C+J">Jordan Boyd-Graber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14642" title="Abstract">arXiv:2305.14642</a> (replaced) [<a href="/pdf/2305.14642" title="Download PDF">pdf</a>, <a href="/format/2305.14642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Newton-Cotes Graph Neural Networks: On the Time Evolution of Dynamic  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lingbing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zequn Sun</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yixuan Lai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14869" title="Abstract">arXiv:2305.14869</a> (replaced) [<a href="/pdf/2305.14869" title="Download PDF">pdf</a>, <a href="/format/2305.14869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Tianqing Fang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenxuan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Baixuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15054" title="Abstract">arXiv:2305.15054</a> (replaced) [<a href="/pdf/2305.15054" title="Download PDF">pdf</a>, <a href="/format/2305.15054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Mechanistic Interpretation of Arithmetic Reasoning in Language Models  using Causal Mediation Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stolfo%2C+A">Alessandro Stolfo</a>, 
<a href="/search/cs?searchtype=author&query=Belinkov%2C+Y">Yonatan Belinkov</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. 18 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15255" title="Abstract">arXiv:2305.15255</a> (replaced) [<a href="/pdf/2305.15255" title="Download PDF">pdf</a>, <a href="/format/2305.15255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spoken Question Answering and Speech Continuation Using  Spectrogram-Powered LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nachmani%2C+E">Eliya Nachmani</a>, 
<a href="/search/cs?searchtype=author&query=Levkovitch%2C+A">Alon Levkovitch</a>, 
<a href="/search/cs?searchtype=author&query=Hirsch%2C+R">Roy Hirsch</a>, 
<a href="/search/cs?searchtype=author&query=Salazar%2C+J">Julian Salazar</a>, 
<a href="/search/cs?searchtype=author&query=Asawaroengchai%2C+C">Chulayuth Asawaroengchai</a>, 
<a href="/search/cs?searchtype=author&query=Mariooryad%2C+S">Soroosh Mariooryad</a>, 
<a href="/search/cs?searchtype=author&query=Rivlin%2C+E">Ehud Rivlin</a>, 
<a href="/search/cs?searchtype=author&query=Skerry-Ryan%2C+R">RJ Skerry-Ryan</a>, 
<a href="/search/cs?searchtype=author&query=Ramanovich%2C+M+T">Michelle Tadmor Ramanovich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15269" title="Abstract">arXiv:2305.15269</a> (replaced) [<a href="/pdf/2305.15269" title="Download PDF">pdf</a>, <a href="/format/2305.15269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing the General Deductive Reasoning Capacity of Large Language  Models Using OOD Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saparov%2C+A">Abulhair Saparov</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+R+Y">Richard Yuanzhe Pang</a>, 
<a href="/search/cs?searchtype=author&query=Padmakumar%2C+V">Vishakh Padmakumar</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+N">Nitish Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Kazemi%2C+S+M">Seyed Mehran Kazemi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N">Najoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">He He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15425" title="Abstract">arXiv:2305.15425</a> (replaced) [<a href="/pdf/2305.15425" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Model Tokenizers Introduce Unfairness Between Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrov%2C+A">Aleksandar Petrov</a>, 
<a href="/search/cs?searchtype=author&query=La+Malfa%2C+E">Emanuele La Malfa</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Bibi%2C+A">Adel Bibi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023, Project webpage: <a href="https://aleksandarpetrov.github.io/tokenization-fairness">this https URL</a>, Code: <a href="https://github.com/AleksandarPetrov/tokenization-fairness">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15832" title="Abstract">arXiv:2305.15832</a> (replaced) [<a href="/pdf/2305.15832" title="Download PDF">pdf</a>, <a href="/format/2305.15832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All Points Matter: Entropy-Regularized Distribution Alignment for  Weakly-supervised 3D Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Liyao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shanshan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16297" title="Abstract">arXiv:2305.16297</a> (replaced) [<a href="/pdf/2305.16297" title="Download PDF">pdf</a>, <a href="/format/2305.16297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbiased Compression Saves Communication in Distributed Optimization:  When and How Much?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yutong He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinmeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+K">Kun Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16337" title="Abstract">arXiv:2305.16337</a> (replaced) [<a href="/pdf/2305.16337" title="Download PDF">pdf</a>, <a href="/format/2305.16337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Handling Realistic Label Noise in BERT Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agro%2C+M+T">Maha Tufail Agro</a>, 
<a href="/search/cs?searchtype=author&query=Aldarmaki%2C+H">Hanan Aldarmaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16772" title="Abstract">arXiv:2305.16772</a> (replaced) [<a href="/pdf/2305.16772" title="Download PDF">pdf</a>, <a href="/format/2305.16772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Scheduling for Time-Sensitive Networking: A Systematic Review  and Experimental Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+C">Chuanyu Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuanbin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Nixon%2C+M">Mark Nixon</a>, 
<a href="/search/cs?searchtype=author&query=Loveless%2C+A">Andrew Loveless</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Song Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17953" title="Abstract">arXiv:2305.17953</a> (replaced) [<a href="/pdf/2305.17953" title="Download PDF">pdf</a>, <a href="/format/2305.17953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase-coded Radar Waveform Design with Quantum Annealing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Presles%2C+T">Timothe Presles</a>, 
<a href="/search/cs?searchtype=author&query=Enderli%2C+C">Cyrille Enderli</a>, 
<a href="/search/cs?searchtype=author&query=Burel%2C+G">Gilles Burel</a>, 
<a href="/search/cs?searchtype=author&query=Baghious%2C+E+H">El Houssain Baghious</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures, 1 table, to be published in IET Radar, Sonar and Navigation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01989" title="Abstract">arXiv:2306.01989</a> (replaced) [<a href="/pdf/2306.01989" title="Download PDF">pdf</a>, <a href="/format/2306.01989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Vectorization Implementation of CRYSTALS-Dilithium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jieyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haoliang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhenyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunlei Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03341" title="Abstract">arXiv:2306.03341</a> (replaced) [<a href="/pdf/2306.03341" title="Download PDF">pdf</a>, <a href="/format/2306.03341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inference-Time Intervention: Eliciting Truthful Answers from a Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kenneth Li</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+O">Oam Patel</a>, 
<a href="/search/cs?searchtype=author&query=Vi%C3%A9gas%2C+F">Fernanda Vi&#xe9;gas</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+H">Hanspeter Pfister</a>, 
<a href="/search/cs?searchtype=author&query=Wattenberg%2C+M">Martin Wattenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight; code: <a href="https://github.com/likenneth/honest_llama">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03717" title="Abstract">arXiv:2306.03717</a> (replaced) [<a href="/pdf/2306.03717" title="Download PDF">pdf</a>, <a href="/format/2306.03717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Description Logics with Abstraction and Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lutz%2C+C">Carsten Lutz</a>, 
<a href="/search/cs?searchtype=author&query=Schulze%2C+L">Lukas Schulze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, Long version of paper accepted at KR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04079" title="Abstract">arXiv:2306.04079</a> (replaced) [<a href="/pdf/2306.04079" title="Download PDF">pdf</a>, <a href="/format/2306.04079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RGBlimp: Robotic Gliding Blimp -- Design, Modeling, Development, and  Aerodynamics Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+Z">Zeyu Sha</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yongjian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Feitian Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Robotics and Automation Letters, vol. 8, no. 11, pp.
  7273-7280, Nov. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05031" title="Abstract">arXiv:2306.05031</a> (replaced) [<a href="/pdf/2306.05031" title="Download PDF">pdf</a>, <a href="/format/2306.05031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizable Lightweight Proxy for Robust NAS against Diverse  Perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ha%2C+H">Hyeonjeong Ha</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minseon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, Code is available at <a href="https://github.com/HyeonjeongHa/CRoZe">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07263" title="Abstract">arXiv:2306.07263</a> (replaced) [<a href="/pdf/2306.07263" title="Download PDF">pdf</a>, <a href="/format/2306.07263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enlarging Stability Region of Urban Networks with Imminent Supply  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+D">Dianchao Lin</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Li Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07818" title="Abstract">arXiv:2306.07818</a> (replaced) [<a href="/pdf/2306.07818" title="Download PDF">pdf</a>, <a href="/format/2306.07818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+K">Kihyuk Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08205" title="Abstract">arXiv:2306.08205</a> (replaced) [<a href="/pdf/2306.08205" title="Download PDF">pdf</a>, <a href="/format/2306.08205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agile Catching with Whole-Body MPC and Blackbox Policy Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abeyruwan%2C+S">Saminda Abeyruwan</a>, 
<a href="/search/cs?searchtype=author&query=Bewley%2C+A">Alex Bewley</a>, 
<a href="/search/cs?searchtype=author&query=Boffi%2C+N+M">Nicholas M. Boffi</a>, 
<a href="/search/cs?searchtype=author&query=Choromanski%2C+K">Krzysztof Choromanski</a>, 
<a href="/search/cs?searchtype=author&query=D%27Ambrosio%2C+D">David D&#x27;Ambrosio</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+D">Deepali Jain</a>, 
<a href="/search/cs?searchtype=author&query=Sanketi%2C+P">Pannag Sanketi</a>, 
<a href="/search/cs?searchtype=author&query=Shankar%2C+A">Anish Shankar</a>, 
<a href="/search/cs?searchtype=author&query=Sindhwani%2C+V">Vikas Sindhwani</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sumeet Singh</a>, 
<a href="/search/cs?searchtype=author&query=Slotine%2C+J">Jean-Jacques Slotine</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+S">Stephen Tu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> L4DC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08877" title="Abstract">arXiv:2306.08877</a> (replaced) [<a href="/pdf/2306.08877" title="Download PDF">pdf</a>, <a href="/format/2306.08877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linguistic Binding in Diffusion Models: Enhancing Attribute  Correspondence through Attention Map Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rassin%2C+R">Royi Rassin</a>, 
<a href="/search/cs?searchtype=author&query=Hirsch%2C+E">Eran Hirsch</a>, 
<a href="/search/cs?searchtype=author&query=Glickman%2C+D">Daniel Glickman</a>, 
<a href="/search/cs?searchtype=author&query=Ravfogel%2C+S">Shauli Ravfogel</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NEURIPS 2023. Our code is publicly available at <a href="https://github.com/RoyiRa/Syntax-Guided-Generation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09927" title="Abstract">arXiv:2306.09927</a> (replaced) [<a href="/pdf/2306.09927" title="Download PDF">pdf</a>, <a href="/format/2306.09927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trained Transformers Learn Linear Models In-Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+R">Ruiqi Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Frei%2C+S">Spencer Frei</a>, 
<a href="/search/stat?searchtype=author&query=Bartlett%2C+P+L">Peter L. Bartlett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, revised definition 3.2 and corollary 4.3
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12802" title="Abstract">arXiv:2306.12802</a> (replaced) [<a href="/pdf/2306.12802" title="Download PDF">pdf</a>, <a href="/format/2306.12802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Otter-Knowledge: benchmarks of multimodal knowledge graph representation  learning from different sources for drug discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lam%2C+H+T">Hoang Thanh Lam</a>, 
<a href="/search/cs?searchtype=author&query=Sbodio%2C+M+L">Marco Luca Sbodio</a>, 
<a href="/search/cs?searchtype=author&query=Galindo%2C+M+M">Marcos Mart&#xed;nez Galindo</a>, 
<a href="/search/cs?searchtype=author&query=Zayats%2C+M">Mykhaylo Zayats</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-D%C3%ADaz%2C+R">Ra&#xfa;l Fern&#xe1;ndez-D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=Valls%2C+V">V&#xed;ctor Valls</a>, 
<a href="/search/cs?searchtype=author&query=Picco%2C+G">Gabriele Picco</a>, 
<a href="/search/cs?searchtype=author&query=Ramis%2C+C+B">Cesar Berrospi Ramis</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez%2C+V">Vanessa L&#xf3;pez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12873" title="Abstract">arXiv:2306.12873</a> (replaced) [<a href="/pdf/2306.12873" title="Download PDF">pdf</a>, <a href="/format/2306.12873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuXi: A cascade machine learning forecasting system for 15-day global  weather forecast
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/physics?searchtype=author&query=Zhong%2C+X">Xiaohui Zhong</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+F">Feng Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Cheng%2C+Y">Yuan Cheng</a>, 
<a href="/search/physics?searchtype=author&query=Xu%2C+Y">Yinghui Xu</a>, 
<a href="/search/physics?searchtype=author&query=Qi%2C+Y">Yuan Qi</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+H">Hao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12991" title="Abstract">arXiv:2306.12991</a> (replaced) [<a href="/pdf/2306.12991" title="Download PDF">pdf</a>, <a href="/format/2306.12991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speech Emotion Diarization: Which Emotion Appears When?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yingzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>, 
<a href="/search/cs?searchtype=author&query=Yacoubi%2C+A">Alya Yacoubi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14852" title="Abstract">arXiv:2306.14852</a> (replaced) [<a href="/pdf/2306.14852" title="Download PDF">pdf</a>, <a href="/format/2306.14852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoarsenConf: Equivariant Coarsening with Aggregated Attention for  Molecular Conformer Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reidenbach%2C+D">Danny Reidenbach</a>, 
<a href="/search/cs?searchtype=author&query=Krishnapriyan%2C+A+S">Aditi S. Krishnapriyan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 main pages (25 total), 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15228" title="Abstract">arXiv:2306.15228</a> (replaced) [<a href="/pdf/2306.15228" title="Download PDF">pdf</a>, <a href="/format/2306.15228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IIFL: Implicit Interactive Fleet Learning from Heterogeneous Human  Supervisors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Datta%2C+G">Gaurav Datta</a>, 
<a href="/search/cs?searchtype=author&query=Hoque%2C+R">Ryan Hoque</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+A">Anrui Gu</a>, 
<a href="/search/cs?searchtype=author&query=Solowjow%2C+E">Eugen Solowjow</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+K">Ken Goldberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15864" title="Abstract">arXiv:2306.15864</a> (replaced) [<a href="/pdf/2306.15864" title="Download PDF">pdf</a>, <a href="/format/2306.15864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Went Wrong? Closing the Sim-to-Real Gap via Differentiable Causal  Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Peide Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xilun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Ziang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengdi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenhao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Francis%2C+J">Jonathan Francis</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bingqing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17817" title="Abstract">arXiv:2306.17817</a> (replaced) [<a href="/pdf/2306.17817" title="Download PDF">pdf</a>, <a href="/format/2306.17817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gervet%2C+T">Theophile Gervet</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Z">Zhou Xian</a>, 
<a href="/search/cs?searchtype=author&query=Gkanatsios%2C+N">Nikolaos Gkanatsios</a>, 
<a href="/search/cs?searchtype=author&query=Fragkiadaki%2C+K">Katerina Fragkiadaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05738" title="Abstract">arXiv:2307.05738</a> (replaced) [<a href="/pdf/2307.05738" title="Download PDF">pdf</a>, <a href="/format/2307.05738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient CHAD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smeding%2C+T">Tom Smeding</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%A1k%C3%A1r%2C+M">Matthijs V&#xe1;k&#xe1;r</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06928" title="Abstract">arXiv:2307.06928</a> (replaced) [<a href="/pdf/2307.06928" title="Download PDF">pdf</a>, <a href="/ps/2307.06928" title="Download PostScript">ps</a>, <a href="/format/2307.06928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ill-Typed Programs Don&#x27;t Evaluate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramsay%2C+S">Steven Ramsay</a>, 
<a href="/search/cs?searchtype=author&query=Walpole%2C+C">Charlie Walpole</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Incorporating anonymous reviewer suggestions from POPL'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07847" title="Abstract">arXiv:2307.07847</a> (replaced) [<a href="/pdf/2307.07847" title="Download PDF">pdf</a>, <a href="/format/2307.07847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Real-time Neural Recovery for Cloud Gaming on mobile devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaoyuan He</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuozhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+D">Diyuan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Lili Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuqing Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08013" title="Abstract">arXiv:2307.08013</a> (replaced) [<a href="/pdf/2307.08013" title="Download PDF">pdf</a>, <a href="/format/2307.08013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Implicit Models: Sparsity Trade-offs Capability in  Weight-tied Model for Vision Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Haobo Song</a>, 
<a href="/search/cs?searchtype=author&query=Majumder%2C+S">Soumajit Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08382" title="Abstract">arXiv:2307.08382</a> (replaced) [<a href="/pdf/2307.08382" title="Download PDF">pdf</a>, <a href="/format/2307.08382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Battery Lifetime Under Varying Usage Conditions from Early  Aging Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tingkai Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zihao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Thelen%2C+A">Adam Thelen</a>, 
<a href="/search/cs?searchtype=author&query=Howey%2C+D">David Howey</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chao Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10069" title="Abstract">arXiv:2307.10069</a> (replaced) [<a href="/pdf/2307.10069" title="Download PDF">pdf</a>, <a href="/format/2307.10069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust constrained nonlinear Model Predictive Control with Gated  Recurrent Unit model -- Extended version
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Schimperna%2C+I">Irene Schimperna</a>, 
<a href="/search/eess?searchtype=author&query=Magni%2C+L">Lalo Magni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to Elsevier for possible pubblication. Copyright may be tranferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11224" title="Abstract">arXiv:2307.11224</a> (replaced) [<a href="/pdf/2307.11224" title="Download PDF">pdf</a>, <a href="/format/2307.11224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jina Embeddings: A Novel Set of High-Performance Sentence Embedding  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%BCnther%2C+M">Michael G&#xfc;nther</a>, 
<a href="/search/cs?searchtype=author&query=Milliken%2C+L">Louis Milliken</a>, 
<a href="/search/cs?searchtype=author&query=Geuter%2C+J">Jonathan Geuter</a>, 
<a href="/search/cs?searchtype=author&query=Mastrapas%2C+G">Georgios Mastrapas</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Han Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11760" title="Abstract">arXiv:2307.11760</a> (replaced) [<a href="/pdf/2307.11760" title="Download PDF">pdf</a>, <a href="/format/2307.11760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Understand and Can be Enhanced by Emotional  Stimuli
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kaijie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+W">Wenxin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+J">Jianxun Lian</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report; short version (v1) was accepted by LLM@IJCAI'23; 32 pages; more work: <a href="https://llm-enhance.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14502" title="Abstract">arXiv:2307.14502</a> (replaced) [<a href="/pdf/2307.14502" title="Download PDF">pdf</a>, <a href="/ps/2307.14502" title="Download PostScript">ps</a>, <a href="/format/2307.14502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Effect of Spoken Language on Speech Enhancement using  Self-Supervised Speech Representation Loss Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Close%2C+G">George Close</a>, 
<a href="/search/eess?searchtype=author&query=Hain%2C+T">Thomas Hain</a>, 
<a href="/search/eess?searchtype=author&query=Goetze%2C+S">Stefan Goetze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WASPAA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15172" title="Abstract">arXiv:2307.15172</a> (replaced) [<a href="/pdf/2307.15172" title="Download PDF">pdf</a>, <a href="/format/2307.15172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oculomotor trajectory mapping on body as an effective intervention to  enhance attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Songlin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16548" title="Abstract">arXiv:2307.16548</a> (replaced) [<a href="/pdf/2307.16548" title="Download PDF">pdf</a>, <a href="/format/2307.16548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Specification of MiniDemographicABM.jl: A simplified agent-based  demographic model of the UK
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elsheikh%2C+A">Atiyah Elsheikh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> reflects MiniDemographicABM.jl Version 1.3, general formal terminology removed to another preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16605" title="Abstract">arXiv:2307.16605</a> (replaced) [<a href="/pdf/2307.16605" title="Download PDF">pdf</a>, <a href="/format/2307.16605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VPP: Efficient Conditional 3D Generation via Voxel-Point Progressive  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zekun Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Muzhou Yu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+R">Runpei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Kaisheng Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00245" title="Abstract">arXiv:2308.00245</a> (replaced) [<a href="/pdf/2308.00245" title="Download PDF">pdf</a>, <a href="/format/2308.00245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hitchhiker&#x27;s Guide to Program Analysis: A Journey with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yu Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yizhuo Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhiyun Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01525" title="Abstract">arXiv:2308.01525</a> (replaced) [<a href="/pdf/2308.01525" title="Download PDF">pdf</a>, <a href="/format/2308.01525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VisAlign: Dataset for Measuring the Degree of Alignment between AI and  Humans in Visual Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jiyoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+S">Seunghyun Won</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joonseok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ghassemi%2C+M">Marzyeh Ghassemi</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaeseok Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+O">O-Kil Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Edward Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at NeurIPS 2023 (Track on Datasets and Benchmarks)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03723" title="Abstract">arXiv:2308.03723</a> (replaced) [<a href="/pdf/2308.03723" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dimensionality Reduction for Improving Out-of-Distribution Detection in  Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woodland%2C+M">McKell Woodland</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+N">Nihil Patel</a>, 
<a href="/search/cs?searchtype=author&query=Taie%2C+M+A">Mais Al Taie</a>, 
<a href="/search/cs?searchtype=author&query=Yung%2C+J+P">Joshua P. Yung</a>, 
<a href="/search/cs?searchtype=author&query=Netherton%2C+T+J">Tucker J. Netherton</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+A+B">Ankit B. Patel</a>, 
<a href="/search/cs?searchtype=author&query=Brock%2C+K+K">Kristy K. Brock</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution is published in the proceedings of UNSURE 2023, Lecture Notes in Computer Science, vol 14291, and is available online at <a href="https://doi.org/10.1007/978-3-031-44336-7_15">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In: UNSURE 2023. LNCS, vol 14291. Springer, Cham (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04440" title="Abstract">arXiv:2308.04440</a> (replaced) [<a href="/pdf/2308.04440" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Big Bang, Low Bar -- Risk Assessment in the Public Arena
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Price%2C+H">Huw Price</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, no figures; extensive revisions to previous version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05896" title="Abstract">arXiv:2308.05896</a> (replaced) [<a href="/pdf/2308.05896" title="Download PDF">pdf</a>, <a href="/format/2308.05896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic-embedded Similarity Prototype for Scene Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chuanxin Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hanbo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yibin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06137" title="Abstract">arXiv:2308.06137</a> (replaced) [<a href="/pdf/2308.06137" title="Download PDF">pdf</a>, <a href="/format/2308.06137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Game-Theoretic Framework for Joint Forecasting and Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kedia%2C+K">Kushal Kedia</a>, 
<a href="/search/cs?searchtype=author&query=Dan%2C+P">Prithwish Dan</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sanjiban Choudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06838" title="Abstract">arXiv:2308.06838</a> (replaced) [<a href="/pdf/2308.06838" title="Download PDF">pdf</a>, <a href="/format/2308.06838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing Topological Graph Neural Networks with Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Truong%2C+Q">Quang Truong</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+P">Peter Chin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06920" title="Abstract">arXiv:2308.06920</a> (replaced) [<a href="/pdf/2308.06920" title="Download PDF">pdf</a>, <a href="/format/2308.06920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug  Development with Chatbots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Hongsong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Guo-Wei Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07476" title="Abstract">arXiv:2308.07476</a> (replaced) [<a href="/pdf/2308.07476" title="Download PDF">pdf</a>, <a href="/ps/2308.07476" title="Download PostScript">ps</a>, <a href="/format/2308.07476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dependent rounding with strong negative-correlation, and scheduling on  unrelated machines to minimize completion time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harris%2C+D+G">David G. Harris</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08963" title="Abstract">arXiv:2308.08963</a> (replaced) [<a href="/pdf/2308.08963" title="Download PDF">pdf</a>, <a href="/format/2308.08963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CONVERT:Contrastive Graph Clustering with Reliable Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xihong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Ke Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sihang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinwang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+E">En Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11763" title="Abstract">arXiv:2308.11763</a> (replaced) [<a href="/pdf/2308.11763" title="Download PDF">pdf</a>, <a href="/format/2308.11763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient set-theoretic algorithms for computing high-order Forman-Ricci  curvature on abstract simplicial complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=de+Souza%2C+D+B">Danillo Barros de Souza</a>, 
<a href="/search/physics?searchtype=author&query=da+Cunha%2C+J+T+S">Jonatas T. S. da Cunha</a>, 
<a href="/search/physics?searchtype=author&query=Santos%2C+F+A+N">Fernando A. N. Santos</a>, 
<a href="/search/physics?searchtype=author&query=Jost%2C+J">J&#xfc;rgen Jost</a>, 
<a href="/search/physics?searchtype=author&query=Rodrigues%2C+S">Serafim Rodrigues</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Analysis, Statistics and Probability (physics.data-an)</span>; Discrete Mathematics (cs.DM); Performance (cs.PF); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13081" title="Abstract">arXiv:2308.13081</a> (replaced) [<a href="/pdf/2308.13081" title="Download PDF">pdf</a>, <a href="/ps/2308.13081" title="Download PostScript">ps</a>, <a href="/format/2308.13081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal specification terminology for demographic agent-based models of  fixed-step single-clocked simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elsheikh%2C+A">Atiyah Elsheikh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Major improvements. arXiv admin note: substantial text overlap with <a href="/abs/2307.16548">arXiv:2307.16548</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00440" title="Abstract">arXiv:2309.00440</a> (replaced) [<a href="/pdf/2309.00440" title="Download PDF">pdf</a>, <a href="/format/2309.00440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Yet another Improvement of Plantard Arithmetic for Faster Kyber on  Low-end 32-bit IoT Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haosong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+W">Wangchen Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+R+C+C">Ray C.C. Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Koc%2C+C+K">Cetin Kaya Koc</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Donglong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01103" title="Abstract">arXiv:2309.01103</a> (replaced) [<a href="/pdf/2309.01103" title="Download PDF">pdf</a>, <a href="/format/2309.01103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Relational Contrastive Learning for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been published as a full paper at RecSys 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01439" title="Abstract">arXiv:2309.01439</a> (replaced) [<a href="/pdf/2309.01439" title="Download PDF">pdf</a>, <a href="/format/2309.01439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Separable Kernel Attention: Rethinking the Large Kernel Attention  Design in CNN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lau%2C+K+W">Kin Wai Lau</a>, 
<a href="/search/cs?searchtype=author&query=Po%2C+L">Lai-Man Po</a>, 
<a href="/search/cs?searchtype=author&query=Rehman%2C+Y+A+U">Yasar Abbas Ur Rehman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03374" title="Abstract">arXiv:2309.03374</a> (replaced) [<a href="/pdf/2309.03374" title="Download PDF">pdf</a>, <a href="/format/2309.03374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics Informed Neural Networks for Modeling of 3D Flow-Thermal  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+S">Saakaar Bhatnagar</a>, 
<a href="/search/cs?searchtype=author&query=Comerford%2C+A">Andrew Comerford</a>, 
<a href="/search/cs?searchtype=author&query=Banaeizadeh%2C+A">Araz Banaeizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05657" title="Abstract">arXiv:2309.05657</a> (replaced) [<a href="/pdf/2309.05657" title="Download PDF">pdf</a>, <a href="/format/2309.05657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the quality of randomized approximations of Tukey&#x27;s depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Briend%2C+S">Simon Briend</a>, 
<a href="/search/stat?searchtype=author&query=Lugosi%2C+G">G&#xe1;bor Lugosi</a>, 
<a href="/search/stat?searchtype=author&query=Oliveira%2C+R+I">Roberto Imbuzeiro Oliveira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06299" title="Abstract">arXiv:2309.06299</a> (replaced) [<a href="/pdf/2309.06299" title="Download PDF">pdf</a>, <a href="/format/2309.06299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Supply and Demand in Public Transportation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bihler%2C+M">Miranda Bihler</a>, 
<a href="/search/cs?searchtype=author&query=Nelson%2C+H">Hala Nelson</a>, 
<a href="/search/cs?searchtype=author&query=Okey%2C+E">Erin Okey</a>, 
<a href="/search/cs?searchtype=author&query=Rivas%2C+N+R">Noe Reyes Rivas</a>, 
<a href="/search/cs?searchtype=author&query=Webb%2C+J">John Webb</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+A">Anna White</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 2022 REU project at James Madison University
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07871" title="Abstract">arXiv:2309.07871</a> (replaced) [<a href="/pdf/2309.07871" title="Download PDF">pdf</a>, <a href="/format/2309.07871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient Dynamics in Linear Quadratic Network Games with Time-Varying  Connectivity and Population Fluctuation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taha%2C+F+A">Feras Al Taha</a>, 
<a href="/search/cs?searchtype=author&query=Rokade%2C+K">Kiran Rokade</a>, 
<a href="/search/cs?searchtype=author&query=Parise%2C+F">Francesca Parise</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures, Extended version of the original paper to appear in the proceedings of the 2023 IEEE Conference on Decision and Control (CDC). Updated numerical example
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Systems and Control (eess.SY); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08420" title="Abstract">arXiv:2309.08420</a> (replaced) [<a href="/pdf/2309.08420" title="Download PDF">pdf</a>, <a href="/format/2309.08420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedDCSR: Federated Cross-domain Sequential Recommendation via  Disentangled Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Dongyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiyuan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Q">Qing Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10532" title="Abstract">arXiv:2309.10532</a> (replaced) [<a href="/pdf/2309.10532" title="Download PDF">pdf</a>, <a href="/format/2309.10532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning  Using Contrastive Perceptual and Conceptual Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+D">Deepayan Sanyal</a>, 
<a href="/search/cs?searchtype=author&query=Ainooson%2C+J">James Ainooson</a>, 
<a href="/search/cs?searchtype=author&query=Michelson%2C+J">Joel Michelson</a>, 
<a href="/search/cs?searchtype=author&query=Farhana%2C+E">Effat Farhana</a>, 
<a href="/search/cs?searchtype=author&query=Kunda%2C+M">Maithilee Kunda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10631" title="Abstract">arXiv:2309.10631</a> (replaced) [<a href="/pdf/2309.10631" title="Download PDF">pdf</a>, <a href="/ps/2309.10631" title="Download PostScript">ps</a>, <a href="/format/2309.10631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Battery-Electric Powertrain System Design for the HorizonUAM Multirotor  Air Taxi Concept
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=J%C3%A4ger%2C+F">Florian J&#xe4;ger</a>, 
<a href="/search/eess?searchtype=author&query=Bertram%2C+O">Oliver Bertram</a>, 
<a href="/search/eess?searchtype=author&query=L%C3%BCbbe%2C+S+M">Sascha M. L&#xfc;bbe</a>, 
<a href="/search/eess?searchtype=author&query=Bismark%2C+A+H">Alexander H. Bismark</a>, 
<a href="/search/eess?searchtype=author&query=Rosenberg%2C+J">Jan Rosenberg</a>, 
<a href="/search/eess?searchtype=author&query=Bartscht%2C+L">Lukas Bartscht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 27 figures, CEAS Aeronautical Journal Special Issue "HorizonUAM - Opportunities and Challenges of Urban Air Mobility"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11895" title="Abstract">arXiv:2309.11895</a> (replaced) [<a href="/pdf/2309.11895" title="Download PDF">pdf</a>, <a href="/format/2309.11895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio Contrastive based Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Q">Qibin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chenghao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yizhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Moubayed%2C+N+A">Noura Al Moubayed</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12273" title="Abstract">arXiv:2309.12273</a> (replaced) [<a href="/pdf/2309.12273" title="Download PDF">pdf</a>, <a href="/format/2309.12273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving VTE Identification through Adaptive NLP Model Selection and  Clinical Expert Rule-based Classifier from Radiology Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jamie Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yusen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hayssen%2C+H">Hilary Hayssen</a>, 
<a href="/search/cs?searchtype=author&query=Englum%2C+B">Brain Englum</a>, 
<a href="/search/cs?searchtype=author&query=Kankaria%2C+A">Aman Kankaria</a>, 
<a href="/search/cs?searchtype=author&query=Mayorga-Carlin%2C+M">Minerva Mayorga-Carlin</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+S">Shalini Sahoo</a>, 
<a href="/search/cs?searchtype=author&query=Sorkin%2C+J">John Sorkin</a>, 
<a href="/search/cs?searchtype=author&query=Lal%2C+B">Brajesh Lal</a>, 
<a href="/search/cs?searchtype=author&query=Yesha%2C+Y">Yelena Yesha</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+P">Phuong Nguyen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 International Conference on Bioinformatics and Biomedicine
  (BIBM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12756" title="Abstract">arXiv:2309.12756</a> (replaced) [<a href="/pdf/2309.12756" title="Download PDF">pdf</a>, <a href="/format/2309.12756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards an MLOps Architecture for XAI in Industrial Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faubel%2C+L">Leonhard Faubel</a>, 
<a href="/search/cs?searchtype=author&query=Woudsma%2C+T">Thomas Woudsma</a>, 
<a href="/search/cs?searchtype=author&query=Methnani%2C+L">Leila Methnani</a>, 
<a href="/search/cs?searchtype=author&query=Ghezeljhemeidan%2C+A+G">Amir Ghorbani Ghezeljhemeidan</a>, 
<a href="/search/cs?searchtype=author&query=Buelow%2C+F">Fabian Buelow</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+K">Klaus Schmid</a>, 
<a href="/search/cs?searchtype=author&query=van+Driel%2C+W+D">Willem D. van Driel</a>, 
<a href="/search/cs?searchtype=author&query=Kloepper%2C+B">Benjamin Kloepper</a>, 
<a href="/search/cs?searchtype=author&query=Theodorou%2C+A">Andreas Theodorou</a>, 
<a href="/search/cs?searchtype=author&query=Nosratinia%2C+M">Mohsen Nosratinia</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A5ng%2C+M">Magnus B&#xe5;ng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13609" title="Abstract">arXiv:2309.13609</a> (replaced) [<a href="/pdf/2309.13609" title="Download PDF">pdf</a>, <a href="/format/2309.13609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vulnerabilities in Video Quality Assessment Models: The Challenge of  Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Ao-Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+Y">Yu Ran</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Weixuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuan-Gen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15302" title="Abstract">arXiv:2309.15302</a> (replaced) [<a href="/pdf/2309.15302" title="Download PDF">pdf</a>, <a href="/format/2309.15302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STERLING: Self-Supervised Terrain Representation Learning from  Unconstrained Robot Experience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karnan%2C+H">Haresh Karnan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Elvin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Farkash%2C+D">Daniel Farkash</a>, 
<a href="/search/cs?searchtype=author&query=Warnell%2C+G">Garrett Warnell</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://hareshkarnan.github.io/sterling/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Conference on Robot Learning (CoRL 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16948" title="Abstract">arXiv:2309.16948</a> (replaced) [<a href="/pdf/2309.16948" title="Download PDF">pdf</a>, <a href="/format/2309.16948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion Bridge Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Linqi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+A">Aaron Lou</a>, 
<a href="/search/cs?searchtype=author&query=Khanna%2C+S">Samar Khanna</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Github: <a href="https://github.com/alexzhou907/DDBM/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00111" title="Abstract">arXiv:2310.00111</a> (replaced) [<a href="/pdf/2310.00111" title="Download PDF">pdf</a>, <a href="/format/2310.00111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-efficient compression of $\mathcal{DH}^2$-matrices for  high-frequency problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=B%C3%B6rm%2C+S">Steffen B&#xf6;rm</a>, 
<a href="/search/math?searchtype=author&query=Henningsen%2C+J">Janne Henningsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01740" title="Abstract">arXiv:2310.01740</a> (replaced) [<a href="/pdf/2310.01740" title="Download PDF">pdf</a>, <a href="/format/2310.01740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Control of Soft Pneumatic Actuators with Approximated Dynamical Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wu-Te Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kurkcu%2C+B">Burak Kurkcu</a>, 
<a href="/search/cs?searchtype=author&query=Hirao%2C+M">Motohiro Hirao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingfeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+G+X">Grace X. Gu</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures, accepted by 2023 IEEE ROBIO conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01819" title="Abstract">arXiv:2310.01819</a> (replaced) [<a href="/pdf/2310.01819" title="Download PDF">pdf</a>, <a href="/format/2310.01819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amazing Combinatorial Creation: Acceptable Swap-Sampling for  Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zedong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://asst2i.github.io/anon/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02357" title="Abstract">arXiv:2310.02357</a> (replaced) [<a href="/pdf/2310.02357" title="Download PDF">pdf</a>, <a href="/ps/2310.02357" title="Download PostScript">ps</a>, <a href="/format/2310.02357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the definition of toxicity in NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berezin%2C+S">Sergey Berezin</a>, 
<a href="/search/cs?searchtype=author&query=Farahbakhsh%2C+R">Reza Farahbakhsh</a>, 
<a href="/search/cs?searchtype=author&query=Crespi%2C+N">Noel Crespi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02391" title="Abstract">arXiv:2310.02391</a> (replaced) [<a href="/pdf/2310.02391" title="Download PDF">pdf</a>, <a href="/format/2310.02391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SE(3)-Stochastic Flow Matching for Protein Backbone Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bose%2C+A+J">Avishek Joey Bose</a>, 
<a href="/search/cs?searchtype=author&query=Akhound-Sadegh%2C+T">Tara Akhound-Sadegh</a>, 
<a href="/search/cs?searchtype=author&query=Fatras%2C+K">Kilian Fatras</a>, 
<a href="/search/cs?searchtype=author&query=Huguet%2C+G">Guillaume Huguet</a>, 
<a href="/search/cs?searchtype=author&query=Rector-Brooks%2C+J">Jarrid Rector-Brooks</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cheng-Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Nica%2C+A+C">Andrei Cristian Nica</a>, 
<a href="/search/cs?searchtype=author&query=Korablyov%2C+M">Maksym Korablyov</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+A">Alexander Tong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02449" title="Abstract">arXiv:2310.02449</a> (replaced) [<a href="/pdf/2310.02449" title="Download PDF">pdf</a>, <a href="/format/2310.02449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of geography on the importance of parameters in infectious  disease models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Arindam Saha</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbani%2C+M">Maziar Ghorbani</a>, 
<a href="/search/cs?searchtype=author&query=Suleimenova%2C+D">Diana Suleimenova</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostou%2C+A">Anastasia Anagnostou</a>, 
<a href="/search/cs?searchtype=author&query=Groen%2C+D">Derek Groen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02596" title="Abstract">arXiv:2310.02596</a> (replaced) [<a href="/pdf/2310.02596" title="Download PDF">pdf</a>, <a href="/format/2310.02596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent  Text-to-3D
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuelin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+P">Ping Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://sweetdreamer3d.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02954" title="Abstract">arXiv:2310.02954</a> (replaced) [<a href="/pdf/2310.02954" title="Download PDF">pdf</a>, <a href="/format/2310.02954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for  In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jing Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yichun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhicheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingxing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiongwei Han</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05202" title="Abstract">arXiv:2310.05202</a> (replaced) [<a href="/pdf/2310.05202" title="Download PDF">pdf</a>, <a href="/format/2310.05202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Cross-Dataset Performance of Distracted Driving Detection With  Score-Softmax Classifier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+C">Cong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jiahao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minghai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jiacai Liao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Libo Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05227" title="Abstract">arXiv:2310.05227</a> (replaced) [<a href="/pdf/2310.05227" title="Download PDF">pdf</a>, <a href="/format/2310.05227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-aware Machine Learning Revolutionizes Scientific Paradigm for  Machine Learning and Process-based Hydrology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qingsong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yilei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Bamber%2C+J">Jonathan Bamber</a>, 
<a href="/search/cs?searchtype=author&query=Tuo%2C+Y">Ye Tuo</a>, 
<a href="/search/cs?searchtype=author&query=Ludwig%2C+R">Ralf Ludwig</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X+X">Xiao Xiang Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 6 figures. arXiv admin note: text overlap with <a href="/abs/2207.05748">arXiv:2207.05748</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05253" title="Abstract">arXiv:2310.05253</a> (replaced) [<a href="/pdf/2310.05253" title="Download PDF">pdf</a>, <a href="/format/2310.05253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Claim Verification via Knowledge-Grounded Reasoning with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+K">Kai Shu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05518" title="Abstract">arXiv:2310.05518</a> (replaced) [<a href="/pdf/2310.05518" title="Download PDF">pdf</a>, <a href="/format/2310.05518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Double Descent in Reinforcement Learning with LSTD and Random  Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brellmann%2C+D">David Brellmann</a>, 
<a href="/search/cs?searchtype=author&query=Berthier%2C+E">Elo&#xef;se Berthier</a>, 
<a href="/search/cs?searchtype=author&query=Filliat%2C+D">David Filliat</a>, 
<a href="/search/cs?searchtype=author&query=Frehse%2C+G">Goran Frehse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05991" title="Abstract">arXiv:2310.05991</a> (replaced) [<a href="/pdf/2310.05991" title="Download PDF">pdf</a>, <a href="/format/2310.05991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Document-level Event Argument Extraction with Contextual Clues  and Role Relevance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wanlong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Shaohuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dingyi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Hong Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of ACL2023, correct some mistakes. arXiv admin note: text overlap with <a href="/abs/2310.05116">arXiv:2310.05116</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06675" title="Abstract">arXiv:2310.06675</a> (replaced) [<a href="/pdf/2310.06675" title="Download PDF">pdf</a>, <a href="/format/2310.06675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tonglet%2C+J">Jonathan Tonglet</a>, 
<a href="/search/cs?searchtype=author&query=Reusens%2C+M">Manon Reusens</a>, 
<a href="/search/cs?searchtype=author&query=Borchert%2C+P">Philipp Borchert</a>, 
<a href="/search/cs?searchtype=author&query=Baesens%2C+B">Bart Baesens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready revision for EMNLP 2023 main conference. Code available at <a href="https://github.com/jtonglet/SEER">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06923" title="Abstract">arXiv:2310.06923</a> (replaced) [<a href="/pdf/2310.06923" title="Download PDF">pdf</a>, <a href="/format/2310.06923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PICProp: Physics-Informed Confidence Propagation for Uncertainty  Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qianli Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W+H">Wai Hoh Tang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhun Deng</a>, 
<a href="/search/cs?searchtype=author&query=Psaros%2C+A">Apostolos Psaros</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023. Code is available at <a href="https://github.com/ShenQianli/PICProp">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07328" title="Abstract">arXiv:2310.07328</a> (replaced) [<a href="/pdf/2310.07328" title="Download PDF">pdf</a>, <a href="/format/2310.07328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of Instruction-tuning Large Language Models in  Chinese
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+Q">Qingyi Si</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yanan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07659" title="Abstract">arXiv:2310.07659</a> (replaced) [<a href="/pdf/2310.07659" title="Download PDF">pdf</a>, <a href="/format/2310.07659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for  Knowledge-Grounded Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Lang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Hongru Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhenglu Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07871" title="Abstract">arXiv:2310.07871</a> (replaced) [<a href="/pdf/2310.07871" title="Download PDF">pdf</a>, <a href="/format/2310.07871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Pretraining on Multimodal Electronic Health Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Junyu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Ziyi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Suhan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yuan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07957" title="Abstract">arXiv:2310.07957</a> (replaced) [<a href="/pdf/2310.07957" title="Download PDF">pdf</a>, <a href="/format/2310.07957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Approach Towards Autoformalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+N">Nilay Patel</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+R">Rahul Saha</a>, 
<a href="/search/cs?searchtype=author&query=Flanigan%2C+J">Jeffrey Flanigan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at MATHAI 2023 @ NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08045" title="Abstract">arXiv:2310.08045</a> (replaced) [<a href="/pdf/2310.08045" title="Download PDF">pdf</a>, <a href="/format/2310.08045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Predictive Inferential Control of Neural State-Space Models for  Autonomous Vehicle Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Askari%2C+I">Iman Askari</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+X">Xumein Tu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shen Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Huazhen Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08130" title="Abstract">arXiv:2310.08130</a> (replaced) [<a href="/pdf/2310.08130" title="Download PDF">pdf</a>, <a href="/format/2310.08130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-grained Conversational Decoding via Isotropic and Proximal Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuxuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiling Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EMNLP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08394" title="Abstract">arXiv:2310.08394</a> (replaced) [<a href="/pdf/2310.08394" title="Download PDF">pdf</a>, <a href="/format/2310.08394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Better Evaluation of Instruction-Following: A Case-Study in  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Skopek%2C+O">Ondrej Skopek</a>, 
<a href="/search/cs?searchtype=author&query=Aralikatte%2C+R">Rahul Aralikatte</a>, 
<a href="/search/cs?searchtype=author&query=Gooding%2C+S">Sian Gooding</a>, 
<a href="/search/cs?searchtype=author&query=Carbune%2C+V">Victor Carbune</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoNLL 2023 camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08740" title="Abstract">arXiv:2310.08740</a> (replaced) [<a href="/pdf/2310.08740" title="Download PDF">pdf</a>, <a href="/format/2310.08740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Zero-Shot Language Agent for Computer Control with Structured  Reflection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gang Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bryan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08877" title="Abstract">arXiv:2310.08877</a> (replaced) [<a href="/pdf/2310.08877" title="Download PDF">pdf</a>, <a href="/format/2310.08877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue  System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Weizhou Shen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yingqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Canbin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fanqi Wan</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08981" title="Abstract">arXiv:2310.08981</a> (replaced) [<a href="/pdf/2310.08981" title="Download PDF">pdf</a>, <a href="/format/2310.08981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-latency Speech Enhancement via Speech Token Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Huaying Xue</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xiulian Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yan Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, submitted to ICASSP2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08996" title="Abstract">arXiv:2310.08996</a> (replaced) [<a href="/pdf/2310.08996" title="Download PDF">pdf</a>, <a href="/format/2310.08996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Qualitative Analysis for Validating IEC 62443-4-2 Requirements in  DevSecOps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%B6ttel%2C+C">Christian G&#xf6;ttel</a>, 
<a href="/search/cs?searchtype=author&query=Kabir-Querrec%2C+M">Ma&#xeb;lle Kabir-Querrec</a>, 
<a href="/search/cs?searchtype=author&query=Kozhaya%2C+D">David Kozhaya</a>, 
<a href="/search/cs?searchtype=author&query=Sivanthi%2C+T">Thanikesavan Sivanthi</a>, 
<a href="/search/cs?searchtype=author&query=Vukovi%C4%87%2C+O">Ognjen Vukovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09135" title="Abstract">arXiv:2310.09135</a> (replaced) [<a href="/pdf/2310.09135" title="Download PDF">pdf</a>, <a href="/format/2310.09135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework  for Cross-Domain Zero-Shot Slot Filling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yin Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP2023 as a long paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09503" title="Abstract">arXiv:2310.09503</a> (replaced) [<a href="/pdf/2310.09503" title="Download PDF">pdf</a>, <a href="/format/2310.09503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JM3D &amp; JM3D-LLM: Elevating 3D Representation with Joint Multi-modal Cues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiayi Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Changli Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yiwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoshuai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09886" title="Abstract">arXiv:2310.09886</a> (replaced) [<a href="/pdf/2310.09886" title="Download PDF">pdf</a>, <a href="/format/2310.09886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lifelong Sequence Generation with Dynamic Module Expansion and  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Chengwei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10092" title="Abstract">arXiv:2310.10092</a> (replaced) [<a href="/pdf/2310.10092" title="Download PDF">pdf</a>, <a href="/ps/2310.10092" title="Download PostScript">ps</a>, <a href="/format/2310.10092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Differential Privacy via Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brahmbhatt%2C+A">Anand Brahmbhatt</a>, 
<a href="/search/cs?searchtype=author&query=Saket%2C+R">Rishi Saket</a>, 
<a href="/search/cs?searchtype=author&query=Havaldar%2C+S">Shreyas Havaldar</a>, 
<a href="/search/cs?searchtype=author&query=Nasery%2C+A">Anshul Nasery</a>, 
<a href="/search/cs?searchtype=author&query=Raghuveer%2C+A">Aravindan Raghuveer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10196" title="Abstract">arXiv:2310.10196</a> (replaced) [<a href="/pdf/2310.10196" title="Download PDF">pdf</a>, <a href="/format/2310.10196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Models for Time Series and Spatio-Temporal Data: A Survey and  Outlook
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Ming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Siqiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">James Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoli Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+V+S">Vincent S. Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work; 24 pages, 3 figures, 3 tables; Github page: <a href="https://github.com/qingsongedu/Awesome-TimeSeries-SpatioTemporal-LM-LLM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10321" title="Abstract">arXiv:2310.10321</a> (replaced) [<a href="/pdf/2310.10321" title="Download PDF">pdf</a>, <a href="/format/2310.10321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hamming Encoder: Mining Discriminative k-mers for Discrete Sequence  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Junjie Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Mudi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lianyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zengyou He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10404" title="Abstract">arXiv:2310.10404</a> (replaced) [<a href="/pdf/2310.10404" title="Download PDF">pdf</a>, <a href="/format/2310.10404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM4SGG: Large Language Model for Weakly Supervised Scene Graph  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kibum Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+K">Kanghoon Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+J">Jaehyeong Jeon</a>, 
<a href="/search/cs?searchtype=author&query=In%2C+Y">Yeonjun In</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+J">Jinyoung Moon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Donghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10477" title="Abstract">arXiv:2310.10477</a> (replaced) [<a href="/pdf/2310.10477" title="Download PDF">pdf</a>, <a href="/format/2310.10477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jianhua Han</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenyong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11097" title="Abstract">arXiv:2310.11097</a> (replaced) [<a href="/pdf/2310.11097" title="Download PDF">pdf</a>, <a href="/format/2310.11097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimenting AI Technologies for Disinformation Combat: the IDMO  Project
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Canale%2C+L">Lorenzo Canale</a>, 
<a href="/search/cs?searchtype=author&query=Messina%2C+A">Alberto Messina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11099" title="Abstract">arXiv:2310.11099</a> (replaced) [<a href="/pdf/2310.11099" title="Download PDF">pdf</a>, <a href="/format/2310.11099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Local Patterns of Child Pornography Consumption in France  using Tor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koebe%2C+T">Till Koebe</a>, 
<a href="/search/cs?searchtype=author&query=del+Villar%2C+Z">Zinnya del Villar</a>, 
<a href="/search/cs?searchtype=author&query=Nutakki%2C+B">Brahmani Nutakki</a>, 
<a href="/search/cs?searchtype=author&query=Sagimbayeva%2C+N">Nursulu Sagimbayeva</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+I">Ingmar Weber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In the first version of the arXiv preprint of this paper, we reported this share to be 16.9 \%. This was based on a misinterpretation of the Tor statistics. After expert discussions, we corrected it and any subsequent analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11254" title="Abstract">arXiv:2310.11254</a> (replaced) [<a href="/pdf/2310.11254" title="Download PDF">pdf</a>, <a href="/ps/2310.11254" title="Download PostScript">ps</a>, <a href="/format/2310.11254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Triangulations Admit Dominating Sets of Size $2n/7$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Christiansen%2C+A+B+G">Aleksander B. G. Christiansen</a>, 
<a href="/search/math?searchtype=author&query=Rotenberg%2C+E">Eva Rotenberg</a>, 
<a href="/search/math?searchtype=author&query=Rutschmann%2C+D">Daniel Rutschmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11638" title="Abstract">arXiv:2310.11638</a> (replaced) [<a href="/pdf/2310.11638" title="Download PDF">pdf</a>, <a href="/format/2310.11638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Assessment of Factual Knowledge in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Linhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Thuy-Trang Vu</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11675" title="Abstract">arXiv:2310.11675</a> (replaced) [<a href="/pdf/2310.11675" title="Download PDF">pdf</a>, <a href="/format/2310.11675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Relevance to Utility: Evidence Retrieval with Feedback for Fact  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiafeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yixing Fan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Acctepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11710" title="Abstract">arXiv:2310.11710</a> (replaced) [<a href="/pdf/2310.11710" title="Download PDF">pdf</a>, <a href="/format/2310.11710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Co-Speech Gesture for Multimodal Aphasia Type Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Daeun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Son%2C+S">Sejung Son</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyolim Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungbae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jinyoung Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11755" title="Abstract">arXiv:2310.11755</a> (replaced) [<a href="/pdf/2310.11755" title="Download PDF">pdf</a>, <a href="/format/2310.11755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RGM: A Robust Generalist Matching Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xinyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages. Code is available at: <a href="https://github.com/aim-uofa/RGM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11772" title="Abstract">arXiv:2310.11772</a> (replaced) [<a href="/pdf/2310.11772" title="Download PDF">pdf</a>, <a href="/format/2310.11772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Long Document Topic Segmentation Models With Enhanced  Coherence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023. Codes is available at <a href="https://github.com/alibaba-damo-academy/SpokenNLP/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11838" title="Abstract">arXiv:2310.11838</a> (replaced) [<a href="/pdf/2310.11838" title="Download PDF">pdf</a>, <a href="/format/2310.11838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Bootstrapping for Uncertainty Quantification in Imaging  Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tachella%2C+J">Julian Tachella</a>, 
<a href="/search/eess?searchtype=author&query=Pereyra%2C+M">Marcelo Pereyra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12004" title="Abstract">arXiv:2310.12004</a> (replaced) [<a href="/pdf/2310.12004" title="Download PDF">pdf</a>, <a href="/format/2310.12004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Super-resolution Via Latent Diffusion: A Sampling-space Mixture Of  Experts And Frequency-augmented Decoder Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Feng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jinxi Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wei Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12187" title="Abstract">arXiv:2310.12187</a> (replaced) [<a href="/pdf/2310.12187" title="Download PDF">pdf</a>, <a href="/format/2310.12187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Session Types With Multiple Senders Single Receiver (report version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zekun Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiong Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12347" title="Abstract">arXiv:2310.12347</a> (replaced) [<a href="/pdf/2310.12347" title="Download PDF">pdf</a>, <a href="/format/2310.12347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VisGrader: Automatic Grading of D3 Visualizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hull%2C+M">Matthew Hull</a>, 
<a href="/search/cs?searchtype=author&query=Pednekar%2C+V">Vivian Pednekar</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+H">Hannah Murray</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+N">Nimisha Roy</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+E">Emmanuel Tung</a>, 
<a href="/search/cs?searchtype=author&query=Routray%2C+S">Susanta Routray</a>, 
<a href="/search/cs?searchtype=author&query=Guerin%2C+C">Connor Guerin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Justin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z+J">Zijie J. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seongmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Roozbahani%2C+M">Mahdi Roozbahani</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+D+H">Duen Horng Chau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12489" title="Abstract">arXiv:2310.12489</a> (replaced) [<a href="/pdf/2310.12489" title="Download PDF">pdf</a>, <a href="/format/2310.12489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI  Responses in Health Consultations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ojo%2C+O+E">Olumide E. Ojo</a>, 
<a href="/search/cs?searchtype=author&query=Adebanji%2C+O+O">Olaronke O. Adebanji</a>, 
<a href="/search/cs?searchtype=author&query=Gelbukh%2C+A">Alexander Gelbukh</a>, 
<a href="/search/cs?searchtype=author&query=Calvo%2C+H">Hiram Calvo</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+A">Anna Feldman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12531" title="Abstract">arXiv:2310.12531</a> (replaced) [<a href="/pdf/2310.12531" title="Download PDF">pdf</a>, <a href="/format/2310.12531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICU: Conquering Language Barriers in Vision-and-Language Modeling by  Dividing the Tasks into Image Captioning and Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Guojun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP23 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12553" title="Abstract">arXiv:2310.12553</a> (replaced) [<a href="/pdf/2310.12553" title="Download PDF">pdf</a>, <a href="/format/2310.12553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explanation-Based Training with Differentiable Insertion/Deletion  Metric-Aware Regularizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoshikawa%2C+Y">Yuya Yoshikawa</a>, 
<a href="/search/cs?searchtype=author&query=Iwata%2C+T">Tomoharu Iwata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12632" title="Abstract">arXiv:2310.12632</a> (replaced) [<a href="/pdf/2310.12632" title="Download PDF">pdf</a>, <a href="/format/2310.12632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Deep Learning-based Online Quality Prediction System for  Welding Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahn%2C+Y">Yannik Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Maack%2C+R">Robert Maack</a>, 
<a href="/search/cs?searchtype=author&query=Buchholz%2C+G">Guido Buchholz</a>, 
<a href="/search/cs?searchtype=author&query=Purrio%2C+M">Marion Purrio</a>, 
<a href="/search/cs?searchtype=author&query=Angerhausen%2C+M">Matthias Angerhausen</a>, 
<a href="/search/cs?searchtype=author&query=Tercan%2C+H">Hasan Tercan</a>, 
<a href="/search/cs?searchtype=author&query=Meisen%2C+T">Tobias Meisen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for CIRP CMS '23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12737" title="Abstract">arXiv:2310.12737</a> (replaced) [<a href="/pdf/2310.12737" title="Download PDF">pdf</a>, <a href="/format/2310.12737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new complex variable solution on noncircular shallow tunnelling with  reasonable far-field displacement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lin%2C+L">Luo-bin Lin</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+F">Fu-quan Chen</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+S">Shang-shun Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 10 figures. arXiv admin note: text overlap with <a href="/abs/2308.03994">arXiv:2308.03994</a> (already rewritten to minimize the text overlap)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Complex Variables (math.CV)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12763" title="Abstract">arXiv:2310.12763</a> (replaced) [<a href="/pdf/2310.12763" title="Download PDF">pdf</a>, <a href="/ps/2310.12763" title="Download PostScript">ps</a>, <a href="/format/2310.12763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Liveness Properties in Geometric Logic for Domain-Theoretic Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Riba%2C+C">Colin Riba</a>, 
<a href="/search/cs?searchtype=author&query=Stern%2C+S">Solal Stern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12781" title="Abstract">arXiv:2310.12781</a> (replaced) [<a href="/pdf/2310.12781" title="Download PDF">pdf</a>, <a href="/format/2310.12781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Density Estimations from Privacy-Protected Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xiong%2C+Y">Yifei Xiong</a>, 
<a href="/search/stat?searchtype=author&query=Ju%2C+N+P">Nianqiao P. Ju</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+S">Sanguo Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12787" title="Abstract">arXiv:2310.12787</a> (replaced) [<a href="/pdf/2310.12787" title="Download PDF">pdf</a>, <a href="/format/2310.12787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DT/MARS-CycleGAN: Improved Object Detection for MARS Phenotyping Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">David Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhengkun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changying Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12821" title="Abstract">arXiv:2310.12821</a> (replaced) [<a href="/pdf/2310.12821" title="Download PDF">pdf</a>, <a href="/format/2310.12821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding  with Large Language Model Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xin Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tengxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shengdong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiqiang Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item329">Cross-lists</a></li>
<li><a href="#item373">Replacements</a></li>
</ul>
<small>[ total of 608 entries:  <b>1-608</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
